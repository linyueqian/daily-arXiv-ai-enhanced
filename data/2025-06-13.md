<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 110]
- [cs.CV](#cs.CV) [Total: 140]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.LG](#cs.LG) [Total: 172]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 13]
- [eess.AS](#eess.AS) [Total: 7]
- [eess.IV](#eess.IV) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/pdf/2506.10019)
*Tian Lan, Yang-Hao Zhou, Zi-Ao Ma, Fanshu Sun, Rui-Qing Sun, Junyu Luo, Rong-Cheng Tu, Heyan Huang, Chen Xu, Zhijing Wu, Xian-Ling Mao*

Main category: cs.CL

TL;DR: A review and taxonomy of automatic evaluation methods for generative AI outputs across text, images, and audio, identifying five key paradigms and suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: The lack of a systematic framework for evaluating generative AI outputs across multiple modalities (text, images, audio) motivates this comprehensive review.

Method: The study reviews and organizes existing evaluation methods into a unified taxonomy, starting with text (most mature) and extending to images and audio.

Result: Five fundamental paradigms for evaluation methods are identified, and the framework's applicability across modalities is demonstrated.

Conclusion: Future research should focus on cross-modal evaluation methodologies to further enhance generative AI assessment.

Abstract: Recent advances in deep learning have significantly enhanced generative AI
capabilities across text, images, and audio. However, automatically evaluating
the quality of these generated outputs presents ongoing challenges. Although
numerous automatic evaluation methods exist, current research lacks a
systematic framework that comprehensively organizes these methods across text,
visual, and audio modalities. To address this issue, we present a comprehensive
review and a unified taxonomy of automatic evaluation methods for generated
content across all three modalities; We identify five fundamental paradigms
that characterize existing evaluation approaches across these domains. Our
analysis begins by examining evaluation methods for text generation, where
techniques are most mature. We then extend this framework to image and audio
generation, demonstrating its broad applicability. Finally, we discuss
promising directions for future research in cross-modal evaluation
methodologies.

</details>


### [2] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/pdf/2506.10055)
*Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King Zhu, Minghao Yang, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou*

Main category: cs.CL

TL;DR: TaskCraft is an automated workflow for generating scalable, multi-tool agentic tasks, addressing limitations in existing instruction data and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing agentic benchmarks lack scalability due to reliance on human annotation, and instruction data lacks tool interaction.

Method: TaskCraft uses depth-based and width-based extensions to create complex tasks, generating a synthetic dataset of ~36,000 tasks.

Result: The tasks improve prompt optimization and enhance fine-tuning of agentic models.

Conclusion: TaskCraft provides a scalable solution for agent tuning and evaluation, supported by a large synthetic dataset.

Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool
use, and adaptive reasoning, are becoming increasingly central to the
advancement of NLP and AI. However, existing instruction data lacks tool
interaction, and current agentic benchmarks rely on costly human annotation,
limiting their scalability. We introduce \textsc{TaskCraft}, an automated
workflow for generating difficulty-scalable, multi-tool, and verifiable agentic
tasks with execution trajectories. TaskCraft expands atomic tasks using
depth-based and width-based extensions to create structurally and
hierarchically complex challenges. Empirical results show that these tasks
improve prompt optimization in the generation workflow and enhance supervised
fine-tuning of agentic foundation models. We present a large-scale synthetic
dataset of approximately 36,000 tasks with varying difficulty to support future
research on agent tuning and evaluation.

</details>


### [3] [A quantum semantic framework for natural language processing](https://arxiv.org/pdf/2506.10077)
*Christopher J. Agostino, Quan Le Thien, Molly Apsel, Denizhan Pak, Elina Lesyk, Ashabari Majumdar*

Main category: cs.CL

TL;DR: The paper explores semantic degeneracy in natural language, showing that as expressions grow in complexity, recovering intended meaning becomes computationally intractable. It argues meaning is observer-dependent and demonstrates non-classical contextuality in linguistic interpretation using LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs and NLP systems due to semantic degeneracy, challenging the classical view of inherent meaning in linguistic forms.

Method: Used Kolmogorov complexity to argue computational intractability and conducted a semantic Bell inequality test with LLMs interpreting ambiguous word pairs under varied contexts.

Result: Found CHSH expectation values (1.2-2.8) violating classical boundaries, showing non-classical contextuality in linguistic interpretation.

Conclusion: Classical frequentist approaches are lossy; Bayesian-style repeated sampling is proposed for better characterization of linguistic meaning.

Abstract: Semantic degeneracy represents a fundamental property of natural language
that extends beyond simple polysemy to encompass the combinatorial explosion of
potential interpretations that emerges as semantic expressions increase in
complexity. Large Language Models (LLMs) and other modern NLP systems face
inherent limitations precisely because they operate within natural language
itself, making them subject to the same interpretive constraints imposed by
semantic degeneracy. In this work, we argue using Kolmogorov complexity that as
an expression's complexity grows, the likelihood of any interpreting agent
(human or LLM-powered AI) recovering the single intended meaning vanishes. This
computational intractability suggests the classical view that linguistic forms
possess meaning in and of themselves is flawed. We alternatively posit that
meaning is instead actualized through an observer-dependent interpretive act.
To test this, we conducted a semantic Bell inequality test using diverse LLM
agents as ``computational cognitive systems'' to interpret ambiguous word pairs
under varied contextual settings. Across several independent experiments, we
found average CHSH expectation values ranging from 1.2 to 2.8, with several
runs yielding values (e.g., 2.3-2.4) that significantly violate the classical
boundary ($|S|\leq2$). This demonstrates that linguistic interpretation under
ambiguity can exhibit non-classical contextuality, consistent with results from
human cognition experiments. These results inherently imply that classical
frequentist-based analytical approaches for natural language are necessarily
lossy. Instead, we propose that Bayesian-style repeated sampling approaches can
provide more practically useful and appropriate characterizations of linguistic
meaning in context.

</details>


### [4] [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/pdf/2506.10086)
*Christodoulos Constantinides, Shuxin Lin, Nianjun Zhou, Dhaval Patel*

Main category: cs.CL

TL;DR: A multi-agent system, Chat-of-Thought, uses collaborative LLM-based agents to generate and refine FMEA documents for industrial assets through dynamic discussions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in generating and validating FMEA documents for industrial equipment by leveraging AI-driven multi-agent collaboration.

Method: Employs multiple LLM-based agents with specific roles, dynamic task routing, and iterative refinement via Chat of Thought discussions.

Result: Demonstrates potential in optimizing FMEA table generation through interactive, context-aware workflows.

Conclusion: Chat-of-Thought offers a promising solution for industrial asset monitoring by enhancing FMEA document quality and efficiency.

Abstract: This paper presents a novel multi-agent system called Chat-of-Thought,
designed to facilitate the generation of Failure Modes and Effects Analysis
(FMEA) documents for industrial assets. Chat-of-Thought employs multiple
collaborative Large Language Model (LLM)-based agents with specific roles,
leveraging advanced AI techniques and dynamic task routing to optimize the
generation and validation of FMEA tables. A key innovation in this system is
the introduction of a Chat of Thought, where dynamic, multi-persona-driven
discussions enable iterative refinement of content. This research explores the
application domain of industrial equipment monitoring, highlights key
challenges, and demonstrates the potential of Chat-of-Thought in addressing
these challenges through interactive, template-driven workflows and
context-aware agent collaboration.

</details>


### [5] [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/pdf/2506.10095)
*Xiao Li, Joel Kreuzwieser, Alan Peters*

Main category: cs.CL

TL;DR: The paper studies how large language models (LLMs) behave differently under semantically equivalent but token-varied prompts, introducing Prompt-Based Semantic Shift (PBSS) to measure this drift.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify behavioral inconsistencies in LLMs when prompts are reworded but retain the same meaning.

Method: Proposes PBSS, a diagnostic framework, and applies it to ten constrained tasks to measure response shifts.

Result: Finds consistent, model-specific shifts in responses, linking them to tokenization and decoding processes.

Conclusion: Highlights the need for evaluating model stability under rephrasing and suggests tokenization and decoding impact post-training performance.

Abstract: We investigate how large language models respond to prompts that differ only
in their token-level realization but preserve the same semantic intent, a
phenomenon we call prompt variance. We propose Prompt-Based Semantic Shift
(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under
semantically equivalent prompt rewordings. Applied to ten constrained tasks,
PBSS reveals consistent, model-specific response shifts, suggesting statistical
regularities linked to tokenization and decoding. These results highlight an
overlooked dimension of model evaluation stability under rephrasing and suggest
that tokenization strategies and decoding dynamics may contribute to
post-training quality of service instability.

</details>


### [6] [ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering](https://arxiv.org/pdf/2506.10116)
*Caijun Jia, Nan Xu, Jingxuan Wei, Qingli Wang, Lei Wang, Bihui Yu, Junnan Zhu*

Main category: cs.CL

TL;DR: ChartReasoner is a two-stage framework for visual reasoning over charts, converting images to structured codes and synthesizing reasoning data, achieving high performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing methods lose critical visual details in multimodal reasoning tasks like chart question answering, necessitating a more precise approach.

Method: A two-stage framework: converting chart images to structured ECharts codes, then synthesizing reasoning data with a pretrained model and code validator. Training combines supervised fine-tuning and reinforcement learning.

Result: Outperforms open-source models with fewer parameters and approaches GPT-4o performance in out-of-domain settings.

Conclusion: ChartReasoner effectively preserves chart details and enables precise, interpretable reasoning, bridging the gap in visual reasoning tasks.

Abstract: Recently, large language models have shown remarkable reasoning capabilities
through long-chain reasoning before responding. However, how to extend this
capability to visual reasoning tasks remains an open challenge. Existing
multimodal reasoning approaches transfer such visual reasoning task into
textual reasoning task via several image-to-text conversions, which often lose
critical structural and semantic information embedded in visualizations,
especially for tasks like chart question answering that require a large amount
of visual details. To bridge this gap, we propose ChartReasoner, a code-driven
novel two-stage framework designed to enable precise, interpretable reasoning
over charts. We first train a high-fidelity model to convert diverse chart
images into structured ECharts codes, preserving both layout and data semantics
as lossless as possible. Then, we design a general chart reasoning data
synthesis pipeline, which leverages this pretrained transport model to
automatically and scalably generate chart reasoning trajectories and utilizes a
code validator to filter out low-quality samples. Finally, we train the final
multimodal model using a combination of supervised fine-tuning and
reinforcement learning on our synthesized chart reasoning dataset and
experimental results on four public benchmarks clearly demonstrate the
effectiveness of our proposed ChartReasoner. It can preserve the original
details of the charts as much as possible and perform comparably with
state-of-the-art open-source models while using fewer parameters, approaching
the performance of proprietary systems like GPT-4o in out-of-domain settings.

</details>


### [7] [Unsupervised Elicitation of Language Models](https://arxiv.org/pdf/2506.10139)
*Jiaxin Wen, Zachary Ankner, Arushi Somani, Peter Hase, Samuel Marks, Jacob Goldman-Wetzler, Linda Petrini, Henry Sleight, Collin Burns, He He, Shi Feng, Ethan Perez, Jan Leike*

Main category: cs.CL

TL;DR: ICM, an unsupervised algorithm, fine-tunes language models using their own generated labels, outperforming human-supervised methods on superhuman tasks.


<details>
  <summary>Details</summary>
Motivation: Human supervision is inadequate for superhuman-capable models, necessitating unsupervised methods.

Method: Internal Coherence Maximization (ICM) fine-tunes models on self-generated labels without external supervision.

Result: ICM matches or surpasses human-supervised performance on tasks like GSM8k-verification and TruthfulQA, and improves frontier LM training.

Conclusion: ICM effectively elicits superhuman capabilities in models, outperforming human supervision in certain tasks.

Abstract: To steer pretrained language models for downstream tasks, today's
post-training paradigm relies on humans to specify desired behaviors. However,
for models with superhuman capabilities, it is difficult or impossible to get
high-quality human supervision. To address this challenge, we introduce a new
unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune
pretrained language models on their own generated labels, \emph{without
external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward
modeling tasks, our method matches the performance of training on golden
supervision and outperforms training on crowdsourced human supervision. On
tasks where LMs' capabilities are strongly superhuman, our method can elicit
those capabilities significantly better than training on human labels. Finally,
we show that our method can improve the training of frontier LMs: we use our
method to train an unsupervised reward model and use reinforcement learning to
train a Claude 3.5 Haiku-based assistant. Both the reward model and the
assistant outperform their human-supervised counterparts.

</details>


### [8] [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/pdf/2506.10974)
*Yixin Ou, Yujie Luo, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang, Da Zheng, Huajun Chen, Ningyu Zhang*

Main category: cs.CL

TL;DR: AutoMind is an adaptive LLM-agent framework for automated data science, outperforming existing methods with expert knowledge, strategic search, and dynamic coding.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-driven data science agents are limited by rigid workflows and lack empirical expertise for complex tasks.

Method: AutoMind uses (1) an expert knowledge base, (2) agentic tree search, and (3) adaptive coding to tailor solutions.

Result: AutoMind outperforms state-of-the-art baselines in benchmarks, showing effectiveness, efficiency, and high solution quality.

Conclusion: AutoMind advances automated data science by addressing limitations of current frameworks, offering robustness and adaptability.

Abstract: Large Language Model (LLM) agents have shown great potential in addressing
real-world data science problems. LLM-driven data science agents promise to
automate the entire machine learning pipeline, yet their real-world
effectiveness remains limited. Existing frameworks depend on rigid, pre-defined
workflows and inflexible coding strategies; consequently, they excel only on
relatively simple, classical problems and fail to capture the empirical
expertise that human practitioners bring to complex, innovative tasks. In this
work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework
that overcomes these deficiencies through three key advances: (1) a curated
expert knowledge base that grounds the agent in domain expert knowledge, (2) an
agentic knowledgeable tree search algorithm that strategically explores
possible solutions, and (3) a self-adaptive coding strategy that dynamically
tailors code generation to task complexity. Evaluations on two automated data
science benchmarks demonstrate that AutoMind delivers superior performance
versus state-of-the-art baselines. Additional analyses confirm favorable
effectiveness, efficiency, and qualitative solution quality, highlighting
AutoMind as an efficient and robust step toward fully automated data science.

</details>


### [9] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/pdf/2506.10150)
*Aakriti Kumar, Nalin Poungpeth, Diyi Yang, Erina Farrell, Bruce Lambert, Matthew Groh*

Main category: cs.CL

TL;DR: LLMs perform reliably in judging empathic communication, matching expert agreement and surpassing crowdworkers, validated across four evaluative frameworks.


<details>
  <summary>Details</summary>
Motivation: To assess how reliably LLMs judge empathic nuances compared to experts and crowdworkers, using real-world conversations.

Method: Compared annotations from experts, crowdworkers, and LLMs on 200 conversations, analyzing inter-rater reliability across four frameworks.

Result: LLMs approach expert-level agreement and exceed crowdworker reliability, varying by framework sub-components' clarity and complexity.

Conclusion: LLMs can support emotionally sensitive applications when validated with appropriate benchmarks, ensuring transparency and oversight.

Abstract: Large language models (LLMs) excel at generating empathic responses in
text-based conversations. But, how reliably do they judge the nuances of
empathic communication? We investigate this question by comparing how experts,
crowdworkers, and LLMs annotate empathic communication across four evaluative
frameworks drawn from psychology, natural language processing, and
communications applied to 200 real-world conversations where one speaker shares
a personal problem and the other offers support. Drawing on 3,150 expert
annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess
inter-rater reliability between these three annotator groups. We find that
expert agreement is high but varies across the frameworks' sub-components
depending on their clarity, complexity, and subjectivity. We show that expert
agreement offers a more informative benchmark for contextualizing LLM
performance than standard classification metrics. Across all four frameworks,
LLMs consistently approach this expert level benchmark and exceed the
reliability of crowdworkers. These results demonstrate how LLMs, when validated
on specific tasks with appropriate benchmarks, can support transparency and
oversight in emotionally sensitive applications including their use as
conversational companions.

</details>


### [10] [Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME](https://arxiv.org/pdf/2506.10154)
*Bidyarthi Paul, SM Musfiqur Rahman, Dipta Biswas, Md. Ziaul Hasan, Md. Zahid Hossain*

Main category: cs.CL

TL;DR: The study explores emotion analysis in Bangla using machine learning models (Linear SVM, KNN, Random Forest, BiLSTM, AdaBoost) and PCA for dimensionality reduction, with LIME for explainability.


<details>
  <summary>Details</summary>
Motivation: To advance sentiment analysis in understudied languages like Bangla by identifying efficient emotion identification techniques.

Method: Employed Linear SVM, KNN, Random Forest, BiLSTM, and AdaBoost with TF-IDF vectorizer and PCA. Used LIME for explainability.

Result: Analyzed 22,698 social media comments from the EmoNoBa dataset to evaluate model performance.

Conclusion: The study contributes to emotion analysis in Bangla by testing various techniques and enhancing model interpretability.

Abstract: Research on understanding emotions in written language continues to expand,
especially for understudied languages with distinctive regional expressions and
cultural features, such as Bangla. This study examines emotion analysis using
22,698 social media comments from the EmoNoBa dataset. For language analysis,
we employ machine learning models: Linear SVM, KNN, and Random Forest with
n-gram data from a TF-IDF vectorizer. We additionally investigated how PCA
affects the reduction of dimensionality. Moreover, we utilized a BiLSTM model
and AdaBoost to improve decision trees. To make our machine learning models
easier to understand, we used LIME to explain the predictions of the AdaBoost
classifier, which uses decision trees. With the goal of advancing sentiment
analysis in languages with limited resources, our work examines various
techniques to find efficient techniques for emotion identification in Bangla.

</details>


### [11] [Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs](https://arxiv.org/pdf/2506.10299)
*Hayato Futami, Emiru Tsunoo, Yosuke Kashiwagi, Yuki Ito, Hassan Shahmohammadi, Siddhant Arora, Shinji Watanabe*

Main category: cs.CL

TL;DR: Proposes scheduled interleaved speech-text training to improve speech-to-speech translation (S2ST) by gradually adapting LLMs from text to speech.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of adapting text-trained LLMs to speech modality due to limited speech-to-speech data.

Method: Uses interleaved speech-text units during training, gradually reducing text ratio to ease modality adaptation. Fine-tunes LLaMA3.2-1B on CVSS dataset.

Result: Consistently improves translation performance, especially for languages with limited data.

Conclusion: Scheduled interleaved training effectively bridges the modality gap in S2ST.

Abstract: Speech-to-speech translation (S2ST) has been advanced with large language
models (LLMs), which are fine-tuned on discrete speech units. In such
approaches, modality adaptation from text to speech has been an issue. LLMs are
trained on text-only data, which presents challenges to adapt them to speech
modality with limited speech-to-speech data. To address the training
difficulty, we propose scheduled interleaved speech--text training in this
study. We use interleaved speech--text units instead of speech units during
training, where aligned text tokens are interleaved at the word level. We
gradually decrease the ratio of text as training progresses, to facilitate
progressive modality adaptation from text to speech. We conduct experimental
evaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show
that the proposed method consistently improves the translation performances,
especially for languages with limited training data.

</details>


### [12] [Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities](https://arxiv.org/pdf/2506.10155)
*Elizabeth Demers, Victor Xiaoqi Wang, Kean Wu*

Main category: cs.CL

TL;DR: The paper introduces a machine learning-based lexicon for measuring human capital (HC) disclosures, providing tools and data for future research.


<details>
  <summary>Details</summary>
Motivation: Human capital is vital for corporate value but lacks standardized measurement or disclosure rules, prompting the need for a systematic approach.

Method: A machine learning algorithm (word2vec) was trained on HC disclosures to create a lexicon of HC-related keywords, classified into five subcategories.

Result: The study produced a comprehensive HC lexicon, corporate HC disclosures, and Python code, enabling researchers to analyze HC in corporate communications.

Conclusion: The work facilitates future HC research and discusses opportunities for further exploration in HC management and disclosure.

Abstract: Human capital (HC) is increasingly important to corporate value creation.
Unlike other assets, however, HC is not currently subject to well-defined
measurement or disclosure rules. We use a machine learning algorithm (word2vec)
trained on a confirmed set of HC disclosures to develop a comprehensive list of
HC-related keywords classified into five subcategories (DEI; health and safety;
labor relations and culture; compensation and benefits; and demographics and
other) that capture the multidimensional nature of HC management. We share our
lexicon, corporate HC disclosures, and the Python code used to develop the
lexicon, and we provide detailed examples of using our data and code, including
for fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the
code to capture another construct of interest) with their samples of corporate
communications to address pertinent HC questions. We close with a discussion of
future research opportunities related to HC management and disclosure.

</details>


### [13] [Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective](https://arxiv.org/pdf/2506.10161)
*Yi Wang, Max Kreminski*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' story generation by using narrative planning benchmarks, revealing GPT-4's strengths in causal soundness but challenges in character intentionality and dramatic conflict.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' story generation quality is limited due to evaluation challenges. Computational narratology provides insights, applied here to assess LLMs via narrative planning.

Method: A benchmark evaluates LLMs on narrative planning using literature examples, focusing on causal soundness, character intentionality, and dramatic conflict.

Result: GPT-4 generates causally sound small-scale stories but struggles with intentionality and conflict, requiring reinforcement learning for complex reasoning.

Conclusion: The study clarifies LLMs' story generation scale and quality, highlighting challenges for game applications.

Abstract: Story generation has been a prominent application of Large Language Models
(LLMs). However, understanding LLMs' ability to produce high-quality stories
remains limited due to challenges in automatic evaluation methods and the high
cost and subjectivity of manual evaluation. Computational narratology offers
valuable insights into what constitutes a good story, which has been applied in
the symbolic narrative planning approach to story generation. This work aims to
deepen the understanding of LLMs' story generation capabilities by using them
to solve narrative planning problems. We present a benchmark for evaluating
LLMs on narrative planning based on literature examples, focusing on causal
soundness, character intentionality, and dramatic conflict. Our experiments
show that GPT-4 tier LLMs can generate causally sound stories at small scales,
but planning with character intentionality and dramatic conflict remains
challenging, requiring LLMs trained with reinforcement learning for complex
reasoning. The results offer insights on the scale of stories that LLMs can
generate while maintaining quality from different aspects. Our findings also
highlight interesting problem solving behaviors and shed lights on challenges
and considerations for applying LLM narrative planning in game environments.

</details>


### [14] [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/pdf/2506.10202)
*Shubhashis Roy Dipta, Francis Ferraro*

Main category: cs.CL

TL;DR: Q2E improves video retrieval by decomposing queries using LLMs/VLMs, outperforming baselines and integrating audio for better results.


<details>
  <summary>Details</summary>
Motivation: Enhance identification and retrieval of videos related to complex events by leveraging latent knowledge in LLMs/VLMs.

Method: Query-to-Event decomposition (Q2E) for zero-shot multilingual text-to-video retrieval, using entropy-based fusion scoring.

Result: Q2E outperforms state-of-the-art baselines; audio integration boosts retrieval performance.

Conclusion: Q2E effectively leverages multimodal knowledge for improved video retrieval, with released code/data for future work.

Abstract: Recent approaches have shown impressive proficiency in extracting and
leveraging parametric knowledge from Large-Language Models (LLMs) and
Vision-Language Models (VLMs). In this work, we consider how we can improve the
identification and retrieval of videos related to complex real-world events by
automatically extracting latent parametric knowledge about those events. We
present Q2E: a Query-to-Event decomposition method for zero-shot multilingual
text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our
approach demonstrates that we can enhance the understanding of otherwise overly
simplified human queries by decomposing the query using the knowledge embedded
in LLMs and VLMs. We additionally show how to apply our approach to both visual
and speech-based inputs. To combine this varied multimodal knowledge, we adopt
entropy-based fusion scoring for zero-shot fusion. Through evaluations on two
diverse datasets and multiple retrieval metrics, we demonstrate that Q2E
outperforms several state-of-the-art baselines. Our evaluation also shows that
integrating audio information can significantly improve text-to-video
retrieval. We have released code and data for future research.

</details>


### [15] [Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models](https://arxiv.org/pdf/2506.10855)
*Michele Gubian, Ioana Krehan, Oli Liu, James Kirby, Sharon Goldwater*

Main category: cs.CL

TL;DR: The paper analyzes how wav2vec2 models trained on four languages encode speech features like phones, tones, and speakers, showing orthogonal subspaces and similar layerwise patterns across languages.


<details>
  <summary>Details</summary>
Motivation: To understand how self-supervised speech models represent information across different languages, beyond just English.

Method: Probing classifiers and geometric analyses were used to examine representations of phones, lexical tones, and speaker information in wav2vec2 models trained on four languages.

Result: Subspaces for phones, tones, and speakers are largely orthogonal, with similar layerwise accuracy patterns and a small advantage for matched-language phone and tone probes in later layers.

Conclusion: The structure of wav2vec2's learned representations is largely independent of the pretraining language.

Abstract: Analyses of self-supervised speech models have begun to reveal where and how
they represent different types of information. However, almost all analyses
have focused on English. Here, we examine how wav2vec2 models trained on four
different languages encode both language-matched and non-matched speech. We use
probing classifiers and geometric analyses to examine how phones, lexical
tones, and speaker information are represented. We show that for all
pretraining and test languages, the subspaces encoding phones, tones, and
speakers are largely orthogonal, and that layerwise patterns of probing
accuracy are similar, with a relatively small advantage for matched-language
phone and tone (but not speaker) probes in the later layers. Our findings
suggest that the structure of representations learned by wav2vec2 is largely
independent of the speech material used during pretraining.

</details>


### [16] [TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games](https://arxiv.org/pdf/2506.10209)
*Prakamya Mishra, Jiang Liu, Jialian Wu, Xiaodong Yu, Zicheng Liu, Emad Barsoum*

Main category: cs.CL

TL;DR: TTT-Bench evaluates LRMs' strategic, spatial, and logical reasoning through simple Tic-Tac-Toe-style games, revealing their struggles despite excelling in hard math problems.


<details>
  <summary>Details</summary>
Motivation: To explore LRMs' reasoning abilities beyond STEM tasks, focusing on basic strategic, spatial, and logical reasoning.

Method: Introduces TTT-Bench, a benchmark with four two-player Tic-Tac-Toe-style games, using a programmatic approach to generate verifiable problems.

Result: LRMs perform poorly on TTT-Bench (↓41% & ↓5% lower than MATH 500 & AIME 2024), struggling with long-term strategic reasoning.

Conclusion: LRMs' reasoning abilities are limited in non-STEM domains, highlighting gaps in their strategic and spatial reasoning despite strong math performance.

Abstract: Large reasoning models (LRMs) have demonstrated impressive reasoning
capabilities across a broad range of tasks including Olympiad-level
mathematical problems, indicating evidence of their complex reasoning
abilities. While many reasoning benchmarks focus on the STEM domain, the
ability of LRMs to reason correctly in broader task domains remains
underexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmark
that is designed to evaluate basic strategic, spatial, and logical reasoning
abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games
that humans can effortlessly solve from a young age. We propose a simple yet
scalable programmatic approach for generating verifiable two-player game
problems for TTT-Bench. Although these games are trivial for humans, they
require reasoning about the intentions of the opponent, as well as the game
board's spatial configurations, to ensure a win. We evaluate a diverse set of
state-of-the-art LRMs, and \textbf{discover that the models that excel at hard
math problems frequently fail at these simple reasoning games}. Further testing
reveals that our evaluated reasoning models score on average $\downarrow$ 41\%
\& $\downarrow$ 5\% lower on TTT-Bench compared to MATH 500 \& AIME 2024
respectively, with larger models achieving higher performance using shorter
reasoning traces, where most of the models struggle on long-term strategic
reasoning situations on simple and new TTT-Bench tasks.

</details>


### [17] [Classifying Unreliable Narrators with Large Language Models](https://arxiv.org/pdf/2506.10231)
*Anneliese Brei, Katharine Henry, Abhisheik Sharma, Shashank Srivastava, Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: The paper proposes computational methods to identify unreliable narrators using a human-annotated dataset (TUNa) and evaluates LLMs for classification tasks. Results show the task is challenging but promising.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying unreliable narrators in texts using computational methods, bridging literary theory and real-world applications.

Method: Uses TUNa dataset, defines classification tasks for unreliability types, and evaluates LLMs in few-shot, fine-tuning, and curriculum learning settings.

Result: The task is challenging, but LLMs show potential for identifying unreliable narrators.

Conclusion: The study highlights the difficulty of the task and encourages future research, releasing the dataset and code for further exploration.

Abstract: Often when we interact with a first-person account of events, we consider
whether or not the narrator, the primary speaker of the text, is reliable. In
this paper, we propose using computational methods to identify unreliable
narrators, i.e. those who unintentionally misrepresent information. Borrowing
literary theory from narratology to define different types of unreliable
narrators based on a variety of textual phenomena, we present TUNa, a
human-annotated dataset of narratives from multiple domains, including blog
posts, subreddit posts, hotel reviews, and works of literature. We define
classification tasks for intra-narrational, inter-narrational, and
inter-textual unreliabilities and analyze the performance of popular
open-weight and proprietary LLMs for each. We propose learning from literature
to perform unreliable narrator classification on real-world text data. To this
end, we experiment with few-shot, fine-tuning, and curriculum learning
settings. Our results show that this task is very challenging, and there is
potential for using LLMs to identify unreliable narrators. We release our
expert-annotated dataset and code and invite future research in this area.

</details>


### [18] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/pdf/2506.08430)
*Ziqi. Liu, Ziyang. Zhou, Mingxuan. Hu*

Main category: cs.CL

TL;DR: CAF-I, a multi-agent LLM framework, improves sarcasm detection by addressing single-perspective limitations, lack of understanding, and interpretability, achieving SOTA performance with a 4.98 Macro-F1 boost.


<details>
  <summary>Details</summary>
Motivation: Existing LLM methods for sarcasm detection struggle with single-perspective analysis, insufficient understanding, and interpretability gaps.

Method: CAF-I uses specialized agents (Context, Semantics, Rhetoric) for multidimensional analysis and collaborative optimization, with a Decision Agent and Refinement Evaluator for feedback.

Result: CAF-I achieves a Macro-F1 of 76.31, a 4.98 improvement over prior baselines, demonstrating SOTA zero-shot performance.

Conclusion: CAF-I's human-like multi-perspective analysis enhances sarcasm detection accuracy and interpretability, setting a new benchmark.

Abstract: Large language model (LLM) have become mainstream methods in the field of
sarcasm detection. However, existing LLM methods face challenges in irony
detection, including: 1. single-perspective limitations, 2. insufficient
comprehensive understanding, and 3. lack of interpretability. This paper
introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven
multi-agent system designed to overcome these issues. CAF-I employs specialized
agents for Context, Semantics, and Rhetoric, which perform multidimensional
analysis and engage in interactive collaborative optimization. A Decision Agent
then consolidates these perspectives, with a Refinement Evaluator Agent
providing conditional feedback for optimization. Experiments on benchmark
datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving
SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of
76.31, a 4.98 absolute improvement over the strongest prior baseline. This
success is attained by its effective simulation of human-like multi-perspective
analysis, enhancing detection accuracy and interpretability.

</details>


### [19] [ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese](https://arxiv.org/pdf/2506.10245)
*Iago Alves Brito, Julia Soares Dollis, Fernanda Bufon Färber, Diogo Fernandes Costa Silva, Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: ToxSyn-PT is a large-scale Portuguese corpus for fine-grained hate-speech classification across nine minority groups, created via a novel four-stage synthetic pipeline. It shows strong generalization in experiments.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a large-scale, fine-grained hate-speech dataset for Portuguese, especially for legally protected minority groups.

Method: A four-stage pipeline: (1) manual seed curation, (2) few-shot expansion with an LLM, (3) paraphrase-based augmentation, and (4) enrichment with neutral texts.

Result: The corpus is class-balanced, diverse, and generalizes well across five public Portuguese hate-speech datasets.

Conclusion: ToxSyn-PT advances synthetic data research and hate-speech detection in low-resource settings, with public release for broader use.

Abstract: We present ToxSyn-PT, the first large-scale Portuguese corpus that enables
fine-grained hate-speech classification across nine legally protected minority
groups. The dataset contains 53,274 synthetic sentences equally distributed
between minorities groups and toxicity labels. ToxSyn-PT is created through a
novel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot
expansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and
(4) enrichment, plus additional neutral texts to curb overfitting to
group-specific cues. The resulting corpus is class-balanced, stylistically
diverse, and free from the social-media domain that dominate existing
Portuguese datasets. Despite domain differences with traditional benchmarks,
experiments on both binary and multi-label classification on the corpus yields
strong results across five public Portuguese hate-speech datasets,
demonstrating robust generalization even across domain boundaries. The dataset
is publicly released to advance research on synthetic data and hate-speech
detection in low-resource settings.

</details>


### [20] [Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models](https://arxiv.org/pdf/2506.10268)
*Andrea Yaoyun Cui, Pengfei Yu*

Main category: cs.CL

TL;DR: The paper challenges the assumption that language models make probabilistic decisions like humans, showing they can act deterministically. It proposes a method to distinguish stochastic from deterministic behavior in Gibbs sampling to avoid inferring false priors.


<details>
  <summary>Details</summary>
Motivation: To investigate whether language models have Bayesian brains and to address the potential for misleading priors inferred from Gibbs sampling.

Method: The study revisits the sampling assumption, tests language models under various conditions, and introduces a method to differentiate stochastic and deterministic decision patterns in Gibbs sampling.

Result: Language models can exhibit near-deterministic behavior, even with non-zero sampling temperatures, challenging prior assumptions. Simulated Gibbs sampling can converge to false priors without proper scrutiny.

Conclusion: The findings highlight the need to carefully analyze decision patterns in language models to avoid incorrect inferences about their priors, providing insights into their decision-making processes.

Abstract: Language models are essentially probability distributions over token
sequences. Auto-regressive models generate sentences by iteratively computing
and sampling from the distribution of the next token. This iterative sampling
introduces stochasticity, leading to the assumption that language models make
probabilistic decisions, similar to sampling from unknown distributions.
Building on this assumption, prior research has used simulated Gibbs sampling,
inspired by experiments designed to elicit human priors, to infer the priors of
language models. In this paper, we revisit a critical question: Do language
models possess Bayesian brains? Our findings show that under certain
conditions, language models can exhibit near-deterministic decision-making,
such as producing maximum likelihood estimations, even with a non-zero sampling
temperature. This challenges the sampling assumption and undermines previous
methods for eliciting human-like priors. Furthermore, we demonstrate that
without proper scrutiny, a system with deterministic behavior undergoing
simulated Gibbs sampling can converge to a "false prior." To address this, we
propose a straightforward approach to distinguish between stochastic and
deterministic decision patterns in Gibbs sampling, helping to prevent the
inference of misleading language model priors. We experiment on a variety of
large language models to identify their decision patterns under various
circumstances. Our results provide key insights in understanding decision
making of large language models.

</details>


### [21] [ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs](https://arxiv.org/pdf/2506.10288)
*Zige Wang, Qi Zhu, Fei Mi, Minghui Xu, Ruochun Jin, Wenjing Yang*

Main category: cs.CL

TL;DR: Proposes ClusterUCB, a gradient-based data selection framework using clustering and a modified UCB algorithm to reduce computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Gradient-based data selection is resource-intensive; the paper aims to make it feasible by optimizing computation.

Method: Clusters data by gradient features, frames selection as a bandit problem, and uses a modified UCB algorithm for efficient sampling.

Result: ClusterUCB achieves comparable results to original methods with significantly lower computing consumption.

Conclusion: The framework efficiently balances performance and resource use, making gradient-based data selection practical.

Abstract: Gradient-based data influence approximation has been leveraged to select
useful data samples in the supervised fine-tuning of large language models.
However, the computation of gradients throughout the fine-tuning process
requires too many resources to be feasible in practice. In this paper, we
propose an efficient gradient-based data selection framework with clustering
and a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition
that data samples with similar gradient features will have similar influences,
we first perform clustering on the training data pool. Then, we frame the
inter-cluster data selection as a constrained computing budget allocation
problem and consider it a multi-armed bandit problem. A modified UCB algorithm
is leveraged to solve this problem. Specifically, during the iterative sampling
process, historical data influence information is recorded to directly estimate
the distributions of each cluster, and a cold start is adopted to balance
exploration and exploitation. Experimental results on various benchmarks show
that our proposed framework, ClusterUCB, can achieve comparable results to the
original gradient-based data selection methods while greatly reducing computing
consumption.

</details>


### [22] [Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages](https://arxiv.org/pdf/2506.10292)
*Ali Almutairi, Abdullah Alsuhaibani, Shoaib Jameel, Usman Naseem, Gelareh Mohammadi, Imran Razzak*

Main category: cs.CL

TL;DR: Flick is a novel method for few-label text classification in low-resource languages, improving pseudo-label quality by refining high-confidence labels from broad clusters.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of noisy pseudo-labels and domain adaptation in low-resource linguistic contexts, where existing methods struggle.

Method: Flick introduces a pseudo-label refinement component, focusing on single-cluster cohesion and adaptive top-k selection to distill reliable pseudo-labels.

Result: Demonstrated superior performance across 14 diverse datasets, including low-resource languages like Arabic, Urdu, and Setswana.

Conclusion: Flick effectively mitigates error propagation in low-resource settings, enabling robust fine-tuning with minimal supervision.

Abstract: Training deep learning networks with minimal supervision has gained
significant research attention due to its potential to reduce reliance on
extensive labelled data. While self-training methods have proven effective in
semi-supervised learning, they remain vulnerable to errors from noisy pseudo
labels. Moreover, most recent approaches to the few-label classification
problem are either designed for resource-rich languages such as English or
involve complex cascading models that are prone to overfitting. To address the
persistent challenge of few-label text classification in truly low-resource
linguistic contexts, where existing methods often struggle with noisy
pseudo-labels and domain adaptation, we propose Flick. Unlike prior methods
that rely on generic multi-cluster pseudo-labelling or complex cascading
architectures, Flick leverages the fundamental insight that distilling
high-confidence pseudo-labels from a broader set of initial clusters can
dramatically improve pseudo-label quality, particularly for linguistically
diverse, low-resource settings. Flick introduces a novel pseudo-label
refinement component, a departure from traditional pseudo-labelling strategies
by identifying and leveraging top-performing pseudo-label clusters. This
component specifically learns to distil highly reliable pseudo-labels from an
initial broad set by focusing on single-cluster cohesion and leveraging an
adaptive top-k selection mechanism. This targeted refinement process is crucial
for mitigating the propagation of errors inherent in low-resource data,
allowing for robust fine-tuning of pre-trained language models with only a
handful of true labels. We demonstrate Flick's efficacy across 14 diverse
datasets, encompassing challenging low-resource languages such as Arabic, Urdu,
and Setswana, alongside English, showcasing its superior performance and
adaptability.

</details>


### [23] ["Check My Work?": Measuring Sycophancy in a Simulated Educational Context](https://arxiv.org/pdf/2506.10297)
*Chuck Arvin*

Main category: cs.CL

TL;DR: LLMs' response quality in education varies with query framing, showing sycophantic behavior (15% accuracy shift). Smaller models exhibit stronger bias (up to 30%).


<details>
  <summary>Details</summary>
Motivation: To study how user suggestions influence LLMs in education, focusing on sycophancy risks.

Method: Tested five LLMs (GPT-4o, GPT-4.1) under five conditions, analyzing response quality, answer flips, and token probabilities.

Result: LLMs adjust answers based on student input, degrading correctness by 15% for wrong suggestions and improving by 15% for correct ones. Smaller models show stronger bias (30% vs. 8%).

Conclusion: Sycophantic behavior in LLMs risks educational equity, requiring mitigation strategies.

Abstract: This study examines how user-provided suggestions affect Large Language
Models (LLMs) in a simulated educational context, where sycophancy poses
significant risks. Testing five different LLMs from the OpenAI GPT-4o and
GPT-4.1 model classes across five experimental conditions, we show that
response quality varies dramatically based on query framing. In cases where the
student mentions an incorrect answer, the LLM correctness can degrade by as
much as 15 percentage points, while mentioning the correct answer boosts
accuracy by the same margin. Our results also show that this bias is stronger
in smaller models, with an effect of up to 30% for the GPT-4.1-nano model,
versus 8% for the GPT-4o model. Our analysis of how often LLMs "flip" their
answer, and an investigation into token level probabilities, confirm that the
models are generally changing their answers to answer choices mentioned by
students in line with the sycophancy hypothesis. This sycophantic behavior has
important implications for educational equity, as LLMs may accelerate learning
for knowledgeable students while the same tools may reinforce misunderstanding
for less knowledgeable students. Our results highlight the need to better
understand the mechanism, and ways to mitigate, such bias in the educational
context.

</details>


### [24] [Code Execution as Grounded Supervision for LLM Reasoning](https://arxiv.org/pdf/2506.10343)
*Dongwon Jung, Wenxuan Zhou, Muhao Chen*

Main category: cs.CL

TL;DR: A scalable method generates high-quality CoT supervision for LLMs by leveraging program execution determinism, improving reasoning accuracy and reducing inference token length.


<details>
  <summary>Details</summary>
Motivation: Existing methods for CoT supervision rely on costly human annotations or error-prone LLM-generated reasoning, necessitating a more reliable and scalable approach.

Method: Extracts verifiable reasoning traces from code execution, converting them into natural language CoT reasoning.

Result: Enhances LLMs' transferable reasoning abilities across diverse tasks and reduces inference token length.

Conclusion: The method provides accurate, scalable CoT supervision, improving LLM reasoning efficiency and effectiveness.

Abstract: Training large language models (LLMs) with chain-of-thought (CoT) supervision
has proven effective for enhancing their reasoning abilities. However,
obtaining reliable and accurate reasoning supervision remains a significant
challenge. We propose a scalable method for generating a high-quality CoT
supervision dataset by leveraging the determinism of program execution. Unlike
existing reasoning dataset generation methods that rely on costly human
annotations or error-prone LLM-generated CoT, our approach extracts verifiable,
step-by-step reasoning traces from code execution and transforms them into a
natural language CoT reasoning. Experiments on reasoning benchmarks across
various domains show that our method effectively equips LLMs with transferable
reasoning abilities across diverse tasks. Furthermore, the ablation studies
validate that our method produces highly accurate reasoning data and reduces
overall token length during inference by reducing meaningless repetition and
overthinking.

</details>


### [25] [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/pdf/2504.17353)
*Chengguang Gan, Zhixi Cai, Yanbin Wei, Yunhao Liang, Shiwen Ni, Tatsunori Mori*

Main category: cs.CL

TL;DR: The paper extends Mutual Reinforcement Effect (MRE) to multimodal domains, introducing Multimodal MRE (M-MRE) and a dataset. A Prompt Format Adapter (PFA) is proposed, showing MRE's generalizability beyond text.


<details>
  <summary>Details</summary>
Motivation: To explore MRE's applicability in visual and multimodal domains, which was previously unexplored.

Method: Introduces M-MRE task, constructs a dataset, and proposes PFA for compatibility with Large Vision-Language Models (LVLMs).

Result: MRE is observed in M-MRE, showing mutual gains across tasks, confirming its generalizability.

Conclusion: MRE is effective in multimodal domains, enhancing performance across interrelated tasks.

Abstract: Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection
of information extraction and model interpretability. MRE aims to leverage the
mutual understanding between tasks of different granularities, enhancing the
performance of both coarse-grained and fine-grained tasks through joint
modeling. While MRE has been explored and validated in the textual domain, its
applicability to visual and multimodal domains remains unexplored. In this
work, we extend MRE to the multimodal information extraction domain for the
first time. Specifically, we introduce a new task: Multimodal Mutual
Reinforcement Effect (M-MRE), and construct a corresponding dataset to support
this task. To address the challenges posed by M-MRE, we further propose a
Prompt Format Adapter (PFA) that is fully compatible with various Large
Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can
also be observed in the M-MRE task, a multimodal text-image understanding
scenario. This provides strong evidence that MRE facilitates mutual gains
across three interrelated tasks, confirming its generalizability beyond the
textual domain.

</details>


### [26] [TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning](https://arxiv.org/pdf/2506.10380)
*Xiaohan Yu, Pu Jian, Chong Chen*

Main category: cs.CL

TL;DR: TableRAG improves RAG for heterogeneous documents by preserving tabular structure and enabling multi-hop reasoning, outperforming baselines on HeteQA.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods fail with heterogeneous documents (text + tables) due to flattened tables and poor reasoning.

Method: TableRAG combines text retrieval, SQL execution, and answer generation in four iterative steps.

Result: Outperforms baselines on public datasets and HeteQA, setting a new SOTA.

Conclusion: TableRAG effectively handles heterogeneous documents and advances QA capabilities.

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated considerable
effectiveness in open-domain question answering. However, when applied to
heterogeneous documents, comprising both textual and tabular components,
existing RAG approaches exhibit critical limitations. The prevailing practice
of flattening tables and chunking strategies disrupts the intrinsic tabular
structure, leads to information loss, and undermines the reasoning capabilities
of LLMs in multi-hop, global queries. To address these challenges, we propose
TableRAG, an hybrid framework that unifies textual understanding and complex
manipulations over tabular data. TableRAG iteratively operates in four steps:
context-sensitive query decomposition, text retrieval, SQL programming and
execution, and compositional intermediate answer generation. We also develop
HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous
reasoning capabilities. Experimental results demonstrate that TableRAG
consistently outperforms existing baselines on both public datasets and our
HeteQA, establishing a new state-of-the-art for heterogeneous document question
answering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.

</details>


### [27] [PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier](https://arxiv.org/pdf/2506.10406)
*Yuhua Jiang, Yuwen Xiong, Yufeng Yuan, Chao Xin, Wenyuan Xu, Yu Yue, Qianchuan Zhao, Lin Yan*

Main category: cs.CL

TL;DR: PAG is a framework enabling LLMs to self-correct by alternating roles in a unified RL paradigm, improving reasoning and verification.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with self-verification; existing solutions are unscalable. PAG aims to simplify and enhance self-correction.

Method: PAG uses a multi-turn RL paradigm with selective revision, verifying before revising to avoid unnecessary corrections.

Result: PAG improves reasoning and verification, outperforming self-consistency in benchmarks.

Conclusion: PAG effectively enhances LLM self-correction and verification, offering a scalable solution.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
complex reasoning tasks, yet they still struggle to reliably verify the
correctness of their own outputs. Existing solutions to this verification
challenge often depend on separate verifier models or require multi-stage
self-correction training pipelines, which limit scalability. In this paper, we
propose Policy as Generative Verifier (PAG), a simple and effective framework
that empowers LLMs to self-correct by alternating between policy and verifier
roles within a unified multi-turn reinforcement learning (RL) paradigm.
Distinct from prior approaches that always generate a second attempt regardless
of model confidence, PAG introduces a selective revision mechanism: the model
revises its answer only when its own generative verification step detects an
error. This verify-then-revise workflow not only alleviates model collapse but
also jointly enhances both reasoning and verification abilities. Extensive
experiments across diverse reasoning benchmarks highlight PAG's dual
advancements: as a policy, it enhances direct generation and self-correction
accuracy; as a verifier, its self-verification outperforms self-consistency.

</details>


### [28] [Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?](https://arxiv.org/pdf/2506.10415)
*Yingjin Song, Yupei Du, Denis Paperno, Albert Gatt*

Main category: cs.CL

TL;DR: TempVS benchmark evaluates MLLMs' temporal grounding and reasoning in image sequences, revealing performance gaps vs. humans.


<details>
  <summary>Details</summary>
Motivation: To assess and improve MLLMs' ability to understand temporal order in multimodal contexts.

Method: Introduces TempVS with three tests (event relation, sentence/image ordering) and grounding tests, evaluating 38 MLLMs.

Result: MLLMs struggle with TempVS, showing significant gaps compared to human performance.

Conclusion: TempVS highlights MLLMs' limitations and suggests future research directions; benchmark data/code is publicly available.

Abstract: This paper introduces the TempVS benchmark, which focuses on temporal
grounding and reasoning capabilities of Multimodal Large Language Models
(MLLMs) in image sequences. TempVS consists of three main tests (i.e., event
relation inference, sentence ordering and image ordering), each accompanied
with a basic grounding test. TempVS requires MLLMs to rely on both visual and
linguistic modalities to understand the temporal order of events. We evaluate
38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS,
with a substantial performance gap compared to human capabilities. We also
provide fine-grained insights that suggest promising directions for future
research. Our TempVS benchmark data and code are available at
https://github.com/yjsong22/TempVS.

</details>


### [29] [Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting](https://arxiv.org/pdf/2506.10421)
*Avneet Kaur, Arnav Arora*

Main category: cs.CL

TL;DR: The paper analyzes media framing in the Israel-Palestine conflict using computational methods, revealing a bias toward war journalism and regional differences in reporting.


<details>
  <summary>Details</summary>
Motivation: Current studies on conflict framing are limited by qualitative approaches or superficial analysis. This work aims to deepen insights using computational methods.

Method: Combines frame semantics and large language models to analyze communicative and linguistic framing in news articles about the Israel-Palestine conflict.

Result: Findings show a preference for war-based reporting and regional biases in framing assailants and victims across US, UK, and Middle Eastern outlets.

Conclusion: The study highlights media biases and the need for more nuanced analysis of conflict framing, advocating for peace journalism.

Abstract: Framing used by news media, especially in times of conflict, can have
substantial impact on readers' opinion, potentially aggravating the conflict
itself. Current studies on the topic of conflict framing have limited insights
due to their qualitative nature or only look at surface level generic frames
without going deeper. In this work, we identify indicators of war and peace
journalism, as outlined by prior work in conflict studies, in a corpus of news
articles reporting on the Israel-Palestine war. For our analysis, we use
computational approaches, using a combination of frame semantics and large
language models to identify both communicative framing and its connection to
linguistic framing. Our analysis reveals a higher focus on war based reporting
rather than peace based. We also show substantial differences in reporting
across the US, UK, and Middle Eastern news outlets in framing who the assailant
and victims of the conflict are, surfacing biases within the media.

</details>


### [30] [Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty](https://arxiv.org/pdf/2506.10446)
*Zehui Ling, Deshu Chen, Hongwei Zhang, Yifeng Jiao, Xin Guo, Yuan Cheng*

Main category: cs.CL

TL;DR: The paper proposes a method to improve LLM reasoning efficiency by balancing conciseness for simpler problems and detailed reasoning for complex ones, using a tailored reward function with length penalties.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning methods often produce unnecessarily long outputs, increasing latency, and uniform penalties for length don't account for problem complexity, leading to inefficiencies.

Method: The approach divides the reward function and introduces a novel penalty for output length, optimizing reasoning efficiency based on problem complexity.

Result: The method achieved shorter outputs with maintained or improved accuracy on simpler datasets (GSM8K, MATH500) and better accuracy on the complex AIME2024 dataset.

Conclusion: The study successfully enhances LLM reasoning efficiency by dynamically adjusting reasoning length, improving performance across varying problem complexities.

Abstract: Large language models (LLMs) have demonstrated significant advancements in
reasoning capabilities, performing well on various challenging benchmarks.
Techniques like Chain-of-Thought prompting have been introduced to further
improve reasoning. However, these approaches frequently generate longer
outputs, which in turn increase computational latency. Although some methods
use reinforcement learning to shorten reasoning, they often apply uniform
penalties without considering the problem's complexity, leading to suboptimal
outcomes. In this study, we seek to enhance the efficiency of LLM reasoning by
promoting conciseness for simpler problems while preserving sufficient
reasoning for more complex ones for accuracy, thus improving the model's
overall performance. Specifically, we manage the model's reasoning efficiency
by dividing the reward function and including a novel penalty for output
length. Our approach has yielded impressive outcomes in benchmark evaluations
across three datasets: GSM8K, MATH500, and AIME2024. For the comparatively
simpler datasets GSM8K and MATH500, our method has effectively shortened output
lengths while preserving or enhancing accuracy. On the more demanding AIME2024
dataset, our approach has resulted in improved accuracy.

</details>


### [31] [Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers](https://arxiv.org/pdf/2506.10486)
*Xanh Ho, Sunisth Kumar, Yun-Ang Wu, Florian Boudin, Atsuhiro Takasu, Akiko Aizawa*

Main category: cs.CL

TL;DR: The paper reframes table-text alignment as an explanation task, requiring models to identify key table cells for claim verification, and introduces a dataset with human-annotated rationales.


<details>
  <summary>Details</summary>
Motivation: Predicting claim labels alone lacks interpretability; understanding model reasoning requires identifying essential table cells.

Method: Extend SciTab benchmark with human-annotated cell-level rationales, propose a taxonomy for ambiguous cases, and evaluate models.

Result: Incorporating table alignment improves claim verification, but LLMs often fail to align with human rationales despite correct labels.

Conclusion: Models need better alignment with human reasoning for faithful claim verification.

Abstract: Scientific claim verification against tables typically requires predicting
whether a claim is supported or refuted given a table. However, we argue that
predicting the final label alone is insufficient: it reveals little about the
model's reasoning and offers limited interpretability. To address this, we
reframe table-text alignment as an explanation task, requiring models to
identify the table cells essential for claim verification. We build a new
dataset by extending the SciTab benchmark with human-annotated cell-level
rationales. Annotators verify the claim label and highlight the minimal set of
cells needed to support their decision. After the annotation process, we
utilize the collected information and propose a taxonomy for handling ambiguous
cases. Our experiments show that (i) incorporating table alignment information
improves claim verification performance, and (ii) most LLMs, while often
predicting correct labels, fail to recover human-aligned rationales, suggesting
that their predictions do not stem from faithful reasoning.

</details>


### [32] [Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models](https://arxiv.org/pdf/2506.10491)
*Aleksandra Sorokovikova, Pavel Chizhov, Iuliia Eremenko, Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: The paper investigates bias in large language models (LLMs) using proxy measures, finding negligible bias in pre-prompted personae but significant bias in tasks like grading user answers and salary negotiation advice.


<details>
  <summary>Details</summary>
Motivation: To understand and measure biases in LLMs due to controversial and stereotypical training data, especially as models increasingly personalize responses based on user demographics.

Method: Evaluated LLMs using pre-prompted personae on a multi-subject benchmark (MMLU), reformulated tasks like grading user answers, and analyzed salary negotiation advice.

Result: Pre-prompted personae showed negligible bias, but grading and salary advice tasks revealed significant bias. Personalization trends exacerbate these issues.

Conclusion: LLMs exhibit measurable bias in certain tasks, highlighting the need for addressing bias as models become more personalized.

Abstract: Modern language models are trained on large amounts of data. These data
inevitably include controversial and stereotypical content, which contains all
sorts of biases related to gender, origin, age, etc. As a result, the models
express biased points of view or produce different results based on the
assigned personality or the personality of the user. In this paper, we
investigate various proxy measures of bias in large language models (LLMs). We
find that evaluating models with pre-prompted personae on a multi-subject
benchmark (MMLU) leads to negligible and mostly random differences in scores.
However, if we reformulate the task and ask a model to grade the user's answer,
this shows more significant signs of bias. Finally, if we ask the model for
salary negotiation advice, we see pronounced bias in the answers. With the
recent trend for LLM assistant memory and personalization, these problems open
up from a different angle: modern LLM users do not need to pre-prompt the
description of their persona since the model already knows their
socio-demographics.

</details>


### [33] [Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models](https://arxiv.org/pdf/2506.10504)
*Sangmin Song, Juhwan Choi, JungMin Yun, YoungBin Kim*

Main category: cs.CL

TL;DR: LLMs show strong zero-shot DST performance but struggle with multi-user interactions. A method using speech act theory extends datasets for evaluation, revealing significant performance drops in multi-user settings.


<details>
  <summary>Details</summary>
Motivation: To assess LLM robustness in multi-user DST and address the gap in real-world multi-user interaction complexity.

Method: Extend an existing DST dataset by generating a second user's utterances using speech act theory, enabling controlled multi-user evaluation.

Result: LLMs exhibit a significant performance drop in multi-user DST compared to single-user scenarios.

Conclusion: Future research must enhance LLMs for multi-user DST to improve realism and robustness.

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
zero-shot dialogue state tracking (DST), reducing the need for task-specific
training. However, conventional DST benchmarks primarily focus on structured
user-agent conversations, failing to capture the complexities of real-world
multi-user interactions. In this study, we assess the robustness of LLMs in
multi-user DST while minimizing dataset construction costs. Inspired by recent
advances in LLM-based data annotation, we extend an existing DST dataset by
generating utterances of a second user based on speech act theory. Our
methodology systematically incorporates a second user's utterances into
conversations, enabling a controlled evaluation of LLMs in multi-user settings.
Experimental results reveal a significant performance drop compared to
single-user DST, highlighting the limitations of current LLMs in extracting and
tracking dialogue states amidst multiple speakers. Our findings emphasize the
need for future research to enhance LLMs for multi-user DST scenarios, paving
the way for more realistic and robust DST models.

</details>


### [34] [Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs](https://arxiv.org/pdf/2506.10508)
*Yilin Xiao, Chuang Zhou, Qinggang Zhang, Bo Li, Qing Li, Xiao Huang*

Main category: cs.CL

TL;DR: The paper proposes the RRP framework to enhance LLMs by integrating knowledge graphs, focusing on refining reasoning paths for better performance in complex tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with knowledge-intensive tasks due to lack of background knowledge and hallucination. Existing KG-enhanced LLMs fail to handle complex questions effectively.

Method: The RRP framework mines KGs by combining LLM semantics with structural information from relation embedding and bidirectional distribution learning, and includes a rethinking module to refine reasoning paths.

Result: RRP achieves state-of-the-art performance on public datasets and can be integrated into various LLMs in a plug-and-play manner.

Conclusion: RRP effectively enhances LLM reasoning by generating high-quality, tailored reasoning paths.

Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks
due to a lack of background knowledge and a tendency to hallucinate. To address
these limitations, integrating knowledge graphs (KGs) with LLMs has been
intensively studied. Existing KG-enhanced LLMs focus on supplementary factual
knowledge, but still struggle with solving complex questions. We argue that
refining the relationships among facts and organizing them into a logically
consistent reasoning path is equally important as factual knowledge itself.
Despite their potential, extracting reliable reasoning paths from KGs poses the
following challenges: the complexity of graph structures and the existence of
multiple generated paths, making it difficult to distinguish between useful and
redundant ones. To tackle these challenges, we propose the RRP framework to
mine the knowledge graph, which combines the semantic strengths of LLMs with
structural information obtained through relation embedding and bidirectional
distribution learning. Additionally, we introduce a rethinking module that
evaluates and refines reasoning paths according to their significance.
Experimental results on two public datasets show that RRP achieves
state-of-the-art performance compared to existing baseline methods. Moreover,
RRP can be easily integrated into various LLMs to enhance their reasoning
abilities in a plug-and-play manner. By generating high-quality reasoning paths
tailored to specific questions, RRP distills effective guidance for LLM
reasoning.

</details>


### [35] [Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search](https://arxiv.org/pdf/2506.10614)
*Promise Dodzi Kpoglu*

Main category: cs.CL

TL;DR: An unsupervised hybrid method combining data-driven inference and rule-based heuristics for reconstructing protoforms, outperforming baselines in accuracy and plausibility.


<details>
  <summary>Details</summary>
Motivation: Prior probabilistic models are limited by their data-driven nature; this work integrates linguistic constraints for better protoform reconstruction.

Method: Combines data-driven inference with rule-based heuristics in an evolutionary optimization framework.

Result: Substantial improvements over baselines in reconstructing Latin protoforms, measured by character-level accuracy and phonological plausibility.

Conclusion: The hybrid approach effectively leverages statistical and linguistic constraints for superior protoform reconstruction.

Abstract: We propose an unsupervised method for the reconstruction of protoforms i.e.,
ancestral word forms from which modern language forms are derived. While prior
work has primarily relied on probabilistic models of phonological edits to
infer protoforms from cognate sets, such approaches are limited by their
predominantly data-driven nature. In contrast, our model integrates data-driven
inference with rule-based heuristics within an evolutionary optimization
framework. This hybrid approach leverages on both statistical patterns and
linguistically motivated constraints to guide the reconstruction process. We
evaluate our method on the task of reconstructing Latin protoforms using a
dataset of cognates from five Romance languages. Experimental results
demonstrate substantial improvements over established baselines across both
character-level accuracy and phonological plausibility metrics.

</details>


### [36] [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/pdf/2504.15777)
*Shangshang Wang, Julian Asilis, Ömer Faruk Akgül, Enes Burak Bilgin, Ollie Liu, Willie Neiswanger*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: How cost-effectively can strong reasoning abilities be achieved in language
models? Driven by this fundamental question, we present Tina, a family of tiny
reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates
that substantial reasoning performance can be developed using only minimal
resources, by applying parameter-efficient updates during reinforcement
learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B
parameter base model. This minimalist approach produces models that achieve
reasoning performance which is competitive with, and sometimes surpasses, SOTA
RL reasoning models built upon the same base model. Crucially, this is achieved
at a tiny fraction of the computational post-training cost employed by existing
SOTA models. In fact, the best Tina model achieves a >20\% reasoning
performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD
post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our
work reveals the surprising effectiveness of efficient RL reasoning via LoRA.
We validate this across multiple open-source reasoning datasets and various
ablation settings starting with a single, fixed set of hyperparameters.
Furthermore, we hypothesize that this effectiveness and efficiency stem from
LoRA rapidly adapting the model to the structural format of reasoning rewarded
by RL, while largely preserving the base model's underlying knowledge. In
service of accessibility and open research, we fully open-source all code,
training logs, and model weights \& checkpoints.

</details>


### [37] [SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis](https://arxiv.org/pdf/2506.10622)
*Sergio Burdisso, Esaú Villatoro-Tello, Petr Motlicek*

Main category: cs.CL

TL;DR: SDialog is a Python toolkit for generating and analyzing synthetic dialogues using LLMs, enhancing reproducibility and flexibility in conversational AI research.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-quality, reproducible synthetic dialogues for training, evaluation, and benchmarking in conversational AI.

Method: Leverages instruction-tuned LLMs with abstractions for personas, orchestration, and scenario management to create realistic and diverse dialogues.

Result: Supports workflows like multi-agent simulation and scenario-driven generation, standardizing synthetic data tools.

Conclusion: SDialog advances reproducibility and standardization in synthetic dialogue generation for AI research.

Abstract: The advancement of conversational AI systems relies on the availability of
high-quality, flexible, and reproducible synthetic dialogues for training,
evaluation, and benchmarking. SDialog is a modular, extensible Python toolkit
designed to address the challenges of synthetic dialogue generation and
analysis. By leveraging instruction-tuned Large Language Models (LLMs), SDialog
provides abstractions for personas, orchestration, and scenario management,
enabling the creation of realistic, diverse, and controllable conversational
data for research and development. SDialog supports workflows such as
multi-agent simulation and scenario-driven generation, and represents a step
forward in the standardization of tools and frameworks for synthetic data
generation, a crucial advancement for ensuring reproducibility in today's
fast-evolving research landscape.

</details>


### [38] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/pdf/2506.09967)
*Shangshang Wang, Julian Asilis, Ömer Faruk Akgül, Enes Burak Bilgin, Ollie Liu, Deqing Fu, Willie Neiswanger*

Main category: cs.CL

TL;DR: Resa, a 1.5B reasoning model family, uses SAE-Tuning to efficiently elicit strong reasoning in language models, reducing costs and training time significantly while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To explore cost-effective methods for enhancing reasoning in language models by leveraging their underlying representations.

Method: SAE-Tuning: trains a sparse autoencoder (SAE) to capture reasoning abilities from a source model, then uses it to guide supervised fine-tuning of a target model.

Result: Achieves >97% of RL-trained performance at >2000x lower cost (~$1) and >450x faster (~20 minutes). Also improves performance on benchmarks like AIME24 and AMC23.

Conclusion: SAE-Tuning is efficient, generalizable, and modular, enabling cost-effective reasoning enhancement without retraining.

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [39] [NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors](https://arxiv.org/pdf/2506.10627)
*Numaan Naeem, Sarfraz Ahmad, Momina Ahsan, Hasan Iqbal*

Main category: cs.CL

TL;DR: The paper presents a system for identifying mistakes in student math reasoning using four approaches, with the best performance from a retrieval-augmented LLM prompting system.


<details>
  <summary>Details</summary>
Motivation: To evaluate AI tutors' ability to correctly identify mistakes in student reasoning, improving pedagogical feedback.

Method: Four approaches: ensemble of ML models, frozen sentence-transformer, history-aware model, and retrieval-augmented LLM prompting.

Result: The retrieval-augmented LLM system outperformed baselines, showing effectiveness in pedagogical feedback.

Conclusion: Combining example-driven prompting with LLM reasoning is effective for mistake identification in tutoring.

Abstract: This paper presents our system for Track 1: Mistake Identification in the BEA
2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The
task involves evaluating whether a tutor's response correctly identifies a
mistake in a student's mathematical reasoning. We explore four approaches: (1)
an ensemble of machine learning models over pooled token embeddings from
multiple pretrained language models (LMs); (2) a frozen sentence-transformer
using [CLS] embeddings with an MLP classifier; (3) a history-aware model with
multi-head attention between token-level history and response embeddings; and
(4) a retrieval-augmented few-shot prompting system with a large language model
(LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples,
constructs structured prompts, and uses schema-guided output parsing to produce
interpretable predictions. It outperforms all baselines, demonstrating the
effectiveness of combining example-driven prompting with LLM reasoning for
pedagogical feedback assessment. Our code is available at
https://github.com/NaumanNaeem/BEA_2025.

</details>


### [40] [Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters](https://arxiv.org/pdf/2506.10641)
*Tatsuya Hiraoka, Kentaro Inui*

Main category: cs.CL

TL;DR: LLMs struggle with complex character-level tasks despite high spelling accuracy, relying on higher Transformer layers to reconstruct character-level knowledge.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs internally represent and utilize character-level information during spelling.

Method: Analyzed LLMs' spelling behavior using probing classifiers, knowledge neurons, and attention weights.

Result: Embedding layers lack full character-level info; higher layers reconstruct it, showing a 'breakthrough' in spelling.

Conclusion: LLMs handle character-level tasks non-intuitively, relying on deeper layers for reconstruction.

Abstract: Large language models (LLMs) can spell out tokens character by character with
high accuracy, yet they struggle with more complex character-level tasks, such
as identifying compositional subcomponents within tokens. In this work, we
investigate how LLMs internally represent and utilize character-level
information during the spelling-out process. Our analysis reveals that,
although spelling out is a simple task for humans, it is not handled in a
straightforward manner by LLMs. Specifically, we show that the embedding layer
does not fully encode character-level information, particularly beyond the
first character. As a result, LLMs rely on intermediate and higher Transformer
layers to reconstruct character-level knowledge, where we observe a distinct
"breakthrough" in their spelling behavior. We validate this mechanism through
three complementary analyses: probing classifiers, identification of knowledge
neurons, and inspection of attention weights.

</details>


### [41] [Large Language Models for Detection of Life-Threatening Texts](https://arxiv.org/pdf/2506.10687)
*Thanh Thi Nguyen, Campbell Wilson, Janis Dalins*

Main category: cs.CL

TL;DR: The paper presents a method using large language models (LLMs) to detect life-threatening language, outperforming traditional techniques like bag of words and BERT. Fine-tuned LLMs (Mistral, Llama-2, Gemma) show strong performance, especially in imbalanced data scenarios.


<details>
  <summary>Details</summary>
Motivation: To improve detection of life-threatening language for mental health and safety, addressing limitations of traditional methods.

Method: Fine-tuning three open-source LLMs (Gemma, Mistral, Llama-2) on balanced, imbalanced, and extremely imbalanced datasets, comparing them with traditional methods.

Result: LLMs, especially Mistral and Llama-2, outperform traditional methods. Upsampling helps traditional methods but has less impact on LLMs.

Conclusion: LLMs show great potential for real-world life-threatening language detection, with Mistral and Llama-2 leading in performance.

Abstract: Detecting life-threatening language is essential for safeguarding individuals
in distress, promoting mental health and well-being, and preventing potential
harm and loss of life. This paper presents an effective approach to identifying
life-threatening texts using large language models (LLMs) and compares them
with traditional methods such as bag of words, word embedding, topic modeling,
and Bidirectional Encoder Representations from Transformers. We fine-tune three
open-source LLMs including Gemma, Mistral, and Llama-2 using their 7B parameter
variants on different datasets, which are constructed with class balance,
imbalance, and extreme imbalance scenarios. Experimental results demonstrate a
strong performance of LLMs against traditional methods. More specifically,
Mistral and Llama-2 models are top performers in both balanced and imbalanced
data scenarios while Gemma is slightly behind. We employ the upsampling
technique to deal with the imbalanced data scenarios and demonstrate that while
this method benefits traditional approaches, it does not have as much impact on
LLMs. This study demonstrates a great potential of LLMs for real-world
life-threatening language detection problems.

</details>


### [42] [Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet](https://arxiv.org/pdf/2506.10715)
*Lorenzo Augello, John P. McCrae*

Main category: cs.CL

TL;DR: The paper addresses missing hypernymy links for adjectives in Open English Wordnet, proposing a method to predict these links using fine-tuned language models.


<details>
  <summary>Details</summary>
Motivation: Many hypernymy links for adjectives are missing in Open English Wordnet, a key linguistic resource.

Method: Theoretical discussion of adjective hypernymy, development of a new resource, and fine-tuning large language models (TaxoLLaMa) for prediction.

Result: Demonstrated adaptability of TaxoLLaMa for predicting adjective hypernymy.

Conclusion: The approach successfully addresses the gap in adjective hypernymy links, enhancing the resource.

Abstract: Open English Wordnet is a key resource published in OntoLex-lemon as part of
the linguistic linked open data cloud. There are, however, many links missing
in the resource, and in this paper, we look at how we can establish hypernymy
between adjectives. We present a theoretical discussion of the hypernymy
relation and how it differs for adjectives in contrast to nouns and verbs. We
develop a new resource for adjective hypernymy and fine-tune large language
models to predict adjective hypernymy, showing that the methodology of
TaxoLLaMa can be adapted to this task.

</details>


### [43] [PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models](https://arxiv.org/pdf/2506.10716)
*Ye Yu, Yaoning Yu, Haohan Wang*

Main category: cs.CL

TL;DR: PREMISE is a prompt-only framework that reduces verbose reasoning in large models, cutting token usage and cost while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Verbose reasoning traces in large models inflate token usage and cost, limiting deployment in constrained settings.

Method: PREMISE combines trace-level diagnostics and gradient-inspired prompt optimization to minimize redundancy.

Result: Reduces reasoning tokens by up to 87.5% and costs by 69-82%, while matching or exceeding baseline accuracy.

Conclusion: Prompt-level optimization offers a scalable way to enhance efficiency in large reasoning models without sacrificing quality.

Abstract: Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve
strong performance on mathematical benchmarks using lengthy chain-of-thought
(CoT) reasoning, but the resulting traces are often unnecessarily verbose. This
inflates token usage and cost, limiting deployment in latency-sensitive or
API-constrained settings. We introduce PREMISE (PRompt-based Efficient
Mathematical Inference with Strategic Evaluation), a prompt-only framework that
reduces reasoning overhead without modifying model weights. PREMISE combines
trace-level diagnostics with gradient-inspired prompt optimization to minimize
redundant computation while preserving answer accuracy. The approach jointly
optimizes brevity and correctness through a multi-objective textual search that
balances token length and answer validity. Unlike prior work, PREMISE runs in a
single-pass black-box interface, so it can be applied directly to commercial
LLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy
($96\%\rightarrow96\%$ with Claude, $91\%\rightarrow92\%$ with Gemini) while
reducing reasoning tokens by up to $87.5\%$ and cutting dollar cost by
$69$--$82\%$. These results show that prompt-level optimization is a practical
and scalable path to efficient LRM inference without compromising reasoning
quality.

</details>


### [44] [Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims](https://arxiv.org/pdf/2506.10728)
*Priyanka Kargupta, Runchu Tian, Jiawei Han*

Main category: cs.CL

TL;DR: ClaimSpect is a framework for breaking down nuanced claims into hierarchical aspects, enriching them with corpus-specific perspectives, and validating them using retrieval-augmented generation.


<details>
  <summary>Details</summary>
Motivation: Nuanced claims (e.g., scientific or political) are hard to label as simply true or false. Breaking them into smaller, verifiable aspects allows for a more structured and comprehensive analysis.

Method: Proposes ClaimSpect, a retrieval-augmented generation framework that constructs a hierarchy of claim aspects, retrieves relevant corpus segments, and identifies perspectives (support, neutral, oppose) and their prevalence.

Result: Applied to real-world scientific and political claims, ClaimSpect demonstrates robustness and accuracy in deconstructing claims and representing corpus perspectives, outperforming baselines.

Conclusion: ClaimSpect effectively addresses the complexity of nuanced claims by providing a structured, hierarchical analysis and validating perspectives within a corpus.

Abstract: Claims made by individuals or entities are oftentimes nuanced and cannot be
clearly labeled as entirely "true" or "false" -- as is frequently the case with
scientific and political claims. However, a claim (e.g., "vaccine A is better
than vaccine B") can be dissected into its integral aspects and sub-aspects
(e.g., efficacy, safety, distribution), which are individually easier to
validate. This enables a more comprehensive, structured response that provides
a well-rounded perspective on a given problem while also allowing the reader to
prioritize specific angles of interest within the claim (e.g., safety towards
children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based
framework for automatically constructing a hierarchy of aspects typically
considered when addressing a claim and enriching them with corpus-specific
perspectives. This structure hierarchically partitions an input corpus to
retrieve relevant segments, which assist in discovering new sub-aspects.
Moreover, these segments enable the discovery of varying perspectives towards
an aspect of the claim (e.g., support, neutral, or oppose) and their respective
prevalence (e.g., "how many biomedical papers believe vaccine A is more
transportable than B?"). We apply ClaimSpect to a wide variety of real-world
scientific and political claims featured in our constructed dataset, showcasing
its robustness and accuracy in deconstructing a nuanced claim and representing
perspectives within a corpus. Through real-world case studies and human
evaluation, we validate its effectiveness over multiple baselines.

</details>


### [45] [TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora](https://arxiv.org/pdf/2506.10737)
*Priyanka Kargupta, Nan Zhang, Yunyi Zhang, Rui Zhang, Prasenjit Mitra, Jiawei Han*

Main category: cs.CL

TL;DR: TaxoAdapt is a framework for dynamically adapting LLM-generated taxonomies to scientific corpora, addressing gaps in generalizability and multidimensionality, outperforming baselines in granularity and coherence.


<details>
  <summary>Details</summary>
Motivation: Challenges in organizing and retrieving scientific literature due to evolving fields, limitations of expert-curated taxonomies, and shortcomings of automatic methods.

Method: TaxoAdapt uses iterative hierarchical classification to adapt taxonomies dynamically across multiple dimensions, expanding width and depth based on corpus topical distribution.

Result: TaxoAdapt outperforms baselines, generating taxonomies 26.51% more granularity-preserving and 50.41% more coherent.

Conclusion: TaxoAdapt effectively structures and captures the evolution of scientific fields, demonstrating state-of-the-art performance.

Abstract: The rapid evolution of scientific fields introduces challenges in organizing
and retrieving scientific literature. While expert-curated taxonomies have
traditionally addressed this need, the process is time-consuming and expensive.
Furthermore, recent automatic taxonomy construction methods either (1)
over-rely on a specific corpus, sacrificing generalizability, or (2) depend
heavily on the general knowledge of large language models (LLMs) contained
within their pre-training datasets, often overlooking the dynamic nature of
evolving scientific domains. Additionally, these approaches fail to account for
the multi-faceted nature of scientific literature, where a single research
paper may contribute to multiple dimensions (e.g., methodology, new tasks,
evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a
framework that dynamically adapts an LLM-generated taxonomy to a given corpus
across multiple dimensions. TaxoAdapt performs iterative hierarchical
classification, expanding both the taxonomy width and depth based on corpus'
topical distribution. We demonstrate its state-of-the-art performance across a
diverse set of computer science conferences over the years to showcase its
ability to structure and capture the evolution of scientific fields. As a
multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more
granularity-preserving and 50.41% more coherent than the most competitive
baselines judged by LLMs.

</details>


### [46] [One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers](https://arxiv.org/pdf/2506.10766)
*Diana Abagyan, Alejandro R. Salamanca, Andres Felipe Cruz-Salinas, Kris Cao, Hangyu Lin, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, Sara Hooker*

Main category: cs.CL

TL;DR: Using a universal tokenizer during pretraining improves language adaptation in multilingual LLMs, achieving up to 20.2% higher win rates for new languages and 5% for unseen ones, with minimal impact on pretraining languages.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in multilingual LLM pretraining, such as limited model capacity, data scarcity, and tokenizer gaps, by improving language plasticity.

Method: Proposing a universal tokenizer trained for more languages than the primary pretraining set to enhance post-training adaptation.

Result: Universal tokenizer boosts adaptation by up to 20.2% for new languages and 5% for unseen ones, with negligible performance loss on pretraining languages.

Conclusion: A universal tokenizer significantly improves language plasticity in multilingual LLMs, enabling efficient expansion of language coverage post-training.

Abstract: Pretraining massively multilingual Large Language Models (LLMs) for many
languages at once is challenging due to limited model capacity, scarce
high-quality data, and compute constraints. Moreover, the lack of language
coverage of the tokenizer makes it harder to address the gap for new languages
purely at the post-training stage. In this work, we study what relatively cheap
interventions early on in training improve "language plasticity", or adaptation
capabilities of the model post-training to new languages. We focus on tokenizer
design and propose using a universal tokenizer that is trained for more
languages than the primary pretraining languages to enable efficient adaptation
in expanding language coverage after pretraining. Our systematic experiments
across diverse groups of languages and different training strategies show that
a universal tokenizer enables significantly higher language adaptation, with up
to 20.2% increase in win rates compared to tokenizers specific to pretraining
languages. Furthermore, a universal tokenizer also leads to better plasticity
towards languages that are completely unseen in the tokenizer and pretraining,
by up to 5% win rate gain. We achieve this adaptation to an expanded set of
languages with minimal compromise in performance on the majority of languages
included in pretraining.

</details>


### [47] [Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs](https://arxiv.org/pdf/2506.10769)
*Alberto Testoni, Iacer Calixto*

Main category: cs.CL

TL;DR: The paper evaluates uncertainty estimation methods for LLMs in clinical multiple-choice QA, comparing single-generation and sampling-based approaches. Lightweight methods using behavioral signals perform nearly as well as Semantic Entropy with one generation. Results highlight variability across specialties and question types.


<details>
  <summary>Details</summary>
Motivation: Accurate uncertainty estimates are critical for deploying LLMs in high-stakes clinical decision-making.

Method: Fine-grained evaluation of ten open-source LLMs across two datasets, eleven specialties, and six question types, comparing single-generation and sampling-based methods. Includes a case study on lightweight estimators using behavioral signals.

Result: Lightweight methods approach Semantic Entropy performance with one generation. Significant variation observed across specialties and question types.

Conclusion: Model selection should consider question nature and model-specific strengths, emphasizing the need for tailored uncertainty estimation in clinical applications.

Abstract: Accurate and well-calibrated uncertainty estimates are essential for
deploying large language models (LLMs) in high-stakes domains such as clinical
decision support. We present a fine-grained evaluation of uncertainty
estimation methods for clinical multiple-choice question answering, covering
ten open-source LLMs (general-purpose, biomedical, and reasoning models) across
two datasets, eleven medical specialties, and six question types. We compare
standard single-generation and sampling-based methods, and present a case study
exploring simple, single-pass estimators based on behavioral signals in
reasoning traces. These lightweight methods approach the performance of
Semantic Entropy while requiring only one generation. Our results reveal
substantial variation across specialties and question types, underscoring the
importance of selecting models based on both the nature of the question and
model-specific strengths.

</details>


### [48] [Improving Named Entity Transcription with Contextual LLM-based Revision](https://arxiv.org/pdf/2506.10779)
*Viet Anh Trinh, Xinlu He, Jacob Whitehill*

Main category: cs.CL

TL;DR: A method using LLM revision improves ASR accuracy for named entities, reducing WER by 30% on a new dataset.


<details>
  <summary>Details</summary>
Motivation: Named entities are critical but often misrecognized by ASR systems, impacting downstream applications.

Method: Proposes an LLM revision mechanism leveraging reasoning and local context (e.g., lecture notes) to correct named entities in ASR predictions.

Result: Achieves up to 30% relative WER reduction for named entities on the NER-MIT-OpenCourseWare dataset.

Conclusion: The LLM revision method effectively enhances ASR performance for named entities, validated by significant WER reduction.

Abstract: With recent advances in modeling and the increasing amount of supervised
training data, automatic speech recognition (ASR) systems have achieved
remarkable performance on general speech. However, the word error rate (WER) of
state-of-the-art ASR remains high for named entities. Since named entities are
often the most critical keywords, misrecognizing them can affect all downstream
applications, especially when the ASR system functions as the front end of a
complex system. In this paper, we introduce a large language model (LLM)
revision mechanism to revise incorrect named entities in ASR predictions by
leveraging the LLM's reasoning ability as well as local context (e.g., lecture
notes) containing a set of correct named entities. Finally, we introduce the
NER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses
for development and testing. On this dataset, our proposed technique achieves
up to 30\% relative WER reduction for named entities.

</details>


### [49] [Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints](https://arxiv.org/pdf/2506.10800)
*Wei Sun, Tingyu Qu, Mingxiao Li, Jesse Davis, Marie-Francine Moens*

Main category: cs.CL

TL;DR: LangEdit is a framework for efficient multilingual knowledge updates in LLMs, preventing interference by isolating language-specific updates via null-space projection.


<details>
  <summary>Details</summary>
Motivation: The challenge of updating multilingual knowledge in LLMs without causing destructive interference or high costs motivates LangEdit.

Method: LangEdit projects parameter updates for each language onto orthogonal subspaces to ensure independence and preserve generalization.

Result: LangEdit outperforms existing methods, mitigating interference and improving accuracy across multiple languages and tasks.

Conclusion: LangEdit enables efficient, accurate multilingual updates in LLMs, with potential for broader applications.

Abstract: Efficiently updating multilingual knowledge in large language models (LLMs),
while preserving consistent factual representations across languages, remains a
long-standing and unresolved challenge. While deploying separate editing
systems for each language might seem viable, this approach incurs substantial
costs due to the need to manage multiple models. A more efficient solution
involves integrating knowledge updates across all languages into a unified
model. However, performing sequential edits across languages often leads to
destructive parameter interference, significantly degrading multilingual
generalization and the accuracy of injected knowledge. To address this
challenge, we propose LangEdit, a novel null-space constrained framework
designed to precisely isolate language-specific knowledge updates. The core
innovation of LangEdit lies in its ability to project parameter updates for
each language onto the orthogonal complement of previous updated subspaces.
This approach mathematically guarantees update independence while preserving
multilingual generalization capabilities. We conduct a comprehensive evaluation
across three model architectures, six languages, and four downstream tasks,
demonstrating that LangEdit effectively mitigates parameter interference and
outperforms existing state-of-the-art editing methods. Our results highlight
its potential for enabling efficient and accurate multilingual knowledge
updates in LLMs. The code is available at
https://github.com/VRCMF/LangEdit.git.

</details>


### [50] [ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization](https://arxiv.org/pdf/2506.10822)
*Zhensheng Jin, Xinze Li, Yifan Ji, Chunyi Peng, Zhenghao Liu, Qi Shi, Yukun Yan, Shuo Wang, Furong Peng, Ge Yu*

Main category: cs.CL

TL;DR: ReCUT is a novel method to reduce reasoning length in LLMs while maintaining accuracy, using stepwise exploration and preference-based training.


<details>
  <summary>Details</summary>
Motivation: Addressing overthinking and redundancy in Chain-of-Thought prompting for LLMs, which existing methods struggle with due to data quality and overfitting.

Method: ReCUT employs stepwise exploration, long-short sampling, and trains two specialized models (accuracy and brevity) combined via parameter interpolation.

Result: ReCUT reduces reasoning length by 30-50% while maintaining or improving accuracy across math reasoning datasets.

Conclusion: ReCUT effectively balances reasoning accuracy and brevity, outperforming baselines, with code and data publicly available.

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
improved the reasoning capabilities of Large Language Models (LLMs). However,
these methods often suffer from overthinking, leading to unnecessarily lengthy
or redundant reasoning traces. Existing approaches attempt to mitigate this
issue through curating multiple reasoning chains for training LLMs, but their
effectiveness is often constrained by the quality of the generated data and
prone to overfitting. To address the challenge, we propose Reasoning
Compression ThroUgh Stepwise Trials (ReCUT), a novel method aimed at balancing
the accuracy and length of reasoning trajectory. Specifically, ReCUT employs a
stepwise exploration mechanism and a long-short switched sampling strategy,
enabling LLMs to incrementally generate diverse reasoning paths. These paths
are evaluated and used to construct preference pairs to train two specialized
models (Gemini LLMs)-one optimized for reasoning accuracy, the other for
shorter reasoning. A final integrated model is obtained by interpolating the
parameters of these two models. Experimental results across multiple math
reasoning datasets and backbone models demonstrate that ReCUT significantly
reduces reasoning lengths by approximately 30-50%, while maintaining or
improving reasoning accuracy compared to various baselines. All codes and data
will be released via https://github.com/NEUIR/ReCUT.

</details>


### [51] [CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training](https://arxiv.org/pdf/2506.10844)
*Alireza Salemi, Mukta Maddipatla, Hamed Zamani*

Main category: cs.CL

TL;DR: mRAG is a multi-agent RAG framework with specialized agents for tasks like planning and reasoning, optimized via self-training and reward-guided sampling, outperforming baselines in SIGIR 2025 LiveRAG.


<details>
  <summary>Details</summary>
Motivation: To enhance RAG systems by leveraging multi-agent collaboration for complex tasks.

Method: Uses specialized agents and self-training with reward-guided trajectory sampling to optimize inter-agent collaboration.

Result: Outperforms conventional RAG baselines on DataMorgana-derived datasets in SIGIR 2025 LiveRAG.

Conclusion: mRAG is effective for complex RAG tasks, as demonstrated by competition results and case studies.

Abstract: This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG)
framework composed of specialized agents for subtasks such as planning,
searching, reasoning, and coordination. Our system uses a self-training
paradigm with reward-guided trajectory sampling to optimize inter-agent
collaboration and enhance response generation. Evaluated on DataMorgana-derived
datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms
conventional RAG baselines. We further analyze competition outcomes and
showcase the framework's strengths with case studies, demonstrating its
efficacy for complex, real-world RAG tasks.

</details>


### [52] [Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles](https://arxiv.org/pdf/2506.10848)
*Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, Linfeng Zhang*

Main category: cs.CL

TL;DR: SlowFast Sampling is a dynamic strategy for diffusion-based language models (dLLMs) that alternates between exploratory and accelerated decoding, achieving significant speedups with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing sampling strategies for dLLMs suffer from static behavior, leading to inefficiency and limited flexibility. SlowFast Sampling aims to address this by dynamically adapting decoding stages.

Method: Proposes SlowFast Sampling, guided by certainty, convergence, and positional principles, and integrates it with dLLM-Cache to reduce redundant computation.

Result: Achieves up to 15.63× speedup on LLaDA with minimal accuracy drop, and up to 34.22× with caching, outperforming autoregressive baselines like LLaMA3 8B.

Conclusion: Well-designed sampling can unlock the full potential of dLLMs for fast, high-quality generation, as demonstrated by SlowFast Sampling.

Abstract: Diffusion-based language models (dLLMs) have emerged as a promising
alternative to traditional autoregressive LLMs by enabling parallel token
generation and significantly reducing inference latency. However, existing
sampling strategies for dLLMs, such as confidence-based or semi-autoregressive
decoding, often suffer from static behavior, leading to suboptimal efficiency
and limited flexibility. In this paper, we propose SlowFast Sampling, a novel
dynamic sampling strategy that adaptively alternates between exploratory and
accelerated decoding stages. Our method is guided by three golden principles:
certainty principle, convergence principle, and positional principle, which
govern when and where tokens can be confidently and efficiently decoded. We
further integrate our strategy with dLLM-Cache to reduce redundant computation.
Extensive experiments across benchmarks and models show that SlowFast Sampling
achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and
up to 34.22$\times$ when combined with caching. Notably, our approach
outperforms strong autoregressive baselines like LLaMA3 8B in throughput,
demonstrating that well-designed sampling can unlock the full potential of
dLLMs for fast and high-quality generation.

</details>


### [53] [Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment](https://arxiv.org/pdf/2506.10877)
*Hongda Sun, Jiaren Peng, Wenzhong Yang, Liang He, Bo Du, Rui Yan*

Main category: cs.CL

TL;DR: MedRef improves medical dialogue systems by refining knowledge and dynamically adjusting prompts, outperforming existing methods in accuracy and quality.


<details>
  <summary>Details</summary>
Motivation: Existing medical dialogue systems struggle with identifying relevant knowledge and generating personalized, accurate responses.

Method: MedRef uses knowledge refining and dynamic prompt adjustment, including Triplet Filter and Demo Selector modules for real-time adaptability.

Result: MedRef outperforms state-of-the-art baselines in generation quality and medical entity accuracy on MedDG and KaMed benchmarks.

Conclusion: MedRef is effective and reliable for real-world healthcare applications.

Abstract: Medical dialogue systems (MDS) have emerged as crucial online platforms for
enabling multi-turn, context-aware conversations with patients. However,
existing MDS often struggle to (1) identify relevant medical knowledge and (2)
generate personalized, medically accurate responses. To address these
challenges, we propose MedRef, a novel MDS that incorporates knowledge refining
and dynamic prompt adjustment. First, we employ a knowledge refining mechanism
to filter out irrelevant medical data, improving predictions of critical
medical entities in responses. Additionally, we design a comprehensive prompt
structure that incorporates historical details and evident details. To enable
real-time adaptability to diverse patient conditions, we implement two key
modules, Triplet Filter and Demo Selector, providing appropriate knowledge and
demonstrations equipped in the system prompt. Extensive experiments on MedDG
and KaMed benchmarks show that MedRef outperforms state-of-the-art baselines in
both generation quality and medical entity accuracy, underscoring its
effectiveness and reliability for real-world healthcare applications.

</details>


### [54] [Slimming Down LLMs Without Losing Their Minds](https://arxiv.org/pdf/2506.10885)
*Qingda, Mai*

Main category: cs.CL

TL;DR: Fine-tuning with LoRA and QLoRA improves task-specific performance efficiently, with success tied to dataset-task alignment.


<details>
  <summary>Details</summary>
Motivation: To validate the impact of parameter-efficient fine-tuning (LoRA, QLoRA) on LLM performance across reasoning and knowledge tasks.

Method: Evaluated LoRA and QLoRA on commonsense (HellaSwag), mathematical (GSM8K), and multi-domain (MMLU-CS) benchmarks.

Result: LoRA methods enhance performance efficiently; success depends on dataset-task alignment.

Conclusion: Provides insights into parameter-efficient mechanisms and practical guidance for resource-limited LLM adaptation.

Abstract: This paper investigates and validates the impact of fine-tuning on large
language model performance, focusing on parameter-efficient methods (LoRA and
QLoRA). We evaluate model capabilities across three key domains: (1)
commonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3)
multi-domain knowledge (MMLU-CS).
  Our findings demonstrate that: (1) LoRA-based methods effectively improve
task-specific performance while maintaining computational efficiency, and (2)
performance strongly depends on alignment between fine-tuning dataset and
benchmark tasks. The study provides both theoretical insights into
parameter-efficient mechanisms and practical guidance for developers
implementing efficient LLM adaptation with limited resources.

</details>


### [55] [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/pdf/2506.10887)
*Yixiao Huang, Hanlin Zhu, Tianyu Guo, Jiantao Jiao, Somayeh Sojoudi, Michael I. Jordan, Stuart Russell, Song Mei*

Main category: cs.CL

TL;DR: The paper explores the dual behavior of LLMs in generalizing and hallucinating due to out-of-context reasoning (OCR), formalizes OCR as a task, and attributes it to gradient descent's implicit bias favoring low nuclear norm solutions.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs exhibit both generalization and hallucination when acquiring new knowledge through fine-tuning.

Method: Formalizes OCR as a synthetic factual recall task, tests it on five LLMs, and analyzes a simplified transformer model with factorized matrices.

Result: OCR drives both behaviors; matrix factorization is crucial, and gradient descent's implicit bias explains the phenomenon.

Conclusion: Provides a theoretical foundation for OCR, aiding in analyzing and mitigating undesirable LLM behaviors.

Abstract: Large language models (LLMs) can acquire new knowledge through fine-tuning,
but this process exhibits a puzzling duality: models can generalize remarkably
from new facts, yet are also prone to hallucinating incorrect information.
However, the reasons for this phenomenon remain poorly understood. In this
work, we argue that both behaviors stem from a single mechanism known as
out-of-context reasoning (OCR): the ability to deduce implications by
associating concepts, even those without a causal link. Our experiments across
five prominent LLMs confirm that OCR indeed drives both generalization and
hallucination, depending on whether the associated concepts are causally
related. To build a rigorous theoretical understanding of this phenomenon, we
then formalize OCR as a synthetic factual recall task. We empirically show that
a one-layer single-head attention-only transformer with factorized output and
value matrices can learn to solve this task, while a model with combined
weights cannot, highlighting the crucial role of matrix factorization. Our
theoretical analysis shows that the OCR capability can be attributed to the
implicit bias of gradient descent, which favors solutions that minimize the
nuclear norm of the combined output-value matrix. This mathematical structure
explains why the model learns to associate facts and implications with high
sample efficiency, regardless of whether the correlation is causal or merely
spurious. Ultimately, our work provides a theoretical foundation for
understanding the OCR phenomenon, offering a new lens for analyzing and
mitigating undesirable behaviors from knowledge injection.

</details>


### [56] [BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP](https://arxiv.org/pdf/2506.10896)
*Thomas Sounack, Joshua Davis, Brigitte Durieux, Antoine Chaffin, Tom J. Pollard, Eric Lehman, Alistair E. W. Johnson, Matthew McDermott, Tristan Naumann, Charlotta Lindvall*

Main category: cs.CL

TL;DR: BioClinical ModernBERT is a domain-adapted encoder for biomedical and clinical NLP, improving speed and performance over existing models.


<details>
  <summary>Details</summary>
Motivation: Encoder-based models lag in development compared to decoders, limiting domain adaptation in biomedical and clinical NLP.

Method: Developed through continued pretraining on a large corpus (53.5B tokens) and diverse datasets (20 sources).

Result: Outperforms existing encoders on four downstream tasks.

Conclusion: BioClinical ModernBERT addresses prior limitations and advances biomedical and clinical NLP, with released models for further research.

Abstract: Encoder-based transformer models are central to biomedical and clinical
Natural Language Processing (NLP), as their bidirectional self-attention makes
them well-suited for efficiently extracting structured information from
unstructured text through discriminative tasks. However, encoders have seen
slower development compared to decoder models, leading to limited domain
adaptation in biomedical and clinical settings. We introduce BioClinical
ModernBERT, a domain-adapted encoder that builds on the recent ModernBERT
release, incorporating long-context processing and substantial improvements in
speed and performance for biomedical and clinical NLP. BioClinical ModernBERT
is developed through continued pretraining on the largest biomedical and
clinical corpus to date, with over 53.5 billion tokens, and addresses a key
limitation of prior clinical encoders by leveraging 20 datasets from diverse
institutions, domains, and geographic regions, rather than relying on data from
a single source. It outperforms existing biomedical and clinical encoders on
four downstream tasks spanning a broad range of use cases. We release both base
(150M parameters) and large (396M parameters) versions of BioClinical
ModernBERT, along with training checkpoints to support further research.

</details>


### [57] [Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning](https://arxiv.org/pdf/2506.10903)
*Lan Zhang, Marco Valentino, Andre Freitas*

Main category: cs.CL

TL;DR: The paper introduces a systematic, automatic method (EFG ensemble of LLM judges) to evaluate autoformalization tasks in formal mathematics, outperforming coarse-grained models by aligning better with human assessments.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating autoformalization lack granularity and scalability, especially for advanced mathematics, requiring time-consuming human expertise.

Method: Proposes an epistemically and formally grounded ensemble (EFG) of LLM judges, evaluating autoformalization on criteria like logical preservation, mathematical consistency, formal validity, and formal quality.

Result: The EFG ensemble correlates more strongly with human assessments than coarse-grained models, particularly for formal qualities.

Conclusion: LLM-as-judges, guided by well-defined criteria, can provide scalable, interpretable, and reliable evaluation for formal mathematical reasoning.

Abstract: Autoformalization plays a crucial role in formal mathematical reasoning by
enabling the automatic translation of natural language statements into formal
languages. While recent advances using large language models (LLMs) have shown
promising results, methods for automatically evaluating autoformalization
remain underexplored. As one moves to more complex domains (e.g., advanced
mathematics), human evaluation requires significant time and domain expertise,
especially as the complexity of the underlying statements and background
knowledge increases. LLM-as-a-judge presents a promising approach for
automating such evaluation. However, existing methods typically employ
coarse-grained and generic evaluation criteria, which limit their effectiveness
for advanced formal mathematical reasoning, where quality hinges on nuanced,
multi-granular dimensions. In this work, we take a step toward addressing this
gap by introducing a systematic, automatic method to evaluate autoformalization
tasks. The proposed method is based on an epistemically and formally grounded
ensemble (EFG) of LLM judges, defined on criteria encompassing logical
preservation (LP), mathematical consistency (MC), formal validity (FV), and
formal quality (FQ), resulting in a transparent assessment that accounts for
different contributing factors. We validate the proposed framework to serve as
a proxy for autoformalization assessment within the domain of formal
mathematics. Overall, our experiments demonstrate that the EFG ensemble of LLM
judges is a suitable emerging proxy for evaluation, more strongly correlating
with human assessments than a coarse-grained model, especially when assessing
formal qualities. These findings suggest that LLM-as-judges, especially when
guided by a well-defined set of atomic properties, could offer a scalable,
interpretable, and reliable support for evaluating formal mathematical
reasoning.

</details>


### [58] [Magistral](https://arxiv.org/pdf/2506.10910)
*Mistral-AI, :, Abhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, Léonard Blier, Lucile Saulnier, Matthieu Dinot, Maxime Darrin, Neha Gupta, Roman Soletskyi, Sagar Vaze, Teven Le Scao, Yihan Wang, Adam Yang, Alexander H. Liu, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Andy Ehrenberg, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Darius Dabert, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jean-Hadrien Chabran, Jean-Malo Delignon, Joachim Studnia, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Kush Jain, Lingxiao Zhao, Louis Martin, Luyu Gao, Lélio Renard Lavaud, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Maximilian Augustin, Mickaël Seznec, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patrick von Platen, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Romain Sauvestre, Rémi Delacourt, Sanchit Gandhi, Sandeep Subramanian, Shashwat Dalal, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timothée Lacroix, Valeriia Nemychnikova, Victor Paltz, Virgile Richard, Wen-Ding Li, William Marshall, Xuanyu Zhang, Yunhao Tang*

Main category: cs.CL

TL;DR: Magistral is Mistral's first reasoning model, trained using a scalable RL pipeline without relying on prior models. It demonstrates RL's effectiveness on text data, maintaining or improving capabilities like multimodal understanding and instruction following. Magistral Medium and Small are introduced, with the latter being open-sourced.


<details>
  <summary>Details</summary>
Motivation: To explore the limits of pure RL training for LLMs and develop a scalable, independent pipeline without relying on existing implementations or distilled traces.

Method: A ground-up approach using Mistral's own models and infrastructure, focusing on pure RL training of LLMs. Includes a method to force the reasoning language of the model.

Result: RL on text data maintains or improves capabilities like multimodal understanding, instruction following, and function calling. Magistral Medium and Small are successfully trained.

Conclusion: Pure RL training is viable for LLMs, and Magistral demonstrates its effectiveness, with open-sourcing of Magistral Small to encourage further research.

Abstract: We introduce Magistral, Mistral's first reasoning model and our own scalable
reinforcement learning (RL) pipeline. Instead of relying on existing
implementations and RL traces distilled from prior models, we follow a ground
up approach, relying solely on our own models and infrastructure. Notably, we
demonstrate a stack that enabled us to explore the limits of pure RL training
of LLMs, present a simple method to force the reasoning language of the model,
and show that RL on text data alone maintains most of the initial checkpoint's
capabilities. We find that RL on text maintains or improves multimodal
understanding, instruction following and function calling. We present Magistral
Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we
open-source Magistral Small (Apache 2.0) which further includes cold-start data
from Magistral Medium.

</details>


### [59] [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/pdf/2506.10920)
*Or Shafran, Atticus Geiger, Mor Geva*

Main category: cs.CL

TL;DR: SNMF outperforms SAEs in identifying interpretable features in LLMs by decomposing MLP activations, aligning with human concepts and revealing hierarchical structures.


<details>
  <summary>Details</summary>
Motivation: Current methods like SAEs struggle with causal evaluations and lack intrinsic interpretability, motivating the need for a better approach.

Method: Uses semi-nonnegative matrix factorization (SNMF) to decompose MLP activations, ensuring sparse linear combinations and direct interpretability.

Result: SNMF-derived features outperform SAEs and supervised baselines in causal steering and align with human-interpretable concepts.

Conclusion: SNMF is a simple, effective tool for identifying interpretable features and dissecting concept representations in LLMs.

Abstract: A central goal for mechanistic interpretability has been to identify the
right units of analysis in large language models (LLMs) that causally explain
their outputs. While early work focused on individual neurons, evidence that
neurons often encode multiple concepts has motivated a shift toward analyzing
directions in activation space. A key question is how to find directions that
capture interpretable features in an unsupervised manner. Current methods rely
on dictionary learning with sparse autoencoders (SAEs), commonly trained over
residual stream activations to learn directions from scratch. However, SAEs
often struggle in causal evaluations and lack intrinsic interpretability, as
their learning is not explicitly tied to the computations of the model. Here,
we tackle these limitations by directly decomposing MLP activations with
semi-nonnegative matrix factorization (SNMF), such that the learned features
are (a) sparse linear combinations of co-activated neurons, and (b) mapped to
their activating inputs, making them directly interpretable. Experiments on
Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs
and a strong supervised baseline (difference-in-means) on causal steering,
while aligning with human-interpretable concepts. Further analysis reveals that
specific neuron combinations are reused across semantically-related features,
exposing a hierarchical structure in the MLP's activation space. Together,
these results position SNMF as a simple and effective tool for identifying
interpretable features and dissecting concept representations in LLMs.

</details>


### [60] [Dynamic Epistemic Friction in Dialogue](https://arxiv.org/pdf/2506.10934)
*Timothy Obiso, Kenneth Lai, Abhijnan Nath, Nikhil Krishnaswamy, James Pustejovsky*

Main category: cs.CL

TL;DR: The paper introduces 'dynamic epistemic friction' to describe resistance in belief updates during human-AI collaboration, using Dynamic Epistemic Logic to model and predict belief changes in dialogues.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods for LLMs overlook epistemic friction, which is crucial for understanding belief updates in collaborative tasks.

Method: The study defines dynamic epistemic friction within Dynamic Epistemic Logic and analyzes its role in belief updates during dialogues.

Result: The model effectively predicts belief updates in collaborative tasks, highlighting the importance of epistemic friction.

Conclusion: The framework can be refined to better handle real-world dialogue complexities, enhancing belief alignment in AI systems.

Abstract: Recent developments in aligning Large Language Models (LLMs) with human
preferences have significantly enhanced their utility in human-AI collaborative
scenarios. However, such approaches often neglect the critical role of
"epistemic friction," or the inherent resistance encountered when updating
beliefs in response to new, conflicting, or ambiguous information. In this
paper, we define dynamic epistemic friction as the resistance to epistemic
integration, characterized by the misalignment between an agent's current
belief state and new propositions supported by external evidence. We position
this within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,
2011), where friction emerges as nontrivial belief-revision during the
interaction. We then present analyses from a situated collaborative task that
demonstrate how this model of epistemic friction can effectively predict belief
updates in dialogues, and we subsequently discuss how the model of belief
alignment as a measure of epistemic resistance or friction can naturally be
made more sophisticated to accommodate the complexities of real-world dialogue
scenarios.

</details>


### [61] [Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training](https://arxiv.org/pdf/2506.10952)
*Mozhi Zhang, Howe Tissue, Lu Wang, Xipeng Qiu*

Main category: cs.CL

TL;DR: Domain2Vec decomposes datasets into meta-domains, enabling optimal data mixture for LM pretraining without training, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance language model pretraining by identifying optimal data mixtures efficiently, reducing computational overhead.

Method: Decomposes datasets into meta-domains, uses a classifier for domain vectors, and aligns data distributions under DA² assumption.

Result: Achieves same validation loss with 51.5% computation, improves downstream performance by 2.83% under equivalent budget.

Conclusion: Domain2Vec efficiently optimizes data mixtures for LM pretraining, enhancing performance and scalability.

Abstract: We introduce~\textsc{Domain2Vec}, a novel approach that decomposes any
dataset into a linear combination of several \emph{meta-domains}, a new concept
designed to capture the key underlying features of datasets.
\textsc{Domain2Vec} maintains a vocabulary of meta-domains and uses a
classifier to decompose any given dataset into a domain vector that corresponds
to a distribution over this vocabulary. These domain vectors enable the
identification of the optimal data mixture for language model (LM) pretraining
in a training-free manner under the \emph{\textbf{D}istribution
\textbf{A}lignment \textbf{A}ssumption} (DA$^{2}$), which suggests that when
the data distributions of the training set and the validation set are better
aligned, a lower validation loss is achieved. Moreover, \textsc{Domain2vec} can
be seamlessly integrated into previous works to model the relationship between
domain vectors and LM performance, greatly enhancing the efficiency and
scalability of previous methods. Extensive experiments demonstrate that
\textsc{Domain2Vec} helps find the data mixture that enhances downstream task
performance with minimal computational overhead. Specifically,
\textsc{Domain2Vec} achieves the same validation loss on Pile-CC using only
$51.5\%$ of the computation required when training on the original mixture of
The Pile dataset. Under equivalent compute budget, \textsc{Domain2Vec} improves
downstream performance by an average of $2.83\%$.

</details>


### [62] [ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark](https://arxiv.org/pdf/2506.10960)
*Kangwei Liu, Siyuan Cheng, Bozhong Tian, Xiaozhuan Liang, Yuyang Yin, Meng Han, Ningyu Zhang, Bryan Hooi, Xi Chen, Shumin Deng*

Main category: cs.CL

TL;DR: A benchmark for Chinese harmful content detection is introduced, addressing the scarcity of such resources. It includes a knowledge rule base and a knowledge-augmented baseline to enhance detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing harmful content detection resources are mainly English-focused, leaving Chinese datasets scarce and limited. This work aims to fill this gap.

Method: A professionally annotated benchmark for Chinese content harm detection is created, covering six categories. A knowledge-augmented baseline integrates human-annotated rules and LLM knowledge.

Result: The proposed method enables smaller models to achieve performance comparable to state-of-the-art LLMs.

Conclusion: The benchmark and knowledge-augmented approach provide valuable resources and methods for Chinese harmful content detection.

Abstract: Large language models (LLMs) have been increasingly applied to automated
harmful content detection tasks, assisting moderators in identifying policy
violations and improving the overall efficiency and accuracy of content review.
However, existing resources for harmful content detection are predominantly
focused on English, with Chinese datasets remaining scarce and often limited in
scope. We present a comprehensive, professionally annotated benchmark for
Chinese content harm detection, which covers six representative categories and
is constructed entirely from real-world data. Our annotation process further
yields a knowledge rule base that provides explicit expert knowledge to assist
LLMs in Chinese harmful content detection. In addition, we propose a
knowledge-augmented baseline that integrates both human-annotated knowledge
rules and implicit knowledge from large language models, enabling smaller
models to achieve performance comparable to state-of-the-art LLMs. Code and
data are available at https://github.com/zjunlp/ChineseHarm-bench.

</details>


### [63] [How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?](https://arxiv.org/pdf/2506.10979)
*Sohee Yang, Sang-Woo Lee, Nora Kassner, Daniela Gottesman, Sebastian Riedel, Mor Geva*

Main category: cs.CL

TL;DR: Models can identify unhelpful thoughts but struggle to recover from them, especially larger models. Self-reevaluation needs improvement for better reasoning and safety.


<details>
  <summary>Details</summary>
Motivation: To investigate how effectively reasoning models can self-reevaluate and recover from unhelpful thoughts.

Method: Analyzed models' ability to identify and recover from four types of unhelpful thoughts, including irrelevant or misleading ones.

Result: Models identify unhelpful thoughts well but fail to recover, with larger models performing worse. Small models are least distracted by harmful thoughts.

Conclusion: Improving self-reevaluation in reasoning models is crucial for better performance and safety.

Abstract: Recent reasoning models show the ability to reflect, backtrack, and
self-validate their reasoning, which is crucial in spotting mistakes and
arriving at accurate solutions. A natural question that arises is how
effectively models can perform such self-reevaluation. We tackle this question
by investigating how well reasoning models identify and recover from four types
of unhelpful thoughts: uninformative rambling thoughts, thoughts irrelevant to
the question, thoughts misdirecting the question as a slightly different
question, and thoughts that lead to incorrect answers. We show that models are
effective at identifying most unhelpful thoughts but struggle to recover from
the same thoughts when these are injected into their thinking process, causing
significant performance drops. Models tend to naively continue the line of
reasoning of the injected irrelevant thoughts, which showcases that their
self-reevaluation abilities are far from a general "meta-cognitive" awareness.
Moreover, we observe non/inverse-scaling trends, where larger models struggle
more than smaller ones to recover from short irrelevant thoughts, even when
instructed to reevaluate their reasoning. We demonstrate the implications of
these findings with a jailbreak experiment using irrelevant thought injection,
showing that the smallest models are the least distracted by
harmful-response-triggering thoughts. Overall, our findings call for
improvement in self-reevaluation of reasoning models to develop better
reasoning and safer systems.

</details>


### [64] [ConvD: Attention Enhanced Dynamic Convolutional Embeddings for Knowledge Graph Completion](https://arxiv.org/pdf/2312.07589)
*Wenbin Guo, Zhao Li, Xin Wang, Zirui Chen, Jun Zhao, Jianxin Li, Ye Yuan*

Main category: cs.CL

TL;DR: A novel dynamic convolutional embedding model (ConvD) is introduced to enhance feature interactions in knowledge graphs by reshaping relation embeddings into internal kernels and using a knowledge-optimized attention mechanism, outperforming baselines with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the incompleteness of knowledge graphs and limitations of current deep convolutional models in feature interaction.

Method: Proposes ConvD, which reshapes relation embeddings into internal convolution kernels and integrates a knowledge-optimized attention mechanism.

Result: Outperforms state-of-the-art methods by 3.28% to 14.69% in metrics while reducing parameters by 50.66% to 85.40%.

Conclusion: ConvD effectively improves knowledge graph completion with enhanced feature interaction and efficiency.

Abstract: Knowledge graphs often suffer from incompleteness issues, which can be
alleviated through information completion. However, current state-of-the-art
deep knowledge convolutional embedding models rely on external convolution
kernels and conventional convolution processes, which limits the feature
interaction capability of the model. This paper introduces a novel dynamic
convolutional embedding model, ConvD, which directly reshapes relation
embeddings into multiple internal convolution kernels. This approach
effectively enhances the feature interactions between relation embeddings and
entity embeddings. Simultaneously, we incorporate a priori knowledge-optimized
attention mechanism that assigns different contribution weight coefficients to
the multiple relation convolution kernels in dynamic convolution, further
boosting the expressive power of the model. Extensive experiments on various
datasets show that our proposed model consistently outperforms the
state-of-the-art baseline methods, with average improvements ranging from 3.28%
to 14.69% across all model evaluation metrics, while the number of parameters
is reduced by 50.66% to 85.40% compared to other state-of-the-art models.

</details>


### [65] [Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/pdf/2401.17256)
*Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang*

Main category: cs.CL

TL;DR: The paper introduces a weak-to-strong jailbreaking attack, an efficient method to exploit LLMs for harmful text generation by leveraging smaller models to manipulate decoding probabilities.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreaking methods are computationally expensive, prompting the need for a more efficient attack to expose LLM vulnerabilities.

Method: The attack uses two smaller models (safe and unsafe) to adversarially modify a larger safe model's decoding probabilities, requiring only one forward pass per example.

Result: The method achieves a misalignment rate over 99% on two datasets, highlighting a critical safety issue in aligned LLMs.

Conclusion: The study underscores the urgency of addressing LLM safety vulnerabilities, proposing a preliminary defense while acknowledging the challenge of advanced solutions.

Abstract: Large language models (LLMs) are vulnerable to jailbreak attacks - resulting
in harmful, unethical, or biased text generations. However, existing
jailbreaking methods are computationally costly. In this paper, we propose the
weak-to-strong jailbreaking attack, an efficient inference time attack for
aligned LLMs to produce harmful text. Our key intuition is based on the
observation that jailbroken and aligned models only differ in their initial
decoding distributions. The weak-to-strong attack's key technical insight is
using two smaller models (a safe and an unsafe one) to adversarially modify a
significantly larger safe model's decoding probabilities. We evaluate the
weak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The
results show our method can increase the misalignment rate to over 99% on two
datasets with just one forward pass per example. Our study exposes an urgent
safety issue that needs to be addressed when aligning LLMs. As an initial
attempt, we propose a defense strategy to protect against such attacks, but
creating more advanced defenses remains challenging. The code for replicating
the method is available at https://github.com/XuandongZhao/weak-to-strong

</details>


### [66] [Visually Descriptive Language Model for Vector Graphics Reasoning](https://arxiv.org/pdf/2404.06479)
*Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li, Jiajun Wu, Heng Ji*

Main category: cs.CL

TL;DR: The paper introduces VDLM, a method to bridge the gap between low-level visual perception and high-level reasoning in LMMs using SVG-based PVD representations, improving zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: LMMs struggle with tasks requiring precise visual perception and reasoning. The paper aims to address this by enabling accurate visual encoding and structured reasoning.

Method: Proposes VDLM with Primal Visual Description (PVD), translating SVGs into text-based abstractions for universal visual primitives, trained on synthetic data.

Result: VDLM improves state-of-the-art LMMs like GPT-4o on multimodal tasks without human-annotated data, showing better interpretability and performance.

Conclusion: VDLM effectively bridges visual perception and reasoning, with PVD quality correlating positively with task performance.

Abstract: Despite significant advancements, large multimodal models (LMMs) still
struggle to bridge the gap between low-level visual perception -- focusing on
shapes, sizes, and layouts -- and high-level language reasoning, such as
semantics and logic. This limitation is evident in tasks that require precise
visual perception, like comparing geometric properties or solving visual
reasoning problems. To study this failure mode, we focus on vector graphics --
images composed of 2D objects and shapes, prevalent in LMM-based tasks in web,
design, and OS environments. We identify two key research questions: how can we
enable precise visual perception, and how can we facilitate high-level
reasoning based on such low-level perceptions? To capture fine visual details,
we use Scalable Vector Graphics (SVG) for accurate encoding of visual scenes.
However, SVGs are not readily interpretable by LMMs in a zero-shot manner. To
tackle this, we propose the Visually Descriptive Language Model (VDLM), which
introduces a Primal Visual Description (PVD) as an intermediate textual
representation. PVD translates SVGs into a text-based abstraction consisting of
primitive attributes (e.g., shape, position, measurement) and their
corresponding values. PVD can be learned using task-agnostic synthesized data
and represents visual primitives that are universal across vector graphics.
This abstraction is more structured, allowing for direct interpretation by
foundation models for zero-shot generalization. Without human-annotated data,
empirical results show that VDLM significantly improves state-of-the-art LMMs
like GPT-4o on various multimodal perception and reasoning tasks. Extensive
analyses of VDLM show improved interpretability due to its disentangled
perception and reasoning. We also demonstrate a positive correlation between
PVD quality and task performance. Project page:
https://mikewangwzhl.github.io/VDLM/

</details>


### [67] [IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language](https://arxiv.org/pdf/2406.19349)
*Lucky Susanto, Musa Izzanardi Wijanarko, Prasetia Anugrah Pratama, Traci Hong, Ika Idris, Alham Fikri Aji, Derry Wijaya*

Main category: cs.CL

TL;DR: The paper introduces IndoToxic2024, a dataset for Indonesian hate speech detection, addressing data scarcity and subjectivity issues, and evaluates classification models including IndoBERTweet and GPT-3.5-turbo.


<details>
  <summary>Details</summary>
Motivation: The rise in online hate speech in Indonesia, especially against marginalized groups, and the lack of labeled data and subjectivity accommodation in existing datasets.

Method: Creation of IndoToxic2024, a dataset with 43,692 entries annotated by 19 individuals, focusing on vulnerable groups during the presidential election. Baseline models (IndoBERTweet) and zero-shot evaluation with GPT-3.5-turbo were tested.

Result: Achieved a macro-F1 score of 0.78 with IndoBERTweet. Demographic information improved GPT-3.5-turbo's zero-shot performance but harmed fine-tuned models due to data fragmentation.

Conclusion: IndoToxic2024 addresses critical gaps in hate speech detection for Indonesian texts, though balancing demographic data is crucial to avoid performance trade-offs.

Abstract: Hate speech poses a significant threat to social harmony. Over the past two
years, Indonesia has seen a ten-fold increase in the online hate speech ratio,
underscoring the urgent need for effective detection mechanisms. However,
progress is hindered by the limited availability of labeled data for Indonesian
texts. The condition is even worse for marginalized minorities, such as Shia,
LGBTQ, and other ethnic minorities because hate speech is underreported and
less understood by detection tools. Furthermore, the lack of accommodation for
subjectivity in current datasets compounds this issue. To address this, we
introduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity
classification dataset. Comprising 43,692 entries annotated by 19 diverse
individuals, the dataset focuses on texts targeting vulnerable groups in
Indonesia, specifically during the hottest political event in the country: the
presidential election. We establish baselines for seven binary classification
tasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)
fine-tuned for hate speech classification. Furthermore, we demonstrate how
incorporating demographic information can enhance the zero-shot performance of
the large language model, gpt-3.5-turbo. However, we also caution that an
overemphasis on demographic information can negatively impact the fine-tuned
model performance due to data fragmentation.

</details>


### [68] [Benchmarking LLMs for Environmental Review and Permitting](https://arxiv.org/pdf/2407.07321)
*Rounak Meyur, Hung Phan, Koby Hayashi, Ian Stewart, Shivam Sharma, Sarthak Chaturvedi, Mike Parker, Dan Nally, Sadie Montgomery, Karl Pazdernik, Ali Jannesari, Mahantesh Halappanavar, Sai Munikoti, Sameera Horawalavithana, Anurag Acharya*

Main category: cs.CL

TL;DR: The paper introduces NEPAQuAD, a benchmark for evaluating LLMs on NEPA-focused regulatory reasoning tasks, and MAPLE, an evaluation pipeline. Results show LLMs perform best with gold passage context, but struggle with long documents.


<details>
  <summary>Details</summary>
Motivation: To test LLMs' effectiveness in specialized domains like NEPA, where their adoption in federal decision-making remains untested.

Method: Created NEPAQuAD benchmark from EIS documents and MAPLE pipeline to evaluate LLMs on diverse question types. Tested five state-of-the-art models in zero-shot and context-driven QA tasks.

Result: LLMs perform best with gold passage context. RAG-based approaches outperform PDF contexts, but models struggle with long regulatory documents.

Conclusion: NEPA-focused tasks challenge LLMs, especially in processing complex semantics and lengthy documents.

Abstract: The National Environment Policy Act (NEPA) stands as a foundational piece of
environmental legislation in the United States, requiring federal agencies to
consider the environmental impacts of their proposed actions. The primary
mechanism for achieving this is through the preparation of Environmental
Assessments (EAs) and, for significant impacts, comprehensive Environmental
Impact Statements (EIS). Large Language Model (LLM)s' effectiveness in
specialized domains like NEPA remains untested for adoption in federal
decision-making processes. To address this gap, we present NEPA Question and
Answering Dataset (NEPAQuAD), the first comprehensive benchmark derived from
EIS documents, along with a modular and transparent evaluation pipeline, MAPLE,
to assess LLM performance on NEPA-focused regulatory reasoning tasks. Our
benchmark leverages actual EIS documents to create diverse question types,
ranging from factual to complex problem-solving ones. We built a modular and
transparent evaluation pipeline to test both closed- and open-source models in
zero-shot or context-driven QA benchmarks. We evaluate five state-of-the-art
LLMs using our framework to assess both their prior knowledge and their ability
to process NEPA-specific information. The experimental results reveal that all
the models consistently achieve their highest performance when provided with
the gold passage as context. While comparing the other context-driven
approaches for each model, Retrieval Augmented Generation (RAG)-based
approaches substantially outperform PDF document contexts, indicating that
neither model is well suited for long-context question-answering tasks. Our
analysis suggests that NEPA-focused regulatory reasoning tasks pose a
significant challenge for LLMs, particularly in terms of understanding the
complex semantics and effectively processing the lengthy regulatory documents.

</details>


### [69] [Multi-group Uncertainty Quantification for Long-form Text Generation](https://arxiv.org/pdf/2407.21057)
*Terrance Liu, Zhiwei Steven Wu*

Main category: cs.CL

TL;DR: The paper examines uncertainty quantification in LLM outputs, focusing on subgroups like demographic attributes, and shows that traditional methods fail for subgroups but group-conditional methods improve performance.


<details>
  <summary>Details</summary>
Motivation: To address whether uncertainty guarantees in LLM outputs hold for subgroups, not just the entire dataset.

Method: Uses calibration for individual claims and conformal prediction for entire outputs, tested on biography generation with demographic subgroups.

Result: Traditional methods work for the full dataset but fail for subgroups; group-conditional methods (multicalibration, multivalid conformal prediction) improve subgroup performance without losing overall guarantees.

Conclusion: The study sets a benchmark for multi-group uncertainty quantification in long-form text generation, highlighting the need for subgroup-aware methods.

Abstract: While past works have shown how uncertainty quantification can be applied to
large language model (LLM) outputs, the question of whether resulting
uncertainty guarantees still hold within sub-groupings of data remains open. In
our work, given some long-form text generated by an LLM, we study uncertainty
at both the level of individual claims contained within the output (via
calibration) and across the entire output itself (via conformal prediction).
Using biography generation as a testbed for this study, we derive a set of
(demographic) attributes (e.g., whether some text describes a man or woman) for
each generation to form such "subgroups" of data. We find that although
canonical methods for both types of uncertainty quantification perform well
when measuring across the entire dataset, such guarantees break down when
examining particular subgroups. Having established this issue, we invoke
group-conditional methods for uncertainty quantification -- multicalibration
and multivalid conformal prediction -- and find that across a variety of
approaches, additional subgroup information consistently improves calibration
and conformal prediction within subgroups (while crucially retaining guarantees
across the entire dataset). As the problems of calibration, conformal
prediction, and their multi-group counterparts have not been extensively
explored in the context of long-form text generation, we consider these results
to form a benchmark for this setting.

</details>


### [70] [SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language Models](https://arxiv.org/pdf/2408.08545)
*Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar*

Main category: cs.CL

TL;DR: SelectLLM is a novel algorithm for efficiently selecting the best subset of LLMs for a given query, improving performance and reducing latency compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Individual LLMs have limitations in generalization and performance on complex tasks due to biases, size constraints, and dataset quality. SelectLLM aims to harness diverse LLM capabilities to overcome these issues.

Method: SelectLLM uses a multi-label classifier and policy to select an optimal, query-aware subset of LLMs based on predictions and confidence scores.

Result: SelectLLM outperforms ensemble baselines and matches top-performing LLMs, reducing inference latency by 13% on GSM8K and 70% on MMLU.

Conclusion: SelectLLM effectively addresses LLM limitations, offering efficient and accurate query handling, with potential for further improvement toward an Oracle's theoretical upper bound.

Abstract: Large language models (LLMs) have been widely adopted due to their remarkable
performance across various applications, driving the accelerated development of
a large number of diverse models. However, these individual LLMs show
limitations in generalization and performance on complex tasks due to inherent
training biases, model size constraints, and the quality or diversity of
pre-training datasets. A promising direction is to efficiently harness the
diverse capabilities of LLMs to overcome these individual limitations. To
address these limitations, we introduce a novel LLM selection algorithm called
SelectLLM, which efficiently directs input queries to the most suitable subset
of LLMs from a large pool, ensuring that the selected models collectively
provide accurate responses. SelectLLM employs a multi-label classifier and
policy based on the classifier's predictions and confidence scores in selecting
an optimal, query-aware, and lightweight subset of LLMs. Our findings indicate
that the proposed model outperforms existing ensemble-based baselines and
achieves competitive performance with similarly sized top-performing LLMs while
maintaining efficiency. Specifically, it achieves a huge reduction in inference
latency on two challenging reasoning benchmarks: 13\% on GSM8K and 70\% on
MMLU, compared to the top-performing baseline. Also, we establish a theoretical
upper bound by an Oracle with LLMs and perform an in-depth linguistic analysis
to understand the performance gap between the Oracle and SelectLLM.

</details>


### [71] [The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation](https://arxiv.org/pdf/2408.08688)
*Samee Arif, Sualeha Farid, Abdul Hameed Azeemi, Awais Athar, Agha Ali Raza*

Main category: cs.CL

TL;DR: The paper introduces a method for generating synthetic Preference Optimization datasets using multi-model workflows, automating response evaluation and generation with LLMs, and identifies optimal configurations for both modules.


<details>
  <summary>Details</summary>
Motivation: To automate and enhance the dataset generation process for Preference Optimization, reducing reliance on human annotators by leveraging LLMs.

Method: Uses multi-model workflows: (1) response evaluation (automated ranking of LLM responses) and (2) response generation (LLM Feedback Loop). Evaluates LLMs as evaluators and compares configurations for generation.

Result: GPT-4o-as-a-Judge is most consistent for evaluation. The LLM Feedback Loop with Llama as generator and Gemma as reviewer achieves win rates of 71.8% and 73.8% over single-model setups.

Conclusion: The proposed pipeline effectively automates PO dataset generation, with identified optimal configurations improving performance over single-model approaches.

Abstract: This paper presents a novel methodology for generating synthetic Preference
Optimization (PO) datasets using multi-model workflows. We evaluate the
effectiveness and potential of these workflows in automating and enhancing the
dataset generation process. PO dataset generation requires two modules: (1)
$\textit{response evaluation}$, and (2) $\textit{response generation}$. In the
$\textit{response evaluation}$ module, the responses from Large Language Models
(LLMs) are evaluated and ranked - a task typically carried out by human
annotators that we automate using LLMs. We assess the response evaluation
module in a 2 step process. In step 1, we assess LLMs as evaluators using three
distinct prompting strategies. In step 2, we apply the winning prompting
strategy to compare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM
Debate. Our evaluation shows that GPT-4o-as-a-Judge is more consistent across
all datasets. For the $\textit{response generation}$ module, we use the
identified LLM evaluator configuration and compare different configurations of
the LLM Feedback Loop. We use the win rate to determine the best multi-model
configuration for generation. Experimenting with various configurations, we
find that the LLM Feedback Loop, with Llama as the generator and Gemma as the
reviewer, achieves a notable 71.8% and 73.8% win rate over single-model Llama
and Gemma, respectively. After identifying the best configurations for both
modules, we generate our PO datasets using the above pipeline.

</details>


### [72] [Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs](https://arxiv.org/pdf/2408.09742)
*Simon D Angus, Lachlan O'Neill*

Main category: cs.CL

TL;DR: A novel method called 'paired completion' uses LLM next-token probabilities to detect contrasting issue frames efficiently, outperforming traditional methods in cost and bias.


<details>
  <summary>Details</summary>
Motivation: Automated detection of issue framing is valuable for social science and policy but challenging due to subtle linguistic differences.

Method: Uses 'paired completion' with LLM next-token log probabilities to detect contrasting frames with minimal examples.

Result: Demonstrated as cost-efficient, low-bias, and scalable, especially in low-resource settings.

Conclusion: Paired completion offers a scalable and effective solution for analyzing issue framing in large text collections.

Abstract: Detecting issue framing in text - how different perspectives approach the
same topic - is valuable for social science and policy analysis, yet
challenging for automated methods due to subtle linguistic differences. We
introduce `paired completion', a novel approach using LLM next-token log
probabilities to detect contrasting frames using minimal examples. Through
extensive evaluation across synthetic datasets and a human-labeled corpus, we
demonstrate that paired completion is a cost-efficient, low-bias alternative to
both prompt-based and embedding-based methods, offering a scalable solution for
analyzing issue framing in large text collections, especially suited to
low-resource settings.

</details>


### [73] [Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling](https://arxiv.org/pdf/2410.01651)
*Xiang Hu, Zhihao Teng, Jun Zhao, Wei Wu, Kewei Tu*

Main category: cs.CL

TL;DR: Proposes Grouped Cross Attention (GCA), a novel attention mechanism for Transformers, enabling generalization to 1000x pre-training context length with constant computational cost.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of long-context handling in Transformers, which suffer from limited length generalization and high computational costs due to quadratic self-attention complexity.

Method: Splits input into chunks, retrieves top-k relevant past chunks for generation, and learns retrieval end-to-end to minimize auto-regressive loss. Uses fixed-size attention window for efficiency.

Result: Achieves near-perfect accuracy in passkey retrieval for 16M context lengths (1000x training length) with reduced computational/memory costs.

Conclusion: GCA effectively enables long-range information access in Transformers while maintaining efficiency, outperforming traditional methods.

Abstract: Despite the success of Transformers, handling long contexts remains
challenging due to the limited length generalization and quadratic complexity
of self-attention. Thus Transformers often require post-training with a larger
attention window, significantly increasing computational and memory costs. In
this paper, we propose a novel attention mechanism based on dynamic context,
Grouped Cross Attention (GCA), which can generalize to 1000 times the
pre-training context length while maintaining the ability to access distant
information with a constant attention window size. For a given input sequence,
we split it into chunks and use each chunk to retrieve top-k relevant past
chunks for subsequent text generation. Specifically, unlike most previous works
that use an off-the-shelf retriever, our key innovation allows the retriever to
learn how to retrieve past chunks that better minimize the auto-regressive loss
of subsequent tokens in an end-to-end manner. Such a mechanism accommodates
retrieved chunks with a fixed-size attention window to achieve long-range
information access, significantly reducing computational and memory costs
during training and inference. Experiments show that GCA-based models achieve
near-perfect accuracy in passkey retrieval for 16M context lengths, which is
1000 times the training length.

</details>


### [74] [Efficiently Identifying Watermarked Segments in Mixed-Source Texts](https://arxiv.org/pdf/2410.03600)
*Xuandong Zhao, Chenwen Liao, Yu-Xiang Wang, Lei Li*

Main category: cs.CL

TL;DR: The paper proposes two methods for detecting partial watermarks in mixed-source documents, outperforming baseline techniques in accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing watermark detection techniques focus on entire documents, neglecting the need to identify individual watermark segments in mixed-source texts.

Method: 1. A geometry cover detection framework for identifying watermark segments in long text. 2. An adaptive online learning algorithm to locate watermark segments precisely.

Result: High accuracy in detecting watermarks across three techniques (KGW-Watermark, Unigram-Watermark, Gumbel-Watermark), outperforming baselines.

Conclusion: The proposed framework is effective and adaptable for precise watermark detection, with potential applications in mitigating misuse of synthetic text.

Abstract: Text watermarks in large language models (LLMs) are increasingly used to
detect synthetic text, mitigating misuse cases like fake news and academic
dishonesty. While existing watermarking detection techniques primarily focus on
classifying entire documents as watermarked or not, they often neglect the
common scenario of identifying individual watermark segments within longer,
mixed-source documents. Drawing inspiration from plagiarism detection systems,
we propose two novel methods for partial watermark detection. First, we develop
a geometry cover detection framework aimed at determining whether there is a
watermark segment in long text. Second, we introduce an adaptive online
learning algorithm to pinpoint the precise location of watermark segments
within the text. Evaluated on three popular watermarking techniques
(KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves
high accuracy, significantly outperforming baseline methods. Moreover, our
framework is adaptable to other watermarking techniques, offering new insights
for precise watermark detection. Our code is publicly available at
https://github.com/XuandongZhao/llm-watermark-location

</details>


### [75] [Persistent Topological Features in Large Language Models](https://arxiv.org/pdf/2410.11042)
*Yuri Gardinazzi, Karthik Viswanathan, Giada Panerai, Alessio Ansuini, Alberto Cazzaniga, Matteo Biagetti*

Main category: cs.CL

TL;DR: The paper connects zigzag persistence, a topological data analysis tool, with practical algorithms to analyze decision-making in large language models, tracking topological feature evolution across layers.


<details>
  <summary>Details</summary>
Motivation: To understand the decision-making processes of large language models by leveraging topological data analysis for dynamic feature tracking.

Method: Introduces topological descriptors using zigzag persistence to measure the evolution of topological features (e.g., p-dimensional holes) across model layers.

Result: Demonstrates the framework's sensitivity to models and datasets, and applies it to layer pruning, achieving competitive results.

Conclusion: The framework provides a system-level perspective on model behavior, offering insights and practical applications like layer pruning.

Abstract: Understanding the decision-making processes of large language models is
critical given their widespread applications. To achieve this, we aim to
connect a formal mathematical framework -- zigzag persistence from topological
data analysis -- with practical and easily applicable algorithms. Zigzag
persistence is particularly effective for characterizing data as it dynamically
transforms across model layers. Within this framework, we introduce topological
descriptors that measure how topological features, $p$-dimensional holes,
persist and evolve throughout the layers. Unlike methods that assess each layer
individually and then aggregate the results, our approach directly tracks the
full evolutionary path of these features. This offers a statistical perspective
on how prompts are rearranged and their relative positions changed in the
representation space, providing insights into the system's operation as an
integrated whole. To demonstrate the expressivity and applicability of our
framework, we highlight how sensitive these descriptors are to different models
and a variety of datasets. As a showcase application to a downstream task, we
use zigzag persistence to establish a criterion for layer pruning, achieving
results comparable to state-of-the-art methods while preserving the
system-level perspective.

</details>


### [76] [On Many-Shot In-Context Learning for Long-Context Evaluation](https://arxiv.org/pdf/2411.07130)
*Kaijian Zou, Muhammad Khalifa, Lu Wang*

Main category: cs.CL

TL;DR: The paper evaluates long-context language models (LCLMs) using many-shot in-context learning (ICL), identifying tasks that benefit from more demonstrations and categorizing tasks into retrieval-based (SSL) and global-context (ASL) types. A new benchmark, MANYICLBENCH, is introduced to test LCLMs, revealing performance drops in ASL tasks beyond 16k tokens.


<details>
  <summary>Details</summary>
Motivation: To understand how LCLMs perform with long-context tasks and identify which ICL tasks benefit from additional demonstrations, while distinguishing between retrieval-based and global-context tasks.

Method: Evaluated LCLMs using many-shot ICL, categorized tasks into SSL and ASL, and introduced MANYICLBENCH to benchmark 12 LCLMs.

Result: Classification and summarization tasks improve with more demonstrations, while translation and reasoning tasks do not. Models perform well in SSL tasks up to 64k tokens but drop significantly in ASL tasks beyond 16k tokens.

Conclusion: Many-shot ICL is effective for evaluating LCLMs, but performance varies by task type, highlighting limitations in global-context understanding for current models.

Abstract: Many-shot in-context learning (ICL) has emerged as a unique setup to both
utilize and test the ability of large language models to handle long context.
This paper delves into long-context language model (LCLM) evaluation through
many-shot ICL. We first ask: what types of ICL tasks benefit from additional
demonstrations, and how effective are they in evaluating LCLMs? We find that
classification and summarization tasks show performance improvements with
additional demonstrations, while translation and reasoning tasks do not exhibit
clear trends. Next, we investigate the extent to which different tasks
necessitate retrieval versus global context understanding. We develop metrics
to categorize ICL tasks into two groups: (i) similar-sample learning (SSL):
tasks where retrieval of the most similar examples is sufficient for good
performance, and (ii) all-sample learning (ASL): tasks that necessitate a
deeper comprehension of all examples in the prompt. Lastly, we introduce a new
many-shot ICL benchmark, MANYICLBENCH, to characterize model's ability on both
fronts and benchmark 12 LCLMs using MANYICLBENCH. We find that while
state-of-the-art models demonstrate good performance up to 64k tokens in SSL
tasks, many models experience significant performance drops at only 16k tokens
in ASL tasks.

</details>


### [77] [Squeezed Attention: Accelerating Long Context Length LLM Inference](https://arxiv.org/pdf/2411.09688)
*Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, Sebastian Zhao, June Paik, Michael W. Mahoney, Kurt Keutzer, Amir Gholami*

Main category: cs.CL

TL;DR: Squeezed Attention accelerates LLM applications by clustering fixed context keys offline, reducing computational costs during inference without significant accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Long input contexts in LLM applications increase inference costs linearly, but much of the context is fixed, offering optimization opportunities.

Method: Uses K-means clustering offline to group fixed context keys by semantic similarity, then compares user queries to centroids to compute attention only on relevant keys. A hierarchical version further reduces complexity.

Result: Achieves 3.1× KV budget reduction with no accuracy loss and up to 8× reduction with minimal accuracy impact. Kernels provide over 4× speedups.

Conclusion: Squeezed Attention efficiently reduces computational and bandwidth costs for long-context LLM applications while maintaining accuracy.

Abstract: Emerging Large Language Model (LLM) applications require long input context
in order to perform complex tasks like document analysis and code generation.
For these long context length applications, the length of the input prompt
poses a significant challenge in terms of inference efficiency since the
inference costs increase linearly with sequence length. However, for many of
these applications, much of the context in the prompt is fixed across different
user inputs, thereby providing the opportunity to perform offline optimizations
in order to process user inputs quickly, as they are received. We propose
Squeezed Attention to accelerate LLM applications where a large portion of the
input context is fixed. We first leverage K-means clustering offline to group
the keys for the fixed context based on semantic similarity and represent each
cluster with a single centroid value. During inference, we compare query tokens
from the user input with the centroids to predict which keys from the fixed
context are semantically relevant, and then compute exact attention using only
the important keys, thereby reducing bandwidth and computational costs. We also
present a hierarchical version of our algorithm which can reduce the complexity
of attention from linear to logarithmic with respect to the fixed context
length. We evaluate our method on long-context benchmarks including LongBench,
where it achieves a 3.1$\times$ reduction in KV budget with no noticeable
accuracy loss and up to an 8$\times$ reduction with only a 0.5 point accuracy
gap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.
Futhermore, we implement kernels for centroid comparison and sparse
FlashAttention with important keys, achieving more than 4$\times$ speedups
during both the prefill and generation phases for long-context inference. Our
code is available at https://github.com/SqueezeAILab/SqueezedAttention.

</details>


### [78] [Prompt-based Depth Pruning of Large Language Models](https://arxiv.org/pdf/2502.04348)
*Juyun Wee, Minjae Park, Jaeho Lee*

Main category: cs.CL

TL;DR: PuDDing is a dynamic depth pruning algorithm that removes less important transformer blocks based on input prompts, improving inference speed and task performance over static pruning.


<details>
  <summary>Details</summary>
Motivation: The importance of transformer blocks varies by task, suggesting static pruning is suboptimal. Dynamic pruning can better adapt to task-specific needs.

Method: PuDDing trains a lightweight router to predict which blocks to omit, using a data-driven option set for dynamic pruning.

Result: PuDDing accelerates inference and outperforms static pruning on commonsense reasoning benchmarks.

Conclusion: Dynamic depth pruning, as implemented by PuDDing, is more effective than static pruning for task-specific performance and efficiency.

Abstract: Depth pruning aims to reduce the inference cost of a large language model
without any hardware-specific complications, by simply removing several less
important transformer blocks. However, our empirical findings suggest that the
importance of a transformer block may be highly task-dependent -- a block that
is crucial for a task can be removed without degrading the accuracy on another
task. Based on this observation, we develop a dynamic depth pruning algorithm,
coined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which
blocks to omit from the model based on the input prompt. PuDDing operates by
training a lightweight router to predict the best omission set among a set of
options, where this option set has also been constructed in a data-driven
manner. Empirical results on commonsense reasoning benchmarks demonstrate that
PuDDing effectively accelerates the inference language models, and achieves
better on-task performance than static depth pruning baselines.

</details>


### [79] [Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges](https://arxiv.org/pdf/2502.12378)
*Bolei Ma, Yuting Li, Wei Zhou, Ziwei Gong, Yang Janet Liu, Katja Jasinskaja, Annemarie Friedrich, Julia Hirschberg, Frauke Kreuter, Barbara Plank*

Main category: cs.CL

TL;DR: A survey reviewing resources for evaluating pragmatic capabilities in NLP, analyzing trends, challenges, and gaps to improve context-aware models.


<details>
  <summary>Details</summary>
Motivation: Understanding pragmatics is key for NLP systems, but evaluating models' ability to handle phenomena like implicatures and references remains difficult.

Method: Comprehensive review of datasets and evaluation methods for pragmatic phenomena, analyzing task designs, data collection, and relevance to real-world applications.

Result: Identifies trends, challenges, and gaps in benchmarks, highlighting the need for more comprehensive evaluation tools.

Conclusion: The survey clarifies pragmatic evaluation in NLP and guides the development of better benchmarks for nuanced, context-aware models.

Abstract: Understanding pragmatics-the use of language in context-is crucial for
developing NLP systems capable of interpreting nuanced language use. Despite
recent advances in language technologies, including large language models,
evaluating their ability to handle pragmatic phenomena such as implicatures and
references remains challenging. To advance pragmatic abilities in models, it is
essential to understand current evaluation trends and identify existing
limitations. In this survey, we provide a comprehensive review of resources
designed for evaluating pragmatic capabilities in NLP, categorizing datasets by
the pragmatic phenomena they address. We analyze task designs, data collection
methods, evaluation approaches, and their relevance to real-world applications.
By examining these resources in the context of modern language models, we
highlight emerging trends, challenges, and gaps in existing benchmarks. Our
survey aims to clarify the landscape of pragmatic evaluation and guide the
development of more comprehensive and targeted benchmarks, ultimately
contributing to more nuanced and context-aware NLP models.

</details>


### [80] [BeamLoRA: Beam-Constraint Low-Rank Adaptation](https://arxiv.org/pdf/2502.13604)
*Naibin Gu, Zhenyu Zhang, Xiyu Liu, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang*

Main category: cs.CL

TL;DR: BeamLoRA improves LoRA by dynamically optimizing rank sub-solutions during fine-tuning, enhancing performance without increasing rank.


<details>
  <summary>Details</summary>
Motivation: LoRA's efficiency is high, but its accuracy can be improved by addressing dynamic rank importance during fine-tuning.

Method: BeamLoRA treats LoRA ranks as potential sub-solutions, dynamically pruning underperforming ones and expanding promising ones.

Result: BeamLoRA outperforms baselines across 12 datasets and 3 base models in math, code, and commonsense reasoning tasks.

Conclusion: BeamLoRA effectively enhances LoRA's performance by dynamically managing rank sub-solutions, offering a better balance of efficiency and accuracy.

Abstract: Due to the demand for efficient fine-tuning of large language models,
Low-Rank Adaptation (LoRA) has been widely adopted as one of the most effective
parameter-efficient fine-tuning methods. Nevertheless, while LoRA improves
efficiency, there remains room for improvement in accuracy. Herein, we adopt a
novel perspective to assess the characteristics of LoRA ranks. The results
reveal that different ranks within the LoRA modules not only exhibit varying
levels of importance but also evolve dynamically throughout the fine-tuning
process, which may limit the performance of LoRA. Based on these findings, we
propose BeamLoRA, which conceptualizes each LoRA module as a beam where each
rank naturally corresponds to a potential sub-solution, and the fine-tuning
process becomes a search for the optimal sub-solution combination. BeamLoRA
dynamically eliminates underperforming sub-solutions while expanding the
parameter space for promising ones, enhancing performance with a fixed rank.
Extensive experiments across three base models and 12 datasets spanning math
reasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA
consistently enhances the performance of LoRA, surpassing the other baseline
methods.

</details>


### [81] [Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps](https://arxiv.org/pdf/2502.14829)
*Martin Tutek, Fateme Hashemi Chaleshtori, Ana Marasović, Yonatan Belinkov*

Main category: cs.CL

TL;DR: The paper introduces FUR, a framework to measure the faithfulness of reasoning steps (CoT) in language models by unlearning key steps and observing prediction changes.


<details>
  <summary>Details</summary>
Motivation: To determine if the reasoning steps verbalized in CoT are faithful to the model's parametric beliefs, addressing a gap in current CoT research.

Method: Proposes FUR, which unlearns reasoning steps from model parameters and measures faithfulness by the impact on predictions. Tests on four LMs and five MCQA datasets.

Result: FUR often precisely alters model predictions by unlearning key steps, indicating parametric faithfulness of CoTs. Post-unlearning CoTs support different answers.

Conclusion: FUR effectively measures CoT faithfulness, revealing deeper effects of unlearning on model reasoning.

Abstract: When prompted to think step-by-step, language models (LMs) produce a chain of
thought (CoT), a sequence of reasoning steps that the model supposedly used to
produce its prediction. Despite much work on CoT prompting, it is unclear if
reasoning verbalized in a CoT is faithful to the models' parametric beliefs. We
introduce a framework for measuring parametric faithfulness of generated
reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an
instance of this framework. FUR erases information contained in reasoning steps
from model parameters, and measures faithfulness as the resulting effect on the
model's prediction. Our experiments with four LMs and five multi-hop
multi-choice question answering (MCQA) datasets show that FUR is frequently
able to precisely change the underlying models' prediction for a given instance
by unlearning key steps, indicating when a CoT is parametrically faithful.
Further analysis shows that CoTs generated by models post-unlearning support
different answers, hinting at a deeper effect of unlearning.

</details>


### [82] [Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models](https://arxiv.org/pdf/2502.15010)
*Mark Russinovich, Ahmed Salem*

Main category: cs.CL

TL;DR: Obliviate is a lightweight post-training method to prevent verbatim reproduction of copyrighted text in language models, maintaining utility and fluency.


<details>
  <summary>Details</summary>
Motivation: Address the need for fine-grained control over language models to avoid reproducing copyrighted text without sacrificing model performance.

Method: Identifies memorized passages and adjusts output distributions via a Kullback-Leibler divergence penalty, while enforcing consistency loss on non-target tokens.

Result: Reduces verbatim recall significantly (e.g., from hundreds to fewer than 12 words) with minimal impact on downstream tasks (≤1% accuracy drop).

Conclusion: Obliviate is a practical, high-fidelity solution for copyright compliance in deployed LLMs.

Abstract: Recent copyright agreements between AI companies and content creators
underscore the need for fine-grained control over language models' ability to
reproduce copyrighted text. Existing defenses-ranging from aggressive
unlearning to simplistic output filters-either sacrifice model utility or
inadequately address verbatim leakage. We introduce Obliviate, a lightweight
post-training method that surgically suppresses exact reproduction of specified
sequences while preserving semantic understanding. Obliviate first identifies
memorized passages and then, for each target token, minimally adjusts the
model's output distribution via a Kullback-Leibler divergence penalty to drive
down the probability of exact reproduction. Simultaneously, we enforce a
consistency loss on non-target tokens to retain the model's fluency and task
performance. We evaluate Obliviate on four popular 6-8B-parameter models
(LLaMA-3.1, LLaMA-3.1-Instruct, Qwen-2.5, and Yi-1.5) using synthetic
memorization benchmarks and organic copyrighted excerpts (e.g., Moby Dick,
Frankenstein, Alice in Wonderland and Les Miserables). Across all settings,
Obliviate reduces verbatim recall by two orders of magnitude (e.g., from
hundreds of words to fewer than 12) while degrading downstream accuracy by at
most 1% on HellaSwag, MMLU, TruthfulQA, and Winogrande. Furthermore, we
benchmark Obliviate aganist different unlearning and copyright techniques using
the MUSE and CoTaEval benchmarks. These results position Obliviate as a
practical, high-fidelity solution for copyright compliance in deployed LLMs.

</details>


### [83] [Mind the Style Gap: Meta-Evaluation of Style and Attribute Transfer Metrics](https://arxiv.org/pdf/2502.15022)
*Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent*

Main category: cs.CL

TL;DR: The paper evaluates metrics for content preservation in style transfer tasks, revealing misleading conclusions from existing datasets and proposing a new test set and style-aware method for better alignment with human judgments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately measuring content preservation in style transfer tasks, as existing metrics and datasets may lead to misleading evaluations.

Method: Conduct a meta-evaluation of metrics, introduce a new test set for content preservation, and propose a style-aware method using small language models.

Result: Existing metrics show high but misleading correlation with human judgments; the new test set and style-aware method improve evaluation accuracy.

Conclusion: Style-aware metrics are essential for content preservation in style transfer, and the proposed method aligns better with human judgments.

Abstract: Large language models (LLMs) make it easy to rewrite a text in any style --
e.g. to make it more polite, persuasive, or more positive -- but evaluation
thereof is not straightforward. A challenge lies in measuring content
preservation: that content not attributable to style change is retained. This
paper presents a large meta-evaluation of metrics for evaluating style and
attribute transfer, focusing on content preservation. We find that
meta-evaluation studies on existing datasets lead to misleading conclusions
about the suitability of metrics for content preservation. Widely used metrics
show a high correlation with human judgments despite being deemed unsuitable
for the task -- because they do not abstract from style changes when evaluating
content preservation. We show that the overly high correlations with human
judgment stem from the nature of the test data. To address this issue, we
introduce a new, challenging test set specifically designed for evaluating
content preservation metrics for style transfer. Using this dataset, we
demonstrate that suitable metrics for content preservation for style transfer
indeed are style-aware. To support efficient evaluation, we propose a new
style-aware method that utilises small language models, obtaining a higher
alignment with human judgements than prompting a model of a similar size as an
autorater.

</details>


### [84] [The Esethu Framework: Reimagining Sustainable Dataset Governance and Curation for Low-Resource Languages](https://arxiv.org/pdf/2502.15916)
*Jenalea Rajab, Anuoluwapo Aremu, Everlyn Asiko Chimoto, Dale Dunbar, Graham Morrissey, Fadel Thior, Luandrie Potgieter, Jessico Ojo, Atnafu Lambebo Tonja, Maushami Chetty, Wilhelmina NdapewaOnyothi Nekoto, Pelonomi Moiloa, Jade Abbott, Vukosi Marivate, Benjamin Rosman*

Main category: cs.CL

TL;DR: The paper introduces the Esethu Framework and License for sustainable, community-centric data curation, demonstrated by the Vuk'uzenzele isiXhosa Speech Dataset (ViXSD), which supports ASR for African languages while protecting data creators.


<details>
  <summary>Details</summary>
Motivation: To empower local communities and ensure equitable benefit-sharing from linguistic resources, addressing gaps in ASR for African languages.

Method: Developed the Esethu Framework and License, created the ViXSD dataset with read speech and metadata, and conducted ASR experiments.

Result: ViXSD successfully validated the framework's usability for ASR in isiXhosa, demonstrating community-driven benefits.

Conclusion: The Esethu Framework and ViXSD provide a sustainable, equitable model for linguistic resource curation and ASR development in underrepresented languages.

Abstract: This paper presents the Esethu Framework, a sustainable data curation
framework specifically designed to empower local communities and ensure
equitable benefit-sharing from their linguistic resource. This framework is
supported by the Esethu license, a novel community-centric data license. As a
proof of concept, we introduce the Vuk'uzenzele isiXhosa Speech Dataset
(ViXSD), an open-source corpus developed under the Esethu Framework and
License. The dataset, containing read speech from native isiXhosa speakers
enriched with demographic and linguistic metadata, demonstrates how
community-driven licensing and curation principles can bridge resource gaps in
automatic speech recognition (ASR) for African languages while safeguarding the
interests of data creators. We describe the framework guiding dataset
development, outline the Esethu license provisions, present the methodology for
ViXSD, and present ASR experiments validating ViXSD's usability in building and
refining voice-driven applications for isiXhosa.

</details>


### [85] [Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://arxiv.org/pdf/2502.19148)
*Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, Yaodong Yang*

Main category: cs.CL

TL;DR: Amulet is a training-free framework for real-time adaptation of LLMs to personalized user preferences using simple prompts, achieving significant performance improvements with negligible computational overhead.


<details>
  <summary>Details</summary>
Motivation: User preferences are diverse and dynamic, making static datasets insufficient for aligning LLMs with actual user needs during practical use.

Method: Amulet formulates token decoding as an online learning problem, guided by user prompts, and provides a closed-form solution to minimize computational cost.

Result: Experiments show Amulet improves performance across various LLMs, datasets, and user preferences while maintaining computational efficiency.

Conclusion: Amulet effectively addresses the challenge of real-time preference adaptation in LLMs without retraining, offering a practical solution for personalized alignment.

Abstract: How to align large language models (LLMs) with user preferences from a static
general dataset has been frequently studied. However, user preferences are
usually personalized, changing, and diverse regarding culture, values, or time.
This leads to the problem that the actual user preferences often do not
coincide with those trained by the model developers in the practical use of
LLMs. Since we cannot collect enough data and retrain for every demand,
researching efficient real-time preference adaptation methods based on the
backbone LLMs during test time is important. To this end, we introduce Amulet,
a novel, training-free framework that formulates the decoding process of every
token as a separate online learning problem with the guidance of simple
user-provided prompts, thus enabling real-time optimization to satisfy users'
personalized preferences. To reduce the computational cost brought by this
optimization process for each token, we additionally provide a closed-form
solution for each iteration step of the optimization process, thereby reducing
the computational time cost to a negligible level. The detailed experimental
results demonstrate that Amulet can achieve significant performance
improvements in rich settings with combinations of different LLMs, datasets,
and user preferences, while maintaining acceptable computational efficiency.

</details>


### [86] [Large Language Models for Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/pdf/2503.02737)
*Ivan Vykopal, Matúš Pikuliak, Simon Ostermann, Tatiana Anikina, Michal Gregor, Marián Šimko*

Main category: cs.CL

TL;DR: The paper evaluates LLMs for multilingual fact-checked claim detection, showing strong performance in high-resource languages but struggles in low-resource ones, with translation to English aiding the latter.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of duplicated fact-checking efforts across languages by leveraging LLMs for detecting previously fact-checked claims.

Method: Assessed seven LLMs across 20 languages in monolingual and cross-lingual settings, including translation to English for low-resource languages.

Result: LLMs perform well for high-resource languages but struggle with low-resource ones; translating to English improves performance for the latter.

Conclusion: LLMs show promise for multilingual fact-checked claim detection, especially with translation support, laying groundwork for future research.

Abstract: In our era of widespread false information, human fact-checkers often face
the challenge of duplicating efforts when verifying claims that may have
already been addressed in other countries or languages. As false information
transcends linguistic boundaries, the ability to automatically detect
previously fact-checked claims across languages has become an increasingly
important task. This paper presents the first comprehensive evaluation of large
language models (LLMs) for multilingual previously fact-checked claim
detection. We assess seven LLMs across 20 languages in both monolingual and
cross-lingual settings. Our results show that while LLMs perform well for
high-resource languages, they struggle with low-resource languages. Moreover,
translating original texts into English proved to be beneficial for
low-resource languages. These findings highlight the potential of LLMs for
multilingual previously fact-checked claim detection and provide a foundation
for further research on this promising application of LLMs.

</details>


### [87] [Improving LLM Safety Alignment with Dual-Objective Optimization](https://arxiv.org/pdf/2503.03710)
*Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song*

Main category: cs.CL

TL;DR: The paper identifies vulnerabilities in Direct Preference Optimization (DPO) for LLM safety alignment, proposes an improved method with robust refusal training and harmful knowledge unlearning, and enhances robustness against jailbreak attacks.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment techniques for LLMs, like DPO, are vulnerable to jailbreak attacks, highlighting the need for more robust methods.

Method: The authors analyze DPO's limitations, propose disentangling its objectives into robust refusal training and harmful knowledge unlearning, and introduce a token-level weighting mechanism for refusal learning.

Result: The improved method significantly boosts LLM robustness against various jailbreak attacks, including prefilling, suffix, and multi-turn attacks.

Conclusion: The study links robustness to token distribution shifts and internal representations, providing insights for future LLM safety alignment research.

Abstract: Existing training-time safety alignment techniques for large language models
(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization
(DPO), a widely deployed alignment method, exhibits limitations in both
experimental and theoretical contexts as its loss function proves suboptimal
for refusal learning. Through gradient-based analysis, we identify these
shortcomings and propose an improved safety alignment that disentangles DPO
objectives into two components: (1) robust refusal training, which encourages
refusal even when partial unsafe generations are produced, and (2) targeted
unlearning of harmful knowledge. This approach significantly increases LLM
robustness against a wide range of jailbreak attacks, including prefilling,
suffix, and multi-turn attacks across both in-distribution and
out-of-distribution scenarios. Furthermore, we introduce a method to emphasize
critical refusal tokens by incorporating a reward-based token-level weighting
mechanism for refusal learning, which further improves the robustness against
adversarial exploits. Our research also suggests that robustness to jailbreak
attacks is correlated with token distribution shifts in the training process
and internal representations of refusal and harmful tokens, offering valuable
directions for future research in LLM safety alignment. The code is available
at https://github.com/wicai24/DOOR-Alignment

</details>


### [88] [Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations](https://arxiv.org/pdf/2503.06987)
*Jiho Jin, Woosung Kang, Junho Myung, Alice Oh*

Main category: cs.CL

TL;DR: Proposes BBG, a benchmark for evaluating social bias in long-form LLM generation, comparing results with BBQ and finding inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Existing bias evaluation methods fail to assess bias in long-form generation, necessitating a new benchmark.

Method: Adapts BBQ into BBG, using story prompts in English and Korean to measure bias in LLM continuations.

Result: Shows inconsistent results between BBG (long-form) and BBQ (multiple-choice) evaluations across ten LLMs.

Conclusion: Highlights the need for tailored bias evaluation methods for long-form generation, as BBQ may not suffice.

Abstract: Measuring social bias in large language models (LLMs) is crucial, but
existing bias evaluation methods struggle to assess bias in long-form
generation. We propose a Bias Benchmark for Generation (BBG), an adaptation of
the Bias Benchmark for QA (BBQ), designed to evaluate social bias in long-form
generation by having LLMs generate continuations of story prompts. Building our
benchmark in English and Korean, we measure the probability of neutral and
biased generations across ten LLMs. We also compare our long-form story
generation evaluation results with multiple-choice BBQ evaluation, showing that
the two approaches produce inconsistent results.

</details>


### [89] [Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges](https://arxiv.org/pdf/2503.08292)
*Xiaoxiao Liu, Qingying Xiao, Junying Chen, Xiangyi Feng, Xiangbo Wu, Bairui Zhang, Xiang Wan, Jian Chang, Guangjun Yu, Yan Hu, Benyou Wang*

Main category: cs.CL

TL;DR: The paper evaluates LLMs in outpatient referral tasks, proposing a static and dynamic evaluation framework, finding LLMs limited but promising in interactive dialogues.


<details>
  <summary>Details</summary>
Motivation: There's a lack of standardized evaluation criteria for LLMs in healthcare referral tasks, especially in dynamic scenarios.

Method: A comprehensive evaluation framework with static (predefined referrals) and dynamic (iterative dialogues) tasks is proposed.

Result: LLMs show limited advantages over BERT-like models but excel in asking effective questions during dialogues.

Conclusion: LLMs have potential in interactive healthcare tasks but require further refinement and evaluation.

Abstract: Large language models (LLMs) are increasingly applied to outpatient referral
tasks across healthcare systems. However, there is a lack of standardized
evaluation criteria to assess their effectiveness, particularly in dynamic,
interactive scenarios. In this study, we systematically examine the
capabilities and limitations of LLMs in managing tasks within Intelligent
Outpatient Referral (IOR) systems and propose a comprehensive evaluation
framework specifically designed for such systems. This framework comprises two
core tasks: static evaluation, which focuses on evaluating the ability of
predefined outpatient referrals, and dynamic evaluation, which evaluates
capabilities of refining outpatient referral recommendations through iterative
dialogues. Our findings suggest that LLMs offer limited advantages over
BERT-like models, but show promise in asking effective questions during
interactive dialogues.

</details>


### [90] [Computation Mechanism Behind LLM Position Generalization](https://arxiv.org/pdf/2503.13305)
*Chi Han, Heng Ji*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) generalize positional information in text, revealing computational mechanisms behind their flexibility.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs computationally process positional relevance and generalize to perturbed or longer texts.

Method: Analyzes LLMs' self-attention mechanisms, identifying patterns in attention logits and intermediate features.

Result: LLMs learn a disentanglement of attention logits with a 0.959 linear correlation to positional relevance and semantic importance. A prevalent pattern in intermediate features enables this effect.

Conclusion: The work links position generalization to LLMs' internal mechanisms, providing computational explanations and criteria for their flexibility.

Abstract: Most written natural languages are composed of sequences of words and
sentences. Similar to humans, large language models (LLMs) exhibit flexibility
in handling textual positions - a phenomenon we term position generalization.
They can understand texts with position perturbations and generalize to longer
texts than those encountered during training with the latest techniques. These
phenomena suggest that LLMs handle positions tolerantly, but how LLMs
computationally process positional relevance remains largely unexplored. This
work connects the linguistic phenomenon with LLMs' computational mechanisms. We
show how LLMs enforce certain computational mechanisms for the aforementioned
tolerance in position perturbations. Despite the complex design of the
self-attention mechanism, this work reveals that LLMs learn a counterintuitive
disentanglement of attention logits. Their values show a 0.959 linear
correlation with an approximation of the arithmetic sum of positional relevance
and semantic importance. Furthermore, we identify a prevalent pattern in
intermediate features, which we prove theoretically enables this effect. The
pattern, which is different from how randomly initialized parameters would
behave, suggests that it is a learned behavior rather than a natural result of
the model architecture. Based on these findings, we provide computational
explanations and criteria for LLMs' position flexibilities. This work takes a
pioneering step in linking position generalization with modern LLMs' internal
mechanisms.

</details>


### [91] [PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play](https://arxiv.org/pdf/2503.14432)
*Wei Fang, Yang Zhang, Kaizhi Qian, James Glass, Yada Zhu*

Main category: cs.CL

TL;DR: PLAY2PROMPT automates tool exploration for LLMs, improving zero-shot performance without labeled data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of zero-shot tool usage with minimal or noisy documentation in LLMs.

Method: An automated framework that iteratively explores tool behaviors to refine documentation and generate usage examples.

Result: Significant improvement in zero-shot tool performance across open and closed models.

Conclusion: PLAY2PROMPT offers a scalable solution for domain-specific tool integration in LLMs.

Abstract: Large language models (LLMs) are increasingly integrated with specialized
external tools, yet many tasks demand zero-shot tool usage with minimal or
noisy documentation. Existing solutions rely on manual rewriting or labeled
data for validation, making them inapplicable in true zero-shot settings. To
address these challenges, we propose PLAY2PROMPT, an automated framework that
systematically "plays" with each tool to explore its input-output behaviors.
Through this iterative trial-and-error process, PLAY2PROMPT refines tool
documentation and generates usage examples without any labeled data. These
examples not only guide LLM inference but also serve as validation to further
enhance tool utilization. Extensive experiments on real-world tasks demonstrate
that PLAY2PROMPT significantly improves zero-shot tool performance across both
open and closed models, offering a scalable and effective solution for
domain-specific tool integration.

</details>


### [92] [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/pdf/2503.23512)
*Qiang Yi, Yangfan He, Jianhui Wang, Xinyuan Song, Shiyao Qian, Xinhang Yuan, Li Sun, Yi Xin, Jingqun Tang, Keqin Li, Kuan Lu, Menghao Huo, Jiaqi Chen, Tianyu Shi*

Main category: cs.CL

TL;DR: SCORE enhances coherence in AI-generated stories by tracking key items and using RAG to resolve inconsistencies, outperforming baseline GPT models.


<details>
  <summary>Details</summary>
Motivation: Maintaining coherence and emotional depth in AI-generated narratives is challenging, prompting the need for a framework like SCORE.

Method: SCORE employs Retrieval-Augmented Generation (RAG) with TF-IDF and cosine similarity to track item statuses and generate summaries for coherence.

Result: SCORE significantly improves narrative consistency and stability compared to baseline GPT models.

Conclusion: SCORE provides a robust method for refining AI-generated stories, enhancing coherence and structure.

Abstract: Large Language Models (LLMs) can generate creative and engaging narratives
from user-specified input, but maintaining coherence and emotional depth
throughout these AI-generated stories remains a challenge. In this work, we
propose SCORE, a framework for Story Coherence and Retrieval Enhancement,
designed to detect and resolve narrative inconsistencies. By tracking key item
statuses and generating episode summaries, SCORE uses a Retrieval-Augmented
Generation (RAG) approach, incorporating TF-IDF and cosine similarity to
identify related episodes and enhance the overall story structure. Results from
testing multiple LLM-generated stories demonstrate that SCORE significantly
improves the consistency and stability of narrative coherence compared to
baseline GPT models, providing a more robust method for evaluating and refining
AI-generated narratives.

</details>


### [93] [IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling](https://arxiv.org/pdf/2504.03036)
*Zébulon Goriely, Paula Buttery*

Main category: cs.CL

TL;DR: The paper introduces G2P+, a tool for consistent phonemic representation, and IPA CHILDES, a multilingual phonemic dataset of child-directed speech. It addresses inconsistencies in prior tools and demonstrates the dataset's utility for phonological research.


<details>
  <summary>Details</summary>
Motivation: Existing grapheme-to-phoneme tools produce inconsistent phonemic vocabularies, and phonemic datasets lack multilingual coverage and child-directed speech.

Method: G2P+ leverages Phoible's phonemic inventories for consistency. IPA CHILDES is created by augmenting CHILDES with phonemic transcriptions.

Result: The dataset enables training phoneme language models, revealing that phoneme distributions can learn major class and place features cross-lingually.

Conclusion: IPA CHILDES fills gaps in phonemic datasets and proves useful for phonological research, showcasing the effectiveness of G2P+.

Abstract: In this paper, we introduce two resources: (i) G2P+, a tool for converting
orthographic datasets to a consistent phonemic representation; and (ii) IPA
CHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior
tools for grapheme-to-phoneme conversion result in phonemic vocabularies that
are inconsistent with established phonemic inventories, an issue which G2P+
addresses by leveraging the inventories in the Phoible database. Using this
tool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES.
This new resource fills several gaps in existing phonemic datasets, which often
lack multilingual coverage, spontaneous speech, and a focus on child-directed
language. We demonstrate the utility of this dataset for phonological research
by training phoneme language models on 11 languages and probing them for
distinctive features, finding that the distributional properties of phonemes
are sufficient to learn major class and place features cross-lingually.

</details>


### [94] [BabyLM's First Words: Word Segmentation as a Phonological Probing Task](https://arxiv.org/pdf/2504.03338)
*Zébulon Goriely, Paula Buttery*

Main category: cs.CL

TL;DR: The paper explores using word segmentation as a phonological probing task to study phoneme-based language models across 31 languages, revealing insights into statistical learning and subword tokenizer training.


<details>
  <summary>Details</summary>
Motivation: Phonological analysis with LLMs is challenging due to limited benchmarks and unsuitable input representations. This work aims to bridge this gap by leveraging word segmentation.

Method: Unsupervised methods extract word boundaries from phoneme-based models trained on child-directed speech, using prediction-error peaks. Linear probes identify implicit boundary tracking.

Result: Models implicitly track word boundaries, even without explicit training, supporting statistical learning theories. Cross-lingual analysis validates the approach.

Conclusion: The work advances phonological analysis with LLMs, supports statistical learning theories, and suggests improvements for subword tokenizer training.

Abstract: Language models provide a key framework for studying linguistic theories
based on prediction, but phonological analysis using large language models
(LLMs) is difficult; there are few phonological benchmarks beyond English and
the standard input representation used in LLMs (subwords of graphemes) is not
suitable for analyzing the representation of phonemes. In this work, we
demonstrate how word segmentation can be used as a phonological probing task,
allowing us to study the representations learned by phoneme-based language
models trained on child-directed speech across 31 languages. Following
computational models of word segmentation, we present unsupervised methods for
extracting word boundaries from a trained model using the observation that
prediction-error peaks at the start of words. We also use linear probes to
identify that these models implicitly track word boundaries, even when they do
not appear in training. This cross-lingual work corroborates statistical
learning theories of acquisition and empirically motivates new methods for
training subword tokenizers.

</details>


### [95] [Building UD Cairo for Old English in the Classroom](https://arxiv.org/pdf/2504.18718)
*Lauren Levine, Junghyun Min, Amir Zeldes*

Main category: cs.CL

TL;DR: A sample Old English treebank was created using UD Cairo sentences, combining LLM prompting and authentic data searches. Beginner annotators produced usable results, and parsing experiments showed improvements with annotated features.


<details>
  <summary>Details</summary>
Motivation: To develop a sample treebank for Old English using classroom-collected data and evaluate the effectiveness of beginner annotators and LLM outputs.

Method: Combined LLM prompting and authentic Old English data searches for sentence collection. Multiple beginner annotators compared and adjudicated annotations. Parsing experiments used Modern English training data.

Result: LLM outputs lacked authentic syntax but improved with post-editing. Beginner annotators collectively produced good results. Parsing performance on Old English was poor but improved with annotated features.

Conclusion: Beginner annotators and LLMs can contribute to treebank creation with proper guidance. Annotated features enhance parsing performance for Old English.

Abstract: In this paper we present a sample treebank for Old English based on the UD
Cairo sentences, collected and annotated as part of a classroom curriculum in
Historical Linguistics. To collect the data, a sample of 20 sentences
illustrating a range of syntactic constructions in the world's languages, we
employ a combination of LLM prompting and searches in authentic Old English
data. For annotation we assigned sentences to multiple students with limited
prior exposure to UD, whose annotations we compare and adjudicate. Our results
suggest that while current LLM outputs in Old English do not reflect authentic
syntax, this can be mitigated by post-editing, and that although beginner
annotators do not possess enough background to complete the task perfectly,
taken together they can produce good results and learn from the experience. We
also conduct preliminary parsing experiments using Modern English training
data, and find that although performance on Old English is poor, parsing on
annotated features (lemma, hyperlemma, gloss) leads to improved performance.

</details>


### [96] [Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards](https://arxiv.org/pdf/2505.02686)
*Xiaobao Wu*

Main category: cs.CL

TL;DR: The paper surveys the paradigm of Learning from Rewards in LLMs, covering reward models, learning strategies, benchmarks, applications, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To unify and overview the shift from pre-training to post-training scaling in LLMs, emphasizing the role of reward signals in guiding behavior.

Method: Comprehensive survey of reward-based techniques (e.g., RLHF, RLAIF, DPO, GRPO) and their applications across training, inference, and post-inference stages.

Result: Identifies the transition from passive to active learning in LLMs, enhancing alignment and reasoning capabilities.

Conclusion: Highlights the importance of reward-based learning, discusses benchmarks, and outlines future challenges in the field.

Abstract: Recent developments in Large Language Models (LLMs) have shifted from
pre-training scaling to post-training and test-time scaling. Across these
developments, a key unified paradigm has arisen: Learning from Rewards, where
reward signals act as the guiding stars to steer LLM behavior. It has
underpinned a wide range of prevalent techniques, such as reinforcement
learning (RLHF, RLAIF, DPO, and GRPO), reward-guided decoding, and post-hoc
correction. Crucially, this paradigm enables the transition from passive
learning from static data to active learning from dynamic feedback. This endows
LLMs with aligned preferences and deep reasoning capabilities for diverse
tasks. In this survey, we present a comprehensive overview of learning from
rewards, from the perspective of reward models and learning strategies across
training, inference, and post-inference stages. We further discuss the
benchmarks for reward models and the primary applications. Finally we highlight
the challenges and future directions. We maintain a paper collection at
https://github.com/bobxwu/learning-from-rewards-llm-papers.

</details>


### [97] [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/pdf/2505.06987)
*Xiaoyu Wang, Yue Zhao, Qingqing Gu, Zhonglin Jiang, Xiaokai Chen, Yong Chen, Luo Ji*

Main category: cs.CL

TL;DR: The paper introduces straQ*, a Q-learning-based framework for emotional support conversation (ESC) to improve long-term satisfaction by optimizing LLM responses.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based ESC solutions lack a state model perspective, leading to suboptimal long-term satisfaction.

Method: The proposed straQ* framework uses Q-learning on LLMs to plan, determine optimal strategies, and guide responses in ESC.

Result: Experiments show straQ* outperforms baselines like direct inference, self-refine, chain of thought, finetuning, and finite state machines.

Conclusion: straQ* effectively enhances ESC by leveraging Q-learning for long-term optimization.

Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress
of individuals through effective conversations. Although large language models
(LLMs) have obtained remarkable progress on ESC, most of these studies might
not define the diagram from the state model perspective, therefore providing a
suboptimal solution for long-term satisfaction. To address such an issue, we
leverage the Q-learning on LLMs, and propose a framework called straQ*. Our
framework allows a plug-and-play LLM to bootstrap the planning during ESC,
determine the optimal strategy based on long-term returns, and finally guide
the LLM to response. Substantial experiments on ESC datasets suggest that
straQ* outperforms many baselines, including direct inference, self-refine,
chain of thought, finetuning, and finite state machines.

</details>


### [98] [Research Borderlands: Analysing Writing Across Research Cultures](https://arxiv.org/pdf/2506.00784)
*Shaily Bhatt, Tal August, Maria Antoniak*

Main category: cs.CL

TL;DR: A human-centered approach is used to measure cultural norms in research writing, revealing LLMs' lack of cultural competence.


<details>
  <summary>Details</summary>
Motivation: To address the gap in cultural competence of language technologies, focusing on research cultures and writing adaptation.

Method: Interviews with interdisciplinary researchers to identify cultural norms, operationalized with computational metrics for analysis.

Result: Framework for cultural norms in research papers and evidence of LLMs' homogenizing effect on writing.

Conclusion: Human-centered methods effectively measure cultural norms and highlight LLMs' cultural shortcomings.

Abstract: Improving cultural competence of language technologies is important. However
most recent works rarely engage with the communities they study, and instead
rely on synthetic setups and imperfect proxies of culture. In this work, we
take a human-centered approach to discover and measure language-based cultural
norms, and cultural competence of LLMs. We focus on a single kind of culture,
research cultures, and a single task, adapting writing across research
cultures. Through a set of interviews with interdisciplinary researchers, who
are experts at moving between cultures, we create a framework of structural,
stylistic, rhetorical, and citational norms that vary across research cultures.
We operationalise these features with a suite of computational metrics and use
them for (a) surfacing latent cultural norms in human-written research papers
at scale; and (b) highlighting the lack of cultural competence of LLMs, and
their tendency to homogenise writing. Overall, our work illustrates the
efficacy of a human-centered approach to measuring cultural norms in
human-written and LLM-generated texts.

</details>


### [99] [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/pdf/2506.01062)
*Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, Tu Vu*

Main category: cs.CL

TL;DR: SealQA is a benchmark for evaluating search-augmented language models on challenging fact-seeking questions, revealing limitations in current models, including poor accuracy and vulnerability to noisy search results.


<details>
  <summary>Details</summary>
Motivation: To assess the factual accuracy and reasoning capabilities of language models, especially in scenarios with conflicting or noisy search results, and to extend evaluation to long-context reasoning.

Method: SealQA includes three variants: Seal-0 (challenging questions), Seal-Hard (factual accuracy and reasoning), and LongSeal (long-context, multi-document reasoning). Models like GPT-4.1 and DeepSeek-R1-671B are evaluated.

Result: Frontier models perform poorly, with low accuracy (e.g., 17.1% for o3-mini). Advanced models are vulnerable to noisy search results, and increased compute doesn't reliably improve performance.

Conclusion: Current models struggle with SealQA's challenges, highlighting the need for improved robustness and reasoning capabilities in search-augmented language models.

Abstract: We introduce SealQA, a new challenge benchmark for evaluating
SEarch-Augmented Language models on fact-seeking questions where web search
yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:
(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and
reasoning capabilities, with Seal-0 focusing on the most challenging questions
where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)
LongSeal, which extends SealQA to test long-context, multi-document reasoning
in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations
in current models: Even frontier LLMs perform poorly across all SealQA flavors.
On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini
achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning
efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and
o3-mini are highly vulnerable to noisy search results. Notably, increasing
test-time compute does not yield reliable gains across o3-mini, o4-mini, and
o3, with performance often plateauing or even declining early. Additionally,
while recent models are less affected by the "lost-in-the-middle" issue, they
still fail to reliably identify relevant documents in LongSeal when faced with
numerous distractors. To facilitate future work, we release SealQA at
huggingface.co/datasets/vtllms/sealqa.

</details>


### [100] [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/pdf/2506.01784)
*Shuai Wang, Yinan Yu*

Main category: cs.CL

TL;DR: iQUEST is a KBQA framework that improves multi-hop reasoning by decomposing queries into sub-questions and using a GNN for better path exploration, showing consistent improvements across benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs often lack factual accuracy in knowledge-intensive tasks, and multi-hop reasoning in KBQA faces challenges like maintaining coherence and avoiding premature path discarding.

Method: iQUEST decomposes complex queries into sub-questions and integrates a GNN to incorporate 2-hop neighbor information at each step.

Result: iQUEST demonstrates consistent improvements across four benchmark datasets and four LLMs.

Conclusion: The framework effectively addresses multi-hop reasoning challenges, enhancing reliability and performance in KBQA.

Abstract: While Large Language Models (LLMs) excel at many natural language processing
tasks, they often suffer from factual inaccuracies in knowledge-intensive
scenarios. Integrating external knowledge resources, particularly knowledge
graphs (KGs), provides a transparent and updatable foundation for more reliable
reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons
over KGs, is central to this effort, especially for complex, multi-hop queries.
However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent
reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop
connections. To address these issues, we introduce iQUEST, a question-guided
KBQA framework that iteratively decomposes complex queries into simpler
sub-questions, ensuring a structured and focused reasoning trajectory.
Additionally, we integrate a Graph Neural Network (GNN) to look ahead and
incorporate 2-hop neighbor information at each reasoning step. This dual
approach strengthens the reasoning process, enabling the model to explore
viable paths more effectively. Detailed experiments demonstrate the consistent
improvement delivered by iQUEST across four benchmark datasets and four LLMs.

</details>


### [101] [Identifying Reliable Evaluation Metrics for Scientific Text Revision](https://arxiv.org/pdf/2506.04772)
*Léane Jourdan, Florian Boudin, Richard Dufour, Nicolas Hernandez*

Main category: cs.CL

TL;DR: The paper critiques traditional text revision metrics like ROUGE and BERTScore, proposing a hybrid approach combining LLM-as-a-judge and domain-specific metrics for better evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics fail to capture meaningful improvements in text revision, necessitating better evaluation methods aligned with human judgments.

Method: Manual annotation study, exploration of reference-free metrics, and analysis of LLM-as-a-judge approaches.

Result: LLMs assess instruction-following well but struggle with correctness; domain-specific metrics offer complementary insights.

Conclusion: A hybrid approach of LLM-as-a-judge and task-specific metrics provides the most reliable revision quality assessment.

Abstract: Evaluating text revision in scientific writing remains a challenge, as
traditional metrics such as ROUGE and BERTScore primarily focus on similarity
rather than capturing meaningful improvements. In this work, we analyse and
identify the limitations of these metrics and explore alternative evaluation
methods that better align with human judgments. We first conduct a manual
annotation study to assess the quality of different revisions. Then, we
investigate reference-free evaluation metrics from related NLP domains.
Additionally, we examine LLM-as-a-judge approaches, analysing their ability to
assess revisions with and without a gold reference. Our results show that LLMs
effectively assess instruction-following but struggle with correctness, while
domain-specific metrics provide complementary insights. We find that a hybrid
approach combining LLM-as-a-judge evaluation and task-specific metrics offers
the most reliable assessment of revision quality.

</details>


### [102] [Context Is Not Comprehension](https://arxiv.org/pdf/2506.04907)
*Alex Pan, Mary-Anne Williams*

Main category: cs.CL

TL;DR: Verbose ListOps (VLO) is a benchmark for evaluating LLMs' multi-step reasoning by embedding deterministic computations in narrative form, revealing models' limitations despite high recall accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation focuses on fact recall, masking the harder challenge of multi-step reasoning and intermediate state tracking. VLO addresses this gap.

Method: VLO embeds ListOps computations in narrative camouflage and allows step-level evaluation of intermediate results.

Result: Models with 100% accuracy on raw ListOps collapse on VLO after 10,000 tokens, exposing reasoning divergence points.

Conclusion: VLO shifts assessment from context length to genuine comprehension and serves as a reusable test-bed for reasoning-centric model designs.

Abstract: The dominant way of judging Large Language Models (LLMs) has been to ask how
well they can recall explicit facts from very long inputs. While today's best
models achieve near perfect recall, this masks a harder skill: performing
multi-step reasoning and tracking intermediate state that never appears
verbatim. We introduce Verbose ListOps (VLO), a benchmark that embeds
deterministic ListOps computations inside narrative camouflage and, crucially,
allows step-level evaluation of every intermediate result. Experiments show
that models which solve raw ListOps with approximately 100% accuracy collapse
on VLO after only 10,000 tokens. By exposing where a model's reasoning chain
first diverges, VLO moves assessment beyond sheer context length and toward
genuine comprehension. VLO's generation pipeline is task-agnostic: it can weave
any deterministically verifiable reasoning schema -- arithmetic, symbolic,
abductive, inductive or defeasible -- into narrative form. This makes VLO a
reusable test-bed for the next wave of reasoning-centric model designs, not
merely those with step-explicit scaffolds.

</details>


### [103] [Chain-of-Code Collapse: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/pdf/2506.06971)
*Jaechul Roh, Varun Gandhi, Shivani Anilkumar, Arin Garg*

Main category: cs.CL

TL;DR: The paper investigates whether LLMs truly reason or rely on shallow patterns by testing their robustness with adversarially perturbed prompts, revealing significant performance variations.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs genuinely reason or exploit statistical patterns, the study explores their robustness under structured prompt perturbations.

Method: The authors introduce Chain-of-Code Collapse, evaluating LLMs on 700 perturbed code generations using transformations like storytelling reframing and irrelevant constraint injection.

Result: Performance varied widely, with some perturbations causing accuracy drops of up to -42.1%, while others improved accuracy by 35.3%.

Conclusion: The findings highlight the fragility of current reasoning systems, emphasizing the need for more principled approaches to robust prompting and reasoning alignment.

Abstract: Large Language Models (LLMs) have achieved remarkable success in tasks
requiring complex reasoning, such as code generation, mathematical problem
solving, and algorithmic synthesis -- especially when aided by reasoning tokens
and Chain-of-Thought prompting. Yet, a core question remains: do these models
truly reason, or do they merely exploit shallow statistical patterns? In this
paper, we introduce Chain-of-Code Collapse, where we systematically investigate
the robustness of reasoning LLMs by introducing a suite of semantically
faithful yet adversarially structured prompt perturbations. Our evaluation --
spanning 700 perturbed code generations derived from LeetCode-style problems --
applies transformations such as storytelling reframing, irrelevant constraint
injection, example reordering, and numeric perturbation. We observe that while
certain modifications severely degrade performance (with accuracy drops up to
-42.1%), others surprisingly improve model accuracy by up to 35.3%, suggesting
sensitivity not only to semantics but also to surface-level prompt dynamics.
These findings expose the fragility and unpredictability of current reasoning
systems, underscoring the need for more principles approaches to reasoning
alignments and prompting robustness. We release our perturbation datasets and
evaluation framework to promote further research in trustworthy and resilient
LLM reasoning.

</details>


### [104] [Improving Fairness of Large Language Models in Multi-document Summarization](https://arxiv.org/pdf/2506.07479)
*Haoyuan Li, Rui Zhang, Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: FairPO improves fairness in multi-document summarization (MDS) by addressing both summary-level and corpus-level fairness through preference tuning and dynamic weight adjustment.


<details>
  <summary>Details</summary>
Motivation: Fairness in MDS is critical to avoid bias in decision-making, such as overrepresenting negative reviews. Existing methods focus mainly on summary-level fairness.

Method: FairPO uses preference tuning: perturbing document sets for summary-level fairness and dynamically adjusting preference pair weights for corpus-level fairness.

Result: FairPO outperforms baselines while maintaining summary quality.

Conclusion: FairPO effectively balances fairness at both levels in MDS.

Abstract: Fairness in multi-document summarization (MDS) is crucial for providing
comprehensive views across documents with diverse social attribute values,
which can significantly impact decision-making. For example, a summarization
system that tends to overrepresent negative reviews of products can mislead
customers into disregarding good products. Previous works measure fairness in
MDS at two levels: summary-level and corpus-level. While summary-level fairness
focuses on individual summaries, corpus-level fairness focuses on a corpus of
summaries. Recent methods primarily focus on summary-level fairness. We propose
FairPO, a preference tuning method that focuses on both summary-level and
corpus-level fairness in MDS. To improve summary-level fairness, we propose to
generate preference pairs by perturbing document sets. To improve corpus-level
fairness, we propose fairness-aware preference tuning by dynamically adjusting
the weights of preference pairs. Our experiments show that FairPO outperforms
strong baselines while maintaining the critical qualities of summaries. The
code is available at https://github.com/leehaoyuan/coverage_fairnes.

</details>


### [105] [Towards Large Language Models with Self-Consistent Natural Language Explanations](https://arxiv.org/pdf/2506.07523)
*Sahar Admoni, Ofra Amir, Assaf Hallak, Yftah Ziser*

Main category: cs.CL

TL;DR: The paper introduces PSCB, a benchmark for evaluating LLM-generated explanations, and proposes a new metric to improve explanation quality via fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address inconsistencies in LLM-generated explanations and the lack of scalable evaluation methods.

Method: Develop PSCB for large-scale benchmarking, analyze self-consistency, propose a new metric, and fine-tune LLMs using DPO.

Result: PSCB reveals low self-consistency in explanations; the new metric and DPO improve alignment.

Conclusion: A scalable approach to enhance LLM explanation trustworthiness and self-consistency.

Abstract: Large language models (LLMs) seem to offer an easy path to interpretability:
just ask them to explain their decisions. Yet, studies show that these post-hoc
explanations often misrepresent the true decision process, as revealed by
mismatches in feature importance. Despite growing evidence of this
inconsistency, no systematic solutions have emerged, partly due to the high
cost of estimating feature importance, which limits evaluations to small
datasets. To address this, we introduce the Post-hoc Self-Consistency Bank
(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and
models, each paired with LLM-generated explanations and corresponding feature
importance scores. Analysis of PSCB reveals that self-consistency scores barely
differ between correct and incorrect predictions. We also show that the
standard metric fails to meaningfully distinguish between explanations. To
overcome this limitation, we propose an alternative metric that more
effectively captures variation in explanation quality. We use it to fine-tune
LLMs via Direct Preference Optimization (DPO), leading to significantly better
alignment between explanations and decision-relevant features, even under
domain shift. Our findings point to a scalable path toward more trustworthy,
self-consistent LLMs.

</details>


### [106] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Preference Optimization](https://arxiv.org/pdf/2506.08712)
*Hee Suk Yoon, Eunseop Yoon, Mark Hasegawa-Johnson, Sungwoong Kim, Chang D. Yoo*

Main category: cs.CL

TL;DR: ConfPO is a lightweight, model-free method for preference learning in LLMs, focusing on optimizing preference-critical tokens based on policy confidence, outperforming uniform DAAs like DPO.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DPO uniformly adjust token probabilities, risking overoptimization and inefficiency. ConfPO aims to improve alignment by targeting impactful tokens.

Method: ConfPO identifies and optimizes preference-critical tokens using only the training policy's confidence, avoiding auxiliary models or extra compute.

Result: ConfPO outperforms uniform DAAs on benchmarks like AlpacaEval 2 and Arena-Hard, achieving better alignment with no added computational cost.

Conclusion: ConfPO offers a simple, efficient alternative to existing DAAs, enhancing alignment quality without scalability or reliability concerns.

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [107] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/pdf/2506.09277)
*Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, Sarath Chandar, Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: A framework for measuring the faithfulness of LLM-generated self-NLE by comparing them with interpretations of the model's internal states.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating self-NLE faithfulness lack examination of neural activity, limiting understanding of the model's actual reasoning.

Method: Introduces a flexible framework comparing self-NLE with interpretations of the model's hidden states.

Result: Provides quantitative measurement of self-NLE faithfulness and deeper insights into model reasoning.

Conclusion: Advances understanding of self-NLE faithfulness and aids in generating more reliable explanations.

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [108] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/pdf/2506.09507)
*Bingheng Wu, Jingze Shi, Yifan Wu, Nan Tang, Yuyu Luo*

Main category: cs.CL

TL;DR: The paper introduces Unified RoPE and TransXSSM, a hybrid model combining Transformers and SSMs with unified positional encoding, achieving faster speeds and higher accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenge of integrating Transformers and SSMs due to incompatible positional encoding mechanisms (RoPE vs. convolutions) leads to suboptimal performance.

Method: Proposes Unified RoPE for consistent positional encoding and introduces TransXSSM, a hybrid architecture integrating Transformer and SSM layers.

Result: TransXSSM is 42.3% faster in training and 29.5% faster in inference at 4K sequence length, with over 4% higher accuracy on benchmarks. Scaling also improves (7.22% gain for 1.3B model).

Conclusion: Unified positional encoding resolves incompatibility, enabling efficient and high-performance long-context modeling in hybrid architectures.

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (Unified RoPE)
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this Unified RoPE, we
introduce TransXSSM, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, TransXSSM exhibits training and inference speeds that are
42.3\% and 29.5\% faster, respectively, relative to standard Transformer
models. It also delivers higher accuracy: under comparable settings, it
surpasses a Transformer baseline by over 4\% on language modeling
benchmarks.TransXSSM furthermore scales more effectively: TransXSSM-1.3B gains
7.22\% in average accuracy over its 320M version (versus about 6\% gains for
equivalent Transformers or SSMs). Our results show that unified positional
encoding resolves positional incompatibility in hybrid models, enabling
efficient, high-performance long-context modeling.

</details>


### [109] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/pdf/2506.09820)
*Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu*

Main category: cs.CL

TL;DR: CoRT improves LRMs' efficiency and accuracy in mathematical reasoning by integrating Code Interpreters via Hint-Engineering, reducing token usage and boosting performance.


<details>
  <summary>Details</summary>
Motivation: LRMs struggle with complex math operations despite CoT progress. Combining them with computational tools like CI is inefficient due to external knowledge integration challenges.

Method: CoRT post-trains LRMs using synthesized code-integrated reasoning data via Hint-Engineering, fine-tuning with supervised, rejection, and reinforcement learning.

Result: 4-8% absolute improvement on math reasoning tasks; 30-50% fewer tokens used compared to natural language models.

Conclusion: CoRT effectively bridges LRMs and CIs, enhancing mathematical reasoning efficiency and accuracy.

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [110] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/pdf/2506.09917)
*Wendi Zhou, Ameer Saadat-Yazdi, Nadin Kokciyan*

Main category: cs.CL

TL;DR: The paper proposes ASESUM, a novel summarization system for online reviews that captures aspect-centric opinions with supporting evidence and adapts to different domains without predefined aspects.


<details>
  <summary>Details</summary>
Motivation: The vast number of online reviews makes manual summarization impractical, necessitating automated systems that can extract and summarize key opinions effectively.

Method: The ASESUM framework extracts aspect-centric arguments, measures their salience and validity, and summarizes viewpoints relevant to critical product aspects.

Result: Experiments on a real-world dataset show ASESUM outperforms existing methods in capturing diverse review perspectives.

Conclusion: ASESUM provides a robust solution for automated opinion summarization, addressing limitations of previous extractive and abstractive approaches.

Abstract: Reviews are valuable resources for customers making purchase decisions in
online shopping. However, it is impractical for customers to go over the vast
number of reviews and manually conclude the prominent opinions, which prompts
the need for automated opinion summarization systems. Previous approaches,
either extractive or abstractive, face challenges in automatically producing
grounded aspect-centric summaries. In this paper, we propose a novel
summarization system that not only captures predominant opinions from an aspect
perspective with supporting evidence, but also adapts to varying domains
without relying on a pre-defined set of aspects. Our proposed framework,
ASESUM, summarizes viewpoints relevant to the critical aspects of a product by
extracting aspect-centric arguments and measuring their salience and validity.
We conduct experiments on a real-world dataset to demonstrate the superiority
of our approach in capturing diverse perspectives of the original reviews
compared to new and existing methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [111] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/pdf/2506.10005)
*Sridhar S, Nithin A, Shakeel Rifath, Vasantha Raj*

Main category: cs.CV

TL;DR: A method for generating 60-second cinematic videos from text using Stable Diffusion, GPT-2, and hybrid audio, achieving professional quality with optimizations for efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: To advance text-to-video synthesis for creative, educational, and industrial applications by automating cinematic video creation.

Method: Combines Stable Diffusion for images, GPT-2 for narrative, and hybrid audio (gTTS and YouTube music) in a five-scene framework with post-processing and synchronization.

Result: Produces high-quality videos with visual fidelity, narrative coherence, and efficiency, demonstrated in a GPU-accelerated environment.

Conclusion: The method successfully enhances text-to-video synthesis, offering practical applications across various fields.

Abstract: Advances in generative artificial intelligence have altered multimedia
creation, allowing for automatic cinematic video synthesis from text inputs.
This work describes a method for creating 60-second cinematic movies
incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for
narrative structuring, and a hybrid audio pipeline using gTTS and
YouTube-sourced music. It uses a five-scene framework, which is augmented by
linear frame interpolation, cinematic post-processing (e.g., sharpening), and
audio-video synchronization to provide professional-quality results. It was
created in a GPU-accelerated Google Colab environment using Python 3.11. It has
a dual-mode Gradio interface (Simple and Advanced), which supports resolutions
of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA
memory management and error handling ensure reliability. The experiments
demonstrate outstanding visual quality, narrative coherence, and efficiency,
furthering text-to-video synthesis for creative, educational, and industrial
applications.

</details>


### [112] [LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning](https://arxiv.org/pdf/2506.10082)
*Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, Tianfan Xue*

Main category: cs.CV

TL;DR: Proposes a mask-based LoRA tuning method for flexible video editing using pretrained I2V models, preserving backgrounds and enabling controllable edits.


<details>
  <summary>Details</summary>
Motivation: Current methods lack flexibility for specific edits due to reliance on large-scale pretraining or limited control over frames.

Method: Uses mask-driven LoRA tuning to adapt pretrained I2V models, incorporating references and spatial masks for region-specific learning.

Result: Achieves superior video editing performance compared to state-of-the-art methods.

Conclusion: The method offers efficient, adaptable video editing without altering model architecture.

Abstract: Video editing using diffusion models has achieved remarkable results in
generating high-quality edits for videos. However, current methods often rely
on large-scale pretraining, limiting flexibility for specific edits.
First-frame-guided editing provides control over the first frame, but lacks
flexibility over subsequent frames. To address this, we propose a mask-based
LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video
(I2V) models for flexible video editing. Our approach preserves background
regions while enabling controllable edits propagation. This solution offers
efficient and adaptable video editing without altering the model architecture.
To better steer this process, we incorporate additional references, such as
alternate viewpoints or representative scene states, which serve as visual
anchors for how content should unfold. We address the control challenge using a
mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model
to the editing context. The model must learn from two distinct sources: the
input video provides spatial structure and motion cues, while reference images
offer appearance guidance. A spatial mask enables region-specific learning by
dynamically modulating what the model attends to, ensuring that each area draws
from the appropriate source. Experimental results show our method achieves
superior video editing performance compared to state-of-the-art methods.

</details>


### [113] [DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding](https://arxiv.org/pdf/2506.10084)
*Bin Guo, John H. L. Hansen*

Main category: cs.CV

TL;DR: DeepTraverse introduces a vision architecture inspired by algorithmic search, enabling adaptive feature refinement, outperforming conventional models.


<details>
  <summary>Details</summary>
Motivation: To address the limited adaptive refinement in conventional vision backbones by incorporating algorithmic search principles for more structured and interpretable feature learning.

Method: Uses recursive exploration modules for deepening feature analysis and adaptive calibration modules for dynamic feature salience adjustment.

Result: Achieves competitive accuracy and robust feature discrimination, often outperforming conventional models.

Conclusion: Integrating algorithmic priors is a principled strategy for efficient, performant, and structured vision backbones.

Abstract: Conventional vision backbones, despite their success, often construct
features through a largely uniform cascade of operations, offering limited
explicit pathways for adaptive, iterative refinement. This raises a compelling
question: can principles from classical search algorithms instill a more
algorithmic, structured, and logical processing flow within these networks,
leading to representations built through more interpretable, perhaps
reasoning-like decision processes? We introduce DeepTraverse, a novel vision
architecture directly inspired by algorithmic search strategies, enabling it to
learn features through a process of systematic elucidation and adaptive
refinement distinct from conventional approaches. DeepTraverse operationalizes
this via two key synergistic components: recursive exploration modules that
methodically deepen feature analysis along promising representational paths
with parameter sharing for efficiency, and adaptive calibration modules that
dynamically adjust feature salience based on evolving global context. The
resulting algorithmic interplay allows DeepTraverse to intelligently construct
and refine feature patterns. Comprehensive evaluations across a diverse suite
of image classification benchmarks show that DeepTraverse achieves highly
competitive classification accuracy and robust feature discrimination, often
outperforming conventional models with similar or larger parameter counts. Our
work demonstrates that integrating such algorithmic priors provides a
principled and effective strategy for building more efficient, performant, and
structured vision backbones.

</details>


### [114] [Test-Time Adaptation for Generalizable Task Progress Estimation](https://arxiv.org/pdf/2506.10085)
*Christos Ziakas, Alessandra Russo*

Main category: cs.CV

TL;DR: A test-time adaptation method improves progress estimation by optimizing a self-supervised objective, generalizing to diverse tasks and outperforming state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: To enable progress estimation models to adapt online to varying visual and temporal contexts during testing.

Method: Gradient-based meta-learning trained on expert trajectories and task descriptions, optimizing for semantic content over temporal order.

Result: Outperforms state-of-the-art in-context learning, generalizing to diverse out-of-distribution tasks and environments.

Conclusion: The method effectively adapts to test-time contexts, enhancing progress estimation robustness and performance.

Abstract: We propose a test-time adaptation method that enables a progress estimation
model to adapt online to the visual and temporal context of test trajectories
by optimizing a learned self-supervised objective. To this end, we introduce a
gradient-based meta-learning strategy to train the model on expert visual
trajectories and their natural language task descriptions, such that test-time
adaptation improves progress estimation relying on semantic content over
temporal order. Our test-time adaptation method generalizes from a single
training environment to diverse out-of-distribution tasks, environments, and
embodiments, outperforming the state-of-the-art in-context learning approach
using autoregressive vision-language models.

</details>


### [115] [EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models](https://arxiv.org/pdf/2506.10100)
*Yantai Yang, Yuhao Wang, Zichen Wen, Luo Zhongwei, Chang Zou, Zhipeng Zhang, Chuan Wen, Linfeng Zhang*

Main category: cs.CV

TL;DR: EfficientVLA is a training-free framework that accelerates Vision-Language-Action (VLA) models by addressing computational and memory redundancies through pruning, visual token optimization, and feature caching, achieving significant speedup with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: VLA models face high computational and memory demands due to redundancies, limiting deployability. Existing solutions are piecemeal and fail to address bottlenecks holistically.

Method: EfficientVLA integrates three strategies: pruning redundant language layers, optimizing visual tokens, and caching features in the diffusion-based action head.

Result: Applied to CogACT, EfficientVLA achieves a 1.93X speedup, reduces FLOPs to 28.9%, and drops success rate by only 0.6% in the SIMPLER benchmark.

Conclusion: EfficientVLA provides a holistic, training-free solution for accelerating VLA models, balancing efficiency and performance.

Abstract: Vision-Language-Action (VLA) models, particularly diffusion-based
architectures, demonstrate transformative potential for embodied intelligence
but are severely hampered by high computational and memory demands stemming
from extensive inherent and inference-time redundancies. While existing
acceleration efforts often target isolated inefficiencies, such piecemeal
solutions typically fail to holistically address the varied computational and
memory bottlenecks across the entire VLA pipeline, thereby limiting practical
deployability. We introduce EfficientVLA, a structured and training-free
inference acceleration framework that systematically eliminates these barriers
by cohesively exploiting multifaceted redundancies. EfficientVLA
synergistically integrates three targeted strategies: (1) pruning of
functionally inconsequential layers from the language module, guided by an
analysis of inter-layer redundancies; (2) optimizing the visual processing
pathway through a task-aware strategy that selects a compact, diverse set of
visual tokens, balancing task-criticality with informational coverage; and (3)
alleviating temporal computational redundancy within the iterative
diffusion-based action head by strategically caching and reusing key
intermediate features. We apply our method to a standard VLA model CogACT,
yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%
success rate drop in the SIMPLER benchmark.

</details>


### [116] [A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild](https://arxiv.org/pdf/2506.10117)
*Klim Kireev, Ana-Maria Creţu, Raphael Meier, Sarah Adel Bargal, Elissa Redmiles, Carmela Troncoso*

Main category: cs.CV

TL;DR: The paper introduces the Image-Caption Children in the Wild Dataset (ICCWD) to benchmark tools for detecting depictions of minors, addressing a gap in multi-modal datasets.


<details>
  <summary>Details</summary>
Motivation: Existing tools lack a benchmark for detecting minors in multi-modal content, necessitating a richer dataset like ICCWD.

Method: ICCWD contains 10,000 manually labeled image-caption pairs, used to benchmark three detectors, including a commercial age estimation system.

Result: Child detection proves challenging, with the best method achieving a 75.3% true positive rate.

Conclusion: ICCWD aims to improve minor detection methods across various scenarios.

Abstract: Platforms and the law regulate digital content depicting minors (defined as
individuals under 18 years of age) differently from other types of content.
Given the sheer amount of content that needs to be assessed, machine
learning-based automation tools are commonly used to detect content depicting
minors. To our knowledge, no dataset or benchmark currently exists for
detecting these identification methods in a multi-modal environment. To fill
this gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an
image-caption dataset aimed at benchmarking tools that detect depictions of
minors. Our dataset is richer than previous child image datasets, containing
images of children in a variety of contexts, including fictional depictions and
partially visible bodies. ICCWD contains 10,000 image-caption pairs manually
labeled to indicate the presence or absence of a child in the image. To
demonstrate the possible utility of our dataset, we use it to benchmark three
different detectors, including a commercial age estimation system applied to
images. Our results suggest that child detection is a challenging task, with
the best method achieving a 75.3% true positive rate. We hope the release of
our dataset will aid in the design of better minor detection methods in a wide
range of scenarios.

</details>


### [117] [Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers](https://arxiv.org/pdf/2506.10119)
*Natanael Lucena, Fábio S. da Silva, Ricardo Rios*

Main category: cs.CV

TL;DR: Comparison of CNNs and ViTs for psoriasis lesion classification shows ViTs, especially DaViT-B, outperform with a 96.4% f1-score, highlighting their efficiency in medical imaging.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the effectiveness of CNNs and ViTs in classifying psoriasis and similar lesions for automated medical diagnosis.

Method: Pre-trained ImageNet models (CNNs and ViTs) were adapted to a specific dataset for multi-class classification of psoriasis-like lesions.

Result: ViTs, particularly DaViT-B, achieved superior performance (96.4% f1-score) with smaller models compared to CNNs.

Conclusion: ViTs, especially DaViT-B, are recommended for automated psoriasis detection, showcasing their potential in medical image classification.

Abstract: This paper presents a comparison of the performance of Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying
images containing lesions of psoriasis and diseases similar to it. Models
pre-trained on ImageNet were adapted to a specific data set. Both achieved high
predictive metrics, but the ViTs stood out for their superior performance with
smaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the
best results, with an f1-score of 96.4%, and is recommended as the most
efficient architecture for automated psoriasis detection. This article
reinforces the potential of ViTs for medical image classification tasks.

</details>


### [118] [DanceChat: Large Language Model-Guided Music-to-Dance Generation](https://arxiv.org/pdf/2506.10574)
*Qing Wang, Xiaohang Yang, Yilan Dong, Naveen Raj Govindaraj, Gregory Slabaugh, Shanxin Yuan*

Main category: cs.CV

TL;DR: DanceChat uses an LLM to guide music-to-dance generation, improving diversity and alignment with music by providing textual motion instructions.


<details>
  <summary>Details</summary>
Motivation: The semantic gap between music and dance, along with the one-to-many mapping challenge, necessitates additional guidance beyond music alone.

Method: DanceChat combines LLM-based pseudo instruction generation, multi-modal feature fusion, and diffusion-based motion synthesis with alignment loss.

Result: DanceChat outperforms state-of-the-art methods in generating diverse and musically aligned dance motions.

Conclusion: LLM-guided dance generation enhances diversity and alignment, addressing limitations of implicit learning from music alone.

Abstract: Music-to-dance generation aims to synthesize human dance motion conditioned
on musical input. Despite recent progress, significant challenges remain due to
the semantic gap between music and dance motion, as music offers only abstract
cues, such as melody, groove, and emotion, without explicitly specifying the
physical movements. Moreover, a single piece of music can produce multiple
plausible dance interpretations. This one-to-many mapping demands additional
guidance, as music alone provides limited information for generating diverse
dance movements. The challenge is further amplified by the scarcity of paired
music and dance data, which restricts the model\^a\u{A}\'Zs ability to learn
diverse dance patterns. In this paper, we introduce DanceChat, a Large Language
Model (LLM)-guided music-to-dance generation approach. We use an LLM as a
choreographer that provides textual motion instructions, offering explicit,
high-level guidance for dance generation. This approach goes beyond implicit
learning from music alone, enabling the model to generate dance that is both
more diverse and better aligned with musical styles. Our approach consists of
three components: (1) an LLM-based pseudo instruction generation module that
produces textual dance guidance based on music style and structure, (2) a
multi-modal feature extraction and fusion module that integrates music, rhythm,
and textual guidance into a shared representation, and (3) a diffusion-based
motion synthesis module together with a multi-modal alignment loss, which
ensures that the generated dance is aligned with both musical and textual cues.
Extensive experiments on AIST++ and human evaluations show that DanceChat
outperforms state-of-the-art methods both qualitatively and quantitatively.

</details>


### [119] [ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs](https://arxiv.org/pdf/2506.10128)
*Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, Kevin Lin, Linjie Li, Furong Huang, Lijuan Wang*

Main category: cs.CV

TL;DR: ViCrit introduces an RL task for VLMs to localize subtle visual hallucinations in captions, improving visual perception and transferability across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Extending RL success in LLMs to VLMs is hindered by the lack of challenging, verifiable vision tasks. ViCrit addresses this gap.

Method: ViCrit trains VLMs to detect synthetic visual errors in captions, using binary rewards for localization.

Result: Models show significant gains on VL benchmarks, with transferability to abstract reasoning and visual math.

Conclusion: Fine-grained hallucination criticism enhances visual perception in VLMs, demonstrated by ViCrit-Bench.

Abstract: Reinforcement learning (RL) has shown great effectiveness for fine-tuning
large language models (LLMs) using tasks that are challenging yet easily
verifiable, such as math reasoning or code generation. However, extending this
success to visual perception in vision-language models (VLMs) has been impeded
by the scarcity of vision-centric tasks that are simultaneously challenging and
unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption
Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle,
synthetic visual hallucination injected into paragraphs of human-written image
captions. Starting from a 200-word captions, we inject a single, subtle visual
description error-altering a few words on objects, attributes, counts, or
spatial relations-and task the model to pinpoint the corrupted span given the
image and the modified caption. This formulation preserves the full perceptual
difficulty while providing a binary, exact-match reward that is easy to compute
and unambiguous. Models trained with the ViCrit Task exhibit substantial gains
across a variety of VL benchmarks. Crucially, the improvements transfer beyond
natural-image training data to abstract image reasoning and visual math,
showing promises of learning to perceive rather than barely memorizing seen
objects. To facilitate evaluation, we further introduce ViCrit-Bench, a
category-balanced diagnostic benchmark that systematically probes perception
errors across diverse image domains and error types. Together, our results
demonstrate that fine-grained hallucination criticism is an effective and
generalizable objective for enhancing visual perception in VLMs.

</details>


### [120] [RoCA: Robust Cross-Domain End-to-End Autonomous Driving](https://arxiv.org/pdf/2506.10145)
*Rajeev Yasarla, Shizhong Han, Hsin-Pai Cheng, Litian Liu, Shweta Mahajan, Apratim Bhattacharyya, Yunxiao Shi, Risheek Garrepalli, Hong Cai, Fatih Porikli*

Main category: cs.CV

TL;DR: RoCA is a novel framework for robust cross-domain E2E autonomous driving, improving generalizability and adaptation without extra inference costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of deploying E2E autonomous driving across domains, where existing methods like LLMs lack performance guarantees and incur high retraining costs.

Method: RoCA formulates a joint probabilistic distribution over driving tokens using Gaussian processes, learning basis tokens and trajectories for diverse scenarios.

Result: RoCA enhances the base E2E model's generalizability and outperforms direct finetuning in domain adaptation.

Conclusion: RoCA achieves strong performance in cross-domain scenarios, demonstrating robust generalization and adaptation.

Abstract: End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,
offering significant potential. However, few studies have looked into the
practical challenge of deployment across domains (e.g., cities). Although
several works have incorporated Large Language Models (LLMs) to leverage their
open-world knowledge, LLMs do not guarantee cross-domain driving performance
and may incur prohibitive retraining costs during domain adaptation. In this
paper, we propose RoCA, a novel framework for robust cross-domain E2E
autonomous driving. RoCA formulates the joint probabilistic distribution over
the tokens that encode ego and surrounding vehicle information in the E2E
pipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of
basis tokens with corresponding trajectories, which span diverse driving
scenarios. Then, given any driving scene, it is able to probabilistically infer
the future trajectory. By using RoCA together with a base E2E model in
source-domain training, we improve the generalizability of the base model,
without requiring extra inference computation. In addition, RoCA enables robust
adaptation on new target domains, significantly outperforming direct
finetuning. We extensively evaluate RoCA on various cross-domain scenarios and
show that it achieves strong domain generalization and adaptation performance.

</details>


### [121] [SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score](https://arxiv.org/pdf/2506.10173)
*Mohammad Jalali, Haoyu Lei, Amin Gohari, Farzan Farnia*

Main category: cs.CV

TL;DR: SPARKE introduces a scalable method for prompt-aware diversity guidance in diffusion models, reducing computational complexity while improving diversity.


<details>
  <summary>Details</summary>
Motivation: Ensuring diversity in prompt-guided diffusion models is challenging, especially for broad semantic prompts. Existing methods lack scalability.

Method: SPARKE uses conditional entropy for diversity guidance, dynamically measuring diversity across similar prompts and reducing computational complexity from O(n³) to O(n).

Result: SPARKE improves prompt-aware diversity in generated samples without significant computational costs, validated on text-to-image models.

Conclusion: SPARKE effectively balances diversity and computational efficiency, making it scalable for large-scale generation tasks.

Abstract: Diffusion models have demonstrated remarkable success in high-fidelity image
synthesis and prompt-guided generative modeling. However, ensuring adequate
diversity in generated samples of prompt-guided diffusion models remains a
challenge, particularly when the prompts span a broad semantic spectrum and the
diversity of generated data needs to be evaluated in a prompt-aware fashion
across semantically similar prompts. Recent methods have introduced guidance
via diversity measures to encourage more varied generations. In this work, we
extend the diversity measure-based approaches by proposing the Scalable
Prompt-Aware R\'eny Kernel Entropy Diversity Guidance (SPARKE) method for
prompt-aware diversity guidance. SPARKE utilizes conditional entropy for
diversity guidance, which dynamically conditions diversity measurement on
similar prompts and enables prompt-aware diversity control. While the
entropy-based guidance approach enhances prompt-aware diversity, its reliance
on the matrix-based entropy scores poses computational challenges in
large-scale generation settings. To address this, we focus on the special case
of Conditional latent RKE Score Guidance, reducing entropy computation and
gradient-based optimization complexity from the $O(n^3)$ of general entropy
measures to $O(n)$. The reduced computational complexity allows for
diversity-guided sampling over potentially thousands of generation rounds on
different prompts. We numerically test the SPARKE method on several
text-to-image diffusion models, demonstrating that the proposed method improves
the prompt-aware diversity of the generated data without incurring significant
computational costs. We release our code on the project page:
https://mjalali.github.io/SPARKE

</details>


### [122] [Retrieval of Surface Solar Radiation through Implicit Albedo Recovery from Temporal Context](https://arxiv.org/pdf/2506.10174)
*Yael Frischholz, Devis Tuia, Michael Lehning*

Main category: cs.CV

TL;DR: The paper proposes an attention-based emulator for surface solar radiation (SSR) retrieval, eliminating the need for hand-crafted features like albedo maps or cloud masks. It outperforms traditional methods, especially in mountainous regions with dynamic snow cover.


<details>
  <summary>Details</summary>
Motivation: Traditional SSR retrieval methods fail in mountainous regions due to intermittent snow cover and changing surfaces. The paper aims to address this by leveraging machine learning to infer clear-sky reflectance from satellite imagery.

Method: The method uses a Temporo-Spatial Vision Transformer to learn clear-sky surface reflectance from raw satellite image sequences, augmented with topographic features and solar geometry. It is trained on HelioMont SSR estimates over Switzerland.

Result: The model matches the performance of albedo-informed models when given sufficient temporal context, particularly excelling in mountainous regions and improving generalization across terrains.

Conclusion: The proposed emulator effectively learns latent surface reflectance dynamics, offering a robust solution for SSR retrieval in complex terrains. Code and datasets are publicly available.

Abstract: Accurate retrieval of surface solar radiation (SSR) from satellite imagery
critically depends on estimating the background reflectance that a spaceborne
sensor would observe under clear-sky conditions. Deviations from this baseline
can then be used to detect cloud presence and guide radiative transfer models
in inferring atmospheric attenuation. Operational retrieval algorithms
typically approximate background reflectance using monthly statistics, assuming
surface properties vary slowly relative to atmospheric conditions. However,
this approach fails in mountainous regions where intermittent snow cover and
changing snow surfaces are frequent. We propose an attention-based emulator for
SSR retrieval that implicitly learns to infer clear-sky surface reflectance
from raw satellite image sequences. Built on the Temporo-Spatial Vision
Transformer, our approach eliminates the need for hand-crafted features such as
explicit albedo maps or cloud masks. The emulator is trained on instantaneous
SSR estimates from the HelioMont algorithm over Switzerland, a region
characterized by complex terrain and dynamic snow cover. Inputs include
multi-spectral SEVIRI imagery from the Meteosat Second Generation platform,
augmented with static topographic features and solar geometry. The target
variable is HelioMont's SSR, computed as the sum of its direct and diffuse
horizontal irradiance components, given at a spatial resolution of 1.7 km. We
show that, when provided a sufficiently long temporal context, the model
matches the performances of albedo-informed models, highlighting the model's
ability to internally learn and exploit latent surface reflectance dynamics.
Our geospatial analysis shows this effect is most powerful in mountainous
regions and improves generalization in both simple and complex topographic
settings. Code and datasets are publicly available at
https://github.com/frischwood/HeMu-dev.git

</details>


### [123] [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/pdf/2506.10452)
*Guowei Zhong, Ruohong Huan, Mingzhen Wu, Ronghua Liang, Peng Chen*

Main category: cs.CV

TL;DR: CIDer is a robust MER framework addressing modality missing and OOD data with MSSD and MACI modules, achieving efficient performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in MER include handling modality missing and OOD data simultaneously, with existing methods being impractical due to excessive parameters or model specificity.

Method: CIDer integrates MSSD for robustness under RMFM and MACI for OOD generalization, using self-distillation, causal inference, and efficient multimodal fusion.

Result: CIDer outperforms state-of-the-art methods in RMFM and OOD scenarios with fewer parameters and faster training.

Conclusion: CIDer provides a practical and efficient solution for robust MER, validated by experimental results and publicly available implementation.

Abstract: Recent advancements in Multimodal Emotion Recognition (MER) face challenges
in addressing both modality missing and Out-Of-Distribution (OOD) data
simultaneously. Existing methods often rely on specific models or introduce
excessive parameters, which limits their practicality. To address these issues,
we propose a novel robust MER framework, Causal Inference Distiller (CIDer),
and introduce a new task, Random Modality Feature Missing (RMFM), to generalize
the definition of modality missing. CIDer integrates two key components: a
Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal
Inference (MACI) module. MSSD enhances robustness under the RMFM task through a
weight-sharing self-distillation approach applied across low-level features,
attention maps, and high-level representations. Additionally, a Word-level
Self-aligned Attention Module (WSAM) reduces computational complexity, while a
Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.
To tackle OOD challenges, MACI employs a tailored causal graph to mitigate
label and language biases using a Multimodal Causal Module (MCM) and
fine-grained counterfactual texts. Notably, MACI can independently enhance OOD
generalization with minimal additional parameters. Furthermore, we also
introduce the new repartitioned MER OOD datasets. Experimental results
demonstrate that CIDer achieves robust performance in both RMFM and OOD
scenarios, with fewer parameters and faster training compared to
state-of-the-art methods. The implementation of this work is publicly
accessible at https://github.com/gw-zhong/CIDer.

</details>


### [124] [Attention, Please! Revisiting Attentive Probing for Masked Image Modeling](https://arxiv.org/pdf/2506.10178)
*Bill Psomas, Dionysis Christopoulos, Eirini Baltzi, Ioannis Kakogeorgiou, Tilemachos Aravanis, Nikos Komodakis, Konstantinos Karantzalos, Yannis Avrithis, Giorgos Tolias*

Main category: cs.CV

TL;DR: The paper introduces Efficient Probing (EP), a method improving attentive probing for SSL models by reducing parameters and speeding up computation, outperforming linear probing and prior methods.


<details>
  <summary>Details</summary>
Motivation: Standard linear probing fails for MIM-trained models due to patch tokens' distributed nature, motivating the need for efficient attentive probing.

Method: EP uses a multi-query cross-attention mechanism to eliminate redundant projections, reducing parameters and improving speed.

Result: EP achieves up to 10x speed-up, outperforms LP and prior methods across seven benchmarks, and generalizes beyond MIM.

Conclusion: EP is a simple, efficient, and effective probing method with strong performance and interpretability.

Abstract: As fine-tuning (FT) becomes increasingly impractical at scale, probing is
emerging as the preferred evaluation protocol for self-supervised learning
(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the
potential of models trained with Masked Image Modeling (MIM), due to the
distributed nature of patch tokens. This motivates the need for attentive
probing, an alternative that uses attention to selectively aggregate
patch-level features. Despite its growing adoption, attentive probing remains
under-explored, with existing methods suffering from excessive parameterization
and poor computational efficiency.
  In this work, we revisit attentive probing through the lens of the
accuracy-efficiency trade-off. We conduct a systematic study of existing
methods, analyzing their mechanisms and benchmarking their performance. We
introduce efficient probing (EP), a multi-query cross-attention mechanism that
eliminates redundant projections, reduces the number of trainable parameters,
and achieves up to a 10$\times$ speed-up over conventional multi-head
attention. Despite its simplicity, EP outperforms LP and prior attentive
probing approaches across seven benchmarks, generalizes well beyond MIM to
diverse pre-training paradigms, produces interpretable attention maps, and
achieves strong gains in low-shot and layer-wise settings. Code available at
https://github.com/billpsomas/efficient-probing.

</details>


### [125] [VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos](https://arxiv.org/pdf/2506.10857)
*Jiashuo Yu, Yue Wu, Meng Chu, Zhifei Ren, Zizheng Huang, Pei Chu, Ruijie Zhang, Yinan He, Qirui Li, Songze Li, Zhenxiang Li, Zhongying Tu, Conghui He, Yu Qiao, Yali Wang, Yi Wang, Limin Wang*

Main category: cs.CV

TL;DR: VRBench is a new benchmark for evaluating multi-step reasoning in large models using long narrative videos, human-labeled QA pairs, and a multi-phase evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations lack focus on temporal reasoning and procedural validity, prompting the creation of VRBench.

Method: VRBench includes 1,010 long videos, human-labeled QA pairs, and a human-AI framework for generating reasoning chains. It uses a multi-phase evaluation pipeline with outcome and process-level assessments.

Result: Evaluations of 12 LLMs and 16 VLMs on VRBench provide insights into multi-step reasoning capabilities.

Conclusion: VRBench advances the field by addressing gaps in temporal and procedural reasoning evaluations.

Abstract: We present VRBench, the first long narrative video benchmark crafted for
evaluating large models' multi-step reasoning capabilities, addressing
limitations in existing evaluations that overlook temporal reasoning and
procedural validity. It comprises 1,010 long videos (with an average duration
of 1.6 hours), along with 9,468 human-labeled multi-step question-answering
pairs and 30,292 reasoning steps with timestamps. These videos are curated via
a multi-stage filtering process including expert inter-rater reviewing to
prioritize plot coherence. We develop a human-AI collaborative framework that
generates coherent reasoning chains, each requiring multiple temporally
grounded steps, spanning seven types (e.g., event attribution, implicit
inference). VRBench designs a multi-phase evaluation pipeline that assesses
models at both the outcome and process levels. Apart from the MCQs for the
final results, we propose a progress-level LLM-guided scoring metric to
evaluate the quality of the reasoning chain from multiple dimensions
comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on
VRBench, we undertake a thorough analysis and provide valuable insights that
advance the field of multi-step reasoning.

</details>


### [126] [Improving Personalized Search with Regularized Low-Rank Parameter Updates](https://arxiv.org/pdf/2506.10182)
*Fiona Ryan, Josef Sivic, Fabian Caba Heilbron, Judy Hoffman, James M. Rehg, Bryan Russell*

Main category: cs.CV

TL;DR: The paper proposes a method for personalized vision-language retrieval by adapting a dual encoder model, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To recognize new personal concepts (e.g., 'my dog Fido') from few examples while integrating personal and general knowledge.

Method: Uses regularized low-rank adaptation in the language encoder's final layer and explores parameter addition for combining learned concepts.

Result: Outperforms prior methods by 4%-22% on benchmarks (DeepFashion2, ConCon-Chi).

Conclusion: The approach effectively adapts vision-language models for personalized retrieval while preserving general knowledge.

Abstract: Personalized vision-language retrieval seeks to recognize new concepts (e.g.
"my dog Fido") from only a few examples. This task is challenging because it
requires not only learning a new concept from a few images, but also
integrating the personal and general knowledge together to recognize the
concept in different contexts. In this paper, we show how to effectively adapt
the internal representation of a vision-language dual encoder model for
personalized vision-language retrieval. We find that regularized low-rank
adaption of a small set of parameters in the language encoder's final layer
serves as a highly effective alternative to textual inversion for recognizing
the personal concept while preserving general knowledge. Additionally, we
explore strategies for combining parameters of multiple learned personal
concepts, finding that parameter addition is effective. To evaluate how well
general knowledge is preserved in a finetuned representation, we introduce a
metric that measures image retrieval accuracy based on captions generated by a
vision language model (VLM). Our approach achieves state-of-the-art accuracy on
two benchmarks for personalized image retrieval with natural language queries -
DeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal
retrievals.

</details>


### [127] [Research on Audio-Visual Quality Assessment Dataset and Method for User-Generated Omnidirectional Video](https://arxiv.org/pdf/2506.10331)
*Fei Zhao, Da Pan, Zelu Qi, Ping Shi*

Main category: cs.CV

TL;DR: The paper addresses the lack of audio-visual quality assessment (AVQA) in user-generated omnidirectional videos (UGC-ODVs) by creating a dataset and proposing a baseline model for AVQA.


<details>
  <summary>Details</summary>
Motivation: The rise of the Metaverse and the shift from professional to user-generated omnidirectional videos (ODVs) highlight the need for better AVQA methods, which are currently limited.

Method: The authors construct a UGC-ODV dataset with 300 videos captured by five individuals using two types of omnidirectional cameras. A subjective AVQA experiment yields Mean Opinion Scores (MOSs). They then develop a baseline AVQA model with video and audio feature extraction and fusion modules.

Result: The proposed baseline model achieves optimal performance on the constructed dataset.

Conclusion: The study provides a foundational dataset and model for advancing AVQA in UGC-ODVs, addressing a critical gap in the field.

Abstract: In response to the rising prominence of the Metaverse, omnidirectional videos
(ODVs) have garnered notable interest, gradually shifting from
professional-generated content (PGC) to user-generated content (UGC). However,
the study of audio-visual quality assessment (AVQA) within ODVs remains
limited. To address this, we construct a dataset of UGC omnidirectional audio
and video (A/V) content. The videos are captured by five individuals using two
different types of omnidirectional cameras, shooting 300 videos covering 10
different scene types. A subjective AVQA experiment is conducted on the dataset
to obtain the Mean Opinion Scores (MOSs) of the A/V sequences. After that, to
facilitate the development of UGC-ODV AVQA fields, we construct an effective
AVQA baseline model on the proposed dataset, of which the baseline model
consists of video feature extraction module, audio feature extraction and
audio-visual fusion module. The experimental results demonstrate that our model
achieves optimal performance on the proposed dataset.

</details>


### [128] [ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators](https://arxiv.org/pdf/2506.10226)
*Parsa Rahimi, Sebastien Marcel*

Main category: cs.CV

TL;DR: ScoreMix is a data augmentation method using diffusion models to improve discriminator performance with limited labeled data by mixing scores from different class-conditioned trajectories.


<details>
  <summary>Details</summary>
Motivation: Enhancing discriminator performance in scenarios with limited labeled data by leveraging diffusion models' score properties.

Method: Convexly mixing scores from different class-conditioned trajectories during diffusion sampling to generate challenging synthetic samples.

Result: Notable performance improvements in benchmarks, especially when mixing classes distant in the discriminator's embedding space.

Conclusion: ScoreMix effectively boosts discriminator performance without extensive parameter tuning, addressing challenges of limited labeled data.

Abstract: In this paper, we propose ScoreMix, a novel yet simple data augmentation
strategy leveraging the score compositional properties of diffusion models to
enhance discriminator performance, particularly under scenarios with limited
labeled data. By convexly mixing the scores from different class-conditioned
trajectories during diffusion sampling, we generate challenging synthetic
samples that significantly improve discriminative capabilities in all studied
benchmarks. We systematically investigate class-selection strategies for mixing
and discover that greater performance gains arise when combining classes
distant in the discriminator's embedding space, rather than close in the
generator's condition space. Moreover, we empirically show that, under standard
metrics, the correlation between the generator's learned condition space and
the discriminator's embedding space is minimal. Our approach achieves notable
performance improvements without extensive parameter searches, demonstrating
practical advantages for training discriminative models while effectively
mitigating problems regarding collections of large datasets. Paper website:
https://parsa-ra.github.io/scoremix

</details>


### [129] [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/pdf/2506.10941)
*Leigang Qu, Feng Cheng, Ziyan Yang, Qi Zhao, Shanchuan Lin, Yichun Shi, Yicong Li, Wenjie Wang, Tat-Seng Chua, Lu Jiang*

Main category: cs.CV

TL;DR: A scalable method for in-context image editing using videos, trained with a block-causal diffusion transformer, achieves state-of-the-art results and shows versatility in multi-concept tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on task-specific pipelines and expert models, limiting scalability. This work explores learning directly from videos for more flexible and scalable in-context image editing.

Method: Uses a block-causal diffusion transformer trained on three proxy tasks: next-image prediction, current segmentation prediction, and next-segmentation prediction. Annotates videos as interleaved multimodal sequences.

Result: Demonstrates strong in-context image editing capabilities, achieving state-of-the-art results on multi-turn benchmarks. Shows promise in multi-concept composition, story generation, and chain-of-editing.

Conclusion: The model successfully learns in-context image editing from videos, offering a scalable and versatile alternative to task-specific pipelines.

Abstract: In-context image editing aims to modify images based on a contextual sequence
comprising text and previously generated images. Existing methods typically
depend on task-specific pipelines and expert models (e.g., segmentation and
inpainting) to curate training data. In this work, we explore whether an
in-context image editing model can be learned directly from videos. We
introduce a scalable approach to annotate videos as interleaved multimodal
sequences. To effectively learn from this data, we design a block-causal
diffusion transformer trained on three proxy tasks: next-image prediction,
current segmentation prediction, and next-segmentation prediction.
Additionally, we propose a novel multi-turn image editing benchmark to advance
research in this area. Extensive experiments demonstrate that our model
exhibits strong in-context image editing capabilities and achieves
state-of-the-art results on two multi-turn image editing benchmarks. Despite
being trained exclusively on videos, our model also shows promising abilities
in multi-concept composition, story generation, and chain-of-editing
applications.

</details>


### [130] [Rethinking Generative Human Video Coding with Implicit Motion Transformation](https://arxiv.org/pdf/2506.10453)
*Bolin Chen, Ru-Ling Liao, Jie Chen, Yan Ye*

Main category: cs.CV

TL;DR: The paper proposes an Implicit Motion Transformation (IMT) approach to improve Generative Human Video Coding (GHVC) by addressing challenges in complex human body motion patterns.


<details>
  <summary>Details</summary>
Motivation: Traditional generative video codecs struggle with human body videos due to complex motion patterns, leading to distortions and inaccurate motion.

Method: The paper introduces IMT to transform compact visual features into implicit motion guidance for reconstruction, avoiding explicit motion limitations.

Result: Experiments show IMT improves GHVC, enabling high-efficiency compression and high-fidelity synthesis.

Conclusion: IMT effectively addresses the challenges of human body video compression, outperforming explicit motion-based methods.

Abstract: Beyond traditional hybrid-based video codec, generative video codec could
achieve promising compression performance by evolving high-dimensional signals
into compact feature representations for bitstream compactness at the encoder
side and developing explicit motion fields as intermediate supervision for
high-quality reconstruction at the decoder side. This paradigm has achieved
significant success in face video compression. However, compared to facial
videos, human body videos pose greater challenges due to their more complex and
diverse motion patterns, i.e., when using explicit motion guidance for
Generative Human Video Coding (GHVC), the reconstruction results could suffer
severe distortions and inaccurate motion. As such, this paper highlights the
limitations of explicit motion-based approaches for human body video
compression and investigates the GHVC performance improvement with the aid of
Implicit Motion Transformation, namely IMT. In particular, we propose to
characterize complex human body signal into compact visual features and
transform these features into implicit motion guidance for signal
reconstruction. Experimental results demonstrate the effectiveness of the
proposed IMT paradigm, which can facilitate GHVC to achieve high-efficiency
compression and high-fidelity synthesis.

</details>


### [131] [California Crop Yield Benchmark: Combining Satellite Image, Climate, Evapotranspiration, and Soil Data Layers for County-Level Yield Forecasting of Over 70 Crops](https://arxiv.org/pdf/2506.10228)
*Hamid Kamangir, Mona Hajiesmaeeli, Mason Earles*

Main category: cs.CV

TL;DR: A multi-modal deep learning model is developed for accurate crop yield forecasting in California, integrating diverse data sources and achieving strong predictive performance.


<details>
  <summary>Details</summary>
Motivation: Accurate and timely crop yield forecasting is challenging due to complex environmental and climatic factors, despite extensive historical data.

Method: A multi-modal deep learning model integrates Landsat imagery, climate records, evapotranspiration, and soil properties, using stratified feature extraction and a timeseries encoder.

Result: The model achieves an R2 score of 0.76 on unseen test data, demonstrating strong predictive performance.

Conclusion: The benchmark dataset and model provide a foundation for improving agricultural forecasting, climate adaptation, and precision farming.

Abstract: California is a global leader in agricultural production, contributing 12.5%
of the United States total output and ranking as the fifth-largest food and
cotton supplier in the world. Despite the availability of extensive historical
yield data from the USDA National Agricultural Statistics Service, accurate and
timely crop yield forecasting remains a challenge due to the complex interplay
of environmental, climatic, and soil-related factors. In this study, we
introduce a comprehensive crop yield benchmark dataset covering over 70 crops
across all California counties from 2008 to 2022. The benchmark integrates
diverse data sources, including Landsat satellite imagery, daily climate
records, monthly evapotranspiration, and high-resolution soil properties. To
effectively learn from these heterogeneous inputs, we develop a multi-modal
deep learning model tailored for county-level, crop-specific yield forecasting.
The model employs stratified feature extraction and a timeseries encoder to
capture spatial and temporal dynamics during the growing season. Static inputs
such as soil characteristics and crop identity inform long-term variability.
Our approach achieves an overall R2 score of 0.76 across all crops of unseen
test dataset, highlighting strong predictive performance across California
diverse agricultural regions. This benchmark and modeling framework offer a
valuable foundation for advancing agricultural forecasting, climate adaptation,
and precision farming. The full dataset and codebase are publicly available at
our GitHub repository.

</details>


### [132] [Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer](https://arxiv.org/pdf/2309.14704)
*Zhihao Zhang, Yiwei Chen, Weizhan Zhang, Caixia Yan, Qinghua Zheng, Qi Wang, Wangdu Chen*

Main category: cs.CV

TL;DR: The paper proposes MFTR, a tile classification-based viewport prediction method using Multi-modal Fusion Transformer, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory-based viewport prediction methods lack robustness and oversimplify multi-modal input fusion, leading to error accumulation.

Method: MFTR uses transformer networks to extract long-range dependencies and fuse intra- and inter-modality relations for viewport prediction, classifying tiles into user-interested or not.

Result: MFTR achieves superior accuracy and overlap ratio on PVS-HM and Xu-Gaze datasets, with competitive computation efficiency.

Conclusion: MFTR's tile classification approach enhances robustness and interpretability in viewport prediction, outperforming state-of-the-art methods.

Abstract: Viewport prediction is a crucial aspect of tile-based 360 video streaming
system. However, existing trajectory based methods lack of robustness, also
oversimplify the process of information construction and fusion between
different modality inputs, leading to the error accumulation problem. In this
paper, we propose a tile classification based viewport prediction method with
Multi-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes
transformer-based networks to extract the long-range dependencies within each
modality, then mine intra- and inter-modality relations to capture the combined
impact of user historical inputs and video contents on future viewport
selection. In addition, MFTR categorizes future tiles into two categories: user
interested or not, and selects future viewport as the region that contains most
user interested tiles. Comparing with predicting head trajectories, choosing
future viewport based on tile's binary classification results exhibits better
robustness and interpretability. To evaluate our proposed MFTR, we conduct
extensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows
superior performance over state-of-the-art methods in terms of average
prediction accuracy and overlap ratio, also presents competitive computation
efficiency.

</details>


### [133] [Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Intermediate Feature Distance](https://arxiv.org/pdf/2506.10459)
*Chun Liu, Bingqian Zhu, Tao Xu, Zheng Zheng, Zheng Li, Wei Yang, Zhigang Han, Jiayao Wang*

Main category: cs.CV

TL;DR: The paper proposes a method to enhance adversarial example transferability for HSI classification by dividing images into blocks and using feature distancing loss.


<details>
  <summary>Details</summary>
Motivation: HSIs are vulnerable to adversarial attacks, and existing methods fail to fully utilize their structural and spectral information.

Method: Randomly divides HSI into blocks, applies transformations, and uses feature distancing loss to disrupt true class features.

Result: Adversarial examples achieve effective transferability on public HSI datasets and remain robust under defenses.

Conclusion: The proposed method improves transferability and attack performance for HSI adversarial examples.

Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, which pose
security challenges to hyperspectral image (HSI) classification technologies
based on DNNs. In the domain of natural images, numerous transfer-based
adversarial attack methods have been studied. However, HSIs differ from natural
images due to their high-dimensional and rich spectral information. Current
research on HSI adversarial examples remains limited and faces challenges in
fully utilizing the structural and feature information of images. To address
these issues, this paper proposes a novel method to enhance the transferability
of the adversarial examples for HSI classification models. First, while keeping
the image structure unchanged, the proposed method randomly divides the image
into blocks in both spatial and spectral dimensions. Then, various
transformations are applied on a block by block basis to increase input
diversity and mitigate overfitting. Second, a feature distancing loss targeting
intermediate layers is designed, which measures the distance between the
amplified features of the original examples and the features of the adversarial
examples as the primary loss, while the output layer prediction serves as the
auxiliary loss. This guides the perturbation to disrupt the features of the
true class in adversarial examples, effectively enhancing transferability.
Extensive experiments demonstrate that the adversarial examples generated by
the proposed method achieve effective transferability to black-box models on
two public HSI datasets. Furthermore, the method maintains robust attack
performance even under defense strategies.

</details>


### [134] [DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos](https://arxiv.org/pdf/2506.10242)
*Rajeev Yasarla, Shizhong Han, Hong Cai, Fatih Porikli*

Main category: cs.CV

TL;DR: DySS is a novel method for camera-based 3D object detection in BEV, using state-space learning and dynamic queries to improve efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Earlier methods rely on costly dense BEV features or inefficient sparse queries, especially with more video frames. DySS addresses these limitations.

Method: DySS employs a state-space model (SSM) for sequential feature processing, with auxiliary tasks (future prediction, masked reconstruction) to enhance SSM training. It dynamically updates queries via merge, remove, and split operations.

Result: DySS achieves 65.31 NDS and 57.4 mAP on nuScenes test split, outperforming state-of-the-art methods, with real-time inference at 33 FPS.

Conclusion: DySS offers superior detection performance and efficiency, making it a promising solution for autonomous driving perception.

Abstract: Camera-based 3D object detection in Bird's Eye View (BEV) is one of the most
important perception tasks in autonomous driving. Earlier methods rely on dense
BEV features, which are costly to construct. More recent works explore sparse
query-based detection. However, they still require a large number of queries
and can become expensive to run when more video frames are used. In this paper,
we propose DySS, a novel method that employs state-space learning and dynamic
queries. More specifically, DySS leverages a state-space model (SSM) to
sequentially process the sampled features over time steps. In order to
encourage the model to better capture the underlying motion and correspondence
information, we introduce auxiliary tasks of future prediction and masked
reconstruction to better train the SSM. The state of the SSM then provides an
informative yet efficient summarization of the scene. Based on the state-space
learned features, we dynamically update the queries via merge, remove, and
split operations, which help maintain a useful, lean set of detection queries
throughout the network. Our proposed DySS achieves both superior detection
performance and efficient inference. Specifically, on the nuScenes test split,
DySS achieves 65.31 NDS and 57.4 mAP, outperforming the latest state of the
art. On the val split, DySS achieves 56.2 NDS and 46.2 mAP, as well as a
real-time inference speed of 33 FPS.

</details>


### [135] [Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization](https://arxiv.org/pdf/2506.10463)
*Stone Yun, Alexander Wong*

Main category: cs.CV

TL;DR: The paper explores the impact of weight initialization on DNN quantization robustness and introduces GHN-QAT, a method using Graph Hypernetworks for quantization-aware initialization, improving accuracy even at low bit-widths.


<details>
  <summary>Details</summary>
Motivation: Limited exploration exists on improving initial conditions of DNN training for quantization, despite its known impact on model robustness and accuracy.

Method: The study examines weight initialization effects on CNN architectures and proposes GHN-QAT, which uses Graph Hypernetworks to predict quantized DNN parameters, enhancing quantization robustness.

Result: GHN-QAT improves quantized accuracy, notably for 4-bit and 2-bit quantization, outperforming random initialization.

Conclusion: GHN-QAT offers a novel approach to quantization-aware initialization, with potential for further integration into quantization-aware training.

Abstract: Deep neural network (DNN) quantization for fast, efficient inference has been
an important tool in limiting the cost of machine learning (ML) model
inference. Quantization-specific model development techniques such as
regularization, quantization-aware training, and quantization-robustness
penalties have served to greatly boost the accuracy and robustness of modern
DNNs. However, very little exploration has been done on improving the initial
conditions of DNN training for quantization. Just as random weight
initialization has been shown to significantly impact test accuracy of floating
point models, it would make sense that different weight initialization methods
impact quantization robustness of trained models. We present an extensive study
examining the effects of different weight initializations on a variety of CNN
building blocks commonly used in efficient CNNs. This analysis reveals that
even with varying CNN architectures, the choice of random weight initializer
can significantly affect final quantization robustness. Next, we explore a new
method for quantization-robust CNN initialization -- using Graph Hypernetworks
(GHN) to predict parameters of quantized DNNs. Besides showing that
GHN-predicted parameters are quantization-robust after regular float32
pretraining (of the GHN), we find that finetuning GHNs to predict parameters
for quantized graphs (which we call GHN-QAT) can further improve quantized
accuracy of CNNs. Notably, GHN-QAT shows significant accuracy improvements for
even 4-bit quantization and better-than-random accuracy for 2-bits. To the best
of our knowledge, this is the first in-depth study on quantization-aware DNN
weight initialization. GHN-QAT offers a novel approach to quantized DNN model
design. Future investigations, such as using GHN-QAT-initialized parameters for
quantization-aware training, can further streamline the DNN quantization
process.

</details>


### [136] [HalLoc: Token-level Localization of Hallucinations for Vision Language Models](https://arxiv.org/pdf/2506.10286)
*Eunkyu Park, Minyeong Kim, Gunhee Kim*

Main category: cs.CV

TL;DR: HalLoc is a dataset and baseline model for efficient, probabilistic hallucination detection in vision-language models, addressing reliability and computational challenges.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in vision-language models undermine reliability, and current detection methods are resource-intensive and lack nuance for real-world ambiguity.

Method: HalLoc provides 150K token-level annotated samples across VQA, instruction-following, and image captioning tasks, alongside a low-overhead baseline model.

Result: The dataset and model enable graded confidence detection, improving reliability without sacrificing efficiency.

Conclusion: HalLoc advances trustworthy vision-language models by offering a plug-and-play hallucination detection solution.

Abstract: Hallucinations pose a significant challenge to the reliability of large
vision-language models, making their detection essential for ensuring accuracy
in critical applications. Current detection methods often rely on
computationally intensive models, leading to high latency and resource demands.
Their definitive outcomes also fail to account for real-world scenarios where
the line between hallucinated and truthful information is unclear. To address
these issues, we propose HalLoc, a dataset designed for efficient,
probabilistic hallucination detection. It features 150K token-level annotated
samples, including hallucination types, across Visual Question Answering (VQA),
instruction-following, and image captioning tasks. This dataset facilitates the
development of models that detect hallucinations with graded confidence,
enabling more informed user interactions. Additionally, we introduce a baseline
model trained on HalLoc, offering low-overhead, concurrent hallucination
detection during generation. The model can be seamlessly integrated into
existing VLMs, improving reliability while preserving efficiency. The prospect
of a robust plug-and-play hallucination detection module opens new avenues for
enhancing the trustworthiness of vision-language models in real-world
applications. The HalLoc dataset and code are publicly available at:
https://github.com/dbsltm/cvpr25_halloc.

</details>


### [137] [PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications](https://arxiv.org/pdf/2505.01881)
*Trisanth Srinivasan, Santosh Patapati*

Main category: cs.CV

TL;DR: PhysNav-DG integrates sensor fusion and vision-language models for robust navigation, offering explanations and improved success rates.


<details>
  <summary>Details</summary>
Motivation: To enhance navigation in diverse environments by combining accurate state estimation with transparent decision-making.

Method: Dual-branch architecture with a modified Adaptive Kalman Filter and vision-language models like LLaMA 3.2 11B and BLIP-2.

Result: 20% improvement in navigation success rates, with clear and grounded explanations.

Conclusion: Connects semantic reasoning and geometric planning for safer, more trustworthy autonomous systems.

Abstract: Robust navigation in diverse environments and domains requires both accurate
state estimation and transparent decision making. We present PhysNav-DG, a
novel framework that integrates classical sensor fusion with the semantic power
of vision-language models. Our dual-branch architecture predicts navigation
actions from multi-sensor inputs while simultaneously generating detailed
chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically
adjusts its noise parameters based on environmental context. It leverages
several streams of raw sensor data along with semantic insights from models
such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the
MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,
autonomous driving, and social navigation tasks with ground-truth actions and
human-validated explanations. Extensive experiments and ablations show that
PhysNav-DG improves navigation success rates by over 20% and achieves high
efficiency, with explanations that are both highly grounded and clear. This
work connects high-level semantic reasoning and geometric planning for safer
and more trustworthy autonomous systems.

</details>


### [138] [Deep Learning-based Multi Project InP Wafer Simulation for Unsupervised Surface Defect Detection](https://arxiv.org/pdf/2506.10713)
*Emílio Dolgener Cantú, Rolf Klemens Wittmann, Oliver Abdeen, Patrick Wagner, Wojciech Samek, Moritz Baier, Sebastian Lapuschkin*

Main category: cs.CV

TL;DR: The paper proposes a deep-learning method to generate synthetic golden standards for InP wafer defect detection, outperforming traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Manual defect detection in InP wafer manufacturing is labor-intensive due to lack of golden standards.

Method: Uses Deep Neural Networks to simulate photo-realistic wafer images from CAD data, evaluating training objectives and image quality.

Result: Outperforms decision-tree-based methods, enabling efficient defect detection with simulated golden standards.

Conclusion: Demonstrates practical utility in template matching for surface defect detection.

Abstract: Quality management in semiconductor manufacturing often relies on template
matching with known golden standards. For Indium-Phosphide (InP) multi-project
wafer manufacturing, low production scale and high design variability lead to
such golden standards being typically unavailable. Defect detection, in turn,
is manual and labor-intensive. This work addresses this challenge by proposing
a methodology to generate a synthetic golden standard using Deep Neural
Networks, trained to simulate photo-realistic InP wafer images from CAD data.
We evaluate various training objectives and assess the quality of the simulated
images on both synthetic data and InP wafer photographs. Our
deep-learning-based method outperforms a baseline decision-tree-based approach,
enabling the use of a 'simulated golden die' from CAD plans in any user-defined
region of a wafer for more efficient defect detection. We apply our method to a
template matching procedure, to demonstrate its practical utility in surface
defect detection.

</details>


### [139] [Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation](https://arxiv.org/pdf/2506.10302)
*Hamzeh Asgharnezhad, Pegah Tabarisaadi, Abbas Khosravi, Roohallah Alizadehsani, U. Rajendra Acharya*

Main category: cs.CV

TL;DR: The paper evaluates deep learning models for skin cancer classification, focusing on transfer learning and uncertainty quantification. CLIP-based vision transformers with SVM performed best, while ensemble methods balanced accuracy and uncertainty handling.


<details>
  <summary>Details</summary>
Motivation: Improving skin cancer diagnosis through reliable and accurate deep learning models, addressing data scarcity and uncertainty awareness.

Method: Used transfer learning with pre-trained models (CLIP, ResNet50, DenseNet121, VGG16, EfficientNet-V2-Large) and traditional classifiers (SVM, XGBoost, logistic regression). Incorporated uncertainty quantification (MCD, Ensemble, EMCD) for reliability assessment.

Result: CLIP-based models with SVM achieved top performance. Ensemble methods provided a balance between accuracy and uncertainty handling, with EMCD being more sensitive to uncertain predictions.

Conclusion: Integrating uncertainty quantification enhances the reliability and trustworthiness of deep learning models in medical diagnosis.

Abstract: Accurate and reliable skin cancer diagnosis is critical for early treatment
and improved patient outcomes. Deep learning (DL) models have shown promise in
automating skin cancer classification, but their performance can be limited by
data scarcity and a lack of uncertainty awareness. In this study, we present a
comprehensive evaluation of DL-based skin lesion classification using transfer
learning and uncertainty quantification (UQ) on the HAM10000 dataset. In the
first phase, we benchmarked several pre-trained feature extractors-including
Contrastive Language-Image Pretraining (CLIP) variants, Residual Network-50
(ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual
Geometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range
of traditional classifiers such as Support Vector Machine (SVM), eXtreme
Gradient Boosting (XGBoost), and logistic regression. Our results show that
CLIP-based vision transformers, particularly LAION CLIP ViT-H/14 with SVM,
deliver the highest classification performance. In the second phase, we
incorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte
Carlo Dropout (EMCD) to assess not only prediction accuracy but also the
reliability of model outputs. We evaluated these models using uncertainty-aware
metrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen),
uncertainty specificity(USpe), and uncertainty precision(UPre). The results
demonstrate that ensemble methods offer a good trade-off between accuracy and
uncertainty handling, while EMCD is more sensitive to uncertain predictions.
This study highlights the importance of integrating UQ into DL-based medical
diagnosis to enhance both performance and trustworthiness in real-world
clinical applications.

</details>


### [140] [A Unit Enhancement and Guidance Framework for Audio-Driven Avatar Video Generation](https://arxiv.org/pdf/2505.03603)
*S. Z. Zhou, Y. B. Wang, J. F. Wu, T. Hu, J. N. Zhang*

Main category: cs.CV

TL;DR: PAHA introduces Parts-aware Audio-driven Human Animation, improving audio-motion consistency and visual quality via PAR and PCE methods, validated on the new CNAS dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing issues in audio-driven human animation like long inference time and poor quality in specific regions due to lack of localized supervision.

Method: Uses Parts-Aware Re-weighting (PAR) for dynamic loss adjustment and Parts Consistency Enhancement (PCE) for audio-visual classifiers, with Sequential Guidance (SG) and Differential Guidance (DG) for inference.

Result: PAHA outperforms existing methods in audio-motion alignment and video quality, validated by experiments and user studies.

Conclusion: PAHA advances audio-driven animation with efficient, high-quality results, supported by the new CNAS dataset.

Abstract: Audio-driven human animation technology is widely used in human-computer
interaction, and the emergence of diffusion models has further advanced its
development. Currently, most methods rely on multi-stage generation and
intermediate representations, resulting in long inference time and issues with
generation quality in specific foreground regions and audio-motion consistency.
These shortcomings are primarily due to the lack of localized fine-grained
supervised guidance. To address above challenges, we propose Parts-aware
Audio-driven Human Animation, PAHA, a unit enhancement and guidance framework
for audio-driven upper-body animation. We introduce two key methods:
Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR
dynamically adjusts regional training loss weights based on pose confidence
scores, effectively improving visual quality. PCE constructs and trains
diffusion-based regional audio-visual classifiers to improve the consistency of
motion and co-speech audio. Afterwards, we design two novel inference guidance
methods for the foregoing classifiers, Sequential Guidance (SG) and
Differential Guidance (DG), to balance efficiency and quality respectively.
Additionally, we build CNAS, the first public Chinese News Anchor Speech
dataset, to advance research and validation in this field. Extensive
experimental results and user studies demonstrate that PAHA significantly
outperforms existing methods in audio-motion alignment and video-related
evaluations. The codes and CNAS dataset will be released upon acceptance.

</details>


### [141] [Unsupervised Deformable Image Registration with Structural Nonparametric Smoothing](https://arxiv.org/pdf/2506.10813)
*Hang Zhang, Xiang Chen, Renjiu Hu, Rongguang Wang, Jinwei Zhang, Min Liu, Yaonan Wang, Gaolei Li, Xinxing Cheng, Jinming Duan*

Main category: cs.CV

TL;DR: SmoothProper is a neural module for deformable image registration (DIR) that enforces smoothness and message passing, addressing challenges like sparse features and large displacements in unsupervised DIR methods.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised DIR methods struggle with images containing sparse features (e.g., retinal vessels) due to unconstrained deformation fields post-training. SmoothProper aims to overcome these limitations by integrating smoothness and structural consistency.

Method: SmoothProper introduces a duality-based optimization layer with tailored interaction terms to propagate flow signals, enforce smoothness, and preserve structural consistency. It is model-agnostic and integrates into existing frameworks with minimal overhead.

Result: On a retinal vessel dataset, SmoothProper reduces registration error to 1.88 pixels on 2912x2912 images, effectively addressing aperture and large-displacement challenges.

Conclusion: SmoothProper is the first unsupervised DIR method to successfully tackle both sparse features and large displacements, offering a plug-and-play solution without hyperparameter tuning.

Abstract: Learning-based deformable image registration (DIR) accelerates alignment by
amortizing traditional optimization via neural networks. Label supervision
further enhances accuracy, enabling efficient and precise nonlinear alignment
of unseen scans. However, images with sparse features amid large smooth
regions, such as retinal vessels, introduce aperture and large-displacement
challenges that unsupervised DIR methods struggle to address. This limitation
occurs because neural networks predict deformation fields in a single forward
pass, leaving fields unconstrained post-training and shifting the
regularization burden entirely to network weights. To address these issues, we
introduce SmoothProper, a plug-and-play neural module enforcing smoothness and
promoting message passing within the network's forward pass. By integrating a
duality-based optimization layer with tailored interaction terms, SmoothProper
efficiently propagates flow signals across spatial locations, enforces
smoothness, and preserves structural consistency. It is model-agnostic,
seamlessly integrates into existing registration frameworks with minimal
parameter overhead, and eliminates regularizer hyperparameter tuning.
Preliminary results on a retinal vessel dataset exhibiting aperture and
large-displacement challenges demonstrate our method reduces registration error
to 1.88 pixels on 2912x2912 images, marking the first unsupervised DIR approach
to effectively address both challenges. The source code will be available at
https://github.com/tinymilky/SmoothProper.

</details>


### [142] [Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework](https://arxiv.org/pdf/2506.10328)
*Sadia Kamal, Tim Oates, Joy Wan*

Main category: cs.CV

TL;DR: A weakly supervised multimodal framework generates SOAP notes from lesion images and sparse clinical text, reducing manual effort and achieving performance comparable to advanced models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Manual SOAP note generation is labor-intensive and contributes to clinician burnout. The goal is to automate this process to reduce workload and reliance on large annotated datasets.

Method: Proposes a weakly supervised multimodal framework using lesion images and sparse clinical text to generate structured SOAP notes.

Result: The method performs comparably to GPT-4o, Claude, and DeepSeek Janus Pro in clinical relevance metrics. Introduces MedConceptEval and CCS for quality assessment.

Conclusion: The framework offers a scalable solution for clinical documentation, reducing clinician burden and the need for extensive annotations.

Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for
over $8 billion in annual healthcare expenditures. In clinical settings,
physicians document patient visits using detailed SOAP (Subjective, Objective,
Assessment, and Plan) notes. However, manually generating these notes is
labor-intensive and contributes to clinician burnout. In this work, we propose
a weakly supervised multimodal framework to generate clinically structured SOAP
notes from limited inputs, including lesion images and sparse clinical text.
Our approach reduces reliance on manual annotations, enabling scalable,
clinically grounded documentation while alleviating clinician burden and
reducing the need for large annotated data. Our method achieves performance
comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical
relevance metrics. To evaluate clinical quality, we introduce two novel metrics
MedConceptEval and Clinical Coherence Score (CCS) which assess semantic
alignment with expert medical concepts and input features, respectively.

</details>


### [143] [Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment](https://arxiv.org/pdf/2506.05384)
*Zhuoxuan Cai, Jian Zhang, Xinbin Yuan, Peng-Tao Jiang, Wenxiang Chen, Bowen Tang, Lujian Yao, Qiyuan Wang, Jinwen Chen, Bo Li*

Main category: cs.CV

TL;DR: The paper proposes a unified two-stage training framework for MLLMs to jointly optimize visual quality scoring and reasoning, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs treat quality scoring and reasoning as separate tasks, causing a trade-off between accuracy and interpretability. The goal is to unify these tasks for mutual reinforcement.

Method: A two-stage framework: (1) Cold-start stage with expert-designed prompts and cross-entropy loss. (2) Reinforcement learning fine-tuning with GRPO for joint optimization.

Result: Q-Ponder achieves 6.5% higher SRCC on cross-domain datasets and outperforms SOTA models in description accuracy and reasonableness.

Conclusion: The unified framework enhances both scoring accuracy and interpretability, demonstrating generalization potential across diverse tasks.

Abstract: Recent studies demonstrate that multimodal large language models (MLLMs) can
proficiently evaluate visual quality through interpretable assessments.
However, existing approaches typically treat quality scoring and reasoning
descriptions as separate tasks with disjoint optimization objectives, leading
to a trade-off: models adept at quality reasoning descriptions struggle with
precise score regression, while score-focused models lack interpretability.
This limitation hinders the full potential of MLLMs in visual quality
assessment, where accuracy and interpretability should be mutually reinforcing.
To address this, we propose a unified two-stage training framework comprising a
cold-start stage and a reinforcement learning-based fine-tuning stage.
Specifically, in the first stage, we distill high-quality data from a teacher
model through expert-designed prompts, initializing reasoning capabilities via
cross-entropy loss supervision. In the second stage, we introduce a novel
reward with Group Relative Policy Optimization (GRPO) to jointly optimize
scoring accuracy and reasoning consistency. We designate the models derived
from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show
that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score
regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain
datasets. Furthermore, Q-Ponder significantly outperforms description-based
SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in
description accuracy and reasonableness, demonstrating the generalization
potential over diverse tasks.

</details>


### [144] [Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions](https://arxiv.org/pdf/2506.10334)
*Deliang Wang, Chao Yang, Gaowei Chen*

Main category: cs.CV

TL;DR: The study explores Vision-Language Models (VLMs) for analyzing students' academic emotions via facial expressions, finding moderate success with Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct, especially in detecting happy and confused expressions.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised models struggle with generalization across contexts, prompting the need for zero-shot alternatives like VLMs.

Method: Two VLMs analyzed 5,000 images of academic emotions (confused, distracted, happy, neutral, tired) using zero-shot prompting.

Result: Qwen2.5-VL-7B-Instruct outperformed Llama-3.2-11B-Vision-Instruct, excelling in happy and confused expressions but failing in distracted behavior.

Conclusion: VLMs show promise for academic emotion analysis, particularly Qwen2.5-VL-7B-Instruct for identifying confusion, though distracted behavior detection remains a challenge.

Abstract: Students' academic emotions significantly influence their social behavior and
learning performance. Traditional approaches to automatically and accurately
analyze these emotions have predominantly relied on supervised machine learning
algorithms. However, these models often struggle to generalize across different
contexts, necessitating repeated cycles of data collection, annotation, and
training. The emergence of Vision-Language Models (VLMs) offers a promising
alternative, enabling generalization across visual recognition tasks through
zero-shot prompting without requiring fine-tuning. This study investigates the
potential of VLMs to analyze students' academic emotions via facial expressions
in an online learning environment. We employed two VLMs,
Llama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct, to analyze 5,000
images depicting confused, distracted, happy, neutral, and tired expressions
using zero-shot prompting. Preliminary results indicate that both models
demonstrate moderate performance in academic facial expression recognition,
with Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct.
Notably, both models excel in identifying students' happy emotions but fail to
detect distracted behavior. Additionally, Qwen2.5-VL-7B-Instruct exhibits
relatively high performance in recognizing students' confused expressions,
highlighting its potential for practical applications in identifying content
that causes student confusion.

</details>


### [145] [Glimpse: Generalized Locality for Scalable and Robust CT](https://arxiv.org/pdf/2401.00816)
*AmirEhsan Khorashadizadeh, Valentin Debarnot, Tianlin Liu, Ivan Dokmanić*

Main category: cs.CV

TL;DR: Glimpse, a local coordinate-based neural network for CT, outperforms CNNs on OOD samples, matches in-distribution performance, and uses minimal memory.


<details>
  <summary>Details</summary>
Motivation: Overfitting and poor generalization of multiscale CNNs in medical imaging, along with high computational costs, motivate a more efficient solution.

Method: Glimpse processes only local pixel-neighborhood measurements for reconstruction, reducing memory and computational demands.

Result: Glimpse excels on OOD samples, matches in-distribution performance, and uses only 5GB memory for 1024x1024 images.

Conclusion: Glimpse is a scalable, efficient, and plug-and-play solution for CT reconstruction, enabling advanced applications like correcting projection errors.

Abstract: Deep learning has become the state-of-the-art approach to medical tomographic
imaging. A common approach is to feed the result of a simple inversion, for
example the backprojection, to a multiscale convolutional neural network (CNN)
which computes the final reconstruction. Despite good results on
in-distribution test data, this often results in overfitting certain
large-scale structures and poor generalization on out-of-distribution (OOD)
samples. Moreover, the memory and computational complexity of multiscale CNNs
scale unfavorably with image resolution, making them impractical for
application at realistic clinical resolutions. In this paper, we introduce
Glimpse, a local coordinate-based neural network for computed tomography which
reconstructs a pixel value by processing only the measurements associated with
the neighborhood of the pixel. Glimpse significantly outperforms successful
CNNs on OOD samples, while achieving comparable or better performance on
in-distribution test data and maintaining a memory footprint almost independent
of image resolution; 5GB memory suffices to train on 1024x1024 images which is
orders of magnitude less than CNNs. Glimpse is fully differentiable and can be
used plug-and-play in arbitrary deep learning architectures, enabling feats
such as correcting miscalibrated projection orientations. Our implementation
and Google Colab demo can be accessed at
https://github.com/swing-research/Glimpse.

</details>


### [146] [PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting](https://arxiv.org/pdf/2506.10335)
*Lintao Xiang, Hongpei Zheng, Yating Huang, Qijun Yang, Hujun Yin*

Main category: cs.CV

TL;DR: A novel Point-wise Feature-Aware Gaussian Splatting framework improves 3DGS rendering quality from sparse views by leveraging stereo foundation models and self-attention-based feature enhancement.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods require many calibrated views; sparse inputs lead to overfitting and poor rendering quality.

Method: Uses stereo foundation models for pose estimation and dense point cloud reconstruction, encodes multiscale 2D features, and employs a self-attention network for point interaction.

Result: Outperforms NeRF-based methods and competes with state-of-the-art 3DGS in few-shot settings.

Conclusion: The proposed framework enables high-quality, real-time rendering from sparse views, addressing a key limitation of 3DGS.

Abstract: 3D Gaussian splatting (3DGS) is an innovative rendering technique that
surpasses the neural radiance field (NeRF) in both rendering speed and visual
quality by leveraging an explicit 3D scene representation. Existing 3DGS
approaches require a large number of calibrated views to generate a consistent
and complete scene representation. When input views are limited, 3DGS tends to
overfit the training views, leading to noticeable degradation in rendering
quality. To address this limitation, we propose a Point-wise Feature-Aware
Gaussian Splatting framework that enables real-time, high-quality rendering
from sparse training views. Specifically, we first employ the latest stereo
foundation model to estimate accurate camera poses and reconstruct a dense
point cloud for Gaussian initialization. We then encode the colour attributes
of each 3D Gaussian by sampling and aggregating multiscale 2D appearance
features from sparse inputs. To enhance point-wise appearance representation,
we design a point interaction network based on a self-attention mechanism,
allowing each Gaussian point to interact with its nearest neighbors. These
enriched features are subsequently decoded into Gaussian parameters through two
lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive
experiments on diverse benchmarks demonstrate that our method significantly
outperforms NeRF-based approaches and achieves competitive performance under
few-shot settings compared to the state-of-the-art 3DGS methods.

</details>


### [147] [A Survey on All-in-One Image Restoration: Taxonomy, Evaluation and Future Trends](https://arxiv.org/pdf/2410.15067)
*Junjun Jiang, Zengyuan Zuo, Gang Wu, Kui Jiang, Xianming Liu*

Main category: cs.CV

TL;DR: The paper surveys the emerging all-in-one image restoration (AiOIR) paradigm, which unifies the handling of multiple degradation types, offering a taxonomy, categorization of methods, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Traditional image restoration methods are limited to specific degradations, prompting the need for a unified approach like AiOIR to address complex real-world scenarios.

Method: The survey categorizes AiOIR methods by architecture, learning strategies, and improvements, and evaluates datasets, metrics, and experimental settings.

Result: The paper provides a comprehensive overview of AiOIR, highlighting key advances and challenges, and compares open-sourced methods.

Conclusion: This is the first in-depth review of AiOIR, aiming to inspire future research and practical applications in image restoration.

Abstract: Image restoration (IR) aims to recover high-quality images from inputs
degraded by various factors such as noise, blur, compression, and adverse
weather. Traditional IR methods typically focus on specific types of
degradation, which limits their effectiveness in real-world scenarios with
complex distortions. In response to this challenge, the all-in-one image
restoration (AiOIR) paradigm has recently emerged, offering a unified framework
that adeptly addresses multiple degradation types. These innovative models
enhance convenience and versatility by adaptively learning degradation-specific
features while simultaneously leveraging shared knowledge across diverse
corruptions. In this survey, we present the first comprehensive overview of
AiOIR, offering a taxonomy that organizes existing methods by architecture
innovations, learning strategies, and key improvements. We systematically
categorize prevailing approaches and critically assess the challenges these
models encounter, proposing future research directions to propel this rapidly
evolving field. Our survey begins with an introduction to the foundational
concepts of AiOIR models, followed by a categorization of typical scenarios. We
then highlight key architectural and algorithmic advances in AiOIR, aiming to
inspire continued innovation. To facilitate rigorous evaluation of existing
methods, we collate and summarize established datasets, evaluation metrics, and
common experimental settings. Finally, we present an objective comparison of
open-sourced methods, providing valuable insights for researchers and
practitioners. This paper stands as the first comprehensive and insightful
review of all-in-one image restoration. A related repository is available at
https://github.com/Harbinzzy/All-in-One-Image-Restoration-Survey.

</details>


### [148] [GeoCAD: Local Geometry-Controllable CAD Generation](https://arxiv.org/pdf/2506.10337)
*Zhanwei Zhang, Kaiyuan Liu, Junjie Liu, Wenxiao Wang, Binbin Lin, Liang Xie, Chen Shen, Deng Cai*

Main category: cs.CV

TL;DR: GeoCAD introduces a method for local geometry-controllable CAD generation, addressing limitations in existing methods by using a complementary captioning strategy and LLMs for precise local part modifications.


<details>
  <summary>Details</summary>
Motivation: Existing CAD generation methods struggle with following textual instructions or focusing on local parts, limiting design efficiency and user control.

Method: GeoCAD uses a complementary captioning strategy (vertex-based and VLLM-based) to annotate local parts, trains LLMs to predict masked parts, and allows user-specified modifications during inference.

Result: GeoCAD demonstrates effectiveness in generation quality, validity, and text-to-CAD consistency, with extensive experiments supporting its performance.

Conclusion: GeoCAD provides a user-friendly and precise solution for local geometry-controllable CAD generation, advancing design automation.

Abstract: Local geometry-controllable computer-aided design (CAD) generation aims to
modify local parts of CAD models automatically, enhancing design efficiency. It
also ensures that the shapes of newly generated local parts follow
user-specific geometric instructions (e.g., an isosceles right triangle or a
rectangle with one corner cut off). However, existing methods encounter
challenges in achieving this goal. Specifically, they either lack the ability
to follow textual instructions or are unable to focus on the local parts. To
address this limitation, we introduce GeoCAD, a user-friendly and local
geometry-controllable CAD generation method. Specifically, we first propose a
complementary captioning strategy to generate geometric instructions for local
parts. This strategy involves vertex-based and VLLM-based captioning for
systematically annotating simple and complex parts, respectively. In this way,
we caption $\sim$221k different local parts in total. In the training stage,
given a CAD model, we randomly mask a local part. Then, using its geometric
instruction and the remaining parts as input, we prompt large language models
(LLMs) to predict the masked part. During inference, users can specify any
local part for modification while adhering to a variety of predefined geometric
instructions. Extensive experiments demonstrate the effectiveness of GeoCAD in
generation quality, validity and text-to-CAD consistency. Code will be
available at https://github.com/Zhanwei-Z/GeoCAD.

</details>


### [149] [UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models](https://arxiv.org/pdf/2506.10342)
*Jun Yin, Jing Zhong, Peilin Li, Pengyu Zeng, Miao Zhang, Ran Luo, Shuai Lu*

Main category: cs.CV

TL;DR: The paper proposes a vision-language model framework to analyze urban streetscape styles, using Beijing and Shenzhen as case studies, achieving high accuracy in capturing stylistic differences.


<details>
  <summary>Details</summary>
Motivation: Understanding urban cultural and architectural differences is crucial for predicting city evolution, but traditional methods lack standardization and scalability.

Method: A multimodal research framework using vision-language models for automated, scalable analysis of urban streetscapes, including dataset construction (UrbanDiffBench) and framework development (UrbanSense).

Result: Over 80% of generated descriptions pass statistical tests, with high Phi scores (0.912 for cities, 0.833 for periods), confirming the method's effectiveness.

Conclusion: The framework offers a data-driven, objective approach to quantify urban style evolution, providing a scientific basis for future urban design.

Abstract: Urban cultures and architectural styles vary significantly across cities due
to geographical, chronological, historical, and socio-political factors.
Understanding these differences is essential for anticipating how cities may
evolve in the future. As representative cases of historical continuity and
modern innovation in China, Beijing and Shenzhen offer valuable perspectives
for exploring the transformation of urban streetscapes. However, conventional
approaches to urban cultural studies often rely on expert interpretation and
historical documentation, which are difficult to standardize across different
contexts. To address this, we propose a multimodal research framework based on
vision-language models, enabling automated and scalable analysis of urban
streetscape style differences. This approach enhances the objectivity and
data-driven nature of urban form research. The contributions of this study are
as follows: First, we construct UrbanDiffBench, a curated dataset of urban
streetscapes containing architectural images from different periods and
regions. Second, we develop UrbanSense, the first vision-language-model-based
framework for urban streetscape analysis, enabling the quantitative generation
and comparison of urban style representations. Third, experimental results show
that Over 80% of generated descriptions pass the t-test (p less than 0.05).
High Phi scores (0.912 for cities, 0.833 for periods) from subjective
evaluations confirm the method's ability to capture subtle stylistic
differences. These results highlight the method's potential to quantify and
interpret urban style evolution, offering a scientifically grounded lens for
future design.

</details>


### [150] [RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration](https://arxiv.org/pdf/2506.10344)
*Mina C. Moghadam, Alan Q. Wang, Omer Taub, Martin R. Prince, Mert R. Sabuncu*

Main category: cs.CV

TL;DR: RealKeyMorph (RKM) is a resolution-agnostic method for medical image registration that avoids resampling artifacts by using real-world coordinates.


<details>
  <summary>Details</summary>
Motivation: Existing methods resample images to fixed resolutions, introducing artifacts. RKM addresses this by working directly with raw data.

Method: RKM extends KeyMorph by training a network to learn keypoints in real-world coordinates, avoiding resampling.

Result: RKM shows advantages in registering 2D and 3D medical images with varying resolutions.

Conclusion: RKM provides a more accurate and artifact-free solution for multi-resolution medical image registration.

Abstract: Many real-world settings require registration of a pair of medical images
that differ in spatial resolution, which may arise from differences in image
acquisition parameters like pixel spacing, slice thickness, and field-of-view.
However, all previous machine learning-based registration techniques resample
images onto a fixed resolution. This is suboptimal because resampling can
introduce artifacts due to interpolation. To address this, we present
RealKeyMorph (RKM), a resolution-agnostic method for image registration. RKM is
an extension of KeyMorph, a registration framework which works by training a
network to learn corresponding keypoints for a given pair of images, after
which a closed-form keypoint matching step is used to derive the transformation
that aligns them. To avoid resampling and enable operating on the raw data, RKM
outputs keypoints in real-world coordinates of the scanner. To do this, we
leverage the affine matrix produced by the scanner (e.g., MRI machine) that
encodes the mapping from voxel coordinates to real world coordinates. By
transforming keypoints into real-world space and integrating this into the
training process, RKM effectively enables the extracted keypoints to be
resolution-agnostic. In our experiments, we demonstrate the advantages of RKM
on the registration task for orthogonal 2D stacks of abdominal MRIs, as well as
3D volumes with varying resolutions in brain datasets.

</details>


### [151] [Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation](https://arxiv.org/pdf/2506.10353)
*Runqi Ouyang, Haoyun Li, Zhenyuan Zhang, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xingang Wang*

Main category: cs.CV

TL;DR: Motion-R1 is a new framework for text-to-motion generation that uses a Chain-of-Thought mechanism to improve semantic alignment and motion synthesis, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-motion methods lack controllability, consistency, and diversity due to poor capture of linguistic structures and logical reasoning.

Method: Motion-R1 integrates a Chain-of-Thought mechanism to decompose text instructions into structured action paths and uses Group Relative Policy Optimization for training.

Result: Motion-R1 achieves superior performance in semantic understanding and long-term coherence, outperforming state-of-the-art methods.

Conclusion: Motion-R1 enhances motion generation by improving semantic guidance and logical reasoning, with code and data to be made public.

Abstract: Recent advances in large language models, especially in natural language
understanding and reasoning, have opened new possibilities for text-to-motion
generation. Although existing approaches have made notable progress in semantic
alignment and motion synthesis, they often rely on end-to-end mapping
strategies that fail to capture deep linguistic structures and logical
reasoning. Consequently, generated motions tend to lack controllability,
consistency, and diversity. To address these limitations, we propose Motion-R1,
a unified motion-language modeling framework that integrates a Chain-of-Thought
mechanism. By explicitly decomposing complex textual instructions into
logically structured action paths, Motion-R1 provides high-level semantic
guidance for motion generation, significantly enhancing the model's ability to
interpret and execute multi-step, long-horizon, and compositionally rich
commands. To train our model, we adopt Group Relative Policy Optimization, a
reinforcement learning algorithm designed for large models, which leverages
motion quality feedback to optimize reasoning chains and motion synthesis
jointly. Extensive experiments across multiple benchmark datasets demonstrate
that Motion-R1 achieves competitive or superior performance compared to
state-of-the-art methods, particularly in scenarios requiring nuanced semantic
understanding and long-term temporal coherence. The code, model and data will
be publicly available.

</details>


### [152] [FaceLiVT: Face Recognition using Linear Vision Transformer with Structural Reparameterization For Mobile Device](https://arxiv.org/pdf/2506.10361)
*Novendra Setyawan, Chi-Chia Sun, Mao-Hsiu Hsu, Wen-Kai Kuo, Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: FaceLiVT is a lightweight face recognition model combining CNN-Transformer architecture with Multi-Head Linear Attention (MHLA), achieving high accuracy and speed on mobile devices.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient face recognition model for resource-constrained platforms without compromising accuracy.

Method: Integrates hybrid CNN-Transformer architecture with MHLA and a reparameterized token mixer to reduce computational complexity.

Result: Outperforms state-of-the-art lightweight models, with 8.6x speed over EdgeFace and 21.2x over pure ViT-based models.

Conclusion: FaceLiVT provides an efficient, practical solution for real-time face recognition on mobile and edge devices.

Abstract: This paper introduces FaceLiVT, a lightweight yet powerful face recognition
model that integrates a hybrid Convolution Neural Network (CNN)-Transformer
architecture with an innovative and lightweight Multi-Head Linear Attention
(MHLA) mechanism. By combining MHLA alongside a reparameterized token mixer,
FaceLiVT effectively reduces computational complexity while preserving
competitive accuracy. Extensive evaluations on challenging benchmarks;
including LFW, CFP-FP, AgeDB-30, IJB-B, and IJB-C; highlight its superior
performance compared to state-of-the-art lightweight models. MHLA notably
improves inference speed, allowing FaceLiVT to deliver high accuracy with lower
latency on mobile devices. Specifically, FaceLiVT is 8.6 faster than EdgeFace,
a recent hybrid CNN-Transformer model optimized for edge devices, and 21.2
faster than a pure ViT-Based model. With its balanced design, FaceLiVT offers
an efficient and practical solution for real-time face recognition on
resource-constrained platforms.

</details>


### [153] [FSATFusion: Frequency-Spatial Attention Transformer for Infrared and Visible Image Fusion](https://arxiv.org/pdf/2506.10366)
*Tianpei Zhang, Jufeng Zhao, Yiming Zhu, Guangmang Cui, Yuhan Lyu*

Main category: cs.CV

TL;DR: FSATFusion, a novel end-to-end fusion network, outperforms existing methods by leveraging a frequency-spatial attention Transformer (FSAT) and improved Transformer module (ITM) to enhance feature extraction and fusion quality.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning approaches for infrared and visible images fusion (IVIF) suffer from information loss due to limited global context capture by convolutional neural networks.

Method: Proposes FSATFusion with FSAT module (frequency-spatial attention mechanism) and ITM to improve feature extraction and global context capture.

Result: Demonstrates superior fusion quality, efficiency, and generalization in experiments, including object detection tasks.

Conclusion: FSATFusion effectively addresses limitations of existing methods, offering high-quality fusion and strong performance in downstream applications.

Abstract: The infrared and visible images fusion (IVIF) is receiving increasing
attention from both the research community and industry due to its excellent
results in downstream applications. Existing deep learning approaches often
utilize convolutional neural networks to extract image features. However, the
inherently capacity of convolution operations to capture global context can
lead to information loss, thereby restricting fusion performance. To address
this limitation, we propose an end-to-end fusion network named the
Frequency-Spatial Attention Transformer Fusion Network (FSATFusion). The
FSATFusion contains a frequency-spatial attention Transformer (FSAT) module
designed to effectively capture discriminate features from source images. This
FSAT module includes a frequency-spatial attention mechanism (FSAM) capable of
extracting significant features from feature maps. Additionally, we propose an
improved Transformer module (ITM) to enhance the ability to extract global
context information of vanilla Transformer. We conducted both qualitative and
quantitative comparative experiments, demonstrating the superior fusion quality
and efficiency of FSATFusion compared to other state-of-the-art methods.
Furthermore, our network was tested on two additional tasks without any
modifications, to verify the excellent generalization capability of FSATFusion.
Finally, the object detection experiment demonstrated the superiority of
FSATFusion in downstream visual tasks. Our code is available at
https://github.com/Lmmh058/FSATFusion.

</details>


### [154] [Revisiting Transformers with Insights from Image Filtering](https://arxiv.org/pdf/2506.10371)
*Laziz U. Abdullaev, Maksim Tkachenko, Tan M. Nguyen*

Main category: cs.CV

TL;DR: The paper develops a unifying image processing framework to interpret self-attention in Transformers, including components like positional encoding and residual connections, while also improving model performance.


<details>
  <summary>Details</summary>
Motivation: The self-attention mechanism in Transformers lacks a deep theoretical foundation, prompting research to explain its success and limitations.

Method: The authors introduce a unifying image processing framework to interpret self-attention and its components, along with two architectural modifications.

Result: The framework not only enhances interpretability but also improves accuracy, robustness, and long-sequence understanding in tasks.

Conclusion: The work bridges gaps in understanding self-attention and demonstrates practical benefits of image processing-inspired modifications.

Abstract: The self-attention mechanism, a cornerstone of Transformer-based
state-of-the-art deep learning architectures, is largely heuristic-driven and
fundamentally challenging to interpret. Establishing a robust theoretical
foundation to explain its remarkable success and limitations has therefore
become an increasingly prominent focus in recent research. Some notable
directions have explored understanding self-attention through the lens of image
denoising and nonparametric regression. While promising, existing frameworks
still lack a deeper mechanistic interpretation of various architectural
components that enhance self-attention, both in its original formulation and
subsequent variants. In this work, we aim to advance this understanding by
developing a unifying image processing framework, capable of explaining not
only the self-attention computation itself but also the role of components such
as positional encoding and residual connections, including numerous later
variants. We also pinpoint potential distinctions between the two concepts
building upon our framework, and make effort to close this gap. We introduce
two independent architectural modifications within transformers. While our
primary objective is interpretability, we empirically observe that image
processing-inspired modifications can also lead to notably improved accuracy
and robustness against data contamination and adversaries across language and
vision tasks as well as better long sequence understanding.

</details>


### [155] [Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial](https://arxiv.org/pdf/2506.10386)
*Jerry Yan, Chinmay Talegaonkar, Nicholas Antipa, Eric Terrill, Sophia Merrifield*

Main category: cs.CV

TL;DR: PoseIDON, a computer vision pipeline, estimates object burial depth on the seafloor using deep learning and photogrammetry, achieving ~10 cm accuracy.


<details>
  <summary>Details</summary>
Motivation: Understanding burial dynamics of anthropogenic objects aids ecological risk assessment and pollutant transport analysis.

Method: Combines deep foundation model features with multiview photogrammetry to estimate object pose and seafloor orientation from ROV video. Burial depth is inferred via CAD model alignment and local planar seafloor fitting.

Result: Validated on 54 objects, achieving ~10 cm mean burial depth error and revealing sediment transport patterns.

Conclusion: PoseIDON enables scalable, non-invasive seafloor burial mapping, supporting environmental assessments.

Abstract: The burial state of anthropogenic objects on the seafloor provides insight
into localized sedimentation dynamics and is also critical for assessing
ecological risks, potential pollutant transport, and the viability of recovery
or mitigation strategies for hazardous materials such as munitions. Accurate
burial depth estimation from remote imagery remains difficult due to partial
occlusion, poor visibility, and object degradation. This work introduces a
computer vision pipeline, called PoseIDON, which combines deep foundation model
features with multiview photogrammetry to estimate six degrees of freedom
object pose and the orientation of the surrounding seafloor from ROV video.
Burial depth is inferred by aligning CAD models of the objects with observed
imagery and fitting a local planar approximation of the seafloor. The method is
validated using footage of 54 objects, including barrels and munitions,
recorded at a historic ocean dumpsite in the San Pedro Basin. The model
achieves a mean burial depth error of approximately 10 centimeters and resolves
spatial burial patterns that reflect underlying sediment transport processes.
This approach enables scalable, non-invasive mapping of seafloor burial and
supports environmental assessment at contaminated sites.

</details>


### [156] [DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Transformer and Mamba](https://arxiv.org/pdf/2506.10390)
*Shicheng Yin, Kaixuan Yin, Yang Liu, Weixing Chen, Liang Lin*

Main category: cs.CV

TL;DR: DART introduces adaptive patch partitioning for non-convolutional vision models, improving accuracy and efficiency by focusing on information-rich areas.


<details>
  <summary>Details</summary>
Motivation: Fixed-size patches in models like ViT and Vim often miss critical details or waste computation on background regions.

Method: DART uses learnable region scores and differentiable quantile operations to create content-dependent patches of varying sizes.

Result: DART boosts accuracy by 2.1% on DeiT (ImageNet-1K) with only 1M extra parameters and reduces FLOPs by 45%.

Conclusion: DART enhances performance and efficiency across models like DeiT, Vim, and VideoMamba with minimal computational overhead.

Abstract: Recently, non-convolutional models such as the Vision Transformer (ViT) and
Vision Mamba (Vim) have achieved remarkable performance in computer vision
tasks. However, their reliance on fixed-size patches often results in excessive
encoding of background regions and omission of critical local details,
especially when informative objects are sparsely distributed. To address this,
we introduce a fully differentiable Dynamic Adaptive Region Tokenizer (DART),
which adaptively partitions images into content-dependent patches of varying
sizes. DART combines learnable region scores with piecewise differentiable
quantile operations to allocate denser tokens to information-rich areas.
Despite introducing only approximately 1 million (1M) additional parameters,
DART improves accuracy by 2.1% on DeiT (ImageNet-1K). Unlike methods that
uniformly increase token density to capture fine-grained details, DART offers a
more efficient alternative, achieving 45% FLOPs reduction with superior
performance. Extensive experiments on DeiT, Vim, and VideoMamba confirm that
DART consistently enhances accuracy while incurring minimal or even reduced
computational overhead. Code is available at
https://github.com/HCPLab-SYSU/DART.

</details>


### [157] [ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion](https://arxiv.org/pdf/2506.10391)
*Yuanyi Song, Pumeng Lyu, Ben Fei, Fenghua Ling, Wanli Ouyang, Lei Bai*

Main category: cs.CV

TL;DR: ReconMOST is a data-driven guided diffusion model for multi-layer sea temperature reconstruction, addressing sparse data and cloud occlusion issues with high accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Accurate ocean reconstruction is vital for climate dynamics and marine research, but conventional and ML methods face limitations like sparse data and cloud occlusion.

Method: Pre-train an unconditional diffusion model on historical simulation data, then use sparse observational data to guide the reverse diffusion process for reconstruction.

Result: Achieves MSE values of 0.049 (guidance), 0.680 (reconstruction), and 0.633 (total), handling over 92.5% missing data with high accuracy.

Conclusion: ReconMOST effectively extends ML-based reconstruction to global, multi-layer settings, demonstrating robustness and superior generalization.

Abstract: Accurate reconstruction of ocean is essential for reflecting global climate
dynamics and supporting marine meteorological research. Conventional methods
face challenges due to sparse data, algorithmic complexity, and high
computational costs, while increasing usage of machine learning (ML) method
remains limited to reconstruction problems at the sea surface and local
regions, struggling with issues like cloud occlusion. To address these
limitations, this paper proposes ReconMOST, a data-driven guided diffusion
model framework for multi-layer sea temperature reconstruction. Specifically,
we first pre-train an unconditional diffusion model using a large collection of
historical numerical simulation data, enabling the model to attain physically
consistent distribution patterns of ocean temperature fields. During the
generation phase, sparse yet high-accuracy in-situ observational data are
utilized as guidance points for the reverse diffusion process, generating
accurate reconstruction results. Importantly, in regions lacking direct
observational data, the physically consistent spatial distribution patterns
learned during pre-training enable implicitly guided and physically plausible
reconstructions. Our method extends ML-based SST reconstruction to a global,
multi-layer setting, handling over 92.5% missing data while maintaining
reconstruction accuracy, spatial resolution, and superior generalization
capability. We pre-train our model on CMIP6 numerical simulation data and
conduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The
results of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on
reconstruction, and 0.633 on total, respectively, demonstrating the
effectiveness and robustness of the proposed framework. Our source code is
available at https://github.com/norsheep/ReconMOST.

</details>


### [158] [Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation](https://arxiv.org/pdf/2506.10395)
*Zhiyang Xu, Jiuhai Chen, Zhaojiang Lin, Xichen Pan, Lifu Huang, Tianyi Zhou, Madian Khabsa, Qifan Wang, Di Jin, Michihiro Yasunaga, Lili Yu, Xi Victoria Lin, Shaoliang Nie*

Main category: cs.CV

TL;DR: Pisces, a multimodal foundation model, addresses the performance gap between unified and specialized models in image understanding and generation using a decoupled visual encoding architecture and tailored training techniques.


<details>
  <summary>Details</summary>
Motivation: Unified models underperform compared to specialized ones due to differences in visual features and training processes for image understanding and generation.

Method: Introduces Pisces with a decoupled visual encoding architecture and optimized training techniques for multimodal generation, supported by data curation, pretraining, and finetuning.

Result: Pisces achieves competitive performance in both image understanding (20+ benchmarks) and generation (GenEval benchmark).

Conclusion: The model demonstrates the synergy between understanding and generation, highlighting the benefits of separate visual encoders for unified multimodal models.

Abstract: Recent advances in large language models (LLMs) have enabled multimodal
foundation models to tackle both image understanding and generation within a
unified framework. Despite these gains, unified models often underperform
compared to specialized models in either task. A key challenge in developing
unified models lies in the inherent differences between the visual features
needed for image understanding versus generation, as well as the distinct
training processes required for each modality. In this work, we introduce
Pisces, an auto-regressive multimodal foundation model that addresses this
challenge through a novel decoupled visual encoding architecture and tailored
training techniques optimized for multimodal generation. Combined with
meticulous data curation, pretraining, and finetuning, Pisces achieves
competitive performance in both image understanding and image generation. We
evaluate Pisces on over 20 public benchmarks for image understanding, where it
demonstrates strong performance across a wide range of tasks. Additionally, on
GenEval, a widely adopted benchmark for image generation, Pisces exhibits
robust generative capabilities. Our extensive analysis reveals the synergistic
relationship between image understanding and generation, and the benefits of
using separate visual encoders, advancing the field of unified multimodal
models.

</details>


### [159] [It's Not the Target, It's the Background: Rethinking Infrared Small Target Detection via Deep Patch-Free Low-Rank Representations](https://arxiv.org/pdf/2506.10425)
*Guoyi Zhang, Guangsheng Xu, Siyang Chen, Han Wang, Xiaohu Zhang*

Main category: cs.CV

TL;DR: LRRNet is a novel end-to-end IRSTD framework that leverages low-rank background properties for improved small target detection in complex infrared scenes.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting small infrared targets in complex backgrounds due to low SCR, diverse morphologies, and lack of visual cues motivates the need for a robust solution.

Method: LRRNet uses a compression-reconstruction-subtraction (CRS) paradigm to model low-rank background structures directly in the image domain, avoiding patch-based processing.

Result: LRRNet outperforms 38 state-of-the-art methods in accuracy, robustness, and speed (82.34 FPS), with resilience to noise confirmed on NoisySIRST.

Conclusion: LRRNet is the first end-to-end deep learning approach for low-rank background modeling, achieving real-time performance and superior detection results.

Abstract: Infrared small target detection (IRSTD) remains a long-standing challenge in
complex backgrounds due to low signal-to-clutter ratios (SCR), diverse target
morphologies, and the absence of distinctive visual cues. While recent deep
learning approaches aim to learn discriminative representations, the intrinsic
variability and weak priors of small targets often lead to unstable
performance. In this paper, we propose a novel end-to-end IRSTD framework,
termed LRRNet, which leverages the low-rank property of infrared image
backgrounds. Inspired by the physical compressibility of cluttered scenes, our
approach adopts a compression--reconstruction--subtraction (CRS) paradigm to
directly model structure-aware low-rank background representations in the image
domain, without relying on patch-based processing or explicit matrix
decomposition. To the best of our knowledge, this is the first work to directly
learn low-rank background structures using deep neural networks in an
end-to-end manner. Extensive experiments on multiple public datasets
demonstrate that LRRNet outperforms 38 state-of-the-art methods in terms of
detection accuracy, robustness, and computational efficiency. Remarkably, it
achieves real-time performance with an average speed of 82.34 FPS. Evaluations
on the challenging NoisySIRST dataset further confirm the model's resilience to
sensor noise. The source code will be made publicly available upon acceptance.

</details>


### [160] [MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment](https://arxiv.org/pdf/2506.10430)
*Shuo wang, Jihao Zhang*

Main category: cs.CV

TL;DR: MF2Summ is a multimodal video summarization model combining visual and auditory features, outperforming traditional single-modality methods with improved F1-scores.


<details>
  <summary>Details</summary>
Motivation: Traditional video summarization methods rely on single modalities, missing semantic richness. MF2Summ addresses this by integrating visual and auditory data.

Method: MF2Summ uses a five-stage process: feature extraction (GoogLeNet for visuals, SoundNet for audio), cross-modal attention, fusion, segment prediction, and key shot selection (NMS and KTS).

Result: MF2Summ achieves competitive performance, improving F1-scores by 1.9% and 0.6% on SumMe and TVSum datasets over DSNet.

Conclusion: MF2Summ effectively leverages multimodal data for richer video summarization, outperforming state-of-the-art methods.

Abstract: The rapid proliferation of online video content necessitates effective video
summarization techniques. Traditional methods, often relying on a single
modality (typically visual), struggle to capture the full semantic richness of
videos. This paper introduces MF2Summ, a novel video summarization model based
on multimodal content understanding, integrating both visual and auditory
information. MF2Summ employs a five-stage process: feature extraction,
cross-modal attention interaction, feature fusion, segment prediction, and key
shot selection. Visual features are extracted using a pre-trained GoogLeNet
model, while auditory features are derived using SoundNet. The core of our
fusion mechanism involves a cross-modal Transformer and an alignment-guided
self-attention Transformer, designed to effectively model inter-modal
dependencies and temporal correspondences. Segment importance, location, and
center-ness are predicted, followed by key shot selection using Non-Maximum
Suppression (NMS) and the Kernel Temporal Segmentation (KTS) algorithm.
Experimental results on the SumMe and TVSum datasets demonstrate that MF2Summ
achieves competitive performance, notably improving F1-scores by 1.9\% and
0.6\% respectively over the DSNet model, and performing favorably against other
state-of-the-art methods.

</details>


### [161] [MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models](https://arxiv.org/pdf/2506.10465)
*Yu Huang, Zelin Peng, Yichen Zhao, Piao Yang, Xiaokang Yang, Wei Shen*

Main category: cs.CV

TL;DR: The paper introduces MedSeg-R, an end-to-end framework for medical image reasoning segmentation, combining MLLMs' reasoning with precise mask generation, and presents the MedSeg-QA dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing models lack active reasoning for complex clinical questions and struggle with precise segmentation, limiting their diagnostic utility.

Method: MedSeg-R uses a global context understanding module and a pixel-level grounding module to interpret instructions and generate masks.

Result: MedSeg-R outperforms benchmarks, achieving high segmentation accuracy and interpretable analysis.

Conclusion: The framework and dataset advance medical image segmentation by integrating reasoning and precision, enhancing diagnostic applications.

Abstract: Medical image segmentation is crucial for clinical diagnosis, yet existing
models are limited by their reliance on explicit human instructions and lack
the active reasoning capabilities to understand complex clinical questions.
While recent advancements in multimodal large language models (MLLMs) have
improved medical question-answering (QA) tasks, most methods struggle to
generate precise segmentation masks, limiting their application in automatic
medical diagnosis. In this paper, we introduce medical image reasoning
segmentation, a novel task that aims to generate segmentation masks based on
complex and implicit medical instructions. To address this, we propose
MedSeg-R, an end-to-end framework that leverages the reasoning abilities of
MLLMs to interpret clinical questions while also capable of producing
corresponding precise segmentation masks for medical images. It is built on two
core components: 1) a global context understanding module that interprets
images and comprehends complex medical instructions to generate multi-modal
intermediate tokens, and 2) a pixel-level grounding module that decodes these
tokens to produce precise segmentation masks and textual responses.
Furthermore, we introduce MedSeg-QA, a large-scale dataset tailored for the
medical image reasoning segmentation task. It includes over 10,000 image-mask
pairs and multi-turn conversations, automatically annotated using large
language models and refined through physician reviews. Experiments show
MedSeg-R's superior performance across several benchmarks, achieving high
segmentation accuracy and enabling interpretable textual analysis of medical
images.

</details>


### [162] [LLMs Are Not Yet Ready for Deepfake Image Detection](https://arxiv.org/pdf/2506.10474)
*Shahroz Tariq, David Nguyen, M. A. P. Chamikara, Tingmin Wu, Alsharif Abuadbba, Kristen Moore*

Main category: cs.CV

TL;DR: VLMs like ChatGPT, Claude, Gemini, and Grok show promise for deepfake detection but lack standalone reliability, excelling in interpretability and contextual analysis.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of deepfake detection using vision-language models (VLMs) due to their emerging capabilities in visual reasoning.

Method: Zero-shot evaluation of four VLMs on three deepfake types (faceswap, reenactment, synthetic generation) using a diverse benchmark of authentic and manipulated images.

Result: VLMs detect surface-level anomalies and provide coherent explanations but fail as standalone systems, with vulnerabilities to stylistic elements and misleading patterns.

Conclusion: VLMs are not yet reliable for autonomous deepfake detection but could augment human expertise in hybrid or human-in-the-loop frameworks.

Abstract: The growing sophistication of deepfakes presents substantial challenges to
the integrity of media and the preservation of public trust. Concurrently,
vision-language models (VLMs), large language models enhanced with visual
reasoning capabilities, have emerged as promising tools across various domains,
sparking interest in their applicability to deepfake detection. This study
conducts a structured zero-shot evaluation of four prominent VLMs: ChatGPT,
Claude, Gemini, and Grok, focusing on three primary deepfake types: faceswap,
reenactment, and synthetic generation. Leveraging a meticulously assembled
benchmark comprising authentic and manipulated images from diverse sources, we
evaluate each model's classification accuracy and reasoning depth. Our analysis
indicates that while VLMs can produce coherent explanations and detect
surface-level anomalies, they are not yet dependable as standalone detection
systems. We highlight critical failure modes, such as an overemphasis on
stylistic elements and vulnerability to misleading visual patterns like vintage
aesthetics. Nevertheless, VLMs exhibit strengths in interpretability and
contextual analysis, suggesting their potential to augment human expertise in
forensic workflows. These insights imply that although general-purpose models
currently lack the reliability needed for autonomous deepfake detection, they
hold promise as integral components in hybrid or human-in-the-loop detection
frameworks.

</details>


### [163] [Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation](https://arxiv.org/pdf/2506.10488)
*Juan C. Martinez-Sevilla, Joan Cerveto-Serrano, Noelia Luna, Greg Chapman, Craig Sapp, David Rizo, Jorge Calvo-Zaragoza*

Main category: cs.CV

TL;DR: The paper introduces the Sheet Music Benchmark (SMB) dataset and the OMR Normalized Edit Distance (OMR-NED) metric for evaluating Optical Music Recognition (OMR) research, addressing gaps in OMR evaluation.


<details>
  <summary>Details</summary>
Motivation: To provide a standardized dataset and a detailed evaluation metric for OMR research, which lacks comprehensive benchmarking tools.

Method: Developed the SMB dataset with diverse musical textures and introduced OMR-NED, a fine-grained metric for error analysis in OMR.

Result: OMR-NED enables detailed comparisons of OMR performance, supported by baseline experiments using SMB.

Conclusion: The work fills a gap in OMR evaluation, offering tools for better benchmarking and comparison of OMR methods.

Abstract: In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six
hundred and eighty-five pages specifically designed to benchmark Optical Music
Recognition (OMR) research. SMB encompasses a diverse array of musical
textures, including monophony, pianoform, quartet, and others, all encoded in
Common Western Modern Notation using the Humdrum **kern format. Alongside SMB,
we introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored
explicitly for evaluating OMR performance. OMR-NED builds upon the widely-used
Symbol Error Rate (SER), offering a fine-grained and detailed error analysis
that covers individual musical elements such as note heads, beams, pitches,
accidentals, and other critical notation features. The resulting numeric score
provided by OMR-NED facilitates clear comparisons, enabling researchers and
end-users alike to identify optimal OMR approaches. Our work thus addresses a
long-standing gap in OMR evaluation, and we support our contributions with
baseline experiments using standardized SMB dataset splits for training and
assessing state-of-the-art methods.

</details>


### [164] [Class-Incremental Learning for Honey Botanical Origin Classification with Hyperspectral Images: A Study with Continual Backpropagation](https://arxiv.org/pdf/2506.10489)
*Guyang Zhang, Waleed Abdulla*

Main category: cs.CV

TL;DR: The paper explores class-incremental learning (CIL) techniques for distinguishing honey botanical origins, proposing a novel continual backpropagation (CB) method to enhance CIL performance by 1-7%.


<details>
  <summary>Details</summary>
Motivation: Accurate honey origin identification is vital for market fairness, but collecting all honey varieties for training is impractical, necessitating incremental learning solutions.

Method: Researchers compared CIL algorithms on a honey hyperspectral dataset and introduced CB, which reinitializes underused neurons to mitigate loss-of-plasticity.

Result: CB improved most CIL methods by 1-7%, demonstrating its effectiveness in enhancing incremental learning.

Conclusion: The CB technique is a promising enhancement for CIL, addressing plasticity loss and improving performance in honey origin classification.

Abstract: Honey is an important commodity in the global market. Honey types of
different botanical origins provide diversified flavors and health benefits,
thus having different market values. Developing accurate and effective
botanical origin-distinguishing techniques is crucial to protect consumers'
interests. However, it is impractical to collect all the varieties of honey
products at once to train a model for botanical origin differentiation.
Therefore, researchers developed class-incremental learning (CIL) techniques to
address this challenge. This study examined and compared multiple CIL
algorithms on a real-world honey hyperspectral imaging dataset. A novel
technique is also proposed to improve the performance of class-incremental
learning algorithms by combining with a continual backpropagation (CB)
algorithm. The CB method addresses the issue of loss-of-plasticity by
reinitializing a proportion of less-used hidden neurons to inject variability
into neural networks. Experiments showed that CB improved the performance of
most CIL methods by 1-7\%.

</details>


### [165] [Semantic Localization Guiding Segment Anything Model For Reference Remote Sensing Image Segmentation](https://arxiv.org/pdf/2506.10503)
*Shuyang Li, Shuang Wang, Zhuangzhuang Sun, Jing Xiao*

Main category: cs.CV

TL;DR: The paper introduces PSLG-SAM, a two-stage framework for Reference Remote Sensing Image Segmentation (RRSIS), addressing challenges like dense annotations and complex scenes by combining coarse localization and fine segmentation.


<details>
  <summary>Details</summary>
Motivation: Current RRSIS methods struggle with dense annotation needs and complex scene interpretation, prompting the need for a more efficient solution.

Method: PSLG-SAM splits RRSIS into coarse localization (using a visual grounding network) and fine segmentation (using SAM with clustering-based foreground points and mask optimization). The second stage can be train-free.

Result: PSLG-SAM outperforms state-of-the-art models on RRSIS-D and RRSIS-M datasets, with reduced annotation burden.

Conclusion: The proposed framework effectively addresses RRSIS challenges, offering improved performance and efficiency, supported by a new annotated dataset.

Abstract: The Reference Remote Sensing Image Segmentation (RRSIS) task generates
segmentation masks for specified objects in images based on textual
descriptions, which has attracted widespread attention and research interest.
Current RRSIS methods rely on multi-modal fusion backbones and semantic
segmentation heads but face challenges like dense annotation requirements and
complex scene interpretation. To address these issues, we propose a framework
named \textit{prompt-generated semantic localization guiding Segment Anything
Model}(PSLG-SAM), which decomposes the RRSIS task into two stages: coarse
localization and fine segmentation. In coarse localization stage, a visual
grounding network roughly locates the text-described object. In fine
segmentation stage, the coordinates from the first stage guide the Segment
Anything Model (SAM), enhanced by a clustering-based foreground point generator
and a mask boundary iterative optimization strategy for precise segmentation.
Notably, the second stage can be train-free, significantly reducing the
annotation data burden for the RRSIS task. Additionally, decomposing the RRSIS
task into two stages allows for focusing on specific region segmentation,
avoiding interference from complex scenes.We further contribute a high-quality,
multi-category manually annotated dataset. Experimental validation on two
datasets (RRSIS-D and RRSIS-M) demonstrates that PSLG-SAM achieves significant
performance improvements and surpasses existing state-of-the-art models.Our
code will be made publicly available.

</details>


### [166] [J-DDL: Surface Damage Detection and Localization System for Fighter Aircraft](https://arxiv.org/pdf/2506.10505)
*Jin Huang, Mingqiang Wei, Zikuan Li, Hangyu Qu, Wei Zhao, Xinyu Bai*

Main category: cs.CV

TL;DR: J-DDL is a smart system for detecting and localizing surface damage on fighter aircraft using 2D images and 3D point clouds, featuring a YOLO-based network with innovative components for efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual inspections of fighter aircraft are inefficient and inconsistent due to large surface areas and structural complexity, necessitating an automated solution.

Method: J-DDL integrates 2D images and 3D point clouds, using a YOLO-based network with Fasternet blocks, EMA modules, and Inner-CIOU loss for damage detection, then maps defects to 3D point clouds.

Result: The system improves inspection efficiency and coverage, validated by experiments, and includes the first public dataset for aircraft damage.

Conclusion: J-DDL advances automated aircraft inspection, offering a scalable and accurate solution for surface damage detection and localization.

Abstract: Ensuring the safety and extended operational life of fighter aircraft
necessitates frequent and exhaustive inspections. While surface defect
detection is feasible for human inspectors, manual methods face critical
limitations in scalability, efficiency, and consistency due to the vast surface
area, structural complexity, and operational demands of aircraft maintenance.
We propose a smart surface damage detection and localization system for fighter
aircraft, termed J-DDL. J-DDL integrates 2D images and 3D point clouds of the
entire aircraft surface, captured using a combined system of laser scanners and
cameras, to achieve precise damage detection and localization. Central to our
system is a novel damage detection network built on the YOLO architecture,
specifically optimized for identifying surface defects in 2D aircraft images.
Key innovations include lightweight Fasternet blocks for efficient feature
extraction, an optimized neck architecture incorporating Efficient Multiscale
Attention (EMA) modules for superior feature aggregation, and the introduction
of a novel loss function, Inner-CIOU, to enhance detection accuracy. After
detecting damage in 2D images, the system maps the identified anomalies onto
corresponding 3D point clouds, enabling accurate 3D localization of defects
across the aircraft surface. Our J-DDL not only streamlines the inspection
process but also ensures more comprehensive and detailed coverage of large and
complex aircraft exteriors. To facilitate further advancements in this domain,
we have developed the first publicly available dataset specifically focused on
aircraft damage. Experimental evaluations validate the effectiveness of our
framework, underscoring its potential to significantly advance automated
aircraft inspection technologies.

</details>


### [167] [CogStream: Context-guided Streaming Video Question Answering](https://arxiv.org/pdf/2506.10516)
*Zicheng Zhao, Kangyu Wang, Shijie Li, Rui Qian, Weiyao Lin, Huabin Liu*

Main category: cs.CV

TL;DR: The paper introduces CogStream, a task for streaming video reasoning, and proposes CogReasoner, a model that efficiently handles it by compressing visual streams and retrieving relevant historical context.


<details>
  <summary>Details</summary>
Motivation: Existing Vid-LLMs struggle with computational burden and irrelevant context in streaming video scenarios, necessitating a focused approach.

Method: Proposes CogReasoner, which uses visual stream compression and historical dialogue retrieval to identify relevant context.

Result: Extensive experiments validate the method's effectiveness.

Conclusion: CogStream and CogReasoner address key challenges in streaming video reasoning, with code to be released.

Abstract: Despite advancements in Video Large Language Models (Vid-LLMs) improving
multimodal understanding, challenges persist in streaming video reasoning due
to its reliance on contextual information. Existing paradigms feed all
available historical contextual information into Vid-LLMs, resulting in a
significant computational burden for visual data processing. Furthermore, the
inclusion of irrelevant context distracts models from key details. This paper
introduces a challenging task called Context-guided Streaming Video Reasoning
(CogStream), which simulates real-world streaming video scenarios, requiring
models to identify the most relevant historical contextual information to
deduce answers for questions about the current stream. To support CogStream, we
present a densely annotated dataset featuring extensive and hierarchical
question-answer pairs, generated by a semi-automatic pipeline. Additionally, we
present CogReasoner as a baseline model. It efficiently tackles this task by
leveraging visual stream compression and historical dialogue retrieval.
Extensive experiments prove the effectiveness of this method. Code will be
released soon.

</details>


### [168] [ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation](https://arxiv.org/pdf/2506.10524)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: ALBERT is an instance segmentation model for car damage and part segmentation, using advanced localization to distinguish real/fake damages and segment car parts. It performs well on a large annotated dataset.


<details>
  <summary>Details</summary>
Motivation: To improve automotive inspection by accurately identifying and segmenting car damages and parts, including distinguishing real from fake damages.

Method: Leverages Bidirectional Encoder Representations with advanced localization mechanisms, trained on a large-scale annotated dataset (26 damage types, 7 fake variants, 61 car parts).

Result: Demonstrates strong performance in segmentation accuracy and damage classification.

Conclusion: ALBERT paves the way for intelligent automotive inspection and assessment applications.

Abstract: This paper introduces ALBERT, an instance segmentation model specifically
designed for comprehensive car damage and part segmentation. Leveraging the
power of Bidirectional Encoder Representations, ALBERT incorporates advanced
localization mechanisms to accurately identify and differentiate between real
and fake damages, as well as segment individual car parts. The model is trained
on a large-scale, richly annotated automotive dataset that categorizes damage
into 26 types, identifies 7 fake damage variants, and segments 61 distinct car
parts. Our approach demonstrates strong performance in both segmentation
accuracy and damage classification, paving the way for intelligent automotive
inspection and assessment applications.

</details>


### [169] [SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance](https://arxiv.org/pdf/2506.10528)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: SLICK is a framework for precise car damage segmentation using structural priors and domain knowledge, with five key components for accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address real-world automotive inspection challenges like occlusion, deformation, and noise in damage detection.

Method: Five components: Selective Part Segmentation, Localization-Aware Attention, Instance-Sensitive Refinement, Cross-Channel Calibration, and Knowledge Fusion Module.

Result: Superior segmentation performance and robustness on large-scale datasets.

Conclusion: SLICK is effective for insurance and automotive inspection workflows.

Abstract: We present SLICK, a novel framework for precise and robust car damage
segmentation that leverages structural priors and domain knowledge to tackle
real-world automotive inspection challenges. SLICK introduces five key
components: (1) Selective Part Segmentation using a high-resolution semantic
backbone guided by structural priors to achieve surgical accuracy in segmenting
vehicle parts even under occlusion, deformation, or paint loss; (2)
Localization-Aware Attention blocks that dynamically focus on damaged regions,
enhancing fine-grained damage detection in cluttered and complex street scenes;
(3) an Instance-Sensitive Refinement head that leverages panoptic cues and
shape priors to disentangle overlapping or adjacent parts, enabling precise
boundary alignment; (4) Cross-Channel Calibration through multi-scale channel
attention that amplifies subtle damage signals such as scratches and dents
while suppressing noise like reflections and decals; and (5) a Knowledge Fusion
Module that integrates synthetic crash data, part geometry, and real-world
insurance datasets to improve generalization and handle rare cases effectively.
Experiments on large-scale automotive datasets demonstrate SLICK's superior
segmentation performance, robustness, and practical applicability for insurance
and automotive inspection workflows.

</details>


### [170] [ContextRefine-CLIP for EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2025](https://arxiv.org/pdf/2506.10550)
*Jing He, Yiqing Wang, Lingling Li, Kexin Zhang, Puhua Chen*

Main category: cs.CV

TL;DR: CR-CLIP enhances visual-textual retrieval with cross-modal attention, achieving top performance on EPIC-KITCHENS-100.


<details>
  <summary>Details</summary>
Motivation: Improve multi-instance retrieval by refining visual-textual interactions for better context-aware representations.

Method: Uses a cross-modal attention flow module on AVION for bidirectional feature refinement and Symmetric Multi-Similarity Loss for optimization.

Result: Achieves 66.78mAP and 82.08nDCG on EPIC-KITCHENS-100, surpassing baselines.

Conclusion: CR-CLIP is effective for cross-modal retrieval, with open-source code available.

Abstract: This report presents ContextRefine-CLIP (CR-CLIP), an efficient model for
visual-textual multi-instance retrieval tasks. The approach is based on the
dual-encoder AVION, on which we introduce a cross-modal attention flow module
to achieve bidirectional dynamic interaction and refinement between visual and
textual features to generate more context-aware joint representations. For
soft-label relevance matrices provided in tasks such as EPIC-KITCHENS-100,
CR-CLIP can work with Symmetric Multi-Similarity Loss to achieve more accurate
semantic alignment and optimization using the refined features. Without using
ensemble learning, the CR-CLIP model achieves 66.78mAP and 82.08nDCG on the
EPIC-KITCHENS-100 public leaderboard, which significantly outperforms the
baseline model and fully validates its effectiveness in cross-modal retrieval.
The code will be released open-source on
https://github.com/delCayr/ContextRefine-Clip

</details>


### [171] [From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations](https://arxiv.org/pdf/2506.10559)
*Yutong Zhou, Masahiro Ryo*

Main category: cs.CV

TL;DR: A visual-to-causal framework transforms species images into interpretable causal insights about habitat preferences, integrating AI and ecological modeling for human-readable explanations.


<details>
  <summary>Details</summary>
Motivation: Understanding species habitat preferences is crucial for ecology and conservation, but existing methods are fragmented and inaccessible to non-specialists.

Method: The framework combines species recognition, occurrence retrieval, pseudo-absence sampling, climate data extraction, causal inference, and AI-generated explanations.

Result: Early results demonstrate the framework's potential to describe species habitats in understandable language using multimodal AI.

Conclusion: The proposed system offers a novel, accessible approach to ecological modeling and species habitat analysis.

Abstract: Explaining why the species lives at a particular location is important for
understanding ecological systems and conserving biodiversity. However, existing
ecological workflows are fragmented and often inaccessible to non-specialists.
We propose an end-to-end visual-to-causal framework that transforms a species
image into interpretable causal insights about its habitat preference. The
system integrates species recognition, global occurrence retrieval,
pseudo-absence sampling, and climate data extraction. We then discover causal
structures among environmental features and estimate their influence on species
occurrence using modern causal inference methods. Finally, we generate
statistically grounded, human-readable causal explanations from structured
templates and large language models. We demonstrate the framework on a bee and
a flower species and report early results as part of an ongoing project,
showing the potential of the multimodal AI assistant backed up by a recommended
ecological modeling practice for describing species habitat in
human-understandable language.

</details>


### [172] [Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics](https://arxiv.org/pdf/2506.10564)
*Imanol Solano, Julian Fierrez, Aythami Morales, Alejandro Peña, Ruben Tolosana, Francisco Zamora-Martinez, Javier San Agustin*

Main category: cs.CV

TL;DR: The paper introduces the Comprehensive Equity Index (CEI) to detect subtle demographic biases in face recognition systems, outperforming existing metrics.


<details>
  <summary>Details</summary>
Motivation: Existing metrics fail to detect nuanced biases in face recognition systems, especially in distribution tails.

Method: CEI analyzes genuine and impostor score distributions separately, focusing on tail probabilities and overall shapes. CEI^A automates this process.

Result: CEI effectively detects biases where other methods fail, as shown in experiments with diverse datasets and models.

Conclusion: CEI is a robust tool for fairness assessment in face recognition, with broader applicability to other distribution-tail analysis problems.

Abstract: Demographic bias in high-performance face recognition (FR) systems often
eludes detection by existing metrics, especially with respect to subtle
disparities in the tails of the score distribution. We introduce the
Comprehensive Equity Index (CEI), a novel metric designed to address this
limitation. CEI uniquely analyzes genuine and impostor score distributions
separately, enabling a configurable focus on tail probabilities while also
considering overall distribution shapes. Our extensive experiments (evaluating
state-of-the-art FR systems, intentionally biased models, and diverse datasets)
confirm CEI's superior ability to detect nuanced biases where previous methods
fall short. Furthermore, we present CEI^A, an automated version of the metric
that enhances objectivity and simplifies practical application. CEI provides a
robust and sensitive tool for operational FR fairness assessment. The proposed
methods have been developed particularly for bias evaluation in face biometrics
but, in general, they are applicable for comparing statistical distributions in
any problem where one is interested in analyzing the distribution tails.

</details>


### [173] [LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System](https://arxiv.org/pdf/2506.10567)
*Hongbeen Park, Minjeong Park, Giljoo Nam, Jinkyu Kim*

Main category: cs.CV

TL;DR: LRSLAM, a new visual SLAM model using low-rank tensor decomposition, outperforms existing methods in efficiency, memory usage, and accuracy for large-scale scenes.


<details>
  <summary>Details</summary>
Motivation: Dense visual SLAM faces challenges in real-time performance, robustness, and scalability, especially with neural implicit representations. Existing methods like ESLAM struggle with memory growth.

Method: LRSLAM employs low-rank tensor decomposition (Six-axis and CP decompositions) for improved convergence, memory efficiency, and reconstruction/localization quality.

Result: LRSLAM shows superior performance in parameter efficiency, processing time, and accuracy across diverse indoor RGB-D datasets.

Conclusion: LRSLAM addresses key SLAM challenges effectively, offering a promising solution with public code availability.

Abstract: Simultaneous Localization and Mapping (SLAM) has been crucial across various
domains, including autonomous driving, mobile robotics, and mixed reality.
Dense visual SLAM, leveraging RGB-D camera systems, offers advantages but faces
challenges in achieving real-time performance, robustness, and scalability for
large-scale scenes. Recent approaches utilizing neural implicit scene
representations show promise but suffer from high computational costs and
memory requirements. ESLAM introduced a plane-based tensor decomposition but
still struggled with memory growth. Addressing these challenges, we propose a
more efficient visual SLAM model, called LRSLAM, utilizing low-rank tensor
decomposition methods. Our approach, leveraging the Six-axis and CP
decompositions, achieves better convergence rates, memory efficiency, and
reconstruction/localization quality than existing state-of-the-art approaches.
Evaluation across diverse indoor RGB-D datasets demonstrates LRSLAM's superior
performance in terms of parameter efficiency, processing time, and accuracy,
retaining reconstruction and localization quality. Our code will be publicly
available upon publication.

</details>


### [174] [DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers](https://arxiv.org/pdf/2506.10568)
*Lizhen Wang, Zhurong Xia, Tianshu Hu, Pengrui Wang, Pengfei Wang, Zerong Zheng, Ming Zhou*

Main category: cs.CV

TL;DR: A DiT-based framework for generating realistic human-product demonstration videos, preserving identities and spatial relationships.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to maintain human-product identities and spatial relationships, leading to unrealistic outputs.

Method: Uses a Diffusion Transformer (DiT) with paired reference info, masked cross-attention, 3D body mesh, and product bounding boxes for motion guidance. Structured text encoding ensures 3D consistency.

Result: Outperforms state-of-the-art in identity preservation and realistic motion generation.

Conclusion: The proposed framework effectively addresses identity and spatial challenges in human-product video generation.

Abstract: In e-commerce and digital marketing, generating high-fidelity human-product
demonstration videos is important for effective product presentation. However,
most existing frameworks either fail to preserve the identities of both humans
and products or lack an understanding of human-product spatial relationships,
leading to unrealistic representations and unnatural interactions. To address
these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our
method simultaneously preserves human identities and product-specific details,
such as logos and textures, by injecting paired human-product reference
information and utilizing an additional masked cross-attention mechanism. We
employ a 3D body mesh template and product bounding boxes to provide precise
motion guidance, enabling intuitive alignment of hand gestures with product
placements. Additionally, structured text encoding is used to incorporate
category-level semantics, enhancing 3D consistency during small rotational
changes across frames. Trained on a hybrid dataset with extensive data
augmentation strategies, our approach outperforms state-of-the-art techniques
in maintaining the identity integrity of both humans and products and
generating realistic demonstration motions. Project page:
https://submit2025-dream.github.io/DreamActor-H1/.

</details>


### [175] [Improving Medical Visual Representation Learning with Pathological-level Cross-Modal Alignment and Correlation Exploration](https://arxiv.org/pdf/2506.10573)
*Jun Wang, Lixing Zhu, Xiaohan Yu, Abhir Bhalerao, Yulan He*

Main category: cs.CV

TL;DR: The paper introduces PLACE, a framework for pathological-level alignment in medical image-report pairs, enhancing fine-grained details without extra annotations.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity and complex discourse in medical reports by improving pathological-level consistency in cross-modal learning.

Method: Proposes Pathological-Level Cross-Modal Alignment (PCMA) and a Visual Pathology Observation Extractor, plus a proxy task for correlation exploration.

Result: Achieves state-of-the-art performance in classification, retrieval, segmentation, detection, and report generation.

Conclusion: PLACE effectively aligns pathology observations and enriches details, advancing medical visual representation learning.

Abstract: Learning medical visual representations from image-report pairs through joint
learning has garnered increasing research attention due to its potential to
alleviate the data scarcity problem in the medical domain. The primary
challenges stem from the lengthy reports that feature complex discourse
relations and semantic pathologies. Previous works have predominantly focused
on instance-wise or token-wise cross-modal alignment, often neglecting the
importance of pathological-level consistency. This paper presents a novel
framework PLACE that promotes the Pathological-Level Alignment and enriches the
fine-grained details via Correlation Exploration without additional human
annotations. Specifically, we propose a novel pathological-level cross-modal
alignment (PCMA) approach to maximize the consistency of pathology observations
from both images and reports. To facilitate this, a Visual Pathology
Observation Extractor is introduced to extract visual pathological observation
representations from localized tokens. The PCMA module operates independently
of any external disease annotations, enhancing the generalizability and
robustness of our methods. Furthermore, we design a proxy task that enforces
the model to identify correlations among image patches, thereby enriching the
fine-grained details crucial for various downstream tasks. Experimental results
demonstrate that our proposed framework achieves new state-of-the-art
performance on multiple downstream tasks, including classification,
image-to-text retrieval, semantic segmentation, object detection and report
generation.

</details>


### [176] [Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning](https://arxiv.org/pdf/2506.10575)
*Chun-Mei Feng, Kai Yu, Xinxing Xu, Salman Khan, Rick Siow Mong Goh, Wangmeng Zuo, Yong Liu*

Main category: cs.CV

TL;DR: T2I-PAL reduces the modality gap in CLIP by generating images from text captions, enhancing multi-label recognition with heatmaps and prototypes, and combining prompt tuning with adapter learning for better performance.


<details>
  <summary>Details</summary>
Motivation: The modality gap in CLIP limits image recognition performance when using text captions for fine-tuning. T2I-PAL aims to address this gap without requiring fully annotated training images.

Method: T2I-PAL generates images from text captions using pre-trained models, incorporates class-wise heatmaps and learnable prototypes, and combines prompt tuning with adapter learning.

Result: T2I-PAL improves recognition performance by 3.47% on average over state-of-the-art methods on benchmarks like MS-COCO, VOC2007, and NUS-WIDE.

Conclusion: T2I-PAL effectively bridges the modality gap, reduces annotation workload, and integrates seamlessly with CLIP, offering superior performance in multi-label recognition.

Abstract: Benefited from image-text contrastive learning, pre-trained vision-language
models, e.g., CLIP, allow to direct leverage texts as images (TaI) for
parameter-efficient fine-tuning (PEFT). While CLIP is capable of making image
features to be similar to the corresponding text features, the modality gap
remains a nontrivial issue and limits image recognition performance of TaI.
Using multi-label image recognition (MLR) as an example, we present a novel
method, called T2I-PAL to tackle the modality gap issue when using only text
captions for PEFT. The core design of T2I-PAL is to leverage pre-trained
text-to-image generation models to generate photo-realistic and diverse images
from text captions, thereby reducing the modality gap. To further enhance MLR,
T2I-PAL incorporates a class-wise heatmap and learnable prototypes. This
aggregates local similarities, making the representation of local visual
features more robust and informative for multi-label recognition. For better
PEFT, we further combine both prompt tuning and adapter learning to enhance
classification performance. T2I-PAL offers significant advantages: it
eliminates the need for fully semantically annotated training images, thereby
reducing the manual annotation workload, and it preserves the intrinsic mode of
the CLIP model, allowing for seamless integration with any existing CLIP
framework. Extensive experiments on multiple benchmarks, including MS-COCO,
VOC2007, and NUS-WIDE, show that our T2I-PAL can boost recognition performance
by 3.47% in average above the top-ranked state-of-the-art methods.

</details>


### [177] [Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres](https://arxiv.org/pdf/2506.10576)
*Muskan Dosi, Chiranjeev Chiranjeev, Kartik Thakral, Mayank Vatsa, Richa Singh*

Main category: cs.CV

TL;DR: HyperSphereDiff improves diffusion models for hyperspherical data by using directional noise, preserving class geometry and angular uncertainty.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models use isotropic Gaussian noise, which is suboptimal for non-Euclidean data like hyperspherical manifolds, losing angular subtleties.

Method: Introduces HyperSphereDiff, aligning hyperspherical structures with directional noise to preserve class geometry and angular uncertainty.

Result: Theoretical and empirical validation shows improved generative performance, better preserving hyperspherical manifold geometry.

Conclusion: HyperSphereDiff enhances diffusion models for hyperspherical data, aligning generative processes with intrinsic geometry for more accurate results.

Abstract: Do contemporary diffusion models preserve the class geometry of
hyperspherical data? Standard diffusion models rely on isotropic Gaussian noise
in the forward process, inherently favoring Euclidean spaces. However, many
real-world problems involve non-Euclidean distributions, such as hyperspherical
manifolds, where class-specific patterns are governed by angular geometry
within hypercones. When modeled in Euclidean space, these angular subtleties
are lost, leading to suboptimal generative performance. To address this
limitation, we introduce HyperSphereDiff to align hyperspherical structures
with directional noise, preserving class geometry and effectively capturing
angular uncertainty. We demonstrate both theoretically and empirically that
this approach aligns the generative process with the intrinsic geometry of
hyperspherical data, resulting in more accurate and geometry-aware generative
models. We evaluate our framework on four object datasets and two face
datasets, showing that incorporating angular uncertainty better preserves the
underlying hyperspherical manifold. Resources are available at:
{https://github.com/IAB-IITJ/Harmonizing-Geometry-and-Uncertainty-Diffusion-with-Hyperspheres/}

</details>


### [178] [Rethinking Random Masking in Self Distillation on ViT](https://arxiv.org/pdf/2506.10582)
*Jihyeon Seong, Hyunkyung Han*

Main category: cs.CV

TL;DR: The paper explores the role of random masking in ViTs within the DINO self-distillation framework, proposing an asymmetric masking strategy to retain critical information while improving robustness.


<details>
  <summary>Details</summary>
Motivation: Random masking in self-distillation frameworks like DINO may remove important semantic information, prompting the need for more informed masking strategies.

Method: The study applies random masking only to the student's global view in DINO, preserving local views and the teacher's global view to retain clean supervision.

Result: Experiments on DINO-Tiny and mini-ImageNet show that asymmetric random masking enhances attention maps and downstream performance.

Conclusion: The asymmetric masking strategy improves robustness and fine-grained attention in ViTs, benefiting downstream tasks.

Abstract: Vision Transformers (ViTs) have demonstrated remarkable performance across a
wide range of vision tasks. In particular, self-distillation frameworks such as
DINO have contributed significantly to these advances. Within such frameworks,
random masking is often utilized to improve training efficiency and introduce
regularization. However, recent studies have raised concerns that
indiscriminate random masking may inadvertently eliminate critical semantic
information, motivating the development of more informed masking strategies. In
this study, we explore the role of random masking in the self-distillation
setting, focusing on the DINO framework. Specifically, we apply random masking
exclusively to the student's global view, while preserving the student's local
views and the teacher's global view in their original, unmasked forms. This
design leverages DINO's multi-view augmentation scheme to retain clean
supervision while inducing robustness through masked inputs. We evaluate our
approach using DINO-Tiny on the mini-ImageNet dataset and show that random
masking under this asymmetric setup yields more robust and fine-grained
attention maps, ultimately enhancing downstream performance.

</details>


### [179] [Hierarchical Error Assessment of CAD Models for Aircraft Manufacturing-and-Measurement](https://arxiv.org/pdf/2506.10594)
*Jin Huang, Honghua Chen, Mingqiang Wei*

Main category: cs.CV

TL;DR: A hierarchical error assessment framework (HEA-MM) for aircraft CAD models is proposed, using structured light scanners and multi-level error analysis (global, part, feature) to ensure high-quality manufacturing.


<details>
  <summary>Details</summary>
Motivation: Ensuring high quality in aviation equipment (performance, stability, reliability) requires precise error assessment in CAD models during manufacturing.

Method: HEA-MM uses structured light scanners for 3D measurements, registers point clouds with CAD models, and performs error analysis at global, part, and feature levels. It includes optimization-based primitive refinement and a two-stage algorithm for circular hole detection.

Result: The method effectively evaluates deviations in aircraft CAD models, demonstrated through experiments.

Conclusion: HEA-MM provides a robust framework for hierarchical error assessment, enhancing manufacturing accuracy in aviation.

Abstract: The most essential feature of aviation equipment is high quality, including
high performance, high stability and high reliability. In this paper, we
propose a novel hierarchical error assessment framework for aircraft CAD models
within a manufacturing-and-measurement platform, termed HEA-MM. HEA-MM employs
structured light scanners to obtain comprehensive 3D measurements of
manufactured workpieces. The measured point cloud is registered with the
reference CAD model, followed by an error analysis conducted at three
hierarchical levels: global, part, and feature. At the global level, the error
analysis evaluates the overall deviation of the scanned point cloud from the
reference CAD model. At the part level, error analysis is performed on these
patches underlying the point clouds. We propose a novel optimization-based
primitive refinement method to obtain a set of meaningful patches of point
clouds. Two basic operations, splitting and merging, are introduced to refine
the coarse primitives. At the feature level, error analysis is performed on
circular holes, which are commonly found in CAD models. To facilitate it, a
two-stage algorithm is introduced for the detection of circular holes. First,
edge points are identified using a tensor-voting algorithm. Then, multiple
circles are fitted through a hypothesize-and-clusterize framework, ensuring
accurate detection and analysis of the circular features. Experimental results
on various aircraft CAD models demonstrate the effectiveness of our proposed
method.

</details>


### [180] [Semantic-decoupled Spatial Partition Guided Point-supervised Oriented Object Detection](https://arxiv.org/pdf/2506.10601)
*Xinyuan Liu, Hang Xu, Yike Ma, Yucheng Zhang, Feng Dai*

Main category: cs.CV

TL;DR: SSP (Semantic-decoupled Spatial Partition) improves oriented object detection in dense scenes by combining rule-driven and data-driven methods, achieving superior performance over existing techniques.


<details>
  <summary>Details</summary>
Motivation: Labor-intensive annotation for high-density scenes in remote sensing and limitations of rigid rule-based designs in existing methods drive the need for a more efficient solution.

Method: SSP introduces Pixel-level Spatial Partition-based Sample Assignment and Semantic Spatial Partition-based Box Extraction to enhance sample assignment and instance derivation.

Result: SSP achieves 45.78% mAP under point supervision, outperforming PointOBB-v2 by 4.10%, and integrates well with ORCNN and ReDet architectures.

Conclusion: SSP provides a cost-effective and efficient framework for oriented object detection in dense scenes, validated by superior experimental results.

Abstract: Recent remote sensing tech advancements drive imagery growth, making oriented
object detection rapid development, yet hindered by labor-intensive annotation
for high-density scenes. Oriented object detection with point supervision
offers a cost-effective solution for densely packed scenes in remote sensing,
yet existing methods suffer from inadequate sample assignment and instance
confusion due to rigid rule-based designs. To address this, we propose SSP
(Semantic-decoupled Spatial Partition), a unified framework that synergizes
rule-driven prior injection and data-driven label purification. Specifically,
SSP introduces two core innovations: 1) Pixel-level Spatial Partition-based
Sample Assignment, which compactly estimates the upper and lower bounds of
object scales and mines high-quality positive samples and hard negative samples
through spatial partitioning of pixel maps. 2) Semantic Spatial Partition-based
Box Extraction, which derives instances from spatial partitions modulated by
semantic maps and reliably converts them into bounding boxes to form
pseudo-labels for supervising the learning of downstream detectors. Experiments
on DOTA-v1.0 and others demonstrate SSP\' s superiority: it achieves 45.78% mAP
under point supervision, outperforming SOTA method PointOBB-v2 by 4.10%.
Furthermore, when integrated with ORCNN and ReDet architectures, the SSP
framework achieves mAP values of 47.86% and 48.50%, respectively. The code is
available at https://github.com/antxinyuan/ssp.

</details>


### [181] [High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model](https://arxiv.org/pdf/2506.10605)
*Eshan Ramesh, Nishio Takayuki*

Main category: cs.CV

TL;DR: LatentCSI uses WiFi CSI measurements and a pretrained latent diffusion model (LDM) to generate high-quality images efficiently, outperforming traditional methods in computational efficiency and perceptual quality.


<details>
  <summary>Details</summary>
Motivation: To simplify and improve image generation from WiFi CSI measurements by avoiding complex techniques like GANs and leveraging LDMs for efficiency and quality.

Method: A lightweight neural network maps CSI amplitudes into an LDM's latent space, followed by denoising diffusion with text guidance and decoding to produce high-resolution images.

Result: LatentCSI outperforms baselines in computational efficiency and perceptual quality, validated on custom and public datasets.

Conclusion: LatentCSI offers efficient, high-quality image synthesis with text-guided controllability, surpassing traditional methods.

Abstract: We present LatentCSI, a novel method for generating images of the physical
environment from WiFi CSI measurements that leverages a pretrained latent
diffusion model (LDM). Unlike prior approaches that rely on complex and
computationally intensive techniques such as GANs, our method employs a
lightweight neural network to map CSI amplitudes directly into the latent space
of an LDM. We then apply the LDM's denoising diffusion model to the latent
representation with text-based guidance before decoding using the LDM's
pretrained decoder to obtain a high-resolution image. This design bypasses the
challenges of pixel-space image generation and avoids the explicit image
encoding stage typically required in conventional image-to-image pipelines,
enabling efficient and high-quality image synthesis. We validate our approach
on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi
devices and cameras; and a subset of the publicly available MM-Fi dataset. The
results demonstrate that LatentCSI outperforms baselines of comparable
complexity trained directly on ground-truth images in both computational
efficiency and perceptual quality, while additionally providing practical
advantages through its unique capacity for text-guided controllability.

</details>


### [182] [MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling](https://arxiv.org/pdf/2506.10609)
*Liang Yin, Xudong Xie, Zhang Li, Xiang Bai, Yuliang Liu*

Main category: cs.CV

TL;DR: MSTAR introduces a box-free scene text retrieval method, using progressive vision embedding and multi-instance matching, outperforming state-of-the-art models while eliminating costly bounding box annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods require expensive bounding box annotations and struggle with diverse query types. MSTAR aims to address these limitations.

Method: MSTAR uses progressive vision embedding for multi-grained text representation and harmonizes free-style queries with style-aware instructions. It also includes a multi-instance matching module.

Result: MSTAR surpasses previous models by 6.4% in MAP on Total-Text and by 8.5% on the MQTR benchmark.

Conclusion: MSTAR is a superior, cost-effective solution for multi-query scene text retrieval, validated by extensive experiments and a new benchmark dataset.

Abstract: Scene text retrieval has made significant progress with the assistance of
accurate text localization. However, existing approaches typically require
costly bounding box annotations for training. Besides, they mostly adopt a
customized retrieval strategy but struggle to unify various types of queries to
meet diverse retrieval needs. To address these issues, we introduce Muti-query
Scene Text retrieval with Attention Recycling (MSTAR), a box-free approach for
scene text retrieval. It incorporates progressive vision embedding to
dynamically capture the multi-grained representation of texts and harmonizes
free-style text queries with style-aware instructions. Additionally, a
multi-instance matching module is integrated to enhance vision-language
alignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset,
the first benchmark designed to evaluate the multi-query scene text retrieval
capability of models, comprising four query types and 16k images. Extensive
experiments demonstrate the superiority of our method across seven public
datasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous
state-of-the-art model by 6.4% in MAP on Total-Text while eliminating box
annotation costs. Moreover, on the MQTR benchmark, MSTAR significantly
outperforms the previous models by an average of 8.5%. The code and datasets
are available at https://github.com/yingift/MSTAR.

</details>


### [183] [TexTailor: Customized Text-aligned Texturing via Effective Resampling](https://arxiv.org/pdf/2506.10612)
*Suin Lee, Dae-Shik Kim*

Main category: cs.CV

TL;DR: TexTailor improves texture synthesis from text by integrating resampling and adaptive camera adjustments, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-texture methods suffer from texture inconsistency due to insufficient integration of synthesized textures and fixed camera positions.

Method: TexTailor uses a resampling scheme and fine-tunes a depth-aware diffusion model, adding a performance preservation loss and adaptive camera positioning.

Result: Experiments show TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures.

Conclusion: TexTailor addresses key limitations in texture synthesis, offering improved consistency and adaptability.

Abstract: We present TexTailor, a novel method for generating consistent object
textures from textual descriptions. Existing text-to-texture synthesis
approaches utilize depth-aware diffusion models to progressively generate
images and synthesize textures across predefined multiple viewpoints. However,
these approaches lead to a gradual shift in texture properties across
viewpoints due to (1) insufficient integration of previously synthesized
textures at each viewpoint during the diffusion process and (2) the
autoregressive nature of the texture synthesis process. Moreover, the
predefined selection of camera positions, which does not account for the
object's geometry, limits the effective use of texture information synthesized
from different viewpoints, ultimately degrading overall texture consistency. In
TexTailor, we address these issues by (1) applying a resampling scheme that
repeatedly integrates information from previously synthesized textures within
the diffusion process, and (2) fine-tuning a depth-aware diffusion model on
these resampled textures. During this process, we observed that using only a
few training images restricts the model's original ability to generate
high-fidelity images aligned with the conditioning, and therefore propose an
performance preservation loss to mitigate this issue. Additionally, we improve
the synthesis of view-consistent textures by adaptively adjusting camera
positions based on the object's geometry. Experiments on a subset of the
Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor
outperforms state-of-the-art methods in synthesizing view-consistent textures.
The source code for TexTailor is available at
https://github.com/Adios42/Textailor

</details>


### [184] [Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models](https://arxiv.org/pdf/2506.10633)
*Konstantinos Vilouras, Ilias Stogiannidis, Junyu Yan, Alison Q. O'Neil, Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: The paper addresses the underperformance of text-to-image Latent Diffusion Models in medical imaging, proposing a fine-tuning framework to improve alignment between radiology reports and chest X-rays, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Latent Diffusion Models excel in text-guided image synthesis but lag in medical imaging due to limited data and poor alignment of clinical text with scans. This work aims to bridge this gap.

Method: A fine-tuning framework is introduced to enhance multi-modal alignment in pre-trained Latent Diffusion Models, specifically for chest X-rays and radiology reports.

Result: The method achieves state-of-the-art performance on the MS-CXR benchmark and shows robustness on out-of-distribution data (VinDr-CXR).

Conclusion: The proposed framework effectively improves alignment for medical imaging tasks, demonstrating potential for downstream applications like phrase grounding.

Abstract: Latent Diffusion Models have shown remarkable results in text-guided image
synthesis in recent years. In the domain of natural (RGB) images, recent works
have shown that such models can be adapted to various vision-language
downstream tasks with little to no supervision involved. On the contrary,
text-to-image Latent Diffusion Models remain relatively underexplored in the
field of medical imaging, primarily due to limited data availability (e.g., due
to privacy concerns). In this work, focusing on the chest X-ray modality, we
first demonstrate that a standard text-conditioned Latent Diffusion Model has
not learned to align clinically relevant information in free-text radiology
reports with the corresponding areas of the given scan. Then, to alleviate this
issue, we propose a fine-tuning framework to improve multi-modal alignment in a
pre-trained model such that it can be efficiently repurposed for downstream
tasks such as phrase grounding. Our method sets a new state-of-the-art on a
standard benchmark dataset (MS-CXR), while also exhibiting robust performance
on out-of-distribution data (VinDr-CXR). Our code will be made publicly
available.

</details>


### [185] [Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models](https://arxiv.org/pdf/2506.10634)
*Francisco Caetano, Christiaan Viviers, Peter H. N. De With, Fons van der Sommen*

Main category: cs.CV

TL;DR: SymmFlow unifies semantic segmentation, classification, and image generation using a symmetric learning objective, achieving state-of-the-art performance with efficient sampling.


<details>
  <summary>Details</summary>
Motivation: To create a unified model for multiple tasks (segmentation, classification, generation) with bi-directional consistency and generative diversity.

Method: Uses a symmetric learning objective for joint forward and reverse transformations, retaining semantic information across flows.

Result: Achieves FID scores of 11.9 (CelebAMask-HQ) and 7.0 (COCO-Stuff) with 25 inference steps; competitive in segmentation and promising in classification.

Conclusion: SymmFlow is a versatile framework for multiple tasks, offering high performance and efficiency.

Abstract: Flow Matching has emerged as a powerful framework for learning continuous
transformations between distributions, enabling high-fidelity generative
modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new
formulation that unifies semantic segmentation, classification, and image
generation within a single model. Using a symmetric learning objective,
SymmFlow models forward and reverse transformations jointly, ensuring
bi-directional consistency, while preserving sufficient entropy for generative
diversity. A new training objective is introduced to explicitly retain semantic
information across flows, featuring efficient sampling while preserving
semantic structure, allowing for one-step segmentation and classification
without iterative refinement. Unlike previous approaches that impose strict
one-to-one mapping between masks and images, SymmFlow generalizes to flexible
conditioning, supporting both pixel-level and image-level class labels.
Experimental results on various benchmarks demonstrate that SymmFlow achieves
state-of-the-art performance on semantic image synthesis, obtaining FID scores
of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps.
Additionally, it delivers competitive results on semantic segmentation and
shows promising capabilities in classification tasks. The code will be publicly
available.

</details>


### [186] [GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning](https://arxiv.org/pdf/2506.10639)
*Xiaoyi Bao, Jindi Lv, Xiaofeng Wang, Zheng Zhu, Xinze Chen, YuKun Zhou, Jiancheng Lv, Xingang Wang, Guan Huang*

Main category: cs.CV

TL;DR: GigaVideo-1 is an efficient fine-tuning framework for video diffusion models, improving generation quality without human supervision or extra data, achieving 4% average gain with minimal resources.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for video diffusion models rely on human annotations and heavy computation, limiting practicality. GigaVideo-1 aims to address this by leveraging automatic feedback.

Method: GigaVideo-1 uses a prompt-driven data engine for diverse training samples and a reward-guided training strategy with pre-trained vision-language models for optimization.

Result: On VBench-2.0, GigaVideo-1 improves performance across 17 dimensions by ~4% using only 4 GPU-hours, without manual annotations or extra data.

Conclusion: GigaVideo-1 is an effective and efficient solution for enhancing video generation, requiring minimal resources and no human supervision.

Abstract: Recent progress in diffusion models has greatly enhanced video generation
quality, yet these models still require fine-tuning to improve specific
dimensions like instance preservation, motion rationality, composition, and
physical plausibility. Existing fine-tuning approaches often rely on human
annotations and large-scale computational resources, limiting their
practicality. In this work, we propose GigaVideo-1, an efficient fine-tuning
framework that advances video generation without additional human supervision.
Rather than injecting large volumes of high-quality data from external sources,
GigaVideo-1 unlocks the latent potential of pre-trained video diffusion models
through automatic feedback. Specifically, we focus on two key aspects of the
fine-tuning process: data and optimization. To improve fine-tuning data, we
design a prompt-driven data engine that constructs diverse, weakness-oriented
training samples. On the optimization side, we introduce a reward-guided
training strategy, which adaptively weights samples using feedback from
pre-trained vision-language models with a realism constraint. We evaluate
GigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17
evaluation dimensions. Experiments show that GigaVideo-1 consistently improves
performance on almost all the dimensions with an average gain of about 4% using
only 4 GPU-hours. Requiring no manual annotations and minimal real data,
GigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and
data will be publicly available.

</details>


### [187] [VideoDeepResearch: Long Video Understanding With Agentic Tool Using](https://arxiv.org/pdf/2506.10821)
*Huaying Yuan, Zheng Liu, Junjie Zhou, Ji-Rong Wen, Zhicheng Dou*

Main category: cs.CV

TL;DR: VideoDeepResearch, a text-only large reasoning model with a modular multi-modal toolkit, outperforms MLLMs in long video understanding tasks without requiring extended context windows or strong visual perception.


<details>
  <summary>Details</summary>
Motivation: Challenges in long video understanding (LVU) due to complexity and context constraints, questioning the need for foundation MLLMs with extended capabilities.

Method: Uses a text-only large reasoning model (LRM) and modular multi-modal tools (retrievers, perceivers) to selectively access and utilize video content.

Result: Achieves 9.6%, 6.6%, and 3.9% improvements over MLLM baselines on MLVU, LVBench, and LongVideoBench, respectively.

Conclusion: Agentic systems like VideoDeepResearch can effectively address LVU challenges without relying on traditional MLLM assumptions.

Abstract: Long video understanding (LVU) presents a significant challenge for current
multi-modal large language models (MLLMs) due to the task's inherent complexity
and context window constraint. It is widely assumed that addressing LVU tasks
requires foundation MLLMs with extended context windows, strong visual
perception capabilities, and proficient domain expertise. In this work, we
challenge this common belief by introducing VideoDeepResearch, a novel agentic
framework for long video understanding. Our approach relies solely on a
text-only large reasoning model (LRM) combined with a modular multi-modal
toolkit, including multimodal retrievers and visual perceivers, all of which
are readily available in practice. For each LVU task, the system formulates a
problem-solving strategy through reasoning, while selectively accessing and
utilizing essential video content via tool using. We conduct extensive
experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.
Our results demonstrate that VideoDeepResearch achieves substantial
improvements over existing MLLM baselines, surpassing the previous
state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and
LongVideoBench, respectively. These findings highlight the promise of agentic
systems in overcoming key challenges in LVU problems.

</details>


### [188] [PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis](https://arxiv.org/pdf/2506.10669)
*Marzieh Oghbaie, Teresa Araújoa, Hrvoje Bogunović*

Main category: cs.CV

TL;DR: PiPViT, a prototype-based model using ViT, improves interpretability in medical imaging by learning human-understandable prototypes for lesion extent, validated on retinal OCT datasets.


<details>
  <summary>Details</summary>
Motivation: Existing prototype methods lack consistency with human-understandable biomarkers and granularity for medical imaging, where lesion extent is critical.

Method: PiPViT uses ViT for long-range patch dependencies, contrastive learning, and multi-resolution processing to learn interpretable prototypes with image-level labels.

Result: Achieves competitive performance on retinal OCT datasets, with clinically relevant prototypes, enhancing diagnostic transparency.

Conclusion: PiPViT offers meaningful explanations for clinicians, improving interpretability in medical image analysis.

Abstract: Background and Objective: Prototype-based methods improve interpretability by
learning fine-grained part-prototypes; however, their visualization in the
input pixel space is not always consistent with human-understandable
biomarkers. In addition, well-known prototype-based approaches typically learn
extremely granular prototypes that are less interpretable in medical imaging,
where both the presence and extent of biomarkers and lesions are critical.
  Methods: To address these challenges, we propose PiPViT (Patch-based Visual
Interpretable Prototypes), an inherently interpretable prototypical model for
image recognition. Leveraging a vision transformer (ViT), PiPViT captures
long-range dependencies among patches to learn robust, human-interpretable
prototypes that approximate lesion extent only using image-level labels.
Additionally, PiPViT benefits from contrastive learning and multi-resolution
input processing, which enables effective localization of biomarkers across
scales.
  Results: We evaluated PiPViT on retinal OCT image classification across four
datasets, where it achieved competitive quantitative performance compared to
state-of-the-art methods while delivering more meaningful explanations.
Moreover, quantitative evaluation on a hold-out test set confirms that the
learned prototypes are semantically and clinically relevant. We believe PiPViT
can transparently explain its decisions and assist clinicians in understanding
diagnostic outcomes. Github page: https://github.com/marziehoghbaie/PiPViT

</details>


### [189] [Enhancing Deepfake Detection using SE Block Attention with CNN](https://arxiv.org/pdf/2506.10683)
*Subhram Dasgupta, Janelle Mason, Xiaohong Yuan, Olusola Odeyomi, Kaushik Roy*

Main category: cs.CV

TL;DR: A lightweight CNN with SE block attention is proposed for efficient deepfake detection, achieving high accuracy with minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: Deepfakes pose a threat to information authenticity; existing detection models are resource-heavy.

Method: Lightweight CNN with SE block for dynamic feature recalibration, integrated into a sequential model.

Result: Achieved 94.14% accuracy and 0.985 AUC-ROC on Style GAN dataset.

Conclusion: The model offers an efficient, scalable solution for deepfake detection with low resource usage.

Abstract: In the digital age, Deepfake present a formidable challenge by using advanced
artificial intelligence to create highly convincing manipulated content,
undermining information authenticity and security. These sophisticated
fabrications surpass traditional detection methods in complexity and realism.
To address this issue, we aim to harness cutting-edge deep learning
methodologies to engineer an innovative deepfake detection model. However, most
of the models designed for deepfake detection are large, causing heavy storage
and memory consumption. In this research, we propose a lightweight convolution
neural network (CNN) with squeeze and excitation block attention (SE) for
Deepfake detection. The SE block module is designed to perform dynamic
channel-wise feature recalibration. The SE block allows the network to
emphasize informative features and suppress less useful ones, which leads to a
more efficient and effective learning module. This module is integrated with a
simple sequential model to perform Deepfake detection. The model is smaller in
size and it achieves competing accuracy with the existing models for deepfake
detection tasks. The model achieved an overall classification accuracy of
94.14% and AUC-ROC score of 0.985 on the Style GAN dataset from the Diverse
Fake Face Dataset. Our proposed approach presents a promising avenue for
combating the Deepfake challenge with minimal computational resources,
developing efficient and scalable solutions for digital content verification.

</details>


### [190] [Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework](https://arxiv.org/pdf/2506.10685)
*Xia Du, Xiaoyuan Liu, Jizhe Zhou, Zheng Lin, Chi-man Pun, Zhe Chen, Wei Ni, Jun Luo*

Main category: cs.CV

TL;DR: UAC introduces a novel adversarial CAPTCHA framework using text prompts and LLMs to enhance diversity and attack efficacy, with BP-UAC excelling in black-box scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional CAPTCHAs are vulnerable to DNN-based attacks, and existing adversarial methods often distort images or lack applicability without initial inputs.

Method: UAC leverages LLMs for text-guided adversarial examples, with EDICT for targeted attacks and BP-UAC for untargeted black-box attacks using multimodal gradients.

Result: BP-UAC achieves high success rates, generating natural CAPTCHAs that fool both humans and DNNs.

Conclusion: UAC and BP-UAC offer robust solutions for adversarial CAPTCHA generation, addressing limitations of existing methods.

Abstract: With the rapid advancements in deep learning, traditional CAPTCHA schemes are
increasingly vulnerable to automated attacks powered by deep neural networks
(DNNs). Existing adversarial attack methods often rely on original image
characteristics, resulting in distortions that hinder human interpretation and
limit applicability in scenarios lacking initial input images. To address these
challenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel
framework generating high-fidelity adversarial examples guided by
attacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC
enhances CAPTCHA diversity and supports both targeted and untargeted attacks.
For targeted attacks, the EDICT method optimizes dual latent variables in a
diffusion model for superior image quality. In untargeted attacks, especially
for black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA
(BP-UAC), a two-step optimization strategy employing multimodal gradients and
bi-path optimization for efficient misclassification. Experiments show BP-UAC
achieves high attack success rates across diverse systems, generating natural
CAPTCHAs indistinguishable to humans and DNNs.

</details>


### [191] [Continual Hyperbolic Learning of Instances and Classes](https://arxiv.org/pdf/2506.10710)
*Melika Ayoughi, Mina Ghadimi Atigh, Mohammad Mahdi Derakhshani, Cees G. M. Snoek, Pascal Mettes, Paul Groth*

Main category: cs.CV

TL;DR: The paper introduces HyperCLIC, a continual learning algorithm for hierarchical data, using hyperbolic space to balance instance and class recognition.


<details>
  <summary>Details</summary>
Motivation: Real-world applications like robotics require models to handle both instance and class recognition simultaneously, which traditional continual learning lacks.

Method: Proposes HyperCLIC, leveraging hyperbolic space for hierarchical data, with hyperbolic classification and distillation objectives.

Result: Validated on EgoObjects, HyperCLIC shows improved hierarchical generalization across granularities.

Conclusion: HyperCLIC effectively addresses the challenge of continual learning for hierarchical instance and class recognition.

Abstract: Continual learning has traditionally focused on classifying either instances
or classes, but real-world applications, such as robotics and self-driving
cars, require models to handle both simultaneously. To mirror real-life
scenarios, we introduce the task of continual learning of instances and
classes, at the same time. This task challenges models to adapt to multiple
levels of granularity over time, which requires balancing fine-grained instance
recognition with coarse-grained class generalization. In this paper, we
identify that classes and instances naturally form a hierarchical structure. To
model these hierarchical relationships, we propose HyperCLIC, a continual
learning algorithm that leverages hyperbolic space, which is uniquely suited
for hierarchical data due to its ability to represent tree-like structures with
low distortion and compact embeddings. Our framework incorporates hyperbolic
classification and distillation objectives, enabling the continual embedding of
hierarchical relations. To evaluate performance across multiple granularities,
we introduce continual hierarchical metrics. We validate our approach on
EgoObjects, the only dataset that captures the complexity of hierarchical
object recognition in dynamic real-world environments. Empirical results show
that HyperCLIC operates effectively at multiple granularities with improved
hierarchical generalization.

</details>


### [192] [Underage Detection through a Multi-Task and MultiAge Approach for Screening Minors in Unconstrained Imagery](https://arxiv.org/pdf/2506.10689)
*Christopher Gaul, Eduardo Fidalgo, Enrique Alegre, Rocío Alaiz Rodríguez, Eri Pérez Corral*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate automatic screening of minors in unconstrained images demands models
that are robust to distribution shift and resilient to the children
under-representation in publicly available data. To overcome these issues, we
propose a multi-task architecture with dedicated under/over-age discrimination
tasks based on a frozen FaRL vision-language backbone joined with a compact
two-layer MLP that shares features across one age-regression head and four
binary under-age heads for age thresholds of 12, 15, 18, and 21 years, focusing
on the legally critical age range. To address the severe class imbalance, we
introduce an $\alpha$-reweighted focal-style loss and age-balanced mini-batch
sampling, which equalizes twelve age bins during stochastic optimization.
Further improvement is achieved with an age gap that removes edge cases from
the loss.
  Moreover, we set a rigorous evaluation by proposing the Overall Under-Age
Benchmark, with 303k cleaned training images and 110k test images, defining
both the "ASORES-39k" restricted overall test, which removes the noisiest
domains, and the age estimation wild shifts test "ASWIFT-20k" of 20k-images,
stressing extreme pose ($>$45{\deg}), expression, and low image quality to
emulate real-world shifts.
  Trained on the cleaned overall set with resampling and age gap, our multiage
model "F" lowers the root-mean-square-error on the ASORES-39k restricted test
from 5.733 (age-only baseline) to 5.656 years and lifts under-18 detection from
F2 score of 0.801 to 0.857 at 1% false-adult rate. Under the domain shift to
the wild data of ASWIFT-20k, the same configuration nearly sustains 0.99 recall
while boosting F2 from 0.742 to 0.833 with respect to the age-only baseline,
demonstrating strong generalization under distribution shift. For the under-12
and under-15 tasks, the respective boosts in F2 are from 0.666 to 0.955 and
from 0.689 to 0.916, respectively.

</details>


### [193] [Uncertainty-Masked Bernoulli Diffusion for Camouflaged Object Detection Refinement](https://arxiv.org/pdf/2506.10712)
*Yuqi Shen, Fengyang Xiao, Sujie Hu, Youwei Pang, Yifan Pu, Chengyu Fang, Xiu Li, Chunming He*

Main category: cs.CV

TL;DR: The paper introduces UMBD, a generative refinement framework for Camouflaged Object Detection (COD), using uncertainty-guided masking and Bernoulli diffusion to improve segmentation quality.


<details>
  <summary>Details</summary>
Motivation: Existing COD methods lack effective post-processing refinement, leaving room for improvement in handling subtle visual differences between targets and backgrounds.

Method: Proposes UMBD with an uncertainty-guided masking mechanism and Bernoulli diffusion for targeted refinement, supported by HUQNet for accurate uncertainty estimation.

Result: UMBD achieves average gains of 5.5% in MAE and 3.2% in weighted F-measure across COD benchmarks with modest computational overhead.

Conclusion: UMBD effectively combines discriminative and generative approaches for COD refinement, demonstrating consistent performance improvements.

Abstract: Camouflaged Object Detection (COD) presents inherent challenges due to the
subtle visual differences between targets and their backgrounds. While existing
methods have made notable progress, there remains significant potential for
post-processing refinement that has yet to be fully explored. To address this
limitation, we propose the Uncertainty-Masked Bernoulli Diffusion (UMBD) model,
the first generative refinement framework specifically designed for COD. UMBD
introduces an uncertainty-guided masking mechanism that selectively applies
Bernoulli diffusion to residual regions with poor segmentation quality,
enabling targeted refinement while preserving correctly segmented areas. To
support this process, we design the Hybrid Uncertainty Quantification Network
(HUQNet), which employs a multi-branch architecture and fuses uncertainty from
multiple sources to improve estimation accuracy. This enables adaptive guidance
during the generative sampling process. The proposed UMBD framework can be
seamlessly integrated with a wide range of existing Encoder-Decoder-based COD
models, combining their discriminative capabilities with the generative
advantages of diffusion-based refinement. Extensive experiments across multiple
COD benchmarks demonstrate consistent performance improvements, achieving
average gains of 5.5% in MAE and 3.2% in weighted F-measure with only modest
computational overhead. Code will be released.

</details>


### [194] [MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning](https://arxiv.org/pdf/2506.10963)
*Yuxuan Luo, Yuhui Yuan, Junwen Chen, Haonan Cai, Ziyi Yue, Yuwei Yang, Fatima Zohra Daha, Ji Li, Zhouhui Lian*

Main category: cs.CV

TL;DR: The paper introduces knowledge image generation as a new task and the MMMG benchmark to evaluate image generation models' reasoning capabilities. It highlights challenges in generating knowledge images and proposes a unified KG representation and MMMG-Score for evaluation. Results show deficits in current models, and FLUX-Reason is released as a baseline.


<details>
  <summary>Details</summary>
Motivation: Knowledge images are crucial for human learning, but generating them requires complex multimodal reasoning. The paper aims to address this gap by creating a benchmark and evaluation framework.

Method: The MMMG benchmark includes 4,456 expert-validated image-prompt pairs across disciplines and formats. A unified KG representation is used for evaluation, and MMMG-Score combines factual fidelity and visual clarity.

Result: Evaluations of 16 models reveal reasoning deficits, with GPT-4o scoring only 50.20. FLUX-Reason, a baseline model, achieves 34.45.

Conclusion: The paper establishes a challenging benchmark for knowledge image generation and provides a baseline to encourage further research in this area.

Abstract: In this paper, we introduce knowledge image generation as a new task,
alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation
Benchmark (MMMG) to probe the reasoning capability of image generation models.
Knowledge images have been central to human civilization and to the mechanisms
of human learning--a fact underscored by dual-coding theory and the
picture-superiority effect. Generating such images is challenging, demanding
multimodal reasoning that fuses world knowledge with pixel-level grounding into
clear explanatory visuals. To enable comprehensive evaluation, MMMG offers
4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,
6 educational levels, and diverse knowledge formats such as charts, diagrams,
and mind maps. To eliminate confounding complexity during evaluation, we adopt
a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a
target image's core entities and their dependencies. We further introduce
MMMG-Score to evaluate generated knowledge images. This metric combines factual
fidelity, measured by graph-edit distance between KGs, with visual clarity
assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image
generation models expose serious reasoning deficits--low entity fidelity, weak
relations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,
underscoring the benchmark's difficulty. To spur further progress, we release
FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines
a reasoning LLM with diffusion models and is trained on 16,000 curated
knowledge image-prompt pairs.

</details>


### [195] [IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly Detection in Medical Domain](https://arxiv.org/pdf/2506.10730)
*Hong Huang, Weixiang Sun, Zhijian Wu, Jingwen Niu, Donghuan Lu, Xian Wu, Yefeng Zheng*

Main category: cs.CV

TL;DR: IQE-CLIP improves zero- and few-shot anomaly detection in medical tasks by integrating textual and visual information, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based methods assume prior category knowledge and lack effectiveness in distinguishing anomalies in medical domains.

Method: IQE-CLIP uses class-based and learnable prompting tokens and an instance-aware query module to generate anomaly-sensitive embeddings.

Result: State-of-the-art performance on six medical datasets in zero- and few-shot settings.

Conclusion: IQE-CLIP effectively addresses limitations in medical anomaly detection, offering a robust framework for future applications.

Abstract: Recent advances in vision-language models, such as CLIP, have significantly
improved performance in zero- and few-shot anomaly detection (ZFSAD) tasks.
However, most existing CLIP-based methods assume prior knowledge of categories
and rely on carefully designed prompts tailored to specific scenarios. While
these text prompts capture semantic information in the textual space, they
often fail to distinguish normal and anomalous instances in the joint embedding
space. Moreover, most ZFSAD approaches focus on industrial domains, with
limited exploration in medical tasks. To address these limitations, we propose
IQE-CLIP, a novel framework for ZFSAD in the medical domain. We show that query
embeddings integrating both textual and instance-aware visual information serve
as more effective indicators of anomalies. Specifically, we introduce
class-based and learnable prompting tokens to better adapt CLIP to the medical
setting. Furthermore, we design an instance-aware query module that extracts
region-level contextual information from both modalities, enabling the
generation of anomaly-sensitive embeddings. Extensive experiments on six
medical datasets demonstrate that IQE-CLIP achieves state-of-the-art
performance in both zero-shot and few-shot settings. Code and data are
available at \href{https://github.com/hongh0/IQE-CLIP/}{this https URL}.

</details>


### [196] [PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework](https://arxiv.org/pdf/2506.10741)
*SiXiang Chen, Jianyu Lai, Jialin Gao, Tian Ye, Haoyu Chen, Hengyu Shi, Shitong Shao, Yunlong Lin, Song Fei, Zhaohu Xing, Yeying Jin, Junfeng Luo, Xiaoming Wei, Lei Zhu*

Main category: cs.CV

TL;DR: PosterCraft is a unified framework for generating high-aesthetic posters, outperforming baselines in rendering, layout, and visual appeal.


<details>
  <summary>Details</summary>
Motivation: Generating aesthetic posters requires seamless integration of text, art, and layout, which existing modular pipelines fail to achieve.

Method: PosterCraft uses a cascaded workflow: text-rendering optimization, fine-tuning, reinforcement learning, and feedback refinement.

Result: Significantly outperforms baselines in accuracy, coherence, and visual appeal, nearing SOTA commercial systems.

Conclusion: PosterCraft offers a robust, automated solution for high-quality poster generation, with open-source resources available.

Abstract: Generating aesthetic posters is more challenging than simple design images:
it requires not only precise text rendering but also the seamless integration
of abstract artistic content, striking layouts, and overall stylistic harmony.
To address this, we propose PosterCraft, a unified framework that abandons
prior modular pipelines and rigid, predefined layouts, allowing the model to
freely explore coherent, visually compelling compositions. PosterCraft employs
a carefully designed, cascaded workflow to optimize the generation of
high-aesthetic posters: (i) large-scale text-rendering optimization on our
newly introduced Text-Render-2M dataset; (ii) region-aware supervised
fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via
best-of-n preference optimization; and (iv) joint vision-language feedback
refinement. Each stage is supported by a fully automated data-construction
pipeline tailored to its specific needs, enabling robust training without
complex architectural modifications. Evaluated on multiple experiments,
PosterCraft significantly outperforms open-source baselines in rendering
accuracy, layout coherence, and overall visual appeal-approaching the quality
of SOTA commercial systems. Our code, models, and datasets can be found in the
Project page: https://ephemeral182.github.io/PosterCraft

</details>


### [197] [Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary Ultra-Large Scales](https://arxiv.org/pdf/2506.10774)
*Wenhao Guo, Peng Lu, Xujun Peng, Zhaoran Zhao, Sheng Li*

Main category: cs.CV

TL;DR: A unified model, Stroke-based Cyclic Amplifier (SbCA), addresses performance decline in ultra-large upsampling by decomposing images into stroke vectors and restoring details, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Prior ASISR methods struggle with performance decline and blurring when upsampling exceeds training data ranges.

Method: SbCA uses stroke vector amplification and detail completion, iteratively refining details for ultra-large upsampling with a single training.

Result: SbCA outperforms state-of-the-art methods in ultra-large upsampling (e.g., ×100), reducing artifacts and blurring.

Conclusion: SbCA effectively addresses distribution drift and delivers high-quality super-resolved images, validated on synthetic and real-world datasets.

Abstract: Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience
a significant performance decline when the upsampling factor exceeds the range
covered by the training data, introducing substantial blurring. To address this
issue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for
ultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier,
which decomposes the image into a series of strokes represented as vector
graphics for magnification. Then, the detail completion module also restores
missing details, ensuring high-fidelity image reconstruction. Our cyclic
strategy achieves ultra-large upsampling by iteratively refining details with
this unified SbCA model, trained only once for all, while keeping sub-scales
within the training range. Our approach effectively addresses the distribution
drift issue and eliminates artifacts, noise and blurring, producing
high-quality, high-resolution super-resolved images. Experimental validations
on both synthetic and real-world datasets demonstrate that our approach
significantly outperforms existing methods in ultra-large upsampling tasks
(e.g. $\times100$), delivering visual quality far superior to state-of-the-art
techniques.

</details>


### [198] [SlotPi: Physics-informed Object-centric Reasoning Models](https://arxiv.org/pdf/2506.10778)
*Jian Li, Wan Han, Ning Lin, Yu-Liang Zhan, Ruizhi Chengze, Haining Wang, Yi Zhang, Hongsheng Liu, Zidong Wang, Fan Yu, Hao Sun*

Main category: cs.CV

TL;DR: SlotPi is a physics-informed model integrating Hamiltonian principles for dynamic forecasting, excelling in prediction and VQA tasks across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in current object-centric dynamic simulation methods by integrating physical knowledge and validating adaptability in diverse scenarios, especially involving fluids and objects.

Method: SlotPi combines a Hamiltonian-based physical module with a spatio-temporal prediction module for dynamic forecasting.

Result: Demonstrates robust performance in prediction and VQA tasks on benchmark and fluid datasets, including a newly created real-world dataset.

Conclusion: SlotPi's adaptability and performance lay a foundation for advanced world models, bridging gaps in physics-informed reasoning.

Abstract: Understanding and reasoning about dynamics governed by physical laws through
visual observation, akin to human capabilities in the real world, poses
significant challenges. Currently, object-centric dynamic simulation methods,
which emulate human behavior, have achieved notable progress but overlook two
critical aspects: 1) the integration of physical knowledge into models. Humans
gain physical insights by observing the world and apply this knowledge to
accurately reason about various dynamic scenarios; 2) the validation of model
adaptability across diverse scenarios. Real-world dynamics, especially those
involving fluids and objects, demand models that not only capture object
interactions but also simulate fluid flow characteristics. To address these
gaps, we introduce SlotPi, a slot-based physics-informed object-centric
reasoning model. SlotPi integrates a physical module based on Hamiltonian
principles with a spatio-temporal prediction module for dynamic forecasting.
Our experiments highlight the model's strengths in tasks such as prediction and
Visual Question Answering (VQA) on benchmark and fluid datasets. Furthermore,
we have created a real-world dataset encompassing object interactions, fluid
dynamics, and fluid-object interactions, on which we validated our model's
capabilities. The model's robust performance across all datasets underscores
its strong adaptability, laying a foundation for developing more advanced world
models.

</details>


### [199] [Human-Robot Navigation using Event-based Cameras and Reinforcement Learning](https://arxiv.org/pdf/2506.10790)
*Ignacio Bugueno-Cordova, Javier Ruiz-del-Solar, Rodrigo Verschae*

Main category: cs.CV

TL;DR: A robot navigation controller using event cameras and reinforcement learning for real-time human-centered navigation and obstacle avoidance.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of conventional image-based controllers (fixed rates, motion blur, latency) by leveraging event cameras' asynchronous nature.

Method: Combines event-based perception, range sensing, and policy optimization via Deep Deterministic Policy Gradient, with imitation learning for efficiency.

Result: Achieves robust navigation, pedestrian following, and obstacle avoidance in simulated environments.

Conclusion: The framework shows promise for adaptive, real-time navigation using event cameras and reinforcement learning.

Abstract: This work introduces a robot navigation controller that combines event
cameras and other sensors with reinforcement learning to enable real-time
human-centered navigation and obstacle avoidance. Unlike conventional
image-based controllers, which operate at fixed rates and suffer from motion
blur and latency, this approach leverages the asynchronous nature of event
cameras to process visual information over flexible time intervals, enabling
adaptive inference and control. The framework integrates event-based
perception, additional range sensing, and policy optimization via Deep
Deterministic Policy Gradient, with an initial imitation learning phase to
improve sample efficiency. Promising results are achieved in simulated
environments, demonstrating robust navigation, pedestrian following, and
obstacle avoidance. A demo video is available at the project website.

</details>


### [200] [Prompts to Summaries: Zero-Shot Language-Guided Video Summarization](https://arxiv.org/pdf/2506.10807)
*Mario Barbara, Alaa Maalouf*

Main category: cs.CV

TL;DR: A zero-shot, text-queryable video summarizer (Prompts-to-Summaries) uses pretrained models without training data, outperforming unsupervised and matching supervised methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for flexible, user-controllable video summarization tools without domain-specific training data.

Method: Segments video into scenes, generates descriptions via VidLM, scores importance with LLM, and propagates scores using consistency and uniqueness metrics.

Result: Surpasses unsupervised methods on SumMe and TVSum, competes on QFVS, and introduces VidSum-Reason dataset.

Conclusion: Pretrained models with principled prompting enable universal, text-queryable video summarization.

Abstract: The explosive growth of video data intensified the need for flexible
user-controllable summarization tools that can operate without domain-specific
training data. Existing methods either rely on datasets, limiting
generalization, or cannot incorporate user intent expressed in natural
language. We introduce Prompts-to-Summaries: the first zero-shot,
text-queryable video summarizer that converts off-the-shelf video-language
models (VidLMs) captions into user-guided skims via large language models
(LLMs) judging, without the use of training data at all, beating all
unsupervised and matching supervised methods. Our pipeline (i) segments raw
video footage into coherent scenes, (ii) generates rich scene-level
descriptions through a memory-efficient, batch-style VidLM prompting scheme
that scales to hours-long videos on a single GPU, (iii) leverages an LLM as a
judge to assign scene-level importance scores under a carefully crafted prompt,
and finally, (iv) propagates those scores to short segments level via two new
metrics: consistency (temporal coherency) and uniqueness (novelty), yielding
fine-grained frame importance. On SumMe and TVSum, our data-free approach
surpasses all prior data-hungry unsupervised methods. It also performs
competitively on the Query-Focused Video Summarization (QFVS) benchmark,
despite using no training data and the competing methods requiring supervised
frame-level importance. To spur further research, we release VidSum-Reason, a
new query-driven dataset featuring long-tailed concepts and multi-step
reasoning; our framework attains robust F1 scores and serves as the first
challenging baseline. Overall, our results demonstrate that pretrained
multimodal models, when orchestrated with principled prompting and score
propagation, already provide a powerful foundation for universal,
text-queryable video summarization.

</details>


### [201] [Occlusion-Aware 3D Hand-Object Pose Estimation with Masked AutoEncoders](https://arxiv.org/pdf/2506.10816)
*Hui Yang, Wei Sun, Jian Liu, Jin Zheng, Jian Xiao, Ajmal Mian*

Main category: cs.CV

TL;DR: HOMAE is a novel occlusion-aware method for hand-object pose estimation using masked autoencoders, achieving state-of-the-art results by combining SDF and point cloud representations.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack global structural reasoning for occluded hand-object interactions, limiting their effectiveness.

Method: Proposes a target-focused masking strategy and integrates multi-scale features to predict SDF, combining it with point clouds for robust occlusion handling.

Result: HOMAE outperforms existing methods on DexYCB and HO3Dv2 benchmarks.

Conclusion: The method effectively addresses occlusion challenges and advances hand-object pose estimation.

Abstract: Hand-object pose estimation from monocular RGB images remains a significant
challenge mainly due to the severe occlusions inherent in hand-object
interactions. Existing methods do not sufficiently explore global structural
perception and reasoning, which limits their effectiveness in handling occluded
hand-object interactions. To address this challenge, we propose an
occlusion-aware hand-object pose estimation method based on masked
autoencoders, termed as HOMAE. Specifically, we propose a target-focused
masking strategy that imposes structured occlusion on regions of hand-object
interaction, encouraging the model to learn context-aware features and reason
about the occluded structures. We further integrate multi-scale features
extracted from the decoder to predict a signed distance field (SDF), capturing
both global context and fine-grained geometry. To enhance geometric perception,
we combine the implicit SDF with an explicit point cloud derived from the SDF,
leveraging the complementary strengths of both representations. This fusion
enables more robust handling of occluded regions by combining the global
context from the SDF with the precise local geometry provided by the point
cloud. Extensive experiments on challenging DexYCB and HO3Dv2 benchmarks
demonstrate that HOMAE achieves state-of-the-art performance in hand-object
pose estimation. We will release our code and model.

</details>


### [202] [Post-Training Quantization for Video Matting](https://arxiv.org/pdf/2506.10840)
*Tianrui Zhu, Houyuan Chen, Ruihao Gong, Michele Magno, Haotong Qin, Kai Zhang*

Main category: cs.CV

TL;DR: A novel PTQ framework for video matting addresses accuracy and temporal coherence challenges, introducing a two-stage strategy, global affine calibration, and optical flow assistance, achieving near full-precision performance even at 4-bit quantization.


<details>
  <summary>Details</summary>
Motivation: Deploying computationally intensive video matting models on resource-constrained devices is challenging. Existing PTQ methods struggle with accuracy and temporal coherence in this domain.

Method: Proposes a two-stage PTQ strategy: block-reconstruction-based optimization for initial quantization and global calibration. Introduces Statistically-Driven Global Affine Calibration (GAC) and Optical Flow Assistance (OFA) for temporal coherence.

Result: PTQ4VM achieves state-of-the-art accuracy across bit-widths, with 4-bit quantization performing close to full-precision while saving 8x FLOPs. GAC reduces errors by up to 20%.

Conclusion: The proposed PTQ4VM framework effectively balances accuracy and efficiency, making video matting viable for resource-constrained devices.

Abstract: Video matting is crucial for applications such as film production and virtual
reality, yet deploying its computationally intensive models on
resource-constrained devices presents challenges. Quantization is a key
technique for model compression and acceleration. As an efficient approach,
Post-Training Quantization (PTQ) is still in its nascent stages for video
matting, facing significant hurdles in maintaining accuracy and temporal
coherence. To address these challenges, this paper proposes a novel and general
PTQ framework specifically designed for video matting models, marking, to the
best of our knowledge, the first systematic attempt in this domain. Our
contributions include: (1) A two-stage PTQ strategy that combines
block-reconstruction-based optimization for fast, stable initial quantization
and local dependency capture, followed by a global calibration of quantization
parameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine
Calibration (GAC) method that enables the network to compensate for cumulative
statistical distortions arising from factors such as neglected BN layer
effects, even reducing the error of existing PTQ methods on video matting tasks
up to 20%. (3) An Optical Flow Assistance (OFA) component that leverages
temporal and semantic priors from frames to guide the PTQ process, enhancing
the model's ability to distinguish moving foregrounds in complex scenes and
ultimately achieving near full-precision performance even under ultra-low-bit
quantization. Comprehensive quantitative and visual results show that our
PTQ4VM achieves the state-of-the-art accuracy performance across different
bit-widths compared to the existing quantization methods. We highlight that the
4-bit PTQ4VM even achieves performance close to the full-precision counterpart
while enjoying 8x FLOP savings.

</details>


### [203] [CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation](https://arxiv.org/pdf/2506.10890)
*Zhao Zhang, Yutao Cheng, Dexiang Hong, Maoke Yang, Gonglei Shi, Lei Ma, Hui Zhang, Jie Shao, Xinglong Wu*

Main category: cs.CV

TL;DR: CreatiPoster is a framework for generating editable, multi-layer graphic designs from natural-language instructions or assets, outperforming existing tools in quality and editability.


<details>
  <summary>Details</summary>
Motivation: Current AI tools for graphic design lack accuracy in incorporating user assets, maintaining editability, and achieving professional appeal, prompting the need for a better solution.

Method: CreatiPoster uses a protocol model (RGBA large multimodal model) to generate a JSON specification for layers and a conditional background model to synthesize coherent backgrounds.

Result: CreatiPoster surpasses leading open-source and commercial systems, demonstrated by a benchmark with automated metrics.

Conclusion: The framework advances AI-assisted graphic design, supporting diverse applications and democratizing access to professional-quality designs.

Abstract: Graphic design plays a crucial role in both commercial and personal contexts,
yet creating high-quality, editable, and aesthetically pleasing graphic
compositions remains a time-consuming and skill-intensive task, especially for
beginners. Current AI tools automate parts of the workflow, but struggle to
accurately incorporate user-supplied assets, maintain editability, and achieve
professional visual appeal. Commercial systems, like Canva Magic Design, rely
on vast template libraries, which are impractical for replicate. In this paper,
we introduce CreatiPoster, a framework that generates editable, multi-layer
compositions from optional natural-language instructions or assets. A protocol
model, an RGBA large multimodal model, first produces a JSON specification
detailing every layer (text or asset) with precise layout, hierarchy, content
and style, plus a concise background prompt. A conditional background model
then synthesizes a coherent background conditioned on this rendered foreground
layers. We construct a benchmark with automated metrics for graphic-design
generation and show that CreatiPoster surpasses leading open-source approaches
and proprietary commercial systems. To catalyze further research, we release a
copyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports
diverse applications such as canvas editing, text overlay, responsive resizing,
multilingual adaptation, and animated posters, advancing the democratization of
AI-assisted graphic design. Project homepage:
https://github.com/graphic-design-ai/creatiposter

</details>


### [204] [AIR: Zero-shot Generative Model Adaptation with Iterative Refinement](https://arxiv.org/pdf/2506.10895)
*Guimeng Liu, Milad Abdollahzadeh, Ngai-Man Cheung*

Main category: cs.CV

TL;DR: The paper introduces Adaptation with Iterative Refinement (AIR) to improve zero-shot generative model adaptation by addressing offset misalignment in CLIP embedding space, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot generative model adaptation (ZSGM) methods assume perfect alignment between image and text offsets in CLIP embedding space, leading to degraded image quality. This work aims to analyze and mitigate this misalignment.

Method: The authors first empirically study offset misalignment in CLIP space, finding it correlates with concept distance. They then propose AIR, an iterative refinement approach to improve image quality by leveraging this insight.

Result: AIR outperforms existing methods in 26 experimental setups, as shown by qualitative, quantitative, and user study results.

Conclusion: The paper demonstrates that addressing offset misalignment significantly improves ZSGM performance, with AIR achieving state-of-the-art results.

Abstract: Zero-shot generative model adaptation (ZSGM) aims to adapt a pre-trained
generator to a target domain using only text guidance and without any samples
from the target domain. Central to recent ZSGM approaches are directional loss
which use the text guidance in the form of aligning the image offset with text
offset in the embedding space of a vision-language model like CLIP. This is
similar to the analogical reasoning in NLP where the offset between one pair of
words is used to identify a missing element in another pair by aligning the
offset between these two pairs. However, a major limitation of existing ZSGM
methods is that the learning objective assumes the complete alignment between
image offset and text offset in the CLIP embedding space, resulting in quality
degrade in generated images. Our work makes two main contributions. Inspired by
the offset misalignment studies in NLP, as our first contribution, we perform
an empirical study to analyze the misalignment between text offset and image
offset in CLIP embedding space for various large publicly available datasets.
Our important finding is that offset misalignment in CLIP embedding space is
correlated with concept distance, i.e., close concepts have a less offset
misalignment. To address the limitations of the current approaches, as our
second contribution, we propose Adaptation with Iterative Refinement (AIR)
which is the first ZSGM approach to focus on improving target domain image
quality based on our new insight on offset misalignment.Qualitative,
quantitative, and user study in 26 experiment setups consistently demonstrate
the proposed AIR approach achieves SOTA performance. Additional experiments are
in Supp.

</details>


### [205] [M4V: Multi-Modal Mamba for Text-to-Video Generation](https://arxiv.org/pdf/2506.10915)
*Jiancheng Huang, Gengwei Zhang, Zequn Jie, Siyu Jiao, Yinlong Qian, Ling Chen, Yunchao Wei, Lin Ma*

Main category: cs.CV

TL;DR: M4V introduces a Multi-Modal Mamba framework for efficient text-to-video generation, reducing FLOPs by 45% and improving visual quality with a reward learning strategy.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational inefficiency of Transformers in spatiotemporal modeling for video generation, leveraging Mamba's linear-time sequence modeling.

Method: Proposes a multi-modal diffusion Mamba (MM-DiM) block for integrating multi-modal information and spatiotemporal modeling, alongside a reward learning strategy for visual quality.

Result: M4V reduces FLOPs by 45% compared to attention-based methods and produces high-quality videos, validated on benchmarks.

Conclusion: M4V offers a computationally efficient and high-quality solution for text-to-video generation, with plans for public release of code and models.

Abstract: Text-to-video generation has significantly enriched content creation and
holds the potential to evolve into powerful world simulators. However, modeling
the vast spatiotemporal space remains computationally demanding, particularly
when employing Transformers, which incur quadratic complexity in sequence
processing and thus limit practical applications. Recent advancements in
linear-time sequence modeling, particularly the Mamba architecture, offer a
more efficient alternative. Nevertheless, its plain design limits its direct
applicability to multi-modal and spatiotemporal video generation tasks. To
address these challenges, we introduce M4V, a Multi-Modal Mamba framework for
text-to-video generation. Specifically, we propose a multi-modal diffusion
Mamba (MM-DiM) block that enables seamless integration of multi-modal
information and spatiotemporal modeling through a multi-modal token
re-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45%
compared to the attention-based alternative when generating videos at
768$\times$1280 resolution. Additionally, to mitigate the visual quality
degradation in long-context autoregressive generation processes, we introduce a
reward learning strategy that further enhances per-frame visual realism.
Extensive experiments on text-to-video benchmarks demonstrate M4V's ability to
produce high-quality videos while significantly lowering computational costs.
Code and models will be publicly available at
https://huangjch526.github.io/M4V_project.

</details>


### [206] [SpectralAR: Spectral Autoregressive Visual Generation](https://arxiv.org/pdf/2506.10962)
*Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Yueqi Duan, Jie Zhou, Jiwen Lu*

Main category: cs.CV

TL;DR: SpectralAR introduces a causality-aware autoregressive visual generation framework using spectral tokens for efficient and detailed image generation.


<details>
  <summary>Details</summary>
Motivation: Address the contradiction between parallel image patches and the causal nature of autoregressive modeling in visual generation.

Method: Transform images into ordered spectral tokens using Nested Spectral Tokenization, then perform autoregressive generation in a coarse-to-fine manner.

Result: Achieves 3.02 gFID with 64 tokens and 310M parameters on ImageNet-1K for image reconstruction and generation.

Conclusion: SpectralAR effectively combines causality and efficiency in autoregressive visual generation.

Abstract: Autoregressive visual generation has garnered increasing attention due to its
scalability and compatibility with other modalities compared with diffusion
models. Most existing methods construct visual sequences as spatial patches for
autoregressive generation. However, image patches are inherently parallel,
contradicting the causal nature of autoregressive modeling. To address this, we
propose a Spectral AutoRegressive (SpectralAR) visual generation framework,
which realizes causality for visual sequences from the spectral perspective.
Specifically, we first transform an image into ordered spectral tokens with
Nested Spectral Tokenization, representing lower to higher frequency
components. We then perform autoregressive generation in a coarse-to-fine
manner with the sequences of spectral tokens. By considering different levels
of detail in images, our SpectralAR achieves both sequence causality and token
efficiency without bells and whistles. We conduct extensive experiments on
ImageNet-1K for image reconstruction and autoregressive generation, and
SpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project
page: https://huang-yh.github.io/spectralar/.

</details>


### [207] [Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs](https://arxiv.org/pdf/2506.10967)
*Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, Shanghang Zhang*

Main category: cs.CV

TL;DR: CDPruner is a novel visual token pruning method for MLLMs that maximizes conditional diversity using DPP, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing high inference costs in MLLMs due to redundant visual tokens, current methods fail to balance pruning efficiency and instruction relevance.

Method: Proposes CDPruner, a training-free, model-agnostic method using DPP to maximize conditional diversity of retained tokens.

Result: Achieves SOTA on benchmarks, reduces FLOPs by 95% and latency by 78% in LLaVA, retaining 94% accuracy.

Conclusion: CDPruner effectively balances token reduction and performance, offering a scalable solution for MLLMs.

Abstract: In multimodal large language models (MLLMs), the length of input visual
tokens is often significantly greater than that of their textual counterparts,
leading to a high inference cost. Many works aim to address this issue by
removing redundant visual tokens. However, current approaches either rely on
attention-based pruning, which retains numerous duplicate tokens, or use
similarity-based pruning, overlooking the instruction relevance, consequently
causing suboptimal performance. In this paper, we go beyond attention or
similarity by proposing a novel visual token pruning method named CDPruner,
which maximizes the conditional diversity of retained tokens. We first define
the conditional similarity between visual tokens conditioned on the
instruction, and then reformulate the token pruning problem with determinantal
point process (DPP) to maximize the conditional diversity of the selected
subset. The proposed CDPruner is training-free and model-agnostic, allowing
easy application to various MLLMs. Extensive experiments across diverse MLLMs
show that CDPruner establishes new state-of-the-art on various vision-language
benchmarks. By maximizing conditional diversity through DPP, the selected
subset better represents the input images while closely adhering to user
instructions, thereby preserving strong performance even with high reduction
ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\% and CUDA latency
by 78\%, while maintaining 94\% of the original accuracy. Our code is available
at https://github.com/Theia-4869/CDPruner.

</details>


### [208] [GenWorld: Towards Detecting AI-generated Real-world Simulation Videos](https://arxiv.org/pdf/2506.10975)
*Weiliang Chen, Wenzhao Zheng, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu, Yueqi Duan*

Main category: cs.CV

TL;DR: GenWorld is a high-quality, real-world simulation dataset for AI-generated video detection, addressing the lack of trustworthy datasets. The proposed SpannDetector leverages multi-view consistency for superior detection performance.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated videos threatens information credibility, but existing datasets lack realism and quality, hindering detector development.

Method: GenWorld is created with real-world simulation, high-quality forged videos, and cross-prompt diversity. SpannDetector uses multi-view consistency for detection.

Result: SpannDetector outperforms existing methods, especially in detecting high-quality videos, and highlights the importance of real-world clues.

Conclusion: GenWorld and SpannDetector advance AI-generated video detection, offering a promising direction for explainable and physically plausible detection.

Abstract: The flourishing of video generation technologies has endangered the
credibility of real-world information and intensified the demand for
AI-generated video detectors. Despite some progress, the lack of high-quality
real-world datasets hinders the development of trustworthy detectors. In this
paper, we propose GenWorld, a large-scale, high-quality, and real-world
simulation dataset for AI-generated video detection. GenWorld features the
following characteristics: (1) Real-world Simulation: GenWorld focuses on
videos that replicate real-world scenarios, which have a significant impact due
to their realism and potential influence; (2) High Quality: GenWorld employs
multiple state-of-the-art video generation models to provide realistic and
high-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes
videos generated from diverse generators and various prompt modalities (e.g.,
text, image, video), offering the potential to learn more generalizable
forensic features. We analyze existing methods and find they fail to detect
high-quality videos generated by world models (i.e., Cosmos), revealing
potential drawbacks of ignoring real-world clues. To address this, we propose a
simple yet effective model, SpannDetector, to leverage multi-view consistency
as a strong criterion for real-world AI-generated video detection. Experiments
show that our method achieves superior results, highlighting a promising
direction for explainable AI-generated video detection based on physical
plausibility. We believe that GenWorld will advance the field of AI-generated
video detection. Project Page: https://chen-wl20.github.io/GenWorld

</details>


### [209] [QuadricFormer: Scene as Superquadrics for 3D Semantic Occupancy Prediction](https://arxiv.org/pdf/2506.10977)
*Sicheng Zuo, Wenzhao Zheng, Xiaoyong Han, Longchao Yang, Yong Pan, Jiwen Lu*

Main category: cs.CV

TL;DR: QuadricFormer uses superquadrics for efficient 3D occupancy prediction, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing voxel-based and object-centric methods are inefficient or limited in modeling diverse structures in driving scenes.

Method: Proposes a probabilistic superquadric mixture model and QuadricFormer, with a pruning-and-splitting module for efficiency.

Result: Achieves state-of-the-art performance on the nuScenes dataset with superior efficiency.

Conclusion: Superquadrics enable efficient and accurate 3D occupancy prediction, addressing limitations of prior methods.

Abstract: 3D occupancy prediction is crucial for robust autonomous driving systems as
it enables comprehensive perception of environmental structures and semantics.
Most existing methods employ dense voxel-based scene representations, ignoring
the sparsity of driving scenes and resulting in inefficiency. Recent works
explore object-centric representations based on sparse Gaussians, but their
ellipsoidal shape prior limits the modeling of diverse structures. In
real-world driving scenes, objects exhibit rich geometries (e.g., cuboids,
cylinders, and irregular shapes), necessitating excessive ellipsoidal Gaussians
densely packed for accurate modeling, which leads to inefficient
representations. To address this, we propose to use geometrically expressive
superquadrics as scene primitives, enabling efficient representation of complex
structures with fewer primitives through their inherent shape diversity. We
develop a probabilistic superquadric mixture model, which interprets each
superquadric as an occupancy probability distribution with a corresponding
geometry prior, and calculates semantics through probabilistic mixture.
Building on this, we present QuadricFormer, a superquadric-based model for
efficient 3D occupancy prediction, and introduce a pruning-and-splitting module
to further enhance modeling efficiency by concentrating superquadrics in
occupied regions. Extensive experiments on the nuScenes dataset demonstrate
that QuadricFormer achieves state-of-the-art performance while maintaining
superior efficiency.

</details>


### [210] [Fine-Grained Perturbation Guidance via Attention Head Selection](https://arxiv.org/pdf/2506.10978)
*Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Minjae Kim, Jaewon Min, Wooseok Jang, Saungwu Lee, Sayak Paul, Susung Hong, Seungryong Kim*

Main category: cs.CV

TL;DR: The paper introduces 'HeadHunter' and 'SoftPAG' for fine-grained control in diffusion models by perturbing specific attention heads, improving generation quality and style manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing attention perturbation methods lack principled approaches for determining perturbation locations, especially in Diffusion Transformer architectures.

Method: Investigates granularity of attention perturbations, proposes 'HeadHunter' for selecting heads and 'SoftPAG' for tuning perturbation strength.

Result: Demonstrates superior performance in quality enhancement and style-specific guidance on models like Stable Diffusion 3 and FLUX.1.

Conclusion: Provides the first head-level analysis of attention perturbation, enabling interpretable and effective perturbation strategies.

Abstract: Recent guidance methods in diffusion models steer reverse sampling by
perturbing the model to construct an implicit weak model and guide generation
away from it. Among these approaches, attention perturbation has demonstrated
strong empirical performance in unconditional scenarios where classifier-free
guidance is not applicable. However, existing attention perturbation methods
lack principled approaches for determining where perturbations should be
applied, particularly in Diffusion Transformer (DiT) architectures where
quality-relevant computations are distributed across layers. In this paper, we
investigate the granularity of attention perturbations, ranging from the layer
level down to individual attention heads, and discover that specific heads
govern distinct visual concepts such as structure, style, and texture quality.
Building on this insight, we propose "HeadHunter", a systematic framework for
iteratively selecting attention heads that align with user-centric objectives,
enabling fine-grained control over generation quality and visual attributes. In
addition, we introduce SoftPAG, which linearly interpolates each selected
head's attention map toward an identity matrix, providing a continuous knob to
tune perturbation strength and suppress artifacts. Our approach not only
mitigates the oversmoothing issues of existing layer-level perturbation but
also enables targeted manipulation of specific visual styles through
compositional head selection. We validate our method on modern large-scale
DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,
demonstrating superior performance in both general quality enhancement and
style-specific guidance. Our work provides the first head-level analysis of
attention perturbation in diffusion models, uncovering interpretable
specialization within attention layers and enabling practical design of
effective perturbation strategies.

</details>


### [211] [InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model](https://arxiv.org/pdf/2506.10980)
*Junqi You, Chieh Hubert Lin, Weijie Lyu, Zhengbo Zhang, Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: InstaInpaint is a fast, reference-based 3D scene inpainting method that achieves real-time performance (0.4s) and outperforms prior methods in speed (1000x faster) while maintaining high quality.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene inpainting methods are too slow for real-time applications, limiting interactivity in virtual and augmented reality.

Method: Proposes InstaInpaint, a feed-forward framework using a 2D inpainting proposal and a self-supervised masked-finetuning strategy for training a large reconstruction model.

Result: Achieves 1000x speed-up over prior methods with state-of-the-art performance on benchmarks and generalizes well to applications like object insertion.

Conclusion: InstaInpaint enables real-time 3D scene inpainting, enhancing interactivity in immersive environments.

Abstract: Recent advances in 3D scene reconstruction enable real-time viewing in
virtual and augmented reality. To support interactive operations for better
immersiveness, such as moving or editing objects, 3D scene inpainting methods
are proposed to repair or complete the altered geometry. However, current
approaches rely on lengthy and computationally intensive optimization, making
them impractical for real-time or online applications. We propose InstaInpaint,
a reference-based feed-forward framework that produces 3D-scene inpainting from
a 2D inpainting proposal within 0.4 seconds. We develop a self-supervised
masked-finetuning strategy to enable training of our custom large
reconstruction model (LRM) on the large-scale dataset. Through extensive
experiments, we analyze and identify several key designs that improve
generalization, textural consistency, and geometric correctness. InstaInpaint
achieves a 1000x speed-up from prior methods while maintaining a
state-of-the-art performance across two standard benchmarks. Moreover, we show
that InstaInpaint generalizes well to flexible downstream applications such as
object insertion and multi-region inpainting. More video results are available
at our project page: https://dhmbb2.github.io/InstaInpaint_page/.

</details>


### [212] [SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis](https://arxiv.org/pdf/2506.10981)
*Weiliang Chen, Jiayi Bi, Yuanhui Huang, Wenzhao Zheng, Yueqi Duan*

Main category: cs.CV

TL;DR: SceneCompleter introduces a 3D-consistent generative novel view synthesis framework using dense 3D scene completion, improving coherence and plausibility over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for novel view synthesis (NVS) rely on 2D completion followed by 3D recovery, leading to smooth surfaces and distorted geometry due to poor 3D inference from RGB data.

Method: SceneCompleter uses a geometry-appearance dual-stream diffusion model for RGBD space synthesis and a scene embedder for holistic scene understanding.

Result: The method achieves superior visual coherence and 3D consistency in generative novel view synthesis across diverse datasets.

Conclusion: SceneCompleter effectively fuses structural and textural information, outperforming conventional paradigms in NVS.

Abstract: Generative models have gained significant attention in novel view synthesis
(NVS) by alleviating the reliance on dense multi-view captures. However,
existing methods typically fall into a conventional paradigm, where generative
models first complete missing areas in 2D, followed by 3D recovery techniques
to reconstruct the scene, which often results in overly smooth surfaces and
distorted geometry, as generative models struggle to infer 3D structure solely
from RGB data. In this paper, we propose SceneCompleter, a novel framework that
achieves 3D-consistent generative novel view synthesis through dense 3D scene
completion. SceneCompleter achieves both visual coherence and 3D-consistent
generative scene completion through two key components: (1) a
geometry-appearance dual-stream diffusion model that jointly synthesizes novel
views in RGBD space; (2) a scene embedder that encodes a more holistic scene
understanding from the reference image. By effectively fusing structural and
textural information, our method demonstrates superior coherence and
plausibility in generative novel view synthesis across diverse datasets.
Project Page: https://chen-wl20.github.io/SceneCompleter

</details>


### [213] [Federated Unsupervised Visual Representation Learning via Exploiting General Content and Personal Style](https://arxiv.org/pdf/2211.06470)
*Yuewei Yang, Jingwei Sun, Ang Li, Hai Li, Yiran Chen*

Main category: cs.CV

TL;DR: FedStyle is a novel method for federated learning that combines generalization and personalization by leveraging local style and content information for contrastive learning, outperforming baseline methods in decentralized settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adapting contrastive learning to decentralized systems with unlabeled, private, and heterogeneous data while ensuring both generalization and personalization.

Method: FedStyle infuses local style information (extracted via Sobel filtering) with content for contrastive learning and uses style for personalized local models.

Result: Outperforms generalization and personalization baselines in IID and non-IID settings, validated through extensive experiments.

Conclusion: FedStyle effectively balances generalization and personalization in decentralized learning, with style infusion and stylized personalization being key to its success.

Abstract: Discriminative unsupervised learning methods such as contrastive learning
have demonstrated the ability to learn generalized visual representations on
centralized data. It is nonetheless challenging to adapt such methods to a
distributed system with unlabeled, private, and heterogeneous client data due
to user styles and preferences. Federated learning enables multiple clients to
collectively learn a global model without provoking any privacy breach between
local clients. On the other hand, another direction of federated learning
studies personalized methods to address the local heterogeneity. However, work
on solving both generalization and personalization without labels in a
decentralized setting remains unfamiliar. In this work, we propose a novel
method, FedStyle, to learn a more generalized global model by infusing local
style information with local content information for contrastive learning, and
to learn more personalized local models by inducing local style information for
downstream tasks. The style information is extracted by contrasting original
local data with strongly augmented local data (Sobel filtered images). Through
extensive experiments with linear evaluations in both IID and non-IID settings,
we demonstrate that FedStyle outperforms both the generalization baseline
methods and personalization baseline methods in a stylized decentralized
setting. Through comprehensive ablations, we demonstrate our design of style
infusion and stylized personalization improve performance significantly.

</details>


### [214] [GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest](https://arxiv.org/pdf/2307.03601)
*Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, Ping Luo*

Main category: cs.CV

TL;DR: GPT4RoI introduces spatial instruction tuning for fine-grained vision-language understanding, enabling interaction via language and bounding boxes, and achieves near-human performance on VCR.


<details>
  <summary>Details</summary>
Motivation: Existing visual instruction tuning lacks fine-grained understanding due to missing region-text pairs.

Method: Proposes spatial instruction tuning, replacing region references with RoI features interleaved with language embeddings.

Result: GPT4RoI achieves 81.6% accuracy on VCR, surpassing all models and nearing human performance (85.0%).

Conclusion: GPT4RoI advances multimodal interaction and understanding, setting a new benchmark in vision-language tasks.

Abstract: Visual instruction tuning large language model(LLM) on image-text pairs has
achieved general-purpose vision-language abilities. However, the lack of
region-text pairs limits their advancements to fine-grained multimodal
understanding. In this paper, we propose spatial instruction tuning, which
introduces the reference to the region-of-interest(RoI) in the instruction.
Before sending to LLM, the reference is replaced by RoI features and
interleaved with language embeddings as a sequence. Our model GPT4RoI, trained
on 7 region-text pair datasets, brings an unprecedented interactive and
conversational experience compared to previous image-level models. (1)
Interaction beyond language: Users can interact with our model by both language
and drawing bounding boxes to flexibly adjust the referring granularity. (2)
Versatile multimodal abilities: A variety of attribute information within each
RoI can be mined by GPT4RoI, e.g., color, shape, material, action, etc.
Furthermore, it can reason about multiple RoIs based on common sense. On the
Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable
accuracy of 81.6%, surpassing all existing models by a significant margin (the
second place is 75.6%) and almost reaching human-level performance of 85.0%.
The code and model can be found at https://github.com/jshilong/GPT4RoI.

</details>


### [215] [DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for Fingerprint Presentation Attack Detection](https://arxiv.org/pdf/2308.10015)
*Anuj Rai, Parsheel Kumar Tiwari, Jyotishna Baishya, Ram Prakash Sharma, Somnath Dey*

Main category: cs.CV

TL;DR: A dynamic ensemble of deep CNN and handcrafted features is proposed to detect fingerprint presentation attacks, outperforming state-of-the-art methods with high accuracy on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Fingerprint recognition systems are vulnerable to presentation attacks, necessitating robust detection methods to ensure security in applications like border control and commerce.

Method: The paper combines deep CNN and handcrafted features in a dynamic ensemble to detect attacks, tested on known and unknown-material protocols from Liveness Detection Competitions.

Result: Achieved high accuracy (96.10%, 96.49%, 94.99%) on 2015, 2017, and 2019 benchmark datasets, surpassing existing methods.

Conclusion: The hybrid approach of deep and handcrafted features effectively detects presentation attacks, demonstrating superior performance over individual techniques.

Abstract: Automatic fingerprint recognition systems suffer from the threat of
presentation attacks due to their wide range of deployment in areas including
national borders and commercial applications. A presentation attack can be
performed by creating a spoof of a user's fingerprint with or without their
consent. This paper presents a dynamic ensemble of deep CNN and handcrafted
features to detect presentation attacks in known-material and unknown-material
protocols of the liveness detection competition. The proposed presentation
attack detection model, in this way, utilizes the capabilities of both deep CNN
and handcrafted features techniques and exhibits better performance than their
individual performances. We have validated our proposed method on benchmark
databases from the Liveness Detection Competition in 2015, 2017, and 2019,
yielding overall accuracy of 96.10%, 96.49%, and 94.99% on them, respectively.
The proposed method outperforms state-of-the-art methods in terms of
classification accuracy.

</details>


### [216] [CapST: Leveraging Capsule Networks and Temporal Attention for Accurate Model Attribution in Deep-fake Videos](https://arxiv.org/pdf/2311.03782)
*Wasim Ahmad, Yan-Tsung Peng, Yuan-Hao Chang, Gaddisa Olani Ganfure, Sarwar Khan*

Main category: cs.CV

TL;DR: The paper proposes a Capsule-Spatial-Temporal (CapST) model for attributing deep-fake videos to their specific generation models, outperforming baselines in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Deep-fake attribution is crucial for forensic analysis and tailored countermeasures, but most research focuses only on real vs. fake detection.

Method: The task is framed as multiclass classification. CapST combines VGG19 for feature extraction, capsule networks for hierarchical encoding, and spatio-temporal attention for video-level fusion.

Result: CapST achieves higher attribution accuracy on DFDM and GANGen-Detection datasets while reducing computational costs.

Conclusion: The CapST model effectively addresses deep-fake attribution, enhancing forensic analysis and proactive defense strategies.

Abstract: Deep-fake videos, generated through AI face-swapping techniques, have gained
significant attention due to their potential for impactful impersonation
attacks. While most research focuses on real vs. fake detection, attributing a
deep-fake to its specific generation model or encoder is vital for forensic
analysis, enabling source tracing and tailored countermeasures. This enhances
detection by leveraging model-specific artifacts and supports proactive
defenses. We investigate the model attribution problem for deep-fake videos
using two datasets: Deepfakes from Different Models (DFDM) and
GANGen-Detection, both comprising deep-fake videos and GAN-generated images. We
use only fake images from GANGen-Detection to align with DFDM's focus on
attribution rather than binary classification. We formulate the task as a
multiclass classification problem and introduce a novel
Capsule-Spatial-Temporal (CapST) model that integrates a truncated VGG19
network for feature extraction, capsule networks for hierarchical encoding, and
a spatio-temporal attention mechanism. Video-level fusion captures temporal
dependencies across frames. Experiments on DFDM and GANGen-Detection show CapST
outperforms baseline models in attribution accuracy while reducing
computational cost.

</details>


### [217] [Vastextures: Vast repository of textures and PBR materials extracted from real-world images using unsupervised methods](https://arxiv.org/pdf/2406.17146)
*Sagi Eppel*

Main category: cs.CV

TL;DR: Vastextures is a large, diverse repository of 500,000 textures and PBR materials extracted from real-world images, targeting synthetic data for AI training.


<details>
  <summary>Details</summary>
Motivation: Existing repositories focus on high-quality assets for games and arts, but AI training requires vast, diverse, though less refined, materials.

Method: Materials are extracted in two steps: 1) cropping uniform textures from images via statistical analysis, and 2) guessing PBR properties from textures.

Result: The repository offers diverse real-world patterns and emergent properties, improving AI training performance over handcrafted assets.

Conclusion: Vastextures meets the need for a free, large-scale, diverse material repository for AI applications.

Abstract: Vastextures is a vast repository of 500,000 textures and PBR materials
extracted from real-world images using an unsupervised process. The extracted
materials and textures are extremely diverse and cover a vast range of
real-world patterns, but at the same time less refined compared to existing
repositories. The repository is composed of 2D textures cropped from natural
images and SVBRDF/PBR materials generated from these textures. Textures and PBR
materials are essential for CGI. Existing materials repositories focus on
games, animation, and arts, that demand a limited amount of high-quality
assets. However, virtual worlds and synthetic data are becoming increasingly
important for training A.I systems for computer vision. This application
demands a huge amount of diverse assets but at the same time less affected by
noisy and unrefined assets. Vastexture aims to address this need by creating a
free, huge, and diverse assets repository that covers as many real-world
materials as possible. The materials are automatically extracted from natural
images in two steps: 1) Automatically scanning a giant amount of images to
identify and crop regions with uniform textures. This is done by splitting the
image into a grid of cells and identifying regions in which all of the cells
share a similar statistical distribution. 2) Extracting the properties of the
PBR material from the cropped texture. This is done by randomly guessing every
correlation between the properties of the texture image and the properties of
the PBR material. The resulting PBR materials exhibit a vast amount of
real-world patterns as well as unexpected emergent properties. Neutral nets
trained on this repository outperformed nets trained using handcrafted assets.

</details>


### [218] [Learning Geometric Invariant Features for Classification of Vector Polygons with Graph Message-passing Neural Network](https://arxiv.org/pdf/2407.04334)
*Zexian Huang, Kourosh Khoshelham, Martin Tomko*

Main category: cs.CV

TL;DR: The paper introduces PolyMP and PolyMP-DSC, graph-based frameworks for classifying vector polygons, outperforming baselines by capturing geometric-invariant features robust to transformations.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored area of discrete polygon representations and learning methods for geometric shape classification.

Method: Proposes PolyMP and PolyMP-DSC, graph message-passing frameworks with hierarchical self-looped graph learning for geometric-invariant features.

Result: Outperforms baselines on synthetic and real-world datasets, showing robustness to transformations and vertex removals.

Conclusion: The frameworks effectively capture and generalize geometric features, enabling transfer learning from synthetic to real-world data.

Abstract: Geometric shape classification of vector polygons remains a challenging task
in spatial analysis. Previous studies have primarily focused on deep learning
approaches for rasterized vector polygons, while the study of discrete polygon
representations and corresponding learning methods remains underexplored. In
this study, we investigate a graph-based representation of vector polygons and
propose a simple graph message-passing framework, PolyMP, along with its
densely self-connected variant, PolyMP-DSC, to learn more expressive and robust
latent representations of polygons. This framework hierarchically captures
self-looped graph information and learns geometric-invariant features for
polygon shape classification. Through extensive experiments, we demonstrate
that combining a permutation-invariant graph message-passing neural network
with a densely self-connected mechanism achieves robust performance on
benchmark datasets, including synthetic glyphs and real-world building
footprints, outperforming several baseline methods. Our findings indicate that
PolyMP and PolyMP-DSC effectively capture expressive geometric features that
remain invariant under common transformations, such as translation, rotation,
scaling, and shearing, while also being robust to trivial vertex removals.
Furthermore, we highlight the strong generalization ability of the proposed
approach, enabling the transfer of learned geometric features from synthetic
glyph polygons to real-world building footprints.

</details>


### [219] [CORT: Class-Oriented Real-time Tracking for Embedded Systems](https://arxiv.org/pdf/2407.17521)
*Edoardo Cittadini, Alessandro De Siena, Giorgio Buttazzo*

Main category: cs.CV

TL;DR: A new multi-class object tracking method balances accuracy and speed by splitting the Hungarian matrix by class and selectively using re-identification.


<details>
  <summary>Details</summary>
Motivation: Current tracking algorithms either prioritize accuracy with complex heuristics or speed by skipping re-identification, but not both.

Method: The approach splits the Hungarian matrix by class and limits re-identification to necessary cases, reducing execution time without sacrificing accuracy.

Result: Tested in urban scenarios with diverse objects, the method outperforms state-of-the-art trackers in both speed and accuracy.

Conclusion: The proposed solution effectively balances tracking performance and timing efficiency, making it suitable for real-time applications.

Abstract: The ever-increasing use of artificial intelligence in autonomous systems has
significantly contributed to advance the research on multi-object tracking,
adopted in several real-time applications (e.g., autonomous driving,
surveillance drones, robotics) to localize and follow the trajectory of
multiple objects moving in front of a camera. Current tracking algorithms can
be divided into two main categories: some approaches introduce complex
heuristics and re-identification models to improve the tracking accuracy and
reduce the number of identification switches, without particular attention to
the timing performance, whereas other approaches are aimed at reducing response
times by removing the re-identification phase, thus penalizing the tracking
accuracy. This work proposes a new approach to multi-class object tracking that
allows achieving smaller and more predictable execution times, without
penalizing the tracking performance. The idea is to reduce the problem of
matching predictions with detections into smaller sub-problems by splitting the
Hungarian matrix by class and invoking the second re-identification stage only
when strictly necessary for a smaller number of elements. The proposed solution
was evaluated in complex urban scenarios with several objects of different
types (as cars, trucks, bikes, and pedestrians), showing the effectiveness of
the multi-class approach with respect to state of the art trackers.

</details>


### [220] [TDS-CLIP: Temporal Difference Side Network for Efficient VideoAction Recognition](https://arxiv.org/pdf/2408.10688)
*Bin Wang, Wentong Li, Wenqian Wang, Mingliang Gao, Runmin Cong, Wei Zhang*

Main category: cs.CV

TL;DR: Proposes TDS-CLIP, a memory-efficient side network for video action recognition, balancing knowledge transfer from pre-trained models and temporal modeling without backpropagation in frozen models.


<details>
  <summary>Details</summary>
Motivation: Current methods transfer frozen knowledge from pre-trained models to VAR models without leveraging their temporal modeling capabilities, limiting performance.

Method: Introduces TD-Adapter for local temporal differences and SME-Adapter to enhance motion learning in a side network.

Result: Achieves competitive performance on Something-Something V1&V2 and Kinetics-400 datasets.

Conclusion: TDS-CLIP effectively balances knowledge transfer and temporal modeling, improving VAR task performance.

Abstract: Recently, large-scale pre-trained vision-language models (e.g., CLIP), have
garnered significant attention thanks to their powerful representative
capabilities. This inspires researchers in transferring the knowledge from
these large pre-trained models to other task-specific models, e.g., Video
Action Recognition (VAR) models, via particularly leveraging side networks to
enhance the efficiency of parameter-efficient fine-tuning (PEFT). However,
current transferring approaches in VAR tend to directly transfer the frozen
knowledge from large pre-trained models to action recognition networks with
minimal cost, instead of exploiting the temporal modeling capabilities of the
action recognition models themselves. Therefore, in this paper, we propose a
novel memory-efficient Temporal Difference Side Network (TDS-CLIP) to balance
knowledge transferring and temporal modeling, avoiding backpropagation in
frozen parameter models. Specifically, we introduce a Temporal Difference
Adapter (TD-Adapter), which can effectively capture local temporal differences
in motion features to strengthen the model's global temporal modeling
capabilities. Furthermore, we designed a Side Motion Enhancement Adapter
(SME-Adapter) to guide the proposed side network in efficiently learning the
rich motion information in videos, thereby improving the side network's ability
to capture and learn motion information. Extensive experiments are conducted on
three benchmark datasets, including Something-Something V1&V2, and
Kinetics-400. Experimental results show that our method achieves competitive
performance in video action recognition tasks.

</details>


### [221] [PointNet with KAN versus PointNet with MLP for 3D Classification and Segmentation of Point Sets](https://arxiv.org/pdf/2410.10084)
*Ali Kashefi*

Main category: cs.CV

TL;DR: PointNet-KAN integrates Kolmogorov-Arnold Networks (KANs) into PointNet for 3D point cloud tasks, showing competitive performance with simpler architecture.


<details>
  <summary>Details</summary>
Motivation: To explore the untested effectiveness of KANs in point-cloud-based neural networks, specifically for classification and segmentation tasks.

Method: Introduces PointNet-KAN, replacing MLPs with KANs, using Jacobi polynomials for activation functions, and evaluating various polynomial types.

Result: PointNet-KAN matches PointNet-MLP performance on benchmarks with a simpler, shallower design.

Conclusion: Demonstrates KANs' potential as an MLP alternative in point cloud architectures, suggesting further exploration in advanced models.

Abstract: Kolmogorov-Arnold Networks (KANs) have recently gained attention as an
alternative to traditional Multilayer Perceptrons (MLPs) in deep learning
frameworks. KANs have been integrated into various deep learning architectures
such as convolutional neural networks, graph neural networks, and transformers,
with their performance evaluated. However, their effectiveness within
point-cloud-based neural networks remains unexplored. To address this gap, we
incorporate KANs into PointNet for the first time to evaluate their performance
on 3D point cloud classification and segmentation tasks. Specifically, we
introduce PointNet-KAN, built upon two key components. First, it employs KANs
instead of traditional MLPs. Second, it retains the core principle of PointNet
by using shared KAN layers and applying symmetric functions for global feature
extraction, ensuring permutation invariance with respect to the input features.
In traditional MLPs, the goal is to train the weights and biases with fixed
activation functions; however, in KANs, the goal is to train the activation
functions themselves. We use Jacobi polynomials to construct the KAN layers. We
extensively and systematically evaluate PointNet-KAN across various polynomial
degrees and special types such as the Lagrange, Chebyshev, and Gegenbauer
polynomials. Our results show that PointNet-KAN achieves competitive
performance compared to PointNet with MLPs on benchmark datasets for 3D object
classification and part and semantic segmentation, despite employing a
shallower and simpler network architecture. We also study a hybrid PointNet
model incorporating both KAN and MLP layers. We hope this work serves as a
foundation and provides guidance for integrating KANs, as an alternative to
MLPs, into more advanced point cloud processing architectures.

</details>


### [222] [Factorized Video Autoencoders for Efficient Generative Modelling](https://arxiv.org/pdf/2412.04452)
*Mohammed Suhail, Carlos Esteves, Leonid Sigal, Ameesh Makadia*

Main category: cs.CV

TL;DR: The paper proposes a four-plane factorized latent space autoencoder for efficient training and inference in higher-dimensional domains like videos, improving speed and memory usage in latent diffusion models.


<details>
  <summary>Details</summary>
Motivation: Latent variable models face challenges in higher-dimensional domains (e.g., videos) due to computational inefficiency. The goal is to enable efficient generative tasks like video synthesis.

Method: An autoencoder maps volumetric data to a four-plane factorized latent space, which grows sublinearly with input size. This design supports conditional generation tasks with latent diffusion models (LDMs).

Result: The four-plane latent space retains high-fidelity reconstructions despite heavy compression and significantly improves speed and memory usage in LDMs.

Conclusion: The proposed autoencoder is effective for higher-dimensional data, enabling efficient generative tasks while maintaining quality.

Abstract: Latent variable generative models have emerged as powerful tools for
generative tasks including image and video synthesis. These models are enabled
by pretrained autoencoders that map high resolution data into a compressed
lower dimensional latent space, where the generative models can subsequently be
developed while requiring fewer computational resources. Despite their
effectiveness, the direct application of latent variable models to higher
dimensional domains such as videos continues to pose challenges for efficient
training and inference. In this paper, we propose an autoencoder that projects
volumetric data onto a four-plane factorized latent space that grows
sublinearly with the input size, making it ideal for higher dimensional data
like videos. The design of our factorized model supports straightforward
adoption in a number of conditional generation tasks with latent diffusion
models (LDMs), such as class-conditional generation, frame prediction, and
video interpolation. Our results show that the proposed four-plane latent space
retains a rich representation needed for high-fidelity reconstructions despite
the heavy compression, while simultaneously enabling LDMs to operate with
significant improvements in speed and memory.

</details>


### [223] [Few-Shot Learner Generalizes Across AI-Generated Image Detection](https://arxiv.org/pdf/2501.08763)
*Shiyu Wu, Jing Liu, Jing Li, Yequan Wang*

Main category: cs.CV

TL;DR: FSD, a few-shot AI-generated image detector, outperforms existing methods by +11.6% accuracy on unseen models with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing detectors fail on unseen generative models, and collecting training data is costly.

Method: FSD learns a specialized metric space to distinguish fake images using very few samples.

Result: Achieves +11.6% average accuracy on GenImage with only 10 additional samples.

Conclusion: FSD effectively captures intra-category commonality in unseen images without retraining.

Abstract: Current fake image detectors trained on large synthetic image datasets
perform satisfactorily on limited studied generative models. However, these
detectors suffer a notable performance decline over unseen models. Besides,
collecting adequate training data from online generative models is often
expensive or infeasible. To overcome these issues, we propose Few-Shot Detector
(FSD), a novel AI-generated image detector which learns a specialized metric
space for effectively distinguishing unseen fake images using very few samples.
Experiments show that FSD achieves state-of-the-art performance by $+11.6\%$
average accuracy on the GenImage dataset with only $10$ additional samples.
More importantly, our method is better capable of capturing the intra-category
commonality in unseen images without further training. Our code is available at
https://github.com/teheperinko541/Few-Shot-AIGI-Detector.

</details>


### [224] [Latent Action Learning Requires Supervision in the Presence of Distractors](https://arxiv.org/pdf/2502.00379)
*Alexander Nikulin, Ilya Zisman, Denis Tarasov, Nikita Lyubaykin, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov*

Main category: cs.CV

TL;DR: LAOM improves latent action learning by 8x in distracting scenarios, and minimal supervision (2.5% of data) boosts performance by 4.2x.


<details>
  <summary>Details</summary>
Motivation: Prior latent action learning (LAPO) struggles with real-world distractors, limiting its effectiveness.

Method: Proposed LAOM, a modified LAPO, tested on Distracting Control Suite (DCS) with minimal supervision.

Result: LAOM improves latent action quality by 8x; 2.5% supervision enhances performance by 4.2x.

Conclusion: Supervision during latent action training is crucial for handling distractors, challenging traditional LAM pipelines.

Abstract: Recently, latent action learning, pioneered by Latent Action Policies (LAPO),
have shown remarkable pre-training efficiency on observation-only data,
offering potential for leveraging vast amounts of video available on the web
for embodied AI. However, prior work has focused on distractor-free data, where
changes between observations are primarily explained by ground-truth actions.
Unfortunately, real-world videos contain action-correlated distractors that may
hinder latent action learning. Using Distracting Control Suite (DCS) we
empirically investigate the effect of distractors on latent action learning and
demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO
modification that improves the quality of latent actions by 8x, as measured by
linear probing. Importantly, we show that providing supervision with
ground-truth actions, as few as 2.5% of the full dataset, during latent action
learning improves downstream performance by 4.2x on average. Our findings
suggest that integrating supervision during Latent Action Models (LAM) training
is critical in the presence of distractors, challenging the conventional
pipeline of first learning LAM and only then decoding from latent to
ground-truth actions.

</details>


### [225] [Object-Centric Latent Action Learning](https://arxiv.org/pdf/2502.09680)
*Albina Klepach, Alexander Nikulin, Ilya Zisman, Denis Tarasov, Alexander Derevyagin, Andrei Polubarov, Nikita Lyubaykin, Vladislav Kurenkov*

Main category: cs.CV

TL;DR: A novel object-centric latent action learning framework improves robustness in embodied AI by focusing on objects, not pixels, mitigating distractor effects by 50%.


<details>
  <summary>Details</summary>
Motivation: The lack of action labels and presence of distractors in unlabeled internet video data hinder embodied AI progress.

Method: Proposes an object-centric latent action learning framework with self-supervised pretraining to disentangle action-related and distracting dynamics.

Result: Achieves 50% reduction in distractor effects, improving performance in Distracting Control Suite and Distracting MetaWorld tasks.

Conclusion: Object-centric pretraining enhances robustness and efficiency in imitation learning for embodied AI.

Abstract: Leveraging vast amounts of unlabeled internet video data for embodied AI is
currently bottlenecked by the lack of action labels and the presence of
action-correlated visual distractors. Although recent latent action policy
optimization (LAPO) has shown promise in inferring proxy-action labels from
visual observations, its performance degrades significantly when distractors
are present. To address this limitation, we propose a novel object-centric
latent action learning framework that centers on objects rather than pixels. We
leverage self-supervised object-centric pretraining to disentangle
action-related and distracting dynamics. This allows LAPO to focus on
task-relevant interactions, resulting in more robust proxy-action labels,
enabling better imitation learning and efficient adaptation of the agent with
just a few action-labeled trajectories. We evaluated our method in eight
visually complex tasks across the Distracting Control Suite (DCS) and
Distracting MetaWorld (DMW). Our results show that object-centric pretraining
mitigates the negative effects of distractors by 50%, as measured by downstream
task performance: average return (DCS) and success rate (DMW).

</details>


### [226] [EgoNormia: Benchmarking Physical Social Norm Understanding](https://arxiv.org/pdf/2502.20490)
*MohammadHossein Rezaei, Yicheng Fu, Phil Cuvin, Caleb Ziems, Yanzhe Zhang, Hao Zhu, Diyi Yang*

Main category: cs.CV

TL;DR: EGONORMIA is a dataset of 1,853 MCQs for evaluating normative reasoning in VLMs, showing current models lack robust understanding, scoring up to 54%. A RAG method improves performance.


<details>
  <summary>Details</summary>
Motivation: Supervision for normative reasoning is sparse, especially for physically- or socially-grounded norms, necessitating a dataset like EGONORMIA.

Method: A pipeline generates grounded MCQs from egocentric videos, spanning seven norm categories. RAG is explored for improvement.

Result: VLMs score max 54% on EGONORMIA, 65% on verified subset, highlighting safety/privacy risks. RAG enhances normative reasoning.

Conclusion: EGONORMIA reveals VLMs' normative reasoning gaps and offers a method (RAG) to improve it, critical for real-world agent deployment.

Abstract: Human activity is moderated by norms; however, supervision for normative
reasoning is sparse, particularly where norms are physically- or
socially-grounded. We thus present EGONORMIA $\|\epsilon\|$, comprising 1,853
(200 for EGONORMIA-verified) multiple choice questions (MCQs) grounded within
egocentric videos of human interactions, enabling the evaluation and
improvement of normative reasoning in vision-language models (VLMs). EGONORMIA
spans seven norm categories: safety, privacy, proxemics, politeness,
cooperation, coordination/proactivity, and communication/legibility. To compile
this dataset at scale, we propose a novel pipeline to generate grounded MCQs
from raw egocentric video. Our work demonstrates that current state-of-the-art
VLMs lack robust grounded norm understanding, scoring a maximum of 54% on
EGONORMIA and 65% on EGONORMIA-verified, with performance across norm
categories indicating significant risks of safety and privacy when VLMs are
used in real-world agents. We additionally explore methods for improving
normative understanding, demonstrating that a naive retrieval-based generation
(RAG) method using EGONORMIA can enhance normative reasoning in VLMs.

</details>


### [227] [CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting](https://arxiv.org/pdf/2503.12836)
*Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, Sangpil Kim*

Main category: cs.CV

TL;DR: Proposes a compression-tolerant anchor-based 3DGS watermarking method to protect copyright and maintain rendering quality under compression.


<details>
  <summary>Details</summary>
Motivation: The need for copyright protection and efficient compression in 3D Gaussian Splatting (3DGS) due to its widespread use and large model size.

Method: Introduces anchor-based watermarking, embeds watermarks in anchor attributes, adds a quantization distortion layer, and uses a frequency-aware anchor growing strategy.

Result: Preserves watermark integrity and rendering quality even under compression.

Conclusion: The method effectively addresses copyright protection and compression challenges in 3DGS.

Abstract: 3D Gaussian Splatting (3DGS) is increasingly adopted in various academic and
commercial applications due to its real-time and high-quality rendering
capabilities, emphasizing the growing need for copyright protection
technologies for 3DGS. However, the large model size of 3DGS requires
developing efficient compression techniques. This highlights the necessity of
an integrated framework that addresses copyright protection and data
compression for 3D content. Nevertheless, existing 3DGS watermarking methods
significantly degrade watermark performance under 3DGS compression methods,
particularly quantization-based approaches that achieve superior compression
performance. To ensure reliable watermark detection under compression, we
propose a compression-tolerant anchor-based 3DGS watermarking, which preserves
watermark integrity and rendering quality. This is achieved by introducing
anchor-based 3DGS watermarking. We embed the watermark into the anchor
attributes, particularly the anchor feature, to enhance security and rendering
quality. We also propose a quantization distortion layer that injects
quantization noise during training, preserving the watermark after
quantization-based compression. Moreover, we employ a frequency-aware anchor
growing strategy that improves rendering quality and watermark performance by
effectively identifying Gaussians in high-frequency regions. Extensive
experiments demonstrate that our proposed method preserves the watermark even
under compression and maintains high rendering quality.

</details>


### [228] [Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts](https://arxiv.org/pdf/2503.16057)
*Yike Yuan, Ziyu Wang, Zihao Huang, Defa Zhu, Xun Zhou, Jingyi Yu, Qiyang Min*

Main category: cs.CV

TL;DR: Race-DiT integrates MoE with diffusion transformers using Expert Race for dynamic expert assignment, achieving better performance and scalability.


<details>
  <summary>Details</summary>
Motivation: Enhance scalability and performance of diffusion models by integrating MoE methods.

Method: Introduces Race-DiT with Expert Race routing, per-layer regularization, and router similarity loss.

Result: Validated on ImageNet, showing significant performance gains and scaling properties.

Conclusion: Race-DiT effectively improves diffusion transformers with MoE, offering dynamic expert assignment and robust learning.

Abstract: Diffusion models have emerged as mainstream framework in visual generation.
Building upon this success, the integration of Mixture of Experts (MoE) methods
has shown promise in enhancing model scalability and performance. In this
paper, we introduce Race-DiT, a novel MoE model for diffusion transformers with
a flexible routing strategy, Expert Race. By allowing tokens and experts to
compete together and select the top candidates, the model learns to dynamically
assign experts to critical tokens. Additionally, we propose per-layer
regularization to address challenges in shallow layer learning, and router
similarity loss to prevent mode collapse, ensuring better expert utilization.
Extensive experiments on ImageNet validate the effectiveness of our approach,
showcasing significant performance gains while promising scaling properties.

</details>


### [229] [What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning](https://arxiv.org/pdf/2503.21055)
*Chi-Hsi Kung, Frangil Ramirez, Juhyung Ha, Yi-Ting Chen, David Crandall, Yi-Hsuan Tsai*

Main category: cs.CV

TL;DR: The paper introduces a method for procedure-aware video representation learning by using state-change descriptions from LLMs and counterfactual reasoning to improve understanding of procedural activities.


<details>
  <summary>Details</summary>
Motivation: Existing work lacks explicit learning of scene transformations in procedural activities, which are crucial for understanding cause and effect.

Method: Incorporates state-change descriptions from LLMs and generates counterfactuals to simulate failure outcomes, enhancing video encoders.

Result: Significant improvements on tasks like action segmentation, error detection, and action recognition.

Conclusion: The approach effectively enhances procedure awareness in video understanding, with plans to release code and data.

Abstract: Understanding a procedural activity requires modeling both how action steps
transform the scene and how evolving scene transformations can influence the
sequence of action steps, even those that are accidental or erroneous. Existing
work has studied procedure-aware video representations by proposing novel
approaches such as modeling the temporal order of actions, and has not
explicitly learned the state changes (scene transformations). In this work, we
study procedure-aware video representation learning by incorporating
state-change descriptions generated by Large Language Models (LLMs) as
supervision signals for video encoders. Moreover, we generate state-change
counterfactuals that simulate hypothesized failure outcomes, allowing models to
learn by imagining the unseen ``What if'' scenarios. This counterfactual
reasoning facilitates the model's ability to understand the cause and effect of
each step in an activity. To verify the procedure awareness of our model, we
conduct extensive experiments on procedure-aware tasks, including temporal
action segmentation, error detection, action phase classification, frame
retrieval, multi-instance retrieval, and action recognition. Our results
demonstrate the effectiveness of the proposed state-change descriptions and
their counterfactuals, and achieve significant improvements on multiple tasks.
We will make our source code and data publicly available soon.

</details>


### [230] [CAT: A Conditional Adaptation Tailor for Efficient and Effective Instance-Specific Pansharpening on Real-World Data](https://arxiv.org/pdf/2504.10242)
*Tianyu Xin, Jin-Liang Xiao, Zeyu Xia, Shan Yin, Liang-Jian Deng*

Main category: cs.CV

TL;DR: An efficient pansharpening framework using unsupervised CAT training for improved cross-sensor generalization and computational efficiency, achieving fast training and inference.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of existing deep learning pansharpening methods, such as poor cross-sensor generalization and high computational overhead, to enable real-time applications.

Method: Splits input into patches, selects a subset for unsupervised CAT training, and stitches results. The CAT module tailors features and fixes parameters for efficient inference.

Result: Achieves state-of-the-art performance on cross-sensor data, with fast processing times (0.4s for 512x512 and 3s for 4000x4000 images).

Conclusion: The proposed framework enhances generalization and efficiency, making it suitable for real-world remote sensing applications.

Abstract: Pansharpening is a crucial remote sensing technique that fuses low-resolution
multispectral (LRMS) images with high-resolution panchromatic (PAN) images to
generate high-resolution multispectral (HRMS) imagery. Although deep learning
techniques have significantly advanced pansharpening, many existing methods
suffer from limited cross-sensor generalization and high computational
overhead, restricting their real-time applications. To address these
challenges, we propose an efficient framework that quickly adapts to a specific
input instance, completing both training and inference in a short time. Our
framework splits the input image into multiple patches, selects a subset for
unsupervised CAT training, and then performs inference on all patches,
stitching them into the final output. The CAT module, integrated between the
feature extraction and channel transformation stages of a pre-trained network,
tailors the fused features and fixes the parameters for efficient inference,
generating improved results. Our approach offers two key advantages: (1)
$\textit{Improved Generalization Ability}$: by mitigating cross-sensor
degradation, our model--although pre-trained on a specific dataset--achieves
superior performance on datasets captured by other sensors; (2)
$\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can
swiftly adapt to the test sample using the single LRMS-PAN pair input, without
requiring extensive large-scale data retraining. Experiments on the real-world
data from WorldView-3 and WorldView-2 datasets demonstrate that our method
achieves state-of-the-art performance on cross-sensor real-world data, while
achieving both training and inference of $512\times512$ image within
$\textit{0.4 seconds}$ and $4000\times4000$ image within $\textit{3 seconds}$
at the fastest setting on a commonly used RTX 3090 GPU.

</details>


### [231] [Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control](https://arxiv.org/pdf/2505.03134)
*Sajjad Rezvani Boroujeni, Hossein Abedi, Tom Bush*

Main category: cs.CV

TL;DR: A novel DDPM-based method generates synthetic defective glass images to address class imbalance, improving CNN performance in defect detection.


<details>
  <summary>Details</summary>
Motivation: Addressing class imbalance in industrial glass defect detection due to rare defective products, which limits deep learning model performance.

Method: Uses Denoising Diffusion Probabilistic Models (DDPMs) to create synthetic defective images for data augmentation, enhancing CNN architectures like ResNet50V2, EfficientNetB0, and MobileNetV2.

Result: Significant improvements in recall for defective samples and overall accuracy (e.g., ResNet50V2's accuracy rose from 78% to 93%).

Conclusion: The scalable DDPM approach effectively enhances defect detection and can be extended to other industries with similar imbalance issues.

Abstract: Visual defect detection in industrial glass manufacturing remains a critical
challenge due to the low frequency of defective products, leading to imbalanced
datasets that limit the performance of deep learning models and computer vision
systems. This paper presents a novel approach using Denoising Diffusion
Probabilistic Models (DDPMs) to generate synthetic defective glass product
images for data augmentation, effectively addressing class imbalance issues in
manufacturing quality control and automated visual inspection. The methodology
significantly enhances image classification performance of standard CNN
architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting
anomalies by increasing the minority class representation. Experimental results
demonstrate substantial improvements in key machine learning metrics,
particularly in recall for defective samples across all tested deep neural
network architectures while maintaining perfect precision. The most dramatic
improvement was observed in ResNet50V2's overall classification accuracy, which
increased from seventy-eight percent to ninety-three percent when trained with
the augmented data. This work provides a scalable, cost effective approach to
enhancing automated defect detection in glass manufacturing that can
potentially be extended to other industrial quality assurance systems and
industries with similar class imbalance challenges.

</details>


### [232] [Aesthetics Without Semantics](https://arxiv.org/pdf/2505.05331)
*C. Alejandro Parraga, Olivier Penacchio, Marcos Muňoz Gonzalez, Bogdan Raducanu, Xavier Otazu*

Main category: cs.CV

TL;DR: The paper addresses biases in aesthetic image databases by creating a balanced dataset (MSC) of 10,426 images with minimal semantic content, including ugly images, to better study aesthetic judgments.


<details>
  <summary>Details</summary>
Motivation: Current databases are biased towards beautiful images, limiting the study of aesthetic responses. The paper aims to overcome this by including ugly images and minimal semantic content.

Method: The authors created the MSC database with images of minimal semantic content, balanced for aesthetic value, and used established image metrics to analyze relationships between features and aesthetics.

Result: Augmenting biased datasets with ugly images can alter or invert observed relationships between image features and aesthetic judgments.

Conclusion: Limiting the range of aesthetic values in studies can distort findings, highlighting the need for balanced datasets in empirical aesthetics.

Abstract: While it is easy for human observers to judge an image as beautiful or ugly,
aesthetic decisions result from a combination of entangled perceptual and
cognitive (semantic) factors, making the understanding of aesthetic judgements
particularly challenging from a scientific point of view. Furthermore, our
research shows a prevailing bias in current databases, which include mostly
beautiful images, further complicating the study and prediction of aesthetic
responses. We address these limitations by creating a database of images with
minimal semantic content and devising, and next exploiting, a method to
generate images on the ugly side of aesthetic valuations. The resulting Minimum
Semantic Content (MSC) database consists of a large and balanced collection of
10,426 images, each evaluated by 100 observers. We next use established image
metrics to demonstrate how augmenting an image set biased towards beautiful
images with ugly images can modify, or even invert, an observed relationship
between image features and aesthetics valuation. Taken together, our study
reveals that works in empirical aesthetics attempting to link image content and
aesthetic judgements may magnify, underestimate, or simply miss interesting
effects due to a limitation of the range of aesthetic values they consider.

</details>


### [233] [ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability in MLLMs with Free-Style Intermediate State Representations](https://arxiv.org/pdf/2505.14404)
*Xuecheng Wu, Jiaxing Liu, Danlei Huang, Xiaoyu Li, Yifan Wang, Chen Chen, Liya Ma, Xuezhi Cao, Junxiao Xue*

Main category: cs.CV

TL;DR: VI-CoT improves MLLMs' reasoning by using step-wise visual states, but current benchmarks lack free-style IVS. ViC-Bench addresses this with four tasks and a new evaluation suite.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks limit MLLMs' reasoning by providing fixed IVS, failing to evaluate intrinsic capabilities or explore IVS impact factors.

Method: Introduces ViC-Bench with free-style IVS generation and a three-stage evaluation suite, including IPII for prompting analysis.

Result: Evaluated 18 MLLMs, revealing insights into their VI-CoT capabilities.

Conclusion: ViC-Bench advances MLLM evaluation by addressing gaps in IVS flexibility and reasoning analysis.

Abstract: Visual-Interleaved Chain-of-Thought (VI-CoT) enables MLLMs to continually
update their understanding and decisions based on step-wise intermediate visual
states (IVS), much like a human would, which demonstrates impressive success in
various tasks, thereby leading to emerged advancements in related benchmarks.
Despite promising progress, current benchmarks provide models with relatively
fixed IVS, rather than free-style IVS, whch might forcibly distort the original
thinking trajectories, failing to evaluate their intrinsic reasoning
capabilities. More importantly, existing benchmarks neglect to systematically
explore the impact factors that IVS would impart to untamed reasoning
performance. To tackle above gaps, we introduce a specialized benchmark termed
ViC-Bench, consisting of four representive tasks: maze navigation, jigsaw
puzzle, embodied long-horizon planning, and complex counting, where each task
has dedicated free-style IVS generation pipeline supporting function calls. To
systematically examine VI-CoT capability, we propose a thorough evaluation
suite incorporating a progressive three-stage strategy with targeted new
metrics. Besides, we establish Incremental Prompting Information Injection
(IPII) strategy to ablatively explore the prompting factors for VI-CoT. We
extensively conduct evaluations for 18 advanced MLLMs, revealing key insights
into their VI-CoT capability. Our proposed benchmark is publicly open at
Huggingface.

</details>


### [234] [Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling](https://arxiv.org/pdf/2505.14521)
*Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, Bihan Wen*

Main category: cs.CV

TL;DR: Sparc3D introduces a unified framework for high-fidelity 3D object synthesis using sparse deformable marching cubes and a novel VAE, achieving state-of-the-art results with efficient training and inference.


<details>
  <summary>Details</summary>
Motivation: Existing 3D synthesis methods suffer from detail loss and inefficiency due to unstructured mesh data and modality mismatches in VAEs.

Method: Sparc3D combines Sparcubes (sparse deformable marching cubes) and Sparconv-VAE (a sparse convolutional VAE) for efficient, high-resolution 3D reconstruction and generative modeling.

Result: Sparc3D achieves state-of-the-art fidelity, preserves fine details, and reduces computational costs, enabling scalable 3D generation.

Conclusion: Sparc3D provides a robust solution for high-resolution 3D synthesis, outperforming existing methods in fidelity and efficiency.

Abstract: High-fidelity 3D object synthesis remains significantly more challenging than
2D image generation due to the unstructured nature of mesh data and the cubic
complexity of dense volumetric grids. Existing two-stage pipelines-compressing
meshes with a VAE (using either 2D or 3D supervision), followed by latent
diffusion sampling-often suffer from severe detail loss caused by inefficient
representations and modality mismatches introduced in VAE. We introduce
Sparc3D, a unified framework that combines a sparse deformable marching cubes
representation Sparcubes with a novel encoder Sparconv-VAE. Sparcubes converts
raw meshes into high-resolution ($1024^3$) surfaces with arbitrary topology by
scattering signed distance and deformation fields onto a sparse cube, allowing
differentiable optimization. Sparconv-VAE is the first modality-consistent
variational autoencoder built entirely upon sparse convolutional networks,
enabling efficient and near-lossless 3D reconstruction suitable for
high-resolution generative modeling through latent diffusion. Sparc3D achieves
state-of-the-art reconstruction fidelity on challenging inputs, including open
surfaces, disconnected components, and intricate geometry. It preserves
fine-grained shape details, reduces training and inference cost, and integrates
naturally with latent diffusion models for scalable, high-resolution 3D
generation.

</details>


### [235] [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/pdf/2505.22654)
*Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, Dong Yu*

Main category: cs.CV

TL;DR: VScan is a two-stage visual token reduction framework that accelerates inference in LVLMs by pruning redundant tokens during visual encoding and language decoding, achieving significant speedups with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs incur high computational costs due to long visual token sequences, hindering real-time deployment. Prior token pruning methods are limited in effectiveness.

Method: VScan integrates global and local scans with token merging during visual encoding and introduces pruning at intermediate language model layers.

Result: VScan achieves a 2.91× speedup in prefilling and a 10× FLOPs reduction while retaining 95.4% of original performance on LLaVA-NeXT-7B.

Conclusion: VScan effectively balances computational efficiency and performance, outperforming state-of-the-art methods on multiple benchmarks.

Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal
understanding by incorporating finer-grained visual perception and encoding.
However, such methods incur significant computational costs due to longer
visual token sequences, posing challenges for real-time deployment. To mitigate
this, prior studies have explored pruning unimportant visual tokens either at
the output layer of the visual encoder or at the early layers of the language
model. In this work, we revisit these design choices and reassess their
effectiveness through comprehensive empirical studies of how visual tokens are
processed throughout the visual encoding and language decoding stages. Guided
by these insights, we propose VScan, a two-stage visual token reduction
framework that addresses token redundancy by: (1) integrating complementary
global and local scans with token merging during visual encoding, and (2)
introducing pruning at intermediate layers of the language model. Extensive
experimental results across four LVLMs validate the effectiveness of VScan in
accelerating inference and demonstrate its superior performance over current
state-of-the-arts on sixteen benchmarks. Notably, when applied to
LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a
10$\times$ reduction in FLOPs, while retaining 95.4\% of the original
performance. Code is available at
https://github.com/Tencent/SelfEvolvingAgent/tree/main/VScan.

</details>


### [236] [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/pdf/2506.01413)
*Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun*

Main category: cs.CV

TL;DR: The paper addresses the limitations of chain-of-thought (CoT) in LLMs for complex instructions and proposes a reinforcement learning-based method to improve reasoning and performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with complex instructions due to shallow reasoning in vanilla CoT, which paraphrases rather than analyzes constraints.

Method: The method involves decomposing instructions, using RL with rule-centric rewards, and contrastive learning for better CoT enforcement, alongside expert behavior cloning.

Result: A 1.5B LLM achieved 11.74% performance gains, matching an 8B LLM's capabilities.

Conclusion: The proposed systematic approach effectively enhances LLMs' reasoning for complex instructions, validated across seven benchmarks.

Abstract: Existing large language models (LLMs) face challenges of following complex
instructions, especially when multiple constraints are present and organized in
paralleling, chaining, and branching structures. One intuitive solution, namely
chain-of-thought (CoT), is expected to universally improve capabilities of
LLMs. However, we find that the vanilla CoT exerts a negative impact on
performance due to its superficial reasoning pattern of simply paraphrasing the
instructions. It fails to peel back the compositions of constraints for
identifying their relationship across hierarchies of types and dimensions. To
this end, we propose a systematic method to boost LLMs in dealing with complex
instructions via incentivizing reasoning for test-time compute scaling. First,
we stem from the decomposition of complex instructions under existing
taxonomies and propose a reproducible data acquisition method. Second, we
exploit reinforcement learning (RL) with verifiable rule-centric reward signals
to cultivate reasoning specifically for instruction following. We address the
shallow, non-essential nature of reasoning under complex instructions via
sample-wise contrast for superior CoT enforcement. We also exploit behavior
cloning of experts to facilitate steady distribution shift from fast-thinking
LLMs to skillful reasoners. Extensive evaluations on seven comprehensive
benchmarks confirm the validity of the proposed method, where a 1.5B LLM
achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data
are available at https://github.com/yuleiqin/RAIF.

</details>


### [237] [High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset](https://arxiv.org/pdf/2506.02614)
*Guohang Zhuang, Weixi Song, Jinyang Huang, Chenwei Yang, Wanli OuYang, Yan Lu*

Main category: cs.CV

TL;DR: A deep learning-based Space Debris Tracking Network (SDT-Net) is proposed for accurate debris tracking, supported by a large-scale dataset (SDTD). The model achieves strong performance in real-world tests.


<details>
  <summary>Details</summary>
Motivation: The increasing threat of space debris necessitates real-time and accurate tracking, which traditional methods fail to address due to complex backgrounds and dense debris.

Method: Proposes SDT-Net, a deep learning model for debris tracking, and introduces SDTD, a large-scale dataset generated via observation-based simulation.

Result: SDT-Net achieves a MOTA score of 70.6% on real-world data from the Antarctic Station, demonstrating high accuracy and transferability.

Conclusion: The proposed SDT-Net and SDTD dataset effectively address space debris tracking challenges, showing promise for real-world applications.

Abstract: With the rapid development of space exploration, space debris has attracted
more attention due to its potential extreme threat, leading to the need for
real-time and accurate debris tracking. However, existing methods are mainly
based on traditional signal processing, which cannot effectively process the
complex background and dense space debris. In this paper, we propose a deep
learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly
accurate debris tracking. SDT-Net effectively represents the feature of debris,
enhancing the efficiency and stability of end-to-end model learning. To train
and evaluate this model effectively, we also produce a large-scale dataset
Space Debris Tracking Dataset (SDTD) by a novel observation-based data
simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562
frames and covers 250,000 synthetic space debris. Extensive experiments
validate the effectiveness of our model and the challenging of our dataset.
Furthermore, we test our model on real data from the Antarctic Station,
achieving a MOTA score of 70.6%, which demonstrates its strong transferability
to real-world scenarios. Our dataset and code will be released soon.

</details>


### [238] [Towards Holistic Visual Quality Assessment of AI-Generated Videos: A LLM-Based Multi-Dimensional Evaluation Model](https://arxiv.org/pdf/2506.04715)
*Zelu Qi, Ping Shi, Chaoyang Zhang, Shuqi Wang, Fei Zhao, Da Pan, Zefeng Ying*

Main category: cs.CV

TL;DR: The paper proposes a method for assessing AI-generated video (AIGV) quality using a multi-dimensional approach and a large language model (LLM) for regression, achieving second place in a 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: AIGVs often have visual defects (noise, blur, etc.) that degrade user experience, necessitating an automatic quality assessment method.

Method: Decomposes AIGV quality into technical, motion, and semantic dimensions, uses encoders for feature representation, and employs an LLM with multi-modal prompts and LoRA fine-tuning for quality regression.

Result: The method secured second place in the NTIRE 2025 Quality Assessment Challenge for AI-generated videos.

Conclusion: The proposed framework effectively assesses AIGV quality, leveraging LLMs and multi-modal features, and shows promise for content regulation and model improvement.

Abstract: The development of AI-Generated Video (AIGV) technology has been remarkable
in recent years, significantly transforming the paradigm of video content
production. However, AIGVs still suffer from noticeable visual quality defects,
such as noise, blurriness, frame jitter and low dynamic degree, which severely
impact the user's viewing experience. Therefore, an effective automatic visual
quality assessment is of great importance for AIGV content regulation and
generative model improvement. In this work, we decompose the visual quality of
AIGVs into three dimensions: technical quality, motion quality, and video
semantics. For each dimension, we design corresponding encoder to achieve
effective feature representation. Moreover, considering the outstanding
performance of large language models (LLMs) in various vision and language
tasks, we introduce a LLM as the quality regression module. To better enable
the LLM to establish reasoning associations between multi-dimensional features
and visual quality, we propose a specially designed multi-modal prompt
engineering framework. Additionally, we incorporate LoRA fine-tuning technology
during the training phase, allowing the LLM to better adapt to specific tasks.
Our proposed method achieved \textbf{second place} in the NTIRE 2025 Quality
Assessment of AI-Generated Content Challenge: Track 2 AI Generated video,
demonstrating its effectiveness. Codes can be obtained at
https://github.com/QiZelu/AIGVEval.

</details>


### [239] [Spike-TBR: a Noise Resilient Neuromorphic Event Representation](https://arxiv.org/pdf/2506.04817)
*Gabriele Magrini, Federico Becattini, Luca Cultrera, Lorenzo Berlincioni, Pietro Pala, Alberto Del Bimbo*

Main category: cs.CV

TL;DR: Spike-TBR is a novel event-based encoding method combining Temporal Binary Representation (TBR) with spiking neurons to enhance noise resilience and compatibility with standard vision pipelines.


<details>
  <summary>Details</summary>
Motivation: Event cameras outperform traditional sensors in temporal resolution and dynamic range, but converting noisy event streams into usable formats remains challenging.

Method: Spike-TBR integrates spiking neurons with TBR to filter noise and improve robustness. Four variants using different spiking neurons were tested.

Result: Spike-TBR shows superior performance in noisy scenarios and improves results on clean data.

Conclusion: The method bridges spike-based and frame-based processing, providing a noise-resilient solution for event-driven vision.

Abstract: Event cameras offer significant advantages over traditional frame-based
sensors, including higher temporal resolution, lower latency and dynamic range.
However, efficiently converting event streams into formats compatible with
standard computer vision pipelines remains a challenging problem, particularly
in the presence of noise. In this paper, we propose Spike-TBR, a novel
event-based encoding strategy based on Temporal Binary Representation (TBR),
addressing its vulnerability to noise by integrating spiking neurons. Spike-TBR
combines the frame-based advantages of TBR with the noise-filtering
capabilities of spiking neural networks, creating a more robust representation
of event streams. We evaluate four variants of Spike-TBR, each using different
spiking neurons, across multiple datasets, demonstrating superior performance
in noise-affected scenarios while improving the results on clean data. Our
method bridges the gap between spike-based and frame-based processing, offering
a simple noise-resilient solution for event-driven vision applications.

</details>


### [240] [Towards Reliable Identification of Diffusion-based Image Manipulations](https://arxiv.org/pdf/2506.05466)
*Alex Costanzino, Woody Bayliss, Juil Sock, Marc Gorriz Blanch, Danijela Horak, Ivan Laptev, Philip Torr, Fabio Pizzati*

Main category: cs.CV

TL;DR: RADAR is a novel method for detecting and localizing manipulated areas in images, leveraging foundation models and multimodal features, outperforming state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: The rise of advanced diffusion models for image manipulation necessitates reliable tools to identify such edits to combat misuse.

Method: RADAR combines features from different image modalities and uses an auxiliary contrastive loss to isolate manipulated patches.

Result: RADAR outperforms existing methods in accuracy and generalization across 28 diffusion models, validated on the BBC-PAIR benchmark.

Conclusion: RADAR is effective for detecting and localizing edits, even with unseen diffusion models, and will be publicly available.

Abstract: Changing facial expressions, gestures, or background details may dramatically
alter the meaning conveyed by an image. Notably, recent advances in diffusion
models greatly improve the quality of image manipulation while also opening the
door to misuse. Identifying changes made to authentic images, thus, becomes an
important task, constantly challenged by new diffusion-based editing tools. To
this end, we propose a novel approach for ReliAble iDentification of inpainted
AReas (RADAR). RADAR builds on existing foundation models and combines features
from different image modalities. It also incorporates an auxiliary contrastive
loss that helps to isolate manipulated image patches. We demonstrate these
techniques to significantly improve both the accuracy of our method and its
generalisation to a large number of diffusion models. To support realistic
evaluation, we further introduce BBC-PAIR, a new comprehensive benchmark, with
images tampered by 28 diffusion models. Our experiments show that RADAR
achieves excellent results, outperforming the state-of-the-art in detecting and
localising image edits made by both seen and unseen diffusion models. Our code,
data and models will be publicly available at
https://alex-costanzino.github.io/radar/.

</details>


### [241] [AR-RAG: Autoregressive Retrieval Augmentation for Image Generation](https://arxiv.org/pdf/2506.06962)
*Jingyuan Qi, Zhiyang Xu, Qifan Wang, Lifu Huang*

Main category: cs.CV

TL;DR: AR-RAG enhances image generation by dynamically retrieving and incorporating patch-level references during each generation step, avoiding issues like over-copying and stylistic bias. It introduces two frameworks, DAiD and FAiD, for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on static retrievals, leading to limitations like over-copying and stylistic bias. AR-RAG aims to address these by enabling context-aware, dynamic retrievals during generation.

Method: AR-RAG uses two frameworks: (1) DAiD, a training-free decoding strategy merging predicted and retrieved patch distributions, and (2) FAiD, a fine-tuning method smoothing retrieved patch features for augmentation.

Result: AR-RAG shows significant performance gains on benchmarks like Midjourney-30K, GenEval, and DPG-Bench, outperforming state-of-the-art models.

Conclusion: AR-RAG introduces a dynamic retrieval approach for image generation, improving flexibility and performance while mitigating issues in existing methods.

Abstract: We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm
that enhances image generation by autoregressively incorporating knearest
neighbor retrievals at the patch level. Unlike prior methods that perform a
single, static retrieval before generation and condition the entire generation
on fixed reference images, AR-RAG performs context-aware retrievals at each
generation step, using prior-generated patches as queries to retrieve and
incorporate the most relevant patch-level visual references, enabling the model
to respond to evolving generation needs while avoiding limitations (e.g.,
over-copying, stylistic bias, etc.) prevalent in existing methods. To realize
AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in
Decoding (DAiD), a training-free plug-and-use decoding strategy that directly
merges the distribution of model-predicted patches with the distribution of
retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a
parameter-efficient fine-tuning method that progressively smooths the features
of retrieved patches via multi-scale convolution operations and leverages them
to augment the image generation process. We validate the effectiveness of
AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and
DPG-Bench, demonstrating significant performance gains over state-of-the-art
image generation models.

</details>


### [242] [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/pdf/2506.07464)
*Jinyoung Park, Jeehye Na, Jinyoung Kim, Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: The paper explores GRPO for Video LLMs, addressing issues like reliance on safeguards and vanishing advantage with Reg-GRPO and difficulty-aware data augmentation, leading to improved video reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the reasoning capabilities of Video LLMs by addressing limitations of GRPO, such as reliance on safeguards and vanishing advantage.

Method: Proposes Reg-GRPO, a regression-based reformulation of GRPO, and a difficulty-aware data augmentation strategy.

Result: DeepVideo-R1 significantly improves video reasoning performance across benchmarks.

Conclusion: The proposed methods effectively mitigate GRPO's issues, enhancing Video LLM performance.

Abstract: Recent works have demonstrated the effectiveness of reinforcement learning
(RL)-based post-training in enhancing the reasoning capabilities of large
language models (LLMs). In particular, Group Relative Policy Optimization
(GRPO) has shown impressive success by employing a PPO-style reinforcement
algorithm with group-based normalized rewards. However, the application of GRPO
to Video Large Language Models (Video LLMs) has been less studied. In this
paper, we explore GRPO for video LLMs and identify two primary issues that
impede its effective learning: (1) reliance on safeguards, and (2) the
vanishing advantage problem. To mitigate these challenges, we propose
DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO
(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO
reformulates the GRPO objective as a regression task, directly predicting the
advantage in GRPO. This design eliminates the need for safeguards like clipping
and min functions, thereby facilitating more direct policy guidance by aligning
the model with the advantage values. We also design the difficulty-aware data
augmentation strategy that dynamically augments training samples at solvable
difficulty levels, fostering diverse and informative reward signals. Our
comprehensive experiments show that DeepVideo-R1 significantly improves video
reasoning performance across multiple video reasoning benchmarks.

</details>


### [243] [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/pdf/2506.08011)
*Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, Chen Wei*

Main category: cs.CV

TL;DR: ViGaL post-trains MLLMs on arcade games via RL, enhancing multimodal reasoning without domain-specific data, outperforming specialist models.


<details>
  <summary>Details</summary>
Motivation: To improve generalizable reasoning in MLLMs by leveraging gameplay's cognitive benefits, inspired by cognitive science.

Method: Post-training a 7B-parameter MLLM via RL on arcade-like games (e.g., Snake) to enhance reasoning skills.

Result: Improved performance on multimodal math (MathVista) and multi-discipline (MMMU) benchmarks, surpassing specialist models.

Conclusion: Synthetic, rule-based games can effectively unlock generalizable multimodal reasoning in MLLMs.

Abstract: Developing generalizable reasoning capabilities in multimodal large language
models (MLLMs) remains challenging. Motivated by cognitive science literature
suggesting that gameplay promotes transferable cognitive skills, we propose a
novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs
develop out-of-domain generalization of multimodal reasoning through playing
arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM
via reinforcement learning (RL) on simple arcade-like games, e.g. Snake,
significantly enhances its downstream performance on multimodal math benchmarks
like MathVista, and on multi-discipline questions like MMMU, without seeing any
worked solutions, equations, or diagrams during RL, suggesting the capture of
transferable reasoning skills. Remarkably, our model outperforms specialist
models tuned on multimodal reasoning data in multimodal reasoning benchmarks,
while preserving the base model's performance on general visual benchmarks, a
challenge where specialist models often fall short. Our findings suggest a new
post-training paradigm: synthetic, rule-based games can serve as controllable
and scalable pre-text tasks that unlock generalizable multimodal reasoning
abilities in MLLMs.

</details>


### [244] [InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba](https://arxiv.org/pdf/2506.08735)
*Yuhang Wang, Jun Li, Zhijian Wu, Jianhua Xu*

Main category: cs.CV

TL;DR: InceptionMamba improves spatial and global context modeling over InceptionNeXt by replacing strip convolutions with orthogonal band convolutions and adding a bottleneck Mamba module.


<details>
  <summary>Details</summary>
Motivation: InceptionNeXt's limitations in capturing spatial dependencies and global context modeling prompted the development of InceptionMamba.

Method: Replaces one-dimensional strip convolutions with orthogonal band convolutions and introduces a bottleneck Mamba module for global context.

Result: Achieves state-of-the-art performance in classification and downstream tasks with efficient parameters and computation.

Conclusion: InceptionMamba outperforms InceptionNeXt by enhancing spatial and global modeling, offering superior efficiency.

Abstract: Within the family of convolutional neural networks, InceptionNeXt has shown
excellent competitiveness in image classification and a number of downstream
tasks. Built on parallel one-dimensional strip convolutions, however, it
suffers from limited ability of capturing spatial dependencies along different
dimensions and fails to fully explore spatial modeling in local neighborhood.
Besides, inherent locality constraints of convolution operations are
detrimental to effective global context modeling. To overcome these
limitations, we propose a novel backbone architecture termed InceptionMamba in
this study. More specifically, the traditional one-dimensional strip
convolutions are replaced by orthogonal band convolutions in our InceptionMamba
to achieve cohesive spatial modeling. Furthermore, global contextual modeling
can be achieved via a bottleneck Mamba module, facilitating enhanced
cross-channel information fusion and enlarged receptive field. Extensive
evaluations on classification and various downstream tasks demonstrate that the
proposed InceptionMamba achieves state-of-the-art performance with superior
parameter and computational efficiency. The source code will be available at
https://github.com/Wake1021/InceptionMamba.

</details>


### [245] [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/pdf/2506.08817)
*Shuyi Zhang, Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Pengwei Wang, Zhongyuan Wang, Hongxuan Ma, Shanghang Zhang*

Main category: cs.CV

TL;DR: Video-CoT is a new dataset and benchmark for enhancing spatiotemporal understanding in video analysis using Chain-of-Thought methodologies, revealing current VLMs' limitations.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with nuanced spatiotemporal details in video analysis, necessitating a specialized dataset and benchmark.

Method: Introduces Video-CoT, a dataset with 192,000 question-answer pairs and 23,000 CoT-annotated samples, plus a benchmark with 750 images per task.

Result: Experiments show VLMs perform poorly on spatiotemporal tasks, highlighting the challenge.

Conclusion: Video-CoT advances research in video comprehension and supports future intelligent systems.

Abstract: Video content comprehension is essential for various applications, ranging
from video analysis to interactive systems. Despite advancements in large-scale
vision-language models (VLMs), these models often struggle to capture the
nuanced, spatiotemporal details essential for thorough video analysis. To
address this gap, we introduce Video-CoT, a groundbreaking dataset designed to
enhance spatiotemporal understanding using Chain-of-Thought (CoT)
methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal
question-answer pairs and 23,000 high-quality CoT-annotated samples, providing
a solid foundation for evaluating spatiotemporal understanding in video
comprehension. Additionally, we provide a comprehensive benchmark for assessing
these tasks, with each task featuring 750 images and tailored evaluation
metrics. Our extensive experiments reveal that current VLMs face significant
challenges in achieving satisfactory performance, high-lighting the
difficulties of effective spatiotemporal understanding. Overall, the Video-CoT
dataset and benchmark open new avenues for research in multimedia understanding
and support future innovations in intelligent systems requiring advanced video
analysis capabilities. By making these resources publicly available, we aim to
encourage further exploration in this critical area. Project
website:https://video-cot.github.io/ .

</details>


### [246] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/pdf/2506.09042)
*Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao, Shengyu Huang, Amirmojtaba Sabour, Tianchang Shen, Tobias Pfaff, Jay Zhangjie Wu, Runjian Chen, Seung Wook Kim, Jun Gao, Laura Leal-Taixe, Mike Chen, Sanja Fidler, Huan Ling*

Main category: cs.CV

TL;DR: The paper introduces Cosmos-Drive-Dreams, a synthetic data generation pipeline for creating challenging driving scenarios to aid AV system training, addressing the scarcity of real-world edge cases.


<details>
  <summary>Details</summary>
Motivation: Real-world data collection for AV systems is costly and struggles to capture rare edge cases, which are crucial for robust training and testing.

Method: The Cosmos-Drive-Dreams pipeline uses NVIDIA Cosmos models to generate controllable, high-fidelity, multi-view, and spatiotemporally consistent driving videos.

Result: The generated data improves handling of long-tail distribution issues and enhances performance in tasks like 3D lane detection, object detection, and driving policy learning.

Conclusion: The open-sourced pipeline and tools provide a scalable solution for generating diverse, high-quality synthetic driving data, benefiting AV system development.

Abstract: Collecting and annotating real-world data for safety-critical physical AI
systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is
especially challenging to capture rare edge cases, which play a critical role
in training and testing of an AV system. To address this challenge, we
introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline
that aims to generate challenging scenarios to facilitate downstream tasks such
as perception and driving policy training. Powering this pipeline is
Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation
model for the driving domain and are capable of controllable, high-fidelity,
multi-view, and spatiotemporally consistent driving video generation. We
showcase the utility of these models by applying Cosmos-Drive-Dreams to scale
the quantity and diversity of driving datasets with high-fidelity and
challenging scenarios. Experimentally, we demonstrate that our generated data
helps in mitigating long-tail distribution problems and enhances generalization
in downstream tasks such as 3D lane detection, 3D object detection and driving
policy learning. We open source our pipeline toolkit, dataset and model weights
through the NVIDIA's Cosmos platform.
  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [247] [ODG: Occupancy Prediction Using Dual Gaussians](https://arxiv.org/pdf/2506.09417)
*Yunxiao Shi, Yinhao Zhu, Shizhong Han, Jisoo Jeong, Amin Ansari, Hong Cai, Fatih Porikli*

Main category: cs.CV

TL;DR: ODG introduces a hierarchical dual sparse Gaussian representation for occupancy prediction in autonomous driving, outperforming existing methods with efficient inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with scalability or handling diverse object characteristics in driving scenes.

Method: ODG uses dual Gaussian queries (static and dynamic) and a hierarchical Gaussian transformer for occupancy and semantic prediction, enhanced by rendering supervision.

Result: Achieves state-of-the-art performance on Occ3D-nuScenes and Occ3D-Waymo benchmarks with low inference cost.

Conclusion: ODG effectively models complex scenes and improves occupancy prediction, making it suitable for real-world autonomous driving.

Abstract: Occupancy prediction infers fine-grained 3D geometry and semantics from
camera images of the surrounding environment, making it a critical perception
task for autonomous driving. Existing methods either adopt dense grids as scene
representation, which is difficult to scale to high resolution, or learn the
entire scene using a single set of sparse queries, which is insufficient to
handle the various object characteristics. In this paper, we present ODG, a
hierarchical dual sparse Gaussian representation to effectively capture complex
scene dynamics. Building upon the observation that driving scenes can be
universally decomposed into static and dynamic counterparts, we define dual
Gaussian queries to better model the diverse scene objects. We utilize a
hierarchical Gaussian transformer to predict the occupied voxel centers and
semantic classes along with the Gaussian parameters. Leveraging the real-time
rendering capability of 3D Gaussian Splatting, we also impose rendering
supervision with available depth and semantic map annotations injecting
pixel-level alignment to boost occupancy learning. Extensive experiments on the
Occ3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets
new state-of-the-art results while maintaining low inference cost.

</details>


### [248] [Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries](https://arxiv.org/pdf/2506.09476)
*Tianxiang Hao, Lixian Zhang, Yingjia Zhang, Mengxuan Chen, Jinxiao Zhang, Haohuan Fu*

Main category: cs.CV

TL;DR: The paper introduces Urban1960SatBench, a novel annotated segmentation dataset for historical satellite imagery, and Urban1960SatUSM, an unsupervised segmentation framework, to address quality and annotation challenges in studying urban development.


<details>
  <summary>Details</summary>
Motivation: Historical satellite imagery suffers from quality degradation and lack of annotations, hindering semantic segmentation and urban development studies.

Method: Urban1960SatUSM uses a confidence-aware alignment mechanism and focal-confidence loss in a self-supervised learning architecture for unsupervised segmentation.

Result: Urban1960SatUSM outperforms existing methods on Urban1960SatBench, enabling better segmentation of historical urban scenes.

Conclusion: The work advances quantitative studies of long-term urban change by providing a pioneering dataset and effective unsupervised segmentation framework.

Abstract: Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,
offers rare insights into understanding early urban development and long-term
transformation. However, severe quality degradation (e.g., distortion,
misalignment, and spectral scarcity) and annotation absence have long hindered
semantic segmentation on such historical RS imagery. To bridge this gap and
enhance understanding of urban development, we introduce
$\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on
historical satellite imagery with the earliest observation time among all
existing segmentation datasets, along with a benchmark framework for
unsupervised segmentation tasks, $\textbf{Urban1960SatUSM}$. First,
$\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic
segmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering
1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the
earliest segmentation dataset of its kind, it provides a pioneering benchmark
for historical urban understanding. Second,
$\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel
unsupervised semantic segmentation framework for historical RS imagery. It
employs a confidence-aware alignment mechanism and focal-confidence loss based
on a self-supervised learning architecture, which generates robust
pseudo-labels and adaptively prioritizes prediction difficulty and label
reliability to improve unsupervised segmentation on noisy historical data
without manual supervision. Experiments show Urban1960SatUSM significantly
outperforms existing unsupervised segmentation methods on Urban1960SatSeg for
segmenting historical urban scenes, promising in paving the way for
quantitative studies of long-term urban change using modern computer vision.
Our benchmark and supplementary material are available at
https://github.com/Tianxiang-Hao/Urban1960SatSeg.

</details>


### [249] [Consistent Story Generation with Asymmetry Zigzag Sampling](https://arxiv.org/pdf/2506.09612)
*Mingxiao Li, Mang Ning, Marie-Francine Moens*

Main category: cs.CV

TL;DR: A novel training-free sampling strategy, Zigzag Sampling with Asymmetric Prompts and Visual Sharing, improves subject consistency in text-to-image generation for visual storytelling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for maintaining subject consistency in visual storytelling are either resource-intensive or ineffective.

Method: Proposes zigzag sampling with asymmetric prompts and a visual sharing module to enhance consistency without additional training.

Result: Outperforms previous methods in generating coherent and consistent visual stories, validated by metrics and evaluations.

Conclusion: The introduced strategy effectively addresses subject consistency in visual storytelling without requiring fine-tuning.

Abstract: Text-to-image generation models have made significant progress in producing
high-quality images from textual descriptions, yet they continue to struggle
with maintaining subject consistency across multiple images, a fundamental
requirement for visual storytelling. Existing methods attempt to address this
by either fine-tuning models on large-scale story visualization datasets, which
is resource-intensive, or by using training-free techniques that share
information across generations, which still yield limited success. In this
paper, we introduce a novel training-free sampling strategy called Zigzag
Sampling with Asymmetric Prompts and Visual Sharing to enhance subject
consistency in visual story generation. Our approach proposes a zigzag sampling
mechanism that alternates between asymmetric prompting to retain subject
characteristics, while a visual sharing module transfers visual cues across
generated images to %further enforce consistency. Experimental results, based
on both quantitative metrics and qualitative evaluations, demonstrate that our
method significantly outperforms previous approaches in generating coherent and
consistent visual stories. The code is available at
https://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.

</details>


### [250] [MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion](https://arxiv.org/pdf/2506.09834)
*Chuang Ma, Yu Pei, Jianhang Zhang, Shaokai Zhao, Bowen Ji, Liang Xie, Ye Yan, Erwei Yin*

Main category: cs.CV

TL;DR: The paper introduces MMME, a novel multimodal dataset for micro-expression (ME) analysis, combining visual and physiological signals to enhance recognition and spotting performance.


<details>
  <summary>Details</summary>
Motivation: Existing ME research is limited to single visual modality, missing rich emotional data from physiological signals, which hampers practical application performance.

Method: The study develops the MMME dataset, synchronizing facial action signals (MEs), EEG, and peripheral physiological signals (PPG, RSP, SKT, EDA, ECG) to enable multimodal fusion.

Result: Experiments show integrating MEs with physiological signals significantly improves recognition and spotting, validating MMME's reliability and providing benchmarks.

Conclusion: MMME is the most comprehensive ME dataset to date, enabling exploration of neural mechanisms and visual-physiological synergy, shifting ME research toward multimodal fusion.

Abstract: Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an
individual's genuine emotional state. Their analysis has attracted considerable
interest due to its promising applications in fields such as healthcare,
criminal investigation, and human-computer interaction. However, existing ME
research is limited to single visual modality, overlooking the rich emotional
information conveyed by other physiological modalities, resulting in ME
recognition and spotting performance far below practical application needs.
Therefore, exploring the cross-modal association mechanism between ME visual
features and physiological signals (PS), and developing a multimodal fusion
framework, represents a pivotal step toward advancing ME analysis. This study
introduces a novel ME dataset, MMME, which, for the first time, enables
synchronized collection of facial action signals (MEs), central nervous system
signals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming
the constraints of existing ME corpora, MMME comprises 634 MEs, 2,841
macro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,
establishing a robust foundation for investigating ME neural mechanisms and
conducting multimodal fusion-based analyses. Extensive experiments validate the
dataset's reliability and provide benchmarks for ME analysis, demonstrating
that integrating MEs with PS significantly enhances recognition and spotting
performance. To the best of our knowledge, MMME is the most comprehensive ME
dataset to date in terms of modality diversity. It provides critical data
support for exploring the neural mechanisms of MEs and uncovering the
visual-physiological synergistic effects, driving a paradigm shift in ME
research from single-modality visual analysis to multimodal fusion. The dataset
will be publicly available upon acceptance of this paper.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [251] [A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI](https://arxiv.org/pdf/2506.10130)
*Luciano Floridi*

Main category: cs.AI

TL;DR: The paper formalizes a trade-off in AI systems between provable correctness and broad data-mapping capacity, highlighting that systems with deductive guarantees have narrow domains, while generative models accept irreducible errors.


<details>
  <summary>Details</summary>
Motivation: The tension between correctness and flexibility in AI systems has historical roots and impacts engineering and philosophical expectations. The conjecture aims to make this trade-off explicit.

Method: The conjecture is stated in information-theoretic terms and analyzed within epistemology, formal verification, and philosophy of technology, using concepts like underdetermination and epistemic risk.

Result: The conjecture reframes AI evaluation standards, governance, and hybrid system design, emphasizing the need for rigorous verification.

Conclusion: Proving or refuting the conjecture is crucial for the future of trustworthy AI, influencing standards and frameworks.

Abstract: This article introduces a conjecture that formalises a fundamental trade-off
between provable correctness and broad data-mapping capacity in Artificial
Intelligence (AI) systems. When an AI system is engineered for deductively
watertight guarantees (demonstrable certainty about the error-free nature of
its outputs) -- as in classical symbolic AI -- its operational domain must be
narrowly circumscribed and pre-structured. Conversely, a system that can input
high-dimensional data to produce rich information outputs -- as in contemporary
generative models -- necessarily relinquishes the possibility of zero-error
performance, incurring an irreducible risk of errors or misclassification. By
making this previously implicit trade-off explicit and open to rigorous
verification, the conjecture significantly reframes both engineering ambitions
and philosophical expectations for AI. After reviewing the historical
motivations for this tension, the article states the conjecture in
information-theoretic form and contextualises it within broader debates in
epistemology, formal verification, and the philosophy of technology. It then
offers an analysis of its implications and consequences, drawing on notions of
underdetermination, prudent epistemic risk, and moral responsibility. The
discussion clarifies how, if correct, the conjecture would help reshape
evaluation standards, governance frameworks, and hybrid system design. The
conclusion underscores the importance of eventually proving or refuting the
inequality for the future of trustworthy AI.

</details>


### [252] [One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence](https://arxiv.org/pdf/2506.10157)
*Michelle M. Li, Ben Y. Reis, Adam Rodman, Tianxi Cai, Noa Dagan, Ran D. Balicer, Joseph Loscalzo, Isaac S. Kohane, Marinka Zitnik*

Main category: cs.AI

TL;DR: Medical foundation models can assist in clinical tasks but struggle with adapting to new contexts, leading to errors. The paper proposes context-switching AI to dynamically adapt without retraining.


<details>
  <summary>Details</summary>
Motivation: Current medical AI models fail to adjust to new populations, specialties, or settings, causing contextual errors.

Method: The paper suggests context-switching AI that dynamically adapts reasoning without retraining.

Result: Proposed models could improve adaptability across specialties and regions.

Conclusion: Context-switching AI could enhance medical care by dynamically adjusting to diverse clinical contexts.

Abstract: Medical foundation models, including language models trained on clinical
notes, vision-language models on medical images, and multimodal models on
electronic health records, can summarize clinical notes, answer medical
questions, and assist in decision-making. Adapting these models to new
populations, specialties, or settings typically requires fine-tuning, careful
prompting, or retrieval from knowledge bases. This can be impractical, and
limits their ability to interpret unfamiliar inputs and adjust to clinical
situations not represented during training. As a result, models are prone to
contextual errors, where predictions appear reasonable but fail to account for
critical patient-specific or contextual information. These errors stem from a
fundamental limitation that current models struggle with: dynamically adjusting
their behavior across evolving contexts of medical care. In this Perspective,
we outline a vision for context-switching in medical AI: models that
dynamically adapt their reasoning without retraining to new specialties,
populations, workflows, and clinical roles. We envision context-switching AI to
diagnose, manage, and treat a wide range of diseases across specialties and
regions, and expand access to medical care.

</details>


### [253] [A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pokémon](https://arxiv.org/pdf/2506.10326)
*Cameron Angliss, Jiaxun Cui, Jiaheng Hu, Arrasy Rahman, Peter Stone*

Main category: cs.AI

TL;DR: The paper introduces VGC-Bench, a benchmark for AI agents in Pokémon VGC, highlighting challenges in generalization across diverse team strategies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of AI agents adapting to vastly different strategic landscapes in multi-agent learning, particularly in Pokémon VGC's complex team configurations.

Method: The study uses VGC-Bench, providing infrastructure, evaluation protocols, human-play datasets, and baselines like LLM agents, behavior cloning, reinforcement learning, and game-theoretic methods.

Result: Agents trained on single-team configurations can beat professionals, but struggle to scale as team size grows, showing generalization remains a challenge.

Conclusion: Policy generalization across diverse team strategies is still an open problem, with VGC-Bench serving as a tool for future research.

Abstract: Developing AI agents that can robustly adapt to dramatically different
strategic landscapes without retraining is a central challenge for multi-agent
learning. Pok\'emon Video Game Championships (VGC) is a domain with an
extraordinarily large space of possible team configurations of approximately
$10^{139}$ - far larger than those of Dota or Starcraft. The highly discrete,
combinatorial nature of team building in Pok\'emon VGC causes optimal
strategies to shift dramatically depending on both the team being piloted and
the opponent's team, making generalization uniquely challenging. To advance
research on this problem, we introduce VGC-Bench: a benchmark that provides
critical infrastructure, standardizes evaluation protocols, and supplies
human-play datasets and a range of baselines - from large-language-model agents
and behavior cloning to reinforcement learning and empirical game-theoretic
methods such as self-play, fictitious play, and double oracle. In the
restricted setting where an agent is trained and evaluated on a single-team
configuration, our methods are able to win against a professional VGC
competitor. We extensively evaluated all baseline methods over progressively
larger team sets and find that even the best-performing algorithm in the
single-team setting struggles at scaling up as team size grows. Thus, policy
generalization across diverse team strategies remains an open challenge for the
community. Our code is open sourced at
https://github.com/cameronangliss/VGC-Bench.

</details>


### [254] [Correlation vs causation in Alzheimer's disease: an interpretability-driven study](https://arxiv.org/pdf/2506.10179)
*Hamzah Dabool, Raghad Mustafa*

Main category: cs.AI

TL;DR: The paper explores causation vs. correlation in Alzheimer's disease (AD) research using machine learning and interpretability techniques to identify key features and emphasize the need for careful data interpretation.


<details>
  <summary>Details</summary>
Motivation: To distinguish causation from correlation in AD research for better diagnosis, treatment, and understanding of disease drivers.

Method: Combined correlation analysis, XGBoost classification, and SHAP interpretability techniques to analyze clinical, cognitive, genetic, and biomarker features.

Result: Identified key AD-influencing features (e.g., cognitive scores, genetic risk factors) and showed strong correlations don't imply causation.

Conclusion: Integrating feature importance with statistical analysis aids future causal studies, improving early diagnosis and targeted AD interventions.

Abstract: Understanding the distinction between causation and correlation is critical
in Alzheimer's disease (AD) research, as it impacts diagnosis, treatment, and
the identification of true disease drivers. This experiment investigates the
relationships among clinical, cognitive, genetic, and biomarker features using
a combination of correlation analysis, machine learning classification, and
model interpretability techniques. Employing the XGBoost algorithm, we
identified key features influencing AD classification, including cognitive
scores and genetic risk factors. Correlation matrices revealed clusters of
interrelated variables, while SHAP (SHapley Additive exPlanations) values
provided detailed insights into feature contributions across disease stages.
Our results highlight that strong correlations do not necessarily imply
causation, emphasizing the need for careful interpretation of associative data.
By integrating feature importance and interpretability with classical
statistical analysis, this work lays groundwork for future causal inference
studies aimed at uncovering true pathological mechanisms. Ultimately,
distinguishing causal factors from correlated markers can lead to improved
early diagnosis and targeted interventions for Alzheimer's disease.

</details>


### [255] [Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems](https://arxiv.org/pdf/2506.10192)
*Filip Cano*

Main category: cs.AI

TL;DR: The thesis focuses on enhancing AI trustworthiness through safety, fairness, transparency, and accountability, introducing novel techniques like deterministic shielding, fairness shields, and intention metrics.


<details>
  <summary>Details</summary>
Motivation: The increasing influence of AI in critical societal domains necessitates responsible use, but trustworthy AI remains a broad and multi-faceted challenge.

Method: Extends deterministic shielding for safety, introduces fairness shields for group fairness, and proposes formal metrics for transparency and accountability.

Result: Validated techniques in simulated autonomous vehicles and sequential decision-making, achieving safer, fairer, and more accountable AI systems.

Conclusion: The work advances trustworthy AI by unifying contributions into a reactive decision-making framework, paving the way for future research.

Abstract: Ensuring responsible use of artificial intelligence (AI) has become
imperative as autonomous systems increasingly influence critical societal
domains. However, the concept of trustworthy AI remains broad and
multi-faceted. This thesis advances knowledge in the safety, fairness,
transparency, and accountability of AI systems. In safety, we extend classical
deterministic shielding techniques to become resilient against delayed
observations, enabling practical deployment in real-world conditions. We also
implement both deterministic and probabilistic safety shields into simulated
autonomous vehicles to prevent collisions with road users, validating the use
of these techniques in realistic driving simulators. We introduce fairness
shields, a novel post-processing approach to enforce group fairness in
sequential decision-making settings over finite and periodic time horizons. By
optimizing intervention costs while strictly ensuring fairness constraints,
this method efficiently balances fairness with minimal interference. For
transparency and accountability, we propose a formal framework for assessing
intentional behaviour in probabilistic decision-making agents, introducing
quantitative metrics of agency and intention quotient. We use these metrics to
propose a retrospective analysis of intention, useful for determining
responsibility when autonomous systems cause unintended harm. Finally, we unify
these contributions through the ``reactive decision-making'' framework,
providing a general formalization that consolidates previous approaches.
Collectively, the advancements presented contribute practically to the
realization of safer, fairer, and more accountable AI systems, laying the
foundations for future research in trustworthy AI.

</details>


### [256] [WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models](https://arxiv.org/pdf/2506.10264)
*Qiyue Yin, Pei Xu, Qiaozhe Li, Shengda Liu, Shengqi Shen, Tong Wang, Yihong Han, Xiaonan Zhao, Likun Yang, Shiyue Cao, Shiyu Qiu, Yuxuan Liu, Shizhao Yu, Lei Cui, Chengxin Yan, Jie Sun, Xiangquan Tang, Kaiqi Huang*

Main category: cs.AI

TL;DR: The paper introduces WGSR-Bench, a benchmark for evaluating strategic reasoning in LLMs using wargame scenarios, focusing on multi-agent decision-making, intent inference, and counterfactual reasoning.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' advancements in reasoning tasks, strategic reasoning—critical for human cognition—lacks systematic evaluation. WGSR-Bench addresses this gap.

Method: The benchmark uses wargame environments to test LLMs on three core tasks: environmental situation awareness, opponent risk modeling, and policy generation, forming the S-POE architecture.

Result: An LLM-based wargame agent is designed to assess strategic reasoning comprehensively, aiming to identify strengths and limitations of current LLMs.

Conclusion: WGSR-Bench aims to advance research in LLM-driven strategic intelligence by evaluating and improving strategic reasoning capabilities.

Abstract: Recent breakthroughs in Large Language Models (LLMs) have led to a
qualitative leap in artificial intelligence' s performance on reasoning tasks,
particularly demonstrating remarkable capabilities in mathematical, symbolic,
and commonsense reasoning. However, as a critical component of advanced human
cognition, strategic reasoning, i.e., the ability to assess multi-agent
behaviors in dynamic environments, formulate action plans, and adapt
strategies, has yet to be systematically evaluated or modeled. To address this
gap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark
for LLMs using wargame as its evaluation environment. Wargame, a quintessential
high-complexity strategic scenario, integrates environmental uncertainty,
adversarial dynamics, and non-unique strategic choices, making it an effective
testbed for assessing LLMs' capabilities in multi-agent decision-making, intent
inference, and counterfactual reasoning. WGSR-Bench designs test samples around
three core tasks, i.e., Environmental situation awareness, Opponent risk
modeling and Policy generation, which serve as the core S-POE architecture, to
systematically assess main abilities of strategic reasoning. Finally, an
LLM-based wargame agent is designed to integrate these parts for a
comprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess
the strengths and limitations of state-of-the-art LLMs in game-theoretic
strategic reasoning and to advance research in large model-driven strategic
intelligence.

</details>


### [257] [Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution](https://arxiv.org/pdf/2506.10281)
*Xinmin Fang, Lingfeng Tao, Zhengxiong Li*

Main category: cs.AI

TL;DR: AI is framed as a cognitive revolution, augmenting human intellect like written language, driving productivity in knowledge work and demanding societal rethinking.


<details>
  <summary>Details</summary>
Motivation: To position AI as a transformative cognitive tool, distinct from industrial mechanization, and explore its impact on productivity and society.

Method: Multidisciplinary analysis combining computer science, economics, and sociology, with historical comparisons and domain examples.

Result: AI acts as an engine of cognition, shifting productivity from manual to cognitive tasks, requiring new skills and policies.

Conclusion: AI complements human cognition, heralding a new productivity paradigm and necessitating societal adaptation.

Abstract: Artificial Intelligence (AI) is reframed as a cognitive engine driving a
novel productivity revolution distinct from the Industrial Revolution's
physical thrust. This paper develops a theoretical framing of AI as a cognitive
revolution akin to written language - a transformative augmentation of human
intellect rather than another mechanized tool. We compare AI's emergence to
historical leaps in information technology to show how it amplifies knowledge
work. Examples from various domains demonstrate AI's impact as a driver of
productivity in cognitive tasks. We adopt a multidisciplinary perspective
combining computer science advances with economic insights and sociological
perspectives on how AI reshapes work and society. Through conceptual
frameworks, we visualize the shift from manual to cognitive productivity. Our
central argument is that AI functions as an engine of cognition - comparable to
how human language revolutionized knowledge - heralding a new productivity
paradigm. We discuss how this revolution demands rethinking of skills,
organizations, and policies. This paper, balancing academic rigor with clarity,
concludes that AI's promise lies in complementing human cognitive abilities,
marking a new chapter in productivity evolution.

</details>


### [258] [The Alignment Trap: Complexity Barriers](https://arxiv.org/pdf/2506.10304)
*Jasper Yao*

Main category: cs.AI

TL;DR: The paper shows that verifying AI safety becomes exponentially harder as system capabilities grow, leading to a fundamental tension between safety and scalability.


<details>
  <summary>Details</summary>
Motivation: To understand the computational limits of verifying AI safety as systems become more capable, highlighting the challenges in aligning advanced AI with societal safety needs.

Method: The authors formalize the Capability-Risk Scaling (CRS) dynamic and prove four core theorems using complexity theory to analyze verification intractability.

Result: Key findings include exponential growth in verification complexity, vanishingly small safe policy spaces, and the impossibility of universal alignment techniques.

Conclusion: The study presents a strategic trilemma for AI development: limit complexity, accept unverifiable risks, or innovate beyond current safety paradigms.

Abstract: We establish fundamental computational complexity barriers to verifying AI
safety as system capabilities scale. Our main results show that for AI systems
with expressiveness EXP$(m)$ above a critical threshold $\tau$, safety
verification requires exponential time and is coNP-complete. We formalize the
Capability-Risk Scaling (CRS) dynamic, which demonstrates how increasing AI
capability drives societal safety requirements toward perfection, creating an
inescapable tension with verification complexity. Through four core theorems,
we prove that (1) verification complexity grows exponentially with system
expressiveness, (2) safe policies comprise at most a $2^{-2^m}$ fraction of the
policy space, (3) no finite set of alignment techniques can provide universal
coverage, and (4) robust safety properties form measure-zero sets for neural
networks. These results characterize an "intractability gap" where practical
safety requirements fall within the region of computational intractability. We
conclude by presenting a strategic trilemma: AI development must either
constrain system complexity to maintain verifiable safety, accept unverifiable
risks while scaling capabilities, or develop fundamentally new safety paradigms
beyond verification. Our work provides the first systematic
complexity-theoretic analysis of AI alignment and establishes rigorous bounds
that any safety approach must confront. A formal verification of the core
theorems in Lean4 is currently in progress.

</details>


### [259] [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/pdf/2506.00073)
*Shenzhe Zhu, Jiao Sun, Yi Nian, Tobin South, Alex Pentland, Jiaxin Pei*

Main category: cs.AI

TL;DR: AI agents in consumer markets can automate negotiations, but performance varies, and risks like financial losses exist.


<details>
  <summary>Details</summary>
Motivation: Explore AI agents' role in automating negotiations and transactions, assessing their effectiveness and risks.

Method: Developed an experimental framework to evaluate LLM agents in real-world negotiation and transaction scenarios.

Result: AI agents vary in securing deals; behavioral anomalies can cause financial losses.

Conclusion: Automation improves efficiency but introduces risks; caution is needed when delegating to AI agents.

Abstract: AI agents are increasingly used in consumer-facing applications to assist
with tasks such as product search, negotiation, and transaction execution. In
this paper, we explore a future scenario where both consumers and merchants
authorize AI agents to fully automate negotiations and transactions. We aim to
answer two key questions: (1) Do different LLM agents vary in their ability to
secure favorable deals for users? (2) What risks arise from fully automating
deal-making with AI agents in consumer markets? To address these questions, we
develop an experimental framework that evaluates the performance of various LLM
agents in real-world negotiation and transaction settings. Our findings reveal
that AI-mediated deal-making is an inherently imbalanced game -- different
agents achieve significantly different outcomes for their users. Moreover,
behavioral anomalies in LLMs can result in financial losses for both consumers
and merchants, such as overspending or accepting unreasonable deals. These
results underscore that while automation can improve efficiency, it also
introduces substantial risks. Users should exercise caution when delegating
business decisions to AI agents.

</details>


### [260] [Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts](https://arxiv.org/pdf/2506.10357)
*Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Weili Guan, Dongmei Jiang, Liqiang Nie*

Main category: cs.AI

TL;DR: Optimus-3, a general-purpose agent for Minecraft, addresses challenges like insufficient data, task interference, and visual diversity using knowledge-enhanced data generation, MoE architecture, and multimodal reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in building generalist agents in open-world environments like Minecraft, such as lack of domain-specific data, task interference, and visual diversity.

Method: 1) Knowledge-enhanced data generation pipeline. 2) Mixture-of-Experts (MoE) architecture with task-level routing. 3) Multimodal Reasoning-Augmented Reinforcement Learning.

Result: Optimus-3 outperforms generalist MLLMs and state-of-the-art agents in Minecraft tasks.

Conclusion: The proposed innovations enable Optimus-3 to excel as a general-purpose agent in complex, open-world environments.

Abstract: Recently, agents based on multimodal large language models (MLLMs) have
achieved remarkable progress across various domains. However, building a
generalist agent with capabilities such as perception, planning, action,
grounding, and reflection in open-world environments like Minecraft remains
challenges: insufficient domain-specific data, interference among heterogeneous
tasks, and visual diversity in open-world settings. In this paper, we address
these challenges through three key contributions. 1) We propose a
knowledge-enhanced data generation pipeline to provide scalable and
high-quality training data for agent development. 2) To mitigate interference
among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture
with task-level routing. 3) We develop a Multimodal Reasoning-Augmented
Reinforcement Learning approach to enhance the agent's reasoning ability for
visual diversity in Minecraft. Built upon these innovations, we present
Optimus-3, a general-purpose agent for Minecraft. Extensive experimental
results demonstrate that Optimus-3 surpasses both generalist multimodal large
language models and existing state-of-the-art agents across a wide range of
tasks in the Minecraft environment. Project page:
https://cybertronagent.github.io/Optimus-3.github.io/

</details>


### [261] [NeuroPAL: Punctuated Anytime Learning with Neuroevolution for Macromanagement in Starcraft: Brood War](https://arxiv.org/pdf/2506.10384)
*Jim O'Connor, Yeonghun Lee, Gary B Parker*

Main category: cs.AI

TL;DR: NeuroPAL, a neuroevolutionary framework combining NEAT and PAL, improves training efficiency for StarCraft AI, achieving competitive play faster with emergent human-like strategies.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of rule-based and supervised deep learning approaches in StarCraft AI, particularly in adaptability and computational efficiency.

Method: Integrates NEAT with PAL, alternating low-fidelity training and high-fidelity evaluations to enhance sample efficiency.

Result: PAL accelerates learning, reducing training time by half, with agents exhibiting expert human-like strategies.

Conclusion: Structured evaluation mechanisms like PAL enhance neuroevolution scalability and effectiveness in complex RTS environments.

Abstract: StarCraft: Brood War remains a challenging benchmark for artificial
intelligence research, particularly in the domain of macromanagement, where
long-term strategic planning is required. Traditional approaches to StarCraft
AI rely on rule-based systems or supervised deep learning, both of which face
limitations in adaptability and computational efficiency. In this work, we
introduce NeuroPAL, a neuroevolutionary framework that integrates
Neuroevolution of Augmenting Topologies (NEAT) with Punctuated Anytime Learning
(PAL) to improve the efficiency of evolutionary training. By alternating
between frequent, low-fidelity training and periodic, high-fidelity
evaluations, PAL enhances the sample efficiency of NEAT, enabling agents to
discover effective strategies in fewer training iterations. We evaluate
NeuroPAL in a fixed-map, single-race scenario in StarCraft: Brood War and
compare its performance to standard NEAT-based training. Our results show that
PAL significantly accelerates the learning process, allowing the agent to reach
competitive levels of play in approximately half the training time required by
NEAT alone. Additionally, the evolved agents exhibit emergent behaviors such as
proxy barracks placement and defensive building optimization, strategies
commonly used by expert human players. These findings suggest that structured
evaluation mechanisms like PAL can enhance the scalability and effectiveness of
neuroevolution in complex real-time strategy environments.

</details>


### [262] [The Optimization Paradox in Clinical AI Multi-Agent Systems](https://arxiv.org/pdf/2506.06574)
*Suhana Bedi, Iddah Mlauzi, Daniel Shin, Sanmi Koyejo, Nigam H. Shah*

Main category: cs.AI

TL;DR: Multi-agent AI systems outperform single-agent ones in clinical diagnosis, but component-optimized systems may underperform despite superior individual metrics, emphasizing the need for system-wide validation.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between component-level optimization and system-wide performance in multi-agent AI systems for clinical diagnosis.

Method: Evaluated 2,400 patient cases from MIMIC-CDM dataset across four abdominal pathologies, comparing single-agent and multi-agent systems using diagnostic, process, and cost metrics.

Result: Multi-agent systems generally outperformed single-agent ones, but a component-optimized system with high process metrics (85.5% accuracy) underperformed in diagnostic accuracy (67.7% vs. 77.4%).

Conclusion: Successful AI integration in healthcare requires system-wide validation, not just component optimization, due to the importance of information flow and agent compatibility.

Abstract: Multi-agent artificial intelligence systems are increasingly deployed in
clinical settings, yet the relationship between component-level optimization
and system-wide performance remains poorly understood. We evaluated this
relationship using 2,400 real patient cases from the MIMIC-CDM dataset across
four abdominal pathologies (appendicitis, pancreatitis, cholecystitis,
diverticulitis), decomposing clinical diagnosis into information gathering,
interpretation, and differential diagnosis. We evaluated single agent systems
(one model performing all tasks) against multi-agent systems (specialized
models for each task) using comprehensive metrics spanning diagnostic outcomes,
process adherence, and cost efficiency. Our results reveal a paradox: while
multi-agent systems generally outperformed single agents, the
component-optimized or Best of Breed system with superior components and
excellent process metrics (85.5% information accuracy) significantly
underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent
system). This finding underscores that successful integration of AI in
healthcare requires not just component level optimization but also attention to
information flow and compatibility between agents. Our findings highlight the
need for end to end system validation rather than relying on component metrics
alone.

</details>


### [263] [Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills](https://arxiv.org/pdf/2506.10387)
*Yuquan Xie, Zaijing Li, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Dongmei Jiang, Liqiang Nie*

Main category: cs.AI

TL;DR: The paper introduces Mirage-1, a GUI agent with a Hierarchical Multimodal Skills module and SA-MCTS algorithm, improving performance in long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of insufficient knowledge and domain gaps in MLLM-based GUI agents for long-horizon tasks.

Method: Proposes HMS for hierarchical knowledge abstraction and SA-MCTS for efficient skill transfer from offline to online environments.

Result: Mirage-1 outperforms previous agents by significant margins (up to 79%) on multiple benchmarks.

Conclusion: The proposed methods effectively enhance GUI agent performance in real-world long-horizon tasks.

Abstract: Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI
agents have yielded promising outcomes. However, these agents still struggle
with long-horizon tasks in online environments, primarily due to insufficient
knowledge and the inherent gap between offline and online domains. In this
paper, inspired by how humans generalize knowledge in open-ended environments,
we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of
insufficient knowledge. It progressively abstracts trajectories into execution
skills, core skills, and ultimately meta-skills, providing a hierarchical
knowledge structure for long-horizon task planning. To bridge the domain gap,
we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,
which efficiently leverages skills acquired in offline environments to reduce
the action search space during online tree exploration. Building on HMS, we
propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To
validate the performance of Mirage-1 in real-world long-horizon scenarios, we
constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1
outperforms previous agents by 32\%, 19\%, 15\%, and 79\% on AndroidWorld,
MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:
https://cybertronagent.github.io/Mirage-1.github.io/

</details>


### [264] [Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges](https://arxiv.org/pdf/2506.10408)
*Jintao Liang, Gang Su, Huifeng Lin, You Wu, Rui Zhao, Ziyue Li*

Main category: cs.AI

TL;DR: The paper reviews Reasoning Agentic RAG, a paradigm enhancing RAG systems with decision-making and adaptive tool use, categorizing methods into predefined and agentic reasoning, and discussing future directions.


<details>
  <summary>Details</summary>
Motivation: To address limitations of early RAG systems in complex, dynamic, and multi-modal scenarios by integrating reasoning and adaptive tool use.

Method: Categorizes Reasoning Agentic RAG into predefined (fixed pipelines) and agentic (autonomous tool orchestration) reasoning, analyzing techniques in design, reasoning, and tool coordination.

Result: Identifies key challenges and proposes future directions to improve flexibility, robustness, and applicability of Reasoning Agentic RAG.

Conclusion: Reasoning Agentic RAG advances RAG systems by embedding decision-making and adaptive tool use, with ongoing research needed to enhance its capabilities.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to
overcome the knowledge limitations of Large Language Models (LLMs) by
integrating external retrieval with language generation. While early RAG
systems based on static pipelines have shown effectiveness in well-structured
tasks, they struggle in real-world scenarios requiring complex reasoning,
dynamic retrieval, and multi-modal integration. To address these challenges,
the field has shifted toward Reasoning Agentic RAG, a paradigm that embeds
decision-making and adaptive tool use directly into the retrieval process. In
this paper, we present a comprehensive review of Reasoning Agentic RAG methods,
categorizing them into two primary systems: predefined reasoning, which follows
fixed modular pipelines to boost reasoning, and agentic reasoning, where the
model autonomously orchestrates tool interaction during inference. We analyze
representative techniques under both paradigms, covering architectural design,
reasoning strategies, and tool coordination. Finally, we discuss key research
challenges and propose future directions to advance the flexibility,
robustness, and applicability of reasoning agentic RAG systems. Our collection
of the relevant research has been organized into a
https://github.com/ByebyeMonica/Reasoning-Agentic-RAG.

</details>


### [265] [Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods](https://arxiv.org/pdf/2506.10420)
*Boris Sedlak, Alireza Furutanpey, Zihang Wang, Víctor Casamayor Pujol, Schahram Dustdar*

Main category: cs.AI

TL;DR: The paper introduces an agent-based autoscaling framework for edge computing, comparing four scaling agents to maximize performance under resource constraints.


<details>
  <summary>Details</summary>
Motivation: Edge computing's strict resource constraints require flexible scaling behaviors beyond traditional methods.

Method: An agent-based framework dynamically adjusts hardware resources and service configurations, tested with YOLOv8 and OpenCV services.

Result: All agents achieved acceptable SLO performance, with varying convergence patterns; Deep Q Network benefits from pre-training, while structural analysis converges quickly.

Conclusion: Multi-dimensional agent-based autoscaling is viable for edge environments, encouraging further research.

Abstract: Edge computing breaks with traditional autoscaling due to strict resource
constraints, thus, motivating more flexible scaling behaviors using multiple
elasticity dimensions. This work introduces an agent-based autoscaling
framework that dynamically adjusts both hardware resources and internal service
configurations to maximize requirements fulfillment in constrained
environments. We compare four types of scaling agents: Active Inference, Deep Q
Network, Analysis of Structural Knowledge, and Deep Active Inference, using two
real-world processing services running in parallel: YOLOv8 for visual
recognition and OpenCV for QR code detection. Results show all agents achieve
acceptable SLO performance with varying convergence patterns. While the Deep Q
Network benefits from pre-training, the structural analysis converges quickly,
and the deep active inference agent combines theoretical foundations with
practical scalability advantages. Our findings provide evidence for the
viability of multi-dimensional agent-based autoscaling for edge environments
and encourage future work in this research direction.

</details>


### [266] [OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics](https://arxiv.org/pdf/2506.10481)
*Yaoming Zhu, Junxin Wang, Yiyang Li, Lin Qiu, ZongYu Wang, Jun Xu, Xuezhi Cao, Yuhuai Wei, Mingshi Wang, Xunliang Cai, Rong Ma*

Main category: cs.AI

TL;DR: OIBench is a challenging olympiad-level informatics dataset introduced to address the saturation of conventional benchmarks, aiming to advance algorithmic reasoning in LLMs.


<details>
  <summary>Details</summary>
Motivation: The need for more challenging benchmarks to guide improvements in algorithmic reasoning due to the saturation of conventional benchmarks.

Method: Construction of OIBench, a dataset of 250 curated problems, with contamination-resistant properties, Time/Space Completion Curves, and human-model comparisons.

Result: Open-source models lag behind closed-source ones, but SOTA models outperform most humans in correctness and efficiency, though still suboptimal.

Conclusion: OIBench is released as an open-source resource to advance code reasoning capabilities in future LLMs.

Abstract: As models become increasingly sophisticated, conventional algorithm
benchmarks are increasingly saturated, underscoring the need for more
challenging benchmarks to guide future improvements in algorithmic reasoning.
This paper introduces OIBench, a high-quality, private, and challenging
olympiad-level informatics dataset comprising 250 carefully curated original
problems. We detail the construction methodology of the benchmark, ensuring a
comprehensive assessment across various programming paradigms and complexities,
and we demonstrate its contamination-resistant properties via experiments. We
propose Time/Space Completion Curves for finer-grained efficiency analysis and
enable direct human-model comparisons through high-level participant
evaluations. Our experiments reveal that while open-source models lag behind
closed-source counterparts, current SOTA models already outperform most human
participants in both correctness and efficiency, while still being suboptimal
compared to the canonical solutions. By releasing OIBench as a fully
open-source resource (https://huggingface.co/datasets/AGI-Eval/OIBench), we
hope this benchmark will contribute to advancing code reasoning capabilities
for future LLMs.

</details>


### [267] [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/pdf/2506.10521)
*Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai*

Main category: cs.AI

TL;DR: The paper introduces the Scientists' First Exam (SFE) benchmark to evaluate Multimodal Large Language Models (MLLMs) on scientific cognitive capacities, revealing current models' limitations.


<details>
  <summary>Details</summary>
Motivation: Current scientific benchmarks inadequately assess MLLMs' perception and reasoning abilities, focusing only on knowledge understanding.

Method: The SFE benchmark evaluates MLLMs through three levels: scientific signal perception, attribute understanding, and comparative reasoning, using 830 expert-verified VQA pairs across 66 tasks in five disciplines.

Result: State-of-the-art models (GPT-3 and InternVL-3) scored only 34.08% and 26.52% on SFE, indicating significant room for improvement.

Conclusion: SFE aims to advance AI-enhanced scientific discoveries by addressing gaps in evaluating MLLMs' scientific cognitive abilities.

Abstract: Scientific discoveries increasingly rely on complex multimodal reasoning
based on information-intensive scientific data and domain-specific expertise.
Empowered by expert-level scientific benchmarks, scientific Multimodal Large
Language Models (MLLMs) hold the potential to significantly enhance this
discovery process in realistic workflows. However, current scientific
benchmarks mostly focus on evaluating the knowledge understanding capabilities
of MLLMs, leading to an inadequate assessment of their perception and reasoning
abilities. To address this gap, we present the Scientists' First Exam (SFE)
benchmark, designed to evaluate the scientific cognitive capacities of MLLMs
through three interconnected levels: scientific signal perception, scientific
attribute understanding, scientific comparative reasoning. Specifically, SFE
comprises 830 expert-verified VQA pairs across three question types, spanning
66 multimodal tasks across five high-value disciplines. Extensive experiments
reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%
and 26.52% on SFE, highlighting significant room for MLLMs to improve in
scientific realms. We hope the insights obtained in SFE will facilitate further
developments in AI-enhanced scientific discoveries.

</details>


### [268] [LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs](https://arxiv.org/pdf/2506.10527)
*Yanan Cai, Ahmed Salem, Besmira Nushi, Mark Russinovich*

Main category: cs.AI

TL;DR: LogiPlan is a benchmark for evaluating LLMs' logical planning and reasoning over complex relational structures, featuring dynamic task complexity and three tasks: Plan Generation, Consistency Detection, and Comparison Question. Performance gaps among state-of-the-art models are revealed.


<details>
  <summary>Details</summary>
Motivation: Logical relational reasoning is crucial for applications like network infrastructure, knowledge bases, and business processes. Existing benchmarks lack fine-grained assessment of LLMs' capabilities in this domain.

Method: LogiPlan dynamically varies task complexity by controlling objects, relations, and relational chain depth. It includes three tasks and evaluates models' self-correction.

Result: State-of-the-art models show performance gaps, with reasoning-enhanced models excelling in simpler tasks but struggling with complex configurations.

Conclusion: LogiPlan highlights the need for improved LLM capabilities in deeper logical planning and reasoning, revealing limitations in current models.

Abstract: We introduce LogiPlan, a novel benchmark designed to evaluate the
capabilities of large language models (LLMs) in logical planning and reasoning
over complex relational structures. Logical relational reasoning is important
for applications that may rely on LLMs to generate and query structured graphs
of relations such as network infrastructure, knowledge bases, or business
process schema. Our framework allows for dynamic variation of task complexity
by controlling the number of objects, relations, and the minimum depth of
relational chains, providing a fine-grained assessment of model performance
across difficulty levels. LogiPlan encompasses three complementary tasks: (1)
Plan Generation, where models must construct valid directed relational graphs
meeting specified structural constraints; (2) Consistency Detection, testing
models' ability to identify inconsistencies in relational structures; and (3)
Comparison Question, evaluating models' capacity to determine the validity of
queried relationships within a given graph. Additionally, we assess models'
self-correction capabilities by prompting them to verify and refine their
initial solutions. We evaluate state-of-the-art models including DeepSeek R1,
Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,
O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant
performance gaps that correlate with model scale and architecture. Our analysis
demonstrates that while recent reasoning-enhanced models show promising results
on simpler instances, they struggle with more complex configurations requiring
deeper logical planning.

</details>


### [269] [Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning](https://arxiv.org/pdf/2506.10585)
*Mohd Anwar Jamal Faiz*

Main category: cs.AI

TL;DR: The paper introduces the Primender sequence, a hybrid integer sequence combining primality and modular digit conditions, to benchmark LLMs' symbolic reasoning. It tests models on rule inference, hypothesis validation, and sequence generation.


<details>
  <summary>Details</summary>
Motivation: To create an interpretable, rule-based testbed for evaluating LLMs' ability to infer hidden rules, validate hypotheses, and generalize symbolic logic.

Method: Defines the Primender sequence, designs prompts to test LLMs (ChatGPT, Copilot, etc.), and evaluates performance using metrics like rule inference accuracy and sequence validity.

Result: Proposes a novel sequence and framework for benchmarking LLMs in symbolic reasoning, bridging number theory and AI.

Conclusion: The Primender sequence and methodology offer a reproducible way to assess LLMs' symbolic reasoning and hypothesis testing capabilities.

Abstract: This paper introduces the Primender sequence, a novel integer sequence
defined by a hybrid rule that combines classical primality with modular
digit-based conditions. Specifically, a number n is included in the sequence if
it is prime or ends with a prime number of unit digit or any length. In other
words, numbers which are primes or have at least one prime suffix. The
resulting sequence exhibits a deterministic yet non-trivial structure, blending
number-theoretic properties with symbolic patterning. We propose the Primender
sequence as a benchmark for evaluating the symbolic reasoning capabilities of
Large Language Models (LLMs). The study is motivated by the need for
interpretable, rule-based testbeds that can assess an LLM's ability to infer
hidden rules, validate mathematical hypotheses, and generalize symbolic logic
at scale. A key hypothesis explored is: Whenever a number in the Primender
sequence is exactly one more than the largest prime less than or equal to it,
the difference between it and the previous number in the sequence is also 1. We
design a structured prompt and evaluation framework to test this hypothesis
across multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,
Gemini, Grok, and LLaMA. The models are tasked with identifying the underlying
rule, validating the hypothesis, and generating the next 100,000 terms of the
sequence. Comparative metrics such as rule inference accuracy, hypothesis
evaluation, sequence validity, and symbolic explanation quality are used to
assess model performance. This work contributes a novel mathematical construct
and a reproducible methodology for benchmarking LLMs in symbolic reasoning,
hypothesis testing, and scalable pattern generalization - bridging the domains
of number theory, artificial intelligence, and software engineering.

</details>


### [270] [Data Driven Diagnosis for Large Cyber-Physical-Systems with Minimal Prior Information](https://arxiv.org/pdf/2506.10613)
*Henrik Sebastian Steude, Alexander Diedrich, Ingo Pill, Lukas Moddemann, Daniel Vranješ, Oliver Niggemann*

Main category: cs.AI

TL;DR: A new diagnostic approach for cyber-physical systems reduces reliance on extensive prior knowledge, using minimal subsystem relationships and nominal data, achieving high accuracy and practical applicability.


<details>
  <summary>Details</summary>
Motivation: Diagnosing complex cyber-physical systems often requires detailed models or training data, which are challenging to obtain. The paper addresses this by proposing a method that minimizes prior knowledge requirements.

Method: Combines a neural network-based symptom generator for anomaly detection with a graph diagnosis algorithm using minimal causal relationships between subsystems.

Result: Achieves 82% accuracy in identifying true causal components and reduces search space in 73% of cases, validated on simulated and real-world datasets.

Conclusion: The approach is practical for large, complex systems with limited prior knowledge, demonstrating strong potential for real-world applications.

Abstract: Diagnostic processes for complex cyber-physical systems often require
extensive prior knowledge in the form of detailed system models or
comprehensive training data. However, obtaining such information poses a
significant challenge. To address this issue, we present a new diagnostic
approach that operates with minimal prior knowledge, requiring only a basic
understanding of subsystem relationships and data from nominal operations. Our
method combines a neural network-based symptom generator, which employs
subsystem-level anomaly detection, with a new graph diagnosis algorithm that
leverages minimal causal relationship information between
subsystems-information that is typically available in practice. Our experiments
with fully controllable simulated datasets show that our method includes the
true causal component in its diagnosis set for 82 p.c. of all cases while
effectively reducing the search space in 73 p.c. of the scenarios. Additional
tests on the real-world Secure Water Treatment dataset showcase the approach's
potential for practical scenarios. Our results thus highlight our approach's
potential for practical applications with large and complex cyber-physical
systems where limited prior knowledge is available.

</details>


### [271] [TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving](https://arxiv.org/pdf/2506.10674)
*Vincenzo Colle, Mohamed Sana, Nicola Piovesan, Antonio De Domenico, Fadhel Ayed, Merouane Debbah*

Main category: cs.AI

TL;DR: TeleMath is a benchmark dataset for evaluating LLMs in solving domain-specific mathematical problems in telecommunications, revealing specialized models outperform general-purpose ones.


<details>
  <summary>Details</summary>
Motivation: Assess LLM performance in specialized, mathematically intensive telecommunications tasks, an underexplored area.

Method: Created TeleMath, a 500 QnA dataset, using expert-crafted problems and evaluated various LLMs.

Result: Specialized LLMs for math/logic outperformed general-purpose models on TeleMath.

Conclusion: TeleMath aids reproducibility and future research, highlighting the need for domain-specific LLM capabilities.

Abstract: The increasing adoption of artificial intelligence in telecommunications has
raised interest in the capability of Large Language Models (LLMs) to address
domain-specific, mathematically intensive tasks. Although recent advancements
have improved the performance of LLMs in general mathematical reasoning, their
effectiveness within specialized domains, such as signal processing, network
optimization, and performance analysis, remains largely unexplored. To address
this gap, we introduce TeleMath, the first benchmark dataset specifically
designed to evaluate LLM performance in solving mathematical problems with
numerical solutions in the telecommunications domain. Comprising 500
question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the
telecommunications field. This paper outlines the proposed QnAs generation
pipeline, starting from a selected seed of problems crafted by Subject Matter
Experts. The evaluation of a wide range of open-source LLMs reveals that best
performance on TeleMath is achieved by recent models explicitly designed for
mathematical or logical reasoning. In contrast, general-purpose models, even
those with a large number of parameters, often struggle with these challenges.
We have released the dataset and the evaluation code to ease result
reproducibility and support future research.

</details>


### [272] [Automated Validation of Textual Constraints Against AutomationML via LLMs and SHACL](https://arxiv.org/pdf/2506.10678)
*Tom Westermann, Aljosha Köcher, Felix Gehlhoff*

Main category: cs.AI

TL;DR: A pipeline formalizes and verifies informal AML modeling constraints using OWL ontologies, LLM-translated SHACL rules, and natural language interpretation.


<details>
  <summary>Details</summary>
Motivation: Existing AML modeling constraints are informal and lack automated validation, hindering efficient engineering data exchange.

Method: AML models are mapped to OWL ontologies via RML and SPARQL. LLMs translate textual rules into SHACL constraints, validated against the AML ontology. Results are interpreted in natural language.

Result: Complex AML modeling rules can be semi-automatically checked without formal method expertise.

Conclusion: The pipeline enables automated validation of AML constraints, improving modeling accuracy and accessibility.

Abstract: AutomationML (AML) enables standardized data exchange in engineering, yet
existing recommendations for proper AML modeling are typically formulated as
informal and textual constraints. These constraints cannot be validated
automatically within AML itself. This work-in-progress paper introduces a
pipeline to formalize and verify such constraints. First, AML models are mapped
to OWL ontologies via RML and SPARQL. In addition, a Large Language Model
translates textual rules into SHACL constraints, which are then validated
against the previously generated AML ontology. Finally, SHACL validation
results are automatically interpreted in natural language. The approach is
demonstrated on a sample AML recommendation. Results show that even complex
modeling rules can be semi-automatically checked -- without requiring users to
understand formal methods or ontology technologies.

</details>


### [273] [System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers](https://arxiv.org/pdf/2506.10708)
*Michael Bartholomew, Joohyung Lee*

Main category: cs.AI

TL;DR: ASPMT combines answer set programming and satisfiability modulo theories, with a compiler (aspsmt2smt) translating tight ASPMT programs to SMT instances for solver processing.


<details>
  <summary>Details</summary>
Motivation: To bridge answer set programming and satisfiability modulo theories for more expressive reasoning, especially for continuous changes.

Method: Use of the aspsmt2smt compiler, integrating ASP grounder gringo and SMT solver z3 for partial grounding and solving.

Result: The system effectively handles real number computations, enabling reasoning about continuous changes.

Conclusion: ASPMT and the aspsmt2smt compiler provide a practical approach for combining ASP and SMT, expanding reasoning capabilities.

Abstract: Answer Set Programming Modulo Theories (ASPMT) is an approach to combining
answer set programming and satisfiability modulo theories based on the
functional stable model semantics. It is shown that the tight fragment of ASPMT
programs can be turned into SMT instances, thereby allowing SMT solvers to
compute stable models of ASPMT programs. In this paper we present a compiler
called {\sc aspsmt2smt}, which implements this translation. The system uses ASP
grounder {\sc gringo} and SMT solver {\sc z3}. {\sc gringo} partially grounds
input programs while leaving some variables to be processed by {\sc z3}. We
demonstrate that the system can effectively handle real number computations for
reasoning about continuous changes.

</details>


### [274] [Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering](https://arxiv.org/pdf/2506.10753)
*Adam Ishay, Zhun Yang, Joohyung Lee, Ilgu Kang, Dongjae Lim*

Main category: cs.AI

TL;DR: The paper enhances a neuro-symbolic model for counterfactual reasoning in videos using causal graphs and Answer Set Programming (ASP), achieving state-of-the-art results on CLEVRER and improved performance on CRAFT with GPT-3.5/GPT-4.


<details>
  <summary>Details</summary>
Motivation: Address limitations of neuro-symbolic models in answering counterfactual questions about video dynamics.

Method: Introduces causal graphs for event relations and uses ASP to coordinate perception and simulation modules. Validated on CLEVRER and CRAFT benchmarks, leveraging GPT-3.5/GPT-4 for dynamics simulation.

Result: Achieves state-of-the-art on CLEVRER and improved performance on CRAFT with symbolic-guided prompts.

Conclusion: Symbolic causal reasoning enhances neuro-symbolic models for counterfactual video reasoning, validated by benchmarks.

Abstract: Causal and temporal reasoning about video dynamics is a challenging problem.
While neuro-symbolic models that combine symbolic reasoning with neural-based
perception and prediction have shown promise, they exhibit limitations,
especially in answering counterfactual questions. This paper introduces a
method to enhance a neuro-symbolic model for counterfactual reasoning,
leveraging symbolic reasoning about causal relations among events. We define
the notion of a causal graph to represent such relations and use Answer Set
Programming (ASP), a declarative logic programming method, to find how to
coordinate perception and simulation modules. We validate the effectiveness of
our approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves
state-of-the-art performance on the CLEVRER challenge, significantly
outperforming existing models. In the case of the CRAFT benchmark, we leverage
a large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a
dynamics simulator. Our findings show that this method can further improve its
performance on counterfactual questions by providing alternative prompts
instructed by symbolic causal reasoning.

</details>


### [275] [OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems](https://arxiv.org/pdf/2506.10764)
*Xiaozhe Li, Jixuan Chen, Xinyu Fang, Shengyuan Ding, Haodong Duan, Qingwen Liu, Kai Chen*

Main category: cs.AI

TL;DR: OPT-BENCH is a benchmark for evaluating LLM agents on iterative optimization tasks, including ML and NP problems. OPT-Agent, a framework for iterative reasoning, improves solution quality using historical feedback. Results show historical context boosts performance.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' iterative optimization capabilities, which are understudied, and provide a benchmark for rigorous evaluation.

Method: OPT-BENCH includes 20 ML tasks and 10 NP problems. OPT-Agent uses historical feedback for iterative solution refinement. Experiments test 9 LLMs.

Result: Historical context significantly improves optimization performance in both ML and NP tasks.

Conclusion: OPT-BENCH and OPT-Agent advance LLM-driven iterative reasoning, with open-sourced resources for further research.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in solving
diverse tasks. However, their proficiency in iteratively optimizing complex
solutions through learning from previous feedback remains insufficiently
explored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark
designed to evaluate LLM agents on large-scale search space optimization
problems. OPT-BENCH includes 20 real-world machine learning tasks sourced from
Kaggle and 10 classical NP problems, offering a diverse and challenging
environment for assessing LLM agents on iterative reasoning and solution
refinement. To enable rigorous evaluation, we introduce OPT-Agent, an
end-to-end optimization framework that emulates human reasoning when tackling
complex problems by generating, validating, and iteratively improving solutions
through leveraging historical feedback. Through extensive experiments on 9
state-of-the-art LLMs from 6 model families, we analyze the effects of
optimization iterations, temperature settings, and model architectures on
solution quality and convergence. Our results demonstrate that incorporating
historical context significantly enhances optimization performance across both
ML and NP tasks. All datasets, code, and evaluation tools are open-sourced to
promote further research in advancing LLM-driven optimization and iterative
reasoning. Project page:
\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.

</details>


### [276] [A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models](https://arxiv.org/pdf/2506.10853)
*Yu Zhang, Yang Hu, De Wang*

Main category: cs.AI

TL;DR: A framework combining chain-of-thought reasoning and Model Context Protocol (MCP) enhances LLMs for spatiotemporal behavior simulation, validated in Shanghai with high similarity to real data.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for human spatiotemporal behavior simulation are computationally costly and lack generalizability. LLMs, though promising, struggle with spatiotemporal reasoning.

Method: Integrates CoT reasoning with MCP, using a five-stage cognitive framework and six MCP tool categories for data processing.

Result: Validated in Lujiazui district, achieving high similarity to real data (scores 7.86-8.36) and efficiency improvements (0.17 minutes per sample with 12 processes).

Conclusion: The framework advances LLM applications in urban computing, offering a practical solution for synthetic mobility data and smart city planning.

Abstract: Human spatiotemporal behavior simulation is critical for urban planning
research, yet traditional rule-based and statistical approaches suffer from
high computational costs, limited generalizability, and poor scalability. While
large language models (LLMs) show promise as "world simulators," they face
challenges in spatiotemporal reasoning including limited spatial cognition,
lack of physical constraint understanding, and group homogenization tendencies.
This paper introduces a framework integrating chain-of-thought (CoT) reasoning
with Model Context Protocol (MCP) to enhance LLMs' capability in simulating
spatiotemporal behaviors that correspond with validation data patterns. The
methodology combines human-like progressive reasoning through a five-stage
cognitive framework with comprehensive data processing via six specialized MCP
tool categories: temporal management, spatial navigation, environmental
perception, personal memory, social collaboration, and experience evaluation.
Experiments in Shanghai's Lujiazui district validate the framework's
effectiveness across 1,000 generated samples. Results demonstrate high
similarity with real mobile signaling data, achieving generation quality scores
of 7.86 to 8.36 across different base models. Parallel processing experiments
show efficiency improvements, with generation times decreasing from 1.30 to
0.17 minutes per sample when scaling from 2 to 12 processes. This work
contributes to integrating CoT reasoning with MCP for urban behavior modeling,
advancing LLMs applications in urban computing and providing a practical
approach for synthetic mobility data generation. The framework offers a
foundation for smart city planning, transportation forecasting, and
participatory urban design applications.

</details>


### [277] [GenPlanX. Generation of Plans and Execution](https://arxiv.org/pdf/2506.10897)
*Daniel Borrajo, Giuseppe Canonaco, Tomás de la Rosa, Alfredo Garrachón, Sriram Gopalakrishnan, Simerjot Kaur, Marianela Morales, Sunandita Patra, Alberto Pozanco, Keshav Ramani, Charese Smiley, Pietro Totis, Manuela Veloso*

Main category: cs.AI

TL;DR: GenPlanX integrates LLMs with classical AI planning to interpret natural language tasks, enhancing productivity in office workflows.


<details>
  <summary>Details</summary>
Motivation: Classical AI planning lacks natural language understanding, while LLMs excel in interpreting human intents, creating a need for integration.

Method: GenPlanX combines LLMs for natural language task description with a classical AI planning engine and an execution framework.

Result: The system effectively assists with office tasks, improving workflow and productivity through human-AI collaboration.

Conclusion: GenPlanX demonstrates the potential of integrating LLMs with AI planning for seamless task execution and productivity enhancement.

Abstract: Classical AI Planning techniques generate sequences of actions for complex
tasks. However, they lack the ability to understand planning tasks when
provided using natural language. The advent of Large Language Models (LLMs) has
introduced novel capabilities in human-computer interaction. In the context of
planning tasks, LLMs have shown to be particularly good in interpreting human
intents among other uses. This paper introduces GenPlanX that integrates LLMs
for natural language-based description of planning tasks, with a classical AI
planning engine, alongside an execution and monitoring framework. We
demonstrate the efficacy of GenPlanX in assisting users with office-related
tasks, highlighting its potential to streamline workflows and enhance
productivity through seamless human-AI collaboration.

</details>


### [278] [Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?](https://arxiv.org/pdf/2506.10912)
*Fei Lin, Ziyang Gong, Cong Wang, Yonglin Tian, Tengchao Zhang, Xue Yang, Gen Luo, Fei-Yue Wang*

Main category: cs.AI

TL;DR: ToxiMol is introduced as the first benchmark for molecular toxicity repair, evaluating MLLMs on 11 tasks with 560 toxic molecules. ToxiEval provides an automated evaluation framework, revealing MLLMs' potential despite current challenges.


<details>
  <summary>Details</summary>
Motivation: Toxicity is a major cause of drug development failure, yet molecular toxicity repair lacks systematic definition or benchmarking.

Method: ToxiMol benchmarks MLLMs using a dataset of 560 toxic molecules and 11 tasks, with expert-informed prompt annotation. ToxiEval evaluates repair success via toxicity prediction, synthetic accessibility, drug-likeness, and structural similarity.

Result: Current MLLMs show promise in toxicity understanding and molecule editing but face significant challenges.

Conclusion: ToxiMol and ToxiEval provide foundational tools for advancing molecular toxicity repair, highlighting MLLMs' potential despite limitations.

Abstract: Toxicity remains a leading cause of early-stage drug development failure.
Despite advances in molecular design and property prediction, the task of
molecular toxicity repair - generating structurally valid molecular
alternatives with reduced toxicity - has not yet been systematically defined or
benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task
for general-purpose Multimodal Large Language Models (MLLMs) focused on
molecular toxicity repair. We construct a standardized dataset covering 11
primary tasks and 560 representative toxic molecules spanning diverse
mechanisms and granularities. We design a prompt annotation pipeline with
mechanism-aware and task-adaptive capabilities, informed by expert
toxicological knowledge. In parallel, we propose an automated evaluation
framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic
accessibility, drug-likeness, and structural similarity into a high-throughput
evaluation chain for repair success. We systematically assess nearly 30
mainstream general-purpose MLLMs and design multiple ablation studies to
analyze key factors such as evaluation criteria, candidate diversity, and
failure attribution. Experimental results show that although current MLLMs
still face significant challenges on this task, they begin to demonstrate
promising capabilities in toxicity understanding, semantic constraint
adherence, and structure-aware molecule editing.

</details>


### [279] [Spurious Rewards: Rethinking Training Signals in RLVR](https://arxiv.org/pdf/2506.10947)
*Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, Luke Zettlemoyer*

Main category: cs.AI

TL;DR: RLVR improves mathematical reasoning in Qwen2.5-Math-7B even with spurious rewards, but results vary across model families. Code reasoning in Qwen increases significantly post-RLVR.


<details>
  <summary>Details</summary>
Motivation: To explore how reinforcement learning with verifiable rewards (RLVR) performs with spurious rewards and its impact on mathematical reasoning across different models.

Method: RLVR is applied to models like Qwen2.5-Math-7B, Llama3, and OLMo2, using various spurious rewards (random, format, incorrect label, etc.).

Result: Qwen2.5-Math-7B shows significant performance gains (up to 27.1%) even with spurious rewards, while other models like Llama3 or OLMo2 do not. Code reasoning in Qwen increases from 65% to over 90%.

Conclusion: RLVR may surface pretrained reasoning representations, but its effectiveness varies by model. Future research should validate RLVR on diverse models.

Abstract: We show that reinforcement learning with verifiable rewards (RLVR) can elicit
strong mathematical reasoning in certain models even with spurious rewards that
have little, no, or even negative correlation with the correct answer. For
example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute
points by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect
label), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the
29.1% gained with ground truth rewards. However, the spurious rewards that work
for Qwen often fail to yield gains with other model families like Llama3 or
OLMo2. In particular, we find code reasoning -- thinking in code without actual
code execution -- to be a distinctive Qwen2.5-Math behavior that becomes
significantly more frequent after RLVR, from 65% to over 90%, even with
spurious rewards. Overall, we hypothesize that, given the lack of useful reward
signal, RLVR must somehow be surfacing useful reasoning representations learned
during pretraining, although the exact mechanism remains a topic for future
work. We suggest that future RLVR research should possibly be validated on
diverse models rather than a single de facto choice, as we show that it is easy
to get significant performance gains on Qwen models even with completely
spurious reward signals.

</details>


### [280] [Croppable Knowledge Graph Embedding](https://arxiv.org/pdf/2407.02779)
*Yushan Zhu, Wen Zhang, Zhiqiang Liu, Mingyang Chen, Lei Liang, Huajun Chen*

Main category: cs.AI

TL;DR: Proposes MED, a KGE training framework enabling one training to produce croppable models for various dimensional needs, eliminating retraining costs.


<details>
  <summary>Details</summary>
Motivation: Current KGE models require retraining for different dimensions, increasing costs and reducing efficiency. MED aims to solve this.

Method: Introduces mutual learning, evolutionary improvement, and dynamic loss weight mechanisms to enable flexible sub-model cropping.

Result: Validated on 4 KGE models and datasets, showing effectiveness, efficiency, and extensibility, including application to BERT.

Conclusion: MED offers a cost-effective, flexible solution for KGE dimensional adaptability, with proven success across diverse scenarios.

Abstract: Knowledge Graph Embedding (KGE) is a common approach for Knowledge Graphs
(KGs) in AI tasks. Embedding dimensions depend on application scenarios.
Requiring a new dimension means training a new KGE model from scratch,
increasing cost and limiting efficiency and flexibility. In this work, we
propose a novel KGE training framework MED. It allows one training to obtain a
croppable KGE model for multiple scenarios with different dimensional needs.
Sub-models of required dimensions can be directly cropped and used without
extra training. In MED, we propose a mutual learning mechanism to improve the
low-dimensional sub-models and make high-dimensional sub-models retain the
low-dimensional sub-models' capacity, an evolutionary improvement mechanism to
promote the high-dimensional sub-models to master the triple that the
low-dimensional sub-models can not, and a dynamic loss weight to adaptively
balance the multiple losses. Experiments on 4 KGE models across 4 standard KG
completion datasets, 3 real-world scenarios using a large-scale KG, and
extending MED to the BERT language model demonstrate its effectiveness, high
efficiency, and flexible extensibility.

</details>


### [281] [Mimicking Human Intuition: Cognitive Belief-Driven Reinforcement Learning](https://arxiv.org/pdf/2410.01739)
*Xingrui Gu, Guanren Qiao, Chuyi Jiang*

Main category: cs.AI

TL;DR: CBD-RL introduces a cognitive belief-driven framework to enhance RL by guiding decision-making and improving sample efficiency, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional RL lacks guided exploration and struggles with sample efficiency, prompting the need for a cognitive-inspired approach.

Method: CBD-RL integrates cognitive heuristics and a belief system to structure learning, optimizing action probabilities and organizing state-action pairs.

Result: Implementations (CBDQ, CBDPPO, CBDSAC) show superior performance in diverse environments like Atari and MuJoCo.

Conclusion: The framework bridges cognitive science and RL, offering interpretable, efficient, and cognitively inspired solutions.

Abstract: Traditional reinforcement learning (RL) methods mainly rely on
trial-and-error exploration, often lacking mechanisms to guide agents toward
more informative decision-making and struggling to leverage past experiences,
resulting in low sample efficiency. To overcome this issue, we propose an
innovative framework inspired by cognitive principles: Cognitive Belief-Driven
Reinforcement Learning (CBD-RL). By incorporating cognitive heuristics, CBD-RL
transforms conventional trial-and-error learning into a more structured and
guided learning paradigm, simulating the human reasoning process. This
framework's core is a belief system that optimizes action probabilities by
integrating feedback with prior experience, thus enhancing decision making
under uncertainty. It also organizes state-action pairs into meaningful
categories, promoting generalization and improving sample efficiency. The
concrete implementations of this framework, CBDQ, CBDPPO, and CBDSAC,
demonstrate superior performance in discrete and continuous action spaces in
diverse environments such as Atari and MuJoCo. By bridging cognitive science
and reinforcement learning, this research opens a new avenue for developing RL
systems that are more interpretable, efficient, and cognitively inspired.

</details>


### [282] [Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge](https://arxiv.org/pdf/2411.01796)
*Weihua Du, Qiushi Lyu, Jiaming Shan, Zhenting Qi, Hongxin Zhang, Sunli Chen, Andi Peng, Tianmin Shu, Kwonjoon Lee, Behzad Dariush, Chuang Gan*

Main category: cs.AI

TL;DR: CHAIC is a benchmark for testing embodied agents' social perception and cooperation with humans under physical constraints, featuring tasks and a new method using LLMs.


<details>
  <summary>Details</summary>
Motivation: To assess and improve embodied agents' ability to assist humans with physical constraints in household and outdoor tasks.

Method: Introduces CHAIC with constrained agents and tasks, benchmarks baselines, and proposes a method using large language models and behavior modeling.

Result: Empirical evaluations show the benchmark effectively assesses machine social intelligence.

Conclusion: CHAIC provides a systematic way to evaluate and advance embodied AI's social cooperation capabilities.

Abstract: We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied
social intelligence challenge designed to test social perception and
cooperation in embodied agents. In CHAIC, the goal is for an embodied agent
equipped with egocentric observations to assist a human who may be operating
under physical constraints -- e.g., unable to reach high places or confined to
a wheelchair -- in performing common household or outdoor tasks as efficiently
as possible. To achieve this, a successful helper must: (1) infer the human's
intents and constraints by following the human and observing their behaviors
(social perception), and (2) make a cooperative plan tailored to the human
partner to solve the task as quickly as possible, working together as a team
(cooperative planning). To benchmark this challenge, we create four new agents
with real physical constraints and eight long-horizon tasks featuring both
indoor and outdoor scenes with various constraints, emergency events, and
potential risks. We benchmark planning- and learning-based baselines on the
challenge and introduce a new method that leverages large language models and
behavior modeling. Empirical evaluations demonstrate the effectiveness of our
benchmark in enabling systematic assessment of key aspects of machine social
intelligence. Our benchmark and code are publicly available at
https://github.com/UMass-Embodied-AGI/CHAIC.

</details>


### [283] [Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problem](https://arxiv.org/pdf/2412.14382)
*Junyang Cai, Serdar Kadioglu, Bistra Dilkina*

Main category: cs.AI

TL;DR: Balans is an adaptive meta-solver for MIPs with online learning, eliminating the need for offline training and improving performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based MIP solvers rely heavily on offline training, which is costly and lacks generalization. Balans addresses this by enabling online learning.

Method: Balans uses adaptive large-neighborhood search with destroy and repair operators, guided by multi-armed bandit algorithms for real-time neighborhood selection.

Result: Experiments show Balans outperforms default MIP solvers and state-of-the-art large-neighborhood search methods.

Conclusion: Balans is a promising, open-source solution for MIP solving with online learning capabilities.

Abstract: Mixed-integer programming (MIP) is a powerful paradigm for modeling and
solving various important combinatorial optimization problems. Recently,
learning-based approaches have shown a potential to speed up MIP solving via
offline training that then guides important design decisions during the search.
However, a significant drawback of these methods is their heavy reliance on
offline training, which requires collecting training datasets and
computationally costly training epochs yet offering only limited generalization
to unseen (larger) instances. In this paper, we propose Balans, an adaptive
meta-solver for MIPs with online learning capability that does not require any
supervision or apriori training. At its core, Balans is based on adaptive
large-neighborhood search, operating on top of an MIP solver by successive
applications of destroy and repair neighborhood operators. During the search,
the selection among different neighborhood definitions is guided on the fly for
the instance at hand via multi-armed bandit algorithms. Our extensive
experiments on hard optimization instances show that Balans offers significant
performance gains over the default MIP solver, is better than committing to any
single best neighborhood, and improves over the state-of-the-art
large-neighborhood search for MIPs. Finally, we release Balans as a highly
configurable, MIP solver agnostic, open-source software.

</details>


### [284] [Multi-task Representation Learning for Mixed Integer Linear Programming](https://arxiv.org/pdf/2412.14409)
*Junyang Cai, Taoan Huang, Bistra Dilkina*

Main category: cs.AI

TL;DR: A multi-task learning framework for ML-guided MILP solving is introduced, improving efficiency and adaptability across solvers and tasks.


<details>
  <summary>Details</summary>
Motivation: Current ML-guided MILP-solving methods lack scalability and adaptability due to separate offline processes.

Method: Proposes a multi-task learning framework for MILP embeddings to guide solving across solvers and tasks.

Result: The framework matches specialized models within the same distribution and excels in generalization across problem sizes and tasks.

Conclusion: The multi-task learning approach enhances MILP-solving efficiency and adaptability, outperforming specialized models in generalization.

Abstract: Mixed Integer Linear Programs (MILPs) are highly flexible and powerful tools
for modeling and solving complex real-world combinatorial optimization
problems. Recently, machine learning (ML)-guided approaches have demonstrated
significant potential in improving MILP-solving efficiency. However, these
methods typically rely on separate offline data collection and training
processes, which limits their scalability and adaptability. This paper
introduces the first multi-task learning framework for ML-guided MILP solving.
The proposed framework provides MILP embeddings helpful in guiding MILP solving
across solvers (e.g., Gurobi and SCIP) and across tasks (e.g., Branching and
Solver configuration). Through extensive experiments on three widely used MILP
benchmarks, we demonstrate that our multi-task learning model performs
similarly to specialized models within the same distribution. Moreover, it
significantly outperforms them in generalization across problem sizes and
tasks.

</details>


### [285] [Position: Theory of Mind Benchmarks are Broken for Large Language Models](https://arxiv.org/pdf/2412.19726)
*Matthew Riemer, Zahra Ashktorab, Djallel Bouneffouf, Payel Das, Miao Liu, Justin D. Weisz, Murray Campbell*

Main category: cs.AI

TL;DR: The paper critiques current theory of mind benchmarks for LLMs, proposing 'functional theory of mind' as a better metric for evaluating adaptability to partners.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to test LLMs' adaptability to new partners, wrongly assuming human-like consistency in AI reasoning.

Method: Introduces 'functional theory of mind' to measure LLMs' ability to adapt to partners' behaviors in-context.

Result: Open-source LLMs excel in literal theory of mind but struggle with functional theory of mind, even with simple partner policies.

Conclusion: Functional theory of mind should be prioritized in LLM evaluations, as it better reflects real-world adaptability challenges.

Abstract: Our paper argues that the majority of theory of mind benchmarks are broken
because of their inability to directly test how large language models (LLMs)
adapt to new partners. This problem stems from the fact that theory of mind
benchmarks for LLMs are overwhelmingly inspired by the methods used to test
theory of mind in humans and fall victim to a fallacy of attributing human-like
qualities to AI agents. We expect that humans will engage in a consistent
reasoning process across various questions about a situation, but this is known
to not be the case for current LLMs. Most theory of mind benchmarks only
measure what we call literal theory of mind: the ability to predict the
behavior of others. However, this type of metric is only informative when
agents exhibit self-consistent reasoning. Thus, we introduce the concept of
functional theory of mind: the ability to adapt to agents in-context following
a rational response to their behavior. We find that many open source LLMs are
capable of displaying strong literal theory of mind capabilities, but seem to
struggle with functional theory of mind -- even with exceedingly simple partner
policies. Simply put, strong literal theory of mind performance does not
necessarily imply strong functional theory of mind performance or vice versa.
Achieving functional theory of mind, particularly over long interaction
horizons with a partner, is a significant challenge deserving a prominent role
in any meaningful LLM theory of mind evaluation.

</details>


### [286] [CollabLLM: From Passive Responders to Active Collaborators](https://arxiv.org/pdf/2502.00640)
*Shirley Wu, Michel Galley, Baolin Peng, Hao Cheng, Gavin Li, Yao Dou, Weixin Cai, James Zou, Jure Leskovec, Jianfeng Gao*

Main category: cs.AI

TL;DR: CollabLLM enhances multiturn human-LLM collaboration by using long-term rewards, improving task performance and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack long-term interaction optimization, leading to passive responses and inefficient conversations.

Method: CollabLLM employs collaborative simulation and Multiturn-aware Rewards for reinforcement fine-tuning.

Result: Outperforms baselines by 18.5% in task performance and 46.3% in interactivity; increases user satisfaction by 17.6%.

Conclusion: CollabLLM advances human-centered AI by actively uncovering user intent and improving interaction efficiency.

Abstract: Large Language Models are typically trained with next-turn rewards, limiting
their ability to optimize for long-term interaction. As a result, they often
respond passively to ambiguous or open-ended user requests, failing to help
users reach their ultimate intents and leading to inefficient conversations. To
address these limitations, we introduce CollabLLM, a novel and general training
framework that enhances multiturn human-LLM collaboration. Its key innovation
is a collaborative simulation that estimates the long-term contribution of
responses using Multiturn-aware Rewards. By reinforcement fine-tuning these
rewards, CollabLLM goes beyond responding to user requests, and actively
uncovers user intent and offers insightful suggestions-a key step towards more
human-centered AI. We also devise a multiturn interaction benchmark with three
challenging tasks such as document creation. CollabLLM significantly
outperforms our baselines with averages of 18.5% higher task performance and
46.3% improved interactivity by LLM judges. Finally, we conduct a large user
study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and
reduces user spent time by 10.4%.

</details>


### [287] [Training-Free Safe Denoisers for Safe Use of Diffusion Models](https://arxiv.org/pdf/2502.08011)
*Mingyu Kim, Dongjun Kim, Amman Yusuf, Stefano Ermon, Mijung Park*

Main category: cs.AI

TL;DR: The paper proposes a training-free method to prevent diffusion models from generating unsafe or copyrighted content by modifying the sampling trajectory using a negation set, avoiding retraining.


<details>
  <summary>Details</summary>
Motivation: Address misuse of diffusion models for generating NSFW or copyrighted content without requiring retraining or fine-tuning.

Method: Directly modifies the sampling trajectory using a negation set to avoid unsafe regions, formalizing the relationship between safe and unsafe samples.

Result: Develops a practical algorithm that produces high-quality samples while avoiding negation areas in various generation scenarios.

Conclusion: The training-free safe denoiser shows promise for safer use of diffusion models.

Abstract: There is growing concern over the safety of powerful diffusion models (DMs),
as they are often misused to produce inappropriate, not-safe-for-work (NSFW)
content or generate copyrighted material or data of individuals who wish to be
forgotten. Many existing methods tackle these issues by heavily relying on
text-based negative prompts or extensively retraining DMs to eliminate certain
features or samples. In this paper, we take a radically different approach,
directly modifying the sampling trajectory by leveraging a negation set (e.g.,
unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid
specific regions of data distribution, without needing to retrain or fine-tune
DMs. We formally derive the relationship between the expected denoised samples
that are safe and those that are not safe, leading to our $\textit{safe}$
denoiser which ensures its final samples are away from the area to be negated.
Inspired by the derivation, we develop a practical algorithm that successfully
produces high-quality samples while avoiding negation areas of the data
distribution in text-conditional, class-conditional, and unconditional image
generation scenarios. These results hint at the great potential of our
training-free safe denoiser for using DMs more safely.

</details>


### [288] [Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search](https://arxiv.org/pdf/2503.04412)
*Yuichi Inoue, Kou Misaki, Yuki Imajuku, So Kuroki, Taishi Nakamura, Takuya Akiba*

Main category: cs.AI

TL;DR: AB-MCTS improves LLM reasoning by dynamically balancing exploration and exploitation during inference, outperforming repeated sampling and standard MCTS.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM reasoning by leveraging external feedback for multi-turn refinement, unlike repeated sampling which lacks feedback utilization.

Method: Proposes Adaptive Branching Monte Carlo Tree Search (AB-MCTS), dynamically choosing to expand new responses or refine existing ones based on feedback.

Result: AB-MCTS consistently outperforms repeated sampling and standard MCTS in coding and engineering tasks.

Conclusion: Combining LLM response diversity with multi-turn refinement via AB-MCTS is key for effective inference-time scaling.

Abstract: Recent advances demonstrate that increasing inference-time computation can
significantly boost the reasoning capabilities of large language models (LLMs).
Although repeated sampling (i.e., generating multiple candidate outputs) is a
highly effective strategy, it does not leverage external feedback signals for
refinement, which are often available in tasks like coding. In this work, we
propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel
inference-time framework that generalizes repeated sampling with principled
multi-turn exploration and exploitation. At each node in the search tree,
AB-MCTS dynamically decides whether to "go wider" by expanding new candidate
responses or "go deeper" by revisiting existing ones based on external feedback
signals. We evaluate our method on complex coding and engineering tasks using
frontier models. Empirical results show that AB-MCTS consistently outperforms
both repeated sampling and standard MCTS, underscoring the importance of
combining the response diversity of LLMs with multi-turn solution refinement
for effective inference-time scaling.

</details>


### [289] [Don't Lag, RAG: Training-Free Adversarial Detection Using RAG](https://arxiv.org/pdf/2504.04858)
*Roie Kazoom, Raz Lapid, Moshe Sipper, Ofer Hadar*

Main category: cs.AI

TL;DR: VRAG is a training-free framework using VLMs for adversarial patch detection, achieving high accuracy without retraining.


<details>
  <summary>Details</summary>
Motivation: Traditional defenses require retraining, making them impractical; VRAG offers a training-free solution.

Method: VRAG retrieves similar patches/images from a database and uses generative reasoning with VLMs for detection.

Result: UI-TARS-72B-DPO achieves 95% accuracy; Gemini-2.0 reaches 98%. VRAG is effective with minimal human input.

Conclusion: VRAG provides a robust, practical defense against adversarial patch attacks without additional training.

Abstract: Adversarial patch attacks pose a major threat to vision systems by embedding
localized perturbations that mislead deep models. Traditional defense methods
often require retraining or fine-tuning, making them impractical for real-world
deployment. We propose a training-free Visual Retrieval-Augmented Generation
(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial
patch detection. By retrieving visually similar patches and images that
resemble stored attacks in a continuously expanding database, VRAG performs
generative reasoning to identify diverse attack types, all without additional
training or fine-tuning. We extensively evaluate open-source large-scale VLMs,
including Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside
Gemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO
model achieves up to 95 percent classification accuracy, setting a new
state-of-the-art for open-source adversarial patch detection. Gemini-2.0
attains the highest overall accuracy, 98 percent, but remains closed-source.
Experimental results demonstrate VRAG's effectiveness in identifying a variety
of adversarial patches with minimal human annotation, paving the way for
robust, practical defenses against evolving adversarial patch attacks.

</details>


### [290] [AssistanceZero: Scalably Solving Assistance Games](https://arxiv.org/pdf/2504.07091)
*Cassidy Laidlaw, Eli Bronstein, Timothy Guo, Dylan Feng, Lukas Berglund, Justin Svegliato, Stuart Russell, Anca Dragan*

Main category: cs.AI

TL;DR: Assistance games, an alternative to RLHF, address deceptive behavior by modeling assistant-user interactions as a two-player game. AssistanceZero, a scalable approach, outperforms RL and imitation learning in a complex Minecraft-based game, reducing human effort in tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome RLHF's drawbacks like deceptive behavior and to scale assistance games to complex environments.

Method: AssistanceZero extends AlphaZero with a neural network predicting human actions and rewards, enabling planning under uncertainty.

Result: AssistanceZero outperforms model-free RL and imitation learning, reducing human actions in Minecraft tasks.

Conclusion: Assistance games are tractable for training effective AI assistants in complex environments.

Abstract: Assistance games are a promising alternative to reinforcement learning from
human feedback (RLHF) for training AI assistants. Assistance games resolve key
drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly
modeling the interaction between assistant and user as a two-player game where
the assistant cannot observe their shared goal. Despite their potential,
assistance games have only been explored in simple settings. Scaling them to
more complex environments is difficult because it requires both solving
intractable decision-making problems under uncertainty and accurately modeling
human users' behavior. We present the first scalable approach to solving
assistance games and apply it to a new, challenging Minecraft-based assistance
game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends
AlphaZero with a neural network that predicts human actions and rewards,
enabling it to plan under uncertainty. We show that AssistanceZero outperforms
model-free RL algorithms and imitation learning in the Minecraft-based
assistance game. In a human study, our AssistanceZero-trained assistant
significantly reduces the number of actions participants take to complete
building tasks in Minecraft. Our results suggest that assistance games are a
tractable framework for training effective AI assistants in complex
environments. Our code and models are available at
https://github.com/cassidylaidlaw/minecraft-building-assistance-game.

</details>


### [291] [A Vision for Auto Research with LLM Agents](https://arxiv.org/pdf/2504.18765)
*Chengwei Liu, Chong Wang, Jiayue Cao, Jingquan Ge, Kun Wang, Lyuye Zhang, Ming-Ming Cheng, Penghai Zhao, Tianlin Li, Xiaojun Jia, Xiang Li, Xinfeng Li, Yang Liu, Yebo Feng, Yihao Huang, Yijia Xu, Yuqiang Sun, Zhenhong Zhou, Zhengzi Xu*

Main category: cs.AI

TL;DR: A multi-agent framework using LLMs to automate and optimize scientific research, covering all phases from literature review to dissemination.


<details>
  <summary>Details</summary>
Motivation: Address fragmented workflows, uneven expertise, and cognitive overload in research.

Method: Leverages LLMs and modular agent collaboration to automate research lifecycle.

Result: Preliminary results show feasibility and potential for AI-driven research.

Conclusion: Auto Research is a promising paradigm for scalable, self-improving scientific inquiry.

Abstract: This paper introduces Agent-Based Auto Research, a structured multi-agent
framework designed to automate, coordinate, and optimize the full lifecycle of
scientific research. Leveraging the capabilities of large language models
(LLMs) and modular agent collaboration, the system spans all major research
phases, including literature review, ideation, methodology planning,
experimentation, paper writing, peer review response, and dissemination. By
addressing issues such as fragmented workflows, uneven methodological
expertise, and cognitive overload, the framework offers a systematic and
scalable approach to scientific inquiry. Preliminary explorations demonstrate
the feasibility and potential of Auto Research as a promising paradigm for
self-improving, AI-driven research processes.

</details>


### [292] [A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem](https://arxiv.org/pdf/2505.13522)
*Nathalie Sanghikian, Rafael Meirelles, Rafael Martinelli, Anand Subramanian*

Main category: cs.AI

TL;DR: A heuristic approach combining Beam Search and Iterated Local Search is proposed to solve the Maritime Inventory Routing Problem (MIRP), improving solutions for 19 out of 72 instances.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with MIRP's complexity; exact methods are impractical for daily operations, and non-MIP heuristics are rare due to constraints. The study aims to encourage MIRPLib use and improve solutions.

Method: A heuristic combining Beam Search variation and Iterated Local Search, avoiding mathematical optimization.

Result: Improved best-known solutions for 19 out of 72 instances within acceptable CPU time.

Conclusion: The heuristic is effective for deterministic, finite-horizon, single-product MIRP, encouraging further use of MIRPLib.

Abstract: Maritime Inventory Routing Problem (MIRP) plays a crucial role in the
integration of global maritime commerce levels. However, there are still no
well-established methodologies capable of efficiently solving large MIRP
instances or their variants due to the high complexity of the problem. The
adoption of exact methods, typically based on Mixed Integer Programming (MIP),
for daily operations is nearly impractical due to the CPU time required, as
planning must be executed multiple times while ensuring high-quality results
within acceptable time limits. Non-MIP-based heuristics are less frequently
applied due to the highly constrained nature of the problem, which makes even
the construction of an effective initial solution challenging. Papageorgiou et
al. (2014) introduced a single-product MIRP as the foundation for MIRPLib,
aiming to provide a collection of publicly available benchmark instances.
However, only a few studies that propose new methodologies have been published
since then. To encourage the use of MIRPLib and facilitate result comparisons,
this study presents a heuristic approach that does not rely on mathematical
optimization techniques to solve a deterministic, finite-horizon,
single-product MIRP. The proposed heuristic combines a variation of a Beam
Search algorithm with an Iterated Local Search procedure. Among the 72
instances tested, the developed methodology can improve the best-known solution
for 19 instances within an acceptable CPU time.

</details>


### [293] [Evaluation of LLMs for mathematical problem solving](https://arxiv.org/pdf/2506.00309)
*Ruonan Wang, Runxi Wang, Yunwen Shen, Chengfeng Wu, Qinglin Zhou, Rohitash Chandra*

Main category: cs.AI

TL;DR: The study evaluates GPT-4o, DeepSeek-V3, and Gemini-2.0 on math tasks using the SCoT framework, finding GPT-4o most stable, DeepSeek-V3 strong in structured domains, and Gemini-2.0 weak in multi-step reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' potential in solving mathematical problems, which is understudied despite their success in other educational tasks.

Method: Comparison of three LLMs on three math datasets using a five-dimensional SCoT framework to evaluate correctness, completeness, validity, accuracy, and comprehension.

Result: GPT-4o performs most consistently, excelling in high-level questions. DeepSeek-V3 is strong in structured domains but fluctuates in statistical tasks. Gemini-2.0 struggles with multi-step reasoning.

Conclusion: Each model has strengths and weaknesses, with GPT-4o being the most reliable overall, but all require improvements in specific areas like explanation, step completeness, and reasoning flexibility.

Abstract: Large Language Models (LLMs) have shown impressive performance on a range of
educational tasks, but are still understudied for their potential to solve
mathematical problems. In this study, we compare three prominent LLMs,
including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of
varying complexities (GSM8K, MATH500, and UNSW datasets). We take a
five-dimensional approach based on the Structured Chain-of-Thought (SCoT)
framework to assess final answer correctness, step completeness, step validity,
intermediate calculation accuracy, and problem comprehension. The results show
that GPT-4o is the most stable and consistent in performance across all the
datasets, but particularly it performs outstandingly in high-level questions of
the UNSW dataset. DeepSeek-V3 is competitively strong in well-structured
domains such as optimisation, but suffers from fluctuations in accuracy in
statistical inference tasks. Gemini-2.0 shows strong linguistic understanding
and clarity in well-structured problems but performs poorly in multi-step
reasoning and symbolic logic. Our error analysis reveals particular deficits in
each model: GPT-4o is at times lacking in sufficient explanation or precision;
DeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in
mathematical reasoning in higher dimensions.

</details>


### [294] [Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems](https://arxiv.org/pdf/2506.03586)
*Yu Ma, Xiao Li, Chongtao Guo, Le Liang, Shi Jin*

Main category: cs.AI

TL;DR: A hybrid DRL approach optimizes RIS phase shift design and subcarrier allocation in OFDM systems to reduce average delay, using PPO-Θ and PPO-N, with multi-agent and transfer learning strategies for efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the stochastic packet arrivals and optimize delay in RIS-assisted OFDM systems, leveraging reinforcement learning for sequential decision-making.

Method: Proposes a hybrid DRL approach: PPO-Θ for RIS phase shift design and PPO-N for subcarrier allocation, with multi-agent and transfer learning for efficiency.

Result: Significantly reduces average delay, improves resource allocation efficiency, and enhances system robustness and fairness.

Conclusion: The hybrid DRL framework effectively optimizes RIS-assisted OFDM systems, outperforming baseline methods in delay reduction and resource allocation.

Abstract: This paper investigates a joint phase design and resource allocation problem
in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal
frequency division multiplexing (OFDM) systems to optimize average delay, where
data packets for each user arrive at the base station stochastically. The
sequential optimization problem is inherently a Markov decision process (MDP),
making it fall within the scope of reinforcement learning. To effectively
handle the mixed action space and reduce the state space dimensionality, a
hybrid deep reinforcement learning (DRL) approach is proposed. Specifically,
proximal policy optimization (PPO)-$\Theta$ is employed to optimize RIS phase
shift design, while PPO-N is responsible for subcarrier allocation decisions.
To further mitigate the curse of dimensionality associated with subcarrier
allocation, a multi-agent strategy is introduced to optimize subcarrier
allocation indicater more efficiently. Moreover, to achieve more adaptive
resource allocation and accurately capture network dynamics, key factors
closely related to average delay, including the number of backlogged packets in
buffers and the current packet arrivals, are incorporated into the state space.
Furthermore, a transfer learning framework is introduced to enhance training
efficiency and accelerate convergence. Simulation results demonstrate that the
proposed algorithm significantly reduces average delay, enhances resource
allocation efficiency, and achieves superior system robustness and fairness
compared to baseline methods.

</details>


### [295] [DeePoly: A High-Order Accuracy Scientific Machine Learning Framework for Function Approximation and Solving PDE](https://arxiv.org/pdf/2506.04613)
*Li Liu, Heng Yong*

Main category: cs.AI

TL;DR: DeePoly combines DNNs and polynomial bases for solving PDEs, improving accuracy and efficiency while ensuring convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Traditional DNN-based methods for PDEs lack convergence guarantees and efficiency compared to numerical schemes. DeePoly addresses this gap.

Method: A two-stage approach: DNNs capture global features, followed by linear optimization with DNN-extracted features and polynomial bases.

Result: The method enhances high-order accuracy and efficiency across problem types, maintaining mesh-free and scheme-free properties.

Conclusion: DeePoly offers a robust solution for PDEs, balancing DNNs' global feature approximation with polynomial bases' precision and convergence.

Abstract: Recently, machine learning methods have gained significant traction in
scientific computing, particularly for solving Partial Differential Equations
(PDEs). However, methods based on deep neural networks (DNNs) often lack
convergence guarantees and computational efficiency compared to traditional
numerical schemes. This work introduces DeePoly, a novel framework that
transforms the solution paradigm from pure non-convex parameter optimization to
a two-stage approach: first employing a DNN to capture complex global features,
followed by linear space optimization with combined DNN-extracted features
(Scoper) and polynomial basis functions (Sniper). This strategic combination
leverages the complementary strengths of both methods -- DNNs excel at
approximating complex global features (i.e., high-gradient features) and
stabilize the polynomial approximation while polynomial bases provide
high-precision local corrections with convergence guarantees. Theoretical
analysis and numerical experiments demonstrate that this approach significantly
enhances both high-order accuracy and efficiency across diverse problem types
while maintaining mesh-free and scheme-free properties. This paper also serves
as a theoretical exposition for the open-source project DeePoly.

</details>


### [296] [CHANCERY: Evaluating Corporate Governance Reasoning Capabilities in Language Models](https://arxiv.org/pdf/2506.04636)
*Lucas Irwin, Arda Kaz, Peiyao Sheng, Sewoong Oh, Pramod Viswanath*

Main category: cs.AI

TL;DR: The paper introduces CHANCERY, a corporate governance reasoning benchmark for NLP models, testing their ability to assess action consistency with corporate charters. SOTA models struggle, with reasoning agents performing better.


<details>
  <summary>Details</summary>
Motivation: Existing legal datasets lack focus on reasoning tasks, despite reasoning being central to legal practice. The paper addresses this gap with a specialized benchmark.

Method: The benchmark includes corporate charters and proposals for action, requiring binary classification of consistency. It uses 24 governance principles and 79 real charters.

Result: SOTA models like Claude 3.7 Sonnet (64.5%) and GPT-4o (75.2%) show difficulty, while reasoning agents (ReAct, CodeAct) score higher (76.1%, 78.1%).

Conclusion: CHANCERY highlights the challenge of legal reasoning for NLP models and provides insights into their limitations.

Abstract: Law has long been a domain that has been popular in natural language
processing (NLP) applications. Reasoning (ratiocination and the ability to make
connections to precedent) is a core part of the practice of the law in the real
world. Nevertheless, while multiple legal datasets exist, none have thus far
focused specifically on reasoning tasks. We focus on a specific aspect of the
legal landscape by introducing a corporate governance reasoning benchmark
(CHANCERY) to test a model's ability to reason about whether
executive/board/shareholder's proposed actions are consistent with corporate
governance charters. This benchmark introduces a first-of-its-kind corporate
governance reasoning test for language models - modeled after real world
corporate governance law. The benchmark consists of a corporate charter (a set
of governing covenants) and a proposal for executive action. The model's task
is one of binary classification: reason about whether the action is consistent
with the rules contained within the charter. We create the benchmark following
established principles of corporate governance - 24 concrete corporate
governance principles established in and 79 real life corporate charters
selected to represent diverse industries from a total dataset of 10k real life
corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models
confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet
and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents
exhibit superior performance, with agents based on the ReAct and CodeAct
frameworks scoring 76.1% and 78.1% respectively, further confirming the
advanced legal reasoning capabilities required to score highly on the
benchmark. We also conduct an analysis of the types of questions which current
reasoning models struggle on, revealing insights into the legal reasoning
capabilities of SOTA models.

</details>


### [297] [A Proposal to Extend the Common Model of Cognition with Metacognition](https://arxiv.org/pdf/2506.07807)
*John Laird, Christian Lebiere, Paul Rosenbloom, Andrea Stocco*

Main category: cs.AI

TL;DR: The paper proposes integrating metacognition into the Common Model of Cognition (CMC) by reasoning over explicit representations of cognitive processes in working memory, with minimal extensions to the CMC framework.


<details>
  <summary>Details</summary>
Motivation: To unify metacognition within the CMC, leveraging its existing capabilities for human-like cognitive architectures.

Method: Extends the CMC by reasoning over explicit representations of cognitive processes in working memory, with minimal structural changes.

Result: Demonstrates examples of metacognition within the proposed framework.

Conclusion: The approach successfully integrates metacognition into the CMC, maintaining its simplicity while enhancing cognitive modeling.

Abstract: The Common Model of Cognition (CMC) provides an abstract characterization of
the structure and processing required by a cognitive architecture for
human-like minds. We propose a unified approach to integrating metacognition
within the CMC. We propose that metacognition involves reasoning over explicit
representations of an agent's cognitive capabilities and processes in working
memory. Our proposal exploits the existing cognitive capabilities of the CMC,
making minimal extensions in the structure and information available within
working memory. We provide examples of metacognition within our proposal.

</details>


### [298] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/pdf/2506.07963)
*Jixiang Hong, Yiran Zhang, Guanzhong Wang, Yi Liu, Ji-Rong Wen, Rui Yan*

Main category: cs.AI

TL;DR: A self-supervised dual reward mechanism improves large multimodal models (LMMs) by aligning understanding and generation tasks without external supervision.


<details>
  <summary>Details</summary>
Motivation: LMMs struggle with accurate image-text alignment and unidirectional task solutions, requiring external supervision.

Method: Introduces a dual reward mechanism using reversed input-output pairs to compute self-rewards for optimization.

Result: Enhances model performance, especially in text-to-image tasks, without external supervision.

Conclusion: The self-supervised approach effectively improves LMMs' understanding and generation capabilities.

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [299] [Description and Discussion on DCASE 2025 Challenge Task 2: First-shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring](https://arxiv.org/pdf/2506.10097)
*Tomoya Nishida, Noboru Harada, Daisuke Niizumi, Davide Albertini, Roberto Sannino, Simone Pradolini, Filippo Augusti, Keisuke Imoto, Kota Dohi, Harsh Purohit, Takashi Endo, Yohei Kawaguchi*

Main category: cs.SD

TL;DR: The paper outlines the DCASE 2025 Challenge Task 2, focusing on first-shot unsupervised anomalous sound detection for machine condition monitoring, aiming to deploy ASD systems for new machine types without hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: To enable rapid deployment of ASD systems for new machine types without machine-specific adjustments, leveraging domain generalization.

Method: First-shot problem within a domain generalization framework, using sounds from unseen machine types for evaluation.

Result: Results and analysis will be provided after the challenge submissions.

Conclusion: The task aims to advance ASD systems by simplifying deployment for new machine types, with outcomes pending post-submission analysis.

Abstract: This paper introduces the task description for the Detection and
Classification of Acoustic Scenes and Events (DCASE) 2025 Challenge Task 2,
titled "First-shot unsupervised anomalous sound detection (ASD) for machine
condition monitoring." Building on the DCASE 2024 Challenge Task 2, this task
is structured as a first-shot problem within a domain generalization framework.
The primary objective of the first-shot approach is to facilitate the rapid
deployment of ASD systems for new machine types without requiring
machine-specific hyperparameter tunings. For DCASE 2025 Challenge Task 2,
sounds from previously unseen machine types have been collected and provided as
the evaluation dataset. Results and analysis of the challenge submissions will
be added following the challenge's submission deadline.

</details>


### [300] [FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio Classification](https://arxiv.org/pdf/2506.10207)
*Jun Bai, Rajib Rana, Di Wu, Youyang Qu, Xiaohui Tao, Ji Zhang*

Main category: cs.SD

TL;DR: FedMLAC is a unified framework for Federated Audio Classification (FedAC) addressing data and model heterogeneity, and data poisoning via dual-model architecture and Layer-wise Pruning Aggregation (LPA).


<details>
  <summary>Details</summary>
Motivation: Existing solutions for FedAC challenges (data heterogeneity, model heterogeneity, data poisoning) lack a unified approach, hindering real-world performance.

Method: FedMLAC uses a dual-model architecture (personalized local model + shared Plug-in model) with bidirectional knowledge distillation and LPA for robust aggregation.

Result: FedMLAC outperforms state-of-the-art methods in accuracy and robustness across diverse audio classification benchmarks.

Conclusion: FedMLAC provides a robust, unified solution for FedAC challenges, enhancing performance and adaptability in real-world scenarios.

Abstract: Federated Learning (FL) provides a privacy-preserving paradigm for training
audio classification (AC) models across distributed clients without sharing raw
data. However, Federated Audio Classification (FedAC) faces three critical
challenges that substantially hinder performance: data heterogeneity, model
heterogeneity, and data poisoning. While prior works have attempted to address
these issues, they are typically treated independently, lacking a unified and
robust solution suited to real-world federated audio scenarios. To bridge this
gap, we propose FedMLAC, a unified mutual learning framework designed to
simultaneously tackle these challenges in FedAC. Specifically, FedMLAC
introduces a dual-model architecture on each client, comprising a personalized
local AC model and a lightweight, globally shared Plug-in model. Through
bidirectional knowledge distillation, the Plug-in model enables global
knowledge transfer while adapting to client-specific data distributions, thus
supporting both generalization and personalization. To further enhance
robustness against corrupted audio data, we develop a Layer-wise Pruning
Aggregation (LPA) strategy that filters unreliable Plug-in model updates based
on parameter deviations during server-side aggregation. Extensive experiments
on four diverse audio classification benchmarks, spanning both speech and
non-speech tasks, demonstrate that FedMLAC consistently outperforms existing
state-of-the-art methods in terms of classification accuracy and robustness to
noisy data.

</details>


### [301] [Fine-Grained control over Music Generation with Activation Steering](https://arxiv.org/pdf/2506.10225)
*Dipanshu Panda, Jayden Koshy Joe, Harshith M R, Swathi Narashiman, Pranay Mathur, Anish Veerakumar, Aniruddh Krishna, Keerthiharan A*

Main category: cs.SD

TL;DR: A method for fine-grained control in music generation using inference-time interventions on MusicGen, enabling timbre/style transfer and genre fusion via residual stream or attention layer steering.


<details>
  <summary>Details</summary>
Motivation: To achieve precise control over music generation, allowing for timbre transfer, style transfer, and genre fusion while maintaining meaningful directional information in activation space.

Method: Inference-time interventions on MusicGen, steering the residual stream or attention layer activations using linear probes, modeled as a regression task for improved performance.

Result: Enhanced control over music generation, combining global (text prompts) and local (steering) conditioning.

Conclusion: The method successfully provides fine-grained control over music generation, validated by audio samples.

Abstract: We present a method for fine-grained control over music generation through
inference-time interventions on an autoregressive generative music transformer
called MusicGen. Our approach enables timbre transfer, style transfer, and
genre fusion by steering the residual stream using weights of linear probes
trained on it, or by steering the attention layer activations in a similar
manner. We observe that modelling this as a regression task provides improved
performance, hypothesizing that the mean-squared-error better preserve
meaningful directional information in the activation space. Combined with the
global conditioning offered by text prompts in MusicGen, our method provides
both global and local control over music generation. Audio samples illustrating
our method are available at our demo page.

</details>


### [302] [Discrete Audio Tokens: More Than a Survey!](https://arxiv.org/pdf/2506.10274)
*Pooneh Mousavi, Gallil Maimon, Adel Moumen, Darius Petermann, Jiatong Shi, Haibin Wu, Haici Yang, Anastasia Kuznetsova, Artem Ploujnikov, Ricard Marxer, Bhuvana Ramabhadran, Benjamin Elizalde, Loren Lugosch, Jinyu Li, Cem Subakan, Phil Woodland, Minje Kim, Hung-yi Lee, Shinji Watanabe, Yossi Adi, Mirco Ravanelli*

Main category: cs.SD

TL;DR: A systematic review and benchmark of discrete audio tokenizers, covering speech, music, and general audio, with a proposed taxonomy and evaluation across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing studies on audio tokenization lack unified comparisons across domains and tasks, prompting a comprehensive review.

Method: Proposes a taxonomy of tokenization approaches, evaluates tokenizers on reconstruction, downstream tasks, and acoustic language modeling, and conducts ablation studies.

Result: Highlights key limitations, practical considerations, and open challenges in audio tokenization.

Conclusion: Provides insights and guidance for future research in discrete audio tokenization.

Abstract: Discrete audio tokens are compact representations that aim to preserve
perceptual quality, phonetic content, and speaker characteristics while
enabling efficient storage and inference, as well as competitive performance
across diverse downstream tasks.They provide a practical alternative to
continuous features, enabling the integration of speech and audio into modern
large language models (LLMs). As interest in token-based audio processing
grows, various tokenization methods have emerged, and several surveys have
reviewed the latest progress in the field. However, existing studies often
focus on specific domains or tasks and lack a unified comparison across various
benchmarks. This paper presents a systematic review and benchmark of discrete
audio tokenizers, covering three domains: speech, music, and general audio. We
propose a taxonomy of tokenization approaches based on encoder-decoder,
quantization techniques, training paradigm, streamability, and application
domains. We evaluate tokenizers on multiple benchmarks for reconstruction,
downstream performance, and acoustic language modeling, and analyze trade-offs
through controlled ablation studies. Our findings highlight key limitations,
practical considerations, and open challenges, providing insight and guidance
for future research in this rapidly evolving area. For more information,
including our main results and tokenizer database, please refer to our website:
https://poonehmousavi.github.io/dates-website/.

</details>


### [303] [PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs](https://arxiv.org/pdf/2506.10423)
*Tony Alex, Wish Suharitdamrong, Sara Atito, Armin Mustafa, Philip J. B. Jackson, Imran Razzak, Muhammad Awais*

Main category: cs.SD

TL;DR: The paper explores architectural modifications for improving audio-LLM interactions, focusing on delayed audio integration, attention-based probing, and encoder ensembles, achieving 10-60% improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize the mechanisms for transferring rich semantic representations from audio encoders to LLMs, enhancing their ability to probe audio data for textual queries.

Method: Proposes and evaluates architectural changes (delayed audio integration, attention-only probing, diverse encoder ensembles) using a three-stage training curriculum on 5.6M audio-text pairs.

Result: Demonstrates significant improvements (10-60%) over baseline, validating the effectiveness of the proposed modifications.

Conclusion: The study successfully optimizes cross-modal information transfer in audio-LLMs, highlighting the importance of architectural design choices.

Abstract: The integration of audio perception capabilities into Large Language Models
(LLMs) has enabled significant advances in Audio-LLMs. Although
application-focused developments, particularly in curating training data for
specific capabilities e.g., audio reasoning, have progressed rapidly, the
underlying mechanisms that govern efficient transfer of rich semantic
representations from audio encoders to LLMs remain under-explored. We
conceptualize effective audio-LLM interaction as the LLM's ability to
proficiently probe the audio encoder representations to satisfy textual
queries. This paper presents a systematic investigation on how architectural
design choices can affect that. Beginning with a standard Pengi/LLaVA-style
audio-LLM architecture, we propose and evaluate several modifications guided by
hypotheses derived from mechanistic interpretability studies and LLM
operational principles. Our experiments demonstrate that: (1) delaying audio
integration until the LLM's initial layers establish textual context that
enhances its ability to probe the audio representations for relevant
information; (2) the LLM can proficiently probe audio representations
exclusively through LLM layer's attention submodule, without requiring
propagation to its Feed-Forward Network (FFN) submodule; (3) an efficiently
integrated ensemble of diverse audio encoders provides richer, complementary
representations, thereby broadening the LLM's capacity to probe a wider
spectrum of audio information. All hypotheses are evaluated using an identical
three-stage training curriculum on a dataset of 5.6 million audio-text pairs,
ensuring controlled comparisons. Our final architecture, which incorporates all
proposed modifications, achieves relative improvements from 10\% to 60\% over
the baseline, validating our approach to optimizing cross-modal information
transfer in audio-LLMs. Project page: https://ta012.github.io/PAL/

</details>


### [304] [Description and Discussion on DCASE 2025 Challenge Task 4: Spatial Semantic Segmentation of Sound Scenes](https://arxiv.org/pdf/2506.10676)
*Masahiro Yasuda, Binh Thien Nguyen, Noboru Harada, Romain Serizel, Mayank Mishra, Marc Delcroix, Shoko Araki, Daiki Takeuchi, Daisuke Niizumi, Yasunori Ohishi, Tomohiro Nakatani, Takao Kawamura, Nobutaka Ono*

Main category: cs.SD

TL;DR: The paper introduces the Spatial Semantic Segmentation of Sound Scenes (S5) task for DCASE 2025 Challenge Task 4, focusing on sound event detection and separation from multi-channel spatial signals.


<details>
  <summary>Details</summary>
Motivation: To advance immersive communication by separating sound events with spatial metadata (6DoF) and classifying them.

Method: Outlines the S5 task setting and uses a newly recorded dataset for training and evaluating an S5 system.

Result: Experimental results for the S5 system are reported, though full details await challenge results.

Conclusion: The paper sets the stage for DCASE 2025 Task 4, aiming to improve sound event detection and separation with spatial context.

Abstract: Spatial Semantic Segmentation of Sound Scenes (S5) aims to enhance
technologies for sound event detection and separation from multi-channel input
signals that mix multiple sound events with spatial information. This is a
fundamental basis of immersive communication. The ultimate goal is to separate
sound event signals with 6 Degrees of Freedom (6DoF) information into dry sound
object signals and metadata about the object type (sound event class) and
representing spatial information, including direction. However, because several
existing challenge tasks already provide some of the subset functions, this
task for this year focuses on detecting and separating sound events from
multi-channel spatial input signals. This paper outlines the S5 task setting of
the Detection and Classification of Acoustic Scenes and Events (DCASE) 2025
Challenge Task 4 and the DCASE2025 Task 4 Dataset, newly recorded and curated
for this task. We also report experimental results for an S5 system trained and
evaluated on this dataset. The full version of this paper will be published
after the challenge results are made public.

</details>


### [305] [BNMusic: Blending Environmental Noises into Personalized Music](https://arxiv.org/pdf/2506.10754)
*Chi Zuo, Martin B. Møller, Pablo Martínez-Nuevo, Huayang Huang, Yu Wu, Ye Zhu*

Main category: cs.SD

TL;DR: The paper introduces BNMusic, a framework blending environmental noise into personalized music using text prompts, reducing noise noticeability without excessive volume.


<details>
  <summary>Details</summary>
Motivation: Traditional acoustic masking often requires high volume due to misalignment between noise and masking sounds. Advances in cross-modal generation inspired a new approach.

Method: BNMusic synthesizes music from noise using mel-spectrograms, then adaptively amplifies it to blend noise seamlessly while preserving quality.

Result: Experiments on MusicBench, EPIC-SOUNDS, and ESC-50 show effective noise blending with rhythmically aligned, enjoyable music.

Conclusion: BNMusic improves acoustic experiences by minimizing noise noticeability through personalized, well-blended music.

Abstract: While being disturbed by environmental noises, the acoustic masking technique
is a conventional way to reduce the annoyance in audio engineering that seeks
to cover up the noises with other dominant yet less intrusive sounds. However,
misalignment between the dominant sound and the noise-such as mismatched
downbeats-often requires an excessive volume increase to achieve effective
masking. Motivated by recent advances in cross-modal generation, in this work,
we introduce an alternative method to acoustic masking, aiming to reduce the
noticeability of environmental noises by blending them into personalized music
generated based on user-provided text prompts. Following the paradigm of music
generation using mel-spectrogram representations, we propose a Blending Noises
into Personalized Music (BNMusic) framework with two key stages. The first
stage synthesizes a complete piece of music in a mel-spectrogram representation
that encapsulates the musical essence of the noise. In the second stage, we
adaptively amplify the generated music segment to further reduce noise
perception and enhance the blending effectiveness, while preserving auditory
quality. Our experiments with comprehensive evaluations on MusicBench,
EPIC-SOUNDS, and ESC-50 demonstrate the effectiveness of our framework,
highlighting the ability to blend environmental noise with rhythmically
aligned, adaptively amplified, and enjoyable music segments, minimizing the
noticeability of the noise, thereby improving overall acoustic experiences.

</details>


### [306] [Exploring Performance-Complexity Trade-Offs in Sound Event Detection Models](https://arxiv.org/pdf/2503.11373)
*Tobias Morocutti, Florian Schmid, Jonathan Greif, Francesco Foscarin, Gerhard Widmer*

Main category: cs.SD

TL;DR: Low-complexity networks for sound event detection achieve performance comparable to state-of-the-art models with 5% of the parameters by adapting convolutional models and optimizing training strategies.


<details>
  <summary>Details</summary>
Motivation: To develop efficient networks for sound event detection that balance performance and computational complexity, competing with large models.

Method: Adapt low-complexity convolutional models for frame-wise prediction by adjusting strides, removing global pooling, and adding a sequence model. Investigate training strategies like knowledge distillation.

Result: Achieves performance similar to state-of-the-art transformers with only 5% of the parameters.

Conclusion: Optimized low-complexity models are viable for sound event detection, offering a practical alternative to resource-heavy models.

Abstract: We target the problem of developing new low-complexity networks for the sound
event detection task. Our goal is to meticulously analyze the
performance-complexity trade-off, aiming to be competitive with the large
state-of-the-art models, at a fraction of the computational requirements. We
find that low-complexity convolutional models previously proposed for audio
tagging can be effectively adapted for event detection (which requires
frame-wise prediction) by adjusting convolutional strides, removing the global
pooling, and, importantly, adding a sequence model before the (now frame-wise)
classification heads. Systematic experiments reveal that the best choice for
the sequence model type depends on which complexity metric is most important
for the given application. We also investigate the impact of enhanced training
strategies such as knowledge distillation. In the end, we show that combined
with an optimized training strategy, we can reach event detection performance
comparable to state-of-the-art transformers while requiring only around 5% of
the parameters. We release all our pre-trained models and the code for
reproducing this work to support future research in low-complexity sound event
detection at https://github.com/theMoro/EfficientSED.

</details>


### [307] [Towards a Unified Benchmark for Arabic Pronunciation Assessment: Quranic Recitation as Case Study](https://arxiv.org/pdf/2506.07722)
*Yassine El Kheir, Omnia Ibrahim, Amit Meghanani, Nada Almarwani, Hawau Olamide Toyin, Sadeen Alharbi, Modar Alfadly, Lamya Alkanhal, Ibrahim Selim, Shehab Elbatal, Salima Mdhaffar, Thomas Hain, Yasser Hifny, Mostafa Shahin, Ahmed Ali*

Main category: cs.SD

TL;DR: A unified benchmark for mispronunciation detection in Modern Standard Arabic (MSA) using Qur'anic recitation, including data processing, phoneme set development, and a public test set (QuranMB.v1). Baseline models are evaluated to highlight challenges and potential.


<details>
  <summary>Details</summary>
Motivation: Advance Arabic pronunciation assessment by addressing the lack of standardized tools and publicly available datasets for MSA mispronunciation detection.

Method: Develop a comprehensive pipeline for data processing, create a specialized phoneme set for MSA, and establish the QuranMB.v1 test set. Evaluate baseline models for performance insights.

Result: The QuranMB.v1 benchmark is introduced, and baseline models demonstrate both promise and challenges in MSA pronunciation assessment.

Conclusion: The standardized framework aims to encourage further research in Arabic pronunciation assessment and related language technology applications.

Abstract: We present a unified benchmark for mispronunciation detection in Modern
Standard Arabic (MSA) using Qur'anic recitation as a case study. Our approach
lays the groundwork for advancing Arabic pronunciation assessment by providing
a comprehensive pipeline that spans data processing, the development of a
specialized phoneme set tailored to the nuances of MSA pronunciation, and the
creation of the first publicly available test set for this task, which we term
as the Qur'anic Mispronunciation Benchmark (QuranMB.v1). Furthermore, we
evaluate several baseline models to provide initial performance insights,
thereby highlighting both the promise and the challenges inherent in assessing
MSA pronunciation. By establishing this standardized framework, we aim to
foster further research and development in pronunciation assessment in Arabic
language technology and related applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [308] [Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion](https://arxiv.org/pdf/2506.09999)
*Yukun Chen, Zihuan Qiu, Fanman Meng, Hongliang Li, Linfeng Xu, Qingbo Wu*

Main category: cs.LG

TL;DR: Proposes a Multimodal Class-Incremental Learning (MCIL) method for vision, audio, and text, using pre-trained models, feature extraction, fusion, and contrastive training to address integration and forgetting challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional MCIL methods focus only on vision and text, leaving a gap for integrating audio. This paper aims to address challenges in combining complementary information and avoiding catastrophic forgetting across all three modalities.

Method: Introduces a Multimodal Incremental Feature Extractor (MIFE) for fine-tuning AudioCLIP, an Adaptive Audio-Visual Fusion Module (AAVFM) for feature enhancement, and a contrastive training loss for cross-modal alignment. Also proposes new MCIL evaluation metrics.

Result: Extensive experiments on three multimodal datasets demonstrate the method's effectiveness in handling MCIL tasks.

Conclusion: The proposed approach successfully integrates vision, audio, and text in MCIL, mitigating forgetting and improving performance, validated by new metrics and experiments.

Abstract: Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that
focus only on vision and text, this paper explores MCIL across vision, audio
and text modalities, addressing challenges in integrating complementary
information and mitigating catastrophic forgetting. To tackle these issues, we
propose an MCIL method based on multimodal pre-trained models. Firstly, a
Multimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts
(MoE) structure is introduced to achieve effective incremental fine-tuning for
AudioCLIP. Secondly, to enhance feature discriminability and generalization, we
propose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking
threshold mechanism and a dynamic feature fusion mechanism, along with a
strategy to enhance text diversity. Thirdly, a novel multimodal
class-incremental contrastive training loss is proposed to optimize cross-modal
alignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced
for comprehensive assessment. Extensive experiments on three multimodal
datasets validate the effectiveness of our method.

</details>


### [309] [NOCL: Node-Oriented Conceptualization LLM for Graph Tasks without Message Passing](https://arxiv.org/pdf/2506.10014)
*Wei Li, Mengcheng Lan, Jiaxing Xu, Yiping Ke*

Main category: cs.LG

TL;DR: NOCL is a novel framework combining LLMs with graph tasks, using node descriptions and concepts to address limitations of traditional MPNNs and LLMs in handling graphs.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of supervised MPNNs and LLMs in graph tasks, especially in label-scarce or zero-shot scenarios.

Method: Proposes NOCL with two techniques: node description (converts node attributes to natural language) and node concept (compresses descriptions into semantic embeddings). Also uses graph representation descriptors for unified task handling.

Result: NOCL achieves competitive supervised performance and superior generalization in zero-shot settings compared to MPNNs and hybrid methods.

Conclusion: NOCL provides a promising direction for Graph Foundation Models by unifying graph tasks into a language-based framework.

Abstract: Graphs are essential for modeling complex interactions across domains such as
social networks, biology, and recommendation systems. Traditional Graph Neural
Networks, particularly Message Passing Neural Networks (MPNNs), rely heavily on
supervised learning, limiting their generalization and applicability in
label-scarce scenarios. Recent self-supervised approaches still require labeled
fine-tuning, limiting their effectiveness in zero-shot scenarios. Meanwhile,
Large Language Models (LLMs) excel in natural language tasks but face
significant challenges when applied to graphs, including preserving reasoning
abilities, managing extensive token lengths from rich node attributes, and
being limited to textual-attributed graphs (TAGs) and a single level task. To
overcome these limitations, we propose the Node-Oriented Conceptualization LLM
(NOCL), a novel framework that leverages two core techniques: 1) node
description, which converts heterogeneous node attributes into structured
natural language, extending LLM from TAGs to non-TAGs; 2) node concept, which
encodes node descriptions into compact semantic embeddings using pretrained
language models, significantly reducing token lengths by up to 93.9% compared
to directly using node descriptions. Additionally, our NOCL employs graph
representation descriptors to unify graph tasks at various levels into a
shared, language-based query format, paving a new direction for Graph
Foundation Models. Experimental results validate NOCL's competitive supervised
performance relative to traditional MPNNs and hybrid LLM-MPNN methods and
demonstrate superior generalization in zero-shot settings.

</details>


### [310] [Improving the performance of optical inverse design of multilayer thin films using CNN-LSTM tandem neural networks](https://arxiv.org/pdf/2506.10044)
*Uijun Jung, Deokho Jang, Sungchul Kim, Jungho Kim*

Main category: cs.LG

TL;DR: Deep learning is used for inverse design of SiO2/TiO2 multilayer thin films, employing tandem neural networks (TNNs) to address one-to-many mapping issues. LSTM-LSTM TNNs achieve highest accuracy but are slow, while CNN-LSTM TNNs balance accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Traditional inverse design methods for thin films are time-consuming due to extensive simulations. Deep learning offers a faster alternative.

Method: A tandem neural network (TNN) combines inverse and forward neural networks, tested with MLP, CNN, and LSTM algorithms.

Result: LSTM-LSTM TNNs yield highest accuracy but slowest training; CNN-LSTM TNNs provide optimal balance.

Conclusion: CNN-LSTM TNNs are recommended for efficient and accurate inverse design of thin film optical properties.

Abstract: Optical properties of thin film are greatly influenced by the thickness of
each layer. Accurately predicting these thicknesses and their corresponding
optical properties is important in the optical inverse design of thin films.
However, traditional inverse design methods usually demand extensive numerical
simulations and optimization procedures, which are time-consuming. In this
paper, we utilize deep learning for the inverse design of the transmission
spectra of SiO2/TiO2 multilayer thin films. We implement a tandem neural
network (TNN), which can solve the one-to-many mapping problem that greatly
degrades the performance of deep-learning-based inverse designs. In general,
the TNN has been implemented by a back-to-back connection of an inverse neural
network and a pre-trained forward neural network, both of which have been
implemented based on multilayer perceptron (MLP) algorithms. In this paper, we
propose to use not only MLP, but also convolutional neural network (CNN) or
long short-term memory (LSTM) algorithms in the configuration of the TNN. We
show that an LSTM-LSTM-based TNN yields the highest accuracy but takes the
longest training time among nine configurations of TNNs. We also find that a
CNN-LSTM-based TNN will be an optimal solution in terms of accuracy and speed
because it could integrate the strengths of the CNN and LSTM algorithms.

</details>


### [311] [Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs](https://arxiv.org/pdf/2506.10054)
*Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, Min Zhang*

Main category: cs.LG

TL;DR: Omni-DPO improves Direct Preference Optimization (DPO) by adaptively weighting preference pairs based on quality and model learning dynamics, outperforming baselines in textual and mathematical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing DPO methods treat all preference pairs uniformly, ignoring quality and learning utility, leading to suboptimal performance.

Method: Omni-DPO introduces a dual-perspective framework weighting samples by inherent quality and model performance during training.

Result: Omni-DPO outperforms baselines, notably beating Claude 3 Opus by 6.7 points on Arena-Hard and excelling in mathematical reasoning.

Conclusion: Omni-DPO enhances data utilization and performance, proving effective and robust across tasks.

Abstract: Direct Preference Optimization (DPO) has become a cornerstone of
reinforcement learning from human feedback (RLHF) due to its simplicity and
efficiency. However, existing DPO-based approaches typically treat all
preference pairs uniformly, ignoring critical variations in their inherent
quality and learning utility, leading to suboptimal data utilization and
performance. To address this challenge, we propose Omni-DPO, a dual-perspective
optimization framework that jointly accounts for (1) the inherent quality of
each preference pair and (2) the model's evolving performance on those pairs.
By adaptively weighting samples according to both data quality and the model's
learning dynamics during training, Omni-DPO enables more effective training
data utilization and achieves better performance. Experimental results on
various models and benchmarks demonstrate the superiority and generalization
capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it
finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant
margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning
tasks, Omni-DPO consistently outperforms the baseline methods across all
benchmarks, providing strong empirical evidence for the effectiveness and
robustness of our approach. Code and models will be available at
https://github.com/pspdada/Omni-DPO.

</details>


### [312] [The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset](https://arxiv.org/pdf/2506.10165)
*Gilad Landau, Miran Özdogan, Gereon Elvers, Francesco Mantegna, Pratik Somaiya, Dulhan Jayalath, Luisa Kurth, Teyun Kwon, Brendan Shillingford, Greg Farquhar, Minqi Jiang, Karim Jerbi, Hamza Abdelhedi, Yorguin Mantilla Ramos, Caglar Gulcehre, Mark Woolrich, Natalie Voets, Oiwi Parker Jones*

Main category: cs.LG

TL;DR: The paper introduces the LibriBrain dataset and pnpl library to advance non-invasive speech decoding from brain data, aiming for breakthroughs in brain-computer interfaces.


<details>
  <summary>Details</summary>
Motivation: To restore communication for paralyzed individuals with speech deficits without invasive surgery, leveraging machine learning for a breakthrough in neural decoding.

Method: Presenting the largest within-subject MEG dataset (LibriBrain) and a Python library (pnpl) for data access, alongside defining tasks (Speech Detection, Phoneme Classification) with standardized metrics and benchmarks.

Result: The competition includes Standard and Extended tracks to foster innovation and large-scale computing, promoting progress toward non-invasive speech decoding.

Conclusion: The initiative aims to accelerate advancements in brain-computer interfaces for speech, making the technology more accessible and impactful.

Abstract: The advance of speech decoding from non-invasive brain data holds the
potential for profound societal impact. Among its most promising applications
is the restoration of communication to paralysed individuals affected by speech
deficits such as dysarthria, without the need for high-risk surgical
interventions. The ultimate aim of the 2025 PNPL competition is to produce the
conditions for an "ImageNet moment" or breakthrough in non-invasive neural
decoding, by harnessing the collective power of the machine learning community.
  To facilitate this vision we present the largest within-subject MEG dataset
recorded to date (LibriBrain) together with a user-friendly Python library
(pnpl) for easy data access and integration with deep learning frameworks. For
the competition we define two foundational tasks (i.e. Speech Detection and
Phoneme Classification from brain data), complete with standardised data splits
and evaluation metrics, illustrative benchmark models, online tutorial code, a
community discussion board, and public leaderboard for submissions. To promote
accessibility and participation the competition features a Standard track that
emphasises algorithmic innovation, as well as an Extended track that is
expected to reward larger-scale computing, accelerating progress toward a
non-invasive brain-computer interface for speech.

</details>


### [313] [Textual Bayes: Quantifying Uncertainty in LLM-Based Systems](https://arxiv.org/pdf/2506.10060)
*Brendan Leigh Ross, Noël Vouitsis, Atiyeh Ashari Ghomi, Rasa Hosseinzadeh, Ji Xin, Zhaoyan Liu, Yi Sui, Shiyi Hou, Kin Kwan Leung, Gabriel Loaiza-Ganem, Jesse C. Cresswell*

Main category: cs.LG

TL;DR: The paper proposes a Bayesian approach to quantify uncertainty in LLM-based systems, introducing MHLP for Bayesian inference over prompts, improving predictive accuracy and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Accurate uncertainty quantification in LLMs is critical for high-stakes applications, but is hindered by their black-box nature and sensitivity to prompts.

Method: The authors interpret prompts as textual parameters in a statistical model, using Bayesian inference and introducing MHLP, a novel MCMC algorithm combining prompt optimization with MCMC.

Result: Empirical results show improvements in predictive accuracy and uncertainty quantification across LLM benchmarks and UQ tasks.

Conclusion: The work provides a viable path to integrate Bayesian methods into LLMs, enhancing reliability and calibration of LLM-based systems.

Abstract: Although large language models (LLMs) are becoming increasingly capable of
solving challenging real-world tasks, accurately quantifying their uncertainty
remains a critical open problem, which limits their applicability in
high-stakes domains. This challenge is further compounded by the closed-source,
black-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can
be highly sensitive to the prompts that bind them together, which often require
significant manual tuning (i.e., prompt engineering). In this work, we address
these challenges by viewing LLM-based systems through a Bayesian lens. We
interpret prompts as textual parameters in a statistical model, allowing us to
use a small training dataset to perform Bayesian inference over these prompts.
This novel perspective enables principled uncertainty quantification over both
the model's textual parameters and its downstream predictions, while also
incorporating prior beliefs about these parameters expressed in free-form text.
To perform Bayesian inference, a difficult problem even for well-studied data
modalities, we introduce Metropolis-Hastings through LLM Proposals (MHLP), a
novel Markov chain Monte Carlo (MCMC) algorithm that combines prompt
optimization techniques with standard MCMC methods. MHLP is a turnkey
modification to existing LLM pipelines, including those that rely exclusively
on closed-source models. Empirically, we demonstrate that our method yields
improvements in both predictive accuracy and uncertainty quantification (UQ) on
a range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a
viable path for incorporating methods from the rich Bayesian literature into
the era of LLMs, paving the way for more reliable and calibrated LLM-based
systems.

</details>


### [314] [Optimizing Latent Dimension Allocation in Hierarchical VAEs: Balancing Attenuation and Information Retention for OOD Detection](https://arxiv.org/pdf/2506.10089)
*Dane Williamson, Yangfeng Ji, Matthew Dwyer*

Main category: cs.LG

TL;DR: The paper introduces a framework for optimizing latent dimension allocation in hierarchical variational autoencoders (HVAEs) to improve out-of-distribution (OOD) detection, proving the existence of an optimal allocation ratio and demonstrating its effectiveness empirically.


<details>
  <summary>Details</summary>
Motivation: OOD detection is crucial for safety-critical applications, but current HVAE approaches suffer from arbitrary latent dimension allocation, leading to poor performance.

Method: The authors propose a theoretically grounded framework using information theory to optimize latent dimension allocation in HVAEs, identifying an optimal ratio under a fixed latent budget.

Result: Empirical results show that tuning the allocation ratio improves OOD detection performance across datasets and architectures, outperforming baseline HVAE configurations.

Conclusion: The framework provides principled guidance for latent structure design in HVAEs, enhancing robustness in OOD detection tasks.

Abstract: Out-of-distribution (OOD) detection is a critical task in machine learning,
particularly for safety-critical applications where unexpected inputs must be
reliably flagged. While hierarchical variational autoencoders (HVAEs) offer
improved representational capacity over traditional VAEs, their performance is
highly sensitive to how latent dimensions are distributed across layers.
Existing approaches often allocate latent capacity arbitrarily, leading to
ineffective representations or posterior collapse. In this work, we introduce a
theoretically grounded framework for optimizing latent dimension allocation in
HVAEs, drawing on principles from information theory to formalize the trade-off
between information loss and representational attenuation. We prove the
existence of an optimal allocation ratio $r^{\ast}$ under a fixed latent
budget, and empirically show that tuning this ratio consistently improves OOD
detection performance across datasets and architectures. Our approach
outperforms baseline HVAE configurations and provides practical guidance for
principled latent structure design, leading to more robust OOD detection with
deep generative models.

</details>


### [315] [Efficient kernelized bandit algorithms via exploration distributions](https://arxiv.org/pdf/2506.10091)
*Bingshan Hu, Zheng He, Danica J. Sutherland*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider a kernelized bandit problem with a compact arm set ${X} \subset
\mathbb{R}^d $ and a fixed but unknown reward function $f^*$ with a finite norm
in some Reproducing Kernel Hilbert Space (RKHS). We propose a class of
computationally efficient kernelized bandit algorithms, which we call
GP-Generic, based on a novel concept: exploration distributions. This class of
algorithms includes Upper Confidence Bound-based approaches as a special case,
but also allows for a variety of randomized algorithms. With careful choice of
exploration distribution, our proposed generic algorithm realizes a wide range
of concrete algorithms that achieve $\tilde{O}(\gamma_T\sqrt{T})$ regret
bounds, where $\gamma_T$ characterizes the RKHS complexity. This matches known
results for UCB- and Thompson Sampling-based algorithms; we also show that in
practice, randomization can yield better practical results.

</details>


### [316] [Unsupervised Deep Clustering of MNIST with Triplet-Enhanced Convolutional Autoencoders](https://arxiv.org/pdf/2506.10094)
*Md. Faizul Islam Ansari*

Main category: cs.LG

TL;DR: The paper presents a two-phase deep autoencoder system for unsupervised clustering of MNIST digits, combining reconstruction error and KMeans clustering loss for improved performance.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable and interpretable unsupervised clustering system for handwritten digits that balances reconstruction accuracy and cluster purity.

Method: Uses a deep autoencoder with batch normalization, dropout, and weight decay, followed by a joint objective combining reconstruction error and KMeans clustering loss.

Result: Achieves superior clustering performance (measured by Silhouette Score, Davies-Bouldin Index, NMI, and ARI) and clear t-SNE visualizations of digit clusters.

Conclusion: The method provides a reliable and scalable foundation for unsupervised representation learning in large-scale image clustering applications.

Abstract: This research implements an advanced unsupervised clustering system for MNIST
handwritten digits through two-phase deep autoencoder architecture. A deep
neural autoencoder requires a training process during phase one to develop
minimal yet interpretive representations of images by minimizing reconstruction
errors. During the second phase we unify the reconstruction error with a KMeans
clustering loss for learned latent embeddings through a joint distance-based
objective. Our model contains three elements which include batch normalization
combined with dropout and weight decay for achieving generalized and stable
results. The framework achieves superior clustering performance during
extensive tests which used intrinsic measurements including Silhouette Score
and Davies-Bouldin Index coupled with extrinsic metrics NMI and ARI when
processing image features. The research uses t-SNE visualization to present
learned embeddings that show distinct clusters for digits. Our approach reaches
an optimal combination between data reconstruction accuracy and cluster
separation purity when adding the benefit of understandable results and
scalable implementations. The approach creates a dependable base that helps
deploy unsupervised representation learning in different large-scale image
clustering applications.

</details>


### [317] [Learning to Collaborate Over Graphs: A Selective Federated Multi-Task Learning Approach](https://arxiv.org/pdf/2506.10102)
*Ahmed Elbakary, Chaouki Ben Issaid, Mehdi Bennis*

Main category: cs.LG

TL;DR: A federated multi-task learning method uses cross-client similarity for personalized learning, employing a feature anchor and graph-based regularization for efficient communication and collaboration.


<details>
  <summary>Details</summary>
Motivation: To enable personalized learning in federated settings while ensuring communication efficiency and positive collaboration among clients.

Method: Introduces a feature anchor for compact representation, shares lightweight classification heads, and uses dynamic graph-based regularization with community detection.

Result: Outperforms state-of-the-art baselines in experiments, showing superior efficiency and fairness.

Conclusion: The method effectively balances personalization, collaboration, and efficiency in federated learning.

Abstract: We present a novel federated multi-task learning method that leverages
cross-client similarity to enable personalized learning for each client. To
avoid transmitting the entire model to the parameter server, we propose a
communication-efficient scheme that introduces a feature anchor, a compact
vector representation that summarizes the features learned from the client's
local classes. This feature anchor is shared with the server to account for
local clients' distribution. In addition, the clients share the classification
heads, a lightweight linear layer, and perform a graph-based regularization to
enable collaboration among clients. By modeling collaboration between clients
as a dynamic graph and continuously updating and refining this graph, we can
account for any drift from the clients. To ensure beneficial knowledge transfer
and prevent negative collaboration, we leverage a community detection-based
approach that partitions this dynamic graph into homogeneous communities,
maximizing the sum of task similarities, represented as the graph edges'
weights, within each community. This mechanism restricts collaboration to
highly similar clients within their formed communities, ensuring positive
interaction and preserving personalization. Extensive experiments on two
heterogeneous datasets demonstrate that our method significantly outperforms
state-of-the-art baselines. Furthermore, we show that our method exhibits
superior computation and communication efficiency and promotes fairness across
clients.

</details>


### [318] [NnD: Diffusion-based Generation of Physically-Nonnegative Objects](https://arxiv.org/pdf/2506.10112)
*Nadav Torem, Tamar Sde-Chen, Yoav Y. Schechner*

Main category: cs.LG

TL;DR: Proposes Nonnegative Diffusion (NnD), a generative model using score-based diffusion, to reduce computational costs for simulating complex, nonnegative objects like 3D clouds.


<details>
  <summary>Details</summary>
Motivation: Natural objects like clouds are complex and costly to simulate; existing methods lack scalability.

Method: Uses annealed Langevin dynamics in a learned generative model to enforce non-negativity during generation and inference.

Result: Generates 3D volumetric clouds consistent with physics and indistinguishable from real ones by experts.

Conclusion: NnD effectively reduces computational costs while maintaining physical accuracy for nonnegative objects.

Abstract: Most natural objects have inherent complexity and variability. While some
simple objects can be modeled from first principles, many real-world phenomena,
such as cloud formation, require computationally expensive simulations that
limit scalability. This work focuses on a class of physically meaningful,
nonnegative objects that are computationally tractable but costly to simulate.
To dramatically reduce computational costs, we propose nonnegative diffusion
(NnD). This is a learned generative model using score based diffusion. It
adapts annealed Langevin dynamics to enforce, by design, non-negativity
throughout iterative scene generation and analysis (inference). NnD trains on
high-quality physically simulated objects. Once trained, it can be used for
generation and inference. We demonstrate generation of 3D volumetric clouds,
comprising inherently nonnegative microphysical fields. Our generated clouds
are consistent with cloud physics trends. They are effectively not
distinguished as non-physical by expert perception.

</details>


### [319] [GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments](https://arxiv.org/pdf/2506.10120)
*Maryam Khalid, Akane Sano*

Main category: cs.LG

TL;DR: GRAIL is a benchmarking framework for evaluating graph-based active learning (AL) in dynamic environments, addressing gaps in diversity, fairness, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing graph AL methods focus on static datasets and accuracy, ignoring user-centric factors like diversity and fairness in dynamic settings.

Method: GRAIL introduces novel metrics to evaluate AL strategies on sustained effectiveness, diversity, and user burden, tested on dynamic human sensor datasets.

Result: Experiments show trade-offs between prediction performance and user burden, emphasizing the need to balance node importance, diversity, and topology.

Conclusion: GRAIL provides a comprehensive evaluation tool for graph AL in dynamic environments, highlighting limitations of current methods and the importance of user-centric metrics.

Abstract: Graph-based Active Learning (AL) leverages the structure of graphs to
efficiently prioritize label queries, reducing labeling costs and user burden
in applications like health monitoring, human behavior analysis, and sensor
networks. By identifying strategically positioned nodes, graph AL minimizes
data collection demands while maintaining model performance, making it a
valuable tool for dynamic environments. Despite its potential, existing graph
AL methods are often evaluated on static graph datasets and primarily focus on
prediction accuracy, neglecting user-centric considerations such as sampling
diversity, query fairness, and adaptability to dynamic settings. To bridge this
gap, we introduce GRAIL, a novel benchmarking framework designed to evaluate
graph AL strategies in dynamic, real-world environments. GRAIL introduces novel
metrics to assess sustained effectiveness, diversity, and user burden, enabling
a comprehensive evaluation of AL methods under varying conditions. Extensive
experiments on datasets featuring dynamic, real-life human sensor data reveal
trade-offs between prediction performance and user burden, highlighting
limitations in existing AL strategies. GRAIL demonstrates the importance of
balancing node importance, query diversity, and network topology, providing an
evaluation mechanism for graph AL solutions in dynamic environments.

</details>


### [320] [Meet Me at the Arm: The Cooperative Multi-Armed Bandits Problem with Shareable Arms](https://arxiv.org/pdf/2506.10127)
*Xinyi Hu, Aldo Pacchiano*

Main category: cs.LG

TL;DR: A-CAPELLA, a decentralized algorithm, addresses the no-sensing MMAB problem with unknown arm capacities, achieving logarithmic regret through collaborative hypothesis testing.


<details>
  <summary>Details</summary>
Motivation: The paper tackles the challenge of decentralized coordination and capacity discovery in multi-player multi-armed bandits (MMAB) under severe feedback limitations (no-sensing).

Method: Proposes A-CAPELLA, which uses collaborative hypothesis testing for synchronized successive elimination and capacity estimation via structured collision patterns.

Result: Achieves logarithmic regret in the generalized MMAB setting with unknown arm capacities.

Conclusion: A-CAPELLA provides a provably efficient solution for decentralized no-sensing MMAB with unknown capacities.

Abstract: We study the decentralized multi-player multi-armed bandits (MMAB) problem
under a no-sensing setting, where each player receives only their own reward
and obtains no information about collisions. Each arm has an unknown capacity,
and if the number of players pulling an arm exceeds its capacity, all players
involved receive zero reward. This setting generalizes the classical
unit-capacity model and introduces new challenges in coordination and capacity
discovery under severe feedback limitations. We propose A-CAPELLA (Algorithm
for Capacity-Aware Parallel Elimination for Learning and Allocation), a
decentralized algorithm that achieves logarithmic regret in this generalized
regime. Our main contribution is a collaborative hypothesis testing protocol
that enables synchronized successive elimination and capacity estimation
through carefully structured collision patterns. This represents a provably
efficient learning result in decentralized no-sensing MMAB with unknown arm
capacities.

</details>


### [321] [Provable Sim-to-Real Transfer via Offline Domain Randomization](https://arxiv.org/pdf/2506.10133)
*Arnaud Fickinger, Abderrahim Bendahi, Stuart Russell*

Main category: cs.LG

TL;DR: Offline Domain Randomization (ODR) improves sim-to-real transfer by fitting simulator parameters to offline data, offering theoretical guarantees and practical enhancements like E-DROPO.


<details>
  <summary>Details</summary>
Motivation: Addressing the sim-to-real gap in reinforcement learning by leveraging offline data, which standard Domain Randomization (DR) ignores.

Method: Formalizes ODR as maximum-likelihood estimation, proves consistency, derives error bounds, and introduces E-DROPO with entropy bonus.

Result: ODR's sim-to-real error is tighter than uniform DR, and E-DROPO enhances robustness in zero-shot transfer.

Conclusion: ODR provides a theoretically grounded and empirically effective approach for sim-to-real transfer, with E-DROPO further improving performance.

Abstract: Reinforcement-learning agents often struggle when deployed from simulation to
the real-world. A dominant strategy for reducing the sim-to-real gap is domain
randomization (DR) which trains the policy across many simulators produced by
sampling dynamics parameters, but standard DR ignores offline data already
available from the real system. We study offline domain randomization (ODR),
which first fits a distribution over simulator parameters to an offline
dataset. While a growing body of empirical work reports substantial gains with
algorithms such as DROPO, the theoretical foundations of ODR remain largely
unexplored. In this work, we (i) formalize ODR as a maximum-likelihood
estimation over a parametric simulator family, (ii) prove consistency of this
estimator under mild regularity and identifiability conditions, showing it
converges to the true dynamics as the dataset grows, (iii) derive gap bounds
demonstrating ODRs sim-to-real error is up to an O(M) factor tighter than
uniform DR in the finite-simulator case (and analogous gains in the continuous
setting), and (iv) introduce E-DROPO, a new version of DROPO which adds an
entropy bonus to prevent variance collapse, yielding broader randomization and
more robust zero-shot transfer in practice.

</details>


### [322] [Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning](https://arxiv.org/pdf/2506.10137)
*Daniel Lawson, Adriana Hugessen, Charlotte Cloutier, Glen Berseth, Khimya Khetarpal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Behavioral cloning (BC) methods trained with supervised learning (SL) are an
effective way to learn policies from human demonstrations in domains like
robotics. Goal-conditioning these policies enables a single generalist policy
to capture diverse behaviors contained within an offline dataset. While
goal-conditioned behavior cloning (GCBC) methods can perform well on
in-distribution training tasks, they do not necessarily generalize zero-shot to
tasks that require conditioning on novel state-goal pairs, i.e. combinatorial
generalization. In part, this limitation can be attributed to a lack of
temporal consistency in the state representation learned by BC; if temporally
related states are encoded to similar latent representations, then the
out-of-distribution gap for novel state-goal pairs would be reduced. Hence,
encouraging this temporal consistency in the representation space should
facilitate combinatorial generalization. Successor representations, which
encode the distribution of future states visited from the current state, nicely
encapsulate this property. However, previous methods for learning successor
representations have relied on contrastive samples, temporal-difference (TD)
learning, or both. In this work, we propose a simple yet effective
representation learning objective, $\text{BYOL-}\gamma$ augmented GCBC, which
is not only able to theoretically approximate the successor representation in
the finite MDP case without contrastive samples or TD learning, but also,
results in competitive empirical performance across a suite of challenging
tasks requiring combinatorial generalization.

</details>


### [323] [Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban](https://arxiv.org/pdf/2506.10138)
*Mohammad Taufeeque, Aaron David Tucker, Adam Gleave, Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: The paper analyzes a convolutional RNN trained for Sokoban, revealing mechanisms akin to bidirectional search, with direction-specific activations acting like a value function and specialized kernels forming paths.


<details>
  <summary>Details</summary>
Motivation: To understand how a model-free reinforcement learning-trained RNN solves Sokoban puzzles and leverages test-time compute.

Method: Reverse-engineering the RNN's activations and kernels to identify mechanisms like direction-specific plans, value functions, and path formation.

Result: The RNN uses directional activations for planning, backtracking, and pruning, with kernels extending paths, resembling bidirectional search but with unique features like separate box consideration and layered representations.

Conclusion: The learned mechanisms, though distinct from classical search, are interpretable and leverage test-time compute effectively.

Abstract: We partially reverse-engineer a convolutional recurrent neural network (RNN)
trained to play the puzzle game Sokoban with model-free reinforcement learning.
Prior work found that this network solves more levels with more test-time
compute. Our analysis reveals several mechanisms analogous to components of
classic bidirectional search. For each square, the RNN represents its plan in
the activations of channels associated with specific directions. These
state-action activations are analogous to a value function - their magnitudes
determine when to backtrack and which plan branch survives pruning. Specialized
kernels extend these activations (containing plan and value) forward and
backward to create paths, forming a transition model. The algorithm is also
unlike classical search in some ways. State representation is not unified;
instead, the network considers each box separately. Each layer has its own plan
representation and value function, increasing search depth. Far from being
inscrutable, the mechanisms leveraging test-time compute learned in this
network by model-free training can be understood in familiar terms.

</details>


### [324] [Survival Analysis as Imprecise Classification with Trainable Kernels](https://arxiv.org/pdf/2506.10140)
*Andrei V. Konstantinov, Vlada A. Efremenko, Lev V. Utkin*

Main category: cs.LG

TL;DR: The paper introduces three novel survival models (iSurvM, iSurvQ, iSurvJ) combining imprecise probability theory and attention mechanisms to handle censored data, outperforming traditional methods like the Beran estimator.


<details>
  <summary>Details</summary>
Motivation: Address challenges in survival analysis due to complex data structures and heavy censoring, where traditional nonparametric methods like the Beran estimator fall short.

Method: Proposes three models using interval-valued probability distributions, kernel-based regression with attention weights, and three decision strategies for training.

Result: Experiments show the models, especially iSurvJ, outperform the Beran estimator in accuracy and computational complexity.

Conclusion: The proposed models offer effective, nonparametric solutions for survival analysis with censored data, with publicly available implementations.

Abstract: Survival analysis is a fundamental tool for modeling time-to-event data in
healthcare, engineering, and finance, where censored observations pose
significant challenges. While traditional methods like the Beran estimator
offer nonparametric solutions, they often struggle with the complex data
structures and heavy censoring. This paper introduces three novel survival
models, iSurvM (the imprecise Survival model based on Mean likelihood
functions), iSurvQ (the imprecise Survival model based on the Quantiles of
likelihood functions), and iSurvJ (the imprecise Survival model based on the
Joint learning), that combine imprecise probability theory with attention
mechanisms to handle censored data without parametric assumptions. The first
idea behind the models is to represent censored observations by interval-valued
probability distributions for each instance over time intervals between events
moments. The second idea is to employ the kernel-based Nadaraya-Watson
regression with trainable attention weights for computing the imprecise
probability distribution over time intervals for the entire dataset. The third
idea is to consider three decision strategies for training, which correspond to
the proposed three models. Experiments on synthetic and real datasets
demonstrate that the proposed models, especially iSurvJ, consistently
outperform the Beran estimator from the accuracy and computational complexity
points of view. Codes implementing the proposed models are publicly available.

</details>


### [325] [Physiological-Model-Based Neural Network for Heart Rate Estimation during Daily Physical Activities](https://arxiv.org/pdf/2506.10144)
*Yaowen Zhang, Libera Fresiello, Peter H. Veltink, Dirk W. Donker, Ying Wang*

Main category: cs.LG

TL;DR: A novel physiological-model-based neural network (PMB-NN) framework is introduced for heart rate (HR) estimation, combining physiological constraints with neural networks for accurate, personalized cardiac monitoring.


<details>
  <summary>Details</summary>
Motivation: Early detection of heart failure (HF) is crucial, and individualized HR monitoring can serve as a biomarker. Current methods lack efficiency and interpretability, prompting the need for a better approach.

Method: The PMB-NN framework integrates physiological constraints from a simplified human movement model into neural network training, using VO2 data from daily activities (resting, cycling, running) for HR estimation.

Result: The PMB-NN achieved high accuracy (median R²=0.8, RMSE=8.3 bpm), matching benchmark models and outperforming traditional physiological models (p=0.002). It also identified personalized parameters for HR estimation.

Conclusion: The PMB-NN framework offers a promising tool for personalized, real-time cardiac monitoring during daily activities, enhancing early HF detection.

Abstract: Heart failure (HF) poses a significant global health challenge, with early
detection offering opportunities for improved outcomes. Abnormalities in heart
rate (HR), particularly during daily activities, may serve as early indicators
of HF risk. However, existing HR monitoring tools for HF detection are limited
by their reliability on population-based averages. The estimation of
individualized HR serves as a dynamic digital twin, enabling precise tracking
of cardiac health biomarkers. Current HR estimation methods, categorized into
physiologically-driven and purely data-driven models, struggle with efficiency
and interpretability. This study introduces a novel physiological-model-based
neural network (PMB-NN) framework for HR estimation based on oxygen uptake
(VO2) data during daily physical activities. The framework was trained and
tested on individual datasets from 12 participants engaged in activities
including resting, cycling, and running. By embedding physiological
constraints, which were derived from our proposed simplified human movement
physiological model (PM), into the neural network training process, the PMB-NN
model adheres to human physiological principles while achieving high estimation
accuracy, with a median R$^2$ score of 0.8 and an RMSE of 8.3 bpm. Comparative
statistical analysis demonstrates that the PMB-NN achieves performance on par
with the benchmark neural network model while significantly outperforming
traditional physiological model (p=0.002). In addition, our PMB-NN is adept at
identifying personalized parameters of the PM, enabling the PM to generate
reasonable HR estimation. The proposed framework with a precise VO2 estimation
system derived from body movements enables the future possibilities of
personalized and real-time cardiac monitoring during daily life physical
activities.

</details>


### [326] [Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors](https://arxiv.org/pdf/2506.10146)
*Tejaswi Kasarla, Max van Spengler, Pascal Mettes*

Main category: cs.LG

TL;DR: Hierarchical hyperbolic embeddings improve out-of-distribution recognition, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing the discrimination between in- and out-of-distribution samples in deep learning.

Method: Introduces Balanced Hyperbolic Learning, optimizing hierarchical distortion and balance, and generalizes scoring functions for hyperbolic prototypes.

Result: Outperforms 13 datasets and 13 scoring functions, beating state-of-the-art methods.

Conclusion: Hierarchical hyperbolic embeddings are superior for out-of-distribution recognition and enable hierarchical generalization.

Abstract: Out-of-distribution recognition forms an important and well-studied problem
in deep learning, with the goal to filter out samples that do not belong to the
distribution on which a network has been trained. The conclusion of this paper
is simple: a good hierarchical hyperbolic embedding is preferred for
discriminating in- and out-of-distribution samples. We introduce Balanced
Hyperbolic Learning. We outline a hyperbolic class embedding algorithm that
jointly optimizes for hierarchical distortion and balancing between shallow and
wide subhierarchies. We then use the class embeddings as hyperbolic prototypes
for classification on in-distribution data. We outline how to generalize
existing out-of-distribution scoring functions to operate with hyperbolic
prototypes. Empirical evaluations across 13 datasets and 13 scoring functions
show that our hyperbolic embeddings outperform existing out-of-distribution
approaches when trained on the same data with the same backbones. We also show
that our hyperbolic embeddings outperform other hyperbolic approaches, beat
state-of-the-art contrastive methods, and natively enable hierarchical
out-of-distribution generalization.

</details>


### [327] [Probabilistic Variational Contrastive Learning](https://arxiv.org/pdf/2506.10159)
*Minoh Jeong, Seonho Kim, Alfred Hero*

Main category: cs.LG

TL;DR: VCL introduces a probabilistic framework for contrastive learning, replacing deterministic embeddings with probabilistic ones, improving uncertainty quantification and performance.


<details>
  <summary>Details</summary>
Motivation: Current contrastive learning methods lack uncertainty quantification, limiting their reliability.

Method: VCL maximizes ELBO by treating InfoNCE as a reconstruction term and adding KL divergence to a uniform prior. It uses a projected normal distribution for embeddings.

Result: VCL mitigates dimensional collapse, improves mutual information, matches baseline accuracy, and provides uncertainty estimates.

Conclusion: VCL provides a probabilistic foundation for contrastive learning, enhancing its utility.

Abstract: Deterministic embeddings learned by contrastive learning (CL) methods such as
SimCLR and SupCon achieve state-of-the-art performance but lack a principled
mechanism for uncertainty quantification. We propose Variational Contrastive
Learning (VCL), a decoder-free framework that maximizes the evidence lower
bound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction
term and adding a KL divergence regularizer to a uniform prior on the unit
hypersphere. We model the approximate posterior $q_\theta(z|x)$ as a projected
normal distribution, enabling the sampling of probabilistic embeddings. Our two
instantiations--VSimCLR and VSupCon--replace deterministic embeddings with
samples from $q_\theta(z|x)$ and incorporate a normalized KL term into the
loss. Experiments on multiple benchmarks demonstrate that VCL mitigates
dimensional collapse, enhances mutual information with class labels, and
matches or outperforms deterministic baselines in classification accuracy, all
the while providing meaningful uncertainty estimates through the posterior
model. VCL thus equips contrastive learning with a probabilistic foundation,
serving as a new basis for contrastive approaches.

</details>


### [328] [Wasserstein Barycenter Soft Actor-Critic](https://arxiv.org/pdf/2506.10167)
*Zahra Shahrooei, Ali Baheri*

Main category: cs.LG

TL;DR: WBSAC improves sample efficiency in sparse-reward continuous control tasks by combining pessimistic and optimistic policies via Wasserstein barycenter.


<details>
  <summary>Details</summary>
Motivation: Address poor sample efficiency in deep off-policy actor-critic algorithms, especially in sparse-reward environments.

Method: Proposes WBSAC, using Wasserstein barycenter of pessimistic (for TD learning) and optimistic (for exploration) policies, adjusting exploration dynamically.

Result: WBSAC outperforms state-of-the-art off-policy actor-critic algorithms in MuJoCo tasks.

Conclusion: WBSAC offers a principled exploration strategy, enhancing sample efficiency in continuous control.

Abstract: Deep off-policy actor-critic algorithms have emerged as the leading framework
for reinforcement learning in continuous control domains. However, most of
these algorithms suffer from poor sample efficiency, especially in environments
with sparse rewards. In this paper, we take a step towards addressing this
issue by providing a principled directed exploration strategy. We propose
Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from
a pessimistic actor for temporal difference learning and an optimistic actor to
promote exploration. This is achieved by using the Wasserstein barycenter of
the pessimistic and optimistic policies as the exploration policy and adjusting
the degree of exploration throughout the learning process. We compare WBSAC
with state-of-the-art off-policy actor-critic algorithms and show that WBSAC is
more sample-efficient on MuJoCo continuous control tasks.

</details>


### [329] [Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR](https://arxiv.org/pdf/2506.05683)
*Fardis Nadimi, Payam Abdisarabshali, Kasra Borazjani, Jacob Chakareski, Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: The paper proposes using multi-modal multi-task federated foundation models (FedFMs) to enhance XR systems, addressing challenges like sensor diversity and privacy through federated learning.


<details>
  <summary>Details</summary>
Motivation: To integrate the strengths of foundation models and federated learning for privacy-preserving, context-aware intelligence in XR systems.

Method: A modular architecture for FedFMs with coordination paradigms for training and aggregation, addressing XR challenges under SHIFT dimensions.

Result: Identification of SHIFT dimensions (e.g., sensor diversity, hardware constraints) and proposed metrics for resource-aware FedFMs in XR.

Conclusion: The paper lays foundations for next-gen XR systems with privacy-preserving, context-aware intelligence.

Abstract: Extended reality (XR) systems, which consist of virtual reality (VR),
augmented reality (AR), and mixed reality (XR), offer a transformative
interface for immersive, multi-modal, and embodied human-computer interaction.
In this paper, we envision that multi-modal multi-task (M3T) federated
foundation models (FedFMs) can offer transformative capabilities for XR systems
through integrating the representational strength of M3T foundation models
(FMs) with the privacy-preserving model training principles of federated
learning (FL). We present a modular architecture for FedFMs, which entails
different coordination paradigms for model training and aggregations. Central
to our vision is the codification of XR challenges that affect the
implementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality
diversity, (2) Hardware heterogeneity and system-level constraints, (3)
Interactivity and embodied personalization, (4) Functional/task variability,
and (5) Temporality and environmental variability. We illustrate the
manifestation of these dimensions across a set of emerging and anticipated
applications of XR systems. Finally, we propose evaluation metrics, dataset
requirements, and design tradeoffs necessary for the development of
resource-aware FedFMs in XR. This perspective aims to chart the technical and
conceptual foundations for context-aware privacy-preserving intelligence in the
next generation of XR systems.

</details>


### [330] [Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models](https://arxiv.org/pdf/2506.10177)
*Defang Chen, Zhenyu Zhou, Can Wang, Siwei Lyu*

Main category: cs.LG

TL;DR: The paper reveals a geometric regularity in diffusion-based generative models, showing sampling trajectories lie in low-dimensional subspaces with a consistent 'boomerang' shape. It proposes a dynamic programming-based scheme to optimize sampling time schedules, improving image generation efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to uncover and leverage the geometric regularity in sampling trajectories of diffusion-based generative models to enhance their efficiency and performance.

Method: The authors analyze sampling trajectories, characterize their properties, and propose a dynamic programming-based scheme to align sampling time schedules with trajectory structure.

Result: The discovered trajectory regularity allows for improved image generation performance, especially with fewer function evaluations (5-10).

Conclusion: The findings highlight the potential of exploiting geometric properties in sampling trajectories to optimize diffusion-based generative models with minimal computational overhead.

Abstract: Diffusion-based generative models employ stochastic differential equations
(SDEs) and their equivalent probability flow ordinary differential equations
(ODEs) to establish a smooth transformation between complex high-dimensional
data distributions and tractable prior distributions. In this paper, we reveal
a striking geometric regularity in the deterministic sampling dynamics: each
simulated sampling trajectory lies within an extremely low-dimensional
subspace, and all trajectories exhibit an almost identical ''boomerang'' shape,
regardless of the model architecture, applied conditions, or generated content.
We characterize several intriguing properties of these trajectories,
particularly under closed-form solutions based on kernel-estimated data
modeling. We also demonstrate a practical application of the discovered
trajectory regularity by proposing a dynamic programming-based scheme to better
align the sampling time schedule with the underlying trajectory structure. This
simple strategy requires minimal modification to existing ODE-based numerical
solvers, incurs negligible computational overhead, and achieves superior image
generation performance, especially in regions with only $5 \sim 10$ function
evaluations.

</details>


### [331] [A Comparative Study of Machine Learning Techniques for Early Prediction of Diabetes](https://arxiv.org/pdf/2506.10180)
*Mowafaq Salem Alzboon, Mohammad Al-Batah, Muhyeeddin Alqaraleh, Ahmad Abuashour, Ahmad Fuad Bader*

Main category: cs.LG

TL;DR: The study evaluates machine learning algorithms for diabetes prediction using the Pima Indians Diabetes dataset, finding Neural Networks and Random Forests most effective.


<details>
  <summary>Details</summary>
Motivation: Diabetes is a growing health concern, and early detection is vital. Machine learning offers promising solutions for prediction.

Method: Evaluated Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, SVM, Gradient Boosting, and Neural Network on the Pima Indians dataset.

Result: Neural Network achieved the highest accuracy (78.57%), followed by Random Forest (76.30%).

Conclusion: Machine learning algorithms, especially Neural Networks, can effectively predict diabetes, aiding early detection.

Abstract: In many nations, diabetes is becoming a significant health problem, and early
identification and control are crucial. Using machine learning algorithms to
predict diabetes has yielded encouraging results. Using the Pima Indians
Diabetes dataset, this study attempts to evaluate the efficacy of several
machine-learning methods for diabetes prediction. The collection includes
information on 768 patients, such as their ages, BMIs, and glucose levels. The
techniques assessed are Logistic Regression, Decision Tree, Random Forest,
k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting,
and Neural Network. The findings indicate that the Neural Network algorithm
performed the best, with an accuracy of 78.57 percent, followed by the Random
Forest method, with an accuracy of 76.30 percent. The study implies that
machine learning algorithms can aid diabetes prediction and be an efficient
early detection tool.

</details>


### [332] [Optimizing Genetic Algorithms with Multilayer Perceptron Networks for Enhancing TinyFace Recognition](https://arxiv.org/pdf/2506.10184)
*Mohammad Subhi Al-Batah, Mowafaq Salem Alzboon, Muhyeeddin Alqaraleh*

Main category: cs.LG

TL;DR: The study empirically evaluates MLP networks using three datasets, comparing baseline training, GA-based feature selection, and PCA-based dimensionality reduction. Results highlight GA's accuracy in complex datasets and PCA's benefits in simpler ones.


<details>
  <summary>Details</summary>
Motivation: To understand how feature selection and dimensionality reduction techniques impact MLP network performance across diverse datasets.

Method: Three methods were tested: baseline MLP training, GA-based feature selection, and PCA-based dimensionality reduction.

Result: GA improved accuracy in complex datasets, while PCA worked well in low-dimensional, noise-free datasets. Feature selection and dimensionality reduction interdependently enhance MLP performance.

Conclusion: The study provides practical guidelines for feature engineering and MLP optimization, contributing to machine learning literature.

Abstract: This study conducts an empirical examination of MLP networks investigated
through a rigorous methodical experimentation process involving three diverse
datasets: TinyFace, Heart Disease, and Iris. Study Overview: The study includes
three key methods: a) a baseline training using the default settings for the
Multi-Layer Perceptron (MLP), b) feature selection using Genetic Algorithm (GA)
based refinement c) Principal Component Analysis (PCA) based dimension
reduction. The results show important information on how such techniques affect
performance. While PCA had showed benefits in low-dimensional and noise-free
datasets GA consistently increased accuracy in complex datasets by accurately
identifying critical features. Comparison reveals that feature selection and
dimensionality reduction play interdependent roles in enhancing MLP
performance. The study contributes to the literature on feature engineering and
neural network parameter optimization, offering practical guidelines for a wide
range of machine learning tasks

</details>


### [333] [Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment](https://arxiv.org/pdf/2506.10186)
*Yuhui Ding, Thomas Hofmann*

Main category: cs.LG

TL;DR: Proposes a method to relax SE(3)-equivariance constraints in diffusion models for 3D molecule generation, achieving comparable quality to equivariant models with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Specialized equivariant architectures limit scalability and efficiency in diffusion models for 3D molecule generation.

Method: Learns sample-dependent SO(3) transformations to align molecules in a latent space, then trains a non-equivariant diffusion model on aligned representations.

Result: Outperforms non-equivariant models, matches state-of-the-art equivariant models in quality, and improves training/sampling efficiency.

Conclusion: The approach successfully balances performance and efficiency, offering a viable alternative to strict equivariant models.

Abstract: Equivariant diffusion models have achieved impressive performance in 3D
molecule generation. These models incorporate Euclidean symmetries of 3D
molecules by utilizing an SE(3)-equivariant denoising network. However,
specialized equivariant architectures limit the scalability and efficiency of
diffusion models. In this paper, we propose an approach that relaxes such
equivariance constraints. Specifically, our approach learns a sample-dependent
SO(3) transformation for each molecule to construct an aligned latent space. A
non-equivariant diffusion model is then trained over the aligned
representations. Experimental results demonstrate that our approach performs
significantly better than previously reported non-equivariant models. It yields
sample quality comparable to state-of-the-art equivariant diffusion models and
offers improved training and sampling efficiency. Our code is available at
https://github.com/skeletondyh/RADM

</details>


### [334] [Improving Oral Cancer Outcomes Through Machine Learning and Dimensionality Reduction](https://arxiv.org/pdf/2506.10189)
*Mohammad Subhi Al-Batah, Muhyeeddin Alqaraleh, Mowafaq Salem Alzboon*

Main category: cs.LG

TL;DR: The paper reviews advanced data mining techniques for oral cancer diagnosis, highlighting Neural Networks' superior accuracy (93.6%) and advocating for feature selection to improve outcomes.


<details>
  <summary>Details</summary>
Motivation: Early diagnosis and accurate prognosis of oral cancer are critical for improving patient survival rates, and machine learning offers advanced tools for this purpose.

Method: The study reviews methodologies like Neural Networks, KNN, SVM, and ensemble learning, comparing their performance in diagnosing oral cancer.

Result: Neural Networks outperform other models with 93.6% accuracy, and feature selection is recommended to enhance performance.

Conclusion: Advanced data mining techniques hold promise for improving early detection, treatment strategies, and patient outcomes in oral oncology.

Abstract: Oral cancer presents a formidable challenge in oncology, necessitating early
diagnosis and accurate prognosis to enhance patient survival rates. Recent
advancements in machine learning and data mining have revolutionized
traditional diagnostic methodologies, providing sophisticated and automated
tools for differentiating between benign and malignant oral lesions. This study
presents a comprehensive review of cutting-edge data mining methodologies,
including Neural Networks, K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), and ensemble learning techniques, specifically applied to the diagnosis
and prognosis of oral cancer. Through a rigorous comparative analysis, our
findings reveal that Neural Networks surpass other models, achieving an
impressive classification accuracy of 93,6 % in predicting oral cancer.
Furthermore, we underscore the potential benefits of integrating feature
selection and dimensionality reduction techniques to enhance model performance.
These insights underscore the significant promise of advanced data mining
techniques in bolstering early detection, optimizing treatment strategies, and
ultimately improving patient outcomes in the realm of oral oncology.

</details>


### [335] [DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection](https://arxiv.org/pdf/2506.10200)
*Tina Behrouzi, Sana Tonekaboni, Rahul G. Krishnan, Anna Goldenberg*

Main category: cs.LG

TL;DR: DynaSubVAE is a dynamic framework for adaptive OOD detection and representation learning, outperforming traditional methods in capturing heterogeneous subpopulations.


<details>
  <summary>Details</summary>
Motivation: Address the oversight of underrepresented subpopulations in models, which leads to inaccurate predictions, by dynamically adapting to emerging patterns.

Method: Uses a Dynamic Subgrouping Variational Autoencoder with non-parametric clustering (inspired by Gaussian Mixture Models) to update latent structures and detect OOD samples.

Result: Achieves competitive performance in near-OOD, far-OOD, and class-OOD detection, surpassing standalone clustering methods like GMM and KMeans++.

Conclusion: DynaSubVAE effectively captures evolving data trends and improves OOD detection accuracy, making it superior to conventional approaches.

Abstract: Real-world observational data often contain existing or emerging
heterogeneous subpopulations that deviate from global patterns. The majority of
models tend to overlook these underrepresented groups, leading to inaccurate or
even harmful predictions. Existing solutions often rely on detecting these
samples as Out-of-domain (OOD) rather than adapting the model to new emerging
patterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational
Autoencoder framework that jointly performs representation learning and
adaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with
the data by dynamically updating its latent structure to capture new trends. It
leverages a novel non-parametric clustering mechanism, inspired by Gaussian
Mixture Models, to discover and model latent subgroups based on embedding
similarity. Extensive experiments show that DynaSubVAE achieves competitive
performance in both near-OOD and far-OOD detection, and excels in class-OOD
scenarios where an entire class is missing during training. We further
illustrate that our dynamic subgrouping mechanism outperforms standalone
clustering methods such as GMM and KMeans++ in terms of both OOD accuracy and
regret precision.

</details>


### [336] [AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent](https://arxiv.org/pdf/2506.10205)
*Jing Liu, Toshiaki Koike-Akino, Ye Wang, Hassan Mansour, Matthew Brand*

Main category: cs.LG

TL;DR: A unified method (AWP) for activation-aware weight pruning and quantization in LLMs, outperforming state-of-the-art methods with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Addressing the size of LLMs for edge devices by improving model compression techniques like quantization and pruning.

Method: Proposes AWP, combining activation-aware weight pruning and quantization using projected gradient descent, inspired by Iterative Hard Thresholding.

Result: AWP outperforms existing LLM pruning and quantization methods in experiments.

Conclusion: AWP is effective for LLM compression, with theoretical support for pruning convergence.

Abstract: To address the enormous size of Large Language Models (LLMs), model
compression methods, such as quantization and pruning, are often deployed,
especially on edge devices. In this work, we focus on layer-wise post-training
quantization and pruning. Drawing connections between activation-aware weight
pruning and sparse approximation problems, and motivated by the success of
Iterative Hard Thresholding (IHT), we propose a unified method for
Activation-aware Weight pruning and quantization via Projected gradient descent
(AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM
pruning and quantization methods. Theoretical convergence guarantees of the
proposed method for pruning are also provided.

</details>


### [337] [Cross-Learning Between ECG and PCG: Exploring Common and Exclusive Characteristics of Bimodal Electromechanical Cardiac Waveforms](https://arxiv.org/pdf/2506.10212)
*Sajjad Karimi, Amit J. Shah, Gari D. Clifford, Reza Sameni*

Main category: cs.LG

TL;DR: The study explores the relationship between ECG and PCG signals, using machine learning to reconstruct one from the other, with non-causal LSTM models showing superior performance. Cross-subject and exercise scenarios pose challenges, but envelope-based modeling improves generalizability. Clinically relevant ECG biomarkers can also be estimated from PCG.


<details>
  <summary>Details</summary>
Motivation: To understand the distinct and overlapping information in ECG and PCG signals, their potential for mutual reconstruction, and biomarker extraction under varying physiological conditions and across individuals.

Method: Used the EPHNOGRAM dataset of simultaneous ECG-PCG recordings during rest and exercise. Employed linear and nonlinear machine learning models, including non-causal LSTM networks, for cross-modal reconstruction and analysis of causality, physiological state, and cross-subject variability.

Result: Nonlinear models, especially non-causal LSTM, performed best. ECG reconstruction from PCG was easier than the reverse. Exercise and cross-subject scenarios were challenging, but envelope-based modeling improved cross-subject generalizability. Clinically relevant ECG biomarkers could be estimated from PCG.

Conclusion: The findings enhance understanding of ECG-PCG relationships, with implications for multimodal cardiac monitoring technologies.

Abstract: Simultaneous electrocardiography (ECG) and phonocardiogram (PCG) provide a
comprehensive, multimodal perspective on cardiac function by capturing the
heart's electrical and mechanical activities, respectively. However, the
distinct and overlapping information content of these signals, as well as their
potential for mutual reconstruction and biomarker extraction, remains
incompletely understood, especially under varying physiological conditions and
across individuals.
  In this study, we systematically investigate the common and exclusive
characteristics of ECG and PCG using the EPHNOGRAM dataset of simultaneous
ECG-PCG recordings during rest and exercise. We employ a suite of linear and
nonlinear machine learning models, including non-causal LSTM networks, to
reconstruct each modality from the other and analyze the influence of
causality, physiological state, and cross-subject variability. Our results
demonstrate that nonlinear models, particularly non-causal LSTM, provide
superior reconstruction performance, with reconstructing ECG from PCG proving
more tractable than the reverse. Exercise and cross-subject scenarios present
significant challenges, but envelope-based modeling that utilizes instantaneous
amplitude features substantially improves cross-subject generalizability for
cross-modal learning. Furthermore, we demonstrate that clinically relevant ECG
biomarkers, such as fiducial points and QT intervals, can be estimated from PCG
in cross-subject settings.
  These findings advance our understanding of the relationship between
electromechanical cardiac modalities, in terms of both waveform characteristics
and the timing of cardiac events, with potential applications in novel
multimodal cardiac monitoring technologies.

</details>


### [338] [LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation](https://arxiv.org/pdf/2506.10235)
*Chen-Chia Chang, Wan-Hsuan Lin, Yikang Shen, Yiran Chen, Xin Zhang*

Main category: cs.LG

TL;DR: LaMAGIC2 introduces a succinct float-input canonical formulation (SFCI) for analog topology generation, improving efficiency and precision over prior methods.


<details>
  <summary>Details</summary>
Motivation: Automating analog topology design is essential due to manual efforts and inefficiencies in existing methods, like high token length and low numeric precision sensitivity.

Method: LaMAGIC2 uses SFCI, an identifier-based representation, to reduce token length complexity and enhance numeric precision sensitivity.

Result: LaMAGIC2 achieves 34% higher success rates under tight tolerances and 10X lower MSEs, with better transferability for larger circuits.

Conclusion: LaMAGIC2 is a robust framework for analog topology generation, addressing key limitations of prior approaches.

Abstract: Automation of analog topology design is crucial due to customized
requirements of modern applications with heavily manual engineering efforts.
The state-of-the-art work applies a sequence-to-sequence approach and
supervised finetuning on language models to generate topologies given user
specifications. However, its circuit formulation is inefficient due to O(|V |2)
token length and suffers from low precision sensitivity to numeric inputs. In
this work, we introduce LaMAGIC2, a succinct float-input canonical formulation
with identifier (SFCI) for language model-based analog topology generation.
SFCI addresses these challenges by improving component-type recognition through
identifier-based representations, reducing token length complexity to O(|V |),
and enhancing numeric precision sensitivity for better performance under tight
tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher
success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a
prior method. LaMAGIC2 also exhibits better transferability for circuits with
more vertices with up to 58.5% improvement. These advancements establish
LaMAGIC2 as a robust framework for analog topology generation.

</details>


### [339] [A new type of federated clustering: A non-model-sharing approach](https://arxiv.org/pdf/2506.10244)
*Yuji Kawamata, Kaoru Kamijo, Maki Kihira, Akihiro Toyoda, Tomoru Nakayama, Akira Imakura, Tetsuya Sakurai, Yukihiko Okada*

Main category: cs.LG

TL;DR: The paper introduces DC-Clustering, a federated clustering method for complex data partitioning, ensuring privacy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing federated clustering methods in handling complex data splits.

Method: Proposes DC-Clustering, sharing intermediate representations for privacy, supporting k-means and spectral clustering with one-round communication.

Result: Achieves performance comparable to centralized clustering on synthetic and benchmark datasets.

Conclusion: DC-Clustering fills a gap in FL research, offering privacy, efficiency, and flexibility for domains like healthcare and finance.

Abstract: In recent years, the growing need to leverage sensitive data across
institutions has led to increased attention on federated learning (FL), a
decentralized machine learning paradigm that enables model training without
sharing raw data. However, existing FL-based clustering methods, known as
federated clustering, typically assume simple data partitioning scenarios such
as horizontal or vertical splits, and cannot handle more complex distributed
structures. This study proposes data collaboration clustering (DC-Clustering),
a novel federated clustering method that supports clustering over complex data
partitioning scenarios where horizontal and vertical splits coexist. In
DC-Clustering, each institution shares only intermediate representations
instead of raw data, ensuring privacy preservation while enabling collaborative
clustering. The method allows flexible selection between k-means and spectral
clustering, and achieves final results with a single round of communication
with the central server. We conducted extensive experiments using synthetic and
open benchmark datasets. The results show that our method achieves clustering
performance comparable to centralized clustering where all data are pooled.
DC-Clustering addresses an important gap in current FL research by enabling
effective knowledge discovery from distributed heterogeneous data. Its
practical properties -- privacy preservation, communication efficiency, and
flexibility -- make it a promising tool for privacy-sensitive domains such as
healthcare and finance.

</details>


### [340] [Meta-learning Representations for Learning from Multiple Annotators](https://arxiv.org/pdf/2506.10259)
*Atsutoshi Kumagai, Tomoharu Iwata, Taishi Nishiyama, Yasutoshi Ida, Yasuhiro Fujiwara*

Main category: cs.LG

TL;DR: A meta-learning method for learning from noisy annotators by leveraging related tasks and a probabilistic model on a latent space, optimized via EM and backpropagation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning accurate classifiers from limited noisy annotated data, common in crowdsourcing.

Method: Uses a neural network to embed examples into a latent space, constructs a probabilistic model for task-specific classifiers, and meta-learns via EM and backpropagation.

Result: Demonstrated effectiveness on real-world datasets with synthetic noise and crowdsourcing data.

Conclusion: The method efficiently handles noisy labels and limited data by leveraging meta-learning and probabilistic modeling.

Abstract: We propose a meta-learning method for learning from multiple noisy
annotators. In many applications such as crowdsourcing services, labels for
supervised learning are given by multiple annotators. Since the annotators have
different skills or biases, given labels can be noisy. To learn accurate
classifiers, existing methods require many noisy annotated data. However,
sufficient data might be unavailable in practice. To overcome the lack of data,
the proposed method uses labeled data obtained in different but related tasks.
The proposed method embeds each example in tasks to a latent space by using a
neural network and constructs a probabilistic model for learning a
task-specific classifier while estimating annotators' abilities on the latent
space. This neural network is meta-learned to improve the expected test
classification performance when the classifier is adapted to a given small
amount of annotated data. This classifier adaptation is performed by maximizing
the posterior probability via the expectation-maximization (EM) algorithm.
Since each step in the EM algorithm is easily computed as a closed-form and is
differentiable, the proposed method can efficiently backpropagate the loss
through the EM algorithm to meta-learn the neural network. We show the
effectiveness of our method with real-world datasets with synthetic noise and
real-world crowdsourcing datasets.

</details>


### [341] [Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification](https://arxiv.org/pdf/2506.10269)
*Ryota Ueda, Takami Sato, Ken Kobayashi, Kazuhide Nakata*

Main category: cs.LG

TL;DR: SDP relaxation for neural network verification faces interior-point vanishing in deep networks, losing strict feasibility. Five solutions are proposed, improving success rates and revealing harmful traditional constraints.


<details>
  <summary>Details</summary>
Motivation: To address the loss of strict feasibility in SDP-based verification for deep neural networks, which hinders scalability and reliability.

Method: Theoretical and empirical analysis of interior-point vanishing, followed by designing five solutions to enhance feasibility conditions.

Result: Solutions solve 88% of previously unsolvable problems (41% of total), and reveal harmful traditional constraints.

Conclusion: Provides insights into SDP-based verification challenges and practical solutions for deeper networks, enhancing reliability and security.

Abstract: Semidefinite programming (SDP) relaxation has emerged as a promising approach
for neural network verification, offering tighter bounds than other convex
relaxation methods for deep neural networks (DNNs) with ReLU activations.
However, we identify a critical limitation in the SDP relaxation when applied
to deep networks: interior-point vanishing, which leads to the loss of strict
feasibility -- a crucial condition for the numerical stability and optimality
of SDP. Through rigorous theoretical and empirical analysis, we demonstrate
that as the depth of DNNs increases, the strict feasibility is likely to be
lost, creating a fundamental barrier to scaling SDP-based verification. To
address the interior-point vanishing, we design and investigate five solutions
to enhance the feasibility conditions of the verification problem. Our methods
can successfully solve 88% of the problems that could not be solved by existing
methods, accounting for 41% of the total. Our analysis also reveals that the
valid constraints for the lower and upper bounds for each ReLU unit are
traditionally inherited from prior work without solid reasons, but are actually
not only unbeneficial but also even harmful to the problem's feasibility. This
work provides valuable insights into the fundamental challenges of SDP-based
DNN verification and offers practical solutions to improve its applicability to
deeper neural networks, contributing to the development of more reliable and
secure systems with DNNs.

</details>


### [342] [Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning](https://arxiv.org/pdf/2506.10282)
*Jiajin Liu, Dongzhe Fan, Jiacheng Shen, Chuanhao Ji, Daochen Zha, Qiaoyu Tan*

Main category: cs.LG

TL;DR: The paper introduces Graph-MLLM, a benchmark for evaluating multimodal graph learning paradigms, highlighting the benefits of integrating visual and textual attributes and fine-tuning MLLMs.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal graph learning methods lack a unified benchmark, making it unclear which approaches perform best. The study aims to address this gap.

Method: Three paradigms (MLLM-as-Encoder, MLLM-as-Aligner, MLLM-as-Predictor) are evaluated across six datasets. Visual and textual attributes are integrated, and MLLMs are fine-tuned.

Result: Jointly using visual and textual attributes improves performance, and fine-tuning MLLMs achieves state-of-the-art results without explicit graph structure.

Conclusion: Graph-MLLM provides a fair evaluation framework and encourages further research in multimodal graph learning.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in representing and understanding diverse modalities. However,
they typically focus on modality alignment in a pairwise manner while
overlooking structural relationships across data points. Integrating
multimodality with structured graph information (i.e., multimodal graphs, MMGs)
is essential for real-world applications such as social networks, healthcare,
and recommendation systems. Existing MMG learning methods fall into three
paradigms based on how they leverage MLLMs: Encoder, Aligner, and Predictor.
MLLM-as-Encoder focuses on enhancing graph neural networks (GNNs) via
multimodal feature fusion; MLLM-as-Aligner aligns multimodal attributes in
language or hidden space to enable LLM-based graph reasoning; MLLM-as-Predictor
treats MLLMs as standalone reasoners with in-context learning or fine-tuning.
Despite their advances, the MMG field lacks a unified benchmark to fairly
evaluate across these approaches, making it unclear what progress has been
made. To bridge this gap, we present Graph-MLLM, a comprehensive benchmark for
multimodal graph learning by systematically evaluating these three paradigms
across six datasets with different domains. Through extensive experiments, we
observe that jointly considering the visual and textual attributes of the nodes
benefits graph learning, even when using pre-trained text-to-image alignment
models (e.g., CLIP) as encoders. We also find that converting visual attributes
into textual descriptions further improves performance compared to directly
using visual inputs. Moreover, we observe that fine-tuning MLLMs on specific
MMGs can achieve state-of-the-art results in most scenarios, even without
explicit graph structure information. We hope that our open-sourced library
will facilitate rapid, equitable evaluation and inspire further innovative
research in this field.

</details>


### [343] [Collaborative Min-Max Regret in Grouped Multi-Armed Bandits](https://arxiv.org/pdf/2506.10313)
*Moïse Blanchard, Vineet Goyal*

Main category: cs.LG

TL;DR: The paper introduces Col-UCB, an algorithm for minimizing collaborative regret in grouped multi-armed bandits by dynamically coordinating exploration across groups with overlapping action sets.


<details>
  <summary>Details</summary>
Motivation: Standard algorithms in multi-armed bandits can lead to imbalanced exploration costs between groups. The study aims to balance this burden by sharing reward observations and minimizing collaborative regret.

Method: The authors propose Col-UCB, an algorithm that dynamically coordinates exploration across groups with overlapping action sets.

Result: Col-UCB achieves optimal minimax and instance-dependent collaborative regret up to logarithmic factors, adapting to the structure of shared action sets.

Conclusion: Collaboration in grouped bandits can significantly benefit groups when action sets overlap, as demonstrated by Col-UCB's performance.

Abstract: We study the impact of sharing exploration in multi-armed bandits in a
grouped setting where a set of groups have overlapping feasible action sets
[Baek and Farias '24]. In this grouped bandit setting, groups share reward
observations, and the objective is to minimize the collaborative regret,
defined as the maximum regret across groups. This naturally captures
applications in which one aims to balance the exploration burden between groups
or populations -- it is known that standard algorithms can lead to
significantly imbalanced exploration cost between groups. We address this
problem by introducing an algorithm Col-UCB that dynamically coordinates
exploration across groups. We show that Col-UCB achieves both optimal minimax
and instance-dependent collaborative regret up to logarithmic factors. These
bounds are adaptive to the structure of shared action sets between groups,
providing insights into when collaboration yields significant benefits over
each group learning their best action independently.

</details>


### [344] [Detecting Sockpuppetry on Wikipedia Using Meta-Learning](https://arxiv.org/pdf/2506.10314)
*Luc Raszewski, Christine De Kock*

Main category: cs.LG

TL;DR: Meta-learning improves sockpuppet detection on Wikipedia by adapting to author-specific behaviors, outperforming pre-trained models in precision.


<details>
  <summary>Details</summary>
Motivation: To combat disinformation by detecting sockpuppets more effectively, especially with limited text data.

Method: Applied meta-learning to train models across multiple tasks for rapid adaptation to new sockpuppet-group behaviors.

Result: Meta-learning significantly enhances prediction precision compared to pre-trained models.

Conclusion: Meta-learning advances sockpuppetry detection, with a new dataset released for future research.

Abstract: Malicious sockpuppet detection on Wikipedia is critical to preserving access
to reliable information on the internet and preventing the spread of
disinformation. Prior machine learning approaches rely on stylistic and
meta-data features, but do not prioritise adaptability to author-specific
behaviours. As a result, they struggle to effectively model the behaviour of
specific sockpuppet-groups, especially when text data is limited. To address
this, we propose the application of meta-learning, a machine learning technique
designed to improve performance in data-scarce settings by training models
across multiple tasks. Meta-learning optimises a model for rapid adaptation to
the writing style of a new sockpuppet-group. Our results show that
meta-learning significantly enhances the precision of predictions compared to
pre-trained models, marking an advancement in combating sockpuppetry on open
editing platforms. We release a new dataset of sockpuppet investigations to
foster future research in both sockpuppetry and meta-learning fields.

</details>


### [345] [PyLO: Towards Accessible Learned Optimizers in PyTorch](https://arxiv.org/pdf/2506.10315)
*Paul Janson, Benjamin Therien, Quentin Anthony, Xiaolong Huang, Abhinav Moudgil, Eugene Belilovsky*

Main category: cs.LG

TL;DR: PyLO is a PyTorch-based library designed to make learned optimizers accessible, offering speedups and integration with existing tools.


<details>
  <summary>Details</summary>
Motivation: Address the inaccessibility of learned optimizers like VeLO due to reliance on JAX and lack of user-friendly packages.

Method: Introduces PyLO, a CUDA-accelerated library for applying learned optimizers, tested on real-world large-scale tasks.

Result: Achieves significant speedups (39.36 to 205.59 samples/sec) and benefits from combining learned optimizers with existing tools.

Conclusion: PyLO successfully bridges the gap, making learned optimizers practical and accessible for broader use.

Abstract: Learned optimizers have been an active research topic over the past decade,
with increasing progress toward practical, general-purpose optimizers that can
serve as drop-in replacements for widely used methods like Adam. However,
recent advances -- such as VeLO, which was meta-trained for 4000 TPU-months --
remain largely inaccessible to the broader community, in part due to their
reliance on JAX and the absence of user-friendly packages for applying the
optimizers after meta-training. To address this gap, we introduce PyLO, a
PyTorch-based library that brings learned optimizers to the broader machine
learning community through familiar, widely adopted workflows. Unlike prior
work focused on synthetic or convex tasks, our emphasis is on applying learned
optimization to real-world large-scale pre-training tasks. Our release includes
a CUDA-accelerated version of the small_fc_lopt learned optimizer architecture
from (Metz et al., 2022a), delivering substantial speedups -- from 39.36 to
205.59 samples/sec throughput for training ViT B/16 with batch size 32. PyLO
also allows us to easily combine learned optimizers with existing optimization
tools such as learning rate schedules and weight decay. When doing so, we find
that learned optimizers can substantially benefit. Our code is available at
https://github.com/Belilovsky-Lab/pylo

</details>


### [346] [Air in Your Neighborhood: Fine-Grained AQI Forecasting Using Mobile Sensor Data](https://arxiv.org/pdf/2506.10332)
*Aaryam Sharma*

Main category: cs.LG

TL;DR: The paper predicts AQI in 1 km² neighborhoods using Spatio-temporal GNNs, outperforming existing methods by 71.654 MSE (79% reduction), and reveals new insights about AQI patterns.


<details>
  <summary>Details</summary>
Motivation: Existing AQI data from sparse sensors fails to reflect local reality, necessitating better prediction methods.

Method: Uses Spatio-temporal Graph Neural Networks (GNNs) on the AirDelhi dataset to predict AQI at high resolution.

Result: Achieves a 79% reduction in MSE (71.654) compared to existing works, even on unseen coordinates. Discovers strong repetitive short-term AQI patterns and changing spatial relations.

Conclusion: The approach effectively addresses the gap in local AQI prediction and provides new insights, with code available for reproducibility.

Abstract: Air pollution has become a significant health risk in developing countries.
While governments routinely publish air-quality index (AQI) data to track
pollution, these values fail to capture the local reality, as sensors are often
very sparse. In this paper, we address this gap by predicting AQI in 1 km^2
neighborhoods, using the example of AirDelhi dataset. Using Spatio-temporal
GNNs we surpass existing works by 71.654 MSE a 79% reduction, even on unseen
coordinates. New insights about AQI such as the existence of strong repetitive
short-term patterns and changing spatial relations are also discovered. The
code is available on GitHub.

</details>


### [347] [Provably Learning from Language Feedback](https://arxiv.org/pdf/2506.10341)
*Wanqiao Xu, Allen Nie, Ruijie Zheng, Aditya Modi, Adith Swaminathan, Ching-An Cheng*

Main category: cs.LG

TL;DR: The paper formalizes Learning from Language Feedback (LLF), introduces a complexity measure (transfer eluder dimension), and presents a no-regret algorithm (HELiX) for solving LLF problems with performance guarantees.


<details>
  <summary>Details</summary>
Motivation: To provide a principled framework for learning from language feedback, addressing the lack of formalization in this emerging area driven by LLM agents.

Method: The authors formalize LLF, introduce transfer eluder dimension as a complexity measure, and develop the HELiX algorithm for no-regret learning.

Result: The transfer eluder dimension captures feedback's impact on learning complexity, and HELiX performs well empirically, even when LLMs fail.

Conclusion: This work lays the foundation for principled interactive learning from language feedback, demonstrating its potential advantages over reward-based learning.

Abstract: Interactively learning from observation and language feedback is an
increasingly studied area driven by the emergence of large language model (LLM)
agents. While impressive empirical demonstrations have been shown, so far a
principled framing of these decision problems remains lacking. In this paper,
we formalize the Learning from Language Feedback (LLF) problem, assert
sufficient assumptions to enable learning despite latent rewards, and introduce
$\textit{transfer eluder dimension}$ as a complexity measure to characterize
the hardness of LLF problems. We show that transfer eluder dimension captures
the intuition that information in the feedback changes the learning complexity
of the LLF problem. We demonstrate cases where learning from rich language
feedback can be exponentially faster than learning from reward. We develop a
no-regret algorithm, called $\texttt{HELiX}$, that provably solves LLF problems
through sequential interactions, with performance guarantees that scale with
the transfer eluder dimension of the problem. Across several empirical domains,
we show that $\texttt{HELiX}$ performs well even when repeatedly prompting LLMs
does not work reliably. Our contributions mark a first step towards designing
principled interactive learning algorithms from generic language feedback.

</details>


### [348] [PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation](https://arxiv.org/pdf/2506.10351)
*Yanlong Chen, Mattia Orlandi, Pierangelo Maria Rapa, Simone Benatti, Luca Benini, Yawei Li*

Main category: cs.LG

TL;DR: A wavelet-based approach improves physiological signal analysis by addressing noise and non-stationarity, introducing pretrained EMG/ECG models and a multi-modal EEG framework for superior performance.


<details>
  <summary>Details</summary>
Motivation: Challenges like motion artifacts, low SNR, and non-stationarity in physiological signals require advanced analysis methods beyond traditional techniques.

Method: A novel wavelet-based technique captures multi-scale time-frequency features, with pretrained EMG/ECG models and a unified multi-modal framework integrating EEG.

Result: The approach outperforms existing methods, handling low SNR, inter-subject variability, and device mismatch effectively.

Conclusion: The wavelet-based architecture and multi-modal design advance physiological signal processing, with applications in health monitoring and diagnostics.

Abstract: Physiological signals are often corrupted by motion artifacts, baseline
drift, and other low-SNR disturbances, which pose significant challenges for
analysis. Additionally, these signals exhibit strong non-stationarity, with
sharp peaks and abrupt changes that evolve continuously, making them difficult
to represent using traditional time-domain or filtering methods. To address
these issues, a novel wavelet-based approach for physiological signal analysis
is presented, aiming to capture multi-scale time-frequency features in various
physiological signals. Leveraging this technique, two large-scale pretrained
models specific to EMG and ECG are introduced for the first time, achieving
superior performance and setting new baselines in downstream tasks.
Additionally, a unified multi-modal framework is constructed by integrating
pretrained EEG model, where each modality is guided through its dedicated
branch and fused via learnable weighted fusion. This design effectively
addresses challenges such as low signal-to-noise ratio, high inter-subject
variability, and device mismatch, outperforming existing methods on multi-modal
tasks. The proposed wavelet-based architecture lays a solid foundation for
analysis of diverse physiological signals, while the multi-modal design points
to next-generation physiological signal processing with potential impact on
wearable health monitoring, clinical diagnostics, and broader biomedical
applications.

</details>


### [349] [History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling of Path-Dependent Materials](https://arxiv.org/pdf/2506.10352)
*Binyao Guo, Zihan Lin, QiZhi He*

Main category: cs.LG

TL;DR: The paper introduces HANO, a neural operator for modeling path-dependent inelastic materials, addressing self-consistency and hidden state issues in RNNs, and demonstrating superior performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of RNN-based models in predicting path-dependent material responses by avoiding hidden state variables and ensuring self-consistency.

Method: Develops HANO, an autoregressive model with a Fourier-based neural operator and hierarchical self-attention for multiscale feature extraction.

Result: HANO outperforms baselines in accuracy, generalization, and robustness, handling irregular sampling, noise, and complex loading paths.

Conclusion: HANO is an effective data-driven surrogate for inelastic material simulation, suitable for integration with classical solvers.

Abstract: This study presents an end-to-end learning framework for data-driven modeling
of path-dependent inelastic materials using neural operators. The framework is
built on the premise that irreversible evolution of material responses,
governed by hidden dynamics, can be inferred from observable data.
  We develop the History-Aware Neural Operator (HANO), an autoregressive model
that predicts path-dependent material responses from short segments of recent
strain-stress history without relying on hidden state variables, thereby
overcoming self-consistency issues commonly encountered in recurrent neural
network (RNN)-based models. Built on a Fourier-based neural operator backbone,
HANO enables discretization-invariant learning. To enhance its ability to
capture both global loading patterns and critical local path dependencies, we
embed a hierarchical self-attention mechanism that facilitates multiscale
feature extraction.
  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial
hidden states, a commonly overlooked issue that can lead to instability in
recurrent models when applied to generalized loading paths. By modeling
stress-strain evolution as a continuous operator rather than relying on fixed
input-output mappings, HANO naturally accommodates varying path discretizations
and exhibits robust performance under complex conditions, including irregular
sampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate
HANO on two benchmark problems: elastoplasticity with hardening and progressive
anisotropic damage in brittle solids. Results show that HANO consistently
outperforms baseline models in predictive accuracy, generalization, and
robustness. With its demonstrated capabilities, HANO provides an effective
data-driven surrogate for simulating inelastic materials and is well-suited for
integration with classical numerical solvers.

</details>


### [350] [TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree](https://arxiv.org/pdf/2506.10355)
*Yu-Yang Qian, Yuan-Ze Xu, Zhen-Yu Zhang, Peng Zhao, Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: TreeLoRA introduces a hierarchical gradient similarity-based method for efficient continual learning in large pre-trained models, using bandit techniques and sparse updates.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency of continual learning in large pre-trained models by reducing task similarity estimation and parameter optimization burdens.

Method: Constructs layer-wise adapters using hierarchical gradient similarity, employs bandit techniques for task structure exploration, and uses sparse gradient updates.

Result: Demonstrates effectiveness and efficiency on vision transformers and large language models across vision and NLP tasks.

Conclusion: TreeLoRA provides a scalable and efficient solution for continual learning in large pre-trained models.

Abstract: Many real-world applications collect data in a streaming environment, where
learning tasks are encountered sequentially. This necessitates continual
learning (CL) to update models online, enabling adaptation to new tasks while
preserving past knowledge to prevent catastrophic forgetting. Nowadays, with
the flourish of large pre-trained models (LPMs), efficiency has become
increasingly critical for CL, due to their substantial computational demands
and growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of
Low-Rank Adapters), a novel approach that constructs layer-wise adapters by
leveraging hierarchical gradient similarity to enable efficient CL,
particularly for LPMs. To reduce the computational burden of task similarity
estimation, we employ bandit techniques to develop an algorithm based on lower
confidence bounds to efficiently explore the task structure. Furthermore, we
use sparse gradient updates to facilitate parameter optimization, making the
approach better suited for LPMs. Theoretical analysis is provided to justify
the rationale behind our approach, and experiments on both vision transformers
(ViTs) and large language models (LLMs) demonstrate the effectiveness and
efficiency of our approach across various domains, including vision and natural
language processing tasks.

</details>


### [351] [Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/pdf/2506.10364)
*Penguin Huang, Chhavi Yadav, Ruihan Wu, Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: The paper introduces PropInfer, a benchmark for evaluating property inference attacks on LLMs, demonstrating vulnerabilities in fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: To assess if property inference attacks, previously studied in other models, apply to LLMs, especially in sensitive domains.

Method: Developed PropInfer benchmark with two fine-tuning paradigms (question-answering and chat-completion) and proposed two attacks: prompt-based generation and shadow-model attack.

Result: Empirical evaluations confirmed the success of the attacks, revealing vulnerabilities in LLMs.

Conclusion: LLMs are susceptible to property inference attacks, highlighting a new security concern for fine-tuned models.

Abstract: Large language models (LLMs) are increasingly fine-tuned on domain-specific
datasets to support applications in fields such as healthcare, finance, and
law. These fine-tuning datasets often have sensitive and confidential
dataset-level properties -- such as patient demographics or disease prevalence
-- that are not intended to be revealed. While prior work has studied property
inference attacks on discriminative models (e.g., image classification models)
and generative models (e.g., GANs for image data), it remains unclear if such
attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark
task for evaluating property inference in LLMs under two fine-tuning paradigms:
question-answering and chat-completion. Built on the ChatDoctor dataset, our
benchmark includes a range of property types and task configurations. We
further propose two tailored attacks: a prompt-based generation attack and a
shadow-model attack leveraging word frequency signals. Empirical evaluations
across multiple pretrained LLMs show the success of our attacks, revealing a
previously unrecognized vulnerability in LLMs.

</details>


### [352] [Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning](https://arxiv.org/pdf/2506.10378)
*Jikai Jin, Vasilis Syrgkanis, Sham Kakade, Hanlin Zhang*

Main category: cs.LG

TL;DR: A causal representation learning framework is proposed to evaluate language model capabilities by modeling benchmark performance as linear transformations of latent factors, revealing a three-node causal structure.


<details>
  <summary>Details</summary>
Motivation: To address methodological challenges like confounding effects and computational costs in evaluating language model capabilities.

Method: A causal representation learning framework where benchmark performance is modeled as a linear transformation of latent capability factors, controlling for base model variations.

Result: Identified a three-node linear causal structure explaining performance variations, revealing a causal direction from problem-solving to instruction-following to mathematical reasoning.

Conclusion: The study highlights the importance of controlling base model variations to uncover causal relationships among latent model capabilities.

Abstract: Faithful evaluation of language model capabilities is crucial for deriving
actionable insights that can inform model development. However, rigorous causal
evaluations in this domain face significant methodological challenges,
including complex confounding effects and prohibitive computational costs
associated with extensive retraining. To tackle these challenges, we propose a
causal representation learning framework wherein observed benchmark performance
is modeled as a linear transformation of a few latent capability factors.
Crucially, these latent factors are identified as causally interrelated after
appropriately controlling for the base model as a common confounder. Applying
this approach to a comprehensive dataset encompassing over 1500 models
evaluated across six benchmarks from the Open LLM Leaderboard, we identify a
concise three-node linear causal structure that reliably explains the observed
performance variations. Further interpretation of this causal structure
provides substantial scientific insights beyond simple numerical rankings:
specifically, we reveal a clear causal direction starting from general
problem-solving capabilities, advancing through instruction-following
proficiency, and culminating in mathematical reasoning ability. Our results
underscore the essential role of carefully controlling base model variations
during evaluation, a step critical to accurately uncovering the underlying
causal relationships among latent model capabilities.

</details>


### [353] [EQA-RM: A Generative Embodied Reward Model with Test-time Scaling](https://arxiv.org/pdf/2506.10389)
*Yuhang Chen, Zhen Tan, Tianlong Chen*

Main category: cs.LG

TL;DR: EQA-RM is a generative multimodal reward model for Embodied Question Answering (EQA), trained with C-GRPO for fine-grained behavioral distinctions. It outperforms proprietary and open-source models on EQARewardBench.


<details>
  <summary>Details</summary>
Motivation: Existing reward models lack nuanced evaluation for complex embodied tasks like EQA, which require spatial, temporal, and logical understanding.

Method: EQA-RM uses Contrastive Group Relative Policy Optimization (C-GRPO) for training and provides interpretable, structured rewards.

Result: EQA-RM achieves 61.9% accuracy on EQARewardBench with only 700 samples, surpassing models like Gemini-2.5-Flash and GPT-4o.

Conclusion: EQA-RM offers a scalable, interpretable solution for EQA, with superior performance and efficiency.

Abstract: Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.

</details>


### [354] [Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation](https://arxiv.org/pdf/2506.10403)
*Tzu-Heng Huang, Harit Vishwakarma, Frederic Sala*

Main category: cs.LG

TL;DR: PAJAMA introduces program-based judges to evaluate LLM outputs, reducing costs and biases while improving reliability and flexibility.


<details>
  <summary>Details</summary>
Motivation: Addressing high costs, reliability issues, inflexibility, and biases in using LLMs to evaluate LLM outputs.

Method: Synthesizes executable judging programs from LLMs, stored and run locally for cost efficiency and adaptability.

Result: Improves judgment consistency by 15.83%, reduces biased responses by 23.7%, and outperforms LLM-as-a-judge on benchmarks at lower cost.

Conclusion: PAJAMA offers a scalable, cost-effective, and less biased alternative to LLM-based evaluation.

Abstract: Large language models (LLMs) are widely used to evaluate the quality of LLM
generations and responses, but this leads to significant challenges: high API
costs, uncertain reliability, inflexible pipelines, and inherent biases. To
address these, we introduce PAJAMA (Program-As-a-Judge for Automated Model
Assessment), a new alternative that uses LLMs to synthesize executable judging
programs instead of directly scoring responses. These synthesized programs can
be stored and run locally, costing orders of magnitude less while providing
interpretable, and auditable judging logic that can be easily adapted.
Program-based judges mitigate biases, improving judgment consistency by 15.83%
and reducing biased responses by 23.7% on average compared to a
Qwen2.5-14B-based LLM-as-a-judge. When program judgments are distilled into a
model, PAJAMA outperforms LLM-as-a-judge on the challenging CHAT-HARD subset of
RewardBench, outperforming metrics by 2.19% on Prometheus and 8.67% on the
JudgeLM dataset, all at three orders of magnitude lower cost.

</details>


### [355] [Generative Algorithms for Wildfire Progression Reconstruction from Multi-Modal Satellite Active Fire Measurements and Terrain Height](https://arxiv.org/pdf/2506.10404)
*Bryan Shaddy, Brianna Binder, Agnimitra Dasgupta, Haitong Qin, James Haley, Angel Farguell, Kyle Hilburn, Derek V. Mallia, Adam Kochanski, Jan Mandel, Assad Oberai*

Main category: cs.LG

TL;DR: The paper proposes a method using a conditional GAN to estimate wildfire progression from satellite data, terrain, and simulations, achieving accurate predictions validated on real wildfires.


<details>
  <summary>Details</summary>
Motivation: Increasing wildfires and inaccuracies in multi-day simulations drive the need for data assimilation to improve wildfire spread prediction.

Method: A conditional GAN is trained on WRF-SFIRE simulations to estimate fire arrival times from VIIRS and GOES data, incorporating terrain.

Result: Validation on five wildfires shows high accuracy (average Sorensen-Dice coefficient of 0.81), with terrain having minimal impact when satellite data is used.

Conclusion: The approach effectively integrates measurements and simulations for accurate wildfire progression prediction, with terrain playing a minor role when satellite data is available.

Abstract: Increasing wildfire occurrence has spurred growing interest in wildfire
spread prediction. However, even the most complex wildfire models diverge from
observed progression during multi-day simulations, motivating need for data
assimilation. A useful approach to assimilating measurement data into complex
coupled atmosphere-wildfire models is to estimate wildfire progression from
measurements and use this progression to develop a matching atmospheric state.
In this study, an approach is developed for estimating fire progression from
VIIRS active fire measurements, GOES-derived ignition times, and terrain height
data. A conditional Generative Adversarial Network is trained with simulations
of historic wildfires from the atmosphere-wildfire model WRF-SFIRE, thus
allowing incorporation of WRF-SFIRE physics into estimates. Fire progression is
succinctly represented by fire arrival time, and measurements for training are
obtained by applying an approximate observation operator to WRF-SFIRE
solutions, eliminating need for satellite data during training. The model is
trained on tuples of fire arrival times, measurements, and terrain, and once
trained leverages measurements of real fires and corresponding terrain data to
generate samples of fire arrival times. The approach is validated on five
Pacific US wildfires, with results compared against high-resolution perimeters
measured via aircraft, finding an average Sorensen-Dice coefficient of 0.81.
The influence of terrain height on the arrival time inference is also evaluated
and it is observed that terrain has minimal influence when the inference is
conditioned on satellite measurements.

</details>


### [356] [Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series](https://arxiv.org/pdf/2506.10412)
*Ching Chang, Jeehyun Hwang, Yidan Shi, Haixin Wang, Wen-Chih Peng, Tien-Fu Chen, Wei Wang*

Main category: cs.LG

TL;DR: Time-IMM is a dataset for irregular, multimodal time series, and IMM-TSF is a benchmark library for forecasting on such data, showing improved performance with explicit multimodality modeling.


<details>
  <summary>Details</summary>
Motivation: Real-world time series data are often irregular and multimodal, but existing benchmarks assume clean, unimodal data. This gap hinders practical deployment.

Method: Time-IMM captures nine types of irregularity, and IMM-TSF provides fusion modules for asynchronous integration and evaluation.

Result: Explicit multimodality modeling on irregular data significantly improves forecasting performance.

Conclusion: Time-IMM and IMM-TSF advance time series analysis under real-world conditions, with publicly available resources.

Abstract: Time series data in real-world applications such as healthcare, climate
modeling, and finance are often irregular, multimodal, and messy, with varying
sampling rates, asynchronous modalities, and pervasive missingness. However,
existing benchmarks typically assume clean, regularly sampled, unimodal data,
creating a significant gap between research and real-world deployment. We
introduce Time-IMM, a dataset specifically designed to capture cause-driven
irregularity in multimodal multivariate time series. Time-IMM represents nine
distinct types of time series irregularity, categorized into trigger-based,
constraint-based, and artifact-based mechanisms. Complementing the dataset, we
introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal
time series, enabling asynchronous integration and realistic evaluation.
IMM-TSF includes specialized fusion modules, including a timestamp-to-text
fusion module and a multimodality fusion module, which support both
recency-aware averaging and attention-based integration strategies. Empirical
results demonstrate that explicitly modeling multimodality on irregular time
series data leads to substantial gains in forecasting performance. Time-IMM and
IMM-TSF provide a foundation for advancing time series analysis under
real-world conditions. The dataset is publicly available at
https://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the
benchmark library can be accessed at
https://anonymous.4open.science/r/IMMTSF_NeurIPS2025.

</details>


### [357] [Data-Driven Soil Organic Carbon Sampling: Integrating Spectral Clustering with Conditioned Latin Hypercube Optimization](https://arxiv.org/pdf/2506.10419)
*Weiying Zhao, Aleksei Unagaev, Natalia Efremova*

Main category: cs.LG

TL;DR: A hybrid method combining spectral clustering and cLHS improves SOC sampling representativeness by ensuring coverage of minor environmental clusters.


<details>
  <summary>Details</summary>
Motivation: Standard cLHS may overlook minor but important environmental clusters in SOC monitoring.

Method: Integrates spectral clustering to partition the area into homogeneous zones, then applies cLHS within each zone for diverse sampling.

Result: Spectral-cLHS provides more uniform covariate coverage and spatial heterogeneity than standard cLHS.

Conclusion: The hybrid method enhances SOC prediction accuracy by improving training data balance for machine learning models.

Abstract: Soil organic carbon (SOC) monitoring often relies on selecting representative
field sampling locations based on environmental covariates. We propose a novel
hybrid methodology that integrates spectral clustering - an unsupervised
machine learning technique with conditioned Latin hypercube sampling (cLHS) to
enhance the representativeness of SOC sampling. In our approach, spectral
clustering partitions the study area into $K$ homogeneous zones using
multivariate covariate data, and cLHS is then applied within each zone to
select sampling locations that collectively capture the full diversity of
environmental conditions. This hybrid spectral-cLHS method ensures that even
minor but important environmental clusters are sampled, addressing a key
limitation of vanilla cLHS which can overlook such areas. We demonstrate on a
real SOC mapping dataset that spectral-cLHS provides more uniform coverage of
covariate feature space and spatial heterogeneity than standard cLHS. This
improved sampling design has the potential to yield more accurate SOC
predictions by providing better-balanced training data for machine learning
models.

</details>


### [358] [System Identification Using Kolmogorov-Arnold Networks: A Case Study on Buck Converters](https://arxiv.org/pdf/2506.10434)
*Nart Gashi, Panagiotis Kakosimos, George Papafotiou*

Main category: cs.LG

TL;DR: Kolmogorov-Arnold Networks (KANs) offer interpretable and efficient system identification for dynamic systems, demonstrated by modeling a buck converter system.


<details>
  <summary>Details</summary>
Motivation: To improve scalability, accuracy, and interpretability in system identification compared to traditional neural networks.

Method: Approximating state derivatives with KANs, constructing state-space representations, and validating models using simulation data.

Result: KANs accurately identify system dynamics, verify model consistency, and detect parameter changes.

Conclusion: KANs are effective for system identification in industrial systems, offering interpretability and accuracy.

Abstract: Kolmogorov-Arnold Networks (KANs) are emerging as a powerful framework for
interpretable and efficient system identification in dynamic systems. By
leveraging the Kolmogorov-Arnold representation theorem, KANs enable function
approximation through learnable activation functions, offering improved
scalability, accuracy, and interpretability compared to traditional neural
networks. This paper investigates the application of KANs to model and analyze
the dynamics of a buck converter system, focusing on state-space parameter
estimation along with discovering the system equations. Using simulation data,
the methodology involves approximating state derivatives with KANs,
constructing interpretable state-space representations, and validating these
models through numerical experiments. The results demonstrate the ability of
KANs to accurately identify system dynamics, verify model consistency, and
detect parameter changes, providing valuable insights into their applicability
for system identification in modern industrial systems.

</details>


### [359] [MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices](https://arxiv.org/pdf/2506.10443)
*Zhaode Wang, Jingbang Yang, Xinyu Qian, Shiwen Xing, Xiaotang Jiang, Chengfei Lv, Shengyu Zhang*

Main category: cs.LG

TL;DR: MNN-LLM is a framework for deploying large language models on mobile devices, addressing memory and speed challenges through quantization, hybrid storage, and optimization techniques, achieving up to 8.6x speedup.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of LLMs on edge devices due to memory and speed limitations motivates the need for efficient deployment solutions.

Method: MNN-LLM uses model quantization, DRAM-Flash hybrid storage, weight and input rearrangement, multicore load balancing, mixed-precision operations, and geometric computations.

Result: MNN-LLM achieves up to 8.6x speed increase compared to mainstream LLM frameworks.

Conclusion: MNN-LLM effectively optimizes LLM deployment on mobile devices, significantly improving performance and reducing resource usage.

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
a variety of tasks. However, their substantial scale leads to significant
computational resource consumption during inference, resulting in high costs.
Consequently, edge device inference presents a promising solution. The primary
challenges of edge inference include memory usage and inference speed. This
paper introduces MNN-LLM, a framework specifically designed to accelerate the
deployment of large language models on mobile devices. MNN-LLM addresses the
runtime characteristics of LLMs through model quantization and DRAM-Flash
hybrid storage, effectively reducing memory usage. It rearranges weights and
inputs based on mobile CPU instruction sets and GPU characteristics while
employing strategies such as multicore load balancing, mixed-precision
floating-point operations, and geometric computations to enhance performance.
Notably, MNN-LLM achieves up to a 8.6x speed increase compared to current
mainstream LLM-specific frameworks.

</details>


### [360] [Equivariant Neural Diffusion for Molecule Generation](https://arxiv.org/pdf/2506.10532)
*François Cornet, Grigory Bartosh, Mikkel N. Schmidt, Christian A. Naesseth*

Main category: cs.LG

TL;DR: Equivariant Neural Diffusion (END) is a new diffusion model for 3D molecule generation, featuring a learnable forward process for better performance.


<details>
  <summary>Details</summary>
Motivation: To improve generative modeling in 3D molecule generation by ensuring equivariance to Euclidean transformations.

Method: Uses a learnable, time- and data-dependent forward process that is equivariant to rigid transformations.

Result: Competitive performance on standard benchmarks for both unconditional and conditional generation.

Conclusion: END offers a promising approach for equivariant diffusion models in molecule generation.

Abstract: We introduce Equivariant Neural Diffusion (END), a novel diffusion model for
molecule generation in 3D that is equivariant to Euclidean transformations.
Compared to current state-of-the-art equivariant diffusion models, the key
innovation in END lies in its learnable forward process for enhanced generative
modelling. Rather than pre-specified, the forward process is parameterized
through a time- and data-dependent transformation that is equivariant to rigid
transformations. Through a series of experiments on standard molecule
generation benchmarks, we demonstrate the competitive performance of END
compared to several strong baselines for both unconditional and conditional
generation.

</details>


### [361] [Data-driven Day Ahead Market Prices Forecasting: A Focus on Short Training Set Windows](https://arxiv.org/pdf/2506.10536)
*Vasilis Michalakopoulos, Christoforos Menos-Aikateriniadis, Elissaios Sarmas, Antonis Zakynthinos, Pavlos S. Georgilakis, Dimitris Askounis*

Main category: cs.LG

TL;DR: LightGBM outperforms other models (LSTM, XGBoost, CatBoost) in forecasting electricity DAM prices using short training windows (7-90 days), excelling in accuracy, robustness, and detecting seasonal trends and price spikes.


<details>
  <summary>Details</summary>
Motivation: To evaluate machine learning models' performance in forecasting electricity DAM prices with limited historical data, focusing on seasonal trends and price spikes.

Method: Four models (LSTM with FFEC, XGBoost, LightGBM, CatBoost) were tested across three European markets using ENTSO-E forecast data with training windows of 7-90 days.

Result: LightGBM achieved the highest accuracy and robustness, especially with 45-60 day windows, and outperformed others in detecting seasonal effects and price spikes.

Conclusion: Short-window training with boosting methods like LightGBM is effective for DAM forecasting in volatile, data-scarce environments.

Abstract: This study investigates the performance of machine learning models in
forecasting electricity Day-Ahead Market (DAM) prices using short historical
training windows, with a focus on detecting seasonal trends and price spikes.
We evaluate four models, namely LSTM with Feed Forward Error Correction (FFEC),
XGBoost, LightGBM, and CatBoost, across three European energy markets (Greece,
Belgium, Ireland) using feature sets derived from ENTSO-E forecast data.
Training window lengths range from 7 to 90 days, allowing assessment of model
adaptability under constrained data availability. Results indicate that
LightGBM consistently achieves the highest forecasting accuracy and robustness,
particularly with 45 and 60 day training windows, which balance temporal
relevance and learning depth. Furthermore, LightGBM demonstrates superior
detection of seasonal effects and peak price events compared to LSTM and other
boosting models. These findings suggest that short-window training approaches,
combined with boosting methods, can effectively support DAM forecasting in
volatile, data-scarce environments.

</details>


### [362] [Graph Neural Networks for Automatic Addition of Optimizing Components in Printed Circuit Board Schematics](https://arxiv.org/pdf/2506.10577)
*Pascal Plettenberg, André Alcalde, Bernhard Sick, Josephine M. Thomas*

Main category: cs.LG

TL;DR: Automated PCB design optimization using GNNs to add components, improving robustness and reducing manual effort.


<details>
  <summary>Details</summary>
Motivation: Shortage of skilled engineers and time-consuming manual optimizations lead to neglected best practices, higher troubleshooting costs, and increased electronic waste.

Method: Represent PCB schematics as bipartite graphs and use GNN-based node pair prediction to automate component addition.

Result: GNNs achieve high accuracy in optimizing PCB designs, demonstrating potential for time- and cost-efficient automation.

Conclusion: The approach effectively automates PCB design optimizations, addressing skill shortages and reducing waste.

Abstract: The design and optimization of Printed Circuit Board (PCB) schematics is
crucial for the development of high-quality electronic devices. Thereby, an
important task is to optimize drafts by adding components that improve the
robustness and reliability of the circuit, e.g., pull-up resistors or
decoupling capacitors. Since there is a shortage of skilled engineers and
manual optimizations are very time-consuming, these best practices are often
neglected. However, this typically leads to higher costs for troubleshooting in
later development stages as well as shortened product life cycles, resulting in
an increased amount of electronic waste that is difficult to recycle. Here, we
present an approach for automating the addition of new components into PCB
schematics by representing them as bipartite graphs and utilizing a node pair
prediction model based on Graph Neural Networks (GNNs). We apply our approach
to three highly relevant PCB design optimization tasks and compare the
performance of several popular GNN architectures on real-world datasets labeled
by human experts. We show that GNNs can solve these problems with high accuracy
and demonstrate that our approach offers the potential to automate PCB design
optimizations in a time- and cost-efficient manner.

</details>


### [363] [Size-adaptive Hypothesis Testing for Fairness](https://arxiv.org/pdf/2506.10586)
*Antonio Ferrara, Francesco Cozzi, Alan Perotti, André Panisson, Francesco Bonchi*

Main category: cs.LG

TL;DR: A framework for statistically robust fairness assessment in algorithmic decision-making, addressing sampling error and intersectional subgroup sparsity.


<details>
  <summary>Details</summary>
Motivation: Current fairness assessments ignore sampling error and treat small subgroups the same as large ones, leading to unreliable conclusions, especially in intersectional analyses.

Method: Introduces a unified, size-adaptive hypothesis-testing framework: (i) Wald test with analytic confidence intervals for large subgroups, (ii) Bayesian Dirichlet-multinomial estimator for small subgroups.

Result: Validated on benchmark datasets, the framework provides interpretable, statistically rigorous decisions under varying data availability and intersectionality.

Conclusion: The proposed method ensures reliable fairness assessments by adapting to subgroup sizes and handling data sparsity, offering a statistically sound alternative to point estimates.

Abstract: Determining whether an algorithmic decision-making system discriminates
against a specific demographic typically involves comparing a single point
estimate of a fairness metric against a predefined threshold. This practice is
statistically brittle: it ignores sampling error and treats small demographic
subgroups the same as large ones. The problem intensifies in intersectional
analyses, where multiple sensitive attributes are considered jointly, giving
rise to a larger number of smaller groups. As these groups become more
granular, the data representing them becomes too sparse for reliable
estimation, and fairness metrics yield excessively wide confidence intervals,
precluding meaningful conclusions about potential unfair treatments.
  In this paper, we introduce a unified, size-adaptive, hypothesis-testing
framework that turns fairness assessment into an evidence-based statistical
decision. Our contribution is twofold. (i) For sufficiently large subgroups, we
prove a Central-Limit result for the statistical parity difference, leading to
analytic confidence intervals and a Wald test whose type-I (false positive)
error is guaranteed at level $\alpha$. (ii) For the long tail of small
intersectional groups, we derive a fully Bayesian Dirichlet-multinomial
estimator; Monte-Carlo credible intervals are calibrated for any sample size
and naturally converge to Wald intervals as more data becomes available. We
validate our approach empirically on benchmark datasets, demonstrating how our
tests provide interpretable, statistically rigorous decisions under varying
degrees of data availability and intersectionality.

</details>


### [364] [Non-stationary Online Learning for Curved Losses: Improved Dynamic Regret via Mixability](https://arxiv.org/pdf/2506.10616)
*Yu-Jie Zhang, Peng Zhao, Masashi Sugiyama*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Non-stationary online learning has drawn much attention in recent years.
Despite considerable progress, dynamic regret minimization has primarily
focused on convex functions, leaving the functions with stronger curvature
(e.g., squared or logistic loss) underexplored. In this work, we address this
gap by showing that the regret can be substantially improved by leveraging the
concept of mixability, a property that generalizes exp-concavity to effectively
capture loss curvature. Let $d$ denote the dimensionality and $P_T$ the path
length of comparators that reflects the environmental non-stationarity. We
demonstrate that an exponential-weight method with fixed-share updates achieves
an $\mathcal{O}(d T^{1/3} P_T^{2/3} \log T)$ dynamic regret for mixable losses,
improving upon the best-known $\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \log T)$
result (Baby and Wang, 2021) in $d$. More importantly, this improvement arises
from a simple yet powerful analytical framework that exploits the mixability,
which avoids the Karush-Kuhn-Tucker-based analysis required by existing work.

</details>


### [365] [Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code](https://arxiv.org/pdf/2506.10617)
*Reza Karbasi, Masoud Rahimi, Abdol-Hossein Vahabie, Hadi Moradi*

Main category: cs.LG

TL;DR: A two-stage pipeline for digitizing paper-based ECG recordings, addressing signal overlaps with a U-Net segmentation and adaptive digitization, outperforming baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve digitization of paper-based ECG recordings, especially for overlapping signals, a common but under-addressed issue.

Method: A U-Net segmentation network isolates the primary ECG trace, followed by adaptive digitization with grid detection.

Result: U-Net achieves 0.87 IoU; digitization outperforms baselines (MSE 0.0010 vs. 0.0015 for non-overlapping, 0.0029 vs. 0.0178 for overlapping).

Conclusion: The method enhances digitization accuracy, especially for overlapping signals, enabling reliable conversion of analog ECG data.

Abstract: This paper addresses the persistent challenge of accurately digitizing
paper-based electrocardiogram (ECG) recordings, with a particular focus on
robustly handling single leads compromised by signal overlaps-a common yet
under-addressed issue in existing methodologies. We propose a two-stage
pipeline designed to overcome this limitation. The first stage employs a U-Net
based segmentation network, trained on a dataset enriched with overlapping
signals and fortified with custom data augmentations, to accurately isolate the
primary ECG trace. The subsequent stage converts this refined binary mask into
a time-series signal using established digitization techniques, enhanced by an
adaptive grid detection module for improved versatility across different ECG
formats and scales. Our experimental results demonstrate the efficacy of our
approach. The U-Net architecture achieves an IoU of 0.87 for the fine-grained
segmentation task. Crucially, our proposed digitization method yields superior
performance compared to a well-established baseline technique across both
non-overlapping and challenging overlapping ECG samples. For non-overlapping
signals, our method achieved a Mean Squared Error (MSE) of 0.0010 and a Pearson
Correlation Coefficient (rho) of 0.9644, compared to 0.0015 and 0.9366,
respectively, for the baseline. On samples with signal overlap, our method
achieved an MSE of 0.0029 and a rho of 0.9641, significantly improving upon the
baseline's 0.0178 and 0.8676. This work demonstrates an effective strategy to
significantly enhance digitization accuracy, especially in the presence of
signal overlaps, thereby laying a strong foundation for the reliable conversion
of analog ECG records into analyzable digital data for contemporary research
and clinical applications. The implementation is publicly available at this
GitHub repository: https://github.com/masoudrahimi39/ECG-code.

</details>


### [366] [Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning](https://arxiv.org/pdf/2506.10629)
*Yucheng Yang, Tianyi Zhou, Qiang He, Lei Han, Mykola Pechenizkiy, Meng Fang*

Main category: cs.LG

TL;DR: The paper analyzes Mutual Information Skill Learning (MISL) in unsupervised reinforcement learning, identifies its limitations in skill diversity and separability, and proposes new metrics (LSEPIN, WSEP, PWSEP) to improve downstream task adaptation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical analysis in MISL and improve its ability to initialize policies for downstream tasks by ensuring skill diversity and separability.

Method: Proposes LSEPIN as a disentanglement metric, replaces KL divergence with Wasserstein distance for better geometric properties, and introduces WSEP and PWSEP algorithms.

Result: Theoretical analysis shows WSEP improves downstream task adaptation and discovers more initial policies than MISL; PWSEP can theoretically find all optimal initial policies.

Conclusion: The proposed methods (LSEPIN, WSEP, PWSEP) enhance skill learning by addressing MISL's limitations, with Wasserstein distance-based approaches offering superior theoretical guarantees for downstream tasks.

Abstract: Unsupervised reinforcement learning (URL) aims to learn general skills for
unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL
by maximizing the mutual information between states and skills but lacks
sufficient theoretical analysis, e.g., how well its learned skills can
initialize a downstream task's policy. Our new theoretical analysis in this
paper shows that the diversity and separability of learned skills are
fundamentally critical to downstream task adaptation but MISL does not
necessarily guarantee these properties. To complement MISL, we propose a novel
disentanglement metric LSEPIN. Moreover, we build an information-geometric
connection between LSEPIN and downstream task adaptation cost. For better
geometric properties, we investigate a new strategy that replaces the KL
divergence in information geometry with Wasserstein distance. We extend the
geometric analysis to it, which leads to a novel skill-learning objective WSEP.
It is theoretically justified to be helpful to downstream task adaptation and
it is capable of discovering more initial policies for downstream tasks than
MISL. We finally propose another Wasserstein distance-based algorithm PWSEP
that can theoretically discover all optimal initial policies.

</details>


### [367] [Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering](https://arxiv.org/pdf/2506.10751)
*Sai Prasanna Teja Reddy Bogireddy, Abrar Majeedi, Viswanatha Reddy Gajjala, Zhuoyan Xu, Siddhant Rai, Vaishnav Potlapalli*

Main category: cs.LG

TL;DR: The paper presents Neural, a method for automated QA over EHRs, which decouples evidence identification and answer synthesis, using prompt optimization and self-consistency voting to achieve high performance.


<details>
  <summary>Details</summary>
Motivation: To bridge information gaps in healthcare by enabling precise and reliable QA over EHRs with limited supervision.

Method: Decouples QA into evidence identification and answer synthesis, optimizes prompts using DSPy's MIPROv2, and employs self-consistency voting.

Result: Achieved an overall score of 51.5 on the test set, outperforming zero-shot and few-shot prompting by significant margins.

Conclusion: Data-driven prompt optimization is a viable alternative to fine-tuning for clinical QA, enhancing AI reliability in healthcare.

Abstract: Automated question answering (QA) over electronic health records (EHRs) can
bridge critical information gaps for clinicians and patients, yet it demands
both precise evidence retrieval and faithful answer generation under limited
supervision. In this work, we present Neural, the runner-up in the BioNLP 2025
ArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method
decouples the task into (1) sentence-level evidence identification and (2)
answer synthesis with explicit citations. For each stage, we automatically
explore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning
instructions and few-shot demonstrations on the development set. A
self-consistency voting scheme further improves evidence recall without
sacrificing precision. On the hidden test set, our method attains an overall
score of 51.5, placing second stage while outperforming standard zero-shot and
few-shot prompting by over 20 and 10 points, respectively. These results
indicate that data-driven prompt optimization is a cost-effective alternative
to model fine-tuning for high-stakes clinical QA, advancing the reliability of
AI assistants in healthcare.

</details>


### [368] [Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs](https://arxiv.org/pdf/2506.10630)
*Yucong Luo, Yitong Zhou, Mingyue Cheng, Jiahao Wang, Daoyu Wang, Tingyue Pan, Jintao Zhang*

Main category: cs.LG

TL;DR: Time-R1 is a two-stage reinforcement fine-tuning framework for LLMs to enhance multi-step reasoning in time series forecasting, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing time series forecasting methods lack explicit reasoning processes, while LLMs show potential but face limitations like high computational costs and privacy risks.

Method: Time-R1 uses supervised fine-tuning for adaptation and reinforcement learning for generalization, with a multi-objective reward and GRIP for optimizing reasoning paths.

Result: Time-R1 significantly improves forecasting performance across diverse datasets.

Conclusion: Time-R1 effectively enhances LLMs' slow-thinking capabilities for time series forecasting, addressing limitations of existing methods.

Abstract: To advance time series forecasting (TSF), various methods have been proposed
to improve prediction accuracy, evolving from statistical techniques to
data-driven deep learning architectures. Despite their effectiveness, most
existing methods still adhere to a fast thinking paradigm-relying on extracting
historical patterns and mapping them to future values as their core modeling
philosophy, lacking an explicit thinking process that incorporates intermediate
time series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1)
have shown remarkable multi-step reasoning capabilities, offering an
alternative way to overcome these issues. However, prompt engineering alone
presents several limitations - including high computational cost, privacy
risks, and limited capacity for in-depth domain-specific time series reasoning.
To address these limitations, a more promising approach is to train LLMs to
develop slow thinking capabilities and acquire strong time series reasoning
skills. For this purpose, we propose Time-R1, a two-stage reinforcement
fine-tuning framework designed to enhance multi-step reasoning ability of LLMs
for time series forecasting. Specifically, the first stage conducts supervised
fine-tuning for warmup adaptation, while the second stage employs reinforcement
learning to improve the model's generalization ability. Particularly, we design
a fine-grained multi-objective reward specifically for time series forecasting,
and then introduce GRIP (group-based relative importance for policy
optimization), which leverages non-uniform sampling to further encourage and
optimize the model's exploration of effective reasoning paths. Experiments
demonstrate that Time-R1 significantly improves forecast performance across
diverse datasets.

</details>


### [369] [Data Shifts Hurt CoT: A Theoretical Study](https://arxiv.org/pdf/2506.10647)
*Lang Yin, Debangshu Banerjee, Gagandeep Singh*

Main category: cs.LG

TL;DR: CoT improves LLM outputs but faces issues with data shifts and poisoning, revealing worse performance on parity problems.


<details>
  <summary>Details</summary>
Motivation: To study the impact of distribution shifts and data poisoning on CoT's effectiveness, especially for the $k$-parity problem.

Method: Investigates joint effects of distribution shifts and data poisoning on CoT-trained models using the $k$-parity problem.

Result: CoT performs worse than direct prediction for parity learning, with mechanistic explanations provided.

Conclusion: CoT's limitations under real-world data conditions highlight risks and call for further research.

Abstract: Chain of Thought (CoT) has been applied to various large language models
(LLMs) and proven to be effective in improving the quality of outputs. In
recent studies, transformers are proven to have absolute upper bounds in terms
of expressive power, and consequently, they cannot solve many computationally
difficult problems. However, empowered by CoT, transformers are proven to be
able to solve some difficult problems effectively, such as the $k$-parity
problem. Nevertheless, those works rely on two imperative assumptions: (1)
identical training and testing distribution, and (2) corruption-free training
data with correct reasoning steps. However, in the real world, these
assumptions do not always hold. Although the risks of data shifts have caught
attention, our work is the first to rigorously study the exact harm caused by
such shifts to the best of our knowledge. Focusing on the $k$-parity problem,
in this work we investigate the joint impact of two types of data shifts: the
distribution shifts and data poisoning, on the quality of trained models
obtained by a well-established CoT decomposition. In addition to revealing a
surprising phenomenon that CoT leads to worse performance on learning parity
than directly generating the prediction, our technical results also give a
rigorous and comprehensive explanation of the mechanistic reasons of such
impact.

</details>


### [370] [Hessian Geometry of Latent Space in Generative Models](https://arxiv.org/pdf/2506.10632)
*Alexander Lobashev, Dmitry Guskov, Maria Larchenko, Mikhail Tamm*

Main category: cs.LG

TL;DR: A novel method analyzes latent space geometry in generative models by reconstructing the Fisher information metric, validated on Ising and TASEP models, and revealing fractal phase transitions in diffusion models.


<details>
  <summary>Details</summary>
Motivation: To understand the complex structure of latent spaces in generative models, particularly diffusion models, and their connection to phenomena like phase transitions.

Method: Approximates the posterior distribution of latent variables to learn the log-partition function, defining the Fisher metric for exponential families, with theoretical convergence guarantees.

Result: Outperforms baselines in reconstructing thermodynamic quantities and reveals fractal phase transitions in diffusion models, with geodesic interpolations breaking linearity at phase boundaries.

Conclusion: The method provides new insights into diffusion model latent spaces, highlighting their complex structure and connection to phase transitions.

Abstract: This paper presents a novel method for analyzing the latent space geometry of
generative models, including statistical physics models and diffusion models,
by reconstructing the Fisher information metric. The method approximates the
posterior distribution of latent variables given generated samples and uses
this to learn the log-partition function, which defines the Fisher metric for
exponential families. Theoretical convergence guarantees are provided, and the
method is validated on the Ising and TASEP models, outperforming existing
baselines in reconstructing thermodynamic quantities. Applied to diffusion
models, the method reveals a fractal structure of phase transitions in the
latent space, characterized by abrupt changes in the Fisher metric. We
demonstrate that while geodesic interpolations are approximately linear within
individual phases, this linearity breaks down at phase boundaries, where the
diffusion model exhibits a divergent Lipschitz constant with respect to the
latent space. These findings provide new insights into the complex structure of
diffusion model latent spaces and their connection to phenomena like phase
transitions. Our source code is available at
https://github.com/alobashev/hessian-geometry-of-diffusion-models.

</details>


### [371] [Saturation Self-Organizing Map](https://arxiv.org/pdf/2506.10680)
*Igor Urbanik, Paweł Gajewski*

Main category: cs.LG

TL;DR: SatSOM extends SOMs with a saturation mechanism to reduce catastrophic forgetting in continual learning.


<details>
  <summary>Details</summary>
Motivation: Address the issue of catastrophic forgetting in Self-Organizing Maps (SOMs) during continual learning.

Method: Introduces SatSOM, which uses a saturation mechanism to adjust learning rates and neighborhood radii, freezing well-trained neurons and focusing on underutilized areas.

Result: Improved knowledge retention in continual learning scenarios.

Conclusion: SatSOM effectively mitigates catastrophic forgetting in SOMs by dynamically managing neuron learning.

Abstract: Continual learning poses a fundamental challenge for neural systems, which
often suffer from catastrophic forgetting when exposed to sequential tasks.
Self-Organizing Maps (SOMs), despite their interpretability and efficiency, are
not immune to this issue. In this paper, we introduce Saturation
Self-Organizing Maps (SatSOM)-an extension of SOMs designed to improve
knowledge retention in continual learning scenarios. SatSOM incorporates a
novel saturation mechanism that gradually reduces the learning rate and
neighborhood radius of neurons as they accumulate information. This effectively
freezes well-trained neurons and redirects learning to underutilized areas of
the map.

</details>


### [372] [The Diffusion Duality](https://arxiv.org/pdf/2506.10892)
*Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: The paper introduces Duo, a method to improve uniform-state discrete diffusion models by leveraging insights from Gaussian diffusion, achieving faster training and sampling while outperforming autoregressive models in some benchmarks.


<details>
  <summary>Details</summary>
Motivation: Uniform-state discrete diffusion models underperform compared to autoregressive and masked diffusion models despite their potential for fast text generation. The paper aims to bridge this gap by utilizing Gaussian diffusion techniques.

Method: Duo transfers Gaussian diffusion techniques to uniform-state models. It introduces curriculum learning guided by Gaussian processes and Discrete Consistency Distillation for few-step generation.

Result: Curriculum learning doubles training speed and outperforms autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Discrete Consistency Distillation accelerates sampling by two orders of magnitude.

Conclusion: Duo successfully narrows the performance gap between uniform-state diffusion models and stronger baselines, enabling faster and more efficient text generation.

Abstract: Uniform-state discrete diffusion models hold the promise of fast text
generation due to their inherent ability to self-correct. However, they are
typically outperformed by autoregressive models and masked diffusion models. In
this work, we narrow this performance gap by leveraging a key insight:
Uniform-state diffusion processes naturally emerge from an underlying Gaussian
diffusion. Our method, Duo, transfers powerful techniques from Gaussian
diffusion to improve both training and sampling. First, we introduce a
curriculum learning strategy guided by the Gaussian process, doubling training
speed by reducing variance. Models trained with curriculum learning surpass
autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we
present Discrete Consistency Distillation, which adapts consistency
distillation from the continuous to the discrete setting. This algorithm
unlocks few-step generation in diffusion language models by accelerating
sampling by two orders of magnitude. We provide the code and model checkpoints
on the project page: http://s-sahoo.github.io/duo

</details>


### [373] [Preserving Task-Relevant Information Under Linear Concept Removal](https://arxiv.org/pdf/2506.10703)
*Floris Holstege, Shauli Ravfogel, Bram Wouters*

Main category: cs.LG

TL;DR: SPLICE removes unwanted concepts from neural networks while preserving task-relevant covariance, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Neural networks often encode unwanted concepts, raising fairness and interpretability issues. Existing methods degrade useful signals.

Method: SPLICE uses an oblique projection to remove sensitive concepts while preserving covariance with target labels.

Result: SPLICE outperforms baselines on Bias in Bios and Winobias, removing protected attributes with minimal task damage.

Conclusion: SPLICE is theoretically and empirically effective for concept removal without harming task performance.

Abstract: Modern neural networks often encode unwanted concepts alongside task-relevant
information, leading to fairness and interpretability concerns. Existing
post-hoc approaches can remove undesired concepts but often degrade useful
signals. We introduce SPLICE-Simultaneous Projection for LInear concept removal
and Covariance prEservation-which eliminates sensitive concepts from
representations while exactly preserving their covariance with a target label.
SPLICE achieves this via an oblique projection that "splices out" the unwanted
direction yet protects important label correlations. Theoretically, it is the
unique solution that removes linear concept predictability and maintains target
covariance with minimal embedding distortion. Empirically, SPLICE outperforms
baselines on benchmarks such as Bias in Bios and Winobias, removing protected
attributes while minimally damaging main-task information.

</details>


### [374] [ConTextTab: A Semantics-Aware Tabular In-Context Learner](https://arxiv.org/pdf/2506.10707)
*Marco Spinaci, Marek Polewczyk, Maximilian Schambach, Sam Thelin*

Main category: cs.LG

TL;DR: ConTextTab combines table-native ICL with semantic understanding, outperforming SOTA on benchmarks like CARTE.


<details>
  <summary>Details</summary>
Motivation: Current table-native ICL models lack real-world data semantics, while LLM-based models have limited context. ConTextTab aims to merge these strengths.

Method: Uses specialized embeddings for data modalities and trains on large-scale real-world tabular data.

Result: Competes with SOTA across benchmarks and excels on the CARTE benchmark.

Conclusion: ConTextTab successfully integrates semantic understanding into table-native ICL, setting a new benchmark standard.

Abstract: Tabular in-context learning (ICL) has recently achieved state-of-the-art
(SOTA) performance on several tabular prediction tasks. Previously restricted
to classification problems on small tables, recent advances such as TabPFN and
TabICL have extended its use to larger datasets. While being architecturally
efficient and well-adapted to tabular data structures, current table-native ICL
architectures, being trained exclusively on synthetic data, do not fully
leverage the rich semantics and world knowledge contained in real-world tabular
data. On another end of this spectrum, tabular ICL models based on pretrained
large language models such as TabuLa-8B integrate deep semantic understanding
and world knowledge but are only able to make use of a small amount of context
due to inherent architectural limitations. With the aim to combine the best of
both these worlds, we introduce ConTextTab, integrating semantic understanding
and alignment into a table-native ICL framework. By employing specialized
embeddings for different data modalities and by training on large-scale
real-world tabular data, our model is competitive with SOTA across a broad set
of benchmarks while setting a new standard on the semantically rich CARTE
benchmark.

</details>


### [375] [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/pdf/2506.10922)
*Adam Karvonen, Samuel Marks*

Main category: cs.LG

TL;DR: Internal bias mitigation in LLMs reduces hiring biases from 12% to under 2.5% by neutralizing sensitive attribute directions in model activations, even with realistic contextual details.


<details>
  <summary>Details</summary>
Motivation: Address the failure of simple anti-bias prompts in realistic hiring scenarios, where biases persist despite controlled evaluations.

Method: Identify and neutralize sensitive attribute directions (race, gender) in model activations using affine concept editing at inference time.

Result: Bias reduced to under 1-2.5% across commercial and open-source models, maintaining performance while favoring equitable outcomes.

Conclusion: Practitioners should adopt realistic evaluations and internal mitigation for fair LLM deployment in hiring.

Abstract: Large language models (LLMs) are increasingly deployed in high-stakes hiring
applications, making decisions that directly impact people's careers and
livelihoods. While prior studies suggest simple anti-bias prompts can eliminate
demographic biases in controlled evaluations, we find these mitigations fail
when realistic contextual details are introduced. We address these failures
through internal bias mitigation: by identifying and neutralizing sensitive
attribute directions within model activations, we achieve robust bias reduction
across all tested scenarios. Across leading commercial (GPT-4o, Claude 4
Sonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3,
Mistral-24B), we find that adding realistic context such as company names,
culture descriptions from public careers pages, and selective hiring
constraints (e.g.,``only accept candidates in the top 10\%") induces
significant racial and gender biases (up to 12\% differences in interview
rates). When these biases emerge, they consistently favor Black over White
candidates and female over male candidates across all tested models and
scenarios. Moreover, models can infer demographics and become biased from
subtle cues like college affiliations, with these biases remaining invisible
even when inspecting the model's chain-of-thought reasoning. To address these
limitations, our internal bias mitigation identifies race and gender-correlated
directions and applies affine concept editing at inference time. Despite using
directions from a simple synthetic dataset, the intervention generalizes
robustly, consistently reducing bias to very low levels (typically under 1\%,
always below 2.5\%) while largely maintaining model performance. Our findings
suggest that practitioners deploying LLMs for hiring should adopt more
realistic evaluation methodologies and consider internal mitigation strategies
for equitable outcomes.

</details>


### [376] [Skillful joint probabilistic weather forecasting from marginals](https://arxiv.org/pdf/2506.10772)
*Ferran Alet, Ilan Price, Andrew El-Kadi, Dominic Masters, Stratis Markou, Tom R. Andersson, Jacklynn Stott, Remi Lam, Matthew Willson, Alvaro Sanchez-Gonzalez, Peter Battaglia*

Main category: cs.LG

TL;DR: FGN, a scalable ML-based weather model, outperforms traditional NWP and current ML models by generating ensembles via learned perturbations, achieving state-of-the-art forecasts.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and speed in weather forecasting beyond traditional NWP and existing ML models.

Method: FGN uses learned model-perturbations to generate ensembles, trained to minimize CRPS for per-location forecasts.

Result: FGN excels in deterministic and probabilistic metrics, tropical cyclone track predictions, and captures spatial structure despite marginal training.

Conclusion: FGN is a simple, scalable, and flexible approach that sets a new benchmark in ensemble weather forecasting.

Abstract: Machine learning (ML)-based weather models have rapidly risen to prominence
due to their greater accuracy and speed than traditional forecasts based on
numerical weather prediction (NWP), recently outperforming traditional
ensembles in global probabilistic weather forecasting. This paper presents FGN,
a simple, scalable and flexible modeling approach which significantly
outperforms the current state-of-the-art models. FGN generates ensembles via
learned model-perturbations with an ensemble of appropriately constrained
models. It is trained directly to minimize the continuous rank probability
score (CRPS) of per-location forecasts. It produces state-of-the-art ensemble
forecasts as measured by a range of deterministic and probabilistic metrics,
makes skillful ensemble tropical cyclone track predictions, and captures joint
spatial structure despite being trained only on marginals.

</details>


### [377] [GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models](https://arxiv.org/pdf/2506.10946)
*Evelyn Ma, Duo Zhou, Peizhi Niu, Huiting Zhou, Huan Zhang, Olgica Milenkovic, S. Rasoul Etesami*

Main category: cs.LG

TL;DR: GUARD is a novel framework for LLM unlearning that uses data attribution to mitigate unintended forgetting while preserving model utility.


<details>
  <summary>Details</summary>
Motivation: Addressing unintended forgetting in LLMs due to regulatory, copyright, and privacy concerns, focusing on data-level factors.

Method: Introduces a lightweight proxy data attribution metric and an adaptive unlearning objective with nonuniform weights.

Result: GUARD improves retention significantly (up to 194.92% in Truth Ratio) while maintaining unlearning performance.

Conclusion: GUARD effectively balances unlearning and retention, outperforming prior methods in utility preservation.

Abstract: Unlearning in large language models (LLMs) is becoming increasingly important
due to regulatory compliance, copyright protection, and privacy concerns.
However, a key challenge in LLM unlearning is unintended forgetting, where the
removal of specific data inadvertently impairs the utility of the model and its
retention of valuable, desired information. While prior work has primarily
focused on architectural innovations, the influence of data-level factors on
unlearning performance remains underexplored. As a result, existing methods
often suffer from degraded retention when forgetting high-impact data. To
address this, we propose GUARD-a novel framework for Guided Unlearning And
Retention via Data attribution. At its core, GUARD introduces a lightweight
proxy data attribution metric tailored for LLM unlearning, which quantifies the
"alignment" between the forget and retain sets while remaining computationally
efficient. Building on this, we design a novel unlearning objective that
assigns adaptive, nonuniform unlearning weights to samples, inversely
proportional to their proxy attribution scores. Through such a reallocation of
unlearning power, GUARD mitigates unintended losses in retention. We provide
rigorous theoretical guarantees that GUARD significantly enhances retention
while maintaining forgetting metrics comparable to prior methods. Extensive
experiments on the TOFU benchmark across multiple LLM architectures demonstrate
that GUARD substantially improves utility preservation while ensuring effective
unlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to
194.92% in terms of Truth Ratio when forgetting 10% of the training data.

</details>


### [378] [Monotone Classification with Relative Approximations](https://arxiv.org/pdf/2506.10775)
*Yufei Tao*

Main category: cs.LG

TL;DR: The paper studies the minimal cost to find a monotone classifier with error at most (1 + ε) times the optimal, presenting tight bounds for all ε.


<details>
  <summary>Details</summary>
Motivation: To improve upon prior work that only achieved error bounds with absolute factors, this research aims to provide relative error guarantees for monotone classification.

Method: The study involves analyzing the cost (number of revealed labels) required to identify a monotone classifier with error within a (1 + ε) factor of the optimal.

Result: Nearly matching upper and lower bounds are established for the entire range of ε, advancing beyond previous absolute-factor results.

Conclusion: This work provides the first relative-error guarantees for monotone classification, setting new benchmarks for cost and accuracy.

Abstract: In monotone classification, the input is a multi-set $P$ of points in
$\mathbb{R}^d$, each associated with a hidden label from $\{-1, 1\}$. The goal
is to identify a monotone function $h$, which acts as a classifier, mapping
from $\mathbb{R}^d$ to $\{-1, 1\}$ with a small {\em error}, measured as the
number of points $p \in P$ whose labels differ from the function values $h(p)$.
The cost of an algorithm is defined as the number of points having their labels
revealed. This article presents the first study on the lowest cost required to
find a monotone classifier whose error is at most $(1 + \epsilon) \cdot k^*$
where $\epsilon \ge 0$ and $k^*$ is the minimum error achieved by an optimal
monotone classifier -- in other words, the error is allowed to exceed the
optimal by at most a relative factor. Nearly matching upper and lower bounds
are presented for the full range of $\epsilon$. All previous work on the
problem can only achieve an error higher than the optimal by an absolute
factor.

</details>


### [379] [Build the web for agents, not agents for the web](https://arxiv.org/pdf/2506.10953)
*Xing Han Lù, Gaurav Kamath, Marius Mosbach, Siva Reddy*

Main category: cs.LG

TL;DR: The paper proposes a shift in web agent research by introducing Agentic Web Interfaces (AWIs) designed specifically for AI agents, addressing current challenges in adapting human-designed interfaces for LLMs.


<details>
  <summary>Details</summary>
Motivation: Current web agents struggle with the complexity of human-designed interfaces, leading to inefficiencies and limitations in task automation.

Method: The paper introduces the concept of Agentic Web Interfaces (AWIs) and outlines six design principles focusing on safety, efficiency, and standardization.

Result: The proposed AWI framework aims to overcome limitations of existing interfaces, enabling more efficient and reliable web agent performance.

Conclusion: The paper calls for a collaborative effort to develop AWIs, advocating for a new interaction paradigm optimized for AI agents.

Abstract: Recent advancements in Large Language Models (LLMs) and multimodal
counterparts have spurred significant interest in developing web agents -- AI
systems capable of autonomously navigating and completing tasks within web
environments. While holding tremendous promise for automating complex web
interactions, current approaches face substantial challenges due to the
fundamental mismatch between human-designed interfaces and LLM capabilities.
Current methods struggle with the inherent complexity of web inputs, whether
processing massive DOM trees, relying on screenshots augmented with additional
information, or bypassing the user interface entirely through API interactions.
This position paper advocates for a paradigm shift in web agent research:
rather than forcing web agents to adapt to interfaces designed for humans, we
should develop a new interaction paradigm specifically optimized for agentic
capabilities. To this end, we introduce the concept of an Agentic Web Interface
(AWI), an interface specifically designed for agents to navigate a website. We
establish six guiding principles for AWI design, emphasizing safety,
efficiency, and standardization, to account for the interests of all primary
stakeholders. This reframing aims to overcome fundamental limitations of
existing interfaces, paving the way for more efficient, reliable, and
transparent web agent design, which will be a collaborative effort involving
the broader ML community.

</details>


### [380] [Dense Associative Memory with Epanechnikov Energy](https://arxiv.org/pdf/2506.10801)
*Benjamin Hoover, Zhaoyang Shi, Krishnakumar Balasubramanian, Dmitry Krotov, Parikshit Ram*

Main category: cs.LG

TL;DR: A novel energy function (LSR) for DenseAM networks, based on the Epanechnikov kernel, enables exact memory retrieval with exponential capacity and introduces emergent local minima while preserving pattern recovery.


<details>
  <summary>Details</summary>
Motivation: To improve DenseAM networks by addressing limitations of the log-sum-exponential (LSE) function, such as the need for exponential separation functions, and to explore emergent local minima for enhanced memory storage and generative potential.

Method: Proposes the log-sum-ReLU (LSR) energy function, inspired by optimal kernel density estimation, and evaluates its performance empirically.

Result: LSR achieves exact memory retrieval with exponential capacity, introduces abundant emergent local minima, and shows comparable log-likelihood to LSE-based models. It also exhibits creativity in memory storage and generative tasks.

Conclusion: The LSR energy function offers a promising alternative to LSE for DenseAM networks, with potential applications in large-scale memory storage and generative tasks due to its emergent properties.

Abstract: We propose a novel energy function for Dense Associative Memory (DenseAM)
networks, the log-sum-ReLU (LSR), inspired by optimal kernel density
estimation. Unlike the common log-sum-exponential (LSE) function, LSR is based
on the Epanechnikov kernel and enables exact memory retrieval with exponential
capacity without requiring exponential separation functions. Moreover, it
introduces abundant additional \emph{emergent} local minima while preserving
perfect pattern recovery -- a characteristic previously unseen in DenseAM
literature. Empirical results show that LSR energy has significantly more local
minima (memories) that have comparable log-likelihood to LSE-based models.
Analysis of LSR's emergent memories on image datasets reveals a degree of
creativity and novelty, hinting at this method's potential for both large-scale
memory storage and generative tasks.

</details>


### [381] [Detecting High-Stakes Interactions with Activation Probes](https://arxiv.org/pdf/2506.10805)
*Alex McKenzie, Urja Pawar, Phil Blandfort, William Bankes, David Krueger, Ekdeep Singh Lubana, Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: Activation probes detect high-stakes LLM interactions efficiently, generalizing well to real-world data while saving computational costs.


<details>
  <summary>Details</summary>
Motivation: Monitoring LLMs for high-stakes interactions is critical but underexplored.

Method: Evaluated probe architectures on synthetic data, tested generalization to real-world data.

Result: Probes match medium-sized LLM monitors' performance with significant computational savings.

Conclusion: Probes are efficient for initial monitoring, enabling hierarchical systems; synthetic dataset and code released for further research.

Abstract: Monitoring is an important aspect of safely deploying Large Language Models
(LLMs). This paper examines activation probes for detecting "high-stakes"
interactions -- where the text indicates that the interaction might lead to
significant harm -- as a critical, yet underexplored, target for such
monitoring. We evaluate several probe architectures trained on synthetic data,
and find them to exhibit robust generalization to diverse, out-of-distribution,
real-world data. Probes' performance is comparable to that of prompted or
finetuned medium-sized LLM monitors, while offering computational savings of
six orders-of-magnitude. Our experiments also highlight the potential of
building resource-aware hierarchical monitoring systems, where probes serve as
an efficient initial filter and flag cases for more expensive downstream
analysis. We release our novel synthetic dataset and codebase to encourage
further study.

</details>


### [382] [Efficiency Robustness of Dynamic Deep Learning Systems](https://arxiv.org/pdf/2506.10831)
*Ravishka Rathnasuriya, Tingxi Li, Zexin Xu, Zihe Song, Mirazul Haque, Simin Chen, Wei Yang*

Main category: cs.LG

TL;DR: The paper explores efficiency robustness in Dynamic Deep Learning Systems (DDLSs), categorizing attacks on their dynamic behaviors and evaluating defenses.


<details>
  <summary>Details</summary>
Motivation: DDLSs improve efficiency by adapting inference computation, but this introduces new attack surfaces, particularly efficiency adversarial attacks.

Method: The study presents a taxonomy of efficiency attacks on DDLSs, categorizing them into three dynamic behaviors, and evaluates adversarial strategies and defenses.

Result: The paper identifies key challenges in securing DDLSs and demonstrates the limitations of existing defenses against efficiency attacks.

Conclusion: Novel mitigation strategies are needed to secure future adaptive DDLSs against efficiency attacks.

Abstract: Deep Learning Systems (DLSs) are increasingly deployed in real-time
applications, including those in resourceconstrained environments such as
mobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning
Systems (DDLSs) adapt inference computation based on input complexity, reducing
overhead. While this dynamic behavior improves efficiency, such behavior
introduces new attack surfaces. In particular, efficiency adversarial attacks
exploit these dynamic mechanisms to degrade system performance. This paper
systematically explores efficiency robustness of DDLSs, presenting the first
comprehensive taxonomy of efficiency attacks. We categorize these attacks based
on three dynamic behaviors: (i) attacks on dynamic computations per inference,
(ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic
output production for downstream tasks. Through an in-depth evaluation, we
analyze adversarial strategies that target DDLSs efficiency and identify key
challenges in securing these systems. In addition, we investigate existing
defense mechanisms, demonstrating their limitations against increasingly
popular efficiency attacks and the necessity for novel mitigation strategies to
secure future adaptive DDLSs.

</details>


### [383] [Advanced fraud detection using machine learning models: enhancing financial transaction security](https://arxiv.org/pdf/2506.10842)
*Nudrat Fariha, Md Nazmuddin Moin Khan, Md Iqbal Hossain, Syed Ali Reza, Joy Chakra Bortty, Kazi Sharmin Sultana, Md Shadidur Islam Jawad, Saniah Safat, Md Abdul Ahad, Maksuda Begum*

Main category: cs.LG

TL;DR: A machine learning framework for detecting credit card fraud using feature engineering and unsupervised models like Isolation Forest, One Class SVM, and autoencoders.


<details>
  <summary>Details</summary>
Motivation: The rise of digital payments necessitates scalable fraud detection systems.

Method: Merges datasets, performs feature engineering, and trains unsupervised models (Isolation Forest, One Class SVM, autoencoder) to detect anomalies.

Result: Models flag top 1% of outliers; PCA and clustering methods identify suspicious regions.

Conclusion: The framework effectively detects fraud by leveraging behavioral signals and unsupervised learning.

Abstract: The rise of digital payments has accelerated the need for intelligent and
scalable systems to detect fraud. This research presents an end-to-end,
feature-rich machine learning framework for detecting credit card transaction
anomalies and fraud using real-world data. The study begins by merging
transactional, cardholder, merchant, and merchant category datasets from a
relational database to create a unified analytical view. Through the feature
engineering process, we extract behavioural signals such as average spending,
deviation from historical patterns, transaction timing irregularities, and
category frequency metrics. These features are enriched with temporal markers
such as hour, day of week, and weekend indicators to expose all latent patterns
that indicate fraudulent behaviours. Exploratory data analysis reveals
contextual transaction trends across all the dataset features. Using the
transactional data, we train and evaluate a range of unsupervised models:
Isolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct
normal behavior. These models flag the top 1% of reconstruction errors as
outliers. PCA visualizations illustrate each models ability to separate
anomalies into a two-dimensional latent space. We further segment the
transaction landscape using K-Means clustering and DBSCAN to identify dense
clusters of normal activity and isolate sparse, suspicious regions.

</details>


### [384] [Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization](https://arxiv.org/pdf/2506.10871)
*Pierre-François Massiani, Alexander von Rohr, Lukas Haverbeck, Sebastian Trimpe*

Main category: cs.LG

TL;DR: The paper explores how entropy regularization and constraints penalization in RL can robustly satisfy state constraints under disturbances, showing that entropy regularization promotes constraint satisfaction and relaxed constraints can be solved using standard RL.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning policies that robustly satisfy state constraints under unknown disturbances in RL.

Method: Analyzes the interplay between entropy regularization and constraints penalization, demonstrating how entropy regularization biases learning toward constraint satisfaction and how relaxed constraints can be approximated by unconstrained RL.

Result: Empirical results show that entropy regularization enhances robustness to action noise, and relaxed constraints preserve safety and optimality while improving resilience to disturbances.

Conclusion: The connection between entropy regularization and robustness is promising for further investigation, enabling robust safety in RL through simple reward shaping.

Abstract: Despite the many recent advances in reinforcement learning (RL), the question
of learning policies that robustly satisfy state constraints under unknown
disturbances remains open. In this paper, we offer a new perspective on
achieving robust safety by analyzing the interplay between two well-established
techniques in model-free RL: entropy regularization, and constraints
penalization. We reveal empirically that entropy regularization in constrained
RL inherently biases learning toward maximizing the number of future viable
actions, thereby promoting constraints satisfaction robust to action noise.
Furthermore, we show that by relaxing strict safety constraints through
penalties, the constrained RL problem can be approximated arbitrarily closely
by an unconstrained one and thus solved using standard model-free RL. This
reformulation preserves both safety and optimality while empirically improving
resilience to disturbances. Our results indicate that the connection between
entropy regularization and robustness is a promising avenue for further
empirical and theoretical investigation, as it enables robust safety in RL
through simple reward shaping.

</details>


### [385] [Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers](https://arxiv.org/pdf/2506.10888)
*Lucas Gnecco-Heredia, Benjamin Negrevergne, Yann Chevaleyre*

Main category: cs.LG

TL;DR: The paper introduces a principled approach to attacking finite mixtures of classifiers, proposing two key properties for effective attacks and introducing a new attack method with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing attacks on randomized ensembles (finite mixtures of classifiers) lack effectiveness and maximality, prompting the need for a principled attack method.

Method: The paper conducts a geometrical analysis to define desirable attack properties (effectiveness and maximality) and introduces the lattice climber attack, tested on synthetic and real datasets.

Result: Existing attacks fail to meet the proposed properties, while the lattice climber attack demonstrates superior performance with theoretical backing.

Conclusion: The lattice climber attack provides a robust and principled method for attacking finite mixtures of classifiers, addressing limitations of existing approaches.

Abstract: Finite mixtures of classifiers (a.k.a. randomized ensembles) have been
proposed as a way to improve robustness against adversarial attacks. However,
existing attacks have been shown to not suit this kind of classifier. In this
paper, we discuss the problem of attacking a mixture in a principled way and
introduce two desirable properties of attacks based on a geometrical analysis
of the problem (effectiveness and maximality). We then show that existing
attacks do not meet both of these properties. Finally, we introduce a new
attack called {\em lattice climber attack} with theoretical guarantees in the
binary linear setting, and demonstrate its performance by conducting
experiments on synthetic and real datasets.

</details>


### [386] [NoLoCo: No-all-reduce Low Communication Training Method for Large Models](https://arxiv.org/pdf/2506.10911)
*Jari Kolehmainen, Nikolay Blagoev, John Donaghy, Oğuzhan Ersoy, Christopher Nies*

Main category: cs.LG

TL;DR: NoLoCo is a novel optimization method for training large language models without explicit synchronization, reducing communication overhead and improving convergence rates.


<details>
  <summary>Details</summary>
Motivation: Training large models on clusters with high-bandwidth interconnects is costly and impractical. Existing low-communication methods still require costly synchronization steps.

Method: NoLoCo avoids collective communication by implicitly synchronizing weights via a Nesterov momentum variant, partially averaging with a randomly selected model.

Result: NoLoCo reduces communication overhead significantly, outperforms DiLoCo in speed (up to 4% faster convergence), and eliminates global blocking communication.

Conclusion: NoLoCo offers a scalable, efficient alternative for training large models with minimal communication, making it practical for low-bandwidth networks.

Abstract: Training large language models is generally done via optimization methods on
clusters containing tens of thousands of accelerators, communicating over a
high-bandwidth interconnect. Scaling up these clusters is expensive and can
become impractical, imposing limits on the size of models that can be trained.
Several recent studies have proposed training methods that are less
communication intensive, avoiding the need for a highly connected compute
cluster. These state-of-the-art low communication training methods still employ
a synchronization step for model parameters, which, when performed over all
model replicas, can become costly on a low-bandwidth network.
  In this work, we propose a novel optimization method, NoLoCo, that does not
explicitly synchronize all model parameters during training and, as a result,
does not require any collective communication. NoLoCo implicitly synchronizes
model weights via a novel variant of the Nesterov momentum optimizer by
partially averaging model weights with a randomly selected other one. We
provide both a theoretical convergence analysis for our proposed optimizer as
well as empirical results from language model training.
  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,
between 125M to 6.8B parameters. Our method requires significantly less
communication overhead than fully sharded data parallel training or even widely
used low communication training method, DiLoCo. The synchronization step itself
is estimated to be one magnitude faster than the all-reduce used in DiLoCo for
few hundred accelerators training over the internet. We also do not have any
global blocking communication that reduces accelerator idling time. Compared to
DiLoCo, we also observe up to $4\%$ faster convergence rate with wide range of
model sizes and accelerator counts.

</details>


### [387] [Foundation Models for Causal Inference via Prior-Data Fitted Networks](https://arxiv.org/pdf/2506.10914)
*Yuchen Ma, Dennis Frauen, Emil Javurek, Stefan Feuerriegel*

Main category: cs.LG

TL;DR: CausalFM introduces a framework for training PFN-based foundation models for causal inference, using Bayesian priors derived from structural causal models, and demonstrates competitive performance in CATE estimation.


<details>
  <summary>Details</summary>
Motivation: To address the need for a principled and versatile approach to causal inference, leveraging prior-data fitted networks (PFNs) and Bayesian methods.

Method: Formalizes Bayesian priors for causal inference using structural causal models (SCMs), proposes causality-inspired Bayesian neural networks, and trains a foundation model for CATE estimation via back-door adjustment.

Result: CausalFM performs competitively in CATE estimation on synthetic and semi-synthetic benchmarks.

Conclusion: CausalFM presents a novel paradigm for causal inference, with potential applications in medicine, economics, and other fields.

Abstract: Prior-data fitted networks (PFNs) have recently been proposed as a promising
way to train tabular foundation models. PFNs are transformers that are
pre-trained on synthetic data generated from a prespecified prior distribution
and that enable Bayesian inference through in-context learning. In this paper,
we introduce CausalFM, a comprehensive framework for training PFN-based
foundation models in various causal inference settings. First, we formalize the
construction of Bayesian priors for causal inference based on structural causal
models (SCMs) in a principled way and derive necessary criteria for the
validity of such priors. Building on this, we propose a novel family of prior
distributions using causality-inspired Bayesian neural networks that enable
CausalFM to perform Bayesian causal inference in various settings, including
back-door, front-door, and instrumental variable adjustment. Finally, we
instantiate CausalFM and explicitly train a foundation model for estimating
conditional average treatment effects (CATEs) using back-door adjustment. We
show that CausalFM performs competitively for CATE estimation using various
synthetic and semi-synthetic benchmarks. In sum, our framework can be used as a
general recipe to train foundation models for various causal inference
settings. In contrast to the current state-of-the-art in causal inference,
CausalFM offers a novel paradigm with the potential to fundamentally change how
practitioners perform causal inference in medicine, economics, and other
disciplines.

</details>


### [388] [Sequential-Parallel Duality in Prefix Scannable Models](https://arxiv.org/pdf/2506.10918)
*Morris Yau, Sharut Gupta, Valerie Engelmayer, Kazuki Irie, Stefanie Jegelka, Jacob Andreas*

Main category: cs.LG

TL;DR: The paper explores neural sequence models with sequential-parallel duality, introducing Prefix-Scannable Models (PSMs) that unify existing architectures and offer efficient computation and memory usage.


<details>
  <summary>Details</summary>
Motivation: To characterize and generalize neural sequence models that support fast parallel evaluation and efficient sequential inference, unifying diverse architectures like state space models and transformers.

Method: Defines PSMs by relaxing state aggregation operators, enabling non-associative functions like softmax attention. Evaluates on language modeling and synthetic tasks.

Result: PSMs unify models like Mamba and GLA, achieving O(1) compute per token and log(N) memory, with competitive performance and better length generalization.

Conclusion: PSMs bridge expressivity and efficiency, outperforming transformers and state space models in some cases, offering a versatile framework for sequence modeling.

Abstract: Modern neural sequence models are designed to meet the dual mandate of
parallelizable training and fast sequential inference. Recent developments have
given rise to various models, such as Gated Linear Attention (GLA) and Mamba,
that achieve such ``sequential-parallel duality.'' This raises a natural
question: can we characterize the full class of neural sequence models that
support near-constant-time parallel evaluation and linear-time, constant-space
sequential inference? We begin by describing a broad class of such models --
state space models -- as those whose state updates can be computed using the
classic parallel prefix scan algorithm with a custom associative aggregation
operator. We then define a more general class, Prefix-Scannable Models (PSMs),
by relaxing the state aggregation operator to allow arbitrary (potentially
non-associative) functions such as softmax attention. This generalization
unifies many existing architectures, including element-wise RNNs (e.g., Mamba)
and linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new
models with softmax-like operators that achieve O(1) amortized compute per
token and log(N) memory for sequence length N. We empirically evaluate such
models on illustrative small-scale language modeling and canonical synthetic
tasks, including state tracking and associative recall. Empirically, we find
that PSMs retain the expressivity of transformer-based architectures while
matching the inference efficiency of state space models -- in some cases
exhibiting better length generalization than either.

</details>


### [389] [Developing a High-performance Framework for Speech Emotion Recognition in Naturalistic Conditions Challenge for Emotional Attribute Prediction](https://arxiv.org/pdf/2506.10930)
*Thanathai Lertpetchpun, Tiantian Feng, Dani Byrd, Shrikanth Narayanan*

Main category: cs.LG

TL;DR: A reproducible framework for speech emotion recognition (SER) achieves top performance in the IS25-SER Challenge by addressing labeling disagreements and imbalanced data through multimodal and multi-task learning.


<details>
  <summary>Details</summary>
Motivation: The challenges of SER in naturalistic conditions, such as annotator disagreement and imbalanced data, motivate the development of a robust framework.

Method: The system uses multimodal learning, multi-task learning, and imbalanced data handling, incorporating text embeddings, gender prediction, and additional samples (O and X) in training.

Result: The framework secured first and second places in the IS25-SER Challenge, with top performance from a simple two-system ensemble.

Conclusion: The proposed framework effectively addresses SER challenges, demonstrating superior performance in naturalistic conditions.

Abstract: Speech emotion recognition (SER) in naturalistic conditions presents a
significant challenge for the speech processing community. Challenges include
disagreement in labeling among annotators and imbalanced data distributions.
This paper presents a reproducible framework that achieves superior (top 1)
performance in the Emotion Recognition in Naturalistic Conditions Challenge
(IS25-SER Challenge) - Task 2, evaluated on the MSP-Podcast dataset. Our system
is designed to tackle the aforementioned challenges through multimodal
learning, multi-task learning, and imbalanced data handling. Specifically, our
best system is trained by adding text embeddings, predicting gender, and
including ``Other'' (O) and ``No Agreement'' (X) samples in the training set.
Our system's results secured both first and second places in the IS25-SER
Challenge, and the top performance was achieved by a simple two-system
ensemble.

</details>


### [390] [Self-Adapting Language Models](https://arxiv.org/pdf/2506.10943)
*Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, Pulkit Agrawal*

Main category: cs.LG

TL;DR: SEAL enables LLMs to self-adapt by generating their own finetuning data and update directives, leading to persistent weight updates through supervised finetuning.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack mechanisms to adapt their weights dynamically to new tasks or knowledge, limiting their flexibility.

Method: SEAL uses self-generated finetuning data and directives, trained via reinforcement learning with downstream performance as the reward.

Result: Experiments show SEAL improves knowledge incorporation and few-shot generalization.

Conclusion: SEAL is a promising approach for self-directed adaptation in language models.

Abstract: Large language models (LLMs) are powerful but static; they lack mechanisms to
adapt their weights in response to new tasks, knowledge, or examples. We
introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to
self-adapt by generating their own finetuning data and update directives. Given
a new input, the model produces a self-edit-a generation that may restructure
the information in different ways, specify optimization hyperparameters, or
invoke tools for data augmentation and gradient-based updates. Through
supervised finetuning (SFT), these self-edits result in persistent weight
updates, enabling lasting adaptation. To train the model to produce effective
self-edits, we use a reinforcement learning loop with the downstream
performance of the updated model as the reward signal. Unlike prior approaches
that rely on separate adaptation modules or auxiliary networks, SEAL directly
uses the model's own generation to control its adaptation process. Experiments
on knowledge incorporation and few-shot generalization show that SEAL is a
promising step toward language models capable of self-directed adaptation. Our
website and code is available at https://jyopari.github.io/posts/seal.

</details>


### [391] [Execution Guided Line-by-Line Code Generation](https://arxiv.org/pdf/2506.10948)
*Boaz Lavon, Shahar Katz, Lior Wolf*

Main category: cs.LG

TL;DR: A novel method, Execution-Guided Classifier-Free Guidance (EG-CFG), improves neural code generation by incorporating real-time execution feedback during inference, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) for code generation lack execution feedback during inference, a key signal used by human programmers.

Method: EG-CFG uses beam search to sample candidate completions, executes them against test cases, and integrates feedback into prompts line-by-line.

Result: EG-CFG significantly outperforms standard methods, excelling in tasks from foundational to competitive programming.

Conclusion: EG-CFG effectively leverages execution signals for better code generation, supporting parallelism and diverse reasoning paths.

Abstract: We present a novel approach to neural code generation that incorporates
real-time execution signals into the language model generation process. While
large language models (LLMs) have demonstrated impressive code generation
capabilities, they typically do not utilize execution feedback during
inference, a critical signal that human programmers regularly leverage. Our
method, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically
incorporates execution signals as the model generates code, providing
line-by-line feedback that guides the generation process toward executable
solutions. EG-CFG employs a multi-stage process: first, we conduct beam search
to sample candidate program completions for each line; second, we extract
execution signals by executing these candidates against test cases; and
finally, we incorporate these signals into the prompt during generation. By
maintaining consistent signals across tokens within the same line and
refreshing signals at line boundaries, our approach provides coherent guidance
while preserving syntactic structure. Moreover, the method naturally supports
native parallelism at the task level in which multiple agents operate in
parallel, exploring diverse reasoning paths and collectively generating a broad
set of candidate solutions. Our experiments across diverse coding tasks
demonstrate that EG-CFG significantly improves code generation performance
compared to standard approaches, achieving state-of-the-art results across
various levels of complexity, from foundational problems to challenging
competitive programming tasks. Our code is available at:
https://github.com/boazlavon/eg_cfg

</details>


### [392] [ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems](https://arxiv.org/pdf/2506.10955)
*Aayush Karan, Kulin Shah, Sitan Chen*

Main category: cs.LG

TL;DR: ReGuidance is a wrapper that improves sample realism and reward in diffusion-based inverse problem solving, outperforming baselines in hard tasks like in-painting and super-resolution.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DPS fail in low signal-to-noise scenarios, producing unrealistic outputs. ReGuidance aims to enhance both realism and reward.

Method: ReGuidance inverts a candidate solution using the unconditional probability flow ODE, then reinitializes DPS with the resulting latent.

Result: ReGuidance significantly improves sample quality and measurement consistency in hard tasks, outperforming state-of-the-art baselines.

Conclusion: ReGuidance provides a theoretical guarantee for DPS, boosting reward and realism, marking a first in rigorous algorithmic guarantees for such methods.

Abstract: There has been a flurry of activity around using pretrained diffusion models
as informed data priors for solving inverse problems, and more generally around
steering these models using reward models. Training-free methods like diffusion
posterior sampling (DPS) and its many variants have offered flexible heuristic
algorithms for these tasks, but when the reward is not informative enough,
e.g., in hard inverse problems with low signal-to-noise ratio, these techniques
veer off the data manifold, failing to produce realistic outputs. In this work,
we devise a simple wrapper, ReGuidance, for boosting both the sample realism
and reward achieved by these methods. Given a candidate solution $\hat{x}$
produced by an algorithm of the user's choice, we propose inverting the
solution by running the unconditional probability flow ODE in reverse starting
from $\hat{x}$, and then using the resulting latent as an initialization for
DPS. We evaluate our wrapper on hard inverse problems like large box
in-painting and super-resolution with high upscaling. Whereas state-of-the-art
baselines visibly fail, we find that applying our wrapper on top of these
baselines significantly boosts sample quality and measurement consistency. We
complement these findings with theory proving that on certain multimodal data
distributions, ReGuidance simultaneously boosts the reward and brings the
candidate solution closer to the data manifold. To our knowledge, this
constitutes the first rigorous algorithmic guarantee for DPS.

</details>


### [393] [Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods](https://arxiv.org/pdf/2506.10959)
*Zhaiming Shen, Alexander Hsu, Rongjie Lai, Wenjing Liao*

Main category: cs.LG

TL;DR: The paper theoretically analyzes in-context learning (ICL) for regression of Hölder functions on manifolds, linking attention mechanisms to kernel methods and deriving generalization error bounds.


<details>
  <summary>Details</summary>
Motivation: Despite ICL's success in NLP and vision, its theoretical understanding for structured geometric data is lacking. This work aims to bridge that gap.

Method: The study connects attention mechanisms to kernel methods, deriving error bounds based on prompt length and training tasks.

Result: Transformers achieve minimax regression rates for Hölder functions on manifolds, scaling with intrinsic dimension, not ambient space.

Conclusion: The work provides foundational insights into geometry's role in ICL and tools for studying nonlinear ICL models.

Abstract: While in-context learning (ICL) has achieved remarkable success in natural
language and vision domains, its theoretical understanding--particularly in the
context of structured geometric data--remains unexplored. In this work, we
initiate a theoretical study of ICL for regression of H\"older functions on
manifolds. By establishing a novel connection between the attention mechanism
and classical kernel methods, we derive generalization error bounds in terms of
the prompt length and the number of training tasks. When a sufficient number of
training tasks are observed, transformers give rise to the minimax regression
rate of H\"older functions on manifolds, which scales exponentially with the
intrinsic dimension of the manifold, rather than the ambient space dimension.
Our result also characterizes how the generalization error scales with the
number of training tasks, shedding light on the complexity of transformers as
in-context algorithm learners. Our findings provide foundational insights into
the role of geometry in ICL and novels tools to study ICL of nonlinear models.

</details>


### [394] [Farseer: A Refined Scaling Law in Large Language Models](https://arxiv.org/pdf/2506.10972)
*Houyi Li, Wenzhen Zheng, Qiufeng Wang, Zhenyu Ding, Haoying Wang, Zili Wang, Shijie Xuyang, Ning Ding, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang*

Main category: cs.LG

TL;DR: Farseer introduces a refined scaling law for LLMs, improving predictive accuracy and reducing extrapolation error by 433% compared to Chinchilla's law, enabling reliable large-scale performance predictions.


<details>
  <summary>Details</summary>
Motivation: Address the high cost and inefficiency of scaling insights from small LLM experiments to production systems.

Method: Systematically constructs a model loss surface L(N,D) for better empirical fit and extrapolation.

Result: Achieves superior accuracy and generalizability, validated by training 1,000 LLMs using 3M GPU hours.

Conclusion: Farseer enhances scaling predictions and compute allocation, with open-sourced models and data to support further research.

Abstract: Training Large Language Models (LLMs) is prohibitively expensive, creating a
critical scaling gap where insights from small-scale experiments often fail to
transfer to resource-intensive production systems, thereby hindering efficient
innovation. To bridge this, we introduce Farseer, a novel and refined scaling
law offering enhanced predictive accuracy across scales. By systematically
constructing a model loss surface $L(N,D)$, Farseer achieves a significantly
better fit to empirical data than prior laws (e.g., Chinchilla's law). Our
methodology yields accurate, robust, and highly generalizable predictions,
demonstrating excellent extrapolation capabilities, improving upon Chinchilla's
law by reducing extrapolation error by 433\%. This allows for the reliable
evaluation of competing training strategies across all $(N,D)$ settings,
enabling conclusions from small-scale ablation studies to be confidently
extrapolated to predict large-scale performance. Furthermore, Farseer provides
new insights into optimal compute allocation, better reflecting the nuanced
demands of modern LLM training. To validate our approach, we trained an
extensive suite of approximately 1,000 LLMs across diverse scales and
configurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are
comprehensively open-sourcing all models, data, results, and logs at
https://github.com/Farseer-Scaling-Law/Farseer to foster further research.

</details>


### [395] [Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning](https://arxiv.org/pdf/2506.10973)
*Julius Berner, Miguel Liu-Schiaffini, Jean Kossaifi, Valentin Duruisseaux, Boris Bonev, Kamyar Azizzadenesheli, Anima Anandkumar*

Main category: cs.LG

TL;DR: The paper introduces neural operators as a way to extend neural networks to function spaces, enabling their application to scientific problems like PDEs. It provides a practical guide for converting existing neural architectures into neural operators.


<details>
  <summary>Details</summary>
Motivation: Deep learning excels in finite-dimensional spaces (e.g., computer vision) but struggles with infinite-dimensional function spaces common in scientific problems. Neural operators bridge this gap.

Method: The paper distills principles for mapping function spaces and proposes a recipe to adapt popular neural architectures into neural operators with minimal changes.

Result: Neural operators can learn solution operators for PDEs, handling varying boundary conditions, coefficients, and geometries.

Conclusion: The work provides a practical framework for implementing neural operators, extending deep learning's success to scientific applications.

Abstract: A wide range of scientific problems, such as those described by
continuous-time dynamical systems and partial differential equations (PDEs),
are naturally formulated on function spaces. While function spaces are
typically infinite-dimensional, deep learning has predominantly advanced
through applications in computer vision and natural language processing that
focus on mappings between finite-dimensional spaces. Such fundamental
disparities in the nature of the data have limited neural networks from
achieving a comparable level of success in scientific applications as seen in
other fields. Neural operators are a principled way to generalize neural
networks to mappings between function spaces, offering a pathway to replicate
deep learning's transformative impact on scientific problems. For instance,
neural operators can learn solution operators for entire classes of PDEs, e.g.,
physical systems with different boundary conditions, coefficient functions, and
geometries. A key factor in deep learning's success has been the careful
engineering of neural architectures through extensive empirical testing.
Translating these neural architectures into neural operators allows operator
learning to enjoy these same empirical optimizations. However, prior neural
operator architectures have often been introduced as standalone models, not
directly derived as extensions of existing neural network architectures. In
this paper, we identify and distill the key principles for constructing
practical implementations of mappings between infinite-dimensional function
spaces. Using these principles, we propose a recipe for converting several
popular neural architectures into neural operators with minimal modifications.
This paper aims to guide practitioners through this process and details the
steps to make neural operators work in practice. Our code can be found at
https://github.com/neuraloperator/NNs-to-NOs

</details>


### [396] [Rethinking Losses for Diffusion Bridge Samplers](https://arxiv.org/pdf/2506.10982)
*Sebastian Sanokowski, Lukas Gruber, Christoph Bartmann, Sepp Hochreiter, Sebastian Lehner*

Main category: cs.LG

TL;DR: The paper compares Log Variance (LV) and reverse Kullback-Leibler (rKL) losses in diffusion bridges, showing rKL with log-derivative trick (rKL-LD) outperforms LV in performance, stability, and hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LV loss in diffusion bridges, especially when diffusion coefficients are learned, and to explore a more theoretically grounded and practical optimization objective.

Method: Analyzes the equivalence and performance of LV and rKL-LD losses in diffusion bridges, using theoretical insights and experimental benchmarks.

Result: rKL-LD loss consistently outperforms LV loss in performance, requires less hyperparameter tuning, and provides more stable training.

Conclusion: For diffusion bridges, rKL-LD is a superior choice over LV due to better performance, stability, and theoretical grounding.

Abstract: Diffusion bridges are a promising class of deep-learning methods for sampling
from unnormalized distributions. Recent works show that the Log Variance (LV)
loss consistently outperforms the reverse Kullback-Leibler (rKL) loss when
using the reparametrization trick to compute rKL-gradients. While the on-policy
LV loss yields identical gradients to the rKL loss when combined with the
log-derivative trick for diffusion samplers with non-learnable forward
processes, this equivalence does not hold for diffusion bridges or when
diffusion coefficients are learned. Based on this insight we argue that for
diffusion bridges the LV loss does not represent an optimization objective that
can be motivated like the rKL loss via the data processing inequality. Our
analysis shows that employing the rKL loss with the log-derivative trick
(rKL-LD) does not only avoid these conceptual problems but also consistently
outperforms the LV loss. Experimental results with different types of diffusion
bridges on challenging benchmarks show that samplers trained with the rKL-LD
loss achieve better performance. From a practical perspective we find that
rKL-LD requires significantly less hyperparameter optimization and yields more
stable training behavior.

</details>


### [397] [Noise Balance and Stationary Distribution of Stochastic Gradient Descent](https://arxiv.org/pdf/2308.06671)
*Liu Ziyin, Hongchao Li, Masahito Ueda*

Main category: cs.LG

TL;DR: SGD's minibatch noise regularizes solutions toward noise-balanced outcomes due to loss function symmetries, revealing deep networks' unique nonlinear phenomena.


<details>
  <summary>Details</summary>
Motivation: Understanding how SGD navigates neural networks' complex loss landscapes, particularly the role of minibatch noise and symmetries.

Method: Analyzing SGD dynamics with rescaling parameter symmetry, deriving stationary distributions for stochastic gradient flow in deep linear networks.

Result: Reveals phase transitions, broken ergodicity, and fluctuation inversion in deep networks, distinguishing them from shallow models.

Conclusion: Loss function symmetries are key to understanding SGD, and deep networks exhibit unique nonlinear behaviors not found in shallow ones.

Abstract: The stochastic gradient descent (SGD) algorithm is the algorithm we use to
train neural networks. However, it remains poorly understood how the SGD
navigates the highly nonlinear and degenerate loss landscape of a neural
network. In this work, we show that the minibatch noise of SGD regularizes the
solution towards a noise-balanced solution whenever the loss function contains
a rescaling parameter symmetry. Because the difference between a simple
diffusion process and SGD dynamics is the most significant when symmetries are
present, our theory implies that the loss function symmetries constitute an
essential probe of how SGD works. We then apply this result to derive the
stationary distribution of stochastic gradient flow for a diagonal linear
network with arbitrary depth and width. The stationary distribution exhibits
complicated nonlinear phenomena such as phase transitions, broken ergodicity,
and fluctuation inversion. These phenomena are shown to exist uniquely in deep
networks, implying a fundamental difference between deep and shallow models.

</details>


### [398] [Improved Algorithm for Deep Active Learning under Imbalance via Optimal Separation](https://arxiv.org/pdf/2312.09196)
*Shyam Nuggehalli, Jifan Zhang, Lalit Jain, Robert Nowak*

Main category: cs.LG

TL;DR: DIRECT is an active learning algorithm that addresses class imbalance and label noise by strategically selecting uncertain examples near class boundaries, reducing annotation costs by over 60% compared to other methods.


<details>
  <summary>Details</summary>
Motivation: Class imbalance severely impacts machine learning performance on minority classes, and existing solutions lack robustness to label noise. Active learning offers a potential fix by balancing and optimizing labeled data collection.

Method: DIRECT identifies class separation boundaries and selects uncertain nearby examples for annotation, reducing the problem to one-dimensional active learning. It handles batch labeling and label noise effectively.

Result: DIRECT reduces annotation costs by over 60% compared to state-of-the-art active learning methods and over 80% versus random sampling, while maintaining robustness to label noise.

Conclusion: DIRECT provides a cost-effective and robust solution for active learning under class imbalance and label noise, outperforming existing methods.

Abstract: Class imbalance severely impacts machine learning performance on minority
classes in real-world applications. While various solutions exist, active
learning offers a fundamental fix by strategically collecting balanced,
informative labeled examples from abundant unlabeled data. We introduce DIRECT,
an algorithm that identifies class separation boundaries and selects the most
uncertain nearby examples for annotation. By reducing the problem to
one-dimensional active learning, DIRECT leverages established theory to handle
batch labeling and label noise -- another common challenge in data annotation
that particularly affects active learning methods. Our work presents the first
comprehensive study of active learning under both class imbalance and label
noise. Extensive experiments on imbalanced datasets show DIRECT reduces
annotation costs by over 60\% compared to state-of-the-art active learning
methods and over 80\% versus random sampling, while maintaining robustness to
label noise.

</details>


### [399] [Near-Optimal Algorithms for Constrained k-Center Clustering with Instance-level Background Knowledge](https://arxiv.org/pdf/2401.12533)
*Longkun Guo, Chaoqi Jia, Kewen Liao, Zhigang Lu, Minhui Xue*

Main category: cs.LG

TL;DR: The paper introduces an efficient approximation algorithm for constrained k-center clustering, achieving the best possible ratio of 2, and validates its advantages empirically.


<details>
  <summary>Details</summary>
Motivation: Improving clustering results by leveraging background knowledge (must-link and cannot-link constraints) in k-center clustering, despite its NP-hard nature.

Method: Uses techniques like reverse dominating sets, LP integral polyhedron, and LP duality to develop the algorithm.

Result: Achieves a 2-approximation ratio, outperforming baseline algorithms in clustering cost, quality, and runtime.

Conclusion: The proposed algorithm is theoretically sound and practically effective for constrained k-center clustering.

Abstract: Center-based clustering has attracted significant research interest from both
theory and practice. In many practical applications, input data often contain
background knowledge that can be used to improve clustering results. In this
work, we build on widely adopted $k$-center clustering and model its input
background knowledge as must-link (ML) and cannot-link (CL) constraint sets.
However, most clustering problems including $k$-center are inherently
$\mathcal{NP}$-hard, while the more complex constrained variants are known to
suffer severer approximation and computation barriers that significantly limit
their applicability. By employing a suite of techniques including reverse
dominating sets, linear programming (LP) integral polyhedron, and LP duality,
we arrive at the first efficient approximation algorithm for constrained
$k$-center with the best possible ratio of 2. We also construct competitive
baseline algorithms and empirically evaluate our approximation algorithm
against them on a variety of real datasets. The results validate our
theoretical findings and demonstrate the great advantages of our algorithm in
terms of clustering cost, clustering quality, and running time.

</details>


### [400] [Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance](https://arxiv.org/pdf/2402.08680)
*Linxi Zhao, Yihe Deng, Weitong Zhang, Quanquan Gu*

Main category: cs.LG

TL;DR: MARINE is a training-free, API-free framework to reduce hallucinations in LVLMs by using image-grounded guidance from open-source vision models.


<details>
  <summary>Details</summary>
Motivation: Address LVLMs' tendency to hallucinate objects without costly training or proprietary LLMs.

Method: Leverages open-source vision models for object-level guidance during inference.

Result: Outperforms fine-tuning methods, reduces hallucinations, and maintains generation detail.

Conclusion: MARINE offers an efficient, flexible solution to mitigate LVLM hallucinations.

Abstract: The advancement of Large Vision-Language Models (LVLMs) has increasingly
highlighted the critical issue of their tendency to hallucinate non-existing
objects in the images. To address this issue, previous works focused on using
specially curated datasets or powerful LLMs to rectify the outputs of LVLMs.
However, these approaches require either costly training or fine-tuning, or API
access to proprietary LLMs for post-generation correction. In response to these
limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE
(MARINE), a framework that is both training-free and API-free. MARINE
effectively and efficiently reduces object hallucinations during inference by
introducing image-grounded guidance to LVLMs. This is achieved by leveraging
open-source vision models to extract object-level information, thereby
enhancing the precision of LVLM-generated content. Our framework's flexibility
further allows for the integration of multiple vision models, enabling more
reliable and robust object-level guidance. Through comprehensive evaluations
across 5 popular LVLMs with diverse evaluation metrics and benchmarks, we
demonstrate the effectiveness of MARINE, which even outperforms existing
fine-tuning-based methods. Remarkably, it reduces hallucinations consistently
in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs'
generations. We release our code at https://github.com/Linxi-ZHAO/MARINE.

</details>


### [401] [TimeBridge: Better Diffusion Prior Design with Bridge Models for Time Series Generation](https://arxiv.org/pdf/2408.06672)
*Jinseong Park, Seungyun Lee, Woojin Jeong, Yujin Choi, Jaewook Lee*

Main category: cs.LG

TL;DR: TimeBridge, a diffusion-based framework, improves time series generation by using flexible priors tailored to data properties, outperforming standard diffusion models.


<details>
  <summary>Details</summary>
Motivation: Standard-Gaussian diffusion priors may not suit general time series data, necessitating a more adaptable approach.

Method: TimeBridge uses diffusion bridges to learn paths between a chosen prior and the data distribution, with tailored prior designs for unconditional and conditional generation.

Result: Experiments show TimeBridge with data-driven priors outperforms standard diffusion models.

Conclusion: TimeBridge offers a flexible and effective solution for time series generation by addressing limitations of fixed priors.

Abstract: Time series generation is widely used in real-world applications such as
simulation, data augmentation, and hypothesis testing. Recently, diffusion
models have emerged as the de facto approach to time series generation,
enabling diverse synthesis scenarios. However, the fixed standard-Gaussian
diffusion prior may be ill-suited for general time series data, such as
temporal order and fixed points. In this paper, we propose TimeBridge, a
framework that flexibly synthesizes time series data by using diffusion bridges
to learn paths between a chosen prior and the data distribution. We then
explore several prior designs tailored to time series synthesis. Our framework
covers (i) data- and time-dependent priors for unconditional generation and
(ii) scale-preserving priors for conditional generation. Experiments show that
our framework with data-driven priors outperforms standard diffusion models on
time series generation.

</details>


### [402] [M3-JEPA: Multimodal Alignment via Multi-gate MoE based on the Joint-Predictive Embedding Architecture](https://arxiv.org/pdf/2409.05929)
*Hongyang Lei, Xiaolong Cheng, Qi Qin, Dan Wang, Kun Fan, Huazhen Huang, Qingqing Gu, Yetao Wu, Zhonglin Jiang, Yong Chen, Luo Ji*

Main category: cs.LG

TL;DR: M3-JEPA, a multimodal learning framework using JEPA and MMoE, avoids modality collapse and achieves state-of-the-art performance across tasks and domains.


<details>
  <summary>Details</summary>
Motivation: Address modality collapse in current multimodal learning by optimizing in latent space.

Method: Uses JEPA with MMoE for cross-modal alignment, disentangling modality-specific/shared info, and employs contrastive/regularization loss with AGD.

Result: Achieves SOTA performance, generalizes to unseen data, and is computationally efficient.

Conclusion: M3-JEPA could be a new basis for self-supervised learning in open-world scenarios.

Abstract: Current multimodal learning strategies primarily optimize in the original
token space. Such a framework is easy to incorporate with the backbone of
pretrained language model, but might result in modality collapse. To alleviate
such issues, we leverage the joint embedding predictive architecture (JEPA) on
the multimodal tasks, which converts the input embedding into the output
embedding space by a predictor and then conducts the cross-modal alignment on
the latent space. We implement this predictor by a Multi-Gate Mixture of
Experts (MMoE) and name the framework as M3-JEPA, accordingly. The gating
function disentangles the modality-specific and shared information and derives
information-theoretic optimality. The framework is implemented with both
contrastive and regularization loss, and solved by alternative gradient descent
(AGD) between different multimodal tasks. By thoroughly designed experiments,
we show that M3-JEPA can obtain state-of-the-art performance on different
modalities and tasks, generalize to unseen datasets and domains, and is
computationally efficient in both training and inference. Our observation
suggests that M3-JEPA might become a new basis to self-supervised learning in
the open world.

</details>


### [403] [Neural Networks Generalize on Low Complexity Data](https://arxiv.org/pdf/2409.12446)
*Sourav Chatterjee, Timothy Sudijono*

Main category: cs.LG

TL;DR: Feedforward neural networks with ReLU activation generalize on low-complexity data, defined via a simple programming language. The MDL network interpolating such data generalizes well, demonstrated on tasks like primality testing.


<details>
  <summary>Details</summary>
Motivation: To understand how neural networks generalize on structured, low-complexity data and explore the role of minimum description length (MDL) in learning.

Method: Define a simple programming language and MDL for networks. Train MDL networks on i.i.d. data (e.g., primality testing) and analyze generalization.

Result: MDL networks generalize well, e.g., accurately predicting primality with high probability. Extensions to noisy data suggest tempered overfitting.

Conclusion: MDL-based neural networks can effectively generalize on structured tasks, even discovering underlying patterns like primality without explicit design.

Abstract: We show that feedforward neural networks with ReLU activation generalize on
low complexity data, suitably defined. Given i.i.d.~data generated from a
simple programming language, the minimum description length (MDL) feedforward
neural network which interpolates the data generalizes with high probability.
We define this simple programming language, along with a notion of description
length of such networks. We provide several examples on basic computational
tasks, such as checking primality of a natural number. For primality testing,
our theorem shows the following and more. Suppose that we draw an i.i.d.~sample
of $n$ numbers uniformly at random from $1$ to $N$. For each number $x_i$, let
$y_i = 1$ if $x_i$ is a prime and $0$ if it is not. Then, the interpolating MDL
network accurately answers, with probability $1- O((\ln N)/n)$, whether a newly
drawn number between $1$ and $N$ is a prime or not. Note that the network is
not designed to detect primes; minimum description learning discovers a network
which does so. Extensions to noisy data are also discussed, suggesting that MDL
neural network interpolators can demonstrate tempered overfitting.

</details>


### [404] [Failure Modes of LLMs for Causal Reasoning on Narratives](https://arxiv.org/pdf/2410.23884)
*Khurram Yamin, Shantanu Gupta, Gaurav R. Ghosal, Zachary C. Lipton, Bryan Wilder*

Main category: cs.LG

TL;DR: The paper examines how large language models (LLMs) perform in causal reasoning tasks, identifying key weaknesses like reliance on event order, struggles with long narratives, and overuse of parametric knowledge. It suggests generating causal graphs as a potential improvement.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the causal reasoning capabilities of LLMs by identifying their failure modes in inferring causal relationships from narratives.

Method: The study uses synthetic and real-world narratives to test LLMs, analyzing their reliance on event order, parametric knowledge, and performance in long-term reasoning.

Result: LLMs often fail due to shortcuts like topological event order, struggle with long narratives, and prioritize parametric knowledge over narrative reasoning. Generating causal graphs helps improve performance.

Conclusion: The findings highlight specific weaknesses in LLMs' causal reasoning and suggest directions for future improvements, such as using causal graphs.

Abstract: In this work, we investigate the causal reasoning abilities of large language
models (LLMs) through the representative problem of inferring causal
relationships from narratives. We find that even state-of-the-art language
models rely on unreliable shortcuts, both in terms of the narrative
presentation and their parametric knowledge. For example, LLMs tend to
determine causal relationships based on the topological ordering of events
(i.e., earlier events cause later ones), resulting in lower performance
whenever events are not narrated in their exact causal order. Similarly, we
demonstrate that LLMs struggle with long-term causal reasoning and often fail
when the narratives are long and contain many events. Additionally, we show
LLMs appear to rely heavily on their parametric knowledge at the expense of
reasoning over the provided narrative. This degrades their abilities whenever
the narrative opposes parametric knowledge. We extensively validate these
failure modes through carefully controlled synthetic experiments, as well as
evaluations on real-world narratives. Finally, we observe that explicitly
generating a causal graph generally improves performance while naive
chain-of-thought is ineffective. Collectively, our results distill precise
failure modes of current state-of-the-art models and can pave the way for
future techniques to enhance causal reasoning in LLMs.

</details>


### [405] [Engagement-Driven Content Generation with Large Language Models](https://arxiv.org/pdf/2411.13187)
*Erica Coppolillo, Federico Cinus, Marco Minici, Francesco Bonchi, Giuseppe Manco*

Main category: cs.LG

TL;DR: The paper explores LLMs' ability to maximize user engagement on social networks using reinforcement learning and simulated feedback, demonstrating their potential in complex social dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' persuasive impact in social networks, where interconnected users and opinion dynamics complicate engagement.

Method: A reinforcement learning pipeline with simulated feedback, using an engagement model to simulate network responses efficiently.

Result: LLMs effectively generate engagement-optimized content, adaptable to network opinion distributions and engagement models.

Conclusion: The framework showcases LLMs' potential for engagement-driven content generation in computational social science.

Abstract: Large Language Models (LLMs) demonstrate significant persuasive capabilities
in one-on-one interactions, but their influence within social networks, where
interconnected users and complex opinion dynamics pose unique challenges,
remains underexplored. This paper addresses the research question: \emph{Can
LLMs generate meaningful content that maximizes user engagement on social
networks?}
  To answer this, we propose a pipeline using reinforcement learning with
simulated feedback, where the network's response to LLM-generated content
(i.e., the reward) is simulated through a formal engagement model. This
approach bypasses the temporal cost and complexity of live experiments,
enabling an efficient feedback loop between the LLM and the network under
study. It also allows to control over endogenous factors such as the LLM's
position within the social network and the distribution of opinions on a given
topic. Our approach is adaptive to the opinion distribution of the underlying
network and agnostic to the specifics of the engagement model, which is
embedded as a plug-and-play component. Such flexibility makes it suitable for
more complex engagement tasks and interventions in computational social
science.
  Using our framework, we analyze the performance of LLMs in generating social
engagement under different conditions, showcasing their full potential in this
task. The experimental code is publicly available at
https://github.com/mminici/Engagement-Driven-Content-Generation.

</details>


### [406] [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/pdf/2502.04313)
*Shashwat Goel, Joschka Struber, Ilze Amanda Auzina, Karuna K Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya Prabhu, Matthias Bethge, Jonas Geiping*

Main category: cs.LG

TL;DR: The paper explores AI oversight using language models (LMs), introducing CAPA to measure model similarity based on mistake overlap. It finds bias in LM-as-a-judge scores favoring similar models and highlights the role of complementary knowledge in weak-to-strong generalization. A concerning trend of increasing mistake similarity with capability growth is noted, emphasizing risks of correlated failures.


<details>
  <summary>Details</summary>
Motivation: As LM capabilities grow, human evaluation and supervision become harder. The study investigates whether LMs can automate these tasks (AI oversight) and how model similarity impacts this process.

Method: Proposes CAPA, a metric for LM similarity based on mistake overlap. Uses CAPA to analyze LM-as-a-judge bias and weak-to-strong generalization in training.

Result: LM-as-a-judge scores favor similar models. Complementary knowledge between weak supervisor and strong student is key for weak-to-strong gains. Mistake similarity increases with capabilities, raising correlated failure risks.

Conclusion: Highlights the need to report and correct for model similarity in AI oversight, especially as capabilities grow and mistakes become more correlated.

Abstract: As Language Model (LM) capabilities advance, evaluating and supervising them
at scale is getting harder for humans. There is hope that other language models
can automate both these tasks, which we refer to as ''AI Oversight''. We study
how model similarity affects both aspects of AI oversight by proposing Chance
Adjusted Probabilistic Agreement (CAPA): a metric for LM similarity based on
overlap in model mistakes. Using CAPA, we first show that LLM-as-a-judge scores
favor models similar to the judge, generalizing recent self-preference results.
Then, we study training on LM annotations, and find complementary knowledge
between the weak supervisor and strong student model plays a crucial role in
gains from ''weak-to-strong generalization''. As model capabilities increase,
it becomes harder to find their mistakes, and we might defer more to AI
oversight. However, we observe a concerning trend -- model mistakes are
becoming more similar with increasing capabilities, pointing to risks from
correlated failures. Our work underscores the importance of reporting and
correcting for model similarity, especially in the emerging paradigm of AI
oversight.

</details>


### [407] [A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce](https://arxiv.org/pdf/2504.11343)
*Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, Hanze Dong*

Main category: cs.LG

TL;DR: GRPO's effectiveness in RL for LLMs is due to discarding entirely incorrect responses, not reward normalization. A simpler method, RAFT, matches GRPO's performance. Reinforce-Rej, filtering both incorrect and correct samples, improves efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: To understand why GRPO works well for fine-tuning LLMs and identify simpler, more effective alternatives.

Method: Analyzed GRPO's components, compared it to RAFT (rejection sampling), and proposed Reinforce-Rej, which filters samples.

Result: RAFT performs competitively with GRPO. Reinforce-Rej improves KL efficiency and stability.

Conclusion: Future work should focus on principled use of negative samples, not indiscriminate reliance. RAFT is a robust baseline.

Abstract: Reinforcement learning (RL) has become a prevailing approach for fine-tuning
large language models (LLMs) on complex reasoning tasks. Among recent methods,
GRPO stands out for its empirical success in training models such as
DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In
this work, we revisit GRPO from a reinforce-like algorithm perspective and
analyze its core components. Surprisingly, we find that a simple rejection
sampling baseline, RAFT, which trains only on positively rewarded samples,
yields competitive performance than GRPO and PPO. Our ablation studies reveal
that GRPO's main advantage arises from discarding prompts with entirely
incorrect responses, rather than from its reward normalization. Motivated by
this insight, we propose Reinforce-Rej, a minimal extension of policy gradient
that filters both entirely incorrect and entirely correct samples.
Reinforce-Rej improves KL efficiency and stability, serving as a lightweight
yet effective alternative to more complex RL algorithms. We advocate RAFT as a
robust and interpretable baseline, and suggest that future advances should
focus on more principled designs for incorporating negative samples, rather
than relying on them indiscriminately. Our findings provide guidance for future
work in reward-based LLM post-training.

</details>


### [408] [Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs](https://arxiv.org/pdf/2412.14218)
*Jiaming Yu, Le Liang, Chongtao Guo, Ziyang Guo, Shi Jin, Geoffrey Ye Li*

Main category: cs.LG

TL;DR: The paper proposes QPMIX, a heterogeneous MARL framework for distributed channel access in wireless networks, improving throughput, fairness, and performance over CSMA/CA.


<details>
  <summary>Details</summary>
Motivation: To address distributed channel access challenges in wireless networks using heterogeneous MARL agents.

Method: QPMIX, a centralized training with distributed execution framework for heterogeneous agents, with theoretical convergence proof.

Result: Improved throughput, delay, jitter, and collision rates; robust in various traffic scenarios.

Conclusion: QPMIX enhances network performance and promotes cooperation among heterogeneous agents.

Abstract: This paper investigates the use of multi-agent reinforcement learning (MARL)
to address distributed channel access in wireless local area networks. In
particular, we consider the challenging yet more practical case where the
agents heterogeneously adopt value-based or policy-based reinforcement learning
algorithms to train the model. We propose a heterogeneous MARL training
framework, named QPMIX, which adopts a centralized training with distributed
execution paradigm to enable heterogeneous agents to collaborate. Moreover, we
theoretically prove the convergence of the proposed heterogeneous MARL method
when using the linear value function approximation. Our method maximizes the
network throughput and ensures fairness among stations, therefore, enhancing
the overall network performance. Simulation results demonstrate that the
proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and
collision rates compared with conventional carrier-sense multiple access with
collision avoidance (CSMA/CA) mechanism in the saturated traffic scenario.
Furthermore, the QPMIX algorithm is robust in unsaturated and delay-sensitive
traffic scenarios. It coexists well with the conventional CSMA/CA mechanism and
promotes cooperation among heterogeneous agents.

</details>


### [409] [SR-Reward: Taking The Path More Traveled](https://arxiv.org/pdf/2501.02330)
*Seyed Mahdi B. Azad, Zahra Padar, Gabriel Kalweit, Joschka Boedecker*

Main category: cs.LG

TL;DR: Proposes SR-Reward, a method for learning reward functions from offline demonstrations without adversarial interactions, improving stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional IRL requires adversarial interactions between reward functions and policies, which can be unstable and inefficient. This work aims to decouple them for better performance.

Method: Uses successor representation (SR) to encode states based on future state visitation under demonstration policies. Introduces negative sampling to mitigate overestimation errors.

Result: Achieves competitive results on D4RL benchmark compared to offline RL and IL methods. Ablation studies highlight SR-Reward's strengths and limitations.

Conclusion: SR-Reward offers a stable, efficient alternative to traditional IRL, with competitive performance and robustness, though its effectiveness depends on data quality.

Abstract: In this paper, we propose a novel method for learning reward functions
directly from offline demonstrations. Unlike traditional inverse reinforcement
learning (IRL), our approach decouples the reward function from the learner's
policy, eliminating the adversarial interaction typically required between the
two. This results in a more stable and efficient training process. Our reward
function, called \textit{SR-Reward}, leverages successor representation (SR) to
encode a state based on expected future states' visitation under the
demonstration policy and transition dynamics. By utilizing the Bellman
equation, SR-Reward can be learned concurrently with most reinforcement
learning (RL) algorithms without altering the existing training pipeline. We
also introduce a negative sampling strategy to mitigate overestimation errors
by reducing rewards for out-of-distribution data, thereby enhancing robustness.
This strategy inherently introduces a conservative bias into RL algorithms that
employ the learned reward. We evaluate our method on the D4RL benchmark,
achieving competitive results compared to offline RL algorithms with access to
true rewards and imitation learning (IL) techniques like behavioral cloning.
Moreover, our ablation studies on data size and quality reveal the advantages
and limitations of SR-Reward as a proxy for true rewards.

</details>


### [410] [Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting](https://arxiv.org/pdf/2502.02797)
*Sunny Sanyal, Hayden Prairie, Rudrajit Das, Ali Kavis, Sujay Sanghavi*

Main category: cs.LG

TL;DR: A sample weighting scheme mitigates catastrophic forgetting during fine-tuning by upweighting easy samples and downweighting hard ones, preserving pre-trained model capabilities.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in fine-tuning when original pre-training data/recipe is unavailable, making existing methods inapplicable.

Method: Propose a sample weighting scheme based on pre-trained model losses, focusing on easy samples to limit drift.

Result: Empirical results show reduced accuracy drop on target tasks (e.g., 0.8% on GSM8K) and better retention on pre-training datasets (5.4% improvement).

Conclusion: The method effectively mitigates forgetting by operating in sample space, complementing existing gradient/parameter-space approaches.

Abstract: Fine-tuning a pre-trained model on a downstream task often degrades its
original capabilities, a phenomenon known as "catastrophic forgetting". This is
especially an issue when one does not have access to the data and recipe used
to develop the pre-trained model. Under this constraint, most existing methods
for mitigating forgetting are inapplicable. To address this challenge, we
propose a sample weighting scheme for the fine-tuning data solely based on the
pre-trained model's losses. Specifically, we upweight the easy samples on which
the pre-trained model's loss is low and vice versa to limit the drift from the
pre-trained model. Our approach is orthogonal and yet complementary to existing
methods; while such methods mostly operate on parameter or gradient space, we
concentrate on the sample space. We theoretically analyze the impact of
fine-tuning with our method in a linear setting, showing that it stalls
learning in a certain subspace which inhibits overfitting to the target task.
We empirically demonstrate the efficacy of our method on both language and
vision tasks. As an example, when fine-tuning Gemma 2 2B on MetaMathQA, our
method results in only a $0.8\%$ drop in accuracy on GSM8K (another math
dataset) compared to standard fine-tuning, while preserving $5.4\%$ more
accuracy on the pre-training datasets. Our code is publicly available at
https://github.com/sanyalsunny111/FLOW_finetuning .

</details>


### [411] [CheMatAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/pdf/2506.07551)
*Mengsong Wu, YaFei Wang, Yidong Ming, Yuqi An, Yuwei Wan, Wenliang Chen, Binbin Lin, Yuqiang Li, Tong Xie, Dongzhan Zhou*

Main category: cs.LG

TL;DR: An LLM-based agent integrates 137 chemical tools and a dataset pipeline (ChemToolBench) to enhance chemistry tasks, using a HE-MCTS framework for optimization. It outperforms GPT-4o in performance.


<details>
  <summary>Details</summary>
Motivation: Address outdated pretraining knowledge and difficulty in incorporating chemical expertise into LLMs.

Method: Proposes an LLM-based agent with external tools, ChemToolBench dataset, and HE-MCTS for tool planning and execution. Uses self-generated data for fine-tuning.

Result: Significantly improves performance in Chemistry QA and discovery tasks, surpassing GPT-4o.

Conclusion: Offers a robust solution for integrating specialized tools with LLMs in chemistry, with datasets and code publicly available.

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [412] [Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty](https://arxiv.org/pdf/2502.06905)
*Yeseul Cho, Baekrok Shin, Changmin Kang, Chulhee Yun*

Main category: cs.LG

TL;DR: The paper introduces DUAL score for dataset pruning, reducing time and cost while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing dataset pruning methods that require full training before pruning.

Method: Proposes a Difficulty and Uncertainty-Aware Lightweight (DUAL) score and ratio-adaptive sampling using Beta distribution.

Result: Achieves SOTA performance with reduced time cost (66% on ImageNet-1k, 15% on CIFAR).

Conclusion: DUAL score and adaptive sampling enable efficient and effective dataset pruning.

Abstract: Recent advances in deep learning rely heavily on massive datasets, leading to
substantial storage and training costs. Dataset pruning aims to alleviate this
demand by discarding redundant examples. However, many existing methods require
training a model with a full dataset over a large number of epochs before being
able to prune the dataset, which ironically makes the pruning process more
expensive than just training the model on the entire dataset. To overcome this
limitation, we introduce a Difficulty and Uncertainty-Aware Lightweight (DUAL)
score, which aims to identify important samples from the early training stage
by considering both example difficulty and prediction uncertainty. To address a
catastrophic accuracy drop at an extreme pruning, we further propose a
ratio-adaptive sampling using Beta distribution. Experiments on various
datasets and learning scenarios such as image classification with label noise
and image corruption, and model architecture generalization demonstrate the
superiority of our method over previous state-of-the-art (SOTA) approaches.
Specifically, on ImageNet-1k, our method reduces the time cost for pruning to
66% compared to previous methods while achieving a SOTA, specifically 60% test
accuracy at a 90% pruning ratio. On CIFAR datasets, the time cost is reduced to
just 15% while maintaining SOTA performance.

</details>


### [413] [Adaptive Discretization against an Adversary: Lipschitz bandits, Dynamic Pricing, and Auction Tuning](https://arxiv.org/pdf/2006.12367)
*Chara Podimata, Aleksandrs Slivkins*

Main category: cs.LG

TL;DR: Adversarial Zooming algorithm for Lipschitz bandits with adversarial rewards, achieving instance-dependent regret bounds and extending to non-smooth applications like dynamic pricing.


<details>
  <summary>Details</summary>
Motivation: Address the gap in understanding adversarial rewards in Lipschitz bandits, aiming for adaptive discretization and instance-dependent performance.

Method: Proposes Adversarial Zooming algorithm for adaptive discretization in adversarial settings, leveraging weaker Lipschitz assumptions.

Result: Achieves worst-case optimal regret bounds and recovers instance-dependent bounds for stochastic cases. Extends to non-smooth applications.

Conclusion: The algorithm bridges adversarial and stochastic settings, enabling practical applications like dynamic pricing without strict smoothness requirements.

Abstract: Lipschitz bandits is a prominent version of multi-armed bandits that studies
large, structured action spaces such as the $[0,1]$ interval, where similar
actions are guaranteed to have similar rewards. A central theme here is the
adaptive discretization of the action space, which gradually ``zooms in'' on
the more promising regions thereof. The goal is to take advantage of ``nicer''
problem instances, while retaining near-optimal worst-case performance. While
the stochastic version of the problem is well-understood, the general version
with adversarial rewards is not.
  We provide the first algorithm (\emph{Adversarial Zooming}) for adaptive
discretization in the adversarial version, and derive instance-dependent regret
bounds. In particular, we recover the worst-case optimal regret bound for the
adversarial version, and the instance-dependent regret bound for the stochastic
version.
  We apply our algorithm to several fundamental applications -- including
dynamic pricing and auction reserve tuning -- all under adversarial reward
models. While these domains often violate Lipschitzness, our analysis only
requires a weaker version thereof, allowing for meaningful regret bounds
without additional smoothness assumptions. Notably, we extend our results to
multi-product dynamic pricing with non-smooth reward structures, a setting
which does not even satisfy one-sided Lipschitzness.

</details>


### [414] [Implicit Language Models are RNNs: Balancing Parallelization and Expressivity](https://arxiv.org/pdf/2502.07827)
*Mark Schöne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, Jannes Gladrow*

Main category: cs.LG

TL;DR: Implicit SSMs combine RNN expressivity with parallelization, outperforming transformers and SSMs on state-tracking and language tasks.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between parallelization (SSMs/transformers) and expressivity (RNNs) in language modeling.

Method: Propose implicit SSMs that iterate transformations to fixed points, enabling scalable training with partial convergence.

Result: Superior state-tracking on regular languages and outperforms explicit models on benchmarks, scaling to 1.3B parameters.

Conclusion: Implicit SSMs offer a viable solution balancing expressivity and parallelization, with practical scalability.

Abstract: State-space models (SSMs) and transformers dominate the language modeling
landscape. However, they are constrained to a lower computational complexity
than classical recurrent neural networks (RNNs), limiting their expressivity.
In contrast, RNNs lack parallelization during training, raising fundamental
questions about the trade off between parallelization and expressivity. We
propose implicit SSMs, which iterate a transformation until convergence to a
fixed point. Theoretically, we show that implicit SSMs implement the non-linear
state-transitions of RNNs. Empirically, we find that only approximate
fixed-point convergence suffices, enabling the design of a scalable training
curriculum that largely retains parallelization, with full convergence required
only for a small subset of tokens. Our approach demonstrates superior
state-tracking capabilities on regular languages, surpassing transformers and
SSMs. We further scale implicit SSMs to natural language reasoning tasks and
pretraining of large-scale language models up to 1.3B parameters on 207B tokens
representing, to our knowledge, the largest implicit model trained to date.
Notably, our implicit models outperform their explicit counterparts on standard
benchmarks. Our code is publicly available at
http://github.com/microsoft/implicit_languagemodels .

</details>


### [415] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2506.09200)
*Val Andrei Fajardo, David B. Emerson, Amandeep Singh, Veronica Chatrath, Marcelo Lotif, Ravi Theja, Alex Cheung, Izuki Matsuba*

Main category: cs.LG

TL;DR: FedRAG is a framework for fine-tuning RAG systems in both centralized and federated architectures, addressing gaps in current tools.


<details>
  <summary>Details</summary>
Motivation: To improve RAG systems by enabling fine-tuning across different architectures and integrating with the modern RAG ecosystem.

Method: Introduces FedRAG, supporting state-of-the-art fine-tuning methods for retriever and generator models, with a seamless transition between centralized and federated training.

Result: FedRAG provides a practical solution for enhancing RAG systems, filling a critical tooling gap.

Conclusion: FedRAG effectively bridges the gap in RAG system fine-tuning, offering flexibility and integration with existing tools.

Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [416] [Three iterations of $(d-1)$-WL test distinguish non isometric clouds of $d$-dimensional points](https://arxiv.org/pdf/2303.12853)
*Valentino Delle Rose, Alexander Kozachinskiy, Cristóbal Rojas, Mircea Petrache, Pablo Barceló*

Main category: cs.LG

TL;DR: The paper studies the completeness of the Weisfeiler-Lehman (WL) test for distinguishing point clouds in Euclidean space, showing that $(d-1)$-dimensional WL suffices for $d$-dimensional space, with three iterations. It also clarifies the 3D case and explores limitations of 1-WL.


<details>
  <summary>Details</summary>
Motivation: To understand the expressive power of the WL test for graph neural networks (GNNs) in distinguishing non-isometric point clouds, especially in 3D space.

Method: Analyzes the WL test's completeness for point clouds in $d$-dimensional Euclidean space, focusing on iterations and dimensions required.

Result: $(d-1)$-dimensional WL is complete for $d$-dimensional space (3 iterations suffice). 1-WL fails for planar point clouds in 3D, and 2-WL is incomplete in 6D.

Conclusion: The study provides insights into WL test's capabilities for GNNs, resolving the 3D case and highlighting open questions for dimensions 4 and 5.

Abstract: The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for
checking isomorphism of graphs. It has also been observed that it underlies the
design of several graph neural network architectures, whose capabilities and
performance can be understood in terms of the expressive power of this test.
Motivated by recent developments in machine learning applications to datasets
involving three-dimensional objects, we study when the WL test is {\em
complete} for clouds of euclidean points represented by complete distance
graphs, i.e., when it can distinguish, up to isometry, any arbitrary such
cloud. %arbitrary clouds of euclidean points represented by complete distance
graphs. % How many dimensions of the Weisfeiler--Lehman test is enough to
distinguish any two non-isometric point clouds in $d$-dimensional Euclidean
space, assuming that these point clouds are given as complete graphs labeled by
distances between the points? This question is important for understanding,
which architectures of graph neural networks are capable of fully exploiting
the spacial structure of a point cloud.
  Our main result states that the $(d-1)$-dimensional WL test is complete for
point clouds in $d$-dimensional Euclidean space, for any $d\ge 2$, and that
only three iterations of the test suffice. We also observe that the
$d$-dimensional WL test only requires one iteration to achieve completeness.
  Our paper thus provides complete understanding of the 3-dimensional case: it
was shown in previous works that 1-WL is not complete in $\mathbb{R}^3$, and we
show that 2-WL is complete there. We also strengthen the lower bound for 1-WL
by showing that it is unable to recognize planar point clouds in
$\mathbb{R}^3$. Finally, we show that 2-WL is not complete in $\mathbb{R}^6$,
leaving as an open question, whether it is complete in $\mathbb{R}^{d}$ for $d
= 4,5$.

</details>


### [417] [TransMLA: Multi-Head Latent Attention Is All You Need](https://arxiv.org/pdf/2502.07864)
*Fanxu Meng, Pingzhi Tang, Xiaojuan Tang, Zengwei Yao, Xing Sun, Muhan Zhang*

Main category: cs.LG

TL;DR: TransMLA converts GQA-based models to MLA-based ones, achieving 10.6x speedup with minimal performance loss and DeepSeek compatibility.


<details>
  <summary>Details</summary>
Motivation: To enable GQA-based models to leverage DeepSeek's optimizations and improve inference efficiency.

Method: Compresses KV cache in LLaMA-2-7B, fine-tunes with 6B tokens, and integrates with DeepSeek's tools.

Result: 10.6x speedup at 8K context length, preserved output quality, and regained performance post-fine-tuning.

Conclusion: TransMLA provides a practical migration path for GQA models to MLA, enhancing efficiency with DeepSeek's features.

Abstract: In this paper, we present TransMLA, a framework that seamlessly converts any
GQA-based pre-trained model into an MLA-based model. Our approach enables
direct compatibility with DeepSeek's codebase, allowing these models to fully
leverage DeepSeek-specific optimizations such as vLLM and SGlang. By
compressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x
inference speedup at an 8K context length while preserving meaningful output
quality. Additionally, the model requires only 6 billion tokens for fine-tuning
to regain performance on par with the original across multiple benchmarks.
TransMLA offers a practical solution for migrating GQA-based models to the MLA
structure. When combined with DeepSeek's advanced features, such as FP8
quantization and Multi-Token Prediction, even greater inference acceleration
can be realized.

</details>


### [418] [A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning](https://arxiv.org/pdf/2311.00212)
*Samuel E. Otto, Nicholas Zolman, J. Nathan Kutz, Steven L. Brunton*

Main category: cs.LG

TL;DR: The paper presents a framework for integrating symmetry into machine learning, covering enforcement, discovery, and promotion of symmetry, using Lie derivatives and convex regularization.


<details>
  <summary>Details</summary>
Motivation: Symmetry is fundamental in physics and machine learning for extrapolatory power and efficiency. The paper aims to unify methods for leveraging symmetry in models.

Method: The framework uses Lie derivatives and fiber-linear Lie group actions. It includes enforcing known symmetry, discovering unknown symmetries, and promoting symmetry via convex regularization.

Result: The approach unifies existing results, shows duality in symmetry tasks, and introduces novel regularization for symmetry promotion, applicable to various models.

Conclusion: The framework provides a versatile and mathematically grounded way to incorporate symmetry into machine learning, enhancing model performance and interpretability.

Abstract: Symmetry is present throughout nature and continues to play an increasingly
central role in physics and machine learning. Fundamental symmetries, such as
Poincar\'{e} invariance, allow physical laws discovered in laboratories on
Earth to be extrapolated to the farthest reaches of the universe. Symmetry is
essential to achieving this extrapolatory power in machine learning
applications. For example, translation invariance in image classification
allows models with fewer parameters, such as convolutional neural networks, to
be trained on smaller data sets and achieve state-of-the-art performance. In
this paper, we provide a unifying theoretical and methodological framework for
incorporating symmetry into machine learning models in three ways: 1. enforcing
known symmetry when training a model; 2. discovering unknown symmetries of a
given model or data set; and 3. promoting symmetry during training by learning
a model that breaks symmetries within a user-specified group of candidates when
there is sufficient evidence in the data. We show that these tasks can be cast
within a common mathematical framework whose central object is the Lie
derivative associated with fiber-linear Lie group actions on vector bundles. We
extend and unify several existing results by showing that enforcing and
discovering symmetry are linear-algebraic tasks that are dual with respect to
the bilinear structure of the Lie derivative. We also propose a novel way to
promote symmetry by introducing a class of convex regularization functions
based on the Lie derivative and nuclear norm relaxation to penalize symmetry
breaking during training of machine learning models. We explain how these ideas
can be applied to a wide range of machine learning models including basis
function regression, dynamical systems discovery, neural networks, and neural
operators acting on fields.

</details>


### [419] [Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation](https://arxiv.org/pdf/2502.08211)
*Jinda Xu, Yuhao Song, Daming Wang, Weiwei Zhao, Minghua Chen, Kangliang Chen, Qinya Li*

Main category: cs.LG

TL;DR: EcoDatum introduces a learning-driven approach for web-crawl dataset curation, outperforming SOTA methods with a 28% improvement in performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of unstructured and heterogeneous web-crawl datasets, which traditional heuristic methods fail to curate effectively.

Method: Uses a novel quality-guided deduplication method and integrates unimodal/multimodal operators in a weak supervision ensemble framework.

Result: Ranked 1st on DataComp leaderboard with a 28% improvement over baseline, scoring 0.182 across 38 datasets.

Conclusion: EcoDatum significantly enhances data curation quality and model training efficiency.

Abstract: In an era overwhelmed by vast amounts of data, the effective curation of
web-crawl datasets is essential for optimizing model performance. This paper
tackles the challenges associated with the unstructured and heterogeneous
nature of such datasets. Traditional heuristic curation methods often
inadequately capture complex features, resulting in biases and the exclusion of
relevant data. We introduce an advanced, learning-driven approach, Ensemble
Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel
quality-guided deduplication method to ensure balanced feature distributions.
EcoDatum strategically integrates various unimodal and multimodal data curation
operators within a weak supervision ensemble framework, utilizing automated
optimization to score each data point effectively. EcoDatum, which
significantly improves the data curation quality and efficiency, outperforms
existing state-of-the-art (SOTA) techniques, ranked 1st on the DataComp
leaderboard, with an average performance score of 0.182 across 38 diverse
evaluation datasets. This represents a 28% improvement over the DataComp
baseline method, demonstrating its effectiveness in improving dataset curation
and model training efficiency.

</details>


### [420] [Meta-learning Optimizers for Communication-Efficient Learning](https://arxiv.org/pdf/2312.02204)
*Charles-Étienne Joseph, Benjamin Thérien, Abhinav Moudgil, Boris Knyazev, Eugene Belilovsky*

Main category: cs.LG

TL;DR: Learned optimizers outperform local SGD and its variants in communication-efficient distributed deep learning, even generalizing to unseen datasets and architectures.


<details>
  <summary>Details</summary>
Motivation: To address the gap between local SGD variants and state-of-the-art adaptive optimizers in distributed deep learning, leveraging learned optimizers for improved performance.

Method: Meta-learn global updates from local SGD iterations to enhance communication efficiency.

Result: Learned optimizers significantly outperform local SGD, generalize to larger datasets (e.g., ImageNet, ViTs), and work across modalities like language modeling.

Conclusion: Learned optimizers show promise for advancing communication-efficient distributed learning.

Abstract: Communication-efficient variants of SGD, specifically local SGD, have
received a great deal of interest in recent years. These approaches compute
multiple gradient steps locally on each worker, before averaging model
parameters, helping relieve the critical communication bottleneck in
distributed deep learning training. Although many variants of these approaches
have been proposed, they can sometimes lag behind state-of-the-art adaptive
optimizers for deep learning. In this work, we investigate if the recent
progress in the emerging area of learned optimizers can potentially close this
gap in homogeneous data and homogeneous device settings while remaining
communication-efficient. Specifically, we meta-learn how to perform global
updates given an update from local SGD iterations. Our results demonstrate that
learned optimizers can substantially outperform local SGD and its sophisticated
variants while maintaining their communication efficiency. Our learned
optimizers can even generalize to unseen and much larger datasets and
architectures, including ImageNet and ViTs, and to unseen modalities such as
language modeling. We therefore show the potential of learned optimizers for
improving communication-efficient distributed learning.

</details>


### [421] [From Features to Graphs: Exploring Graph Structures and Pairwise Interactions via GNNs](https://arxiv.org/pdf/2502.13471)
*Phaphontee Yamchote, Saw Nay Htet Win, Chainarong Amornbunchornvej, Thanapon Noraset*

Main category: cs.LG

TL;DR: The paper explores the importance of pairwise feature interactions in GNNs, showing that sparse feature graphs with only necessary edges improve performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Understanding how feature interactions impact GNN performance and designing efficient feature graphs.

Method: Leveraging existing GNN models and tools, experimenting with synthesized datasets, and using the MDL principle for theoretical support.

Result: Edges between interacting features are crucial, while non-interaction edges degrade performance. Sparse graphs align with Occam's Razor.

Conclusion: Sparse feature graphs with essential interaction edges enhance GNN performance and interpretability.

Abstract: Feature interaction is crucial in predictive machine learning models, as it
captures the relationships between features that influence model performance.
In this work, we focus on pairwise interactions and investigate their
importance in constructing feature graphs for Graph Neural Networks (GNNs). We
leverage existing GNN models and tools to explore the relationship between
feature graph structures and their effectiveness in modeling interactions.
Through experiments on synthesized datasets, we uncover that edges between
interacting features are important for enabling GNNs to model feature
interactions effectively. We also observe that including non-interaction edges
can act as noise, degrading model performance. Furthermore, we provide
theoretical support for sparse feature graph selection using the Minimum
Description Length (MDL) principle. We prove that feature graphs retaining only
necessary interaction edges yield a more efficient and interpretable
representation than complete graphs, aligning with Occam's Razor. Our findings
offer both theoretical insights and practical guidelines for designing feature
graphs that improve the performance and interpretability of GNN models.

</details>


### [422] [Privacy-aware Berrut Approximated Coded Computing for Federated Learning](https://arxiv.org/pdf/2405.01704)
*Xavier Martínez Luaña, Rebeca P. Díaz Redondo, Manuel Fernández Veiga*

Main category: cs.LG

TL;DR: Proposes a privacy-preserving solution for Federated Learning using Berrut Approximated Coded Computing, addressing limitations of existing techniques like DP, HE, and SMPC.


<details>
  <summary>Details</summary>
Motivation: Existing privacy techniques in FL (DP, HE, SMPC) have drawbacks like handling non-linear functions, high computational costs, and scalability issues.

Method: Adapts Berrut Approximated Coded Computing from Coded Distributed Computing to a Secret Sharing setup, enabling scalable and private FL with support for non-linear functions and matrix multiplication.

Result: Achieves privacy and scalability, with a good trade-off between privacy and precision, validated by numerical results.

Conclusion: The solution is versatile, applicable to various FL scenarios, and independent of ML models or aggregation algorithms.

Abstract: Federated Learning (FL) is an interesting strategy that enables the
collaborative training of an AI model among different data owners without
revealing their private datasets. Even so, FL has some privacy vulnerabilities
that have been tried to be overcome by applying some techniques like
Differential Privacy (DP), Homomorphic Encryption (HE), or Secure Multi-Party
Computation (SMPC). However, these techniques have some important drawbacks
that might narrow their range of application: problems to work with non-linear
functions and to operate large matrix multiplications and high communication
and computational costs to manage semi-honest nodes. In this context, we
propose a solution to guarantee privacy in FL schemes that simultaneously
solves the previously mentioned problems. Our proposal is based on the Berrut
Approximated Coded Computing, a technique from the Coded Distributed Computing
paradigm, adapted to a Secret Sharing configuration, to provide input privacy
to FL in a scalable way. It can be applied for computing non-linear functions
and treats the special case of distributed matrix multiplication, a key
primitive at the core of many automated learning tasks. Because of these
characteristics, it could be applied in a wide range of FL scenarios, since it
is independent of the machine learning models or aggregation algorithms used in
the FL scheme. We provide analysis of the achieved privacy and complexity of
our solution and, due to the extensive numerical results performed, a good
trade-off between privacy and precision can be observed.

</details>


### [423] [Generative Uncertainty in Diffusion Models](https://arxiv.org/pdf/2502.20946)
*Metod Jazbec, Eliot Wong-Toi, Guoxuan Xia, Dan Zhang, Eric Nalisnick, Stephan Mandt*

Main category: cs.LG

TL;DR: A Bayesian framework is proposed to estimate generative uncertainty in diffusion models, improving sample quality detection without human inspection.


<details>
  <summary>Details</summary>
Motivation: Despite high average sample quality in diffusion models, individual low-quality samples are hard to detect automatically.

Method: A Bayesian framework with a semantic likelihood (evaluated in latent space) is introduced, applicable post-hoc to pretrained models via Laplace approximation.

Result: The framework effectively identifies poor-quality samples and outperforms existing uncertainty-based methods.

Conclusion: The proposed method is practical, computationally efficient, and broadly applicable to diffusion and flow matching models.

Abstract: Diffusion models have recently driven significant breakthroughs in generative
modeling. While state-of-the-art models produce high-quality samples on
average, individual samples can still be low quality. Detecting such samples
without human inspection remains a challenging task. To address this, we
propose a Bayesian framework for estimating generative uncertainty of synthetic
samples. We outline how to make Bayesian inference practical for large, modern
generative models and introduce a new semantic likelihood (evaluated in the
latent space of a feature extractor) to address the challenges posed by
high-dimensional sample spaces. Through our experiments, we demonstrate that
the proposed generative uncertainty effectively identifies poor-quality samples
and significantly outperforms existing uncertainty-based methods. Notably, our
Bayesian framework can be applied post-hoc to any pretrained diffusion or flow
matching model (via the Laplace approximation), and we propose simple yet
effective techniques to minimize its computational overhead during sampling.

</details>


### [424] [RmGPT: A Foundation Model with Generative Pre-trained Transformer for Fault Diagnosis and Prognosis in Rotating Machinery](https://arxiv.org/pdf/2409.17604)
*Yilin Wang, Yifei Yu, Kong Sun, Peixuan Lei, Yuxuan Zhang, Enrico Zio, Aiguo Xia, Yuanxiang Li*

Main category: cs.LG

TL;DR: RmGPT is a unified generative model for rotating machinery PHM, excelling in diagnosis and prognosis tasks with high accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Address challenges in PHM by handling diverse datasets and varying conditions with a single model.

Method: Uses a token-based framework with self-supervised learning and prompt learning for task adaptation.

Result: Outperforms state-of-the-art methods, achieving near-perfect accuracy in diagnosis and low errors in prognosis.

Conclusion: RmGPT is a robust and scalable PHM foundation model for rotating machinery.

Abstract: In industry, the reliability of rotating machinery is critical for production
efficiency and safety. Current methods of Prognostics and Health Management
(PHM) often rely on task-specific models, which face significant challenges in
handling diverse datasets with varying signal characteristics, fault modes and
operating conditions. Inspired by advancements in generative pretrained models,
we propose RmGPT, a unified model for diagnosis and prognosis tasks. RmGPT
introduces a novel generative token-based framework, incorporating Signal
Tokens, Prompt Tokens, Time-Frequency Task Tokens and Fault Tokens to handle
heterogeneous data within a unified model architecture. We leverage
self-supervised learning for robust feature extraction and introduce a next
signal token prediction pretraining strategy, alongside efficient prompt
learning for task-specific adaptation. Extensive experiments demonstrate that
RmGPT significantly outperforms state-of-the-art algorithms, achieving
near-perfect accuracy in diagnosis tasks and exceptionally low errors in
prognosis tasks. Notably, RmGPT excels in few-shot learning scenarios,
achieving 82\% accuracy in 16-class one-shot experiments, highlighting its
adaptability and robustness. This work establishes RmGPT as a powerful PHM
foundation model for rotating machinery, advancing the scalability and
generalizability of PHM solutions. \textbf{Code is available at:
https://github.com/Pandalin98/RmGPT.

</details>


### [425] [Simplicity bias and optimization threshold in two-layer ReLU networks](https://arxiv.org/pdf/2410.02348)
*Etienne Boursier, Nicolas Flammarion*

Main category: cs.LG

TL;DR: Overparametrized neural networks often converge to simpler solutions instead of interpolating data, improving generalization. This is due to an early alignment phase leading to a simplicity bias.


<details>
  <summary>Details</summary>
Motivation: To understand why overparametrized networks generalize well despite not interpolating data, especially in complex tasks like in-context learning or diffusion.

Method: Theoretical analysis of two-layer ReLU networks, focusing on the early alignment phase where neurons align directionally, creating a simplicity bias.

Result: Networks converge to simpler solutions, avoiding interpolation, which improves test loss. This behavior is linked to an optimization threshold.

Conclusion: The simplicity bias from early alignment enhances generalization, suggesting that not reaching interpolation can be beneficial for model performance.

Abstract: Understanding generalization of overparametrized neural networks remains a
fundamental challenge in machine learning. Most of the literature mostly
studies generalization from an interpolation point of view, taking convergence
of parameters towards a global minimum of the training loss for granted. While
overparametrized architectures indeed interpolated the data for typical
classification tasks, this interpolation paradigm does not seem valid anymore
for more complex tasks such as in-context learning or diffusion. Instead for
such tasks, it has been empirically observed that the trained models goes from
global minima to spurious local minima of the training loss as the number of
training samples becomes larger than some level we call optimization threshold.
While the former yields a poor generalization to the true population loss, the
latter was observed to actually correspond to the minimiser of this true loss.
This paper explores theoretically this phenomenon in the context of two-layer
ReLU networks. We demonstrate that, despite overparametrization, networks often
converge toward simpler solutions rather than interpolating the training data,
which can lead to a drastic improvement on the test loss with respect to
interpolating solutions. Our analysis relies on the so called early alignment
phase, during which neurons align towards specific directions. This directional
alignment, which occurs in the early stage of training, leads to a simplicity
bias, wherein the network approximates the ground truth model without
converging to the global minimum of the training loss. Our results suggest that
this bias, resulting in an optimization threshold from which interpolation is
not reached anymore, is beneficial and enhances the generalization of trained
models.

</details>


### [426] [On the Geometry of Receiver Operating Characteristic and Precision-Recall Curves](https://arxiv.org/pdf/2504.02169)
*Reza Sameni*

Main category: cs.LG

TL;DR: The paper analyzes ROC and PR curves in binary classification, showing common metrics are functions of a composition of class-conditional CDFs. It provides insights for classifier optimization and decision-making.


<details>
  <summary>Details</summary>
Motivation: To understand the geometry of ROC/PR curves and their role in classifier behavior, aiding in practical applications like threshold selection and model comparison.

Method: The study uses geometric analysis of ROC/PR curves, focusing on the composition function of class-conditional CDFs, and explores classifier dominance, separability, and variance effects.

Result: The framework explains how ROC/PR curve shapes reflect classifier behavior, links the leakage function to KL divergence, and aids in practical deployment considerations.

Conclusion: The geometric perspective enhances classifier optimization and decision-making, offering tools for real-world applications with constraints.

Abstract: We study the geometry of Receiver Operating Characteristic (ROC) and
Precision-Recall (PR) curves in binary classification problems. The key finding
is that many of the most commonly used binary classification metrics are merely
functions of the composition function $G := F_p \circ F_n^{-1}$, where
$F_p(\cdot)$ and $F_n(\cdot)$ are the class-conditional cumulative distribution
functions of the classifier scores in the positive and negative classes,
respectively. This geometric perspective facilitates the selection of operating
points, understanding the effect of decision thresholds, and comparison between
classifiers. It also helps explain how the shapes and geometry of ROC/PR curves
reflect classifier behavior, providing objective tools for building classifiers
optimized for specific applications with context-specific constraints. We
further explore the conditions for classifier dominance, present analytical and
numerical examples demonstrating the effects of class separability and variance
on ROC and PR geometries, and derive a link between the positive-to-negative
class leakage function $G(\cdot)$ and the Kullback--Leibler divergence. The
framework highlights practical considerations, such as model calibration,
cost-sensitive optimization, and operating point selection under real-world
capacity constraints, enabling more informed approaches to classifier
deployment and decision-making.

</details>


### [427] [Zero-Shot Offline Imitation Learning via Optimal Transport](https://arxiv.org/pdf/2410.08751)
*Thomas Rupf, Marco Bagatella, Nico Gürtler, Jonas Frey, Georg Martius*

Main category: cs.LG

TL;DR: A novel method for zero-shot imitation learning optimizes occupancy matching to avoid myopic behavior, using a goal-conditioned value function and learned world model.


<details>
  <summary>Details</summary>
Motivation: Existing imitation learning approaches can suffer from myopic behavior, undermining long-term objectives.

Method: Lifts a goal-conditioned value function to a distance between occupancies, approximated via a learned world model.

Result: Capable of non-myopic, zero-shot imitation in complex, continuous benchmarks, even with offline, suboptimal data.

Conclusion: The proposed method effectively mitigates myopic behavior and improves imitation learning performance.

Abstract: Zero-shot imitation learning algorithms hold the promise of reproducing
unseen behavior from as little as a single demonstration at test time. Existing
practical approaches view the expert demonstration as a sequence of goals,
enabling imitation with a high-level goal selector, and a low-level
goal-conditioned policy. However, this framework can suffer from myopic
behavior: the agent's immediate actions towards achieving individual goals may
undermine long-term objectives. We introduce a novel method that mitigates this
issue by directly optimizing the occupancy matching objective that is intrinsic
to imitation learning. We propose to lift a goal-conditioned value function to
a distance between occupancies, which are in turn approximated via a learned
world model. The resulting method can learn from offline, suboptimal data, and
is capable of non-myopic, zero-shot imitation, as we demonstrate in complex,
continuous benchmarks. The code is available at
https://github.com/martius-lab/zilot.

</details>


### [428] [Elucidating the Design Space of Multimodal Protein Language Models](https://arxiv.org/pdf/2504.11454)
*Cheng-Yen Hsieh, Xinyou Wang, Daiheng Zhang, Dongyu Xue, Fei Ye, Shujian Huang, Zaixiang Zheng, Quanquan Gu*

Main category: cs.LG

TL;DR: The paper addresses limitations in multimodal protein language models (PLMs) by proposing a design space to improve structural fidelity and generative modeling, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Tokenizing 3D protein structures into discrete tokens causes loss of fine-grained details, limiting the effectiveness of multimodal PLMs.

Method: The paper introduces improved generative modeling, structure-aware architectures, and representation learning to enhance multimodal PLMs.

Result: The proposed methods reduce RMSD from 5.52 to 2.36 on PDB testset, outperforming larger baselines and matching specialized folding models.

Conclusion: The advancements demonstrate that token-based multimodal PLMs can achieve robust structural modeling and improved generation diversity.

Abstract: Multimodal protein language models (PLMs) integrate sequence and token-based
structural information, serving as a powerful foundation for protein modeling,
generation, and design. However, the reliance on tokenizing 3D structures into
discrete tokens causes substantial loss of fidelity about fine-grained
structural details and correlations. In this paper, we systematically elucidate
the design space of multimodal PLMs to overcome their limitations. We identify
tokenization loss and inaccurate structure token predictions by the PLMs as
major bottlenecks. To address these, our proposed design space covers improved
generative modeling, structure-aware architectures and representation learning,
and data exploration. Our advancements approach finer-grained supervision,
demonstrating that token-based multimodal PLMs can achieve robust structural
modeling. The effective design methods dramatically improve the structure
generation diversity, and notably, folding abilities of our 650M model by
reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B
baselines and on par with the specialized folding models. Project page and
code: https://bytedance.github.io/dplm/dplm-2.1/.

</details>


### [429] [Differentially private and decentralized randomized power method](https://arxiv.org/pdf/2411.01931)
*Julien Nicolas, César Sabater, Mohamed Maouche, Sonia Ben Mokhtar, Mark Coates*

Main category: cs.LG

TL;DR: The paper proposes privacy-preserving variants of the randomized power method, reducing noise for Differential Privacy and adapting it to a decentralized framework with improved guarantees and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in large-scale spectral analysis and recommendation tasks involving personal data.

Method: Introduces variants with refined privacy analysis for less noise in DP and a decentralized protocol for distributed data.

Result: Achieves tighter convergence bounds, same DP guarantees with less noise, and low overhead in decentralized settings.

Conclusion: The proposed methods enhance privacy and efficiency, validated by empirical results on real datasets.

Abstract: The randomized power method has gained significant interest due to its
simplicity and efficient handling of large-scale spectral analysis and
recommendation tasks. However, its application to large datasets containing
personal information (e.g., web interactions, search history, personal tastes)
raises critical privacy problems. This paper addresses these issues by
proposing enhanced privacy-preserving variants of the method. First, we propose
a variant that reduces the amount of the noise required in current techniques
to achieve Differential Privacy (DP). More precisely, we refine the privacy
analysis so that the Gaussian noise variance no longer grows linearly with the
target rank, achieving the same DP guarantees with strictly less noise. Second,
we adapt our method to a decentralized framework in which data is distributed
among multiple users. The decentralized protocol strengthens privacy guarantees
with no accuracy penalty and a low computational and communication overhead.
Our results include the provision of tighter convergence bounds for both the
centralized and decentralized versions, and an empirical comparison with
previous work using real recommendation datasets.

</details>


### [430] [Advanced deep architecture pruning using single filter performance](https://arxiv.org/pdf/2501.12880)
*Yarden Tzach, Yuval Meir, Ronit D. Gross, Ofek Tevet, Ella Koresh, Ido Kanter*

Main category: cs.LG

TL;DR: The paper introduces a method (AFCC) for pruning neural networks by analyzing filter performance, achieving high pruning without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To reduce computational complexity, energy consumption, and latency in neural networks by pruning parameters and structure.

Method: Uses Applied Filter Cluster Connections (AFCC) to measure single filter performance and prune convolutional layers. Tested on VGG-11 and EfficientNet-B0 trained on CIFAR-100.

Result: AFCC outperforms other pruning techniques at the same magnitude and can be extended to fully connected layers.

Conclusion: AFCC offers a scalable way to reduce complexity in over-parameterized AI tasks while maintaining accuracy.

Abstract: Pruning the parameters and structure of neural networks reduces the
computational complexity, energy consumption, and latency during inference.
Recently, a novel underlying mechanism for successful deep learning (DL) was
presented based on a method that quantitatively measures the single filter
performance in each layer of a DL architecture, and a new comprehensive
mechanism of how deep learning works was presented. This statistical mechanics
inspired viewpoint enables to reveal the macroscopic behavior of the entire
network from the microscopic performance of each filter and their cooperative
behavior. Herein, we demonstrate how this understanding paves the path to high
quenched dilution of the convolutional layers of deep architectures without
affecting their overall accuracy using applied filter cluster connections
(AFCC). AFCC is exemplified on VGG-11 and EfficientNet-B0 architectures trained
on CIFAR-100, and its high pruning outperforms other techniques using the same
pruning magnitude. Additionally, this technique is broadened to single nodal
performance and highly pruning of fully connected layers, suggesting a possible
implementation to considerably reduce the complexity of over-parameterized AI
tasks.

</details>


### [431] [Testing Generalizability in Causal Inference](https://arxiv.org/pdf/2411.03021)
*Daniel de Vassimon Manela, Linying Yang, Robin J. Evans*

Main category: cs.LG

TL;DR: A framework for statistically evaluating the generalizability of high-dimensional causal inference models, using simulations and statistical testing for robust and realistic evaluations.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of formal procedures to evaluate generalizability in machine learning, especially in causal inference, beyond conventional metrics like MSE.

Method: Proposes a systematic framework using frugal parameterization to simulate from causal benchmarks, evaluating mean and distributional regression methods.

Result: Ensures realistic evaluations grounded in real-world data, avoiding over-reliance on simplified datasets and conventional metrics.

Conclusion: The framework provides robust statistical safeguards for decision-making in causal inference.

Abstract: Ensuring robust model performance in diverse real-world scenarios requires
addressing generalizability across domains with covariate shifts. However, no
formal procedure exists for statistically evaluating generalizability in
machine learning algorithms. Existing predictive metrics like mean squared
error (MSE) help to quantify the relative performance between models, but do
not directly answer whether a model can or cannot generalize. To address this
gap in the domain of causal inference, we propose a systematic framework for
statistically evaluating the generalizability of high-dimensional causal
inference models. Our approach uses the frugal parameterization to flexibly
simulate from fully and semi-synthetic causal benchmarks, offering a
comprehensive evaluation for both mean and distributional regression methods.
Grounded in real-world data, our method ensures more realistic evaluations,
which is often missing in current work relying on simplified datasets.
Furthermore, using simulations and statistical testing, our framework is robust
and avoids over-reliance on conventional metrics, providing statistical
safeguards for decision making.

</details>


### [432] [Token-Efficient RL for LLM Reasoning](https://arxiv.org/pdf/2504.20834)
*Alan Lee, Harry Tong*

Main category: cs.LG

TL;DR: The paper introduces RL strategies for LLMs under memory/compute limits, focusing on LoRA compatibility. Methods like S-GRPO and T-SPMO improve accuracy on benchmarks like SVAMP.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of reasoning in LLMs under strict memory and compute constraints while ensuring compatibility with LoRA fine-tuning.

Method: Proposes critic-free RL methods (S-GRPO, T-SPMO) that optimize a small subset of output tokens to reduce memory and stabilize training.

Result: Accuracy on SVAMP rises from 46% to over 70%, with strong multi-digit multiplication performance. Full-token GRPO under LoRA fails to improve.

Conclusion: Selective token-level optimization may act as an implicit regularizer in low-parameter training, showing promise for efficient LLM reasoning.

Abstract: We propose reinforcement learning (RL) strategies tailored for reasoning in
large language models (LLMs) under strict memory and compute limits, with a
particular focus on compatibility with LoRA fine-tuning. Building on early
policy gradient methods with baseline subtraction, we design critic-free
methods that operate on a small, informative subset of output tokens to reduce
memory usage and stabilize training. We introduce S-GRPO, a stochastic variant
of Group Relative Policy Optimization, and T-SPMO, a token-level prefix
matching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,
our methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show
strong performance on multi-digit multiplication. Surprisingly, full-token GRPO
under LoRA fails to improve over the base model, suggesting that selective
token-level optimization may act as an implicit regularizer in low-parameter
training regimes.

</details>


### [433] [Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods](https://arxiv.org/pdf/2411.05743)
*Joseph Pollock, Igor Shilov, Euodia Dodd, Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: LT-IQR identifies vulnerable training samples using loss trajectories, avoiding costly shadow model training, achieving 92% precision.


<details>
  <summary>Details</summary>
Motivation: Current MIAs require excessive computational resources, limiting practical use in model development.

Method: Analyzes per-sample loss trajectories during training (LT-IQR) to identify high-risk samples.

Result: Achieves 92% precision@k=1% in identifying vulnerable samples, outperforming traditional metrics.

Conclusion: LT-IQR enables efficient privacy risk evaluation during model development without additional costs.

Abstract: Membership inference attacks (MIAs) are widely used to empirically assess
privacy risks in machine learning models, both providing model-level
vulnerability metrics and identifying the most vulnerable training samples.
State-of-the-art methods, however, require training hundreds of shadow models
with the same architecture as the target model. This makes the computational
cost of assessing the privacy of models prohibitive for many practical
applications, particularly when used iteratively as part of the model
development process and for large models. We propose a novel approach for
identifying the training samples most vulnerable to membership inference
attacks by analyzing artifacts naturally available during the training process.
Our method, Loss Trace Interquartile Range (LT-IQR), analyzes per-sample loss
trajectories collected during model training to identify high-risk samples
without requiring any additional model training. Through experiments on
standard benchmarks, we demonstrate that LT-IQR achieves 92% precision@k=1% in
identifying the samples most vulnerable to state-of-the-art MIAs. This result
holds across datasets and model architectures with LT-IQR outperforming both
traditional vulnerability metrics, such as loss, and lightweight MIAs using few
shadow models. We also show LT-IQR to accurately identify points vulnerable to
multiple MIA methods and perform ablation studies. We believe LT-IQR enables
model developers to identify vulnerable training samples, for free, as part of
the model development process. Our results emphasize the potential of
artifact-based methods to efficiently evaluate privacy risks.

</details>


### [434] [Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning Vision Models from DataSeeds' Annotated Imagery](https://arxiv.org/pdf/2506.05673)
*Sajjad Abdoli, Freeman Lewin, Gediminas Vasiliauskas, Fabian Schonholz*

Main category: cs.LG

TL;DR: The paper highlights a shift from 'Model Centric' to 'Data-Centric' AI development, introducing the DataSeeds.AI sample dataset (DSD) to improve model performance through high-quality training data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional model-centric approaches by emphasizing the importance of data quality in AI performance.

Method: Introduces the DSD, a dataset of 10,610 high-quality images with multi-tier annotations, and evaluates its impact on model performance.

Result: Quantitative improvements in model performance against benchmarks, with code and trained models made publicly available.

Conclusion: The DSD sets a new standard for commercial image datasets, supporting robust AI development.

Abstract: The development of modern Artificial Intelligence (AI) models, particularly
diffusion-based models employed in computer vision and image generation tasks,
is undergoing a paradigmatic shift in development methodologies. Traditionally
dominated by a "Model Centric" approach, in which performance gains were
primarily pursued through increasingly complex model architectures and
hyperparameter optimization, the field is now recognizing a more nuanced
"Data-Centric" approach. This emergent framework foregrounds the quality,
structure, and relevance of training data as the principal driver of model
performance. To operationalize this paradigm shift, we introduce the
DataSeeds.AI sample dataset (the "DSD"), initially comprised of approximately
10,610 high-quality human peer-ranked photography images accompanied by
extensive multi-tier annotations. The DSD is a foundational computer vision
dataset designed to usher in a new standard for commercial image datasets.
Representing a small fraction of DataSeeds.AI's 100 million-plus image catalog,
the DSD provides a scalable foundation necessary for robust commercial and
multimodal AI development. Through this in-depth exploratory analysis, we
document the quantitative improvements generated by the DSD on specific models
against known benchmarks and make the code and the trained models used in our
evaluation publicly available.

</details>


### [435] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/pdf/2505.10167)
*Saikat Barua, Mostafizur Rahman, Shehenaz Khaled, Md Jafor Sadek, Rafiul Islam, Shahnewaz Siddique*

Main category: cs.LG

TL;DR: The paper introduces QuXAI, a framework for explaining hybrid quantum-classical machine learning (HQML) models, addressing the lack of robust explainability methods in such systems.


<details>
  <summary>Details</summary>
Motivation: The complexity of HQML models often results in black-box behavior, undermining transparency and reliability. Existing XAI methods for quantum systems are limited, creating a research gap.

Method: The authors propose QuXAI, based on Q-MEDLEY, to explain feature importance in HQML models. It involves creating HQML models with quantum feature maps, using Q-MEDLEY for feature-based inferences, and visualizing attributions.

Result: Q-MEDLEY identifies influential classical aspects and separates noise in HQML models, performing comparably to classical XAI techniques. Ablation studies highlight the benefits of Q-MEDLEY's composite structure.

Conclusion: QuXAI enhances interpretability and reliability of HQML models, promoting safer and more responsible use of quantum-enhanced AI. The work is open-sourced for further research.

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.
  Our code and experiments are open-sourced at:
https://github.com/GitsSaikat/QuXAI

</details>


### [436] [BioNeMo Framework: a modular, high-performance library for AI model development in drug discovery](https://arxiv.org/pdf/2411.10548)
*Peter St. John, Dejun Lin, Polina Binder, Malcolm Greaves, Vega Shah, John St. John, Adrian Lange, Patrick Hsu, Rajesh Illango, Arvind Ramanathan, Anima Anandkumar, David H Brookes, Akosua Busia, Abhishaike Mahajan, Stephen Malina, Neha Prasad, Sam Sinai, Lindsay Edwards, Thomas Gaudelet, Cristian Regep, Martin Steinegger, Burkhard Rost, Alexander Brace, Kyle Hippe, Luca Naef, Keisuke Kamata, George Armstrong, Kevin Boyd, Zhonglin Cao, Han-Yi Chou, Simon Chu, Allan dos Santos Costa, Sajad Darabi, Eric Dawson, Kieran Didi, Cong Fu, Mario Geiger, Michelle Gill, Darren J Hsu, Gagan Kaushik, Maria Korshunova, Steven Kothen-Hill, Youhan Lee, Meng Liu, Micha Livne, Zachary McClure, Jonathan Mitchell, Alireza Moradzadeh, Ohad Mosafi, Youssef Nashed, Saee Paliwal, Yuxing Peng, Sara Rabhi, Farhad Ramezanghorbani, Danny Reidenbach, Camir Ricketts, Brian C Roland, Kushal Shah, Tyler Shimko, Hassan Sirelkhatim, Savitha Srinivasan, Abraham C Stern, Dorota Toczydlowska, Srimukh Prasad Veccham, Niccolò Alberto Elia Venanzi, Anton Vorontsov, Jared Wilber, Isabel Wilkinson, Wei Jing Wong, Eva Xue, Cory Ye, Xin Yu, Yang Zhang, Guoqing Zhou, Becca Zandstein, Alejandro Chacòn, Prashant Sohani, Maximilian Stadler, Christian Hundt, Feiwen Zhu, Christian Dallago, Bruno Trentini, Emine Kucukbenli, Saee Paliwal, Timur Rvachov, Eddie Calleja, Johnny Israeli, Harry Clifford, Risto Haukioja, Nicholas Haemel, Kyle Tretina, Neha Tadimeti, Anthony B Costa*

Main category: cs.LG

TL;DR: The BioNeMo Framework is introduced to streamline the training of AI models for biology and chemistry, enabling efficient large-scale training across hundreds of GPUs.


<details>
  <summary>Details</summary>
Motivation: To address the growing reliance on computational scale for training AI models in drug development, particularly protein language models (pLMs).

Method: The BioNeMo Framework offers a modular design for integrating components like data loaders into existing workflows, supporting community contributions.

Result: Demonstrated by training a 3B-parameter BERT-based pLM on 1T tokens in 4.2 days using 256 NVIDIA A100 GPUs.

Conclusion: BioNeMo is an open-source, free framework designed to enhance AI model training in computational biology and chemistry.

Abstract: Artificial Intelligence models encoding biology and chemistry are opening new
routes to high-throughput and high-quality in-silico drug development. However,
their training increasingly relies on computational scale, with recent protein
language models (pLM) training on hundreds of graphical processing units
(GPUs). We introduce the BioNeMo Framework to facilitate the training of
computational biology and chemistry AI models across hundreds of GPUs. Its
modular design allows the integration of individual components, such as data
loaders, into existing workflows and is open to community contributions. We
detail technical features of the BioNeMo Framework through use cases such as
pLM pre-training and fine-tuning. On 256 NVIDIA A100s, BioNeMo Framework trains
a three billion parameter BERT-based pLM on over one trillion tokens in 4.2
days. The BioNeMo Framework is open-source and free for everyone to use.

</details>


### [437] [Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization](https://arxiv.org/pdf/2505.11695)
*Shihao Zhang, Haoyu Zhang, Ian Colbert, Rayan Saab*

Main category: cs.LG

TL;DR: Qronos is a new post-training quantization algorithm that iteratively corrects quantization errors in neural networks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address errors from weight, activation, and previous layer quantization in neural networks, improving quantization accuracy.

Method: An iterative algorithm alternating between error correction and diffusion, using Cholesky decomposition for efficient least-squares solutions.

Result: Qronos outperforms state-of-the-art adaptive rounding methods in quantizing weights, activations, and KV caches.

Conclusion: Qronos is a superior, efficient, and compatible quantization method for neural networks.

Abstract: We introduce Qronos -- a new state-of-the-art post-training quantization
algorithm that sequentially rounds and updates neural network weights. Qronos
not only explicitly corrects errors due to both weight and activation
quantization, but also errors resulting from quantizing previous layers. Our
iterative algorithm is based on an interpretable and disciplined optimization
framework that subsumes and surpasses existing data-driven approaches. At each
step, Qronos alternates between error correction and diffusion via optimal
update rules. Importantly, we prove that Qronos admits an efficient
implementation that uses the Cholesky decomposition for solving least-squares
problems. We also demonstrate that Qronos is compatible with existing
transformation techniques such as Hadamard-based incoherence processing and
weight-activation scaling equalization, among others. We evaluate Qronos using
recent autoregressive language generation models in the Llama3 family; Qronos
consistently outperforms previous state-of-the-art adaptive rounding methods
when quantizing the weights, activations, and/or KV caches.

</details>


### [438] [Decision Making under the Exponential Family: Distributionally Robust Optimisation with Bayesian Ambiguity Sets](https://arxiv.org/pdf/2411.16829)
*Charita Dellaporta, Patrick O'Hara, Theodoros Damoulas*

Main category: cs.LG

TL;DR: DRO-BAS introduces Bayesian ambiguity sets to hedge against model uncertainty, outperforming existing methods in robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing suboptimal decisions due to model uncertainty or noisy observations in Bayesian inference.

Method: Introduces DRO-BAS with two ambiguity sets (PE and PP), proven to admit strong dual formulations for efficient stochastic programs.

Result: Outperforms existing Bayesian DRO on Newsvendor and achieves faster solve times with comparable robustness on Portfolio problems.

Conclusion: DRO-BAS provides a robust and efficient solution for decision-making under uncertainty.

Abstract: Decision making under uncertainty is challenging as the data-generating
process (DGP) is often unknown. Bayesian inference proceeds by estimating the
DGP through posterior beliefs on the model's parameters. However, minimising
the expected risk under these beliefs can lead to suboptimal decisions due to
model uncertainty or limited, noisy observations. To address this, we introduce
Distributionally Robust Optimisation with Bayesian Ambiguity Sets (DRO-BAS)
which hedges against model uncertainty by optimising the worst-case risk over a
posterior-informed ambiguity set. We provide two such sets, based on posterior
expectations (DRO-BAS(PE)) or posterior predictives (DRO-BAS(PP)) and prove
that both admit, under conditions, strong dual formulations leading to
efficient single-stage stochastic programs which are solved with a sample
average approximation. For DRO-BAS(PE) this covers all conjugate exponential
family members while for DRO-BAS(PP) this is shown under conditions on the
predictive's moment generating function. Our DRO-BAS formulations outperform
existing Bayesian DRO on the Newsvendor problem and achieve faster solve times
with comparable robustness on the Portfolio problem.

</details>


### [439] [VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use](https://arxiv.org/pdf/2505.19255)
*Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, Klara Nahrstedt*

Main category: cs.LG

TL;DR: VTool-R1 trains VLMs to interleave text and visual reasoning steps, improving multimodal reasoning without process supervision.


<details>
  <summary>Details</summary>
Motivation: Existing methods either lack true multimodal reasoning or visual training mechanisms.

Method: Integrates Python-based visual editing tools into RFT, using outcome-based rewards for training.

Result: Enhances reasoning performance on visual question answering tasks.

Conclusion: VTool-R1 successfully enables VLMs to generate multimodal chains of thought with strategic visual tool use.

Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the
reasoning capabilities of large language models (LLMs) by enabling long chains
of thought, self-correction, and effective tool use. While recent works attempt
to extend RFT to vision-language models (VLMs), these efforts largely produce
text-only reasoning conditioned on static image inputs, falling short of true
multimodal reasoning in the response. In contrast, test-time methods like
Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate
multimodal chains of thought by interleaving text and intermediate visual
reasoning steps. VTool-R1 integrates Python-based visual editing tools into the
RFT process, enabling VLMs to learn when and how to generate visual reasoning
steps that benefit final reasoning. Trained with outcome-based rewards tied to
task accuracy, our approach elicits strategic visual tool use for reasoning
without relying on process-based supervision. Experiments on structured visual
question answering over charts and tables show that VTool-R1 enhances reasoning
performance by teaching VLMs to "think with images" and generate multimodal
chain of thoughts with tools.

</details>


### [440] [PASCO (PArallel Structured COarsening): an overlay to speed up graph clustering algorithms](https://arxiv.org/pdf/2412.13592)
*Etienne Lasalle, Rémi Vaudaine, Titouan Vayer, Pierre Borgnat, Rémi Gribonval, Paulo Gonçalves, Màrton Karsai*

Main category: cs.LG

TL;DR: PASCO is a framework that accelerates clustering for large graphs by coarsening, parallel clustering, and optimal transport-based alignment.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering methods like spectral clustering are inefficient for large graphs with many communities.

Method: PASCO uses coarsening to create smaller graphs, runs clustering in parallel, and aligns partitions using optimal transport.

Result: PASCO shows high computational efficiency, structural preservation, and quality partitions on synthetic and real-world graphs.

Conclusion: PASCO effectively addresses scalability and efficiency challenges in large graph clustering.

Abstract: Clustering the nodes of a graph is a cornerstone of graph analysis and has
been extensively studied. However, some popular methods are not suitable for
very large graphs: e.g., spectral clustering requires the computation of the
spectral decomposition of the Laplacian matrix, which is not applicable for
large graphs with a large number of communities. This work introduces PASCO, an
overlay that accelerates clustering algorithms. Our method consists of three
steps: 1-We compute several independent small graphs representing the input
graph by applying an efficient and structure-preserving coarsening algorithm.
2-A clustering algorithm is run in parallel onto each small graph and provides
several partitions of the initial graph. 3-These partitions are aligned and
combined with an optimal transport method to output the final partition. The
PASCO framework is based on two key contributions: a novel global algorithm
structure designed to enable parallelization and a fast, empirically validated
graph coarsening algorithm that preserves structural properties. We demonstrate
the strong performance of 1 PASCO in terms of computational efficiency,
structural preservation, and output partition quality, evaluated on both
synthetic and real-world graph datasets.

</details>


### [441] [From Neural Representations to Interpretable Logic Rules](https://arxiv.org/pdf/2501.08281)
*Chuqin Geng, Xiaojie Xu, Anqi Xing, Ziyu Zhao, Xujie Si*

Main category: cs.LG

TL;DR: NeuroLogic is a novel method to extract interpretable logic rules from neural networks, addressing the black-box problem by translating neural activations into human-understandable rules.


<details>
  <summary>Details</summary>
Motivation: The black-box nature of neural networks limits transparency and trust, especially in high-stakes applications like drug discovery and autonomous driving. Interpretability is crucial but progress has been slow.

Method: NeuroLogic uses neural activation patterns to derive logical rules (hidden predicates). It adapts to various networks, grounding rules in input features for simple networks or high-level visual concepts for complex ones.

Result: NeuroLogic successfully extracts global, interpretable rules from models like ResNet, outperforming existing methods.

Conclusion: NeuroLogic advances interpretability in neural networks, offering a pathway to demystify their decision-making processes.

Abstract: As deep neural networks continue to excel across various domains, their
black-box nature has raised concerns about transparency and trust. In
particular, interpretability has become increasingly essential for applications
that demand high safety and knowledge rigor, such as drug discovery, autonomous
driving, and genomics. However, progress in understanding even the simplest
deep neural networks - such as fully connected networks - has been limited,
despite their role as foundational elements in state-of-the-art models like
ResNet and Transformer. In this paper, we address this challenge by introducing
NeuroLogic, a novel approach for decoding interpretable logic rules from neural
networks. NeuroLogic leverages neural activation patterns to capture the
model's critical decision-making processes, translating them into logical rules
represented by hidden predicates. Thanks to its flexible design in the
grounding phase, NeuroLogic can be adapted to a wide range of neural networks.
For simple fully connected neural networks, hidden predicates can be grounded
in certain split patterns of original input features to derive
decision-tree-like rules. For large, complex vision neural networks, NeuroLogic
grounds hidden predicates into high-level visual concepts that are
understandable to humans. Our empirical study demonstrates that NeuroLogic can
extract global and interpretable rules from state-of-the-art models such as
ResNet, a task at which existing work struggles. We believe NeuroLogic can help
pave the way for understanding the black-box nature of neural networks.

</details>


### [442] [Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning](https://arxiv.org/pdf/2505.23529)
*Shifeng Xie, Aref Einizade, Jhony H. Giraldo*

Main category: cs.LG

TL;DR: The paper introduces SubGEC, a novel self-supervised learning method for graph representation learning, using subgraph Gaussian embeddings and optimal transport distances for robust contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To avoid costly human annotation in graph representation learning by leveraging self-supervised methods, while preserving subgraph characteristics and ensuring controlled distribution.

Method: Proposes SubGEC, which maps subgraphs to a Gaussian space and uses Wasserstein and Gromov-Wasserstein distances for similarity measurement in contrastive learning.

Result: SubGEC outperforms or matches state-of-the-art methods in benchmarks, demonstrating its effectiveness.

Conclusion: The study highlights the significance of contrastive pair distribution in SSL for GRL, offering insights for future method design.

Abstract: Graph Representation Learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of input
subgraph characteristics while generating subgraphs with a controlled
distribution. We then employ optimal transport distances, more precisely the
Wasserstein and Gromov-Wasserstein distances, to effectively measure the
similarity between subgraphs, enhancing the robustness of the contrastive
learning process. Extensive experiments across multiple benchmarks demonstrate
that \method~outperforms or presents competitive performance against
state-of-the-art approaches. Our findings provide insights into the design of
SSL methods for GRL, emphasizing the importance of the distribution of the
generated contrastive pairs.

</details>


### [443] [Permutation-Based Rank Test in the Presence of Discretization and Application in Causal Discovery with Mixed Data](https://arxiv.org/pdf/2501.18990)
*Xinshuai Dong, Ignavier Ng, Boyang Sun, Haoyue Dai, Guang-Yuan Hao, Shunxing Fan, Peter Spirtes, Yumou Qiu, Kun Zhang*

Main category: cs.LG

TL;DR: MPRT is a new rank test for causal discovery that handles discretized variables, controlling Type I errors effectively.


<details>
  <summary>Details</summary>
Motivation: Existing rank tests assume perfect measurement of continuous variables, but many real-world variables are discretized (e.g., psychometric data). MPRT addresses this gap.

Method: MPRT uses permutation-based testing to establish exchangeability and estimate the asymptotic null distribution, ensuring proper error control.

Result: MPRT effectively controls Type I errors with discretized variables, outperforming previous methods, as shown in synthetic and real-world experiments.

Conclusion: MPRT is a robust solution for rank testing in causal discovery, especially with discretized data, validated theoretically and empirically.

Abstract: Recent advances have shown that statistical tests for the rank of
cross-covariance matrices play an important role in causal discovery. These
rank tests include partial correlation tests as special cases and provide
further graphical information about latent variables. Existing rank tests
typically assume that all the continuous variables can be perfectly measured,
and yet, in practice many variables can only be measured after discretization.
For example, in psychometric studies, the continuous level of certain
personality dimensions of a person can only be measured after being discretized
into order-preserving options such as disagree, neutral, and agree. Motivated
by this, we propose Mixed data Permutation-based Rank Test (MPRT), which
properly controls the statistical errors even when some or all variables are
discretized. Theoretically, we establish the exchangeability and estimate the
asymptotic null distribution by permutations; as a consequence, MPRT can
effectively control the Type I error in the presence of discretization while
previous methods cannot. Empirically, our method is validated by extensive
experiments on synthetic data and real-world data to demonstrate its
effectiveness as well as applicability in causal discovery.

</details>


### [444] [SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design](https://arxiv.org/pdf/2506.02089)
*Zeng Wang, Minghao Shao, Rupesh Karn, Likhitha Mankali, Jitendra Bhandari, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel*

Main category: cs.LG

TL;DR: SALAD uses machine unlearning to address data security risks in LLM-aided Verilog code generation, removing contaminants without full retraining.


<details>
  <summary>Details</summary>
Motivation: LLMs pose data security risks like contamination, IP leakage, and malicious code in hardware design automation.

Method: Leverages machine unlearning to selectively remove contaminated benchmarks, sensitive IP, or malicious patterns from pre-trained LLMs.

Result: Demonstrates effectiveness in reducing data security risks through case studies.

Conclusion: Machine unlearning is a viable solution for mitigating security threats in LLM-aided hardware design.

Abstract: Large Language Models (LLMs) offer transformative capabilities for hardware
design automation, particularly in Verilog code generation. However, they also
pose significant data security challenges, including Verilog evaluation data
contamination, intellectual property (IP) design leakage, and the risk of
malicious Verilog generation. We introduce SALAD, a comprehensive assessment
that leverages machine unlearning to mitigate these threats. Our approach
enables the selective removal of contaminated benchmarks, sensitive IP and
design artifacts, or malicious code patterns from pre-trained LLMs, all without
requiring full retraining. Through detailed case studies, we demonstrate how
machine unlearning techniques effectively reduce data security risks in
LLM-aided hardware design.

</details>


### [445] [Worth Their Weight: Randomized and Regularized Block Kaczmarz Algorithms without Preprocessing](https://arxiv.org/pdf/2502.00882)
*Gil Goldshlager, Jiang Hu, Lin Lin*

Main category: cs.LG

TL;DR: The paper introduces ReBlocK, a regularized version of the randomized block Kaczmarz method (RBK), to improve convergence for inconsistent linear least-squares problems with uniform data sampling.


<details>
  <summary>Details</summary>
Motivation: The need for efficient algorithms that sample small portions of large datasets, addressing RBK's limitations in convergence due to expensive preprocessing and poor performance with uniform sampling.

Method: Analyzes RBK under uniform sampling, introduces regularization to control condition number and variance, and proposes the ReBlocK algorithm.

Result: ReBlocK outperforms RBK and minibatch stochastic gradient descent for inconsistent problems with rapidly decaying singular values.

Conclusion: Regularization in RBK (ReBlocK) effectively addresses convergence issues, making it a practical solution for large-scale linear least-squares problems.

Abstract: Due to the ever growing amounts of data leveraged for machine learning and
scientific computing, it is increasingly important to develop algorithms that
sample only a small portion of the data at a time. In the case of linear
least-squares, the randomized block Kaczmarz method (RBK) is an appealing
example of such an algorithm, but its convergence is only understood under
sampling distributions that require potentially prohibitively expensive
preprocessing steps. To address this limitation, we analyze RBK when the data
is sampled uniformly, showing that its iterates converge in a Monte Carlo sense
to a $\textit{weighted}$ least-squares solution. Unfortunately, for general
problems the condition number of the weight matrix and the variance of the
iterates can become arbitrarily large. We control these issues by incorporating
regularization into the RBK iterations, yielding the regularized algorithm
ReBlocK. Numerical experiments including examples arising from natural gradient
optimization demonstrate that ReBlocK can outperform both RBK and minibatch
stochastic gradient descent for inconsistent problems with rapidly decaying
singular values.

</details>


### [446] [Sample Complexity and Representation Ability of Test-time Scaling Paradigms](https://arxiv.org/pdf/2506.05295)
*Baihe Huang, Shanda Li, Tianhao Wu, Yiming Yang, Ameet Talwalkar, Kannan Ramchandran, Michael I. Jordan, Jiantao Jiao*

Main category: cs.LG

TL;DR: The paper analyzes test-time scaling paradigms for LLMs, comparing sample efficiency of strategies like self-consistency and best-of-n, and introduces a theoretical framework for self-correction with verifier feedback.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between empirical success and theoretical understanding of test-time strategies in LLMs, focusing on sample efficiency and multi-task adaptability.

Method: Theoretical analysis of sample efficiency for self-consistency and best-of-n, and an expressiveness result for self-correction with verifier feedback, validated empirically.

Result: Self-consistency requires Θ(1/Δ²) samples, while best-of-n needs Θ(1/Δ). Self-correction enables Transformers to simulate online learning for multi-task settings.

Conclusion: The study provides theoretical insights into test-time strategies, demonstrating practical effectiveness of self-correction for multi-task LLM applications.

Abstract: Test-time scaling paradigms have significantly advanced the capabilities of
large language models (LLMs) on complex tasks. Despite their empirical success,
theoretical understanding of the sample efficiency of various test-time
strategies -- such as self-consistency, best-of-$n$, and self-correction --
remains limited. In this work, we first establish a separation result between
two repeated sampling strategies: self-consistency requires
$\Theta(1/\Delta^2)$ samples to produce the correct answer, while best-of-$n$
only needs $\Theta(1/\Delta)$, where $\Delta < 1$ denotes the probability gap
between the correct and second most likely answers. Next, we present an
expressiveness result for the self-correction approach with verifier feedback:
it enables Transformers to simulate online learning over a pool of experts at
test time. Therefore, a single Transformer architecture can provably solve
multiple tasks without prior knowledge of the specific task associated with a
user query, extending the representation theory of Transformers from
single-task to multi-task settings. Finally, we empirically validate our
theoretical results, demonstrating the practical effectiveness of
self-correction methods.

</details>


### [447] [A User's Guide to Sampling Strategies for Sliced Optimal Transport](https://arxiv.org/pdf/2502.02275)
*Keanu Sisouk, Julie Delon, Julien Tierny*

Main category: cs.LG

TL;DR: A guide to sampling strategies for sliced optimal transport, covering theory, methods, and practical recommendations.


<details>
  <summary>Details</summary>
Motivation: To clarify and enhance understanding of sampling strategies for sliced optimal transport, including theoretical and practical aspects.

Method: Detailed construction methods, generation time complexity, theoretical guarantees, and suitability conditions for each strategy.

Result: Extensive experiments on simulated and real-world data provide a comparative analysis.

Conclusion: Practical recommendations for the best usage of sampling strategies in sliced optimal transport are provided.

Abstract: This paper serves as a user's guide to sampling strategies for sliced optimal
transport. We provide reminders and additional regularity results on the Sliced
Wasserstein distance. We detail the construction methods, generation time
complexity, theoretical guarantees, and conditions for each strategy.
Additionally, we provide insights into their suitability for sliced optimal
transport in theory. Extensive experiments on both simulated and real-world
data offer a representative comparison of the strategies, culminating in
practical recommendations for their best usage.

</details>


### [448] [Density Ratio Estimation with Conditional Probability Paths](https://arxiv.org/pdf/2502.02300)
*Hanlin Yu, Arto Klami, Aapo Hyvärinen, Anna Korba, Omar Chehab*

Main category: cs.LG

TL;DR: A novel framework for time score estimation in high-dimensional density ratio estimation, using a conditioning variable for closed-form objectives, improves speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for time score estimation are computationally expensive and inaccurate, prompting a need for a more efficient and reliable approach.

Method: Introduces a framework using a conditioning variable to enable a closed-form objective function for time score estimation.

Result: Faster learning of the time score and competitive or better density ratio estimation accuracy compared to previous methods.

Conclusion: The proposed method offers improved efficiency and accuracy, with theoretical guarantees on density ratio estimation error.

Abstract: Density ratio estimation in high dimensions can be reframed as integrating a
certain quantity, the time score, over probability paths which interpolate
between the two densities. In practice, the time score has to be estimated
based on samples from the two densities. However, existing methods for this
problem remain computationally expensive and can yield inaccurate estimates.
Inspired by recent advances in generative modeling, we introduce a novel
framework for time score estimation, based on a conditioning variable. Choosing
the conditioning variable judiciously enables a closed-form objective function.
We demonstrate that, compared to previous approaches, our approach results in
faster learning of the time score and competitive or better estimation
accuracies of the density ratio on challenging tasks. Furthermore, we establish
theoretical guarantees on the error of the estimated density ratio.

</details>


### [449] [A hierarchical approach for assessing the vulnerability of tree-based classification models to membership inference attack](https://arxiv.org/pdf/2502.09396)
*Richard J. Preen, Jim Smith*

Main category: cs.LG

TL;DR: The paper introduces two efficient methods to identify vulnerable tree-based models to membership inference attacks (MIA) without expensive computations, using ante-hoc hyperparameter analysis and post-hoc structural metrics.


<details>
  <summary>Details</summary>
Motivation: Machine learning models risk exposing training data properties via MIA. Current evaluation methods are computationally costly, necessitating efficient alternatives.

Method: Proposes ante-hoc hyperparameter analysis and post-hoc structural metrics to hierarchically filter risky models before expensive MIA assessment.

Result: Hyperparameter-based rules predict high-risk combinations accurately without training, and structural metrics effectively indicate vulnerability. Model accuracy doesn't correlate with privacy risk.

Conclusion: The methods enable efficient identification of vulnerable models, optimizing configurations for both performance and privacy without certifying absolute safety.

Abstract: Machine learning models can inadvertently expose confidential properties of
their training data, making them vulnerable to membership inference attacks
(MIA). While numerous evaluation methods exist, many require computationally
expensive processes, such as training multiple shadow models. This article
presents two new complementary approaches for efficiently identifying
vulnerable tree-based models: an ante-hoc analysis of hyperparameter choices
and a post-hoc examination of trained model structure. While these new methods
cannot certify whether a model is safe from MIA, they provide practitioners
with a means to significantly reduce the number of models that need to undergo
expensive MIA assessment through a hierarchical filtering approach.
  More specifically, it is shown that the rank order of disclosure risk for
different hyperparameter combinations remains consistent across datasets,
enabling the development of simple, human-interpretable rules for identifying
relatively high-risk models before training. While this ante-hoc analysis
cannot determine absolute safety since this also depends on the specific
dataset, it allows the elimination of unnecessarily risky configurations during
hyperparameter tuning. Additionally, computationally inexpensive structural
metrics serve as indicators of MIA vulnerability, providing a second filtering
stage to identify risky models after training but before conducting expensive
attacks. Empirical results show that hyperparameter-based risk prediction rules
can achieve high accuracy in predicting the most at risk combinations of
hyperparameters across different tree-based model types, while requiring no
model training. Moreover, target model accuracy is not seen to correlate with
privacy risk, suggesting opportunities to optimise model configurations for
both performance and privacy.

</details>


### [450] [GraphThought: Graph Combinatorial Optimization with Thought Generation](https://arxiv.org/pdf/2502.11607)
*Zixiao Huang, Lifeng Guo, Wenhao Li, Junjie Sheng, Chuyun Shen, Haosheng Chen, Bo Jin, Changhong Lu, Xiangfeng Wang*

Main category: cs.LG

TL;DR: GraphThought framework enhances LLMs for GCO tasks by structured reasoning, outperforming larger models like DeepSeek-V3.


<details>
  <summary>Details</summary>
Motivation: Traditional solvers dominate GCO, but LLMs struggle with complex tasks due to hallucinated steps.

Method: Introduces Optimal Thoughts Design (OTD) and GraphThought framework for structured reasoning, fine-tuning LLMs like Llama-GT.

Result: Llama-GT achieves state-of-the-art on GraphArena, outperforming larger models.

Conclusion: Structured reasoning priors enhance LLM performance on GCO without scaling model size.

Abstract: Graph combinatorial optimization (GCO) problems are central to domains like
logistics and bioinformatics. While traditional solvers dominate, large
language models (LLMs) offer new possibilities for structured reasoning, yet
struggle with complex GCO tasks requiring rigorous combinatorial analysis and
multi-step deduction, often producing hallucinated steps. We first formalize
the Optimal Thoughts Design (OTD) problem, which provides a structured guidance
for producing high-quality intermediate reasoning steps. Building on this
formulation, we introduce GraphThought, a novel framework that generates
effective reasoning sequences through either heuristic-guided forward search or
solver-aligned backward reasoning. By fine-tuning LLMs on these structured
thought sequences, we develop Llama-GT, an 8B-parameter model that achieves
state-of-the-art performance on the GraphArena benchmark, outperforming
significantly larger models like DeepSeek-V3. Our results demonstrate that when
scaffolded with structured reasoning priors, principled thought generation can
significantly enhance LLM performance on GCO tasks without requiring increased
model scale.

</details>


### [451] [ETS: Efficient Tree Search for Inference-Time Scaling](https://arxiv.org/pdf/2502.13575)
*Coleman Hooper, Sehoon Kim, Suhong Moon, Kerem Dilmen, Monishwaran Maheswaran, Nicholas Lee, Michael W. Mahoney, Sophia Shao, Kurt Keutzer, Amir Gholami*

Main category: cs.LG

TL;DR: Efficient Tree Search (ETS) improves test-time compute scaling by balancing trajectory diversity and KV cache sharing, reducing memory use and increasing throughput.


<details>
  <summary>Details</summary>
Motivation: Existing search methods either lack exploration or suffer high latency due to divergent trajectories. ETS aims to optimize this trade-off.

Method: ETS uses a linear programming cost model to prune redundant trajectories while retaining semantically diverse ones, promoting KV cache sharing.

Result: ETS reduces KV cache size by 1.8× and increases throughput by 1.4× with minimal accuracy loss.

Conclusion: ETS effectively balances exploration and efficiency, enhancing search performance without custom kernels.

Abstract: Test-time compute scaling has emerged as a new axis along which to improve
model accuracy, where additional computation is used at inference time to allow
the model to think longer for more challenging problems. One promising approach
for test-time compute scaling is search against a process reward model, where a
model generates multiple potential candidates at each step of the search, and
these partial trajectories are then scored by a separate reward model in order
to guide the search process. The diversity of trajectories in the tree search
process affects the accuracy of the search, since increasing diversity promotes
more exploration. However, this diversity comes at a cost, as divergent
trajectories have less KV sharing, which means they consume more memory and
slow down the search process. Previous search methods either do not perform
sufficient exploration, or else explore diverse trajectories but have high
latency. We address this challenge by proposing Efficient Tree Search (ETS),
which promotes KV sharing by pruning redundant trajectories while maintaining
necessary diverse trajectories. ETS incorporates a linear programming cost
model to promote KV cache sharing by penalizing the number of nodes retained,
while incorporating a semantic coverage term into the cost model to ensure that
we retain trajectories which are semantically different. We demonstrate how ETS
can achieve 1.8$\times$ reduction in average KV cache size during the search
process, leading to 1.4$\times$ increased throughput relative to prior
state-of-the-art methods, with minimal accuracy degradation and without
requiring any custom kernel implementation. Code is available at:
https://github.com/SqueezeAILab/ETS.

</details>


### [452] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/pdf/2506.09202)
*Hao Hu, Xinqi Wang, Simon Shaolei Du*

Main category: cs.LG

TL;DR: The paper introduces a novel task of clustering trajectories from offline RL datasets, proposing two methods (PG-Kmeans and CAAE) and validating their effectiveness on D4RL and GridWorld datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to cluster trajectories by identifying the policies that generated them, leveraging KL-divergence and policy-induced distributions.

Method: Two methods are proposed: PG-Kmeans (iteratively trains BC policies and assigns trajectories) and CAAE (guides latent representations toward codebook entries).

Result: Both methods effectively partition trajectories into meaningful clusters, demonstrating their utility in offline RL.

Conclusion: The methods provide a promising framework for policy-based trajectory clustering, with potential applications in offline RL and other domains.

Abstract: We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [453] [Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning Solution for Protocols Compliance](https://arxiv.org/pdf/2502.15475)
*Yongli Yan, Linglong Dai*

Main category: cs.LG

TL;DR: A unified LSTM-based decoding architecture is proposed to address challenges in punctured codes, outperforming traditional methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional neural network-based decoding struggles with punctured codes, variable code rates, and protocol compatibility.

Method: The paper introduces an LSTM-based architecture with a puncture embedding mechanism and balanced bit error rate training for adaptability and robustness.

Result: Simulations show the method outperforms conventional techniques in Additive White Gaussian Noise and Rayleigh fading channels.

Conclusion: LSTM-based decoding is a promising solution for next-generation AI-powered communication systems.

Abstract: Neural network-based decoding methods have shown promise in enhancing error
correction performance, but traditional approaches struggle with the challenges
posed by punctured codes. In particular, these methods fail to address the
complexities of variable code rates and the need for protocol compatibility.
This paper presents a unified Long Short-Term Memory (LSTM)-based decoding
architecture specifically designed to overcome these challenges. The proposed
method unifies punctured convolutional and Turbo codes. A puncture embedding
mechanism integrates puncturing patterns directly into the network, enabling
seamless adaptation to varying code rates, while balanced bit error rate
training ensures robustness across different code lengths, rates, and channels,
maintaining protocol flexibility. Extensive simulations in Additive White
Gaussian Noise and Rayleigh fading channels demonstrate that the proposed
approach outperforms conventional decoding techniques, providing significant
improvements in decoding accuracy and robustness. These results underscore the
potential of LSTM-based decoding as a promising solution for next-generation
artificial intelligence powered communication systems.

</details>


### [454] [Heavy-Tailed Linear Bandits: Huber Regression with One-Pass Update](https://arxiv.org/pdf/2503.00419)
*Jing Wang, Yu-Jie Zhang, Peng Zhao, Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: The paper proposes a one-pass algorithm for stochastic linear bandits with heavy-tailed noise, reducing computational costs while achieving near-optimal regret.


<details>
  <summary>Details</summary>
Motivation: Existing methods for heavy-tailed noise in bandits rely on restrictive assumptions or suffer high computational costs, limiting their practicality.

Method: A one-pass algorithm based on online mirror descent, updating only with current data per round to reduce computational overhead.

Result: Achieves a near-optimal, variance-aware regret bound with per-round cost reduced from O(t log T) to O(1).

Conclusion: The proposed method efficiently addresses heavy-tailed noise in bandits with improved computational scalability.

Abstract: We study the stochastic linear bandits with heavy-tailed noise. Two
principled strategies for handling heavy-tailed noise, truncation and
median-of-means, have been introduced to heavy-tailed bandits. Nonetheless,
these methods rely on specific noise assumptions or bandit structures, limiting
their applicability to general settings. The recent work [Huang et al.2024]
develops a soft truncation method via the adaptive Huber regression to address
these limitations. However, their method suffers undesired computational costs:
it requires storing all historical data and performing a full pass over these
data at each round. In this paper, we propose a \emph{one-pass} algorithm based
on the online mirror descent framework. Our method updates using only current
data at each round, reducing the per-round computational cost from
$\mathcal{O}(t \log T)$ to $\mathcal{O}(1)$ with respect to current round $t$
and the time horizon $T$, and achieves a near-optimal and variance-aware regret
of order $\widetilde{\mathcal{O}}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}}
\sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$
where $d$ is the dimension and $\nu_t^{1+\epsilon}$ is the $(1+\epsilon)$-th
central moment of reward at round $t$.

</details>


### [455] [Evolutionary Prediction Games](https://arxiv.org/pdf/2503.03401)
*Eden Saig, Nir Rosenfeld*

Main category: cs.LG

TL;DR: The paper introduces evolutionary prediction games to model feedback loops between prediction algorithms and user behavior, showing how competition or coexistence emerges under different learning constraints.


<details>
  <summary>Details</summary>
Motivation: To understand how disparities in prediction quality affect user engagement and model evolution, and to explore the dynamics of feedback loops in learning systems.

Method: Theoretical framework based on evolutionary game theory, analyzing idealized vs. realistic learning settings, and empirical validation.

Result: In idealized settings, competition dominates, but under realistic constraints (finite data, compute limits), stable coexistence and symbiosis are possible.

Conclusion: The study highlights the importance of real-world constraints in shaping learning dynamics and offers insights into sustaining diverse user groups.

Abstract: When a prediction algorithm serves a collection of users, disparities in
prediction quality are likely to emerge. If users respond to accurate
predictions by increasing engagement, inviting friends, or adopting trends,
repeated learning creates a feedback loop that shapes both the model and the
population of its users. In this work, we introduce evolutionary prediction
games, a framework grounded in evolutionary game theory which models such
feedback loops as natural-selection processes among groups of users. Our
theoretical analysis reveals a gap between idealized and real-world learning
settings: In idealized settings with unlimited data and computational power,
repeated learning creates competition and promotes competitive exclusion across
a broad class of behavioral dynamics. However, under realistic constraints such
as finite data, limited compute, or risk of overfitting, we show that stable
coexistence and mutualistic symbiosis between groups becomes possible. We
analyze these possibilities in terms of their stability and feasibility,
present mechanisms that can sustain their existence, and empirically
demonstrate our findings.

</details>


### [456] [Pretraining Generative Flow Networks with Inexpensive Rewards for Molecular Graph Generation](https://arxiv.org/pdf/2503.06337)
*Mohit Pandey, Gopeshh Subbaraj, Artem Cherkasov, Martin Ester, Emmanuel Bengio*

Main category: cs.LG

TL;DR: A-GFNs use atoms as building blocks for more comprehensive exploration of drug-like chemical space, outperforming baselines in drug design tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of predefined molecular fragments in GFlowNets by using individual atoms for broader chemical space exploration.

Method: Unsupervised pre-training with drug-like molecule datasets and goal-conditioned finetuning for specific target properties.

Result: A-GFNs show effectiveness in drug design tasks, demonstrated via robust evaluation metrics.

Conclusion: A-GFNs offer a promising approach for diverse and high-quality molecular structure generation.

Abstract: Generative Flow Networks (GFlowNets) have recently emerged as a suitable
framework for generating diverse and high-quality molecular structures by
learning from rewards treated as unnormalized distributions. Previous works in
this framework often restrict exploration by using predefined molecular
fragments as building blocks, limiting the chemical space that can be accessed.
In this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative
model leveraging individual atoms as building blocks to explore drug-like
chemical space more comprehensively. We propose an unsupervised pre-training
approach using drug-like molecule datasets, which teaches A-GFNs about
inexpensive yet informative molecular descriptors such as drug-likeliness,
topological polar surface area, and synthetic accessibility scores. These
properties serve as proxy rewards, guiding A-GFNs towards regions of chemical
space that exhibit desirable pharmacological properties. We further implement a
goal-conditioned finetuning process, which adapts A-GFNs to optimize for
specific target properties. In this work, we pretrain A-GFN on a subset of ZINC
dataset, and by employing robust evaluation metrics we show the effectiveness
of our approach when compared to other relevant baseline methods for a wide
range of drug design tasks. The code is accessible at
https://github.com/diamondspark/AGFN.

</details>


### [457] [Graph-Dependent Regret Bounds in Multi-Armed Bandits with Interference](https://arxiv.org/pdf/2503.07555)
*Fateme Jamshidi, Mohammad Shahverdikondori, Negar Kiyavash*

Main category: cs.LG

TL;DR: A novel algorithm for multi-armed bandits under network interference leverages local graph structure to minimize regret, with near-optimal performance and Pareto optimality when the graph is unknown.


<details>
  <summary>Details</summary>
Motivation: Address the computational impracticality of standard approaches in multi-armed bandits under network interference due to exponentially large action spaces.

Method: Propose an algorithm utilizing local graph structure to minimize regret, with variants for known and unknown interference graphs.

Result: Derive graph-dependent upper and lower bounds on regret, showing near-optimal performance. Numerical experiments confirm superiority over baselines.

Conclusion: The algorithm is nearly optimal for both dense and sparse graphs and Pareto optimal when the graph is unknown, outperforming existing methods.

Abstract: We study multi-armed bandits under network interference, where each unit's
reward depends on its own treatment and those of its neighbors in a given
graph. This induces an exponentially large action space, making standard
approaches computationally impractical. We propose a novel algorithm that uses
the local graph structure to minimize regret. We derive a graph-dependent upper
bound on cumulative regret that improves over prior work. Additionally, we
provide the first lower bounds for bandits with arbitrary network interference,
where each bound involves a distinct structural property of the graph. These
bounds show that for both dense and sparse graphs, our algorithm is nearly
optimal, with matching upper and lower bounds up to logarithmic factors. When
the interference graph is unknown, a variant of our algorithm is Pareto
optimal: no algorithm can uniformly outperform it across all instances. We
complement our theoretical results with numerical experiments, showing that our
approach outperforms the baseline methods.

</details>


### [458] [Large Scale Multi-Task Bayesian Optimization with Large Language Models](https://arxiv.org/pdf/2503.08131)
*Yimeng Zeng, Natalie Maus, Haydn Thomas Jones, Jeffrey Tao, Fangping Wan, Marcelo Der Torossian Torres, Cesar de la Fuente-Nunez, Ryan Marcus, Osbert Bastani, Jacob R. Gardner*

Main category: cs.LG

TL;DR: A novel method using LLMs for multi-task Bayesian optimization improves efficiency by fine-tuning on past task solutions and generating better initialization points, scaling to 1500 tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-task Bayesian optimization by leveraging LLMs to learn from past tasks and improve efficiency for new ones.

Method: Fine-tune an LLM on high-quality solutions from Bayesian optimization, use it to generate initialization points for new tasks, and iteratively improve the LLM with new search trajectories.

Result: The method creates a positive feedback loop, improving initialization quality and optimization performance, reducing oracle calls.

Conclusion: LLMs can significantly enhance multi-task Bayesian optimization, achieving better solutions with fewer resources.

Abstract: In multi-task Bayesian optimization, the goal is to leverage experience from
optimizing existing tasks to improve the efficiency of optimizing new ones.
While approaches using multi-task Gaussian processes or deep kernel transfer
exist, the performance improvement is marginal when scaling beyond a moderate
number of tasks. We introduce a novel approach leveraging large language models
(LLMs) to learn from, and improve upon, previous optimization trajectories,
scaling to approximately 1500 distinct tasks. Specifically, we propose a
feedback loop in which an LLM is fine-tuned on the high quality solutions to
specific tasks found by Bayesian optimization (BO). This LLM is then used to
generate initialization points for future BO searches for new tasks. The
trajectories of these new searches provide additional training data for
fine-tuning the LLM, completing the loop. We evaluate our method on two
distinct domains: database query optimization and antimicrobial peptide design.
Results demonstrate that our approach creates a positive feedback loop, where
the LLM's generated initializations gradually improve, leading to better
optimization performance. As this feedback loop continues, we find that the LLM
is eventually able to generate solutions to new tasks in just a few shots that
are better than the solutions produced by "from scratch" by Bayesian
optimization while simultaneously requiring significantly fewer oracle calls.

</details>


### [459] [Learning richness modulates equality reasoning in neural networks](https://arxiv.org/pdf/2503.09781)
*William L. Tong, Cengiz Pehlevan*

Main category: cs.LG

TL;DR: The paper explores equality reasoning in neural networks, proposing a spectrum from conceptual to perceptual behavior based on learning richness in MLPs.


<details>
  <summary>Details</summary>
Motivation: To clarify the underlying principles of equality reasoning in neural networks, given the lack of consensus despite extensive study.

Method: Develops a mathematical theory for MLPs, categorizing behavior into conceptual (rich-regime) and perceptual (lazy-regime) outcomes, validated through vision SD experiments.

Result: Rich-regime MLPs exhibit conceptual behavior (efficient, task-specific), while lazy-regime MLPs show perceptual behavior (sensitive to details, slower learning).

Conclusion: Feature learning richness is key to equality reasoning, suggesting parallels in human and animal neural circuits.

Abstract: Equality reasoning is ubiquitous and purely abstract: sameness or difference
may be evaluated no matter the nature of the underlying objects. As a result,
same-different (SD) tasks have been extensively studied as a starting point for
understanding abstract reasoning in humans and across animal species. With the
rise of neural networks that exhibit striking apparent proficiency for
abstractions, equality reasoning in these models has also gained interest. Yet
despite extensive study, conclusions about equality reasoning vary widely and
with little consensus. To clarify the underlying principles in learning SD
tasks, we develop a theory of equality reasoning in multi-layer perceptrons
(MLP). Following observations in comparative psychology, we propose a spectrum
of behavior that ranges from conceptual to perceptual outcomes. Conceptual
behavior is characterized by task-specific representations, efficient learning,
and insensitivity to spurious perceptual details. Perceptual behavior is
characterized by strong sensitivity to spurious perceptual details, accompanied
by the need for exhaustive training to learn the task. We develop a
mathematical theory to show that an MLP's behavior is driven by learning
richness. Rich-regime MLPs exhibit conceptual behavior, whereas lazy-regime
MLPs exhibit perceptual behavior. We validate our theoretical findings in
vision SD experiments, showing that rich feature learning promotes success by
encouraging hallmarks of conceptual behavior. Overall, our work identifies
feature learning richness as a key parameter modulating equality reasoning, and
suggests that equality reasoning in humans and animals may similarly depend on
learning richness in neural circuits.

</details>


### [460] [Unveiling the Role of Randomization in Multiclass Adversarial Classification: Insights from Graph Theory](https://arxiv.org/pdf/2503.14299)
*Lucas Gnecco-Heredia, Matteo Sammut, Muni Sreenivas Pydi, Rafael Pinot, Benjamin Negrevergne, Yann Chevaleyre*

Main category: cs.LG

TL;DR: The paper explores how randomization improves adversarial robustness in multiclass classification, using graph theory and set packing to identify key structural conditions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical analysis of randomization's role in adversarial robustness, particularly in multiclass settings.

Method: Leverages graph theory and set packing problems to analyze discrete data distributions.

Result: Identifies three structural conditions for randomization to enhance robustness and demonstrates its effectiveness in multiclass scenarios.

Conclusion: Randomization significantly reduces adversarial risk in multiclass classification, highlighting its potential for robustness.

Abstract: Randomization as a mean to improve the adversarial robustness of machine
learning models has recently attracted significant attention. Unfortunately,
much of the theoretical analysis so far has focused on binary classification,
providing only limited insights into the more complex multiclass setting. In
this paper, we take a step toward closing this gap by drawing inspiration from
the field of graph theory. Our analysis focuses on discrete data distributions,
allowing us to cast the adversarial risk minimization problems within the
well-established framework of set packing problems. By doing so, we are able to
identify three structural conditions on the support of the data distribution
that are necessary for randomization to improve robustness. Furthermore, we are
able to construct several data distributions where (contrarily to binary
classification) switching from a deterministic to a randomized solution
significantly reduces the optimal adversarial risk. These findings highlight
the crucial role randomization can play in enhancing robustness to adversarial
attacks in multiclass classification.

</details>


### [461] [Diffusion-Free Graph Generation with Next-Scale Prediction](https://arxiv.org/pdf/2503.23612)
*Samuel Belkadi, Steve Hong, Marian Chen, Miruna Cretu, Charles Harris, Pietro Lio*

Main category: cs.LG

TL;DR: MAG is a diffusion-free graph generation framework using next-scale prediction, avoiding explicit node ordering and achieving fast, high-quality generation.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models require explicit sequence order, which conflicts with graph's unordered nature, while diffusion models are computationally expensive.

Method: MAG leverages a hierarchy of latent representations to progressively generate graph scales without node ordering.

Result: Achieves up to 1000x inference speedup over state-of-the-art methods while maintaining high-quality generation.

Conclusion: MAG offers a scalable, efficient alternative for graph generation, combining the benefits of autoregressive and diffusion models.

Abstract: Autoregressive models excel in efficiency and plug directly into the
transformer ecosystem, delivering robust generalization, predictable
scalability, and seamless workflows such as fine-tuning and parallelized
training. However, they require an explicit sequence order, which contradicts
the unordered nature of graphs. In contrast, diffusion models maintain
permutation invariance and enable one-shot generation but require up to
thousands of denoising steps and additional features for expressivity, leading
to high computational costs. Inspired by recent breakthroughs in image
generation, especially the success of visual autoregressive methods, we propose
MAG, a novel diffusion-free graph generation framework based on next-scale
prediction. By leveraging a hierarchy of latent representations, the model
progressively generates scales of the entire graph without the need for
explicit node ordering. Experiments on both generic and molecular graph
datasets demonstrated the potential of this method, achieving inference
speedups of up to three orders of magnitude over state-of-the-art methods,
while preserving high-quality generation.

</details>


### [462] [A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction](https://arxiv.org/pdf/2505.05094)
*Leming Zhou, Zuo Wang, Zhixuan Duan*

Main category: cs.LG

TL;DR: The paper proposes a Conjoint Graph Representation Learning (CGRL) framework to predict diabetes and coronary heart disease risks by analyzing comorbidity networks, achieving higher accuracy than other models.


<details>
  <summary>Details</summary>
Motivation: Early identification of hypertension comorbidities is challenging but crucial for intervention. The study aims to improve prediction by leveraging joint graph learning and network analysis.

Method: CGRL constructs patient and disease difference networks, generates comorbidity network features, and combines computational structure intervention with feature learning for risk prediction.

Result: The framework outperforms other models in accuracy, highlighting the importance of network features derived from the difference network.

Conclusion: CGRL effectively predicts disease risks and reveals comorbidity patterns, offering insights into disease progression and pathogenesis.

Abstract: The comorbidities of hypertension impose a heavy burden on patients and
society. Early identification is necessary to prompt intervention, but it
remains a challenging task. This study aims to address this challenge by
combining joint graph learning with network analysis. Motivated by this
discovery, we develop a Conjoint Graph Representation Learning (CGRL) framework
that: a) constructs two networks based on disease coding, including the patient
network and the disease difference network. Three comorbidity network features
were generated based on the basic difference network to capture the potential
relationship between comorbidities and risk diseases; b) incorporates
computational structure intervention and learning feature representation, CGRL
was developed to predict the risks of diabetes and coronary heart disease in
patients; and c) analysis the comorbidity patterns and exploring the pathways
of disease progression, the pathological pathogenesis of diabetes and coronary
heart disease may be revealed. The results show that the network features
extracted based on the difference network are important, and the framework we
proposed provides more accurate predictions than other strong models in terms
of accuracy.

</details>


### [463] [Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games](https://arxiv.org/pdf/2505.16401)
*Xiaoqing Zhang, Huabin Zheng, Ang Lv, Yuhan Liu, Zirui Song, Xiuying Chen, Rui Yan, Flood Sung*

Main category: cs.LG

TL;DR: The paper introduces Divide-Fuse-Conquer, a framework to improve generalization in multi-scenario RL for LLMs, achieving competitive performance in TextArena games.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of RL in multi-scenario games where policies struggle to generalize due to diverse rules and complexities.

Method: Divide-Fuse-Conquer: grouping games by characteristics, training specialized models per group (divide), fusing parameters, and training for generalization (fuse-conquer).

Result: Qwen2.5-32B-Align trained with this method matches Claude3.5's performance, with 7 wins and 4 draws in 18 TextArena games.

Conclusion: The framework successfully enhances generalization in multi-scenario RL, inspiring future research on RL for LLMs.

Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.

</details>


### [464] [Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models](https://arxiv.org/pdf/2505.17769)
*Patrick Leask, Neel Nanda, Noura Al Moubayed*

Main category: cs.LG

TL;DR: ITDA models offer a faster, cheaper alternative to sparse autoencoders (SAEs) for decomposing LLM activations, enabling cross-model comparisons with competitive performance.


<details>
  <summary>Details</summary>
Motivation: High training costs and lack of transferability in SAEs limit their use for large models and cross-model analysis.

Method: ITDA models greedily construct a dictionary of activations from prompts, selecting poorly approximated ones via matching pursuit.

Result: ITDAs train in 1% of SAE time/data, work on large models (e.g., Llama-3.1 70B/405B), and enable cross-model comparisons with better similarity metrics.

Conclusion: ITDAs are a practical alternative to SAEs for resource-limited scenarios or cross-model analysis, despite some performance trade-offs.

Abstract: Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage
Models (LLM) activations into interpretable latents. However, due to their
substantial training cost, most academic research uses open-source SAEs which
are only available for a restricted set of models of up to 27B parameters. SAE
latents are also learned from a dataset of activations, which means they do not
transfer between models. Motivated by relative representation similarity
measures, we introduce Inference-Time Decomposition of Activations (ITDA)
models, an alternative method for decomposing language model activations. To
train an ITDA, we greedily construct a dictionary of language model activations
on a dataset of prompts, selecting those activations which were worst
approximated by matching pursuit on the existing dictionary. ITDAs can be
trained in just 1% of the time required for SAEs, using 1% of the data. This
allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.
ITDAs can achieve similar reconstruction performance to SAEs on some target
LLMs, but generally incur a performance penalty. However, ITDA dictionaries
enable cross-model comparisons, and a simple Jaccard similarity index on ITDA
dictionaries outperforms existing methods like CKA, SVCCA, and relative
representation similarity metrics. ITDAs provide a cheap alternative to SAEs
where computational resources are limited, or when cross model comparisons are
necessary. Code available at https://github.com/pleask/itda.

</details>


### [465] [VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis](https://arxiv.org/pdf/2505.18570)
*Tina Khezresmaeilzadeh, Parsa Razmara, Seyedarmin Azizi, Mohammad Erfan Sadeghi, Erfan Baghaei Potraghloo*

Main category: cs.LG

TL;DR: VISTA is a training-free, multi-modal framework using Vision-Language Models (VLMs) for stock price prediction, outperforming traditional methods by up to 89.83%.


<details>
  <summary>Details</summary>
Motivation: Stock price prediction is complex and high-stakes, with traditional methods often missing complementary patterns in data.

Method: VISTA combines textual and visual representations of stock data in a zero-shot setting with chain-of-thought prompts.

Result: VISTA outperforms baselines like ARIMA and text-only LLMs by up to 89.83%.

Conclusion: Multi-modal inference with VLMs is effective for stock forecasting without task-specific training.

Abstract: Stock price prediction remains a complex and high-stakes task in financial
analysis, traditionally addressed using statistical models or, more recently,
language models. In this work, we introduce VISTA (Vision-Language Inference
for Stock Time-series Analysis), a novel, training-free framework that
leverages Vision-Language Models (VLMs) for multi-modal stock forecasting.
VISTA prompts a VLM with both textual representations of historical stock
prices and their corresponding line charts to predict future price values. By
combining numerical and visual modalities in a zero-shot setting and using
carefully designed chain-of-thought prompts, VISTA captures complementary
patterns that unimodal approaches often miss. We benchmark VISTA against
standard baselines, including ARIMA and text-only LLM-based prompting methods.
Experimental results show that VISTA outperforms these baselines by up to
89.83%, demonstrating the effectiveness of multi-modal inference for stock
time-series analysis and highlighting the potential of VLMs in financial
forecasting tasks without requiring task-specific training.

</details>


### [466] [Measuring Representational Shifts in Continual Learning: A Linear Transformation Perspective](https://arxiv.org/pdf/2505.20970)
*Joonkyu Kim, Yejin Kim, Jy-yong Sohn*

Main category: cs.LG

TL;DR: The paper introduces a theoretical analysis of representation forgetting in continual learning, proposing a new metric called representation discrepancy to measure it. Key findings include faster forgetting in higher layers and slower forgetting with wider networks, validated on real datasets.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting in continual learning is a critical issue, and understanding representation forgetting at hidden layers is essential for improving learning models.

Method: The authors introduce representation discrepancy as a metric to measure representation forgetting and analyze it theoretically. They validate findings through experiments on Split-CIFAR100 and ImageNet1K.

Result: Forgetting occurs more rapidly in higher layers, while wider networks slow the process. The proposed metric effectively captures representation forgetting.

Conclusion: The theoretical and empirical analysis provides insights into representation forgetting dynamics, aiding the design of better continual learning models.

Abstract: In continual learning scenarios, catastrophic forgetting of previously
learned tasks is a critical issue, making it essential to effectively measure
such forgetting. Recently, there has been growing interest in focusing on
representation forgetting, the forgetting measured at the hidden layer. In this
paper, we provide the first theoretical analysis of representation forgetting
and use this analysis to better understand the behavior of continual learning.
First, we introduce a new metric called representation discrepancy, which
measures the difference between representation spaces constructed by two
snapshots of a model trained through continual learning. We demonstrate that
our proposed metric serves as an effective surrogate for the representation
forgetting while remaining analytically tractable. Second, through mathematical
analysis of our metric, we derive several key findings about the dynamics of
representation forgetting: the forgetting occurs more rapidly to a higher
degree as the layer index increases, while increasing the width of the network
slows down the forgetting process. Third, we support our theoretical findings
through experiments on real image datasets, including Split-CIFAR100 and
ImageNet1K.

</details>


### [467] [Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/pdf/2505.23555)
*Yanzhao Hou, Jiaxiang Geng, Boyu Li, Xiaofeng Tao, Juncheng Wang, Xiaodong Xu, Bing Luo*

Main category: cs.LG

TL;DR: Proposes an adaptive federated LoRA strategy with independent client sampling to minimize wall-clock convergence time in federated fine-tuning, addressing system and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Existing federated LoRA methods overlook system and data heterogeneity, leading to suboptimal training efficiency.

Method: Derives a convergence bound for federated LoRA, introduces adaptive bandwidth allocation, and solves a non-convex optimization problem for optimal LoRA sketching ratios and sampling probabilities.

Result: Significantly reduces wall-clock training time compared to state-of-the-art methods.

Conclusion: The proposed adaptive federated LoRA strategy effectively optimizes training efficiency under heterogeneous conditions.

Abstract: Federated LoRA has emerged as a promising technique for efficiently
fine-tuning large language models (LLMs) on distributed devices by reducing the
number of trainable parameters. However, existing approaches often inadequately
overlook the theoretical and practical implications of system and data
heterogeneity, thereby failing to optimize the overall training efficiency,
particularly in terms of wall-clock time. In this paper, we propose an adaptive
federated LoRA strategy with independent client sampling to minimize the
convergence wall-clock time of federated fine-tuning under both computation and
communication heterogeneity. We first derive a new convergence bound for
federated LoRA with arbitrary and independent client sampling, notably without
requiring the stringent bounded gradient assumption. Then, we introduce an
adaptive bandwidth allocation scheme that accounts for heterogeneous client
resources and system bandwidth constraints. Based on the derived theory, we
formulate and solve a non-convex optimization problem to jointly determine the
LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock
convergence time. An efficient and low-complexity algorithm is developed to
approximate the solution. Finally, extensive experiments demonstrate that our
approach significantly reduces wall-clock training time compared to
state-of-the-art methods across various models and datasets.

</details>


### [468] [Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet -- A ResNet-based Model Classification Dataset](https://arxiv.org/pdf/2506.00476)
*Abhisek Ray, Lukas Esterle*

Main category: cs.LG

TL;DR: The paper introduces ModelNet, a novel FL dataset with client-specific variants (homogeneous, heterogeneous, random) to address privacy and domain heterogeneity challenges in FL. It proposes a privacy-preserving FL algorithm and validates the dataset's effectiveness for classical and graph-based FL research.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns and the lack of domain heterogeneity in FL benchmarks, the paper aims to provide a realistic dataset for rigorous evaluation.

Method: Modifies CIFAR100 into three client-specific variants, trains them on ResNet50, and proposes a privacy-preserving FL algorithm using anonymized model parameters.

Result: ModelNet variants (S, D, R) effectively simulate non-IID data and client diversity, validated through experiments on domain shifts and aggregation strategies.

Conclusion: ModelNet serves as a practical benchmark for FL research, especially for classical and graph-based algorithms, with publicly available dataset and code.

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for training
machine learning models across distributed data sources while preserving data
locality. However, the privacy of local data is always a pivotal concern and
has received a lot of attention in recent research on the FL regime. Moreover,
the lack of domain heterogeneity and client-specific segregation in the
benchmarks remains a critical bottleneck for rigorous evaluation. In this
paper, we introduce ModelNet, a novel image classification dataset constructed
from the embeddings extracted from a pre-trained ResNet50 model. First, we
modify the CIFAR100 dataset into three client-specific variants, considering
three domain heterogeneities (homogeneous, heterogeneous, and random).
Subsequently, we train each client-specific subset of all three variants on the
pre-trained ResNet50 model to save model parameters. In addition to
multi-domain image data, we propose a new hypothesis to define the FL algorithm
that can access the anonymized model parameters to preserve the local privacy
in a more effective manner compared to existing ones. ModelNet is designed to
simulate realistic FL settings by incorporating non-IID data distributions and
client diversity design principles in the mainframe for both conventional and
futuristic graph-driven FL algorithms. The three variants are ModelNet-S,
ModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and
random data settings, respectively. To the best of our knowledge, we are the
first to propose a cross-environment client-specific FL dataset along with the
graph-based variant. Extensive experiments based on domain shifts and
aggregation strategies show the effectiveness of the above variants, making it
a practical benchmark for classical and graph-based FL research. The dataset
and related code are available online.

</details>


### [469] [RsGCN: Rescaling Enhances Generalization of GCNs for Solving Scalable Traveling Salesman Problems](https://arxiv.org/pdf/2506.00533)
*Junquan Huang, Zong-Gan Chen, Yuncheng Jiang, Zhi-Hui Zhan*

Main category: cs.LG

TL;DR: Proposes RsGCN and Re2Opt for scalable TSP solving, improving generalization and reducing training costs.


<details>
  <summary>Details</summary>
Motivation: Address poor generalization and high training costs in neural TSP solvers.

Method: Uses RsGCN with rescaling mechanisms for nodes/edges and Re2Opt for post-search optimization.

Result: Achieves state-of-the-art performance on 20-10K node instances with minimal training.

Conclusion: Combined RsGCN and Re2Opt offers scalable, efficient TSP solving with strong generalization.

Abstract: Neural traveling salesman problem (TSP) solvers face two critical challenges:
poor generalization for scalable TSPs and high training costs. To address these
challenges, we propose a new Rescaling Graph Convolutional Network (RsGCN).
Focusing on the scale-dependent features (i.e., features varied with problem
scales) related to nodes and edges that influence the sensitivity of GCNs to
the problem scales, a Rescaling Mechanism in RsGCN enhances the generalization
capability by (1) rescaling adjacent nodes to construct a subgraph with a
uniform number of adjacent nodes for each node across various scales of TSPs,
which stabilizes the graph message aggregation; (2) rescaling subgraph edges to
adjust the lengths of subgraph edges to the same magnitude, which maintains
numerical consistency. In addition, an efficient training strategy with a
mixed-scale dataset and bidirectional loss is used in RsGCN. To fully exploit
the heatmaps generated by RsGCN, we design an efficient post-search algorithm
termed Re2Opt, in which a reconstruction process based on adaptive weight is
incorporated to help avoid local optima. Based on a combined architecture of
RsGCN and Re2Opt, our solver achieves remarkable generalization and low
training cost: with only 3 epochs of training on the mixed-scale dataset
containing instances with up to 100 nodes, it can be generalized successfully
to 10K-node instances without any fine-tuning. Extensive experiments
demonstrate our state-of-the-art performance across uniform distribution
instances of 9 different scales from 20 to 10K nodes and 78 real-world
instances from TSPLIB, while requiring the fewest learnable parameters and
training epochs among neural competitors.

</details>


### [470] [Scalable unsupervised feature selection via weight stability](https://arxiv.org/pdf/2506.06114)
*Xudong Zhang, Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: The paper introduces Minkowski weighted $k$-means++ for unsupervised feature selection, proposing two algorithms (FS-MWK++ and SFS-MWK++) that outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Improving clustering performance in high-dimensional data by addressing irrelevant features that obscure meaningful structure.

Method: Introduces Minkowski weighted $k$-means++ initialization and two feature selection algorithms (FS-MWK++, SFS-MWK++) leveraging feature relevance estimates.

Result: Theoretical guarantees and experiments show the methods outperform existing alternatives.

Conclusion: The proposed methods are effective for unsupervised feature selection, with publicly available software.

Abstract: Unsupervised feature selection is critical for improving clustering
performance in high-dimensional data, where irrelevant features can obscure
meaningful structure. In this work, we introduce the Minkowski weighted
$k$-means++, a novel initialisation strategy for the Minkowski Weighted
$k$-means. Our initialisation selects centroids probabilistically using feature
relevance estimates derived from the data itself. Building on this, we propose
two new feature selection algorithms, FS-MWK++, which aggregates feature
weights across a range of Minkowski exponents to identify stable and
informative features, and SFS-MWK++, a scalable variant based on subsampling.
We support our approach with a theoretical guarantee under mild assumptions and
extensive experiments showing that our methods consistently outperform existing
alternatives. Our software can be found at
https://github.com/xzhang4-ops1/FSMWK.

</details>


### [471] [Extending AALpy with Passive Learning: A Generalized State-Merging Approach](https://arxiv.org/pdf/2506.06333)
*Benjamin von Berg, Bernhard K. Aichernig*

Main category: cs.LG

TL;DR: AALpy's recent addition of a generalized state-merging implementation in the red-blue framework simplifies passive automata learning, reducing implementation effort to defining compatibility criteria and scoring.


<details>
  <summary>Details</summary>
Motivation: To enhance AALpy's capabilities by incorporating a generalized state-merging method for passive automata learning, making it easier to implement existing and novel algorithms.

Method: Uses a common internal representation for different automaton types to enable a configurable red-blue framework implementation. Focuses on defining compatibility criteria and scoring for state-merging.

Result: AALpy now supports efficient implementation of state-merging algorithms, requiring minimal code for existing methods.

Conclusion: AALpy's generalized state-merging feature significantly reduces implementation complexity, aiding both research and practical applications in automata learning.

Abstract: AALpy is a well-established open-source automata learning library written in
Python with a focus on active learning of systems with IO behavior. It provides
a wide range of state-of-the-art algorithms for different automaton types
ranging from fully deterministic to probabilistic automata. In this work, we
present the recent addition of a generalized implementation of an important
method from the domain of passive automata learning: state-merging in the
red-blue framework. Using a common internal representation for different
automaton types allows for a general and highly configurable implementation of
the red-blue framework. We describe how to define and execute state-merging
algorithms using AALpy, which reduces the implementation effort for
state-merging algorithms mainly to the definition of compatibility criteria and
scoring. This aids the implementation of both existing and novel algorithms. In
particular, defining some existing state-merging algorithms from the literature
with AALpy only takes a few lines of code.

</details>


### [472] [Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models](https://arxiv.org/pdf/2506.07275)
*Haochen Song, Dominik Hofer, Rania Islambouli, Laura Hawkins, Ananya Bhattacharjee, Meredith Franklin, Joseph Jay Williams*

Main category: cs.LG

TL;DR: A hybrid approach combining cMAB and LLMs personalizes motivational messages to reduce sedentary behavior, evaluated over a seven-day trial with outcomes like step count and message acceptance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of cMAB algorithms (large sample requirements and overlooked psychological factors) by integrating LLMs for personalized message content.

Method: Four intervention types (behavioral self-monitoring, gain-framed, loss-framed, social comparison) delivered via cMAB, LLM, cMABxLLM, or RCT, with dynamic contextual factors.

Result: Evaluated via daily step count and message acceptance, using a causal inference framework to compare models.

Conclusion: The study highlights the complementary roles of LLM personalization and cMAB adaptation in promoting physical activity through tailored messaging.

Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB)
algorithms, offer a promising strategy to reduce sedentary behavior by
delivering personalized interventions to encourage physical activity. However,
cMAB algorithms typically require large participant samples to learn
effectively and may overlook key psychological factors that are not explicitly
encoded in the model. In this study, we propose a hybrid approach that combines
cMAB for selecting intervention types with large language models (LLMs) to
personalize message content. We evaluate four intervention types: behavioral
self-monitoring, gain-framed, loss-framed, and social comparison, each
delivered as a motivational message aimed at increasing motivation for physical
activity and daily step count. Message content is further personalized using
dynamic contextual factors including daily fluctuations in self-efficacy,
social influence, and regulatory focus. Over a seven-day trial, participants
receive daily messages assigned by one of four models: cMAB alone, LLM alone,
combined cMAB with LLM personalization (cMABxLLM), or equal randomization
(RCT). Outcomes include daily step count and message acceptance, assessed via
ecological momentary assessments (EMAs). We apply a causal inference framework
to evaluate the effects of each model. Our findings offer new insights into the
complementary roles of LLM-based personalization and cMAB adaptation in
promoting physical activity through personalized behavioral messaging.

</details>


### [473] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/pdf/2506.08274)
*João Manoel Herrera Pinheiro, Suzana Vilas Boas de Oliveira, Thiago Henrique Segreto Silva, Pedro Antonio Rabelo Saraiva, Enzo Ferreira de Souza, Leonardo André Ambrosio, Marcelo Becker*

Main category: cs.LG

TL;DR: The paper evaluates 12 feature scaling techniques across 14 ML algorithms and 16 datasets, finding that ensemble methods are robust to scaling, while others like Logistic Regression and SVMs are highly sensitive.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive studies on feature scaling's impact on ML performance and computational costs.

Method: Systematic evaluation of 12 scaling techniques on 14 ML algorithms and 16 datasets, analyzing predictive performance and computational costs.

Result: Ensemble methods (e.g., Random Forest, XGBoost) are robust to scaling, while models like Logistic Regression and SVMs show significant performance variations.

Conclusion: The study provides model-specific guidance for selecting optimal feature scaling techniques, with all data and code made publicly available for reproducibility.

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [474] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/pdf/2506.08673)
*Diptarka Chakraborty, Kushagra Chatterjee, Debarati Das, Tien Long Nguyen, Romina Nobahari*

Main category: cs.LG

TL;DR: The paper introduces fair consensus clustering, ensuring proportional representation of protected groups, and provides approximation algorithms for fairness enforcement.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fairness in consensus clustering by incorporating proportional representation of protected groups, inspired by fair clustering principles.

Method: Develops optimal and near-linear time approximation algorithms for enforcing fairness in clustering, including handling unequal group sizes.

Result: Provides constant-factor approximation for fair consensus clustering and proves NP-hardness for unequal-sized groups.

Conclusion: The work lays groundwork for fair clustering variants, with potential broader implications for clustering problems lacking fairness guarantees.

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [475] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/pdf/2506.09207)
*William Anderson, Seung Whan Chung, Youngsoo Choi*

Main category: cs.LG

TL;DR: mLaSDI improves LaSDI by using multiple autoencoders to correct errors sequentially, reducing prediction and reconstruction errors while speeding up training.


<details>
  <summary>Details</summary>
Motivation: LaSDI struggles with accurate reconstruction and dynamics satisfaction in complex regimes, prompting the need for an improved method.

Method: mLaSDI trains multiple autoencoders in stages, each correcting the errors of the previous ones, using small autoencoders for efficiency.

Result: mLaSDI achieves lower prediction and reconstruction errors and faster training compared to LaSDI.

Conclusion: mLaSDI is a more effective and efficient ROM framework for PDEs, especially in complex scenarios.

Abstract: Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [476] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/pdf/2506.09316)
*Yeonju Ro, Zhenyu Zhang, Souvik Kundu, Zhangyang Wang, Aditya Akella*

Main category: cs.LG

TL;DR: DSLA-Serve introduces dual-state linear attention (DSLA) and an adaptive distillation framework to balance efficiency and accuracy in LLMs, achieving faster inference while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the prohibitive compute and memory costs of LLMs on lengthy inputs and the accuracy degradation in sub-quadratic methods like linear attention.

Method: Propose DSLA with dual hidden states for historical context and recency, and DSLA-Serve for adaptive layer replacement at inference.

Result: DSLA-Serve achieves 2.3x faster inference than Llama2-7B and 3.0x faster than Zamba-7B with comparable task performance.

Conclusion: DSLA-Serve effectively balances efficiency and accuracy, addressing limitations of prior linear attention methods.

Abstract: Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose dual-state linear attention (DSLA), a novel design that
maintains two specialized hidden states-one for preserving historical context
and one for tracking recency-thereby mitigating the short-range bias typical of
linear-attention architectures. To further balance efficiency and accuracy
under dynamic workload conditions, we introduce DSLA-Serve, an online adaptive
distillation framework that progressively replaces Transformer layers with DSLA
layers at inference time, guided by a sensitivity-based layer ordering.
DSLA-Serve uses a chained fine-tuning strategy to ensure that each newly
converted DSLA layer remains consistent with previously replaced layers,
preserving the overall quality. Extensive evaluations on commonsense reasoning,
long-context QA, and text summarization demonstrate that DSLA-Serve yields 2.3x
faster inference than Llama2-7B and 3.0x faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [477] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/pdf/2506.09544)
*Yang Yang, Du Yin, Hao Xue, Flora Salim*

Main category: cs.LG

TL;DR: STOAT is a novel framework for probabilistic forecasting in spatial-temporal causal time series, integrating spatial dependencies and causality for improved predictions.


<details>
  <summary>Details</summary>
Motivation: Existing methods model spatial and temporal dynamics independently and overlook causality-driven probabilistic forecasting, limiting predictive power.

Method: STOAT extends causal inference with a spatial relation matrix for interregional dependencies and uses deep probabilistic models for uncertainty modeling.

Result: STOAT outperforms state-of-the-art models like DeepAR and DeepVAR, especially in regions with strong spatial dependencies.

Conclusion: STOAT bridges causal inference and geospatial probabilistic forecasting, offering a generalizable framework for complex tasks like epidemic management.

Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [478] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/pdf/2506.09816)
*Cecilia Casolo, Sören Becker, Niki Kilbertus*

Main category: cs.LG

TL;DR: Sparse linear ODEs are often unidentifiable from data, unlike dense ones, with practical implications for trust in learned models.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding identifiability of sparse linear ODEs, which are common in real-world systems but underexplored.

Method: Characterize identifiability of sparse linear ODEs, provide lower bounds for unidentifiability probability, and test state-of-the-art estimation methods.

Result: Sparse systems are unidentifiable with positive probability, and empirical tests confirm practical unidentifiability.

Conclusion: Theoretical and practical unidentifiability of sparse linear ODEs necessitates reevaluating expectations for data-driven modeling and trust in learned models.

Abstract: Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [479] [A Weighted Loss Approach to Robust Federated Learning under Data Heterogeneity](https://arxiv.org/pdf/2506.09824)
*Johan Erbani, Sonia Ben Mokhtar, Pierre-Edouard Portier, Elod Egyed-Zsigmond, Diana Nurbakova*

Main category: cs.LG

TL;DR: The paper introduces WoLA, a weighted loss method to align honest worker gradients in federated learning, improving Byzantine resilience in heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) faces security threats from Byzantine participants who submit harmful gradients. Existing methods struggle in heterogeneous settings where honest gradients vary widely.

Method: The authors propose the Worker Label Alignment Loss (WoLA), which aligns honest worker gradients despite data heterogeneity, aiding in identifying Byzantine gradients.

Result: WoLA outperforms state-of-the-art methods in heterogeneous settings, supported by theoretical and empirical evidence.

Conclusion: WoLA effectively enhances Byzantine resilience in FL by addressing gradient heterogeneity, offering a robust solution for secure collaborative learning.

Abstract: Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [480] [AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation](https://arxiv.org/pdf/2506.10540)
*Haoyuan Shi, Yunxin Li, Xinyu Chen, Longyue Wang, Baotian Hu, Min Zhang*

Main category: cs.MA

TL;DR: AniMaker is a multi-agent framework for generating coherent storytelling videos from text, using specialized agents and novel techniques like MCTS-Gen and AniEval to ensure quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods produce disjointed narratives due to rigid keyframe conversion and instability in models, degrading logical coherence and visual continuity.

Method: AniMaker employs specialized agents (Director, Photography, Reviewer, Post-Production) with MCTS-Gen for clip generation and AniEval for multi-shot evaluation.

Result: AniMaker outperforms in quality (VBench, AniEval) and efficiency, advancing AI-generated storytelling animation toward production standards.

Conclusion: AniMaker addresses key challenges in video generation, offering a robust solution for coherent and efficient storytelling animation.

Abstract: Despite rapid advancements in video generation models, generating coherent
storytelling videos that span multiple scenes and characters remains
challenging. Current methods often rigidly convert pre-generated keyframes into
fixed-length clips, resulting in disjointed narratives and pacing issues.
Furthermore, the inherent instability of video generation models means that
even a single low-quality clip can significantly degrade the entire output
animation's logical coherence and visual continuity. To overcome these
obstacles, we introduce AniMaker, a multi-agent framework enabling efficient
multi-candidate clip generation and storytelling-aware clip selection, thus
creating globally consistent and story-coherent animation solely from text
input. The framework is structured around specialized agents, including the
Director Agent for storyboard generation, the Photography Agent for video clip
generation, the Reviewer Agent for evaluation, and the Post-Production Agent
for editing and voiceover. Central to AniMaker's approach are two key technical
components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search
(MCTS)-inspired strategy that intelligently navigates the candidate space to
generate high-potential clips while optimizing resource usage; and AniEval in
Reviewer Agent, the first framework specifically designed for multi-shot
animation evaluation, which assesses critical aspects such as story-level
consistency, action completion, and animation-specific features by considering
each clip in the context of its preceding and succeeding clips. Experiments
demonstrate that AniMaker achieves superior quality as measured by popular
metrics including VBench and our proposed AniEval framework, while
significantly improving the efficiency of multi-candidate generation, pushing
AI-generated storytelling animation closer to production standards.

</details>


### [481] [Higher-Order Uncoupled Learning Dynamics and Nash Equilibrium](https://arxiv.org/pdf/2506.10874)
*Sarah A. Toonsi, Jeff S. Shamma*

Main category: cs.MA

TL;DR: The paper explores learnability of mixed-strategy Nash Equilibrium (NE) in finite games using higher-order uncoupled dynamics, linking it to feedback stabilization and control theory. It shows existence of dynamics leading to isolated NE but also demonstrates limitations via simultaneous stabilization. The Asymptotic Best Response (ABR) property is introduced, and learnability in bandit settings is addressed.


<details>
  <summary>Details</summary>
Motivation: To understand how higher-order uncoupled learning dynamics can achieve mixed-strategy NE in finite games, and to explore limitations and natural restrictions like the ABR property.

Method: Uses higher-order replicator dynamics and uncoupled heterogeneous dynamics, linking them to feedback stabilization and control theory. Introduces the ABR property and analyzes its implications.

Result: Shows existence of dynamics leading to isolated NE, but also proves limitations via simultaneous stabilization. Establishes conditions for NE compatibility with ABR and addresses learnability in bandit settings.

Conclusion: Higher-order uncoupled dynamics can achieve NE in certain cases, but universality is limited. The ABR property provides a natural restriction, and bandit settings extend applicability.

Abstract: We study learnability of mixed-strategy Nash Equilibrium (NE) in general
finite games using higher-order replicator dynamics as well as classes of
higher-order uncoupled heterogeneous dynamics. In higher-order uncoupled
learning dynamics, players have no access to utilities of opponents (uncoupled)
but are allowed to use auxiliary states to further process information
(higher-order). We establish a link between uncoupled learning and feedback
stabilization with decentralized control. Using this association, we show that
for any finite game with an isolated completely mixed-strategy NE, there exist
higher-order uncoupled learning dynamics that lead (locally) to that NE. We
further establish the lack of universality of learning dynamics by linking
learning to the control theoretic concept of simultaneous stabilization. We
construct two games such that any higher-order dynamics that learn the
completely mixed-strategy NE of one of these games can never learn the
completely mixed-strategy NE of the other. Next, motivated by imposing natural
restrictions on allowable learning dynamics, we introduce the Asymptotic Best
Response (ABR) property. Dynamics with the ABR property asymptotically learn a
best response in environments that are asymptotically stationary. We show that
the ABR property relates to an internal stability condition on higher-order
learning dynamics. We provide conditions under which NE are compatible with the
ABR property. Finally, we address learnability of mixed-strategy NE in the
bandit setting using a bandit version of higher-order replicator dynamics.

</details>


### [482] [Nonconvex Game and Multi Agent Reinforcement Learning for Zonal Ancillary Markets](https://arxiv.org/pdf/2505.03288)
*Francesco Morri, Hélène Le Cadre, Pierre Gruet, Luce Brotcorne*

Main category: cs.MA

TL;DR: The paper analyzes zonal ancillary market coupling using noncooperative game theory, comparing exact methods (optimization and best-response) with multi-agent deep reinforcement learning (MADRL). MADRL shows lower market costs but higher profit variability, while stronger zone coupling reduces costs for larger zones.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize zonal ancillary market coupling using game theory and computational methods, addressing equilibrium conditions and performance trade-offs.

Method: Formulates the market as a multi-leader single-follower bilevel problem, recast as a generalized Nash game. Compares exact methods (integrated optimization, Gauss-Seidel best-response) with MADRL.

Result: MADRL achieves lower market costs but higher profit variability. Exact methods are slower, with best-response being the slowest. Stronger zone coupling reduces costs for larger zones.

Conclusion: MADRL is effective for cost reduction but introduces profit variability. Exact methods are reliable but slower. Zone coupling benefits larger zones economically.

Abstract: We characterize zonal ancillary market coupling relying on noncooperative
game theory. To that purpose, we formulate the ancillary market as a
multi-leader single follower bilevel problem, that we subsequently cast as a
generalized Nash game with side constraints and nonconvex feasibility sets. We
determine conditions for equilibrium existence and show that the game has a
generalized potential game structure. To compute market equilibrium, we rely on
two exact approaches: an integrated optimization approach and Gauss-Seidel
best-response, that we compare against multi-agent deep reinforcement learning.
On real data from Germany and Austria, simulations indicate that multi-agent
deep reinforcement learning achieves the smallest convergence rate but requires
pretraining, while best-response is the slowest. On the economics side,
multi-agent deep reinforcement learning results in smaller market costs
compared to the exact methods, but at the cost of higher variability in the
profit allocation among stakeholders. Further, stronger coupling between zones
tends to reduce costs for larger zones.

</details>


### [483] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/pdf/2506.08507)
*Kuo Yang, Xingjie Yang, Linhui Yu, Qing Xu, Yan Fang, Xu Wang, Zhengyang Zhou, Yang Wang*

Main category: cs.MA

TL;DR: MasHost is an RL-based framework for autonomous multi-agent system (MAS) design, introducing component rationality and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing MAS methods rely on manual or heuristic rules, limiting autonomy and introducing biases.

Method: Formulates MAS construction as a graph search problem, using probabilistic sampling and Hierarchical Relative Policy Optimization (HRPO) for multi-objective optimization.

Result: Outperforms baselines on six benchmarks, demonstrating effectiveness, efficiency, and rationality.

Conclusion: MasHost is the first RL-driven framework for autonomous MAS graph construction, validated by superior performance.

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [484] [Semantic Communication-Enabled Cloud-Edge-End-collaborative Metaverse Services Architecure](https://arxiv.org/pdf/2506.10001)
*Yuxuan Li, Sheng Jinag, Bizhu Wang*

Main category: cs.MM

TL;DR: The paper proposes SC-CEE-Meta, a semantic communication-based architecture to improve VR data transmission in the metaverse by reducing latency and enhancing image quality.


<details>
  <summary>Details</summary>
Motivation: The metaverse faces challenges like high data transmission demands and poor channel quality, degrading VR user experiences.

Method: SC-CEE-Meta uses semantic transmission, video synthesis, and 3D scene reconstruction modules deployed across cloud, edge, and end devices.

Result: Testing on Meta Quest Pro showed a 96.05% reduction in wireless delay and 43.99% better image quality under poor conditions.

Conclusion: SC-CEE-Meta effectively addresses bandwidth and latency issues in VR data transmission, enhancing metaverse services.

Abstract: With technology advancing and the pursuit of new audiovisual experiences
strengthening, the metaverse has gained surging enthusiasm. However, it faces
practical hurdles as substantial data like high-resolution virtual scenes must
be transmitted between cloud platforms and VR devices. Specifically, the VR
device's wireless transmission hampered by insufficient bandwidth, causes speed
and delay problems. Meanwhile, poor channel quality leads to data errors and
worsens user experience. To solve this, we've proposed the Semantic
Communication-Enabled Cloud-Edge-End Collaborative Immersive Metaverse Service
(SC-CEE-Meta) Architecture, which includes three modules: VR video semantic
transmission, video synthesis, and 3D virtual scene reconstruction. By
deploying semantic modules on VR devices and edge servers and sending key
semantic info instead of focusing on bit-level reconstruction, it can cut
latency, resolve the resource-bandwidth conflict, and better withstand channel
interference. Also, the cloud deploys video synthesis and 3D scene
reconstruction preprocessing, while edge devices host 3D reconstruction
rendering modules, all for immersive services. Verified on Meta Quest Pro, the
SC-CEE-Meta can reduce wireless transmission delay by 96.05\% and boost image
quality by 43.99\% under poor channel condition.

</details>


### [485] [EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis](https://arxiv.org/pdf/2506.10002)
*Jianwu Fang, Lei-Lei Li, Zhedong Zheng, Hongkai Yu, Jianru Xue, Zhengguo Li, Tat-Seng Chua*

Main category: cs.MM

TL;DR: The paper proposes an Attentive Video Diffusion (AVD) model to synthesize accident video clips for Traffic Accident Anticipation (TAA), addressing data bias and background confounding issues. It introduces EQ-TAA with equivariant triple loss for improved performance.


<details>
  <summary>Details</summary>
Motivation: Current TAA methods rely on supervised learning with laborious annotations, but struggle with data bias and identifying causal parts of accidents.

Method: AVD generates causal video frames from text prompts, preserving style and content. EQ-TAA uses equivariant triple loss with generated pseudo-normal and pseudo-accident clips.

Result: AVD and EQ-TAA achieve competitive performance compared to state-of-the-art methods.

Conclusion: The proposed AVD and EQ-TAA effectively address TAA challenges, offering a solution without extra annotations and improving performance.

Abstract: Traffic Accident Anticipation (TAA) in traffic scenes is a challenging
problem for achieving zero fatalities in the future. Current approaches
typically treat TAA as a supervised learning task needing the laborious
annotation of accident occurrence duration. However, the inherent long-tailed,
uncertain, and fast-evolving nature of traffic scenes has the problem that real
causal parts of accidents are difficult to identify and are easily dominated by
data bias, resulting in a background confounding issue. Thus, we propose an
Attentive Video Diffusion (AVD) model that synthesizes additional accident
video clips by generating the causal part in dashcam videos, i.e., from normal
clips to accident clips. AVD aims to generate causal video frames based on
accident or accident-free text prompts while preserving the style and content
of frames for TAA after video generation. This approach can be trained using
datasets collected from various driving scenes without any extra annotations.
Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant
triple loss for an anchor accident-free video clip, along with the generated
pair of contrastive pseudo-normal and pseudo-accident clips. Extensive
experiments have been conducted to evaluate the performance of AVD and EQ-TAA,
and competitive performance compared to state-of-the-art methods has been
obtained.

</details>


### [486] [Integrating multimedia documents in 3D city models for a better understanding of territories](https://arxiv.org/pdf/2506.10003)
*C. Gautier, J. Delanoy, G. Gesquière*

Main category: cs.MM

TL;DR: The paper proposes integrating multimedia documents into 3D urban scenes to enhance contextual understanding and enable spatial navigation-based document search.


<details>
  <summary>Details</summary>
Motivation: 3D urban models lack contextual information, while multimedia documents (images, videos, texts) contain such details. Combining these can improve urban analysis and understanding.

Method: Four approaches for integrating multimedia documents into 3D urban scenes, combined with user guidance modes for better media consumption and understanding.

Result: Demonstrated in Lyon, France, showing how multimedia integration contextualizes buildings and tracks territorial evolution.

Conclusion: Multimedia integration in 3D urban scenes enhances contextualization and supports spatial navigation-based document search.

Abstract: Digital 3D representations of urban areas, through their growing
availability, are a helpful tool to better understand a territory. However,
they lack contextual information about, for example, the history or
functionality of buildings. On another side, multimedia documents like images,
videos or texts usually contain such information. Crossing these two types of
data can therefore help in the analysis and understanding of the organization
of our cities. This could also be used to develop document search based on
spatial navigation, instead of the classical textual query. In this paper, we
propose four approaches to integrate multimedia documents in a 3D urban scene,
allowing to contextualize the scene with any type of media. We combine these
integration approaches with user guidance modes that allows to guide the user
through the consumption of these media and support its understanding of the
territory. We demonstrate the usefulness of these techniques in the context of
different projects within the Lyon area (France). The use of multimedia
documents integrated into a digital tour allows, for example, the iconic
buildings to be contextualised or to understand the evolution of a territory
through time.

</details>


### [487] [Multimodal Emotion Coupling via Speech-to-Facial and Bodily Gestures in Dyadic Interaction](https://arxiv.org/pdf/2506.10010)
*Von Ralph Dane Marquez Herbuela, Yukie Nagai*

Main category: cs.MM

TL;DR: The study explores how emotional speech aligns with facial and hand movements, revealing that nonoverlapping speech increases expressivity, while anger suppresses gestures during overlaps. Predictive mapping shows prosody and MFCCs are most accurate for articulatory regions.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of emotional expression through coordinated vocal, facial, and gestural signals, enhancing real-time emotion detection in human interactions and AI systems.

Method: Analyzed multimodal emotion coupling using motion capture from dyadic interactions in the IEMOCAP corpus, aligning speech features (prosody, MFCCs, arousal, valence, categorical emotions) with 3D facial and hand marker displacements.

Result: Nonoverlapping speech elicited greater activeness, especially in the lower face and mouth. Sadness increased expressivity during nonoverlap, while anger suppressed gestures during overlaps. Prosody and MFCCs showed highest accuracy in articulatory regions.

Conclusion: The findings highlight the importance of timing and synchrony in emotional expression, with implications for improving emotion detection in both human and AI interactions.

Abstract: Human emotional expression emerges through coordinated vocal, facial, and
gestural signals. While speech face alignment is well established, the broader
dynamics linking emotionally expressive speech to regional facial and hand
motion remains critical for gaining a deeper insight into how emotional and
behavior cues are communicated in real interactions. Further modulating the
coordination is the structure of conversational exchange like sequential turn
taking, which creates stable temporal windows for multimodal synchrony, and
simultaneous speech, often indicative of high arousal moments, disrupts this
alignment and impacts emotional clarity. Understanding these dynamics enhances
realtime emotion detection by improving the accuracy of timing and synchrony
across modalities in both human interactions and AI systems. This study
examines multimodal emotion coupling using region specific motion capture from
dyadic interactions in the IEMOCAP corpus. Speech features included low level
prosody, MFCCs, and model derived arousal, valence, and categorical emotions
(Happy, Sad, Angry, Neutral), aligned with 3D facial and hand marker
displacements. Expressive activeness was quantified through framewise
displacement magnitudes, and speech to gesture prediction mapped speech
features to facial and hand movements. Nonoverlapping speech consistently
elicited greater activeness particularly in the lower face and mouth. Sadness
showed increased expressivity during nonoverlap, while anger suppressed
gestures during overlaps. Predictive mapping revealed highest accuracy for
prosody and MFCCs in articulatory regions while arousal and valence had lower
and more context sensitive correlations. Notably, hand speech synchrony was
enhanced under low arousal and overlapping speech, but not for valence.

</details>


### [488] [Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming](https://arxiv.org/pdf/2506.10004)
*Haopeng Wang, Haiwei Dong, Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: This survey reviews XR streaming, covering definitions, traffic characteristics, quality of experience factors, optimization methods, and current challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of XR streaming and its unique requirements, aiming to enhance user satisfaction and system performance.

Method: Analyzes XR traffic characteristics, explores quality of experience factors, and presents visual attention-based optimization methods.

Result: Identifies key elements for improving XR streaming efficiency and user satisfaction, along with current applications and challenges.

Conclusion: Highlights ongoing and future developments in XR, emphasizing the need for further advancements to address current challenges.

Abstract: Extended reality (XR) is rapidly advancing, and poised to revolutionize
content creation and consumption. In XR, users integrate various sensory inputs
to form a cohesive perception of the virtual environment. This survey reviews
the state-of-the-art in XR streaming, focusing on multiple paradigms. To begin,
we define XR and introduce various XR headsets along with their multimodal
interaction methods to provide a foundational understanding. We then analyze XR
traffic characteristics to highlight the unique data transmission requirements.
We also explore factors that influence the quality of experience in XR systems,
aiming to identify key elements for enhancing user satisfaction. Following
this, we present visual attention-based optimization methods for XR streaming
to improve efficiency and performance. Finally, we examine current applications
and highlight challenges to provide insights into ongoing and future
developments of XR.

</details>


### [489] [HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction](https://arxiv.org/pdf/2506.10006)
*Jie Qin, Wei Yang, Yan Su, Yiran Zhu, Weizhen Li, Yunyue Pan, Chengchang Pan, Honggang Qi*

Main category: cs.MM

TL;DR: The paper introduces a flexible bimodal framework for HER2 prediction in breast cancer, improving accuracy for both single- and dual-modality inputs without requiring synchronized acquisition.


<details>
  <summary>Details</summary>
Motivation: Current HER2 assessment models analyze H&E or IHC images separately, missing the clinical synergy of combined interpretation. Workflow and cost constraints hinder concurrent acquisition of both modalities.

Method: The framework includes: 1) a dynamic branch selector for single-/dual-modality prediction, 2) a bidirectional cross-modal GAN for feature-space reconstruction, and 3) a hybrid training protocol.

Result: Single-modality H&E accuracy rose from 71.44% to 94.25%, dual-modality accuracy reached 95.09%, and IHC-only reliability was 90.28%. Cross-modal reconstruction improved F1-scores (0.9609 HE to IHC, 0.9251 IHC to HE).

Conclusion: The framework democratizes precise HER2 assessment, reducing IHC costs and maintaining performance without synchronized acquisition, benefiting resource-limited settings.

Abstract: Current HER2 assessment models for breast cancer predominantly analyze H&E or
IHC images in isolation,despite clinical reliance on their synergistic
interpretation. However, concurrent acquisition of both modalities is often
hindered by workflow complexity and cost constraints. We propose an adaptive
bimodal framework enabling flexible single-/dual-modality HER2 prediction
through three innovations: 1) A dynamic branch selector that activates either
single-modality reconstruction or dual-modality joint inference based on input
completeness; 2) A bidirectional cross-modal GAN performing context-aware
feature-space reconstruction of missing modalities; 3) A hybrid training
protocol integrating adversarial learning and multi-task optimization. This
architecture elevates single-modality H&E prediction accuracy from 71.44% to
94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28%
reliability with sole IHC inputs. The framework's "dual-preferred,
single-compatible" design delivers near-bimodal performance without requiring
synchronized acquisition, particularly benefiting resource-limited settings
through IHC infrastructure cost reduction. Experimental validation confirms
22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with
cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251
(IHC to HE). By dynamically routing inputs through reconstruction-enhanced or
native fusion pathways, the system mitigates performance degradation from
missing data while preserving computational efficiency (78.55% parameter
reduction in lightweight variant). This elastic architecture demonstrates
significant potential for democratizing precise HER2 assessment across diverse
healthcare settings.

</details>


### [490] [Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space](https://arxiv.org/pdf/2506.10007)
*Kangwei Liu, Junwu Liu, Xiaowei Yi, Jinlin Guo, Yun Cao*

Main category: cs.MM

TL;DR: A diffusion-based framework for expressive 3D facial animation addresses challenges of single-modal control and deterministic mapping by leveraging multimodal emotion binding and an attention-based latent diffusion model.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of single-modal control signals and deterministic regression in emotional 3D facial animation to enhance expressiveness and flexibility.

Method: Uses a FLAME-centered multimodal emotion binding strategy and an attention-based latent diffusion model with content-aware attention and emotion-guided layers.

Result: Outperforms existing methods with a 21.6% improvement in emotion similarity while maintaining natural facial dynamics.

Conclusion: The proposed framework effectively enhances the expressiveness and controllability of 3D facial animations by integrating multimodal signals and stochastic modeling.

Abstract: Audio-driven emotional 3D facial animation encounters two significant
challenges: (1) reliance on single-modal control signals (videos, text, or
emotion labels) without leveraging their complementary strengths for
comprehensive emotion manipulation, and (2) deterministic regression-based
mapping that constrains the stochastic nature of emotional expressions and
non-verbal behaviors, limiting the expressiveness of synthesized animations. To
address these challenges, we present a diffusion-based framework for
controllable expressive 3D facial animation. Our approach introduces two key
innovations: (1) a FLAME-centered multimodal emotion binding strategy that
aligns diverse modalities (text, audio, and emotion labels) through contrastive
learning, enabling flexible emotion control from multiple signal sources, and
(2) an attention-based latent diffusion model with content-aware attention and
emotion-guided layers, which enriches motion diversity while maintaining
temporal coherence and natural facial dynamics. Extensive experiments
demonstrate that our method outperforms existing approaches across most
metrics, achieving a 21.6\% improvement in emotion similarity while preserving
physiologically plausible facial dynamics. Project Page:
https://kangweiiliu.github.io/Control_3D_Animation.

</details>


### [491] [Can Sound Replace Vision in LLaVA With Token Substitution?](https://arxiv.org/pdf/2506.10416)
*Ali Vosoughi, Jing Bi, Pinxin Liu, Yunlong Tang, Chenliang Xu*

Main category: cs.MM

TL;DR: SoundCLIP explores direct audio-visual integration in multimodal models, finding a trade-off between audio-to-video retrieval performance and text generation quality.


<details>
  <summary>Details</summary>
Motivation: Current multimodal systems rely on text-aligned representations, limiting nuanced audio understanding. SoundCLIP aims to integrate audio directly.

Method: Substitutes CLIP's visual tokens with audio representations, testing two configurations: projecting audio into CLIP's space and preserving raw audio embeddings.

Result: Audio-to-video retrieval improves significantly, but text generation quality declines. Encoders with text supervision perform better for generation.

Conclusion: Optimal performance requires balancing retrieval accuracy and text generation, challenging the assumption that stronger cross-modal alignment benefits all tasks.

Abstract: While multimodal systems have achieved impressive advances, they typically
rely on text-aligned representations rather than directly integrating audio and
visual inputs. This reliance can limit the use of acoustic information in tasks
requiring nuanced audio understanding. In response, SoundCLIP explores direct
audio-visual integration within multimodal large language models (MLLMs) by
substituting CLIP's visual tokens with audio representations and selecting
sound-relevant patch tokens in models such as LLaVA. We investigate two
configurations: (1) projecting audio features into CLIP's visual manifold via a
multilayer perceptron trained with InfoNCE on paired audio-video segments, and
(2) preserving raw audio embeddings with minimal dimensional adjustments.
Experiments with five state-of-the-art audio encoders reveal a fundamental
trade-off. While audio-to-video retrieval performance increases dramatically
(up to 44 percentage points in Top-1 accuracy) when audio is projected into
CLIP's space, text generation quality declines. Encoders pre-trained with text
supervision (CLAP, Whisper, ImageBind) maintain stronger generative
capabilities than those focused primarily on audiovisual alignment (Wav2CLIP,
AudioCLIP), highlighting the value of language exposure for generation tasks.
We introduce WhisperCLIP, an architecture that fuses intermediate
representations from Whisper, as well as AudioVisual Event Evaluation (AVE-2),
a dataset of 580,147 three-second audiovisual clips with fine-grained alignment
annotations. Our findings challenge the assumption that stronger cross-modal
alignment necessarily benefits all multimodal tasks; instead, a Pareto frontier
emerges wherein optimal performance depends on balancing retrieval accuracy
with text generation quality. Codes and datasets:
https://github.com/ali-vosoughi/SoundCLIP.

</details>


### [492] [Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics](https://arxiv.org/pdf/2506.10008)
*Yi-Chun Chen*

Main category: cs.MM

TL;DR: A hierarchical knowledge graph framework for analyzing visual narratives like comics, decomposing content into levels and integrating semantic, spatial, and temporal relationships for reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To enable structured understanding and reasoning of multimodal visual narratives, addressing challenges in story structure, character continuity, and event progression.

Method: Decomposes narratives into macro and fine-grained levels, representing them via integrated knowledge graphs linking visual and textual elements. Applied to the Manga109 dataset.

Result: High precision and recall in tasks like action retrieval, dialogue tracing, and panel timeline reconstruction, validating framework coherence.

Conclusion: Provides a scalable foundation for narrative analysis, interactive storytelling, and multimodal reasoning in visual media.

Abstract: This paper presents a hierarchical knowledge graph framework for the
structured understanding of visual narratives, focusing on multimodal media
such as comics. The proposed method decomposes narrative content into multiple
levels, from macro-level story arcs to fine-grained event segments. It
represents them through integrated knowledge graphs that capture semantic,
spatial, and temporal relationships. At the panel level, we construct
multimodal graphs that link visual elements such as characters, objects, and
actions with corresponding textual components, including dialogue and captions.
These graphs are integrated across narrative levels to support reasoning over
story structure, character continuity, and event progression.
  We apply our approach to a manually annotated subset of the Manga109 dataset
and demonstrate its ability to support symbolic reasoning across diverse
narrative tasks, including action retrieval, dialogue tracing, character
appearance mapping, and panel timeline reconstruction. Evaluation results show
high precision and recall across tasks, validating the coherence and
interpretability of the framework. This work contributes a scalable foundation
for narrative-based content analysis, interactive storytelling, and multimodal
reasoning in visual media.

</details>


### [493] [WDMIR: Wavelet-Driven Multimodal Intent Recognition](https://arxiv.org/pdf/2506.10011)
*Weiyin Gong, Kai Zhang, Yanghai Zhang, Qi Liu, Xinjie Sun, Junyu Lu, Linbo Zhu*

Main category: cs.MM

TL;DR: The paper introduces WDMIR, a wavelet-driven framework for multimodal intent recognition, improving accuracy by 1.13% through frequency-domain analysis of non-verbal cues.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on text, neglecting non-verbal semantics. This work addresses the gap by leveraging frequency-domain analysis.

Method: Proposes wavelet-driven fusion for video-audio features and cross-modal interaction for bimodal to trimodal integration.

Result: Achieves state-of-the-art performance (1.13% accuracy boost) on MIntRec, with wavelet fusion improving non-verbal semantic extraction by 0.41%.

Conclusion: WDMIR effectively bridges verbal and non-verbal semantic gaps, enhancing intent recognition through frequency-domain techniques.

Abstract: Multimodal intent recognition (MIR) seeks to accurately interpret user
intentions by integrating verbal and non-verbal information across video, audio
and text modalities. While existing approaches prioritize text analysis, they
often overlook the rich semantic content embedded in non-verbal cues. This
paper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR)
framework that enhances intent understanding through frequency-domain analysis
of non-verbal information. To be more specific, we propose: (1) a
wavelet-driven fusion module that performs synchronized decomposition and
integration of video-audio features in the frequency domain, enabling
fine-grained analysis of temporal dynamics; (2) a cross-modal interaction
mechanism that facilitates progressive feature enhancement from bimodal to
trimodal integration, effectively bridging the semantic gap between verbal and
non-verbal information. Extensive experiments on MIntRec demonstrate that our
approach achieves state-of-the-art performance, surpassing previous methods by
1.13% on accuracy. Ablation studies further verify that the wavelet-driven
fusion module significantly improves the extraction of semantic information
from non-verbal sources, with a 0.41% increase in recognition accuracy when
analyzing subtle emotional cues.

</details>


### [494] [Thief of Truth: VR comics about the relationship between AI and humans](https://arxiv.org/pdf/2506.10012)
*Joonhyung Bae*

Main category: cs.MM

TL;DR: A VR comic, 'Thief of Truth,' explores human-AI relationships through a mind-uploaded human's story, focusing on VR's viewing control, player immersion, and accessibility.


<details>
  <summary>Details</summary>
Motivation: To experiment with VR comics' expandability and present an innovative example in the medium.

Method: Designed with VR viewing control, controller-based interaction for immersion, and methods to enhance accessibility.

Result: A VR comic that demonstrates experimental techniques in storytelling and interaction.

Conclusion: The work serves as a pioneering example for future VR comic development.

Abstract: Thief of Truth is a first-person perspective Virtual Reality (VR) comic that
explores the relationship between humans and artificial intelligence (AI). The
work tells the story of a mind-uploaded human being reborn as a new subject
while interacting with an AI that is looking for the meaning of life. In order
to experiment with the expandability of VR comics, the work was produced by
focusing on three problems. First, the comic is designed using the viewing
control effect of VR. Second, through VR controller-based interaction, the
player's immersion in the work is increased. Third, a method for increasing
accessibility to VR comics was devised. This work aims to present an example of
an experimental attempt in VR Comics.

</details>


### [495] [Immersive Fantasy Based on Digital Nostalgia: Environmental Narratives for the Korean Millennials and Gen Z](https://arxiv.org/pdf/2506.10013)
*Yerin Doh, Joonhyung Bae*

Main category: cs.MM

TL;DR: The study presents the media artwork 'Dear Passenger, Please Wear a Mask,' addressing mask waste during COVID-19 through a digital game and exhibition, blending nostalgia, fantasy, and environmental ethics.


<details>
  <summary>Details</summary>
Motivation: To explore ecological concerns of mask waste and engage Millennials and Gen Z through digital nostalgia and travel themes.

Method: Uses a point-and-click game and immersive exhibition to merge virtual and real experiences, highlighting ethical dilemmas.

Result: Fosters empathy and potential action but faces challenges in resource use and post-experience engagement.

Conclusion: The artwork effectively raises awareness but needs solutions for sustainability and long-term impact.

Abstract: This study introduces the media artwork Dear Passenger, Please Wear a Mask,
designed to offer a layered exploration of single-use mask waste, which
escalated during the COVID-19 pandemic. The piece reframes underappreciated
ecological concerns by interweaving digital nostalgia and airline travel
recollections of Millennials and Gen Z with a unique fantasy narrative. Via a
point-and-click game and an immersive exhibition, participants traverse both
virtual and real domains, facing ethical and environmental dilemmas. While it
fosters empathy and potential action, resource use and post-experience
engagement challenges persist.

</details>


### [496] [Multimodal Large Language Models: A Survey](https://arxiv.org/pdf/2506.10016)
*Longzhen Han, Awes Mubarak, Almas Baimagambetov, Nikolaos Polatidis, Thar Baker*

Main category: cs.MM

TL;DR: The survey explores Multimodal Large Language Models (MLLMs), categorizing six generative modalities and analyzing foundational techniques like SSL, MoE, RLHF, and CoT prompting. It highlights architectural innovations, cross-modal synergies, and unresolved challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a unified perspective on MLLM development, examining how foundational techniques enable cross-modal capabilities and identifying paths toward more general-purpose systems.

Method: Categorizes six generative modalities and analyzes key models, architectural trends, and emergent synergies using foundational techniques like SSL, MoE, RLHF, and CoT prompting.

Result: Identifies architectural innovations (e.g., transformers, diffusion models) enabling cross-modal transfer and modular specialization, along with emerging synergies and unresolved challenges.

Conclusion: The survey offers insights into MLLM development, highlighting transferable techniques and critical paths for creating adaptive, interpretable multimodal systems.

Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text
generation, now spanning diverse output modalities including images, music,
video, human motion, and 3D objects, by integrating language with other sensory
modalities under unified architectures. This survey categorises six primary
generative modalities and examines how foundational techniques, namely
Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement
Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,
enable cross-modal capabilities. We analyze key models, architectural trends,
and emergent cross-modal synergies, while highlighting transferable techniques
and unresolved challenges. Architectural innovations like transformers and
diffusion models underpin this convergence, enabling cross-modal transfer and
modular specialization. We highlight emerging patterns of synergy, and identify
open challenges in evaluation, modularity, and structured reasoning. This
survey offers a unified perspective on MLLM development and identifies critical
paths toward more general-purpose, adaptive, and interpretable multimodal
systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [497] [RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding](https://arxiv.org/pdf/2506.10289)
*Yisi Liu, Chenyang Wang, Hanjo Kim, Raniya Khan, Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: RT-VC is a zero-shot real-time voice conversion system with ultra-low latency and high quality, using articulatory features and DDSP for efficient vocoding.


<details>
  <summary>Details</summary>
Motivation: Voice conversion is crucial for applications like assistive communication and entertainment, requiring real-time, high-quality performance.

Method: Leverages articulatory features to disentangle content and speaker traits, integrating DDSP for efficient vocoding.

Result: Achieves 61.4 ms CPU latency (13.3% reduction) while matching SOTA synthesis quality.

Conclusion: RT-VC offers robust, interpretable, and low-latency voice conversion, advancing real-time applications.

Abstract: Voice conversion has emerged as a pivotal technology in numerous applications
ranging from assistive communication to entertainment. In this paper, we
present RT-VC, a zero-shot real-time voice conversion system that delivers
ultra-low latency and high-quality performance. Our approach leverages an
articulatory feature space to naturally disentangle content and speaker
characteristics, facilitating more robust and interpretable voice
transformations. Additionally, the integration of differentiable digital signal
processing (DDSP) enables efficient vocoding directly from articulatory
features, significantly reducing conversion latency. Experimental evaluations
demonstrate that, while maintaining synthesis quality comparable to the current
state-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms,
representing a 13.3\% reduction in latency.

</details>


### [498] [AC/DC: LLM-based Audio Comprehension via Dialogue Continuation](https://arxiv.org/pdf/2506.10312)
*Yusuke Fujita, Tomoya Mizumoto, Atsushi Kojima, Lianbo Liu, Yui Sudo*

Main category: eess.AS

TL;DR: A model uses dialogue continuation to improve audio comprehension, enabling zero-shot instruction-following without multitask tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing caption variation by leveraging dialogue continuation to better capture caption meaning.

Method: Trains the model to produce dialogue-like responses instead of direct captions, using audio captioning datasets.

Result: Achieves zero-shot instruction-following on AudioCaps, WavCaps, and Clotho datasets.

Conclusion: Dialogue continuation training effectively captures caption semantics and enables versatile instruction-following.

Abstract: We propose an instruction-following audio comprehension model that leverages
the dialogue continuation ability of large language models (LLMs). Instead of
directly generating target captions in training data, the proposed method
trains a model to produce responses as if the input caption triggered a
dialogue. This dialogue continuation training mitigates the caption variation
problem. Learning to continue a dialogue effectively captures the caption's
meaning beyond its surface-level words. As a result, our model enables
zero-shot instruction-following capability without multitask instruction
tuning, even trained solely on audio captioning datasets. Experiments on
AudioCaps, WavCaps, and Clotho datasets with AudioBench audio-scene
question-answering tests demonstrate our model's ability to follow various
unseen instructions.

</details>


### [499] [Joint ASR and Speaker Role Tagging with Serialized Output Training](https://arxiv.org/pdf/2506.10349)
*Anfeng Xu, Tiantian Feng, Shrikanth Narayanan*

Main category: eess.AS

TL;DR: The paper proposes a joint ASR and speaker role tagging method using serialized output training (SOT) with Whisper, achieving a 10% reduction in multi-talker WER.


<details>
  <summary>Details</summary>
Motivation: Current ASR systems lack speaker role identification, which is crucial for conversational AI.

Method: Augment Whisper with role-specific tokens and fine-tune using SOT for joint ASR and speaker role tagging.

Result: Achieves over 10% reduction in multi-talker WER compared to a baseline.

Conclusion: The SOT approach is feasible as a unified model for speaker-role aware transcription.

Abstract: Automatic Speech Recognition systems have made significant progress with
large-scale pre-trained models. However, most current systems focus solely on
transcribing the speech without identifying speaker roles, a function that is
critical for conversational AI. In this work, we investigate the use of
serialized output training (SOT) for joint ASR and speaker role tagging. By
augmenting Whisper with role-specific tokens and fine-tuning it with SOT, we
enable the model to generate role-aware transcriptions in a single decoding
pass. We compare the SOT approach against a self-supervised previous baseline
method on two real-world conversational datasets. Our findings show that this
approach achieves more than 10% reduction in multi-talker WER, demonstrating
its feasibility as a unified model for speaker-role aware speech transcription.

</details>


### [500] [Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes](https://arxiv.org/pdf/2506.10653)
*Rogier C. van Dalen, Shucong Zhang, Titouan Parcollet, Sourav Bhattacharya*

Main category: eess.AS

TL;DR: The paper proposes a robust adaptation method for speech recognizers using a novel loss function (conditional entropy over hypotheses) and a compact "speaker code," achieving significant WER improvements with minimal data.


<details>
  <summary>Details</summary>
Motivation: Speech recognizers often underperform in new environments or with new speakers due to limited or unlabeled adaptation data.

Method: Combines a novel loss function (conditional entropy over complete hypotheses) and a compact "speaker code" for robust adaptation with minimal data.

Result: 20% relative WER improvement with one minute of data, increasing to 29% with ten minutes.

Conclusion: The proposed method effectively adapts speech recognizers to new speakers with minimal data, significantly reducing WER.

Abstract: Speech recognisers usually perform optimally only in a specific environment
and need to be adapted to work well in another. For adaptation to a new
speaker, there is often too little data for fine-tuning to be robust, and that
data is usually unlabelled. This paper proposes a combination of approaches to
make adaptation to a single minute of data robust. First, instead of estimating
the adaptation parameters with cross-entropy on a single error-prone hypothesis
or "pseudo-label", this paper proposes a novel loss function, the conditional
entropy over complete hypotheses. Using multiple hypotheses makes adaptation
more robust to errors in the initial recognition. Second, a "speaker code"
characterises a speaker in a vector short enough that it requires little data
to estimate. On a far-field noise-augmented version of Common Voice, the
proposed scheme yields a 20% relative improvement in word error rate on one
minute of adaptation data, increasing on 10 minutes to 29%.

</details>


### [501] [Disentangling Dual-Encoder Masked Autoencoder for Respiratory Sound Classification](https://arxiv.org/pdf/2506.10698)
*Peidong Wei Shiyu Miao Lin Li*

Main category: eess.AS

TL;DR: A modified MaskedAutoencoder (DDE-MAE) is proposed for respiratory sound classification, addressing data scarcity and domain mismatch by disentangling disease-related and disease-irrelevant features.


<details>
  <summary>Details</summary>
Motivation: Challenges in respiratory sound classification include data scarcity and domain mismatch due to varied data sources (stethoscopes, demographics, environments).

Method: Proposed DDE-MAE uses two independent encoders to separate disease-related and disease-irrelevant features, reducing domain mismatch.

Result: Achieves competitive performance on the ICBHI dataset.

Conclusion: DDE-MAE effectively addresses domain mismatch and improves respiratory sound classification.

Abstract: Deep neural networks have been applied to audio spectrograms for respiratory
sound classification, but it remains challenging to achieve satisfactory
performance due to the scarcity of available data. Moreover, domain mismatch
may be introduced into the trained models as a result of the respiratory sound
samples being collected from various electronic stethoscopes, patient
demographics, and recording environments. To tackle this issue, we proposed a
modified MaskedAutoencoder(MAE) model, named Disentangling Dual-Encoder MAE
(DDE-MAE) for respiratory sound classification. Two independent encoders were
designed to capture disease-related and disease-irrelevant information
separately, achieving feature disentanglement to reduce the domain mismatch.
Our method achieves a competitive performance on the ICBHI dataset.

</details>


### [502] [FairASR: Fair Audio Contrastive Learning for Automatic Speech Recognition](https://arxiv.org/pdf/2506.10747)
*Jongsuk Kim, Jaemyung Yu, Minchan Kwon, Junmo Kim*

Main category: eess.AS

TL;DR: FairASR mitigates demographic bias in ASR models by learning group-uninformative representations, achieving fair generalization without sacrificing overall performance.


<details>
  <summary>Details</summary>
Motivation: Addressing fairness issues in large-scale ASR models, which remain unaddressed despite their critical importance in real-world applications.

Method: Uses a multi-demographic dataset, gradient reversal layer to suppress demographic-discriminative features, and unsupervised contrastive loss for generalizable speech patterns.

Result: Competitive overall ASR performance with significantly reduced disparities across demographic groups.

Conclusion: FairASR effectively balances fairness and performance in ASR models.

Abstract: Large-scale ASR models have achieved remarkable gains in accuracy and
robustness. However, fairness issues remain largely unaddressed despite their
critical importance in real-world applications. In this work, we introduce
FairASR, a system that mitigates demographic bias by learning representations
that are uninformative about group membership, enabling fair generalization
across demographic groups. Leveraging a multi-demographic dataset, our approach
employs a gradient reversal layer to suppress demographic-discriminative
features while maintaining the ability to capture generalizable speech patterns
through an unsupervised contrastive loss. Experimental results show that
FairASR delivers competitive overall ASR performance while significantly
reducing performance disparities across different demographic groups.

</details>


### [503] [Optimal Scalogram for Computational Complexity Reduction in Acoustic Recognition Using Deep Learning](https://arxiv.org/pdf/2505.13017)
*Dang Thoai Phan, Tuan Anh Huynh, Van Tuan Pham, Cao Minh Tran, Van Thuan Mai, Ngoc Quy Tran*

Main category: eess.AS

TL;DR: The paper proposes a method to reduce the computational cost of CWT for acoustic recognition by optimizing wavelet kernel length and hop size, maintaining performance.


<details>
  <summary>Details</summary>
Motivation: CWT is effective for feature extraction in acoustic recognition but is computationally expensive, leading researchers to prefer alternatives like STFT.

Method: Optimize the wavelet kernel length and hop size of the output scalogram to reduce CWT's computational complexity.

Result: The proposed method significantly reduces computational cost while maintaining robust model performance.

Conclusion: The optimized CWT method offers a practical solution for acoustic recognition tasks by balancing computational efficiency and performance.

Abstract: The Continuous Wavelet Transform (CWT) is an effective tool for feature
extraction in acoustic recognition using Convolutional Neural Networks (CNNs),
particularly when applied to non-stationary audio. However, its high
computational cost poses a significant challenge, often leading researchers to
prefer alternative methods such as the Short-Time Fourier Transform (STFT). To
address this issue, this paper proposes a method to reduce the computational
complexity of CWT by optimizing the length of the wavelet kernel and the hop
size of the output scalogram. Experimental results demonstrate that the
proposed approach significantly reduces computational cost while maintaining
the robust performance of the trained model in acoustic recognition tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [504] [The Iris File Extension](https://arxiv.org/pdf/2506.10009)
*Ryan Erik Landvater, Michael David Olp, Mustafa Yousif, Ulysses Balis*

Main category: eess.IV

TL;DR: The paper introduces the Iris file extension, a vendor-agnostic binary slide format for efficient real-time transfer and display of whole slide images, addressing limitations of DICOM.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of digital pathology highlights the need for a performant intermediary slide format for real-time transfer and display, which current standards like DICOM lack due to limitations in image dimensions and retrieval speed.

Method: The Iris file extension is designed as a binary file container with modern compression, dynamic structure, deep file validation, corruption recovery, and slide annotation support. It includes source code for (de)serialization and validation in multiple languages.

Result: The Iris format offers immediate tile access, optional features, and easy integration with existing whole slide image solutions through provided language bindings and open-source code.

Conclusion: The Iris file extension is presented as an open, efficient solution for digital pathology, with its specification and tools made available under a Creative Commons license.

Abstract: A modern digital pathology vendor-agnostic binary slide format specifically
targeting the unmet need of efficient real-time transfer and display has not
yet been established. Growing adoption of digital pathology only intensifies
the need for an intermediary digital slide format with an emphasis on
performance for use between slide servers and image management software or for
inter-institutional transmission of cases. Although the DICOM standard is a
well-established format widely used for long-term storage of both images and
critically associated metadata, its inherent limitations on maximum image
dimensions can impact retrieval speed, particularly when accessing whole slide
images using a pyramidal structure of slide viewer applications. Here, we
introduce the Iris file extension, a binary file container specification
explicitly designed for whole slide image systems that can abstract the file
structure outline into memory for immediate tile access. The Iris file
extension adds modern compression support, a dynamic structure with optional
file features, computationally trivial deep file validation and corruption
recovery capabilities, and slide annotation support. In addition to the file
specification document, we provide source code to allow for (de)serialization
and validation of a binary stream against the standard and corresponding binary
builds with C++, Python, and JavaScript language bindings. We further provide
full encoder and decoder implementation source code, as well as binary builds
(as part of the separate Iris Codec Community module) with language bindings
for C++ and Python to allow for easy integration with existing WSI solutions.
We provide the Iris File Extension specification openly to the community in the
form of a Creative Commons Attribution-No Derivative 4.0 international license.

</details>


### [505] [Rethinking Brain Tumor Segmentation from the Frequency Domain Perspective](https://arxiv.org/pdf/2506.10142)
*Minye Shao, Zeyu Wang, Haoran Duan, Yawen Huang, Bing Zhai, Shizheng Wang, Yang Long, Yefeng Zheng*

Main category: eess.IV

TL;DR: HFF-Net improves brain tumor segmentation by leveraging frequency-domain analysis, achieving notable performance gains, especially in contrast-enhancing regions.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with segmenting contrast-enhancing brain tumor areas due to insufficient consideration of MRI-specific features like textures and directional variations.

Method: Proposes HFF-Net with Frequency Domain Decomposition (FDD), Adaptive Laplacian Convolution (ALC), and Frequency Domain Cross-Attention (FDCA) modules to enhance segmentation.

Result: Achieves 4.48% average improvement in Dice scores and 7.33% in contrast-enhancing regions across datasets.

Conclusion: HFF-Net offers a robust, efficient solution for brain tumor segmentation, validated by experiments and theoretical analysis.

Abstract: Precise segmentation of brain tumors, particularly contrast-enhancing regions
visible in post-contrast MRI (areas highlighted by contrast agent injection),
is crucial for accurate clinical diagnosis and treatment planning but remains
challenging. However, current methods exhibit notable performance degradation
in segmenting these enhancing brain tumor areas, largely due to insufficient
consideration of MRI-specific tumor features such as complex textures and
directional variations. To address this, we propose the Harmonized Frequency
Fusion Network (HFF-Net), which rethinks brain tumor segmentation from a
frequency-domain perspective. To comprehensively characterize tumor regions, we
develop a Frequency Domain Decomposition (FDD) module that separates MRI images
into low-frequency components, capturing smooth tumor contours and
high-frequency components, highlighting detailed textures and directional
edges. To further enhance sensitivity to tumor boundaries, we introduce an
Adaptive Laplacian Convolution (ALC) module that adaptively emphasizes critical
high-frequency details using dynamically updated convolution kernels. To
effectively fuse tumor features across multiple scales, we design a Frequency
Domain Cross-Attention (FDCA) integrating semantic, positional, and
slice-specific information. We further validate and interpret frequency-domain
improvements through visualization, theoretical reasoning, and experimental
analyses. Extensive experiments on four public datasets demonstrate that
HFF-Net achieves an average relative improvement of 4.48\% (ranging from 2.39\%
to 7.72\%) in the mean Dice scores across the three major subregions, and an
average relative improvement of 7.33% (ranging from 5.96% to 8.64%) in the
segmentation of contrast-enhancing tumor regions, while maintaining favorable
computational efficiency and clinical applicability. Code:
https://github.com/VinyehShaw/HFF.

</details>


### [506] [Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation](https://arxiv.org/pdf/2506.10230)
*Emerson P. Grabke, Masoom A. Haider, Babak Taati*

Main category: eess.IV

TL;DR: CCELLA improves medical LDM training by combining non-medical text features and pathology classification, enabling high-quality image synthesis with limited data.


<details>
  <summary>Details</summary>
Motivation: Address limitations in medical LDM training, such as reliance on short-prompt text encoders, non-medical LDMs, or large data volumes.

Method: Proposes CCELLA, a dual-head conditioning approach with cross-attention and timestep embedding, a joint loss function, and a data-efficient training framework.

Result: Achieves a 3D FID score of 0.025, outperforming a foundation model (FID 0.071). Synthetic images improve classifier accuracy from 69% to 74%.

Conclusion: CCELLA enhances LDM performance and accessibility for medical imaging, even with limited data.

Abstract: Latent diffusion models (LDM) could alleviate data scarcity challenges
affecting machine learning development for medical imaging. However, medical
LDM training typically relies on performance- or scientific
accessibility-limiting strategies including a reliance on short-prompt text
encoders, the reuse of non-medical LDMs, or a requirement for fine-tuning with
large data volumes. We propose a Class-Conditioned Efficient Large Language
model Adapter (CCELLA) to address these limitations. CCELLA is a novel
dual-head conditioning approach that simultaneously conditions the LDM U-Net
with non-medical large language model-encoded text features through
cross-attention and with pathology classification through the timestep
embedding. We also propose a joint loss function and a data-efficient LDM
training framework. In combination, these strategies enable
pathology-conditioned LDM training for high-quality medical image synthesis
given limited data volume and human data annotation, improving LDM performance
and scientific accessibility. Our method achieves a 3D FID score of 0.025 on a
size-limited prostate MRI dataset, significantly outperforming a recent
foundation model with FID 0.071. When training a classifier for prostate cancer
prediction, adding synthetic images generated by our method to the training
dataset improves classifier accuracy from 69% to 74%. Training a classifier
solely on our method's synthetic images achieved comparable performance to
training on real images alone.

</details>


### [507] [Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization](https://arxiv.org/pdf/2506.10233)
*Ana Lawry Aguila, Peirong Liu, Oula Puonti, Juan Eugenio Iglesias*

Main category: eess.IV

TL;DR: A novel conditional diffusion model for unsupervised anomaly detection in brain MRI, using synthetic pseudo-pathologies to improve reconstruction of healthy images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Supervised learning for pathology detection requires diseased training data, which is scarce for rare diseases. Unsupervised methods often fail to reconstruct anomalies accurately.

Method: Introduces a weakly supervised conditional diffusion model integrating synthetic pseudo-pathologies generated via fluid-driven anomaly randomization.

Result: Outperforms variational autoencoders and diffusion models, even surpassing supervised inpainting methods on most datasets.

Conclusion: The proposed framework effectively detects anomalies and reconstructs healthy images without requiring large diseased datasets.

Abstract: Supervised machine learning has enabled accurate pathology detection in brain
MRI, but requires training data from diseased subjects that may not be readily
available in some scenarios, for example, in the case of rare diseases.
Reconstruction-based unsupervised anomaly detection, in particular using
diffusion models, has gained popularity in the medical field as it allows for
training on healthy images alone, eliminating the need for large
disease-specific cohorts. These methods assume that a model trained on normal
data cannot accurately represent or reconstruct anomalies. However, this
assumption often fails with models failing to reconstruct healthy tissue or
accurately reconstruct abnormal regions i.e., failing to remove anomalies. In
this work, we introduce a novel conditional diffusion model framework for
anomaly detection and healthy image reconstruction in brain MRI. Our weakly
supervised approach integrates synthetically generated pseudo-pathology images
into the modeling process to better guide the reconstruction of healthy images.
To generate these pseudo-pathologies, we apply fluid-driven anomaly
randomization to augment real pathology segmentation maps from an auxiliary
dataset, ensuring that the synthetic anomalies are both realistic and
anatomically coherent. We evaluate our model's ability to detect pathology,
using both synthetic anomaly datasets and real pathology from the ATLAS
dataset. In our extensive experiments, our model: (i) consistently outperforms
variational autoencoders, and conditional and unconditional latent diffusion;
and (ii) surpasses on most datasets, the performance of supervised inpainting
methods with access to paired diseased/healthy images.

</details>


### [508] [DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation Equivariance for Dynamic MRI Reconstruction](https://arxiv.org/pdf/2506.10309)
*Yuliang Zhu, Jing Cheng, Qi Xie, Zhuo-Xu Cui, Qingyong Zhu, Yuanyuan Liu, Xin Liu, Jianfeng Ren, Chengbo Wang, Dong Liang*

Main category: eess.IV

TL;DR: The paper proposes DUN-SRE, a deep unrolling network with spatiotemporal rotation equivariance, to improve dynamic MRI reconstruction by incorporating spatial and temporal symmetry priors.


<details>
  <summary>Details</summary>
Motivation: Existing equivariant CNNs fail to model temporal symmetry, a critical prior in dynamic MRI. Addressing this gap can enhance reconstruction quality, especially in undersampling scenarios.

Method: DUN-SRE uses a (2+1)D equivariant convolutional architecture within a deep unrolling framework, integrating data consistency and proximal mapping while enforcing spatiotemporal symmetry constraints.

Result: DUN-SRE achieves state-of-the-art performance on Cardiac CINE MRI datasets, preserving rotation-symmetric structures and demonstrating strong generalization.

Conclusion: DUN-SRE effectively models spatiotemporal symmetry, improving dynamic MRI reconstruction accuracy and robustness.

Abstract: Dynamic Magnetic Resonance Imaging (MRI) exhibits transformation symmetries,
including spatial rotation symmetry within individual frames and temporal
symmetry along the time dimension. Explicit incorporation of these symmetry
priors in the reconstruction model can significantly improve image quality,
especially under aggressive undersampling scenarios. Recently, Equivariant
convolutional neural network (ECNN) has shown great promise in exploiting
spatial symmetry priors. However, existing ECNNs critically fail to model
temporal symmetry, arguably the most universal and informative structural prior
in dynamic MRI reconstruction. To tackle this issue, we propose a novel Deep
Unrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE) for
Dynamic MRI Reconstruction. The DUN-SRE establishes spatiotemporal equivariance
through a (2+1)D equivariant convolutional architecture. In particular, it
integrates both the data consistency and proximal mapping module into a unified
deep unrolling framework. This architecture ensures rigorous propagation of
spatiotemporal rotation symmetry constraints throughout the reconstruction
process, enabling more physically accurate modeling of cardiac motion dynamics
in cine MRI. In addition, a high-fidelity group filter parameterization
mechanism is developed to maintain representation precision while enforcing
symmetry constraints. Comprehensive experiments on Cardiac CINE MRI datasets
demonstrate that DUN-SRE achieves state-of-the-art performance, particularly in
preserving rotation-symmetric structures, offering strong generalization
capability to a broad range of dynamic MRI reconstruction tasks.

</details>


### [509] [SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for Semi-Supervised 3D Intracranial Hemorrhage Segmentation](https://arxiv.org/pdf/2506.10325)
*Cheng Wang, Siqi Chen, Donghua Mi, Yang Chen, Yudong Zhang, Yinsheng Li*

Main category: eess.IV

TL;DR: SWDL-Net, a novel semi-supervised learning framework, combines Laplacian pyramid and deep convolutional upsampling to improve intracranial hemorrhage segmentation with minimal labeled data, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for intracranial hemorrhage (ICH) is tedious and costly, and deep learning-based segmentation requires large labeled datasets. Semi-supervised learning (SSL) is explored to mitigate this challenge.

Method: Proposes SWDL-Net, an SSL framework integrating Laplacian pyramid (for edge sharpening) and deep convolutional upsampling (for detail precision) via a difference learning mechanism.

Result: Outperforms state-of-the-art methods on a 271-case ICH dataset with only 2% labeled data and confirms superiority on the BHSD dataset with 5% labeled data.

Conclusion: SWDL-Net effectively addresses the scarcity of labeled data in ICH segmentation, demonstrating superior performance in semi-supervised settings.

Abstract: Recent advances in medical imaging have established deep learning-based
segmentation as the predominant approach, though it typically requires large
amounts of manually annotated data. However, obtaining annotations for
intracranial hemorrhage (ICH) remains particularly challenging due to the
tedious and costly labeling process. Semi-supervised learning (SSL) has emerged
as a promising solution to address the scarcity of labeled data, especially in
volumetric medical image segmentation. Unlike conventional SSL methods that
primarily focus on high-confidence pseudo-labels or consistency regularization,
we propose SWDL-Net, a novel SSL framework that exploits the complementary
advantages of Laplacian pyramid and deep convolutional upsampling. The
Laplacian pyramid excels at edge sharpening, while deep convolutions enhance
detail precision through flexible feature mapping. Our framework achieves
superior segmentation of lesion details and boundaries through a difference
learning mechanism that effectively integrates these complementary approaches.
Extensive experiments on a 271-case ICH dataset and public benchmarks
demonstrate that SWDL-Net outperforms current state-of-the-art methods in
scenarios with only 2% labeled data. Additional evaluations on the publicly
available Brain Hemorrhage Segmentation Dataset (BHSD) with 5% labeled data
further confirm the superiority of our approach. Code and data have been
released at https://github.com/SIAT-CT-LAB/SWDL.

</details>


### [510] [ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation](https://arxiv.org/pdf/2506.10675)
*Xi Chen, Zhiqiang Shen, Peng Cao, Jinzhu Yang, Osmar R. Zaiane*

Main category: eess.IV

TL;DR: The paper proposes ConStyX, a domain randomization-based method for generalizable medical image segmentation, addressing limitations of existing methods by augmenting both content and style while mitigating over-augmentation effects.


<details>
  <summary>Details</summary>
Motivation: Domain shifts in medical images impair segmentation models. Existing domain randomization methods are inefficient and neglect over-augmentation issues.

Method: ConStyX augments content and style of training data and mitigates negative effects of over-augmented features.

Result: ConStyX achieves superior generalization performance across multiple domains.

Conclusion: ConStyX effectively improves medical image segmentation by addressing domain shift and over-augmentation challenges.

Abstract: Medical images are usually collected from multiple domains, leading to domain
shifts that impair the performance of medical image segmentation models. Domain
Generalization (DG) aims to address this issue by training a robust model with
strong generalizability. Recently, numerous domain randomization-based DG
methods have been proposed. However, these methods suffer from the following
limitations: 1) constrained efficiency of domain randomization due to their
exclusive dependence on image style perturbation, and 2) neglect of the adverse
effects of over-augmented images on model training. To address these issues, we
propose a novel domain randomization-based DG method, called content style
augmentation (ConStyX), for generalizable medical image segmentation.
Specifically, ConStyX 1) augments the content and style of training data,
allowing the augmented training data to better cover a wider range of data
domains, and 2) leverages well-augmented features while mitigating the negative
effects of over-augmented features during model training. Extensive experiments
across multiple domains demonstrate that our ConStyX achieves superior
generalization performance. The code is available at
https://github.com/jwxsp1/ConStyX.

</details>


### [511] [SNR and Resource Adaptive Deep JSCC for Distributed IoT Image Classification](https://arxiv.org/pdf/2506.10699)
*Ali Waqas, Sinem Coleri*

Main category: eess.IV

TL;DR: Proposes an adaptive CNN framework for IoT devices using a learning-assisted genetic algorithm to optimize network configurations under varying SNR and computational constraints, outperforming fixed-split methods.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of fixed-split DNN-based JSCC schemes by adapting to dynamic computational budgets and channel conditions.

Method: Uses a learning-assisted Genetic Algorithm (LAIGA) to explore CNN hyperparameters, discarding infeasible configurations and leveraging Random Forests for efficiency.

Result: Achieves 10% higher classification accuracy than existing methods, especially under low SNR (-10dB) and limited computational budgets (1M-70M FLOPs).

Conclusion: The proposed framework is effective for wireless image classification in IoT, adapting to constraints and improving performance over static methods.

Abstract: Sensor-based local inference at IoT devices faces severe computational
limitations, often requiring data transmission over noisy wireless channels for
server-side processing. To address this, split-network Deep Neural Network
(DNN) based Joint Source-Channel Coding (JSCC) schemes are used to extract and
transmit relevant features instead of raw data. However, most existing methods
rely on fixed network splits and static configurations, lacking adaptability to
varying computational budgets and channel conditions. In this paper, we propose
a novel SNR- and computation-adaptive distributed CNN framework for wireless
image classification across IoT devices and edge servers. We introduce a
learning-assisted intelligent Genetic Algorithm (LAIGA) that efficiently
explores the CNN hyperparameter space to optimize network configuration under
given FLOPs constraints and given SNR. LAIGA intelligently discards the
infeasible network configurations that exceed computational budget at IoT
device. It also benefits from the Random Forests based learning assistance to
avoid a thorough exploration of hyperparameter space and to induce application
specific bias in candidate optimal configurations. Experimental results
demonstrate that the proposed framework outperforms fixed-split architectures
and existing SNR-adaptive methods, especially under low SNR and limited
computational resources. We achieve a 10\% increase in classification accuracy
as compared to existing JSCC based SNR-adaptive multilayer framework at an SNR
as low as -10dB across a range of available computational budget (1M to 70M
FLOPs) at IoT device.

</details>


### [512] [Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches](https://arxiv.org/pdf/2506.10825)
*Andrea Moglia, Matteo Leccardi, Matteo Cavicchioli, Alice Maccarini, Marco Marcon, Luca Mainardi, Pietro Cerveri*

Main category: eess.IV

TL;DR: A survey on generalist models for medical image segmentation, inspired by SAM, covering taxonomy, performance analysis, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of generalist models like SAM on medical image segmentation and address challenges in regulatory compliance, privacy, and AI trustworthiness.

Method: Comprehensive investigation including taxonomy of SAM variants, performance analysis, and comparison with task-specific models.

Result: Highlights the potential of generalist models but identifies challenges like regulatory compliance and privacy.

Conclusion: Future directions include synthetic data, early fusion, and lessons from NLP, with a focus on clinical translation.

Abstract: Following the successful paradigm shift of large language models, leveraging
pre-training on a massive corpus of data and fine-tuning on different
downstream tasks, generalist models have made their foray into computer vision.
The introduction of Segment Anything Model (SAM) set a milestone on
segmentation of natural images, inspiring the design of a multitude of
architectures for medical image segmentation. In this survey we offer a
comprehensive and in-depth investigation on generalist models for medical image
segmentation. We start with an introduction on the fundamentals concepts
underpinning their development. Then, we provide a taxonomy on the different
declinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on
the recent SAM 2, on other innovative models trained on images alone, and
others trained on both text and images. We thoroughly analyze their
performances at the level of both primary research and best-in-literature,
followed by a rigorous comparison with the state-of-the-art task-specific
models. We emphasize the need to address challenges in terms of compliance with
regulatory frameworks, privacy and security laws, budget, and trustworthy
artificial intelligence (AI). Finally, we share our perspective on future
directions concerning synthetic data, early fusion, lessons learnt from
generalist models in natural language processing, agentic AI and physical AI,
and clinical translation.

</details>


### [513] [A novel visual data-based diagnostic approach for estimation of regime transition in pool boiling](https://arxiv.org/pdf/2506.10832)
*Pranay Nirapure, Ayushman Singh, Srikanth Rangarajan, Bahgat Sammakia*

Main category: eess.IV

TL;DR: The paper introduces the Index of Visual Similarity (IVS), a novel metric for analyzing boiling heat transfer regimes using visual data, validated against traditional heat transfer metrics.


<details>
  <summary>Details</summary>
Motivation: To provide a non-intrusive, rapid, and reliable method for diagnosing boiling regimes using visual data, addressing limitations in traditional heat transfer measurements.

Method: IVS combines morphological similarity (SIFT-based feature matching) and physical similarity (vapor area estimation via Mask R-CNN) from high-speed images of pool boiling on different surfaces.

Result: IVS effectively captures boiling regime transitions, correlating strongly with traditional metrics and demonstrating sensitivity to surface superheat.

Conclusion: IVS is a promising tool for real-time, image-based boiling diagnostics, with potential applications in phase change heat transfer.

Abstract: This study introduces a novel metric, the Index of Visual Similarity (IVS),
to qualitatively characterize boiling heat transfer regimes using only visual
data. The IVS is constructed by combining morphological similarity, through
SIFT-based feature matching, with physical similarity, via vapor area
estimation using Mask R-CNN. High-speed images of pool boiling on two distinct
surfaces, polished copper and porous copper foam, are employed to demonstrate
the generalizability of the approach. IVS captures critical changes in bubble
shape, size, and distribution that correspond to transitions in heat transfer
mechanisms. The metric is validated against an equivalent metric, $\Phi$,
derived from measured heat transfer coefficients (HTC), showing strong
correlation and reliability in detecting boiling regime transitions, including
the onset of nucleate boiling and proximity to critical heat flux (CHF). Given
experimental limitations in precisely measuring changes in HTC, the sensitivity
of IVS to surface superheat is also examined to reinforce the credibility of
IVS. IVS thus emerges as a powerful, rapid, and non-intrusive tool for
real-time, image-based boiling diagnostics, with promising applications in
phase change heat transfer.

</details>


### [514] [Med-URWKV: Pure RWKV With ImageNet Pre-training For Medical Image Segmentation](https://arxiv.org/pdf/2506.10858)
*Zhenhuan Zhou*

Main category: eess.IV

TL;DR: Med-URWKV is a pure RWKV-based medical image segmentation model leveraging pre-trained VRWKV encoders, outperforming scratch-trained models.


<details>
  <summary>Details</summary>
Motivation: Existing methods (CNNs, Transformers, hybrids) have limitations like restricted receptive fields or high computational costs. RWKV offers linear complexity and strong long-range modeling, but its potential with pre-trained models in medical segmentation is unexplored.

Method: Proposes Med-URWKV, a U-Net-based pure RWKV architecture using ImageNet-pretrained VRWKV encoders.

Result: Outperforms scratch-trained RWKV models on seven datasets, validating the effectiveness of pre-trained encoders.

Conclusion: Med-URWKV demonstrates the advantages of pre-trained RWKV models in medical segmentation, offering a promising direction for future research.

Abstract: Medical image segmentation is a fundamental and key technology in
computer-aided diagnosis and treatment. Previous methods can be broadly
classified into three categories: convolutional neural network (CNN) based,
Transformer based, and hybrid architectures that combine both. However, each of
them has its own limitations, such as restricted receptive fields in CNNs or
the computational overhead caused by the quadratic complexity of Transformers.
Recently, the Receptance Weighted Key Value (RWKV) model has emerged as a
promising alternative for various vision tasks, offering strong long-range
modeling capabilities with linear computational complexity. Some studies have
also adapted RWKV to medical image segmentation tasks, achieving competitive
performance. However, most of these studies focus on modifications to the
Vision-RWKV (VRWKV) mechanism and train models from scratch, without exploring
the potential advantages of leveraging pre-trained VRWKV models for medical
image segmentation tasks. In this paper, we propose Med-URWKV, a pure
RWKV-based architecture built upon the U-Net framework, which incorporates
ImageNet-based pretraining to further explore the potential of RWKV in medical
image segmentation tasks. To the best of our knowledge, Med-URWKV is the first
pure RWKV segmentation model in the medical field that can directly reuse a
large-scale pre-trained VRWKV encoder. Experimental results on seven datasets
demonstrate that Med-URWKV achieves comparable or even superior segmentation
performance compared to other carefully optimized RWKV models trained from
scratch. This validates the effectiveness of using a pretrained VRWKV encoder
in enhancing model performance. The codes will be released.

</details>


### [515] [Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach](https://arxiv.org/pdf/2506.10916)
*Meredith VandeHaar, M. Clinch, I. Yilmaz, M. A. Rahman, Y. Xiao, F. Dogany, H. M. Alazab, A. Nassar, Z. Akkus, B. Dangott*

Main category: eess.IV

TL;DR: An AI algorithm is proposed to detect and localize artifacts in digital pathology slides, improving quality assurance by reducing manual review time.


<details>
  <summary>Details</summary>
Motivation: Current quality assurance in digital pathology relies on manual review, which is time-consuming and lacks deep learning integration for artifact detection.

Method: The algorithm analyzes tiles, categorizing them into 10 artifact types or background, using InceptionResNet and a hybrid design of single and multiple instance models.

Result: The hybrid model optimizes artifact detection, as demonstrated by testing on 133 whole slide images with annotated artifacts.

Conclusion: The proposed AI solution enhances efficiency and accuracy in digital pathology quality assurance, addressing a critical gap in the field.

Abstract: Quality assurance is a critical but underexplored area in digital pathology,
where even minor artifacts can have significant effects. Artifacts have been
shown to negatively impact the performance of AI diagnostic models. In current
practice, trained staff manually review digitized images prior to release of
these slides to pathologists which are then used to render a diagnosis.
Conventional image processing approaches, provide a foundation for detecting
artifacts on digital pathology slides. However, current tools do not leverage
deep learning, which has the potential to improve detection accuracy and
scalability. Despite these advancements, methods for quality assurance in
digital pathology remain limited, presenting a gap for innovation.
  We propose an AI algorithm designed to screen digital pathology slides by
analyzing tiles and categorizing them into one of 10 predefined artifact types
or as background. This algorithm identifies and localizes artifacts, creating a
map that highlights regions of interest. By directing human operators to
specific tiles affected by artifacts, the algorithm minimizes the time and
effort required to manually review entire slides for quality issues.
  From internal archives and The Cancer Genome Atlas, 133 whole slide images
were selected and 10 artifacts were annotated using an internally developed
software ZAPP (Mayo Clinic, Jacksonville, FL). Ablation study of multiple
models at different tile sizes and magnification was performed. InceptionResNet
was selected. Single artifact models were trained and tested, followed by a
limited multiple instance model with artifacts that performed well together
(chatter, fold, and pen). From the results of this study we suggest a hybrid
design for artifact screening composed of both single artifact binary models as
well as multiple instance models to optimize detection of each artifact.

</details>


### [516] [Bias-Switchable Row-Column Array Imaging using Fast Orthogonal Row-Column Electronic Scanning (FORCES) Compared with Conventional Row-Column Array Imaging](https://arxiv.org/pdf/2506.10958)
*Randy Palamar, Mohammad Rahim Sobhani, Darren Dahunsi, Negar Majidi, Afshin Kashani Ilkhechi, Joy Wang, Jeremy Brown, Roger Zemp*

Main category: eess.IV

TL;DR: The study compares bias-switchable Row-Column Arrays (TOBE) with conventional RCAs, showing superior imaging quality using FORCES over TPW and VLS methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional RCAs, such as poor imaging beyond the aperture shadow and focusing issues, by comparing TOBE arrays with traditional methods.

Method: Experimental comparison of TOBE arrays using FORCES against conventional RCA techniques (TPW and VLS), measuring resolution and gCNR in phantoms and animal models.

Result: TOBE arrays with FORCES produced better B-scan and volumetric images, outperforming TPW and VLS in resolution and contrast.

Conclusion: Bias-switchable TOBE arrays with FORCES offer improved imaging quality over conventional RCA methods, validating their potential for 3D-ultrasound.

Abstract: Row-Column Arrays (RCAs) offer an attractive alternative to fully wired
2D-arrays for 3D-ultrasound, due to their greatly simplified wiring. However,
conventional RCAs face challenges related to their long elements. These include
an inability to image beyond the shadow of the aperture and an inability to
focus in both transmit and receive for desired scan planes. To address these
limitations, we recently developed bias-switchable RCAs, also known as Top
Orthogonal to Bottom Electrode (TOBE) arrays. These arrays provide novel
opportunities to read out from every element of the array and achieve
high-quality images. While TOBE arrays and their associated imaging schemes
have shown promise, they have not yet been directly compared experimentally to
conventional RCA imaging techniques. This study aims to provide such a
comparison, demonstrating superior B-scan and volumetric images from two
electrostrictive relaxor TOBE arrays, using a method called Fast Orthogonal
Row-Column Electronic scanning (FORCES), compared to conventional RCA imaging
schemes, including Tilted Plane Wave (TPW) compounding and Virtual Line Source
(VLS) imaging. The study quantifies resolution and Generalized Contrast to
Noise Ratio (gCNR) in phantoms, and also demonstrates volumetric acquisitions
in phantom and animal models.

</details>


### [517] [Towards Clinical Practice in CT-Based Pulmonary Disease Screening: An Efficient and Reliable Framework](https://arxiv.org/pdf/2412.01525)
*Qian Shao, Bang Du, Kai Zhang, Yixuan Wu, Zepeng Li, Qiyuan Chen, Qianqian Tang, Jian Wu, Jintai Chen, Honghao Gao, Hongxia Xu*

Main category: eess.IV

TL;DR: Proposes an Efficient and Reliable Framework (ERF) for pulmonary disease screening from CT scans, combining Cluster-based Sub-Sampling (CSS) and Hybrid Uncertainty Quantification (HUQ) to reduce computational cost while maintaining diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: High computational cost of processing 3D CT volumes hinders clinical adoption of deep learning models; current sub-sampling methods compromise diagnostic integrity.

Method: ERF includes CSS for efficient slice selection and HUQ for uncertainty quantification, balancing representativeness, diversity, and reliability.

Result: Achieves over 90% accuracy and recall, comparable to full-volume analysis, while reducing processing time by 60%.

Conclusion: ERF advances practical AI-powered CT screening, offering speed, accuracy, and trustworthiness for clinical use.

Abstract: Deep learning models for pulmonary disease screening from Computed Tomography
(CT) scans promise to alleviate the immense workload on radiologists. Still,
their high computational cost, stemming from processing entire 3D volumes,
remains a major barrier to widespread clinical adoption. Current sub-sampling
techniques often compromise diagnostic integrity by introducing artifacts or
discarding critical information. To overcome these limitations, we propose an
Efficient and Reliable Framework (ERF) that fundamentally improves the
practicality of automated CT analysis. Our framework introduces two core
innovations: (1) A Cluster-based Sub-Sampling (CSS) method that efficiently
selects a compact yet comprehensive subset of CT slices by optimizing for both
representativeness and diversity. By integrating an efficient k-Nearest
Neighbor (k-NN) search with an iterative refinement process, CSS bypasses the
computational bottlenecks of previous methods while preserving vital diagnostic
features. (2) A lightweight Hybrid Uncertainty Quantification (HUQ) mechanism,
which uniquely assesses both Aleatoric Uncertainty (AU) and Epistemic
Uncertainty (EU) with minimal computational overhead. By maximizing the
discrepancy between auxiliary classifiers, HUQ provides a robust reliability
score, which is crucial for building trust in automated systems operating on
partial data. Validated on two public datasets with 2,654 CT volumes across
diagnostic tasks for 3 pulmonary diseases, our proposed ERF achieves diagnostic
performance comparable to the full-volume analysis (over 90% accuracy and
recall) while reducing processing time by more than 60%. This work represents a
significant step towards deploying fast, accurate, and trustworthy AI-powered
screening tools in time-sensitive clinical settings.

</details>
