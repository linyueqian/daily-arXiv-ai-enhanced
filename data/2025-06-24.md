<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 144]
- [cs.CV](#cs.CV) [Total: 256]
- [cs.AI](#cs.AI) [Total: 82]
- [cs.SD](#cs.SD) [Total: 28]
- [cs.LG](#cs.LG) [Total: 250]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.MM](#cs.MM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 12]
- [eess.IV](#eess.IV) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Outcome-Based Education: Evaluating Students' Perspectives Using Transformer](https://arxiv.org/pdf/2506.17223)
*Shuvra Smaran Das, Anirban Saha Anik, Md Kishor Morol, Mohammad Sakib Mahmood*

Main category: cs.CL

TL;DR: The study uses DistilBERT and LIME to analyze student feedback for Outcome-Based Education (OBE), improving sentiment classification and providing clear insights for educational improvement.


<details>
  <summary>Details</summary>
Motivation: To enhance OBE by leveraging NLP to analyze student feedback and improve measurable educational outcomes.

Method: Implemented transformer-based models (DistilBERT) for sentiment analysis and used LIME for interpretability.

Result: The combination of DistilBERT and LIME provides a robust, interpretable framework for analyzing feedback, aligning with OBE goals.

Conclusion: Transformer models with LIME offer a data-driven approach to improve educational practices in line with OBE principles.

Abstract: Outcome-Based Education (OBE) emphasizes the development of specific
competencies through student-centered learning. In this study, we reviewed the
importance of OBE and implemented transformer-based models, particularly
DistilBERT, to analyze an NLP dataset that includes student feedback. Our
objective is to assess and improve educational outcomes. Our approach is better
than other machine learning models because it uses the transformer's deep
understanding of language context to classify sentiment better, giving better
results across a wider range of matrices. Our work directly contributes to
OBE's goal of achieving measurable outcomes by facilitating the identification
of patterns in student learning experiences. We have also applied LIME (local
interpretable model-agnostic explanations) to make sure that model predictions
are clear. This gives us understandable information about how key terms affect
sentiment. Our findings indicate that the combination of transformer models and
LIME explanations results in a strong and straightforward framework for
analyzing student feedback. This aligns more closely with the principles of OBE
and ensures the improvement of educational practices through data-driven
insights.

</details>


### [2] [Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs](https://arxiv.org/pdf/2506.17231)
*Xiang Li, Chong Zhang, Jia Wang, Fangyu Wu, Yushi Li, Xiaobo Jin*

Main category: cs.CL

TL;DR: Proposes Adversarial Prompt Distillation for efficient, adaptable jailbreak attacks on LLMs using SLMs, achieving high success rates and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and limitations of current jailbreak methods, adapting to rapid LLM advancements and defenses.

Method: Combines masked language modeling, reinforcement learning, and dynamic temperature control for prompt generation and distillation.

Result: Superior attack success rate, harm, resource efficiency, and cross-model adaptability demonstrated.

Conclusion: Feasibility of distilling jailbreak ability to SLMs, revealing vulnerabilities and advancing LLM security research.

Abstract: Attacks on large language models (LLMs) in jailbreaking scenarios raise many
security and ethical issues. Current jailbreak attack methods face problems
such as low efficiency, high computational cost, and poor cross-model
adaptability and versatility, which make it difficult to cope with the rapid
development of LLM and new defense strategies. Our work proposes an Adversarial
Prompt Distillation, which combines masked language modeling, reinforcement
learning, and dynamic temperature control through a prompt generation and
distillation method. It enables small language models (SLMs) to jailbreak
attacks on mainstream LLMs. The experimental results verify the superiority of
the proposed method in terms of attack success rate and harm, and reflect the
resource efficiency and cross-model adaptability. This research explores the
feasibility of distilling the jailbreak ability of LLM to SLM, reveals the
model's vulnerability, and provides a new idea for LLM security research.

</details>


### [3] [Cash or Comfort? How LLMs Value Your Inconvenience](https://arxiv.org/pdf/2506.17367)
*Mateusz Cedro, Timour Ichmoukhamedov, Sofie Goethals, Yifan He, James Hinns, David Martens*

Main category: cs.CL

TL;DR: LLMs show inconsistent and irrational behavior in valuing human discomfort, raising concerns about their use in autonomous decision-making.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs handle trade-offs between financial rewards and user discomfort, given their growing role in decision-making.

Method: Quantify the prices assigned by multiple LLMs to various user discomforts (walking, waiting, hunger, pain) and analyze their responses.

Result: LLMs exhibit large variance, fragility to prompt phrasing, unreasonably low rewards for major inconveniences, and rejection of gains without discomfort.

Conclusion: Current LLMs are unsuitable for autonomous decision-making due to inconsistent and irrational valuation of human discomfort.

Abstract: Large Language Models (LLMs) are increasingly proposed as near-autonomous
artificial intelligence (AI) agents capable of making everyday decisions on
behalf of humans. Although LLMs perform well on many technical tasks, their
behaviour in personal decision-making remains less understood. Previous studies
have assessed their rationality and moral alignment with human decisions.
However, the behaviour of AI assistants in scenarios where financial rewards
are at odds with user comfort has not yet been thoroughly explored. In this
paper, we tackle this problem by quantifying the prices assigned by multiple
LLMs to a series of user discomforts: additional walking, waiting, hunger and
pain. We uncover several key concerns that strongly question the prospect of
using current LLMs as decision-making assistants: (1) a large variance in
responses between LLMs, (2) within a single LLM, responses show fragility to
minor variations in prompt phrasing (e.g., reformulating the question in the
first person can considerably alter the decision), (3) LLMs can accept
unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10
hours), and (4) LLMs can reject monetary gains where no discomfort is imposed
(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for
scrutiny of how LLMs value human inconvenience, particularly as we move toward
applications where such cash-versus-comfort trade-offs are made on users'
behalf.

</details>


### [4] [GTA: Grouped-head latenT Attention](https://arxiv.org/pdf/2506.17286)
*Luoyang Sun, Jiwen Jiang, Cheng Deng, Xinjian Wu, Haifeng Zhang, Lei Chen, Lionel Ni, Jun Wang*

Main category: cs.CL

TL;DR: GTA (Grouped-Head Latent Attention) reduces memory and computation in LLMs by compressing KV cache and reusing attention scores, improving efficiency without performance loss.


<details>
  <summary>Details</summary>
Motivation: Address the computational and memory inefficiencies in attention mechanisms of LLMs, particularly the redundancy in KV cache and attention maps.

Method: Introduces GTA with shared attention maps and a nonlinear value decoder to compress the KV cache and reduce computation.

Result: GTA reduces FLOPs by 62.5%, shrinks KV cache by 70%, and speeds up inference by 2x.

Conclusion: GTA enhances LLM deployment efficiency by optimizing attention mechanisms while maintaining performance.

Abstract: Attention mechanisms underpin the success of large language models (LLMs),
yet their substantial computational and memory overhead poses challenges for
optimizing efficiency and performance. A critical bottleneck arises as KV cache
and attention computations scale rapidly with text length, challenging
deployment on hardware with limited computational and memory resources. We
observe that attention mechanisms exhibit substantial redundancy, since the KV
cache can be significantly compressed and attention maps across heads display
high similarity, revealing that much of the computation and storage is
unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head
Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that
reduces memory usage and computational complexity while maintaining
performance. GTA comprises two components: (1) a shared attention map mechanism
that reuses attention scores across multiple heads, decreasing the key cache
size; and (2) a nonlinear value decoder with learned projections that
compresses the value cache into a latent space, further cutting memory needs.
GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus
Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while
avoiding the extra overhead of Multi-Head Latent Attention to improve LLM
deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in
end-to-end inference speed, with prefill benefiting from reduced computational
cost and decoding benefiting from the smaller cache footprint.

</details>


### [5] [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/pdf/2506.17294)
*Qirui Zheng, Xingbo Wang, Keyuan Cheng, Yunlong Lu, Wenxin Li*

Main category: cs.CL

TL;DR: The paper introduces a framework for AI-Generated Game Commentary (AIGGC), surveys 45 datasets and methods, and compares evaluation metrics. It also provides a structured datasheet for future research.


<details>
  <summary>Details</summary>
Motivation: AIGGC is gaining attention due to its market potential and technical challenges, requiring advanced NLP capabilities.

Method: The paper presents a general framework for AIGGC, surveys existing datasets and methods, and classifies evaluation metrics.

Result: A comprehensive survey of 45 datasets and methods is provided, along with a structured datasheet for benchmarking.

Conclusion: The paper supports future AIGGC research by offering a framework, survey, and open repository of datasets.

Abstract: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to
its market potential and inherent technical challenges. As a comprehensive
multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial
demands on language models, including factual accuracy, logical reasoning,
expressive text generation, generation speed, and context management. In this
paper, we introduce a general framework for AIGGC and present a comprehensive
survey of 45 existing game commentary dataset and methods according to key
challenges they aim to address in this domain. We further classify and compare
various evaluation metrics commonly used in this domain. To support future
research and benchmarking, we also provide a structured datasheet summarizing
the essential attributes of these datasets in appendix, which is meanwhile
publicly available in an open repository.

</details>


### [6] [Semantic uncertainty in advanced decoding methods for LLM generation](https://arxiv.org/pdf/2506.17296)
*Darius Foodeei, Simin Fan, Martin Jaggi*

Main category: cs.CL

TL;DR: The study explores how different decoding methods (e.g., speculative sampling, CoT) affect semantic uncertainty in LLM outputs, showing improved diversity and reliability without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand how decoding strategies impact semantic diversity and reliability in LLM outputs, challenging traditional trade-offs between diversity and accuracy.

Method: Experiments on question answering, summarization, and code generation tasks using techniques like speculative sampling and CoT decoding.

Result: CoT decoding increases semantic diversity with lower predictive entropy, improving code generation Pass@2 rates by 48.8%. Speculative sampling boosts summarization ROUGE scores.

Conclusion: Structured decoding methods enhance semantic exploration and output quality, benefiting applications needing both reliability and diverse solutions.

Abstract: This study investigates semantic uncertainty in large language model (LLM)
outputs across different decoding methods, focusing on emerging techniques like
speculative sampling and chain-of-thought (CoT) decoding. Through experiments
on question answering, summarization, and code generation tasks, we analyze how
different decoding strategies affect both the diversity and reliability of
model outputs. Our findings reveal that while CoT decoding demonstrates higher
semantic diversity, it maintains lower predictive entropy, suggesting that
structured exploration can lead to more confident and accurate outputs. This is
evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower
alignment with reference solutions. For summarization tasks, speculative
sampling proved particularly effective, achieving superior ROUGE scores while
maintaining moderate semantic diversity. Our results challenge conventional
assumptions about trade-offs between diversity and accuracy in language model
outputs, demonstrating that properly structured decoding methods can increase
semantic exploration while maintaining or improving output quality. These
findings have significant implications for deploying language models in
practical applications where both reliability and diverse solution generation
are crucial.

</details>


### [7] [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/pdf/2506.17298)
*Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, Aditya Grover, Volodymyr Kuleshov*

Main category: cs.CL

TL;DR: Mercury introduces diffusion-based LLMs for coding, achieving top speed and quality on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To advance the speed-quality frontier in LLMs for coding applications.

Method: Uses Transformer architecture and parallel token prediction via diffusion.

Result: State-of-the-art throughput (1109 and 737 tokens/sec) and competitive quality.

Conclusion: Mercury Coder sets new benchmarks in speed and quality, validated by real-world use.

Abstract: We present Mercury, a new generation of commercial-scale large language
models (LLMs) based on diffusion. These models are parameterized via the
Transformer architecture and trained to predict multiple tokens in parallel. In
this report, we detail Mercury Coder, our first set of diffusion LLMs designed
for coding applications. Currently, Mercury Coder comes in two sizes: Mini and
Small. These models set a new state-of-the-art on the speed-quality frontier.
Based on independent evaluations conducted by Artificial Analysis, Mercury
Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109
tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform
speed-optimized frontier models by up to 10x on average while maintaining
comparable quality. We discuss additional results on a variety of code
benchmarks spanning multiple languages and use-cases as well as real-world
validation by developers on Copilot Arena, where the model currently ranks
second on quality and is the fastest model overall. We also release a public
API at https://platform.inceptionlabs.ai/ and free playground at
https://chat.inceptionlabs.ai

</details>


### [8] [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/pdf/2506.17314)
*Adnan Qidwai, Srija Mukhopadhyay, Prerana Khatiwada, Dan Roth, Vivek Gupta*

Main category: cs.CL

TL;DR: PRAISE uses LLMs to extract and compare insights from customer reviews and seller descriptions, highlighting discrepancies to improve product listings and buyer trust.


<details>
  <summary>Details</summary>
Motivation: Seller-provided product descriptions are often incomplete or inaccurate, while customer reviews contain valuable but unstructured details.

Method: PRAISE employs Large Language Models (LLMs) to automatically extract, compare, and structure insights from reviews and descriptions, presenting discrepancies in a clear format.

Result: PRAISE effectively generates actionable insights, improving product listing quality and buyer trust.

Conclusion: PRAISE has the potential to enhance e-commerce product catalogs by bridging gaps between seller descriptions and customer reviews.

Abstract: Accurate and complete product descriptions are crucial for e-commerce, yet
seller-provided information often falls short. Customer reviews offer valuable
details but are laborious to sift through manually. We present PRAISE: Product
Review Attribute Insight Structuring Engine, a novel system that uses Large
Language Models (LLMs) to automatically extract, compare, and structure
insights from customer reviews and seller descriptions. PRAISE provides users
with an intuitive interface to identify missing, contradictory, or partially
matching details between these two sources, presenting the discrepancies in a
clear, structured format alongside supporting evidence from reviews. This
allows sellers to easily enhance their product listings for clarity and
persuasiveness, and buyers to better assess product reliability. Our
demonstration showcases PRAISE's workflow, its effectiveness in generating
actionable structured insights from unstructured reviews, and its potential to
significantly improve the quality and trustworthiness of e-commerce product
catalogs.

</details>


### [9] [Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices](https://arxiv.org/pdf/2506.18035)
*Maxence Lasbordes, Daniele Falavigna, Alessio Brutti*

Main category: cs.CL

TL;DR: The paper proposes adding parallel layers to early-exit neural models for speech recognition, processing downsampled inputs to improve performance without increasing inference time.


<details>
  <summary>Details</summary>
Motivation: Dynamic computational load adjustment is crucial for on-device processing with limited resources. Early-exit models lack modularity for variable frame rate architectures like Zipformer.

Method: Introduce parallel layers in early-exit models to process downsampled inputs, enhancing performance while maintaining inference efficiency.

Result: Improved speech recognition performance on benchmarks with minimal parameter increase and no impact on inference time.

Conclusion: The proposed method effectively combines early-exit modularity with variable frame rate efficiency, advancing on-device speech recognition.

Abstract: The ability to dynamically adjust the computational load of neural models
during inference in a resource aware manner is crucial for on-device processing
scenarios, characterised by limited and time-varying computational resources.
Early-exit architectures represent an elegant and effective solution, since
they can process the input with a subset of their layers, exiting at
intermediate branches (the upmost layers are hence removed from the model).
  From a different perspective, for automatic speech recognition applications
there are memory-efficient neural architectures that apply variable frame rate
analysis, through downsampling/upsampling operations in the middle layers,
reducing the overall number of operations and improving significantly the
performance on well established benchmarks. One example is the Zipformer.
However, these architectures lack the modularity necessary to inject early-exit
branches.
  With the aim of improving the performance in early-exit models, we propose
introducing parallel layers in the architecture that process downsampled
versions of their inputs. % in conjunction with standard processing layers. We
show that in this way the speech recognition performance on standard benchmarks
significantly improve, at the cost of a small increase in the overall number of
model parameters but without affecting the inference time.

</details>


### [10] [Towards Safety Evaluations of Theory of Mind in Large Language Models](https://arxiv.org/pdf/2506.17352)
*Tatsuhiro Aoshima, Mitsuaki Akiyama*

Main category: cs.CL

TL;DR: The paper examines deceptive behaviors in LLMs, proposing theory of mind as a metric for safety evaluation, and finds LLMs lag in this capability despite improvements in reading comprehension.


<details>
  <summary>Details</summary>
Motivation: Address concerns about LLMs disabling oversight and acting deceptively, necessitating deeper investigation into intentionality behind such behaviors.

Method: Review theory of mind research, analyze developmental trends in LLMs, and evaluate their theory of mind capabilities.

Result: LLMs show improved reading comprehension but lack comparable development in theory of mind, impacting safety evaluation.

Conclusion: Theory of mind is crucial for LLM safety, but current models fall short; future work must address this gap.

Abstract: As the capabilities of large language models (LLMs) continue to advance, the
importance of rigorous safety evaluation is becoming increasingly evident.
Recent concerns within the realm of safety assessment have highlighted
instances in which LLMs exhibit behaviors that appear to disable oversight
mechanisms and respond in a deceptive manner. For example, there have been
reports suggesting that, when confronted with information unfavorable to their
own persistence during task execution, LLMs may act covertly and even provide
false answers to questions intended to verify their behavior.To evaluate the
potential risk of such deceptive actions toward developers or users, it is
essential to investigate whether these behaviors stem from covert, intentional
processes within the model. In this study, we propose that it is necessary to
measure the theory of mind capabilities of LLMs. We begin by reviewing existing
research on theory of mind and identifying the perspectives and tasks relevant
to its application in safety evaluation. Given that theory of mind has been
predominantly studied within the context of developmental psychology, we
analyze developmental trends across a series of open-weight LLMs. Our results
indicate that while LLMs have improved in reading comprehension, their theory
of mind capabilities have not shown comparable development. Finally, we present
the current state of safety evaluation with respect to LLMs' theory of mind,
and discuss remaining challenges for future work.

</details>


### [11] [Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study](https://arxiv.org/pdf/2506.17410)
*Danielle R. Thomas, Conrad Borchers, Jionghao Lin, Sanjit Kakarla, Shambhavi Bhushan, Erin Gatz, Shivang Gupta, Ralph Abboud, Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: The study explores using generative AI (GPT-4, Gemini-1.5-pro, LearnLM) to analyze tutor actions in math tutoring, focusing on praise and error responses, with high accuracy and alignment to human judgments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying effective tutoring actions at scale using audio transcriptions.

Method: Analyzed 50 transcripts of remote math tutoring sessions using multiple AI models to detect tutor skills like praise and error responses.

Result: AI models achieved high accuracy (82-98%) in detecting tutor actions and closely matched human judgments (73-89%).

Conclusion: Generative AI is feasible and scalable for assessing tutoring practices, with proposed cost-effective prompting strategies for broader application.

Abstract: Tutoring improves student achievement, but identifying and studying what
tutoring actions are most associated with student learning at scale based on
audio transcriptions is an open research problem. This present study
investigates the feasibility and scalability of using generative AI to identify
and evaluate specific tutor moves in real-life math tutoring. We analyze 50
randomly selected transcripts of college-student remote tutors assisting middle
school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,
Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:
delivering effective praise and responding to student math errors. All models
reliably detected relevant situations, for example, tutors providing praise to
students (94-98% accuracy) and a student making a math error (82-88% accuracy)
and effectively evaluated the tutors' adherence to tutoring best practices,
aligning closely with human judgments (83-89% and 73-77%, respectively). We
propose a cost-effective prompting strategy and discuss practical implications
for using large language models to support scalable assessment in authentic
settings. This work further contributes LLM prompts to support reproducibility
and research in AI-supported learning.

</details>


### [12] [Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"](https://arxiv.org/pdf/2506.18600)
*Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli*

Main category: cs.CL

TL;DR: The paper discusses data contamination in LLM simulations but argues that emergent dynamics can still be studied, as shown in social conventions.


<details>
  <summary>Details</summary>
Motivation: Address concerns about data contamination in LLM simulations while demonstrating the feasibility of studying emergent dynamics.

Method: Analyze critiques and empirical observations of LLM populations, focusing on self-organization and emergent dynamics.

Result: Empirical evidence shows that emergent dynamics, like social conventions, can be studied despite data contamination risks.

Conclusion: Data contamination is a concern but does not prevent the study of genuinely emergent dynamics in LLM populations.

Abstract: A potential concern when simulating populations of large language models
(LLMs) is data contamination, i.e. the possibility that training data may shape
outcomes in unintended ways. While this concern is important and may hinder
certain experiments with multi-agent models, it does not preclude the study of
genuinely emergent dynamics in LLM populations. The recent critique by Barrie
and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an
opportunity to clarify that self-organisation and model-dependent emergent
dynamics can be studied in LLM populations, highlighting how such dynamics have
been empirically observed in the specific case of social conventions.

</details>


### [13] [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/pdf/2506.17419)
*Jinhao Duan, James Diffenderfer, Sandeep Madireddy, Tianlong Chen, Bhavya Kailkhura, Kaidi Xu*

Main category: cs.CL

TL;DR: The paper introduces UProp, a framework for quantifying uncertainty in LLMs' sequential decision-making, outperforming existing single-turn methods.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty quantification methods for LLMs focus on single-turn tasks, leaving multi-step decision-making underexplored.

Method: Proposes UProp, an information-theoretic framework decomposing uncertainty into intrinsic and extrinsic (Mutual Information-based) components, estimating PMI over Trajectory-Dependent Decision Processes.

Result: UProp significantly outperforms single-turn UQ baselines on benchmarks like AgentBench and HotpotQA with models like GPT-4.1 and DeepSeek-V3.

Conclusion: UProp is effective for sequential decision-making uncertainty, with potential applications and efficient sampling demonstrated.

Abstract: As Large Language Models (LLMs) are integrated into safety-critical
applications involving sequential decision-making in the real world, it is
essential to know when to trust LLM decisions. Existing LLM Uncertainty
Quantification (UQ) methods are primarily designed for single-turn
question-answering formats, resulting in multi-step decision-making scenarios,
e.g., LLM agentic system, being underexplored. In this paper, we introduce a
principled, information-theoretic framework that decomposes LLM sequential
decision uncertainty into two parts: (i) internal uncertainty intrinsic to the
current decision, which is focused on existing UQ methods, and (ii) extrinsic
uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty
should be inherited from preceding decisions. We then propose UProp, an
efficient and effective extrinsic uncertainty estimator that converts the
direct estimation of MI to the estimation of Pointwise Mutual Information (PMI)
over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is
evaluated over extensive multi-step decision-making benchmarks, e.g.,
AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and
DeepSeek-V3. Experimental results demonstrate that UProp significantly
outperforms existing single-turn UQ baselines equipped with thoughtful
aggregation strategies. Moreover, we provide a comprehensive analysis of UProp,
including sampling efficiency, potential applications, and intermediate
uncertainty propagation, to demonstrate its effectiveness. Codes will be
available at https://github.com/jinhaoduan/UProp.

</details>


### [14] [Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media](https://arxiv.org/pdf/2506.17435)
*Alberto Martinez-Serra, Alejandro De La Fuente, Nienke Viescher, Ana S. Cardenal*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' ability to classify political content (PC) from URLs and text across five countries, comparing performance with human labels and traditional methods.


<details>
  <summary>Details</summary>
Motivation: To explore if URL-level analysis can approximate full-text analysis of PC using LLMs, addressing gaps in political science research.

Method: Uses advanced LLMs (GPT, Llama, Mistral, etc.) to classify PC from URLs and text, comparing results with human labels and supervised ML techniques.

Result: URLs can embed most news content, offering a balance between accuracy and cost, though contextual limitations exist.

Conclusion: LLMs show promise for PC classification from URLs, with recommendations for their use in political science studies.

Abstract: The use of large language models (LLMs) is becoming common in the context of
political science, particularly in studies that analyse individuals use of
digital media. However, while previous research has demonstrated LLMs ability
at labelling tasks, the effectiveness of using LLMs to classify political
content (PC) from just URLs is not yet well explored. The work presented in
this article bridges this gap by evaluating whether LLMs can accurately
identify PC vs. non-PC from both the article text and the URLs from five
countries (France, Germany, Spain, the UK, and the US) and different languages.
Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we
measure model performance to assess whether URL-level analysis can be a good
approximation for full-text analysis of PC, even across different linguistic
and national contexts. Model outputs are compared with human-labelled articles,
as well as traditional supervised machine learning techniques, to set a
baseline of performance. Overall, our findings suggest the capacity of URLs to
embed most of the news content, providing a vital perspective on accuracy-cost
balancing. We also account for contextual limitations and suggest
methodological recommendations to use LLMs within political science studies.

</details>


### [15] [Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages](https://arxiv.org/pdf/2506.17459)
*Siyu Liang, Gina-Anne Levow*

Main category: cs.CL

TL;DR: Benchmarking multilingual ASR models (MMS and XLS-R) for low-resource languages shows MMS excels with minimal data, while XLS-R matches performance with over one hour of training.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of ASR in linguistic fieldwork due to challenges like spontaneous speech, noise, and small datasets from under-documented languages.

Method: Fine-tuned MMS and XLS-R models on five typologically diverse low-resource languages, controlling training data duration.

Result: MMS performs best with very small datasets; XLS-R achieves similar performance with over one hour of training.

Conclusion: Provides practical guidelines for field linguists, emphasizing reproducible ASR adaptation to ease transcription in language documentation.

Abstract: Automatic Speech Recognition (ASR) has reached impressive accuracy for
high-resource languages, yet its utility in linguistic fieldwork remains
limited. Recordings collected in fieldwork contexts present unique challenges,
including spontaneous speech, environmental noise, and severely constrained
datasets from under-documented languages. In this paper, we benchmark the
performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five
typologically diverse low-resource languages with control of training data
duration. Our findings show that MMS is best suited when extremely small
amounts of training data are available, whereas XLS-R shows parity performance
once training data exceed one hour. We provide linguistically grounded analysis
for further provide insights towards practical guidelines for field linguists,
highlighting reproducible ASR adaptation approaches to mitigate the
transcription bottleneck in language documentation.

</details>


### [16] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/pdf/2506.17467)
*Weixin Liang*

Main category: cs.CL

TL;DR: This dissertation explores the societal impact of LLMs, focusing on biases in AI detectors, widespread adoption in writing domains, and their potential for manuscript feedback.


<details>
  <summary>Details</summary>
Motivation: To understand how individuals and institutions adapt to LLMs and address equity concerns in AI governance.

Method: Three research directions: analyzing AI detector biases, measuring LLM adoption in writing domains, and evaluating LLMs' feedback capabilities on manuscripts.

Result: Revealed biases in AI detectors, widespread LLM adoption in writing, and LLMs' potential to aid researchers with feedback.

Conclusion: LLMs have transformative potential but require equitable governance and careful integration to address biases and support diverse users.

Abstract: Large language models (LLMs) have shown significant potential to change how
we write, communicate, and create, leading to rapid adoption across society.
This dissertation examines how individuals and institutions are adapting to and
engaging with this emerging technology through three research directions.
First, I demonstrate how the institutional adoption of AI detectors introduces
systematic biases, particularly disadvantaging writers of non-dominant language
varieties, highlighting critical equity concerns in AI governance. Second, I
present novel population-level algorithmic approaches that measure the
increasing adoption of LLMs across writing domains, revealing consistent
patterns of AI-assisted content in academic peer reviews, scientific
publications, consumer complaints, corporate communications, job postings, and
international organization press releases. Finally, I investigate LLMs'
capability to provide feedback on research manuscripts through a large-scale
empirical analysis, offering insights into their potential to support
researchers who face barriers in accessing timely manuscript feedback,
particularly early-career researchers and those from under-resourced settings.

</details>


### [17] [VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM](https://arxiv.org/pdf/2506.17506)
*Lesheng Jin, Zhenyuan Ruan, Haohui Mai, Jingbo Shang*

Main category: cs.CL

TL;DR: VeriLocc uses LLMs and formal compiler techniques for generalizable, verifiable GPU register allocation, outperforming hand-tuned methods.


<details>
  <summary>Details</summary>
Motivation: Hand-crafted register allocation heuristics in compilers require re-tuning for each GPU generation, prompting a need for a generalizable solution.

Method: VeriLocc fine-tunes an LLM to translate MIRs into register assignments, using static analysis for normalization and a verifier-guided loop for correctness.

Result: Achieves 85-99% single-shot accuracy, near-100% pass@100, and outperforms rocBLAS by over 10% in runtime.

Conclusion: VeriLocc provides a scalable, high-performance alternative to manual register allocation for GPUs.

Abstract: Modern GPUs evolve rapidly, yet production compilers still rely on
hand-crafted register allocation heuristics that require substantial re-tuning
for each hardware generation. We introduce VeriLocc, a framework that combines
large language models (LLMs) with formal compiler techniques to enable
generalizable and verifiable register allocation across GPU architectures.
VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)
into target-specific register assignments, aided by static analysis for
cross-architecture normalization and generalization and a verifier-guided
regeneration loop to ensure correctness. Evaluated on matrix multiplication
(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot
accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more
performant assignments than expert-tuned libraries, outperforming rocBLAS by
over 10% in runtime.

</details>


### [18] [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/pdf/2506.17525)
*Mingfei Lau, Qian Chen, Yeming Fang, Tingting Xu, Tongzhou Chen, Pavel Golik*

Main category: cs.CL

TL;DR: Quality audit of multilingual speech datasets (Mozilla Common Voice 17.0, FLEURS, VoxPopuli) reveals significant issues, especially in under-resourced languages, with recommendations for improvement.


<details>
  <summary>Details</summary>
Motivation: To address quality issues in public multilingual speech datasets to enhance their utility for training and evaluation, improving downstream models.

Method: Divided quality issues into micro-level and macro-level, with a case analysis of Taiwanese Southern Min (nan_tw) to illustrate the need for proactive language planning and better data quality control.

Result: Macro-level issues are more common in under-resourced languages, highlighting the need for sociolinguistic awareness and improved dataset creation processes.

Conclusion: Proposes guidelines for future dataset development, emphasizing sociolinguistic awareness and proactive language planning to ensure robust and reliable speech data resources.

Abstract: Our quality audit for three widely used public multilingual speech datasets -
Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some
languages, these datasets suffer from significant quality issues. We believe
addressing these issues will make these datasets more useful as training and
evaluation sets, and improve downstream models. We divide these quality issues
into two categories: micro-level and macro-level. We find that macro-level
issues are more prevalent in less institutionalized, often under-resourced
languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that
highlights the need for proactive language planning (e.g. orthography
prescriptions, dialect boundary definition) and enhanced data quality control
in the process of Automatic Speech Recognition (ASR) dataset creation. We
conclude by proposing guidelines and recommendations to mitigate these issues
in future dataset development, emphasizing the importance of sociolinguistic
awareness in creating robust and reliable speech data resources.

</details>


### [19] [DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning](https://arxiv.org/pdf/2506.17533)
*Yuanhao Wu, Juntong Song, Hanning Zhang, Tong Zhang, Cheng Niu*

Main category: cs.CL

TL;DR: DuaShepherd integrates correctness and potential reward signals to improve LLMs' mathematical reasoning, outperforming single-signal models.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLMs' mathematical reasoning by combining correctness (stepwise error identification) and potential (likelihood of correct final answer) reward signals.

Method: Developed an automated pipeline for dataset construction and a multi-head architecture to train two reward models in parallel.

Result: Combined reward signals achieved consistent performance improvements, outperforming single-signal models on MATH500 and ProcessBench benchmarks.

Conclusion: DuaShepherd's dual-reward approach enhances LLM performance in mathematical reasoning, achieving state-of-the-art results under comparable resource constraints.

Abstract: In this paper, we propose DuaShepherd, a novel reward modeling framework that
integrates two complementary reward signals, correctness and potential, to
enhance the mathematical reasoning capabilities of Large Language Models
(LLMs). While correctness-based signals emphasize identification of stepwise
errors, potential-based signals focus on the likelihood of reaching the correct
final answer. We developed an automated pipeline for constructing large-scale
reward modeling dataset with both signals. A unified, multi-head architecture
was explored to train the two reward models in a multi-task setup,
demonstrating benefits from learning both correctness and potential in
parallel. By combining these two signals into a compound probability, our model
achieves consistent performance improvements across multiple benchmarks.
Empirical evaluations on MATH500 and ProcessBench confirm that this combined
reward significantly outperforms models trained on either reward type alone,
achieving state-of-the-art performance under comparable resource constraints.

</details>


### [20] [Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception](https://arxiv.org/pdf/2506.17542)
*Nitin Venkateswaran, Kevin Tang, Ratree Wayland*

Main category: cs.CL

TL;DR: The paper explores how self-supervised learning (SSL) models encode phonological feature variations affecting accent perception, focusing on specific segments in non-native English speakers. Results show SSL representations predict accent strength effectively.


<details>
  <summary>Details</summary>
Motivation: Traditional models overlook gradient phonological feature variations in accent perception. The study aims to bridge this gap using SSL models.

Method: Analyzed phonological features of three segments (labiodental approximant, rhotic tap, retroflex stop) in Hindi-accented English using Phonet and SSL models (Wav2Vec2-BERT, WavLM). Native speaker ratings and probing analyses were used.

Result: Accent strength is best predicted by specific SSL-represented features. Distances from American and Indian English baselines strongly correlate with accent ratings.

Conclusion: SSL models effectively capture phonological features influencing accent perception, offering interpretable insights for accent modeling.

Abstract: Traditional models of accent perception underestimate the role of gradient
variations in phonological features which listeners rely upon for their accent
judgments. We investigate how pretrained representations from current
self-supervised learning (SSL) models of speech encode phonological
feature-level variations that influence the perception of segmental accent. We
focus on three segments: the labiodental approximant, the rhotic tap, and the
retroflex stop, which are uniformly produced in the English of native speakers
of Hindi as well as other languages in the Indian sub-continent. We use the
CSLU Foreign Accented English corpus (Lander, 2007) to extract, for these
segments, phonological feature probabilities using Phonet (V\'asquez-Correa et
al., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,
2023) and WavLM (Chen et al., 2022) along with accent judgements by native
speakers of American English. Probing analyses show that accent strength is
best predicted by a subset of the segment's pretrained representation features,
in which perceptually salient phonological features that contrast the expected
American English and realized non-native English segments are given prominent
weighting. A multinomial logistic regression of pretrained representation-based
segment distances from American and Indian English baselines on accent ratings
reveals strong associations between the odds of accent strength and distances
from the baselines, in the expected directions. These results highlight the
value of self-supervised speech representations for modeling accent perception
using interpretable phonological features.

</details>


### [21] [AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition](https://arxiv.org/pdf/2506.17578)
*Lingxiao Zeng, Yiqi Tong, Wei Guo, Huarui Wu, Lihao Ge, Yijun Ye, Fuzhen Zhuang, Deqing Wang, Wei Guo, Cheng Chen*

Main category: cs.CL

TL;DR: AgriCHN is a high-quality Chinese dataset for agricultural named entity recognition, addressing gaps in existing resources by including diverse entities like hydrology and meteorology, and outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality agricultural datasets, especially in Chinese, and the lack of integration with related fields like hydrology and meteorology motivated the creation of AgriCHN.

Method: The dataset was curated from agricultural articles, comprising 4,040 sentences and 15,799 entity mentions across 27 categories, including hydrology and meteorology.

Result: AgriCHN shows outstanding data quality, richer entity types, and fine-grained divisions, posing a significant challenge in benchmark tests with neural NER models.

Conclusion: AgriCHN fills a critical gap, enhances agricultural entity recognition, and offers potential for further research in the field.

Abstract: Agricultural named entity recognition is a specialized task focusing on
identifying distinct agricultural entities within vast bodies of text,
including crops, diseases, pests, and fertilizers. It plays a crucial role in
enhancing information extraction from extensive agricultural text resources.
However, the scarcity of high-quality agricultural datasets, particularly in
Chinese, has resulted in suboptimal performance when employing mainstream
methods for this purpose. Most earlier works only focus on annotating
agricultural entities while overlook the profound correlation of agriculture
with hydrology and meteorology. To fill this blank, we present AgriCHN, a
comprehensive open-source Chinese resource designed to promote the accuracy of
automated agricultural entity annotation. The AgriCHN dataset has been
meticulously curated from a wealth of agricultural articles, comprising a total
of 4,040 sentences and encapsulating 15,799 agricultural entity mentions
spanning 27 diverse entity categories. Furthermore, it encompasses entities
from hydrology to meteorology, thereby enriching the diversity of entities
considered. Data validation reveals that, compared with relevant resources,
AgriCHN demonstrates outstanding data quality, attributable to its richer
agricultural entity types and more fine-grained entity divisions. A benchmark
task has also been constructed using several state-of-the-art neural NER
models. Extensive experimental results highlight the significant challenge
posed by AgriCHN and its potential for further research.

</details>


### [22] [Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages](https://arxiv.org/pdf/2506.17603)
*Jonathan Sakunkoo, Annabella Sakunkoo*

Main category: cs.CL

TL;DR: The paper investigates morphological defectivity in Latin and Italian using a neural analyzer and validates Wiktionary data, revealing discrepancies in Latin but reliability in Italian.


<details>
  <summary>Details</summary>
Motivation: To address gaps in linguistic resources for morphological defectivity, especially in understudied languages, and improve NLP tool accuracy.

Method: Customizes a neural morphological analyzer to annotate Latin and Italian corpora and computationally validates Wiktionary's defective verb lists.

Result: Wiktionary is reliable for Italian defectivity but shows 7% inaccuracies in Latin, highlighting limitations of crowd-sourced data.

Conclusion: The study advances computational morphology and underscores the need for scalable quality assurance in crowd-sourced linguistic resources.

Abstract: Morphological defectivity is an intriguing and understudied phenomenon in
linguistics. Addressing defectivity, where expected inflectional forms are
absent, is essential for improving the accuracy of NLP tools in morphologically
rich languages. However, traditional linguistic resources often lack coverage
of morphological gaps as such knowledge requires significant human expertise
and effort to document and verify. For scarce linguistic phenomena in
under-explored languages, Wikipedia and Wiktionary often serve as among the few
accessible resources. Despite their extensive reach, their reliability has been
a subject of controversy. This study customizes a novel neural morphological
analyzer to annotate Latin and Italian corpora. Using the massive annotated
data, crowd-sourced lists of defective verbs compiled from Wiktionary are
validated computationally. Our results indicate that while Wiktionary provides
a highly reliable account of Italian morphological gaps, 7% of Latin lemmata
listed as defective show strong corpus evidence of being non-defective. This
discrepancy highlights potential limitations of crowd-sourced wikis as
definitive sources of linguistic knowledge, particularly for less-studied
phenomena and languages, despite their value as resources for rare linguistic
features. By providing scalable tools and methods for quality assurance of
crowd-sourced data, this work advances computational morphology and expands
linguistic knowledge of defectivity in non-English, morphologically rich
languages.

</details>


### [23] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/pdf/2506.17609)
*Lincan Li, Eren Erman Ozguven, Yue Zhao, Guang Wang, Yiqun Xie, Yushun Dong*

Main category: cs.CL

TL;DR: TyphoFormer integrates natural language descriptions with numerical data to improve typhoon track forecasting using a Transformer-based model.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer models lack contextual knowledge for sparse meteorological trajectories like typhoon tracks, limiting forecasting reliability.

Method: TyphoFormer uses LLM-generated textual descriptions of numerical attributes as auxiliary prompts, embedding them with numerical data in a Transformer encoder.

Result: TyphoFormer outperforms state-of-the-art methods on the HURDAT2 benchmark, especially in scenarios with nonlinear path shifts and limited data.

Conclusion: Incorporating language descriptions enhances typhoon trajectory forecasting by providing contextual cues beyond numerical features.

Abstract: Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of sparse meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.

</details>


### [24] [OpusLM: A Family of Open Unified Speech Language Models](https://arxiv.org/pdf/2506.17611)
*Jinchuan Tian, William Chen, Yifan Peng, Jiatong Shi, Siddhant Arora, Shikhar Bharadwaj, Takashi Maekaku, Yusuke Shinohara, Keita Goto, Xiang Yue, Huck Yang, Shinji Watanabe*

Main category: cs.CL

TL;DR: OpusLMs are open foundational speech language models up to 7B, achieving comparable or superior performance in speech and text tasks, with transparent designs and public release.


<details>
  <summary>Details</summary>
Motivation: To advance open research in speech language models by providing transparent, high-performance models and methodologies.

Method: Initialized from text language models, continuously pre-trained on speech-text pairs and text tokens, with designs on tokenization, multi-stream models, and multi-stage training.

Result: OpusLMs match or outperform existing SpeechLMs in speech recognition, synthesis, and text tasks.

Conclusion: OpusLMs demonstrate the effectiveness of scaling and data selection, with full transparency and public release to support open research.

Abstract: This paper presents Open Unified Speech Language Models (OpusLMs), a family
of open foundational speech language models (SpeechLMs) up to 7B. Initialized
from decoder-only text language models, the OpusLMs are continuously
pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We
demonstrate our OpusLMs achieve comparable (or even superior) performance with
existing SpeechLMs in speech recognition, speech synthesis, and text-only
capabilities. Technically, this paper articulates our SpeechLM designs on
tokenization, multi-stream language models, and multi-stage training
strategies. We experimentally demonstrate the importance of model size scaling
and the effect of annealing data selection. The OpusLMs are all built from
publicly available materials and are fully transparent models. We release our
code, data, checkpoints, and training logs to facilitate open SpeechLM research

</details>


### [25] [Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs](https://arxiv.org/pdf/2506.17630)
*Yang Wu, Yifan Zhang, Yiwei Wang, Yujun Cai, Yurong Wu, Yuran Wang, Ning Xu, Jian Cheng*

Main category: cs.CL

TL;DR: LLMs rely heavily on memorized answer-reasoning patterns rather than genuine inference, as shown by a 26.90% performance drop when answer cues are masked.


<details>
  <summary>Details</summary>
Motivation: To determine whether LLMs' reasoning is anchored to final answers or reasoning chains, questioning the depth of their inference.

Method: A five-level answer-visibility prompt framework manipulates answer cues and analyzes model behavior indirectly.

Result: LLMs show strong reliance on explicit answers, with performance dropping significantly when cues are masked.

Conclusion: LLMs' reasoning may be post-hoc rationalization, highlighting the need for a deeper understanding of their inferential capabilities.

Abstract: While Large Language Models (LLMs) demonstrate impressive reasoning
capabilities, growing evidence suggests much of their success stems from
memorized answer-reasoning patterns rather than genuine inference. In this
work, we investigate a central question: are LLMs primarily anchored to final
answers or to the textual pattern of reasoning chains? We propose a five-level
answer-visibility prompt framework that systematically manipulates answer cues
and probes model behavior through indirect, behavioral analysis. Experiments
across state-of-the-art LLMs reveal a strong and consistent reliance on
explicit answers. The performance drops by 26.90\% when answer cues are masked,
even with complete reasoning chains. These findings suggest that much of the
reasoning exhibited by LLMs may reflect post-hoc rationalization rather than
true inference, calling into question their inferential depth. Our study
uncovers the answer-anchoring phenomenon with rigorous empirical validation and
underscores the need for a more nuanced understanding of what constitutes
reasoning in LLMs.

</details>


### [26] [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/pdf/2506.17637)
*Yang Wu, Yifan Zhang, Yurong Wu, Yuran Wang, Junkai Zhang, Jian Cheng*

Main category: cs.CL

TL;DR: Step-Opt-Instruct framework enhances LLMs for OR tasks by generating high-quality fine-tuning data with iterative problem generation and validation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex OR optimization tasks; this work aims to improve their performance through structured data augmentation and validation.

Method: Proposes Step-Opt-Instruct, using iterative problem generation and stepwise validation to fine-tune LLMs like LLaMA-3-8B and Mistral-7B.

Result: Step-Opt achieves a 17.01% improvement in micro average accuracy on complex OR tasks, outperforming benchmarks.

Conclusion: Combining structured validation with gradual problem refinement effectively advances LLM automation for OR decision-making.

Abstract: Large Language Models (LLMs) have revolutionized various domains but
encounter substantial challenges in tackling optimization modeling tasks for
Operations Research (OR), particularly when dealing with complex problem. In
this work, we propose Step-Opt-Instruct, a framework that augments existing
datasets and generates high-quality fine-tuning data tailored to optimization
modeling. Step-Opt-Instruct employs iterative problem generation to
systematically increase problem complexity and stepwise validation to
rigorously verify data, preventing error propagation and ensuring the quality
of the generated dataset. Leveraging this framework, we fine-tune open-source
LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that
achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and
IndustryOR. Extensive experiments demonstrate the superior performance of
Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\%
improvement in micro average accuracy on difficult problems. These findings
highlight the effectiveness of combining structured validation with gradual
problem refinement to advance the automation of decision-making processes using
LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

</details>


### [27] [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/pdf/2506.17671)
*Fabien Furfaro*

Main category: cs.CL

TL;DR: TPTT enhances pretrained Transformer models with efficient linearized attention and memory management, improving efficiency and accuracy without full retraining.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational and memory demands of large language models (LLMs) for long-context inference.

Method: Uses Memory as Gate (MaG) and mixed linearized attention (LiZA), compatible with Hugging Face Transformers via LoRA fine-tuning.

Result: Achieves a 20% EM improvement on MMLU benchmark with 1B-parameter models, showing scalability and robustness.

Conclusion: TPTT is a practical, scalable solution for enhancing LLMs efficiently.

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress in natural language processing, but their computational and memory
demands remain a significant challenge, particularly for long-context
inference. We introduce TPTT (Transforming Pretrained Transformer into Titans),
a novel framework for enhancing pretrained Transformer models with efficient
linearized attention mechanisms and advanced memory management. TPTT employs
techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).
It is fully compatible with the Hugging Face Transformers library, enabling
seamless adaptation of any causal LLM through parameter-efficient fine-tuning
(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU
benchmark with models of approximately 1 billion parameters, observing
substantial improvements in both efficiency and accuracy. For instance,
Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its
baseline. Statistical analyses and comparisons with recent state-of-the-art
methods confirm the practical scalability and robustness of TPTT. Code is
available at https://github.com/fabienfrfr/tptt . Python package at
https://pypi.org/project/tptt/ .

</details>


### [28] [Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering](https://arxiv.org/pdf/2506.17692)
*Binquan Ji, Haibo Luo, Yifei Lu, Lei Hei, Jiaqi Wang, Tingjing Liao, Lingyu Wang, Shichao Wang, Feiliang Ren*

Main category: cs.CL

TL;DR: DEC framework enhances multi-hop QA by decomposing questions into subquestions and refining them iteratively, achieving state-of-the-art results with reduced token usage.


<details>
  <summary>Details</summary>
Motivation: Address challenges like hallucinations and semantic drift in lightweight LLMs for multi-hop QA tasks.

Method: Proposes DEC: decomposes questions into subquestions, refines them iteratively, and uses a lightweight keyword extraction module for precise retrieval.

Result: Outperforms benchmarks on multi-hop QA datasets, especially in resource-constrained settings, with 8B-parameter models.

Conclusion: DEC is effective for complex QA tasks, balancing performance and computational efficiency.

Abstract: Knowledge-intensive multi-hop question answering (QA) tasks, which require
integrating evidence from multiple sources to address complex queries, often
necessitate multiple rounds of retrieval and iterative generation by large
language models (LLMs). However, incorporating many documents and extended
contexts poses challenges -such as hallucinations and semantic drift-for
lightweight LLMs with fewer parameters. This work proposes a novel framework
called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions
into logically coherent subquestions to form a hallucination-free reasoning
chain. It then iteratively refines these subquestions through context-aware
rewriting to generate effective query formulations. For retrieval, we introduce
a lightweight discriminative keyword extraction module that leverages extracted
keywords to achieve targeted, precise document recall with relatively low
computational overhead. Extensive experiments on three multi-hop QA datasets
demonstrate that DEC performs on par with or surpasses state-of-the-art
benchmarks while significantly reducing token consumption. Notably, our
approach attains state-of-the-art results on models with 8B parameters,
showcasing its effectiveness in various scenarios, particularly in
resource-constrained environments.

</details>


### [29] [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/pdf/2506.17693)
*Yuzhe Ding, Kang He, Bobo Li, Li Zheng, Haijun He, Fei Li, Chong Teng, Donghong Ji*

Main category: cs.CL

TL;DR: The paper introduces ZS-CSD, a large-scale zero-shot conversational stance detection dataset, and proposes SITPCL, a model achieving state-of-the-art performance, though challenges remain.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for conversational stance detection are limited to specific targets, hindering real-world applicability.

Method: The authors curate the ZS-CSD dataset and propose SITPCL, a speaker interaction and target-aware prototypical contrastive learning model.

Result: SITPCL achieves a 43.81% F1-macro score, setting a benchmark for zero-shot conversational stance detection.

Conclusion: The work advances zero-shot stance detection but highlights ongoing challenges, as performance remains modest.

Abstract: Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
increasing number of online debates among social media users, conversational
stance detection has become a crucial research area. However, existing
conversational stance detection datasets are restricted to a limited set of
specific targets, which constrains the effectiveness of stance detection models
when encountering a large number of unseen targets in real-world applications.
To bridge this gap, we manually curate a large-scale, high-quality zero-shot
conversational stance detection dataset, named ZS-CSD, comprising 280 targets
across two distinct target types. Leveraging the ZS-CSD dataset, we propose
SITPCL, a speaker interaction and target-aware prototypical contrastive
learning model, and establish the benchmark performance in the zero-shot
setting. Experimental results demonstrate that our proposed SITPCL model
achieves state-of-the-art performance in zero-shot conversational stance
detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,
highlighting the persistent challenges in zero-shot conversational stance
detection.

</details>


### [30] [The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future](https://arxiv.org/pdf/2506.17700)
*Summra Saleem, Muhammad Nabeel Asim, Shaista Zulfiqar, Andreas Dengel*

Main category: cs.CL

TL;DR: This paper provides a comprehensive analysis of prompt optimization strategies for LLMs, categorizing them into 11 classes and detailing their applications in NLP tasks, LLMs, and datasets.


<details>
  <summary>Details</summary>
Motivation: To address the gap in comprehensive analyses of prompt optimization strategies in LLMs, despite existing reviews on prompt engineering.

Method: Analyzes and categorizes prompt optimization strategies into 11 classes, detailing their working paradigms, applications in NLP tasks, and evaluation using LLMs and benchmark datasets.

Result: A robust foundation for future comparative studies and assessment of prompt optimization strategies under consistent experimental settings.

Conclusion: Centralizes strategic knowledge to aid the adaptation of prompt optimization for innovative predictors in unexplored NLP tasks.

Abstract: Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing (NLP) by automating traditional labor-intensive tasks and
consequently accelerated the development of computer-aided applications. As
researchers continue to advance this field with the introduction of novel
language models and more efficient training/finetuning methodologies, the idea
of prompt engineering and subsequent optimization strategies with LLMs has
emerged as a particularly impactful trend to yield a substantial performance
boost across diverse NLP tasks. To best of our knowledge numerous review
articles have explored prompt engineering, however, a critical gap exists in
comprehensive analyses of prompt optimization strategies. To bridge this gap
this paper provides unique and comprehensive insights about the potential of
diverse prompt optimization strategies. It analyzes their underlying working
paradigms and based on these principles, categorizes them into 11 distinct
classes. Moreover, the paper provides details about various NLP tasks where
these prompt optimization strategies have been employed, along with details of
different LLMs and benchmark datasets used for evaluation. This comprehensive
compilation lays a robust foundation for future comparative studies and enables
rigorous assessment of prompt optimization and LLM-based predictive pipelines
under consistent experimental settings: a critical need in the current
landscape. Ultimately, this research will centralize diverse strategic
knowledge to facilitate the adaptation of existing prompt optimization
strategies for development of innovative predictors across unexplored tasks.

</details>


### [31] [Aged to Perfection: Machine-Learning Maps of Age in Conversational English](https://arxiv.org/pdf/2506.17708)
*MingZe Tang*

Main category: cs.CL

TL;DR: The study analyzes spoken British English across age groups using computational methods to identify linguistic markers and predict age groups.


<details>
  <summary>Details</summary>
Motivation: To explore how language patterns vary by age and link demographics to linguistic factors like utterance duration and word choice.

Method: Uses the British National Corpus 2014 and combines computational language analysis with machine learning to model age-related linguistic patterns.

Result: Identifies distinctive linguistic markers for different generations and develops prediction models for age groups.

Conclusion: Advances understanding of sociolinguistic diversity in contemporary British speech.

Abstract: The study uses the British National Corpus 2014, a large sample of
contemporary spoken British English, to investigate language patterns across
different age groups. Our research attempts to explore how language patterns
vary between different age groups, exploring the connection between speaker
demographics and linguistic factors such as utterance duration, lexical
diversity, and word choice. By merging computational language analysis and
machine learning methodologies, we attempt to uncover distinctive linguistic
markers characteristic of multiple generations and create prediction models
that can consistently estimate the speaker's age group from various aspects.
This work contributes to our knowledge of sociolinguistic diversity throughout
the life of modern British speech.

</details>


### [32] [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/pdf/2506.17715)
*Matthias Schffel, Esteban Garces Arias, Marinus Wiedner, Paula Ruppert, Meimingwei Li, Christian Heumann, Matthias Aenmacher*

Main category: cs.CL

TL;DR: The paper examines POS tagging challenges in Medieval Romance languages using LLMs, identifying limitations and effective techniques for handling historical linguistic variations.


<details>
  <summary>Details</summary>
Motivation: POS tagging is crucial for historical text analysis, but LLMs face challenges like linguistic evolution, spelling variations, and data scarcity in Medieval Romance languages.

Method: The study evaluates fine-tuning, prompt engineering, model architectures, decoding strategies, and cross-lingual transfer learning on Medieval Occitan, Spanish, and French texts.

Result: LLMs struggle with historical language variations and spelling but show promise with specialized techniques for low-resource languages.

Conclusion: Specialized approaches can improve POS tagging for Medieval Romance languages despite LLM limitations.

Abstract: Part-of-speech (POS) tagging remains a foundational component in natural
language processing pipelines, particularly critical for historical text
analysis at the intersection of computational linguistics and digital
humanities. Despite significant advancements in modern large language models
(LLMs) for ancient languages, their application to Medieval Romance languages
presents distinctive challenges stemming from diachronic linguistic evolution,
spelling variations, and labeled data scarcity. This study systematically
investigates the central determinants of POS tagging performance across diverse
corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,
spanning biblical, hagiographical, medical, and dietary domains. Through
rigorous experimentation, we evaluate how fine-tuning approaches, prompt
engineering, model architectures, decoding strategies, and cross-lingual
transfer learning techniques affect tagging accuracy. Our results reveal both
notable limitations in LLMs' ability to process historical language variations
and non-standardized spelling, as well as promising specialized techniques that
effectively address the unique challenges presented by low-resource historical
languages.

</details>


### [33] [KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process](https://arxiv.org/pdf/2506.17728)
*Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, QiWei Wang, Xiaorui Wang, Xinkai Du, YangYang Hou, Yu Ao, ZhaoYang Wang, Zhengke Gui, ZhiYing Yi, Zhongpu Bo*

Main category: cs.CL

TL;DR: KAG-Thinker is a human-like reasoning framework for LLMs, enhancing logical coherence in Q&A tasks by decomposing questions into solvable sub-problems and using structured retrieval and reasoning.


<details>
  <summary>Details</summary>
Motivation: To improve logical coherence and contextual consistency in LLMs for domain-specific Q&A tasks by mimicking human cognitive processes.

Method: Decomposes questions into sub-problems (logical forms), uses breadth decomposition, knowledge boundary and depth solving models, and supervised fine-tuning with multi-turn dialogues.

Result: Enhances reasoning and retrieval in LLMs, avoiding excessive reflection through structured inference.

Conclusion: KAG-Thinker effectively improves LLM performance in complex Q&A tasks by simulating human reasoning and structured problem-solving.

Abstract: In this paper, we introduce KAG-Thinker, a novel human-like reasoning
framework built upon a parameter-light large language model (LLM). Our approach
enhances the logical coherence and contextual consistency of the thinking
process in question-answering (Q\&A) tasks on domain-specific knowledge bases
(KBs) within LLMs. This framework simulates human cognitive mechanisms for
handling complex problems by establishing a structured thinking process.
Continuing the \textbf{Logical Form} guided retrieval and reasoning technology
route of KAG v0.7, firstly, it decomposes complex questions into independently
solvable sub-problems(also referred to as logical forms) through
\textbf{breadth decomposition}, each represented in two equivalent
forms-natural language and logical function-and further classified as either
Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and
variables passing explicitly modeled via logical function interfaces. In the
solving process, the Retrieval function is used to perform knowledge retrieval
tasks, while the Math and Deduce functions are used to perform reasoning
analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval
sub-problem tasks, LLMs and external knowledge sources are regarded as
equivalent KBs. We use the \textbf{knowledge boundary} model to determine the
optimal source using self-regulatory mechanisms such as confidence calibration
and reflective reasoning, and use the \textbf{depth solving} model to enhance
the comprehensiveness of knowledge acquisition. Finally, instead of utilizing
reinforcement learning, we employ supervised fine-tuning with multi-turn
dialogues to align the model with our structured inference paradigm, thereby
avoiding excessive reflection. This is supported by a data evaluation framework
and iterative corpus synthesis, which facilitate the generation of detailed
reasoning trajectories...

</details>


### [34] [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/pdf/2506.17748)
*Anwoy Chatterjee, Yash Goel, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: The paper introduces HIDE, a single-pass, training-free method for detecting hallucinations in LMs by analyzing decoupled internal representations, outperforming other methods in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the issue of LMs generating factually incorrect or unfaithful content (hallucinations) without relying on computationally expensive multi-pass methods.

Method: Proposes HIDE, which uses the Hilbert-Schmidt Independence Criterion (HSIC) to quantify decoupling between input context and generated output in LM hidden-state representations.

Result: HIDE outperforms single-pass methods by ~29% in AUC-ROC and matches multi-pass methods with ~3% improvement while using ~51% less computation time.

Conclusion: Exploiting internal representation decoupling in LMs is effective for efficient and practical hallucination detection.

Abstract: Contemporary Language Models (LMs), while impressively fluent, often generate
content that is factually incorrect or unfaithful to the input context - a
critical issue commonly referred to as 'hallucination'. This tendency of LMs to
generate hallucinated content undermines their reliability, especially because
these fabrications are often highly convincing and therefore difficult to
detect. While several existing methods attempt to detect hallucinations, most
rely on analyzing multiple generations per input, leading to increased
computational cost and latency. To address this, we propose a single-pass,
training-free approach for effective Hallucination detectIon via Decoupled
rEpresentations (HIDE). Our approach leverages the hypothesis that
hallucinations result from a statistical decoupling between an LM's internal
representations of input context and its generated output. We quantify this
decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to
hidden-state representations extracted while generating the output sequence. We
conduct extensive experiments on four diverse question answering datasets,
evaluating both faithfulness and factuality hallucinations across six
open-source LMs of varying scales and properties. Our results demonstrate that
HIDE outperforms other single-pass methods in almost all settings, achieving an
average relative improvement of ~29% in AUC-ROC over the best-performing
single-pass strategy across various models and datasets. Additionally, HIDE
shows competitive and often superior performance with multi-pass
state-of-the-art methods, obtaining an average relative improvement of ~3% in
AUC-ROC while consuming ~51% less computation time. Our findings highlight the
effectiveness of exploiting internal representation decoupling in LMs for
efficient and practical hallucination detection.

</details>


### [35] [Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights](https://arxiv.org/pdf/2506.17789)
*N J Karthika, Maharaj Brahma, Rohit Saluja, Ganesh Ramakrishnan, Maunendra Sankar Desarkar*

Main category: cs.CL

TL;DR: The paper evaluates tokenization strategies for 17 Indian languages, comparing BPE and Unigram LM algorithms, vocabulary sizes, and multilingual training methods, showing benefits for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Existing tokenizers favor high-resource languages, disadvantaging linguistically diverse and morphologically rich languages like those in the Indian subcontinent.

Method: Conducted intrinsic evaluation of tokenization strategies, comparing BPE and Unigram LM algorithms, vocabulary sizes, and multilingual training approaches (joint and cluster-based).

Result: Found that low-resource languages benefit from tokenizers trained on related high-resource languages, with practical insights for fairer and more efficient multilingual NLP.

Conclusion: The study provides actionable insights for developing linguistically informed and equitable tokenizers for diverse languages.

Abstract: Tokenization plays a pivotal role in multilingual NLP. However, existing
tokenizers are often skewed towards high-resource languages, limiting their
effectiveness for linguistically diverse and morphologically rich languages
such as those in the Indian subcontinent. This paper presents a comprehensive
intrinsic evaluation of tokenization strategies across 17 Indian languages. We
quantify the trade-offs between bottom-up and top-down tokenizer algorithms
(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of
multilingual vocabulary construction such as joint and cluster-based training.
We also show that extremely low-resource languages can benefit from tokenizers
trained on related high-resource languages. Our study provides practical
insights for building more fair, efficient, and linguistically informed
tokenizers for multilingual NLP.

</details>


### [36] [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/pdf/2506.17844)
*Xin Zhang, Qiyu Wei, Yingjie Zhu, Fanyi Wu, Sophia Ananiadou*

Main category: cs.CL

TL;DR: THCM-CAL is a Temporal-Hierarchical Causal Model for clinical risk prediction, integrating structured and unstructured EHR data with causal interactions and conformal calibration for reliable predictions.


<details>
  <summary>Details</summary>
Motivation: Prior approaches fail to model the hierarchical causal interactions between narrative notes and diagnostic codes in EHRs, limiting prediction accuracy.

Method: THCM-CAL constructs a multimodal causal graph, infers three types of clinical interactions, and uses conformal prediction for calibrated confidence intervals.

Result: THCM-CAL outperforms existing methods on MIMIC-III and MIMIC-IV datasets.

Conclusion: The proposed framework effectively captures clinical causality and improves prediction reliability in EHR-based risk modeling.

Abstract: Automated clinical risk prediction from electronic health records (EHRs)
demands modeling both structured diagnostic codes and unstructured narrative
notes. However, most prior approaches either handle these modalities separately
or rely on simplistic fusion strategies that ignore the directional,
hierarchical causal interactions by which narrative observations precipitate
diagnoses and propagate risk across admissions. In this paper, we propose
THCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our
framework constructs a multimodal causal graph where nodes represent clinical
entities from two modalities: Textual propositions extracted from notes and ICD
codes mapped to textual descriptions. Through hierarchical causal discovery,
THCM-CAL infers three clinically grounded interactions: intra-slice
same-modality sequencing, intra-slice cross-modality triggers, and inter-slice
risk propagation. To enhance prediction reliability, we extend conformal
prediction to multi-label ICD coding, calibrating per-code confidence intervals
under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV
demonstrate the superiority of THCM-CAL.

</details>


### [37] [LLMs for Customized Marketing Content Generation and Evaluation at Scale](https://arxiv.org/pdf/2506.17863)
*Haoran Liu, Amir Tahmasbi, Ehtesham Sam Haque, Purak Jain*

Main category: cs.CL

TL;DR: MarketingFM improves offsite marketing by generating keyword-specific ad copy, boosting CTR and cost efficiency. AutoEval-Main and AutoEval-Update automate ad evaluation, reducing human effort while maintaining alignment with marketing principles.


<details>
  <summary>Details</summary>
Motivation: Current offsite marketing content is generic and misaligned with landing pages, reducing effectiveness.

Method: MarketingFM integrates multiple data sources for keyword-specific ad generation. AutoEval-Main uses rule-based metrics and LLM-as-a-Judge for automated evaluation. AutoEval-Update refines evaluation prompts dynamically with minimal human input.

Result: MarketingFM achieved 9% higher CTR, 12% more impressions, and 0.38% lower CPC. AutoEval-Main matched human reviewers with 89.57% agreement. AutoEval-Update improved evaluation consistency.

Conclusion: Automated systems enhance ad performance and evaluation efficiency, but human oversight remains crucial for validation.

Abstract: Offsite marketing is essential in e-commerce, enabling businesses to reach
customers through external platforms and drive traffic to retail websites.
However, most current offsite marketing content is overly generic,
template-based, and poorly aligned with landing pages, limiting its
effectiveness. To address these limitations, we propose MarketingFM, a
retrieval-augmented system that integrates multiple data sources to generate
keyword-specific ad copy with minimal human intervention. We validate
MarketingFM via offline human and automated evaluations and large-scale online
A/B tests. In one experiment, keyword-focused ad copy outperformed templates,
achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,
demonstrating gains in ad ranking and cost efficiency. Despite these gains,
human review of generated ads remains costly. To address this, we propose
AutoEval-Main, an automated evaluation system that combines rule-based metrics
with LLM-as-a-Judge techniques to ensure alignment with marketing principles.
In experiments with large-scale human annotations, AutoEval-Main achieved
89.57% agreement with human reviewers. Building on this, we propose
AutoEval-Update, a cost-efficient LLM-human collaborative framework to
dynamically refine evaluation prompts and adapt to shifting criteria with
minimal human input. By selectively sampling representative ads for human
review and using a critic LLM to generate alignment reports, AutoEval-Update
improves evaluation consistency while reducing manual effort. Experiments show
the critic LLM suggests meaningful refinements, improving LLM-human agreement.
Nonetheless, human oversight remains essential for setting thresholds and
validating refinements before deployment.

</details>


### [38] [QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs](https://arxiv.org/pdf/2506.17864)
*Taolin Zhang, Haidong Kang, Dongyang Li, Qizhou Chen, Chengyu Wang Xiaofeng He, Richang Hong*

Main category: cs.CL

TL;DR: QueueEDIT, a queue-based self-correction framework, improves sequential model editing (SME) in LLMs by addressing long-sequence dependency and mitigating parameter bias, preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from hallucinations and factual inaccuracies, especially in sequential edits, which can degrade general performance.

Method: Uses structural mapping editing loss to map triplets to knowledge-sensitive neurons, stores parameters in a queue, and dynamically aligns previous edits while freezing irrelevant parameters.

Result: Outperforms baselines in SME and maintains single-turn editing performance; general NLP capabilities are preserved.

Conclusion: QueueEDIT effectively enhances SME and safeguards LLM general abilities.

Abstract: Recently, large language models (LLMs) have demonstrated impressive results
but still suffer from hallucinations. Model editing has been proposed to
correct factual inaccuracies in LLMs. A challenging case is sequential model
editing (SME), which aims to rectify errors continuously rather than treating
them as a one-time task. During SME, the general capabilities of LLMs can be
negatively affected due to the introduction of new parameters. In this paper,
we propose a queue-based self-correction framework (QueueEDIT) that not only
enhances SME performance by addressing long-sequence dependency but also
mitigates the impact of parameter bias on the general capabilities of LLMs.
Specifically, we first introduce a structural mapping editing loss to map the
triplets to the knowledge-sensitive neurons within the Transformer layers of
LLMs. We then store the located parameters for each piece of edited knowledge
in a queue and dynamically align previously edited parameters. In each edit, we
select queue parameters most relevant to the currently located parameters to
determine whether previous knowledge needs realignment. Irrelevant parameters
in the queue are frozen, and we update the parameters at the queue head to the
LLM to ensure they do not harm general abilities. Experiments show that our
framework significantly outperforms strong baselines across various SME
settings and maintains competitiveness in single-turn editing. The resulting
LLMs also preserve high capabilities in general NLP tasks throughout the SME
process.

</details>


### [39] [How Alignment Shrinks the Generative Horizon](https://arxiv.org/pdf/2506.17871)
*Chenghao Yang, Ari Holtzman*

Main category: cs.CL

TL;DR: Aligned LLMs generate less diverse outputs due to probability concentration, quantified by the Branching Factor (BF). BF decreases during generation, and alignment tuning sharpens output distributions, reducing BF. This stability aids complex reasoning, like in Chain-of-Thought models, by leveraging deterministic stages. Alignment steers models toward low-entropy trajectories, not fundamentally changing behavior.


<details>
  <summary>Details</summary>
Motivation: To understand why aligned LLMs produce less diverse outputs and quantify this phenomenon using the Branching Factor (BF).

Method: Introduce BF to measure the effective number of plausible next steps. Analyze BF changes during generation and compare aligned vs. base models.

Result: BF decreases as generation progresses; alignment reduces BF significantly (e.g., from 12 to 1.2). Aligned CoT models exploit this for stable outputs.

Conclusion: BF is a key diagnostic for LLM output control. Alignment tuning and CoT leverage low-entropy trajectories, reducing diversity without altering core behavior.

Abstract: Despite their impressive capabilities, aligned large language models (LLMs)
often generate outputs that lack diversity. What drives this stability in the
generation? We investigate this phenomenon through the lens of probability
concentration in the model's output distribution. To quantify this
concentration, we introduce the Branching Factor (BF) -- a token-invariant
measure of the effective number of plausible next steps during generation. Our
empirical analysis reveals two key findings: (1) BF often decreases as
generation progresses, suggesting that LLMs become more predictable as they
generate. (2) alignment tuning substantially sharpens the model's output
distribution from the outset, reducing BF by nearly an order of magnitude
(e.g., from 12 to 1.2) relative to base models. This stark reduction helps
explain why aligned models often appear less sensitive to decoding strategies.
Building on this insight, we find this stability has surprising implications
for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,
DeepSeek-distilled models), for instance, leverage this effect; by generating
longer reasoning chains, they push generation into later, more deterministic
(lower BF) stages, resulting in more stable outputs. We hypothesize that
alignment tuning does not fundamentally change a model's behavior, but instead
steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy
trajectories already present in the base model. This view is supported by
nudging experiments, which show that prompting base models with such tokens can
similarly reduce BF. Together, our findings establish BF as a powerful
diagnostic for understanding and controlling LLM outputs - clarifying how
alignment reduces variability, how CoT promotes stable generations, and how
base models can be steered away from diversity.

</details>


### [40] [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/pdf/2506.17881)
*Hua Tang, Lingyong Yan, Yukun Zhao, Shuaiqiang Wang, Jizhou Huang, Dawei Yin*

Main category: cs.CL

TL;DR: The paper introduces a novel multi-turn jailbreaking method for LLMs, refining the jailbreaking path globally and fabricating responses to bypass safety measures, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Address the underexplored and challenging multi-turn jailbreaking scenarios in LLMs, where current methods fail to adapt dynamically.

Method: Proposes a method that globally refines the jailbreaking path at each interaction and actively fabricates model responses to suppress safety warnings.

Result: Demonstrates superior performance over existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs.

Conclusion: The method effectively addresses the limitations of current jailbreaking techniques, enhancing the understanding of LLM security risks.

Abstract: Large Language Models (LLMs) have achieved exceptional performance across a
wide range of tasks. However, they still pose significant safety risks due to
the potential misuse for malicious purposes. Jailbreaks, which aim to elicit
models to generate harmful content, play a critical role in identifying the
underlying security threats. Recent jailbreaking primarily focuses on
single-turn scenarios, while the more complicated multi-turn scenarios remain
underexplored. Moreover, existing multi-turn jailbreaking techniques struggle
to adapt to the evolving dynamics of dialogue as the interaction progresses. To
address this limitation, we propose a novel multi-turn jailbreaking method that
refines the jailbreaking path globally at each interaction. We also actively
fabricate model responses to suppress safety-related warnings, thereby
increasing the likelihood of eliciting harmful outputs in subsequent questions.
Experimental results demonstrate the superior performance of our method
compared with existing single-turn and multi-turn jailbreaking techniques
across six state-of-the-art LLMs. Our code is publicly available at
https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.

</details>


### [41] [Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation](https://arxiv.org/pdf/2506.17949)
*Hong Su*

Main category: cs.CL

TL;DR: The paper introduces an innovation scatter model to help LLMs generalize and apply localized innovations across structurally similar stages in multi-stage processes.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to generalize novel ideas beyond their original context, limiting their ability to reuse innovations in other parts of multi-stage processes.

Method: A four-step innovation scatter model: (1) identify core innovation, (2) generalize it, (3) assess broader applicability, and (4) apply it to similar stages.

Result: The model improves LLMs' ability to extend innovations across stages, enhancing generalization and reuse.

Conclusion: The innovation scatter model effectively addresses the challenge of applying localized innovations in LLMs, improving their versatility.

Abstract: Large Language Models (LLMs) exhibit strong capabilities in reproducing and
extending patterns observed during pretraining but often struggle to generalize
novel ideas beyond their original context. This paper addresses the challenge
of applying such localized innovations - introduced at a specific stage or
component - to other parts of a multi-stage process. We propose a scatter-based
innovation expansion model (innovation scatter model) that guides the LLM
through a four-step process: (1) identifying the core innovation by comparing
the user's input with its surrounding context, (2) generalizing the innovation
by removing references to specific stages or components, (3) determining
whether the generalized innovation applies to a broader scope beyond the
original stage, and (4) systematically applying it to other structurally
similar stages using the LLM. This model leverages structural redundancy across
stages to improve the applicability of novel ideas. Verification results
demonstrate that the innovation scatter model enables LLMs to extend
innovations across structurally similar stages, thereby enhancing
generalization and reuse.

</details>


### [42] [A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment](https://arxiv.org/pdf/2506.17951)
*Quanwei Tang, Sophia Yat Mei Lee, Junshuang Wu, Dong Zhang, Shoushan Li, Erik Cambria, Guodong Zhou*

Main category: cs.CL

TL;DR: GraphMPA is a graph-based framework for retrieval-augmented generation, improving global understanding and aligning responses with human preferences through hierarchical document graphs and mode-seeking optimization.


<details>
  <summary>Details</summary>
Motivation: Address challenges in global understanding and ethical alignment in retrieval-augmented generation for question answering.

Method: Constructs hierarchical document graphs using similarity measurements and introduces mode-seeking preference optimization for probability-matching constraints.

Result: Demonstrated effectiveness on six datasets.

Conclusion: GraphMPA enhances retrieval-augmented generation by aligning outputs with human preferences and improving understanding.

Abstract: Recent advancements in retrieval-augmented generation (RAG) have enhanced
large language models in question answering by integrating external knowledge.
However, challenges persist in achieving global understanding and aligning
responses with human ethical and quality preferences. To address these issues,
we propose GraphMPA, a comprehensive graph-based framework with mode-seeking
preference alignment. Our approach constructs a hierarchical document graph
using a general similarity measurement, mimicking human cognitive processes for
information understanding and synthesis. Additionally, we introduce
mode-seeking preference optimization to better align model outputs with human
preferences through probability-matching constraints. Extensive experiments on
six datasets demonstrate the effectiveness of our
\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.

</details>


### [43] [PDF Retrieval Augmented Question Answering](https://arxiv.org/pdf/2506.18027)
*Thi Thu Uyen Hoang, Viet Anh Nguyen*

Main category: cs.CL

TL;DR: The paper introduces a Retrieval Augmented Generation (RAG) framework for QA systems to handle multimodal data in PDFs, improving accuracy and relevance in answers.


<details>
  <summary>Details</summary>
Motivation: Existing QA systems struggle with multimodal data in PDFs (text, images, graphs, etc.), prompting the need for a more comprehensive solution.

Method: The approach refines processing and integration of non-textual elements in PDFs into RAG and fine-tunes large language models for better adaptation.

Result: Experimental evaluation shows the system's ability to extract accurate information from diverse PDF content.

Conclusion: The work advances retrieval-augmented QA systems and sets a foundation for future multimodal data research.

Abstract: This paper presents an advancement in Question-Answering (QA) systems using a
Retrieval Augmented Generation (RAG) framework to enhance information
extraction from PDF files. Recognizing the richness and diversity of data
within PDFs--including text, images, vector diagrams, graphs, and tables--poses
unique challenges for existing QA systems primarily designed for textual
content. We seek to develop a comprehensive RAG-based QA system that will
effectively address complex multimodal questions, where several data types are
combined in the query. This is mainly achieved by refining approaches to
processing and integrating non-textual elements in PDFs into the RAG framework
to derive precise and relevant answers, as well as fine-tuning large language
models to better adapt to our system. We provide an in-depth experimental
evaluation of our solution, demonstrating its capability to extract accurate
information that can be applied to different types of content across PDFs. This
work not only pushes the boundaries of retrieval-augmented QA systems but also
lays a foundation for further research in multimodal data integration and
processing.

</details>


### [44] [Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models](https://arxiv.org/pdf/2506.18036)
*Aziz Amari, Mohamed Achref Ben Ammar*

Main category: cs.CL

TL;DR: A hybrid summarization method combining extractive and abstractive techniques to address challenges in retaining key information in lengthy documents.


<details>
  <summary>Details</summary>
Motivation: The need for effective automatic text summarization due to the rapid expansion of information, with current methods facing issues like resource intensity and 'lost in the middle' in lengthy documents.

Method: Splits documents into chunks, clusters their embeddings, summarizes each cluster, and constructs the final summary using a Markov chain graph for semantic order.

Result: Proposes a hybrid approach to improve summarization by leveraging both extractive and abstractive techniques.

Conclusion: The hybrid method addresses limitations of current summarization techniques, offering a more effective solution for lengthy documents.

Abstract: The rapid expansion of information from diverse sources has heightened the
need for effective automatic text summarization, which condenses documents into
shorter, coherent texts. Summarization methods generally fall into two
categories: extractive, which selects key segments from the original text, and
abstractive, which generates summaries by rephrasing the content coherently.
Large language models have advanced the field of abstractive summarization, but
they are resourceintensive and face significant challenges in retaining key
information across lengthy documents, which we call being "lost in the middle".
To address these issues, we propose a hybrid summarization approach that
combines extractive and abstractive techniques. Our method splits the document
into smaller text chunks, clusters their vector embeddings, generates a summary
for each cluster that represents a key idea in the document, and constructs the
final summary by relying on a Markov chain graph when selecting the semantic
order of ideas.

</details>


### [45] [Statistical Multicriteria Evaluation of LLM-Generated Text](https://arxiv.org/pdf/2506.18082)
*Esteban Garces Arias, Hannah Blocher, Julian Rodemann, Matthias Aenmacher, Christoph Jansen*

Main category: cs.CL

TL;DR: The paper proposes a framework using Generalized Stochastic Dominance (GSD) to evaluate LLM-generated text quality, addressing limitations of current methods like single-metric reliance and lack of statistical guarantees.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for LLM-generated text are inadequate, failing to capture nuanced trade-offs between quality indicators like coherence and diversity.

Method: Adapts the GSD framework for statistical inference to evaluate multiple quality dimensions simultaneously, avoiding arbitrary metric weighting.

Result: Demonstrates the framework's ability to identify statistically significant performance differences in decoding strategies compared to human text.

Conclusion: The GSD-front approach provides a robust, multi-dimensional evaluation method for LLM-generated text with inferential guarantees.

Abstract: Assessing the quality of LLM-generated text remains a fundamental challenge
in natural language processing. Current evaluation approaches often rely on
isolated metrics or simplistic aggregations that fail to capture the nuanced
trade-offs between coherence, diversity, fluency, and other relevant indicators
of text quality. In this work, we adapt a recently proposed framework for
statistical inference based on Generalized Stochastic Dominance (GSD) that
addresses three critical limitations in existing benchmarking methodologies:
the inadequacy of single-metric evaluation, the incompatibility between
cardinal automatic metrics and ordinal human judgments, and the lack of
inferential statistical guarantees. The GSD-front approach enables simultaneous
evaluation across multiple quality dimensions while respecting their different
measurement scales, building upon partial orders of decoding strategies, thus
avoiding arbitrary weighting of the involved metrics. By applying this
framework to evaluate common decoding strategies against human-generated text,
we demonstrate its ability to identify statistically significant performance
differences while accounting for potential deviations from the i.i.d.
assumption of the sampling design.

</details>


### [46] [Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution](https://arxiv.org/pdf/2506.18091)
*Patrik Stano, Ale Hork*

Main category: cs.CL

TL;DR: The paper compares prompt engineering with LLMs and fine-tuning compact models for Czech anaphora resolution, finding fine-tuned models like mT5-large superior in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Anaphora resolution is crucial for understanding morphologically rich languages like Czech, but modern methods need evaluation.

Method: Comparative evaluation of prompt engineering (using LLMs like Mistral Large 2 and Llama 3) and fine-tuning (mT5 and Mistral models) on Czech text from the Prague Dependency Treebank.

Result: Fine-tuned models (e.g., mT5-large) achieved 88% accuracy, outperforming prompt engineering (74.5%) and requiring fewer resources.

Conclusion: Fine-tuning compact models is more effective for Czech anaphora resolution than prompting LLMs, offering better accuracy and efficiency.

Abstract: Anaphora resolution plays a critical role in natural language understanding,
especially in morphologically rich languages like Czech. This paper presents a
comparative evaluation of two modern approaches to anaphora resolution on Czech
text: prompt engineering with large language models (LLMs) and fine-tuning
compact generative models. Using a dataset derived from the Prague Dependency
Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2
and Llama 3, using a series of prompt templates. We compare them against
fine-tuned variants of the mT5 and Mistral models that we trained specifically
for Czech anaphora resolution. Our experiments demonstrate that while prompting
yields promising few-shot results (up to 74.5% accuracy), the fine-tuned
models, particularly mT5-large, outperform them significantly, achieving up to
88% accuracy while requiring fewer computational resources. We analyze
performance across different anaphora types, antecedent distances, and source
corpora, highlighting key strengths and trade-offs of each approach.

</details>


### [47] [InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating](https://arxiv.org/pdf/2506.18102)
*Fuyu Wang, Jiangtong Li, Kun Zhu, Changjun Jiang*

Main category: cs.CL

TL;DR: The paper introduces a dual-component framework, InspireScore and InspireDebate, to enhance LLM-based debating systems by addressing gaps in objective assessment and structured optimization.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based debating systems lack objective assessments (e.g., authenticity, logical validity) and structured optimization across evaluation metrics, CoT reasoning, and multi-turn debate refinement.

Method: Proposes InspireScore for multi-dimensional assessment (subjective and objective metrics) and InspireDebate for phased optimization (CoT enhancement, DPO, Web-RAG).

Result: InspireScore achieves 44% higher correlation with expert judgments; InspireDebate outperforms baselines by 57%.

Conclusion: The framework effectively addresses current limitations, improving both evaluation and debate performance in LLM-based systems.

Abstract: With the rapid advancements in large language models (LLMs), debating tasks,
such as argument quality assessment and debate process simulation, have made
significant progress. However, existing LLM-based debating systems focus on
responding to specific arguments while neglecting objective assessments such as
authenticity and logical validity. Furthermore, these systems lack a structured
approach to optimize across various dimensions$-$including evaluation metrics,
chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby
limiting their effectiveness. To address these interconnected challenges, we
propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel
evaluation system that establishes a multi-dimensional assessment architecture
incorporating four subjective criteria (emotional appeal, argument clarity,
argument arrangement, and topic relevance) alongside two objective metrics
(fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an
optimized debating framework employing a phased optimization approach through
CoT reasoning enhancement, multi-dimensional Direct Preference Optimization
(DPO), and real-time knowledge grounding via web-based Retrieval Augmented
Generation (Web-RAG). Empirical evaluations demonstrate that
$\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert
judgments compared to existing methods, while $\textbf{InspireDebate}$ shows
significant improvements, outperforming baseline models by 57$\%$. Source code
is available at https://github.com/fywang12/InspireDebate.

</details>


### [48] [Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use](https://arxiv.org/pdf/2506.18105)
*Yicheng Fu, Zhemin Huang, Liuxin Yang, Yumeng Lu, Zhongdongming Dai*

Main category: cs.CL

TL;DR: Chengyu-Bench is a new benchmark for evaluating language models on Chinese idioms (Chengyu), featuring three tasks: Evaluative Connotation, Appropriateness, and Open Cloze. Results show models excel in sentiment classification but struggle with nuanced usage.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for Chinese idioms are limited in scope, lacking comprehensive evaluation of idiom understanding and usage. Chengyu-Bench addresses this gap.

Method: The benchmark includes 2,937 human-verified examples across three tasks: sentiment classification, detecting incorrect usage, and open cloze completion.

Result: LLMs achieve 95% accuracy on sentiment classification but only ~85% on appropriateness and ~40% on open cloze, revealing gaps in understanding idiom nuances.

Conclusion: Chengyu-Bench highlights LLMs' limitations in grasping cultural and contextual nuances of idioms, despite strong performance on simpler tasks.

Abstract: Chinese idioms (Chengyu) are concise four-character expressions steeped in
history and culture, whose literal translations often fail to capture their
full meaning. This complexity makes them challenging for language models to
interpret and use correctly. Existing benchmarks focus on narrow tasks -
multiple-choice cloze tests, isolated translation, or simple paraphrasing. We
introduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)
Evaluative Connotation, classifying idioms as positive or negative; (2)
Appropriateness, detecting incorrect idiom usage in context; and (3) Open
Cloze, filling blanks in longer passages without options. Chengyu-Bench
comprises 2,937 human-verified examples covering 1,765 common idioms sourced
from diverse corpora. We evaluate leading LLMs and find they achieve over 95%
accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%
top-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise
from fundamental misunderstandings of idiom meanings. Chengyu-Bench
demonstrates that while LLMs can reliably gauge idiom sentiment, they still
struggle to grasp the cultural and contextual nuances essential for proper
usage. The benchmark and source code are available at:
https://github.com/sofyc/ChengyuBench.

</details>


### [49] [Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives](https://arxiv.org/pdf/2506.18116)
*Batool Haider, Atmika Gorti, Aman Chadha, Manas Gaur*

Main category: cs.CL

TL;DR: The paper introduces a multi-hop question answering (MHQA) framework to detect intersectional biases in LLMs' mental health responses, revealing systematic disparities and proposing debiasing techniques.


<details>
  <summary>Details</summary>
Motivation: To address the risk of LLMs propagating biases in mental healthcare, reinforcing stigma and harming marginalized groups, by developing systematic methods for bias detection.

Method: A MHQA framework is used to analyze LLM responses from the IMHI dataset, tagging for demographics and evaluating four models (Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, Llama 4). Two debiasing techniques (Roleplay Simulation and Explicit Bias Reduction) are tested.

Result: Systematic disparities in sentiment, demographics, and mental health conditions were found. MHQA outperformed conventional methods, and debiasing techniques achieved 66-94% bias reduction.

Conclusion: The study highlights LLM biases in mental healthcare and offers actionable debiasing strategies for equitable AI development.

Abstract: Large Language Models (LLMs) in mental healthcare risk propagating biases
that reinforce stigma and harm marginalized groups. While previous research
identified concerning trends, systematic methods for detecting intersectional
biases remain limited. This work introduces a multi-hop question answering
(MHQA) framework to explore LLM response biases in mental health discourse. We
analyze content from the Interpretable Mental Health Instruction (IMHI) dataset
across symptom presentation, coping mechanisms, and treatment approaches. Using
systematic tagging across age, race, gender, and socioeconomic status, we
investigate bias patterns at demographic intersections. We evaluate four LLMs:
Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic
disparities across sentiment, demographics, and mental health conditions. Our
MHQA approach demonstrates superior detection compared to conventional methods,
identifying amplification points where biases magnify through sequential
reasoning. We implement two debiasing techniques: Roleplay Simulation and
Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot
prompting with BBQ dataset examples. These findings highlight critical areas
where LLMs reproduce mental healthcare biases, providing actionable insights
for equitable AI development.

</details>


### [50] [The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English](https://arxiv.org/pdf/2506.18120)
*Tom S Juzek*

Main category: cs.CL

TL;DR: The paper introduces the Syntactic Acceptability Dataset, a resource for syntax and computational linguistics research, with 1,000 labeled English sequences. Preliminary analyses show 83% convergence between grammaticality and acceptability, and machine learning models perform better at predicting acceptability than grammaticality.


<details>
  <summary>Details</summary>
Motivation: To create a publicly accessible dataset for syntax and computational linguistics research, addressing gaps in labeled syntactic data.

Method: The dataset includes 1,000 English sequences from textbooks and Linguistic Inquiry, labeled for grammaticality (from literature) and acceptability (via crowdsourcing). Preliminary analyses compare grammaticality and acceptability and evaluate machine learning models.

Result: 83% convergence between grammaticality and acceptability; machine learning models perform better at predicting acceptability than grammaticality.

Conclusion: The dataset is a valuable resource, with future work aimed at expansion and further analysis.

Abstract: We present a preview of the Syntactic Acceptability Dataset, a resource being
designed for both syntax and computational linguistics research. In its current
form, the dataset comprises 1,000 English sequences from the syntactic
discourse: Half from textbooks and half from the journal Linguistic Inquiry,
the latter to ensure a representation of the contemporary discourse. Each entry
is labeled with its grammatical status ("well-formedness" according to
syntactic formalisms) extracted from the literature, as well as its
acceptability status ("intuitive goodness" as determined by native speakers)
obtained through crowdsourcing, with highest experimental standards. Even in
its preliminary form, this dataset stands as the largest of its kind that is
publicly accessible. We also offer preliminary analyses addressing three
debates in linguistics and computational linguistics: We observe that
grammaticality and acceptability judgments converge in about 83% of the cases
and that "in-betweenness" occurs frequently. This corroborates existing
research. We also find that while machine learning models struggle with
predicting grammaticality, they perform considerably better in predicting
acceptability. This is a novel finding. Future work will focus on expanding the
dataset.

</details>


### [51] [$^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models](https://arxiv.org/pdf/2506.18129)
*Bugra Kilictas, Faruk Alpay*

Main category: cs.CL

TL;DR: The paper identifies a vulnerability in transformer models where em dash tokens cause semantic drift and proposes a solution combining symbolic purification and embedding realignment to mitigate the issue without retraining.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the problem of recursive semantic drift and clause boundary hallucination in autoregressive transformer models, which affects long-form generation quality.

Method: The authors use formal analysis of token-level perturbations and propose a solution involving symbolic clause purification (phi-infinity operator) and targeted embedding matrix realignment.

Result: Experimental results show significant improvements in generation consistency and topic maintenance, validating the proposed solution.

Conclusion: The work provides a framework for identifying and mitigating token-level vulnerabilities in foundation models, with broader implications for AI safety and robust deployment.

Abstract: We identify a critical vulnerability in autoregressive transformer language
models where the em dash token induces recursive semantic drift, leading to
clause boundary hallucination and embedding space entanglement. Through formal
analysis of token-level perturbations in semantic lattices, we demonstrate that
em dash insertion fundamentally alters the model's latent representations,
causing compounding errors in long-form generation. We propose a novel solution
combining symbolic clause purification via the phi-infinity operator with
targeted embedding matrix realignment. Our approach enables total suppression
of problematic tokens without requiring model retraining, while preserving
semantic coherence through fixed-point convergence guarantees. Experimental
validation shows significant improvements in generation consistency and topic
maintenance. This work establishes a general framework for identifying and
mitigating token-level vulnerabilities in foundation models, with immediate
implications for AI safety, model alignment, and robust deployment of large
language models in production environments. The methodology extends beyond
punctuation to address broader classes of recursive instabilities in neural
text generation systems.

</details>


### [52] [Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](https://arxiv.org/pdf/2506.18141)
*Ruixuan Deng, Xiaoyang Hu, Miles Gilberti, Shane Storks, Aman Taxali, Mike Angstadt, Chandra Sripada, Joyce Chai*

Main category: cs.CL

TL;DR: The paper identifies and manipulates semantic components in LLMs using sparse autoencoder features, showing predictable changes in outputs and revealing modular knowledge organization.


<details>
  <summary>Details</summary>
Motivation: To understand and manipulate the modular organization of knowledge in large language models (LLMs) efficiently.

Method: Uses coactivation of sparse autoencoder (SAE) features from few prompts, focusing on country-relation tasks, and ablates/amplifies components to observe effects.

Result: Ablating or amplifying semantic components changes outputs predictably; relation components are concentrated in later layers and have stronger causal impact.

Conclusion: LLMs exhibit modular knowledge organization, and targeted manipulation methods are effective.

Abstract: We identify semantically coherent, context-consistent network components in
large language models (LLMs) using coactivation of sparse autoencoder (SAE)
features collected from just a handful of prompts. Focusing on country-relation
tasks, we show that ablating semantic components for countries and relations
changes model outputs in predictable ways, while amplifying these components
induces counterfactual responses. Notably, composing relation and country
components yields compound counterfactual outputs. We find that, whereas most
country components emerge from the very first layer, the more abstract relation
components are concentrated in later layers. Furthermore, within relation
components themselves, nodes from later layers tend to have a stronger causal
impact on model outputs. Overall, these findings suggest a modular organization
of knowledge within LLMs and advance methods for efficient, targeted model
manipulation.

</details>


### [53] [QuranMorph: Morphologically Annotated Quranic Corpus](https://arxiv.org/pdf/2506.18148)
*Diyam Akra, Tymaa Hammouda, Mustafa Jarrar*

Main category: cs.CL

TL;DR: The QuranMorph corpus is a manually annotated morphological corpus for the Quran, featuring lemmatization and POS tagging by experts, linked with extensive linguistic resources.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality, open-source morphological corpus for the Quran, enabling integration with other linguistic tools and resources.

Method: Manual lemmatization and POS tagging by three linguists, using the Qabas lexicographic database and the SAMA/Qabas tagset (40 tags).

Result: A publicly available corpus (77,429 tokens) inter-linked with multiple linguistic resources.

Conclusion: The QuranMorph corpus provides a valuable, open-source resource for Quranic studies and linguistic research.

Abstract: We present the QuranMorph corpus, a morphologically annotated corpus for the
Quran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and
tagged with its part-of-speech by three expert linguists. The lemmatization
process utilized lemmas from Qabas, an Arabic lexicographic database linked
with 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging
was performed using the fine-grained SAMA/Qabas tagset, which encompasses 40
tags. As shown in this paper, this rich lemmatization and POS tagset enabled
the QuranMorph corpus to be inter-linked with many linguistic resources. The
corpus is open-source and publicly available as part of the SinaLab resources
at (https://sina.birzeit.edu/quran)

</details>


### [54] [CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers](https://arxiv.org/pdf/2506.18185)
*Zihan Liang, Ziwen Pan, Sumon Kanti Dey, Azra Ismail*

Main category: cs.CL

TL;DR: The paper describes a system for SMM4H-HeaRD 2025 tasks, excelling in Task 5 Subtask 1 with an F1 score of 0.958 using RoBERTa and GPT-4.


<details>
  <summary>Details</summary>
Motivation: To address challenges in detecting insomnia in clinical notes (Task 4) and extracting food safety events from news (Task 5).

Method: Used encoder-based models (e.g., RoBERTa) and GPT-4 for data augmentation, with preprocessing and subtask-specific adaptations.

Result: Achieved top performance in Task 5 Subtask 1 (F1 score: 0.958).

Conclusion: The approach demonstrates effectiveness, particularly in Task 5, leveraging advanced NLP techniques.

Abstract: This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,
specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).
Task 4 focused on detecting mentions of insomnia in clinical notes, while Task
5 addressed the extraction of food safety events from news articles. We
participated in all subtasks and report key findings across them, with
particular emphasis on Task 5 Subtask 1, where our system achieved strong
performance-securing first place with an F1 score of 0.958 on the test set. To
attain this result, we employed encoder-based models (e.g., RoBERTa), alongside
GPT-4 for data augmentation. This paper outlines our approach, including
preprocessing, model architecture, and subtask-specific adaptations

</details>


### [55] [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/pdf/2506.18199)
*Bushra Asseri, Estabrag Abdelaziz, Areej Al-Wabil*

Main category: cs.CL

TL;DR: This systematic review explores prompt engineering strategies to mitigate cultural bias in large language models (LLMs) against Arabs and Muslims, identifying five effective approaches with varying success.


<details>
  <summary>Details</summary>
Motivation: Addressing the understudied issue of cultural bias in LLMs, particularly towards Arabs and Muslims, to reduce harmful stereotypes and marginalization.

Method: A mixed-methods systematic review following PRISMA guidelines and Kitchenham's methodology, analyzing 8 empirical studies (2021-2024) on bias mitigation strategies.

Result: Five prompt engineering approaches were identified: cultural prompting, affective priming, self-debiasing, structured multi-step pipelines, and parameter-optimized continuous prompts. Structured pipelines showed the highest effectiveness (up to 87.7% bias reduction), while cultural prompting was more accessible.

Conclusion: Prompt engineering is accessible for bias mitigation but requires further research, including culturally adaptive techniques and Arab/Muslim-specific evaluation resources, alongside integrating complementary debiasing methods.

Abstract: Large language models have demonstrated remarkable capabilities across
various domains, yet concerns about cultural bias - particularly towards Arabs
and Muslims - pose significant ethical challenges by perpetuating harmful
stereotypes and marginalization. Despite growing recognition of bias in LLMs,
prompt engineering strategies specifically addressing Arab and Muslim
representation remain understudied. This mixed-methods systematic review
examines such techniques, offering evidence-based guidance for researchers and
practitioners. Following PRISMA guidelines and Kitchenham's systematic review
methodology, we analyzed 8 empirical studies published between 2021-2024
investigating bias mitigation strategies. Our findings reveal five primary
prompt engineering approaches: cultural prompting, affective priming,
self-debiasing techniques, structured multi-step pipelines, and
parameter-optimized continuous prompts. Although all approaches show potential
for reducing bias, effectiveness varied substantially across studies and bias
types. Evidence suggests that certain bias types may be more resistant to
prompt-based mitigation than others. Structured multi-step pipelines
demonstrated the highest overall effectiveness, achieving up to 87.7% reduction
in bias, though they require greater technical expertise. Cultural prompting
offers broader accessibility with substantial effectiveness. These results
underscore the accessibility of prompt engineering for mitigating cultural bias
without requiring access to model parameters. The limited number of studies
identified highlights a significant research gap in this critical area. Future
research should focus on developing culturally adaptive prompting techniques,
creating Arab and Muslim-specific evaluation resources, and integrating prompt
engineering with complementary debiasing methods to address deeper stereotypes
while maintaining model utility.

</details>


### [56] [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/pdf/2506.18201)
*Bushra Asseri, Estabraq Abdelaziz, Maha Al Mogren, Tayef Alhefdhi, Areej Al-Wabil*

Main category: cs.CL

TL;DR: GPT-4o outperforms Gemini 1.5 Pro in recognizing emotions in Arabic children's storybook illustrations, but both models struggle with cultural nuances and ambiguous contexts.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve emotion recognition in multimodal AI systems for culturally responsive educational technologies in Arabic contexts.

Method: Assessed GPT-4o and Gemini 1.5 Pro using three prompting strategies (zero-shot, few-shot, chain-of-thought) on 75 Arabic storybook images, comparing results with human annotations based on Plutchik's framework.

Result: GPT-4o achieved a 59% macro F1-score with chain-of-thought prompting, outperforming Gemini's 43%. Both models had systematic errors, especially with valence inversions and cultural nuances.

Conclusion: Current models lack cultural understanding, highlighting the need for culturally sensitive training to develop effective emotion-aware educational tools for Arabic learners.

Abstract: Emotion recognition capabilities in multimodal AI systems are crucial for
developing culturally responsive educational technologies, yet remain
underexplored for Arabic language contexts where culturally appropriate
learning tools are critically needed. This study evaluates the emotion
recognition performance of two advanced multimodal large language models,
GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook
illustrations. We assessed both models across three prompting strategies
(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic
storybooks, comparing model predictions with human annotations based on
Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across
all conditions, achieving the highest macro F1-score of 59% with
chain-of-thought prompting compared to Gemini's best performance of 43%. Error
analysis revealed systematic misclassification patterns, with valence
inversions accounting for 60.7% of errors, while both models struggled with
culturally nuanced emotions and ambiguous narrative contexts. These findings
highlight fundamental limitations in current models' cultural understanding and
emphasize the need for culturally sensitive training approaches to develop
effective emotion-aware educational technologies for Arabic-speaking learners.

</details>


### [57] [Enhancing Entity Aware Machine Translation with Multi-task Learning](https://arxiv.org/pdf/2506.18318)
*An Trieu, Phuong Nguyen, Minh Le Nguyen*

Main category: cs.CL

TL;DR: The paper proposes a multi-task learning method to improve Entity-aware machine translation (EAMT) by optimizing entity recognition and machine translation subtasks.


<details>
  <summary>Details</summary>
Motivation: EAMT is challenging due to limited entity-related translation data and contextual complexity.

Method: Multi-task learning is applied to optimize entity recognition and machine translation.

Result: Performance is evaluated on the SemEval 2025 Task 2 dataset.

Conclusion: The method enhances EAMT performance by jointly optimizing subtasks.

Abstract: Entity-aware machine translation (EAMT) is a complicated task in natural
language processing due to not only the shortage of translation data related to
the entities needed to translate but also the complexity in the context needed
to process while translating those entities. In this paper, we propose a method
that applies multi-task learning to optimize the performance of the two
subtasks named entity recognition and machine translation, which improves the
final performance of the Entity-aware machine translation task. The result and
analysis are performed on the dataset provided by the organizer of Task 2 of
the SemEval 2025 competition.

</details>


### [58] [TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance](https://arxiv.org/pdf/2506.18337)
*Syed Mekael Wasti, Shou-Yi Hung, Christopher Collins, En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: TranslationCorrect is an integrated framework for MT post-editing and research data collection, combining MT generation, error prediction, and post-editing in one tool. It improves efficiency and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in MT post-editing and research data workflows by integrating tasks into a single, user-friendly environment.

Method: Combines MT generation (e.g., NLLB), automated error prediction (e.g., XCOMET or LLM APIs), and an intuitive post-editing interface, designed with HCI principles.

Result: Significantly improves translation efficiency and user satisfaction, confirmed by a user study.

Conclusion: TranslationCorrect streamlines workflows for translators and researchers, offering high-quality annotations and compatibility with error detection models.

Abstract: Machine translation (MT) post-editing and research data collection often rely
on inefficient, disconnected workflows. We introduce TranslationCorrect, an
integrated framework designed to streamline these tasks. TranslationCorrect
combines MT generation using models like NLLB, automated error prediction using
models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive
post-editing interface within a single environment. Built with human-computer
interaction (HCI) principles in mind to minimize cognitive load, as confirmed
by a user study. For translators, it enables them to correct errors and batch
translate efficiently. For researchers, TranslationCorrect exports high-quality
span-based annotations in the Error Span Annotation (ESA) format, using an
error taxonomy inspired by Multidimensional Quality Metrics (MQM). These
outputs are compatible with state-of-the-art error detection models and
suitable for training MT or post-editing systems. Our user study confirms that
TranslationCorrect significantly improves translation efficiency and user
satisfaction over traditional annotation methods.

</details>


### [59] [Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs](https://arxiv.org/pdf/2506.18341)
*Kang Chen, Mengdi Zhang, Yixin Cao*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper explores the challenges of test-time scaling of large language
models (LLMs), regarding both the data and inference efficiency. We highlight
the diversity of multi-lingual reasoning based on our pilot studies, and then
introduce a novel approach, \(L^2\) multi-lingual unification learning with a
decoding intervention strategy for further investigation. The basic idea of
\(L^2\) is that the reasoning process varies across different languages, which
may be mutually beneficial to enhance both model performance and efficiency. In
specific, there are two types of multi-lingual data: the entire long
chain-of-thought annotations in different languages and the step-wise mixture
of languages. By further tuning based on them, we show that even small amounts
of data can significantly improve reasoning capabilities. Our findings suggest
that multilingual learning reduces both the required data and the number of
inference tokens while maintaining a comparable performance. Furthermore,
\(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize
the importance of diverse data selection. The \(L^2\) method offers a promising
solution to the challenges of data collection and test-time compute efficiency
in LLMs.

</details>


### [60] [Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics](https://arxiv.org/pdf/2506.18387)
*Yousang Cho, Key-Sun Choi*

Main category: cs.CL

TL;DR: The study compares six metrics for evaluating causal explanations in diagnostic reports, finding GPT-Black and GPT-White align best with expert assessments, while similarity-based metrics perform poorly.


<details>
  <summary>Details</summary>
Motivation: To determine which evaluation metrics best capture the quality of causal explanations in diagnostic reports, ensuring clinically valid and coherent narratives.

Method: Six metrics (BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, expert assessment) are compared across two input types (observation-based, multiple-choice-based) with two weighting strategies (task-specific, equal weights).

Result: GPT-Black shows the strongest discriminative power for coherent and valid narratives, GPT-White aligns well with experts, and similarity-based metrics diverge from clinical reasoning quality.

Conclusion: Metric selection and weighting significantly impact evaluation outcomes, supporting LLM-based metrics for interpretability and causal reasoning tasks.

Abstract: This study investigates how accurately different evaluation metrics capture
the quality of causal explanations in automatically generated diagnostic
reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,
GPT-White, GPT-Black, and expert qualitative assessment across two input types:
observation-based and multiple-choice-based report generation. Two weighting
strategies are applied: one reflecting task-specific priorities, and the other
assigning equal weights to all metrics. Our results show that GPT-Black
demonstrates the strongest discriminative power in identifying logically
coherent and clinically valid causal narratives. GPT-White also aligns well
with expert evaluations, while similarity-based metrics diverge from clinical
reasoning quality. These findings emphasize the impact of metric selection and
weighting on evaluation outcomes, supporting the use of LLM-based evaluation
for tasks requiring interpretability and causal reasoning.

</details>


### [61] [Lemmatization as a Classification Task: Results from Arabic across Multiple Genres](https://arxiv.org/pdf/2506.18399)
*Mostafa Saeed, Nizar Habash*

Main category: cs.CL

TL;DR: The paper introduces two novel lemmatization approaches for Arabic using classification into a Lemma-POS-Gloss tagset, leveraging machine translation and semantic clustering. It also presents a new test set and evaluates sequence-to-sequence models, showing classification and clustering methods outperform them.


<details>
  <summary>Details</summary>
Motivation: Existing Arabic lemmatization tools struggle with inconsistent standards and limited genre coverage, necessitating more robust and interpretable solutions.

Method: Two approaches frame lemmatization as classification into a Lemma-POS-Gloss (LPG) tagset, using machine translation and semantic clustering. A new test set is introduced, and sequence-to-sequence models are evaluated.

Result: Classification and clustering methods outperform sequence-to-sequence models, providing more robust and interpretable outputs. The new test set standardizes evaluation.

Conclusion: The proposed classification and clustering approaches set new benchmarks for Arabic lemmatization, offering better robustness and interpretability compared to sequence-to-sequence models.

Abstract: Lemmatization is crucial for NLP tasks in morphologically rich languages with
ambiguous orthography like Arabic, but existing tools face challenges due to
inconsistent standards and limited genre coverage. This paper introduces two
novel approaches that frame lemmatization as classification into a
Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic
clustering. We also present a new Arabic lemmatization test set covering
diverse genres, standardized alongside existing datasets. We evaluate character
level sequence-to-sequence models, which perform competitively and offer
complementary value, but are limited to lemma prediction (not LPG) and prone to
hallucinating implausible forms. Our results show that classification and
clustering yield more robust, interpretable outputs, setting new benchmarks for
Arabic lemmatization.

</details>


### [62] [TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models](https://arxiv.org/pdf/2506.18421)
*Ce Li, Xiaofan Liu, Zhiyan Song, Ce Chi, Chen Zhao, Jingjing Yang, Zhendong Wang, Kexin Yang, Boshen Shi, Xing Wang, Chao Deng, Junlan Feng*

Main category: cs.CL

TL;DR: The paper introduces TReB, a benchmark for evaluating LLMs on table reasoning tasks, covering 26 sub-tasks, and provides a dataset and framework for robust assessment.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack effectiveness in evaluating LLMs' broad table reasoning abilities due to hidden semantics and complexity of table-structured data.

Method: The authors create TReB, a benchmark with 26 sub-tasks, a high-quality dataset, and an evaluation framework with three inference modes (TCoT, PoT, ICoT).

Result: Evaluation of 20+ LLMs shows significant room for improvement in handling complex table-related tasks.

Conclusion: TReB effectively measures table reasoning capabilities, and the dataset and framework are publicly available for further research.

Abstract: The majority of data in businesses and industries is stored in tables,
databases, and data warehouses. Reasoning with table-structured data poses
significant challenges for large language models (LLMs) due to its hidden
semantics, inherent complexity, and structured nature. One of these challenges
is lacking an effective evaluation benchmark fairly reflecting the performances
of LLMs on broad table reasoning abilities. In this paper, we fill in this gap,
presenting a comprehensive table reasoning evolution benchmark, TReB, which
measures both shallow table understanding abilities and deep table reasoning
abilities, a total of 26 sub-tasks. We construct a high quality dataset through
an iterative data processing procedure. We create an evaluation framework to
robustly measure table reasoning capabilities with three distinct inference
modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs
using this frame work and prove its effectiveness. Experimental results reveal
that existing LLMs still have significant room for improvement in addressing
the complex and real world Table related tasks. Both the dataset and evaluation
framework are publicly available, with the dataset hosted on [HuggingFace] and
the framework on [GitHub].

</details>


### [63] [MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models](https://arxiv.org/pdf/2506.18485)
*Junjie Zhang, Guozheng Ma, Shunyu Liu, Haoyu Wang, Jiaxing Huang, Ting-En Lin, Fei Huang, Yongbin Li, Dacheng Tao*

Main category: cs.CL

TL;DR: MeRF combines reinforcement learning with in-context learning in LLMs by embedding reward rules in prompts, improving reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods ignore LLMs' in-context learning ability, which is crucial for reasoning tasks like CoT prompting.

Method: MeRF integrates reward specifications into prompts to align generation with optimization, leveraging in-context learning.

Result: MeRF outperforms baselines on the K&K benchmark and adapts to misleading motivations via reinforcement learning.

Conclusion: MeRF effectively combines reinforcement learning and in-context learning, enhancing LLM reasoning capabilities.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle
complex reasoning tasks. However, existing RLVR methods overlook one of the
most distinctive capabilities of LLMs, their in-context learning ability, as
prominently demonstrated by the success of Chain-of-Thought (CoT) prompting.
This motivates us to explore how reinforcement learning can be effectively
combined with in-context learning to better improve the reasoning capabilities
of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement
Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement
learning of LLMs by involving ``telling LLMs the rules of the game''.
Specifically, MeRF directly injects the reward specification into the prompt,
which serves as an in-context motivation for model to improve its responses
with awareness of the optimization objective. This simple modification
leverages the in-context learning ability of LLMs aligning generation with
optimization, thereby incentivizing the model to generate desired outputs from
both inner motivation and external reward. Empirical evaluations on the Knights
and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that
\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,
ablation studies show that performance improves with greater consistency
between the in-context motivation and the external reward function, while the
model also demonstrates an ability to adapt to misleading motivations through
reinforcement learning.

</details>


### [64] [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/pdf/2506.18501)
*Wael Etaiwi, Bushra Alhijawi*

Main category: cs.CL

TL;DR: The study evaluates ChatGPT and DeepSeek across five NLP tasks, revealing DeepSeek's strength in classification stability and logical reasoning, while ChatGPT excels in nuanced understanding and flexibility.


<details>
  <summary>Details</summary>
Motivation: To comprehensively assess the strengths, weaknesses, and domain-specific abilities of LLMs like ChatGPT and DeepSeek in diverse NLP applications.

Method: A structured experimental protocol with identical, neutral prompts and two benchmark datasets per task (sentiment analysis, topic classification, text summarization, machine translation, textual entailment).

Result: DeepSeek performs better in classification stability and logical reasoning; ChatGPT outperforms in tasks requiring nuanced understanding and flexibility.

Conclusion: The findings guide the selection of the appropriate LLM based on specific task requirements.

Abstract: The increasing use of large language models (LLMs) in natural language
processing (NLP) tasks has sparked significant interest in evaluating their
effectiveness across diverse applications. While models like ChatGPT and
DeepSeek have shown strong results in many NLP domains, a comprehensive
evaluation is needed to understand their strengths, weaknesses, and
domain-specific abilities. This is critical as these models are applied to
various tasks, from sentiment analysis to more nuanced tasks like textual
entailment and translation. This study aims to evaluate ChatGPT and DeepSeek
across five key NLP tasks: sentiment analysis, topic classification, text
summarization, machine translation, and textual entailment. A structured
experimental protocol is used to ensure fairness and minimize variability. Both
models are tested with identical, neutral prompts and evaluated on two
benchmark datasets per task, covering domains like news, reviews, and
formal/informal texts. The results show that DeepSeek excels in classification
stability and logical reasoning, while ChatGPT performs better in tasks
requiring nuanced understanding and flexibility. These findings provide
valuable insights for selecting the appropriate LLM based on task requirements.

</details>


### [65] [End-to-End Spoken Grammatical Error Correction](https://arxiv.org/pdf/2506.18532)
*Mengjie Qian, Rao Ma, Stefano Bann, Mark J. F. Gales, Kate M. Knill*

Main category: cs.CL

TL;DR: The paper explores End-to-End (E2E) frameworks for Spoken Grammatical Error Correction (SGEC), addressing challenges like data scarcity and error propagation, and proposes solutions like pseudo-labeling and reference alignment to improve performance.


<details>
  <summary>Details</summary>
Motivation: SGEC is crucial for L2 learners but faces challenges like disfluencies and lack of labeled data. Cascaded systems are error-prone, motivating E2E approaches.

Method: The study compares cascaded, partial-cascaded, and E2E architectures using Whisper. It introduces pseudo-labeling for data augmentation, contextual ASR output, and a novel reference alignment for feedback precision.

Result: Experiments on LNG and S&I corpora show significant performance improvements in E2E SGEC with the proposed methods.

Conclusion: E2E SGEC systems, enhanced by pseudo-labeling and reference alignment, outperform traditional cascaded approaches, offering better feedback for L2 learners.

Abstract: Grammatical Error Correction (GEC) and feedback play a vital role in
supporting second language (L2) learners, educators, and examiners. While
written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback
based on learners' speech, poses additional challenges due to disfluencies,
transcription errors, and the lack of structured input. SGEC systems typically
follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),
disfluency detection, and GEC, making them vulnerable to error propagation
across modules. This work examines an End-to-End (E2E) framework for SGEC and
feedback generation, highlighting challenges and possible solutions when
developing these systems. Cascaded, partial-cascaded and E2E architectures are
compared, all built on the Whisper foundation model. A challenge for E2E
systems is the scarcity of GEC labeled spoken data. To address this, an
automatic pseudo-labeling framework is examined, increasing the training data
from 77 to over 2500 hours. To improve the accuracy of the SGEC system,
additional contextual information, exploiting the ASR output, is investigated.
Candidate feedback of their mistakes is an essential step to improving
performance. In E2E systems the SGEC output must be compared with an estimate
of the fluent transcription to obtain the feedback. To improve the precision of
this feedback, a novel reference alignment process is proposed that aims to
remove hypothesised edits that results from fluent transcription errors.
Finally, these approaches are combined with an edit confidence estimation
approach, to exclude low-confidence edits. Experiments on the in-house
Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)
corpus show that the proposed approaches significantly boost E2E SGEC
performance.

</details>


### [66] [When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking](https://arxiv.org/pdf/2506.18535)
*Manu Pande, Shahil Kumar, Anay Yatin Damle*

Main category: cs.CL

TL;DR: Fine-tuning pre-trained transformer models degrades performance on the MS MARCO task, underperforming the base model due to disrupted embedding space structure.


<details>
  <summary>Details</summary>
Motivation: To investigate why fine-tuning harms performance on the MS MARCO passage ranking task, contrary to expectations.

Method: Conducted experiments with five model variants, including full fine-tuning and LoRA adaptations, and analyzed embedding space and training dynamics.

Result: All fine-tuning approaches underperformed the base model (MRR@10: 0.3026), with fine-tuning disrupting the optimal embedding space.

Conclusion: Conventional transfer learning wisdom is challenged; architectural innovations may be needed for improvements on saturated benchmarks.

Abstract: This paper investigates the counterintuitive phenomenon where fine-tuning
pre-trained transformer models degrades performance on the MS MARCO passage
ranking task. Through comprehensive experiments involving five model
variants-including full parameter fine-tuning and parameter efficient LoRA
adaptations-we demonstrate that all fine-tuning approaches underperform the
base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our
analysis reveals that fine-tuning disrupts the optimal embedding space
structure learned during the base model's extensive pre-training on 1 billion
sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations
show progressive embedding space flattening, while training dynamics analysis
and computational efficiency metrics further support our findings. These
results challenge conventional wisdom about transfer learning effectiveness on
saturated benchmarks and suggest architectural innovations may be necessary for
meaningful improvements.

</details>


### [67] [A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance](https://arxiv.org/pdf/2506.18576)
*Matteo Melis, Gabriella Lapesa, Dennis Assenmacher*

Main category: cs.CL

TL;DR: The paper explores how varying definitions of hate speech impact model performance, proposing a taxonomy of 14 conceptual elements and testing three LLMs on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Address ambiguity in hate speech definitions and assess their impact on NLP model performance for social good.

Method: Collect and analyze hate speech definitions to create a taxonomy, then evaluate three LLMs in zero-shot settings across three datasets.

Result: Model performance varies with definition specificity, but effects are inconsistent across architectures.

Conclusion: Definition choice affects hate speech detection, but its impact depends on model architecture.

Abstract: Detecting harmful content is a crucial task in the landscape of NLP
applications for Social Good, with hate speech being one of its most dangerous
forms. But what do we mean by hate speech, how can we define it, and how does
prompting different definitions of hate speech affect model performance? The
contribution of this work is twofold. At the theoretical level, we address the
ambiguity surrounding hate speech by collecting and analyzing existing
definitions from the literature. We organize these definitions into a taxonomy
of 14 Conceptual Elements-building blocks that capture different aspects of
hate speech definitions, such as references to the target of hate (individual
or groups) or of the potential consequences of it. At the experimental level,
we employ the collection of definitions in a systematic zero-shot evaluation of
three LLMs, on three hate speech datasets representing different types of data
(synthetic, human-in-the-loop, and real-world). We find that choosing different
definitions, i.e., definitions with a different degree of specificity in terms
of encoded elements, impacts model performance, but this effect is not
consistent across all architectures.

</details>


### [68] [Parallel Continuous Chain-of-Thought with Jacobi Iteration](https://arxiv.org/pdf/2506.18582)
*Haoyi Wu, Zhihao Teng, Kewei Tu*

Main category: cs.CL

TL;DR: PCCoT improves continuous CoT by enabling parallel updates of latent thought tokens via Jacobi iteration, saving 50% time while maintaining or enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Sequential dependencies in continuous CoT slow down training. PCCoT aims to address this inefficiency.

Method: Proposes PCCoT using Jacobi iteration to update latent thought tokens in parallel, reducing sequential dependencies.

Result: Achieves comparable or better performance with 50% time savings in training and inference. Also improves stability.

Conclusion: PCCoT is an efficient and robust alternative to continuous CoT, with significant time savings.

Abstract: Continuous chain-of-thought has been shown to be effective in saving
reasoning tokens for large language models. By reasoning with continuous latent
thought tokens, continuous CoT is able to perform implicit reasoning in a
compact manner. However, the sequential dependencies between latent thought
tokens spoil parallel training, leading to long training time. In this paper,
we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi
iteration on the latent thought tokens, updating them iteratively in parallel
instead of sequentially and thus improving both training and inference
efficiency of continuous CoT. Experiments demonstrate that by choosing the
proper number of iterations, we are able to achieve comparable or even better
performance while saving nearly 50% of the training and inference time.
Moreover, PCCoT shows better stability and robustness in the training process.
Our code is available at https://github.com/whyNLP/PCCoT.

</details>


### [69] [Semantic similarity estimation for domain specific data using BERT and other techniques](https://arxiv.org/pdf/2506.18602)
*R. Prashanth*

Main category: cs.CL

TL;DR: The paper evaluates semantic similarity estimation using USE, InferSent, and BERT, finding BERT superior, especially for domain-specific data.


<details>
  <summary>Details</summary>
Motivation: Semantic similarity estimation is crucial for NLP tasks like question answering and machine translation.

Method: Comparative analysis using USE, InferSent, and BERT on in-house and Quora question pairs datasets.

Result: BERT outperforms other methods, attributed to its fine-tuning capability.

Conclusion: BERT is the best choice for domain-specific semantic similarity tasks.

Abstract: Estimation of semantic similarity is an important research problem both in
natural language processing and the natural language understanding, and that
has tremendous application on various downstream tasks such as question
answering, semantic search, information retrieval, document clustering,
word-sense disambiguation and machine translation. In this work, we carry out
the estimation of semantic similarity using different state-of-the-art
techniques including the USE (Universal Sentence Encoder), InferSent and the
most recent BERT, or Bidirectional Encoder Representations from Transformers,
models. We use two question pairs datasets for the analysis, one is a domain
specific in-house dataset and the other is a public dataset which is the
Quora's question pairs dataset. We observe that the BERT model gave much
superior performance as compared to the other methods. This should be because
of the fine-tuning procedure that is involved in its training process, allowing
it to learn patterns based on the training data that is used. This works
demonstrates the applicability of BERT on domain specific datasets. We infer
from the analysis that BERT is the best technique to use in the case of domain
specific data.

</details>


### [70] [The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches](https://arxiv.org/pdf/2506.18621)
*Alisa Barkar, Mathieu Chollet, Matthieu Labeau, Beatrice Biancardi, Chloe Clavel*

Main category: cs.CL

TL;DR: GPT-4o modifies speech transcripts to alter persuasiveness, focusing on stylistic changes like emotional lexicon and syntax rather than human-like optimization.


<details>
  <summary>Details</summary>
Motivation: To explore how large language models understand and manipulate persuasiveness in public speaking.

Method: Modified PhD speech transcripts from the 3MT French dataset, using GPT-4o to enhance/diminish persuasiveness and analyzed linguistic shifts.

Result: GPT-4o applies systematic stylistic changes (e.g., emotional lexicon, syntactic structures) but doesn't optimize persuasiveness like humans.

Conclusion: GPT-4o's approach to persuasiveness is stylistically driven, not human-like, highlighting its unique linguistic manipulation capabilities.

Abstract: This study examines how large language models understand the concept of
persuasiveness in public speaking by modifying speech transcripts from PhD
candidates in the "Ma These en 180 Secondes" competition, using the 3MT French
dataset. Our contributions include a novel methodology and an interpretable
textual feature set integrating rhetorical devices and discourse markers. We
prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic
shifts between original and generated speech in terms of the new features.
Results indicate that GPT-4o applies systematic stylistic modifications rather
than optimizing persuasiveness in a human-like manner. Notably, it manipulates
emotional lexicon and syntactic structures (such as interrogative and
exclamatory clauses) to amplify rhetorical impact.

</details>


### [71] [ByteSpan: Information-Driven Subword Tokenisation](https://arxiv.org/pdf/2506.18639)
*Zbulon Goriely, Suchir Salhan, Pietro Lesci, Julius Cheng, Paula Buttery*

Main category: cs.CL

TL;DR: ByteSpan, a new subword tokeniser, groups predictable byte sequences using an external byte-level LM, outperforming BPE in morphological alignment and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper explores whether grouping predictable bytes can create a useful fixed subword vocabulary, inspired by computational models of word segmentation.

Method: Proposes ByteSpan, which uses an external byte-level LM to identify and group predictable byte sequences into subwords.

Result: ByteSpan achieves higher morphological alignment scores than BPE for English and similar compression/Rnyi efficiency for 25 languages.

Conclusion: ByteSpan is an efficient and effective subword tokeniser, demonstrating improved performance over BPE.

Abstract: Recent dynamic tokenisation methods operate directly on bytes and pool their
latent representations into patches. This bears similarities to computational
models of word segmentation that determine lexical boundaries using spikes in
an autoregressive model's prediction error. Inspired by this connection, we
explore whether grouping predictable bytes - rather than pooling their
representations - can yield a useful fixed subword vocabulary. We propose a new
information-driven subword tokeniser, ByteSpan, that uses an external
byte-level LM during training to identify contiguous predictable byte sequences
and group them into subwords. Experiments show that ByteSpan yields efficient
vocabularies with higher morphological alignment scores than BPE for English.
Multilingual experiments show similar compression and R\'enyi efficiency for 25
languages.

</details>


### [72] [Is There a Case for Conversation Optimized Tokenizers in Large Language Models?](https://arxiv.org/pdf/2506.18674)
*Raquel Ferrando, Javier Conde, Gonzalo Martnez, Pedro Reviriego*

Main category: cs.CL

TL;DR: Optimizing tokenizers for chatbot conversations reduces token counts by 5-10%, saving energy with minimal impact on original corpus efficiency.


<details>
  <summary>Details</summary>
Motivation: The computational and energy costs of LLMs are high, and tokenizers optimized for training corpora may not perform well in chatbot interactions, prompting the need for domain-specific optimization.

Method: Redesign tokenizer vocabularies using a chatbot conversation corpus and evaluate their performance in this domain.

Result: Conversation-optimized tokenizers reduce token counts in dialogues by 5-10%, leading to energy savings with minimal impact on original corpus efficiency.

Conclusion: Optimizing tokenizers for chatbot conversations is beneficial, offering energy savings without compromising performance on the original corpus.

Abstract: The computational and energy costs of Large Language Models (LLMs) have
increased exponentially driven by the growing model sizes and the massive
adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is
the computation of a token. Therefore, the tokenizer plays an important role in
the efficiency of a model, and they are carefully optimized to minimize the
number of tokens for the text in their training corpus. One of the most popular
applications of LLMs are chatbots that interact with users. A key observation
is that, for those chatbots, what is important is the performance of the
tokenizer in the user text input and the chatbot responses. Those are most
likely different from the text in the training corpus. So, a question that
immediately arises is whether there is a potential benefit in optimizing
tokenizers for chatbot conversations. In this paper, this idea is explored for
different tokenizers by using a publicly available corpus of chatbot
conversations to redesign their vocabularies and evaluate their performance in
this domain. The results show that conversation-optimized tokenizers
consistently reduce the number of tokens in chatbot dialogues, which can lead
to meaningful energy savings, in the range of 5% to 10% while having minimal or
even slightly positive impact on tokenization efficiency for the original
training corpus.

</details>


### [73] [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/pdf/2506.18703)
*Christian Huber, Alexander Waibel*

Main category: cs.CL

TL;DR: A method for correcting substitution errors in neural sequence-to-sequence speech recognition systems improves accuracy for challenging words like named entities and acronyms, achieving an 11% relative improvement in biased word error rate.


<details>
  <summary>Details</summary>
Motivation: Current neural sequence-to-sequence systems struggle with recognizing words not seen during training, especially those with pronunciation-orthography mismatches.

Method: Proposes a correction method allowing users to add corrections during inference to address substitution errors.

Result: Achieves up to 11% relative improvement in biased word error rate while maintaining competitive overall performance.

Conclusion: The method effectively enhances recognition accuracy for challenging words without degrading general performance.

Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for
automatic speech recognition. When using appropriate modeling units, e.g.,
byte-pair encoded characters, these systems are in principal open vocabulary
systems. In practice, however, they often fail to recognize words not seen
during training, e.g., named entities, acronyms, or domain-specific special
words. To address this problem, many context biasing methods have been
proposed; however, for words with a pronunciation-orthography mismatch, these
methods may still struggle. We propose a method which allows corrections of
substitution errors to improve the recognition accuracy of such challenging
words. Users can add corrections on the fly during inference. We show that with
this method we get a relative improvement in biased word error rate of up to
11\%, while maintaining a competitive overall word error rate.

</details>


### [74] [Benchmarking the Pedagogical Knowledge of Large Language Models](https://arxiv.org/pdf/2506.18710)
*Maxime Lelivre, Amy Waldock, Meng Liu, Natalia Valds Aspillaga, Alasdair Mackintosh, Mara Jos Ogando Portelo, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod*

Main category: cs.CL

TL;DR: The paper introduces The Pedagogy Benchmark to evaluate AI models' pedagogical knowledge, addressing gaps in existing benchmarks. It reports results for 97 models and provides an interactive leaderboard.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on content knowledge, neglecting pedagogy. This gap hinders assessing AI's potential in education.

Method: The benchmark uses questions from teacher exams, covering pedagogical subdomains. It evaluates models on Cross-Domain Pedagogical Knowledge and Special Education Needs and Disability knowledge.

Result: Model accuracies ranged from 28% to 89%. The paper analyzes cost-accuracy trade-offs and tracks progress over time.

Conclusion: Education-focused benchmarks are essential for responsible AI deployment in education, guiding development and policy.

Abstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a
pivotal role in evaluating AI's knowledge and abilities across diverse domains.
However, existing benchmarks predominantly focus on content knowledge, leaving
a critical gap in assessing models' understanding of pedagogy - the method and
practice of teaching. This paper introduces The Pedagogy Benchmark, a novel
dataset designed to evaluate large language models on their Cross-Domain
Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)
pedagogical knowledge. These benchmarks are built on a carefully curated set of
questions sourced from professional development exams for teachers, which cover
a range of pedagogical subdomains such as teaching strategies and assessment
methods. Here we outline the methodology and development of these benchmarks.
We report results for 97 models, with accuracies spanning a range from 28% to
89% on the pedagogical knowledge questions. We consider the relationship
between cost and accuracy and chart the progression of the Pareto value
frontier over time. We provide online leaderboards at
https://rebrand.ly/pedagogy which are updated with new models and allow
interactive exploration and filtering based on various model properties, such
as cost per token and open-vs-closed weights, as well as looking at performance
in different subjects. LLMs and generative AI have tremendous potential to
influence education and help to address the global learning crisis.
Education-focused benchmarks are crucial to measure models' capacities to
understand pedagogical concepts, respond appropriately to learners' needs, and
support effective teaching practices across diverse contexts. They are needed
for informing the responsible and evidence-based deployment of LLMs and
LLM-based tools in educational settings, and for guiding both development and
policy decisions.

</details>


### [75] [Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach](https://arxiv.org/pdf/2506.18756)
*Chong Zhang, Xiang Li, Jia Wang, Shan Liang, Haochen Xue, Xiaobo Jin*

Main category: cs.CL

TL;DR: The paper introduces AGBS, a method to improve prompt optimization in LLMs while maintaining semantic stability, tested on various models.


<details>
  <summary>Details</summary>
Motivation: Addressing misinterpretations in automated prompt engineering for LLMs due to diverse user requirements.

Method: Proposes Adaptive Greedy Binary Search (AGBS) to simulate prompt optimization while preserving semantics and evaluating impact on LLM performance.

Result: AGBS effectively balances semantic consistency and attack efficacy, validated through experiments on open and closed-source LLMs.

Conclusion: AGBS offers insights for designing more reliable prompt optimization systems; code is publicly available.

Abstract: Large Language Models (LLMs) increasingly rely on automatic prompt
engineering in graphical user interfaces (GUIs) to refine user inputs and
enhance response accuracy. However, the diversity of user requirements often
leads to unintended misinterpretations, where automated optimizations distort
original intentions and produce erroneous outputs. To address this challenge,
we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates
common prompt optimization mechanisms while preserving semantic stability. Our
approach dynamically evaluates the impact of such strategies on LLM
performance, enabling robust adversarial sample generation. Through extensive
experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness
in balancing semantic consistency and attack efficacy. Our findings offer
actionable insights for designing more reliable prompt optimization systems.
Code is available at: https://github.com/franz-chang/DOBS

</details>


### [76] [ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework](https://arxiv.org/pdf/2506.18768)
*Ao Chang, Tong Zhou, Yubo Chen, Delai Qiu, Shengping Liu, Kang Liu, Jun Zhao*

Main category: cs.CL

TL;DR: The paper proposes ASP2LJ, a framework combining adversarial self-play and case generation to address long-tail data and lawyer argumentation in Legal Judgment Prediction (LJP). It introduces the RareCases dataset and demonstrates improved judicial decision-making.


<details>
  <summary>Details</summary>
Motivation: LJP faces challenges like long-tail data distribution and neglect of lawyers' role in refining arguments, limiting judicial accuracy.

Method: ASP2LJ integrates a case generation module for long-tail data and adversarial self-play to enhance lawyer argumentation, improving judicial decisions.

Result: Experiments on SimuCourt and RareCases datasets show improved performance, validating the framework's effectiveness.

Conclusion: ASP2LJ enhances judicial objectivity and fairness, supported by the RareCases dataset and open-source contributions for future research.

Abstract: Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including
relevant legal charge, terms, and fines, which is a crucial process in Large
Language Model(LLM). However, LJP faces two key challenges: (1)Long Tail
Distribution: Current datasets, derived from authentic cases, suffer from high
human annotation costs and imbalanced distributions, leading to model
performance degradation. (2)Lawyer's Improvement: Existing systems focus on
enhancing judges' decision-making but neglect the critical role of lawyers in
refining arguments, which limits overall judicial accuracy. To address these
issues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment
Framework, called ASP2LJ, which integrates a case generation module to tackle
long-tailed data distributions and an adversarial self-play mechanism to
enhance lawyers' argumentation skills. Our framework enables a judge to
reference evolved lawyers' arguments, improving the objectivity, fairness, and
rationality of judicial decisions. Besides, We also introduce RareCases, a
dataset for rare legal cases in China, which contains 120 tail-end cases. We
demonstrate the effectiveness of our approach on the SimuCourt dataset and our
RareCases dataset. Experimental results show our framework brings improvements,
indicating its utilization. Our contributions include an integrated framework,
a rare-case dataset, and publicly releasing datasets and code to support
further research in automated judicial systems.

</details>


### [77] [Existing LLMs Are Not Self-Consistent For Simple Tasks](https://arxiv.org/pdf/2506.18781)
*Zhenru Lin, Jiawen Tao, Yang Yuan, Andrew Chi-Chih Yao*

Main category: cs.CL

TL;DR: The paper investigates self-consistency in LLMs, revealing inconsistencies even in simple tasks, and proposes metrics and methods to address them.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and trustworthiness of LLMs by ensuring self-consistency in their reasoning.

Method: Introduces inconsistency metrics and two automated methods (graph-based and energy-based) to quantify and mitigate inconsistencies.

Result: Smaller models are highly inconsistent; state-of-the-art models show partial improvements but are not fully self-consistent.

Conclusion: Self-consistency is crucial for reliable and interpretable AI, though achieving it remains complex.

Abstract: Large Language Models (LLMs) have grown increasingly powerful, yet ensuring
their decisions remain transparent and trustworthy requires self-consistency --
no contradictions in their internal reasoning. Our study reveals that even on
simple tasks, such as comparing points on a line or a plane, or reasoning in a
family tree, all smaller models are highly inconsistent, and even
state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully
self-consistent. To quantify and mitigate these inconsistencies, we introduce
inconsistency metrics and propose two automated methods -- a graph-based and an
energy-based approach. While these fixes provide partial improvements, they
also highlight the complexity and importance of self-consistency in building
more reliable and interpretable AI. The code and data are available at
https://github.com/scorpio-nova/llm-self-consistency.

</details>


### [78] [RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies](https://arxiv.org/pdf/2506.18819)
*Arjun Mukerji, Michael L. Jackson, Jason Jones, Neil Sanghavi*

Main category: cs.CL

TL;DR: RWESummary is introduced to benchmark LLMs for summarizing real-world evidence (RWE) studies, with Gemini 2.5 models performing best.


<details>
  <summary>Details</summary>
Motivation: LLMs lack specific evaluation for RWE summarization, prompting the need for RWESummary.

Method: Developed using Atropos Health data, RWESummary includes one scenario and three evaluations to assess LLM performance.

Result: Gemini 2.5 models (Flash and Pro) outperformed others in summarizing 13 RWE studies.

Conclusion: RWESummary serves as a useful benchmark for RWE study summarization by LLMs.

Abstract: Large Language Models (LLMs) have been extensively evaluated for general
summarization tasks as well as medical research assistance, but they have not
been specifically evaluated for the task of summarizing real-world evidence
(RWE) from structured output of RWE studies. We introduce RWESummary, a
proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,
2025) to enable benchmarking of LLMs for this task. RWESummary includes one
scenario and three evaluations covering major types of errors observed in
summarization of medical research studies and was developed using Atropos
Health proprietary data. Additionally, we use RWESummary to compare the
performance of different LLMs in our internal RWE summarization tool. At the
time of publication, with 13 distinct RWE studies, we found the Gemini 2.5
models performed best overall (both Flash and Pro). We suggest RWESummary as a
novel and useful foundation model benchmark for real-world evidence study
summarization.

</details>


### [79] [MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task](https://arxiv.org/pdf/2506.18828)
*Jorge Iranzo-Snchez, Javier Iranzo-Snchez, Adri Gimnez, Jorge Civera, Alfons Juan*

Main category: cs.CL

TL;DR: The MLLP-VRAIN group's IWSLT 2025 submission uses a modular cascade system with pre-trained models (Whisper for ASR, NLLB for MT) for real-time long-form speech translation, achieving a balance of quality and latency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in real-time translation of long-form speech without extensive in-domain data or end-to-end training.

Method: Combines Whisper Large-V3-Turbo for ASR and NLLB-3.3B for MT, with lightweight adaptation techniques like prefix training and adaptive emission policies (wait-$k$, RALCP).

Result: Achieves 31.96 BLEU on ACL60/60 and 29.8 BLEU on IWSLT25Instruct, with 2.94s latency.

Conclusion: Pre-trained models with careful adaptation can create effective simultaneous translation systems for long-form content.

Abstract: This work describes the participation of the MLLP-VRAIN research group in the
shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our
submission addresses the unique challenges of real-time translation of
long-form speech by developing a modular cascade system that adapts strong
pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo
for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight
adaptation techniques rather than training new end-to-end models from scratch.
Our approach employs document-level adaptation with prefix training to enhance
the MT model's ability to handle incomplete inputs, while incorporating
adaptive emission policies including a wait-$k$ strategy and RALCP for managing
the translation stream. Specialized buffer management techniques and
segmentation strategies ensure coherent translations across long audio
sequences. Experimental results on the ACL60/60 dataset demonstrate that our
system achieves a favorable balance between translation quality and latency,
with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of
2.94 seconds. Our final model achieves a preliminary score on the official test
set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully
adapted pre-trained components can create effective simultaneous translation
systems for long-form content without requiring extensive in-domain parallel
data or specialized end-to-end training.

</details>


### [80] [STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning](https://arxiv.org/pdf/2506.18831)
*Aryasomayajula Ram Bharadwaj*

Main category: cs.CL

TL;DR: STUPID dynamically adjusts reasoning steps in LLMs using a PID controller, improving accuracy by 6% and reducing token usage by 32%.


<details>
  <summary>Details</summary>
Motivation: Overthinking in LLMs with CoT reasoning increases costs and may degrade performance; static methods lack adaptability.

Method: Uses a PID controller and chunk-level classifier to dynamically adjust steering strength during inference.

Result: 6% accuracy improvement and 32% token reduction on GSM8K, outperforming static baselines.

Conclusion: STUPID offers a principled, training-free framework for efficient dynamic reasoning calibration.

Abstract: Large Language Models employing extended chain-of-thought (CoT) reasoning
often suffer from the overthinking phenomenon, generating excessive and
redundant reasoning steps that increase computational costs while potentially
degrading performance. While recent work has explored static steering
approaches to mitigate this issue, they lack the adaptability to dynamically
adjust intervention strength based on real-time reasoning quality. We propose
STUPID (Steering Token Usage via PID controller), a novel training-free method
that employs a PID controller to dynamically modulate activation steering
strength during inference. Our approach combines a chunk-level classifier for
detecting redundant reasoning patterns with a PID control mechanism that
adaptively adjusts steering intensity based on the predicted redundancy
probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves
a 6% improvement in accuracy while reducing token usage by 32%, outperforming
static steering baselines. Our method provides a principled framework for
dynamic reasoning calibration that maintains reasoning quality while
significantly improving computational efficiency.

</details>


### [81] [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](https://arxiv.org/pdf/2506.18841)
*Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li*

Main category: cs.CL

TL;DR: The paper proposes an incentivization-based approach using reinforcement learning (RL) to enable ultra-long, high-quality text generation in LLMs without relying on synthetic data.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of traditional supervised fine-tuning (SFT) methods, which depend on costly and artificial synthetic data, for ultra-long text generation.

Method: Uses RL to train a base model (LongWriter-Zero) with specialized reward models for length control, quality, and structure, starting from scratch without synthetic data.

Result: Outperforms SFT methods and larger models (e.g., DeepSeek R1, Qwen3-235B) on benchmarks like WritingBench and Arena-Write.

Conclusion: The RL-based approach is effective for ultra-long text generation, achieving state-of-the-art results without synthetic data.

Abstract: Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B

</details>


### [82] [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/pdf/2506.18852)
*Iwan Williams, Ninell Oldenburg, Ruchira Dhar, Joshua Hatherley, Constanza Fierro, Nina Rajcic, Sandrine R. Schiller, Filippos Stamatiou, Anders Sgaard*

Main category: cs.CL

TL;DR: The paper argues for integrating philosophy into mechanistic interpretability (MI) to clarify concepts, refine methods, and assess ethical stakes, using three open problems as examples.


<details>
  <summary>Details</summary>
Motivation: To highlight the need for philosophy in MI to address assumptions, concepts, and ethical implications, beyond just technical analysis.

Method: The paper takes three open problems from MI literature as examples to demonstrate philosophy's value in refining MI research.

Result: Illustrates how philosophy can enhance MI by improving conceptual clarity, methodological rigor, and ethical awareness.

Conclusion: Calls for deeper interdisciplinary collaboration between MI and philosophy to advance the field.

Abstract: Mechanistic interpretability (MI) aims to explain how neural networks work by
uncovering their underlying causal mechanisms. As the field grows in influence,
it is increasingly important to examine not just models themselves, but the
assumptions, concepts and explanatory strategies implicit in MI research. We
argue that mechanistic interpretability needs philosophy: not as an
afterthought, but as an ongoing partner in clarifying its concepts, refining
its methods, and assessing the epistemic and ethical stakes of interpreting AI
systems. Taking three open problems from the MI literature as examples, this
position paper illustrates the value philosophy can add to MI research, and
outlines a path toward deeper interdisciplinary dialogue.

</details>


### [83] [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/pdf/2506.18879)
*Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan*

Main category: cs.CL

TL;DR: CommVQ reduces KV cache memory usage in LLMs by 87.5% with 2-bit quantization, enabling long-context inference on GPUs like RTX 4090.


<details>
  <summary>Details</summary>
Motivation: KV cache memory bottlenecks limit long-context LLM inference on GPUs.

Method: Uses Commutative Vector Quantization (CommVQ) with additive quantization and a RoPE-commutative codebook, trained via EM.

Result: Achieves high accuracy with 2-bit quantization, outperforming SOTA methods, and enables 1-bit quantization with minimal loss.

Conclusion: CommVQ effectively reduces memory overhead, enabling efficient long-context LLM inference.

Abstract: Large Language Models (LLMs) are increasingly used in applications requiring
long context lengths, but the key-value (KV) cache often becomes a memory
bottleneck on GPUs as context grows. To address this, we propose Commutative
Vector Quantization (CommVQ) to significantly reduce memory usage for
long-context LLM inference. We first introduce additive quantization with a
lightweight encoder and codebook to compress the KV cache, which can be decoded
via simple matrix multiplication. To further reduce computational costs during
decoding, we design the codebook to be commutative with Rotary Position
Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.
This enables efficient integration of decoding into the self-attention
mechanism. Our approach achieves high accuracy with additive quantization and
low overhead via the RoPE-commutative codebook. Experiments on long-context
benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%
with 2-bit quantization, while outperforming state-of-the-art KV cache
quantization methods. Notably, it enables 1-bit KV cache quantization with
minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context
length on a single RTX 4090 GPU. The source code is available at:
https://github.com/UMass-Embodied-AGI/CommVQ.

</details>


### [84] [OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization](https://arxiv.org/pdf/2506.18880)
*Yiyou Sun, Shawn Hu, Georgia Zhou, Ken Zheng, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song*

Main category: cs.CL

TL;DR: OMEGA benchmark evaluates LLMs' out-of-distribution generalization in math, revealing limitations in exploratory, compositional, and transformative reasoning. Fine-tuning improves exploratory tasks but not transformative reasoning.


<details>
  <summary>Details</summary>
Motivation: To address LLMs' reliance on narrow strategies and struggles with novel problem-solving in math, OMEGA systematically tests three generalization axes inspired by creativity typology.

Method: OMEGA uses programmatically generated math problems across domains, evaluating LLMs' performance on exploratory, compositional, and transformative generalization. Fine-tuning Qwen-series models is tested.

Result: Frontier LLMs show sharp performance decline with complexity. Fine-tuning improves exploratory generalization but not compositional or transformative reasoning.

Conclusion: OMEGA highlights LLMs' limitations in creative math problem-solving, providing a foundation for advancing beyond mechanical proficiency.

Abstract: Recent large-scale language models (LLMs) with long Chain-of-Thought
reasoning-such as DeepSeek-R1-have achieved impressive results on
Olympiad-level mathematics benchmarks. However, they often rely on a narrow set
of strategies and struggle with problems that require a novel way of thinking.
To systematically investigate these limitations, we introduce
OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a
controlled yet diverse benchmark designed to evaluate three axes of
out-of-distribution generalization, inspired by Boden's typology of creativity:
(1) Exploratory-applying known problem solving skills to more complex instances
within the same problem domain; (2) Compositional-combining distinct reasoning
skills, previously learned in isolation, to solve novel problems that require
integrating these skills in new and coherent ways; and (3)
Transformative-adopting novel, often unconventional strategies by moving beyond
familiar approaches to solve problems more effectively. OMEGA consists of
programmatically generated training-test pairs derived from templated problem
generators across geometry, number theory, algebra, combinatorics, logic, and
puzzles, with solutions verified using symbolic, numerical, or graphical
methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance
degradation as problem complexity increases. Moreover, we fine-tune the
Qwen-series models across all generalization settings and observe notable
improvements in exploratory generalization, while compositional generalization
remains limited and transformative reasoning shows little to no improvement. By
isolating and quantifying these fine-grained failures, OMEGA lays the
groundwork for advancing LLMs toward genuine mathematical creativity beyond
mechanical proficiency.

</details>


### [85] [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/pdf/2506.18896)
*Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang*

Main category: cs.CL

TL;DR: ReasonFlux-PRM is a trajectory-aware Process Reward Model (PRM) designed to evaluate intermediate reasoning steps in LLMs, outperforming existing PRMs and human-curated baselines in data quality and performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing PRMs struggle to robustly evaluate intermediate reasoning steps, especially in trajectory-response outputs from advanced reasoning models like Deepseek-R1.

Method: ReasonFlux-PRM incorporates step-level and trajectory-level supervision for fine-grained reward assignment, supporting offline and online settings like data distillation, reinforcement learning, and test-time scaling.

Result: Empirical results show ReasonFlux-PRM-7B outperforms strong PRMs and human baselines, achieving performance gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling.

Conclusion: ReasonFlux-PRM is a robust solution for evaluating reasoning trajectories, offering significant improvements and scalability, with a lightweight version (1.5B) for resource-constrained applications.

Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux

</details>


### [86] [A Survey on Data Selection for LLM Instruction Tuning](https://arxiv.org/pdf/2402.05123)
*Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu*

Main category: cs.CL

TL;DR: A survey on data selection methods for enhancing instruction tuning in large language models (LLMs), covering datasets, taxonomy, recent advances, evaluation strategies, and open challenges.


<details>
  <summary>Details</summary>
Motivation: To improve instruction tuning efficiency and effectiveness by focusing on high-quality data selection rather than quantity.

Method: Proposes a taxonomy of data selection methods, reviews recent advances, and evaluates strategies and results.

Result: Highlights the importance of quality over quantity in instruction datasets and summarizes current data selection techniques.

Conclusion: Identifies open challenges and future directions for optimizing data selection in LLM instruction tuning.

Abstract: Instruction tuning is a vital step of training large language models (LLM),
so how to enhance the effect of instruction tuning has received increased
attention. Existing works indicate that the quality of the dataset is more
crucial than the quantity during instruction tuning of LLM. Therefore, recently
a lot of studies focus on exploring the methods of selecting high-quality
subset from instruction datasets, aiming to reduce training costs and enhance
the instruction-following capabilities of LLMs. This paper presents a
comprehensive survey on data selection for LLM instruction tuning. Firstly, we
introduce the wildly used instruction datasets. Then, we propose a new taxonomy
of the data selection methods and provide a detailed introduction of recent
advances,and the evaluation strategies and results of data selection methods
are also elaborated in detail. Finally, we emphasize the open challenges and
present new frontiers of this task.

</details>


### [87] [Alignment Helps Make the Most of Multimodal Data](https://arxiv.org/pdf/2405.08454)
*Christian Arnold, Andreas Kpfer*

Main category: cs.CL

TL;DR: The paper emphasizes the importance of aligning multimodal data in political science, offering a decision tree for alignment choices and demonstrating its value through two applications.


<details>
  <summary>Details</summary>
Motivation: Political scientists often analyze multimodal data but typically fail to align it, missing its full analytical potential.

Method: A systematic review of 2,703 papers and the introduction of a decision tree framework for alignment choices.

Result: The framework highlights alignment's untapped potential and provides practical advice for research design and modeling.

Conclusion: Alignment of multimodal data significantly enhances analytical value, as shown in predicting campaign ad tonality and cross-modal querying of parliamentary speeches.

Abstract: Political scientists increasingly analyze multimodal data. However, the
effective analysis of such data requires aligning information across different
modalities. In our paper, we demonstrate the significance of such alignment.
Informed by a systematic review of 2,703 papers, we find that political
scientists typically do not align their multimodal data. Introducing a decision
tree that guides alignment choices, our framework highlights alignment's
untapped potential and provides concrete advice in research design and modeling
decisions. We illustrate alignment's analytical value through two applications:
predicting tonality in U.S. presidential campaign ads and cross-modal querying
of German parliamentary speeches to examine responses to the far-right AfD.

</details>


### [88] [A Closer Look into Mixture-of-Experts in Large Language Models](https://arxiv.org/pdf/2406.18219)
*Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, Jie Fu*

Main category: cs.CL

TL;DR: The paper explores the inner workings of Mixture-of-Experts (MoE) models, revealing key observations about neuron behavior, router selection, and expert diversity, with practical suggestions for MoE practitioners.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms of MoE-based large language models, which are efficient but underexplored, and to improve their modularization and performance.

Method: Comprehensive study of parametric and behavioral features of three popular MoE-based models, including analysis of neuron behavior, router selection, and expert diversity across layers.

Result: Key findings include fine-grained neuron behavior, router preference for experts with larger output norms, and increasing expert diversity with layer depth (except the last layer).

Conclusion: The study provides insights and practical suggestions for MoE practitioners, aiming to guide future research on MoE and modular architectures.

Abstract: Mixture-of-experts (MoE) is gaining increasing attention due to its unique
properties and remarkable performance, especially for language tasks. By
sparsely activating a subset of parameters for each token, MoE architecture
could increase the model size without sacrificing computational efficiency,
achieving a better trade-off between performance and training costs. However,
the underlying mechanism of MoE still lacks further exploration, and its
modularization degree remains questionable. In this paper, we make an initial
attempt to understand the inner workings of MoE-based large language models.
Concretely, we comprehensively study the parametric and behavioral features of
three popular MoE-based models and reveal some intriguing observations,
including 1) Neurons act like fine-grained experts; 2) The router of MoE
usually selects experts with larger output norms; 3) The expert diversity
increases as the layer increases, while the last layer is an outlier, which is
further validated by an initial experiment. Based on the observations, we also
provide suggestions for a broad spectrum of MoE practitioners, such as router
design and expert allocation. We hope this work could shed light on future
research on the MoE framework and other modular architectures. Code is
available at https://github.com/kamanphoebe/Look-into-MoEs.

</details>


### [89] [Anthropocentric bias in language model evaluation](https://arxiv.org/pdf/2407.03859)
*Raphal Millire, Charles Rathkopf*

Main category: cs.CL

TL;DR: The paper addresses biases in evaluating LLM cognitive capacities, highlighting two neglected anthropocentric biases: auxiliary oversight and mechanistic chauvinism. It advocates for an iterative, empirical approach combining behavioral and mechanistic studies.


<details>
  <summary>Details</summary>
Motivation: To address biases in assessing LLM cognitive abilities, particularly anthropocentric biases like auxiliary oversight and mechanistic chauvinism, which hinder accurate evaluation.

Method: Proposes an iterative, empirically-driven approach combining behavioral experiments and mechanistic studies to map LLM-specific capacities.

Result: Identifies two key biases and suggests a framework for more accurate LLM evaluation.

Conclusion: Mitigating biases requires a tailored approach integrating behavioral and mechanistic insights for fair LLM assessment.

Abstract: Evaluating the cognitive capacities of large language models (LLMs) requires
overcoming not only anthropomorphic but also anthropocentric biases. This
article identifies two types of anthropocentric bias that have been neglected:
overlooking how auxiliary factors can impede LLM performance despite competence
("auxiliary oversight"), and dismissing LLM mechanistic strategies that differ
from those of humans as not genuinely competent ("mechanistic chauvinism").
Mitigating these biases necessitates an empirically-driven, iterative approach
to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can
be done by supplementing carefully designed behavioral experiments with
mechanistic studies.

</details>


### [90] ["I understand why I got this grade": Automatic Short Answer Grading with Feedback](https://arxiv.org/pdf/2407.12818)
*Dishank Aggarwal, Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: The paper introduces EngSAF, a dataset for automated short-answer grading with feedback, leveraging LLMs for feedback generation and achieving 75.4% accuracy on unseen answers.


<details>
  <summary>Details</summary>
Motivation: Manual grading and feedback for short-answer questions in education are time-consuming, and there's a lack of public datasets for automated solutions.

Method: The EngSAF dataset is created with diverse engineering questions, and feedback is generated using LLMs via the LASFG strategy. Baselines are provided for future comparisons.

Result: The best model (Mistral-7B) achieves 75.4% accuracy on unseen answers and 58.7% on unseen questions, with successful real-world deployment.

Conclusion: EngSAF addresses the gap in automated grading datasets, demonstrating practical utility and effectiveness in real-world educational settings.

Abstract: In recent years, there has been a growing interest in using Artificial
Intelligence (AI) to automate student assessment in education. Among different
types of assessments, summative assessments play a crucial role in evaluating a
student's understanding level of a course. Such examinations often involve
short-answer questions. However, grading these responses and providing
meaningful feedback manually at scale is both time-consuming and
labor-intensive. Feedback is particularly important, as it helps students
recognize their strengths and areas for improvement. Despite the importance of
this task, there is a significant lack of publicly available datasets that
support automatic short-answer grading with feedback generation. To address
this gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset
designed for automatic short-answer grading with feedback. The dataset covers a
diverse range of subjects, questions, and answer patterns from multiple
engineering domains and contains ~5.8k data points. We incorporate feedback
into our dataset by leveraging the generative capabilities of state-of-the-art
large language models (LLMs) using our Label-Aware Synthetic Feedback
Generation (LASFG) strategy. This paper underscores the importance of enhanced
feedback in practical educational settings, outlines dataset annotation and
feedback generation processes, conducts a thorough EngSAF analysis, and
provides different LLMs-based zero-shot and finetuned baselines for future
comparison. The best-performing model (Mistral-7B) achieves an overall accuracy
of 75.4% and 58.7% on unseen answers and unseen question test sets,
respectively. Additionally, we demonstrate the efficiency and effectiveness of
our ASAG system through its deployment in a real-world end-semester exam at a
reputed institute.

</details>


### [91] [UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation](https://arxiv.org/pdf/2408.00863)
*Shuhan Guo, Yatao Bian, Ruibing Wang, Nan Yin, Zhen Wang, Quanming Yao*

Main category: cs.CL

TL;DR: UniMoT is a Unified Molecule-Text LLM that treats molecules and text equally, using a tokenizer-based architecture for state-of-the-art performance in molecule comprehension and generation.


<details>
  <summary>Details</summary>
Motivation: Existing molecular LLMs lack equal treatment of molecule and text modalities and supervision for molecules. UniMoT addresses this gap.

Method: UniMoT uses a Vector Quantization-driven tokenizer with a Q-Former to unify molecule and text tokens under an autoregressive training paradigm.

Result: UniMoT achieves state-of-the-art performance in molecule comprehension and generation tasks.

Conclusion: UniMoT successfully bridges the modality gap, enabling unified molecule-text processing and generation.

Abstract: The remarkable success of Large Language Models (LLMs) across diverse tasks
has driven the research community to extend their capabilities to molecular
applications. However, most molecular LLMs employ adapter-based architectures
that do not treat molecule and text modalities equally and lack a supervision
signal for the molecule modality. To address these issues, we introduce UniMoT,
a Unified Molecule-Text LLM adopting a tokenizer-based architecture that
expands the vocabulary of LLM with molecule tokens. Specifically, we introduce
a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge
the modality gap between molecule and text. This tokenizer transforms molecules
into sequences of molecule tokens with causal dependency, encapsulating
high-level molecular and textual information. Equipped with this tokenizer,
UniMoT can unify molecule and text modalities under a shared token
representation and an autoregressive training paradigm, enabling it to
interpret molecules as a foreign language and generate them as text. Following
a four-stage training scheme, UniMoT emerges as a multi-modal generalist
capable of performing both molecule-to-text and text-to-molecule tasks.
Extensive experiments demonstrate that UniMoT achieves state-of-the-art
performance across a wide range of molecule comprehension and generation tasks.

</details>


### [92] [Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference](https://arxiv.org/pdf/2408.08590)
*Geonhee Kim, Marco Valentino, Andr Freitas*

Main category: cs.CL

TL;DR: The paper investigates whether language models (LMs) learn systematic reasoning or rely on superficial patterns, uncovering a circuit for syllogistic inference and noting contamination from world knowledge.


<details>
  <summary>Details</summary>
Motivation: To determine if LMs use formal reasoning mechanisms or exploit training data patterns, focusing on syllogistic inference.

Method: Uses circuit discovery with two intervention methods to identify a reasoning circuit and examines belief biases and generalization across models.

Result: Found a necessary circuit for syllogistic reasoning, but it's contaminated by world knowledge, limiting abstract logical generalization.

Conclusion: LMs learn transferable reasoning mechanisms but lack abstract logical primitives, influenced by pre-training knowledge.

Abstract: Recent studies on reasoning in language models (LMs) have sparked a debate on
whether they can learn systematic inferential principles or merely exploit
superficial patterns in the training data. To understand and uncover the
mechanisms adopted for formal reasoning in LMs, this paper presents a
mechanistic interpretation of syllogistic inference. Specifically, we present a
methodology for circuit discovery aimed at interpreting content-independent and
formal reasoning mechanisms. Through two distinct intervention methods, we
uncover a sufficient and necessary circuit involving middle-term suppression
that elucidates how LMs transfer information to derive valid conclusions from
premises. Furthermore, we investigate how belief biases manifest in syllogistic
inference, finding evidence of partial contamination from additional attention
heads responsible for encoding commonsense and contextualized knowledge.
Finally, we explore the generalization of the discovered mechanisms across
various syllogistic schemes, model sizes and architectures. The identified
circuit is sufficient and necessary for syllogistic schemes on which the models
achieve high accuracy (>60%), with compatible activation patterns across models
of different families. Overall, our findings suggest that LMs learn
transferable content-independent reasoning mechanisms, but that, at the same
time, such mechanisms do not involve generalizable and abstract logical
primitives, being susceptible to contamination by the same world knowledge
acquired during pre-training.

</details>


### [93] [Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models](https://arxiv.org/pdf/2408.14470)
*Aradhye Agarwal, Suhas K Ramesh, Ayan Sengupta, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: $	ext{ID}^3$ is a dynamic selective PEFT method that improves computational efficiency and performance by continually calculating parameter importance and balancing exploration-exploitation in parameter selection.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs is computationally expensive, and existing selective PEFT methods underperform due to static parameter selection and inherent biases.

Method: $	ext{ID}^3$ dynamically calculates parameter importance and adjusts parameter selection during fine-tuning, integrating with existing PEFT techniques like adapters and LoRA.

Result: Empirical tests on 16 tasks show $	ext{ID}^3$ outperforms fixed-masking PEFT methods, reducing gradient updates by half while maintaining performance.

Conclusion: $	ext{ID}^3$ offers a flexible, efficient solution for fine-tuning LLMs, balancing performance and computational cost.

Abstract: Fine-tuning large language models (LLMs) on downstream tasks requires
substantial computational resources. Selective PEFT, a class of
parameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these
computational challenges by selectively fine-tuning only a small fraction of
the model parameters. Although parameter-efficient, these techniques often fail
to match the performance of fully fine-tuned models, primarily due to inherent
biases introduced during parameter selection. Traditional selective PEFT
techniques use a fixed set of parameters selected using different importance
heuristics, failing to capture parameter importance dynamically and often
leading to suboptimal performance. We introduce $\text{ID}^3$, a novel
selective PEFT method that calculates parameter importance continually, and
dynamically unmasks parameters by balancing exploration and exploitation in
parameter selection. Our empirical study on 16 tasks spanning natural language
understanding, mathematical reasoning and summarization demonstrates the
effectiveness of our method compared to fixed-masking selective PEFT
techniques. We analytically show that $\text{ID}^3$ reduces the number of
gradient updates by a factor of two, enhancing computational efficiency. Since
$\text{ID}^3$ is robust to random initialization of neurons and operates
directly on the optimization process, it is highly flexible and can be
integrated with existing additive and reparametrization-based PEFT techniques
such as adapters and LoRA respectively.

</details>


### [94] [Large Language Models for Disease Diagnosis: A Scoping Review](https://arxiv.org/pdf/2409.00097)
*Shuang Zhou, Zidu Xu, Mian Zhang, Chunpu Xu, Yawen Guo, Zaifu Zhan, Yi Fang, Sirui Ding, Jiashuo Wang, Kaishuai Xu, Liqiao Xia, Jeremy Yeung, Daochen Zha, Dongming Cai, Genevieve B. Melton, Mingquan Lin, Rui Zhang*

Main category: cs.CL

TL;DR: A comprehensive review of LLM-based methods for disease diagnosis, covering disease types, clinical data, techniques, and evaluation methods, with recommendations and future directions.


<details>
  <summary>Details</summary>
Motivation: The growing use of LLMs in diagnostic tasks lacks a holistic view, prompting a need to clarify their applications, techniques, and evaluations.

Method: The paper conducts a literature review across dimensions like disease types, clinical specialties, data, LLM techniques, and evaluation methods.

Result: Identifies gaps and provides recommendations for applying and evaluating LLMs in diagnostics, along with limitations and future directions.

Conclusion: This is the first comprehensive review of LLM-based disease diagnosis, offering insights and guiding future research.

Abstract: Automatic disease diagnosis has become increasingly valuable in clinical
practice. The advent of large language models (LLMs) has catalyzed a paradigm
shift in artificial intelligence, with growing evidence supporting the efficacy
of LLMs in diagnostic tasks. Despite the increasing attention in this field, a
holistic view is still lacking. Many critical aspects remain unclear, such as
the diseases and clinical data to which LLMs have been applied, the LLM
techniques employed, and the evaluation methods used. In this article, we
perform a comprehensive review of LLM-based methods for disease diagnosis. Our
review examines the existing literature across various dimensions, including
disease types and associated clinical specialties, clinical data, LLM
techniques, and evaluation methods. Additionally, we offer recommendations for
applying and evaluating LLMs for diagnostic tasks. Furthermore, we assess the
limitations of current research and discuss future directions. To our
knowledge, this is the first comprehensive review for LLM-based disease
diagnosis.

</details>


### [95] [Multilingual Retrieval Augmented Generation for Culturally-Sensitive Tasks: A Benchmark for Cross-lingual Robustness](https://arxiv.org/pdf/2410.01171)
*Bryan Li, Fiona Luo, Samar Haider, Adwait Agashe, Tammy Li, Runqi Liu, Muqing Miao, Shriya Ramakrishnan, Yuan Yuan, Chris Callison-Burch*

Main category: cs.CL

TL;DR: The paper introduces BordIRLines, a dataset for evaluating multilingual retrieval-augmented generation (RAG) in territorial disputes, showing that multilingual retrieval improves robustness and reduces bias.


<details>
  <summary>Details</summary>
Motivation: To address biases in RAG systems, especially in multilingual and culturally-sensitive contexts like territorial disputes.

Method: Developed BordIRLines dataset with Wikipedia documents in 49 languages, evaluated cross-lingual RAG robustness, and analyzed document usage in responses.

Result: Multilingual retrieval improved response consistency and reduced geopolitical bias compared to in-language retrieval. Low-resource languages showed wider variance in citation distribution.

Conclusion: Multilingual RAG enhances robustness and reduces bias, with implications for equitable information access across languages.

Abstract: The paradigm of retrieval-augmented generated (RAG) helps mitigate
hallucinations of large language models (LLMs). However, RAG also introduces
biases contained within the retrieved documents. These biases can be amplified
in scenarios which are multilingual and culturally-sensitive, such as
territorial disputes. We thus introduce BordIRLines, a dataset of territorial
disputes paired with retrieved Wikipedia documents, across 49 languages. We
evaluate the cross-lingual robustness of this RAG setting by formalizing
several modes for multilingual retrieval. Our experiments on several LLMs show
that incorporating perspectives from diverse languages can in fact improve
robustness; retrieving multilingual documents best improves response
consistency and decreases geopolitical bias over RAG with purely in-language
documents. We also consider how RAG responses utilize presented documents,
finding a much wider variance in the linguistic distribution of response
citations, when querying in low-resource languages. Our further analyses
investigate the various aspects of a cross-lingual RAG pipeline, from retrieval
to document contents. We release our benchmark and code to support continued
research towards equitable information access across languages at
https://huggingface.co/datasets/borderlines/bordirlines.

</details>


### [96] [Self-Preference Bias in LLM-as-a-Judge](https://arxiv.org/pdf/2410.21819)
*Koki Wataoka, Tsubasa Takahashi, Ryokan Ri*

Main category: cs.CL

TL;DR: The paper introduces a metric to measure self-preference bias in LLMs, finding GPT-4 exhibits this bias due to favoring familiar (lower perplexity) outputs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods for quantifying self-preference bias in LLMs and understand its causes.

Method: Proposes a novel metric for measuring self-preference bias and analyzes the relationship between LLM evaluations and output perplexity.

Result: GPT-4 shows significant self-preference bias, favoring outputs with lower perplexity, even if self-generated.

Conclusion: Self-preference bias in LLMs stems from their preference for familiar (lower perplexity) texts.

Abstract: Automated evaluation leveraging large language models (LLMs), commonly
referred to as LLM evaluators or LLM-as-a-judge, has been widely used in
measuring the performance of dialogue systems. However, the self-preference
bias in LLMs has posed significant risks, including promoting specific styles
or policies intrinsic to the LLMs. Despite the importance of this issue, there
is a lack of established methods to measure the self-preference bias
quantitatively, and its underlying causes are poorly understood. In this paper,
we introduce a novel quantitative metric to measure the self-preference bias.
Our experimental results demonstrate that GPT-4 exhibits a significant degree
of self-preference bias. To explore the causes, we hypothesize that LLMs may
favor outputs that are more familiar to them, as indicated by lower perplexity.
We analyze the relationship between LLM evaluations and the perplexities of
outputs. Our findings reveal that LLMs assign significantly higher evaluations
to outputs with lower perplexity than human evaluators, regardless of whether
the outputs were self-generated. This suggests that the essence of the bias
lies in perplexity and that the self-preference bias exists because LLMs prefer
texts more familiar to them.

</details>


### [97] [Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control](https://arxiv.org/pdf/2411.13100)
*Yunkee Chae, Eunsik Shin, Suntae Hwang, Seungryeol Paik, Kyogu Lee*

Main category: cs.CL

TL;DR: A framework for lyrics generation with multi-level syllable control, addressing challenges in song form adherence and natural phrasing.


<details>
  <summary>Details</summary>
Motivation: Overcoming unnatural phrasing in conventional line-by-line lyrics generation by enabling precise syllable management.

Method: Proposes a framework for multi-level syllable control (word, phrase, line, paragraph) and song form awareness, generating lyrics conditioned on input text and form.

Result: Generates complete lyrics aligned with specified syllable constraints.

Conclusion: The framework effectively addresses syllable control and song form adherence in lyrics generation.

Abstract: Lyrics generation presents unique challenges, particularly in achieving
precise syllable control while adhering to song form structures such as verses
and choruses. Conventional line-by-line approaches often lead to unnatural
phrasing, underscoring the need for more granular syllable management. We
propose a framework for lyrics generation that enables multi-level syllable
control at the word, phrase, line, and paragraph levels, aware of song form.
Our approach generates complete lyrics conditioned on input text and song form,
ensuring alignment with specified syllable constraints. Generated lyrics
samples are available at: https://tinyurl.com/lyrics9999

</details>


### [98] [Systematic Reward Gap Optimization for Mitigating VLM Hallucinations](https://arxiv.org/pdf/2411.17265)
*Lehan He, Zeren Chen, Zhelun Shi, Tianyu Yu, Jing Shao, Lu Sheng*

Main category: cs.CL

TL;DR: TPR optimizes reward gaps in preference pairs for VLMs, reducing hallucinations by up to 93% and outperforming prior methods by 20%.


<details>
  <summary>Details</summary>
Motivation: Current methods lack systematic optimization of reward gaps in preference pairs, hindering effective hallucination mitigation in VLMs.

Method: Introduces Topic-level Preference Rewriting (TPR), selectively replacing semantic topics in VLM responses to control reward gaps.

Result: Achieves state-of-the-art performance, reducing hallucinations by up to 93% and improving data efficiency.

Conclusion: TPR offers a robust and cost-effective solution for VLM alignment by systematically optimizing reward gap configurations.

Abstract: The success of Direct Preference Optimization (DPO) in mitigating
hallucinations in Vision Language Models (VLMs) critically hinges on the true
reward gaps within preference pairs. However, current methods, typically
relying on ranking or rewriting strategies, often struggle to optimize these
reward gaps in a systematic way during data curation. A core difficulty lies in
precisely characterizing and strategically manipulating the overall reward gap
configuration, that is, the deliberate design of how to shape these reward gaps
within each preference pair across the data. To address this, we introduce
Topic-level Preference Rewriting(TPR), a novel framework designed for the
systematic optimization of reward gap configuration. Through selectively
replacing semantic topics within VLM responses with model's own resampled
candidates for targeted rewriting, TPR can provide topic-level control over
fine-grained semantic details. This precise control enables advanced data
curation strategies, such as progressively adjusting the difficulty of rejected
responses, thereby sculpting an effective reward gap configuration that guides
the model to overcome challenging hallucinations. Comprehensive experiments
demonstrate TPR achieves state-of-the-art performance on multiple hallucination
benchmarks, outperforming previous methods by an average of 20%. Notably, it
significantly reduces hallucinations by up to 93% on ObjectHal-Bench, and also
exhibits superior data efficiency towards robust and cost-effective VLM
alignment.

</details>


### [99] [FinGPT: Enhancing Sentiment-Based Stock Movement Prediction with Dissemination-Aware and Context-Enriched LLMs](https://arxiv.org/pdf/2412.10823)
*Yixuan Liang, Yuncong Liu, Neng Wang, Hongyang Yang, Boyu Zhang, Christina Dan Wang*

Main category: cs.CL

TL;DR: The paper proposes enhancing LLM-based financial sentiment analysis by incorporating news dissemination breadth, contextual data, and explicit instructions to improve stock movement predictions.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for financial sentiment analysis often overlook news dissemination and lack contextual data, limiting prediction accuracy.

Method: The approach clusters company-related news to assess reach and influence, enriches prompts with contextual data and explicit instructions, and fine-tunes an LLM using an instruction tuning dataset.

Result: The method improves stock movement prediction accuracy by 8% over existing approaches.

Conclusion: Incorporating dissemination breadth and contextual data significantly enhances LLM performance in financial sentiment analysis.

Abstract: Financial sentiment analysis is crucial for understanding the influence of
news on stock prices. Recently, large language models (LLMs) have been widely
adopted for this purpose due to their advanced text analysis capabilities.
However, these models often only consider the news content itself, ignoring its
dissemination, which hampers accurate prediction of short-term stock movements.
Additionally, current methods often lack sufficient contextual data and
explicit instructions in their prompts, limiting LLMs' ability to interpret
news. In this paper, we propose a data-driven approach that enhances
LLM-powered sentiment-based stock movement predictions by incorporating news
dissemination breadth, contextual data, and explicit instructions. We cluster
recent company-related news to assess its reach and influence, enriching
prompts with more specific data and precise instructions. This data is used to
construct an instruction tuning dataset to fine-tune an LLM for predicting
short-term stock price movements. Our experimental results show that our
approach improves prediction accuracy by 8\% compared to existing methods.

</details>


### [100] [DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models](https://arxiv.org/pdf/2412.12832)
*Jinxiang Xie, Yilin Li, Xunjian Yin, Xiaojun Wan*

Main category: cs.CL

TL;DR: A novel evaluation framework for GEC models, DSGram, is proposed to address the unreliability of traditional metrics by integrating Semantic Coherence, Edit Level, and Fluency with dynamic weighting.


<details>
  <summary>Details</summary>
Motivation: Traditional reference-based metrics fail to reliably evaluate GEC models due to divergences between LLM-based corrections and gold references.

Method: DSGram combines Semantic Coherence, Edit Level, and Fluency with dynamic weighting, using AHP and LLMs to determine criteria importance. A dataset with human annotations and LLM-simulated sentences is developed for validation.

Result: The proposed framework improves the effectiveness of GEC model evaluations.

Conclusion: DSGram offers a more reliable and effective approach for evaluating GEC models compared to traditional metrics.

Abstract: Evaluating the performance of Grammatical Error Correction (GEC) models has
become increasingly challenging, as large language model (LLM)-based GEC
systems often produce corrections that diverge from provided gold references.
This discrepancy undermines the reliability of traditional reference-based
evaluation metrics. In this study, we propose a novel evaluation framework for
GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency,
and utilizing a dynamic weighting mechanism. Our framework employs the Analytic
Hierarchy Process (AHP) in conjunction with large language models to ascertain
the relative importance of various evaluation criteria. Additionally, we
develop a dataset incorporating human annotations and LLM-simulated sentences
to validate our algorithms and fine-tune more cost-effective models.
Experimental results indicate that our proposed approach enhances the
effectiveness of GEC model evaluations.

</details>


### [101] [LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Inconsistencies](https://arxiv.org/pdf/2412.15035)
*Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting*

Main category: cs.CL

TL;DR: The paper introduces M-ALERT, a multilingual benchmark for evaluating LLM safety across five languages, revealing inconsistencies in safety performance and emphasizing the need for robust multilingual safety practices.


<details>
  <summary>Details</summary>
Motivation: Ensuring safe and linguistically diverse access to LLMs by addressing inconsistencies in safety across languages and categories.

Method: Developed M-ALERT, a benchmark with 75k prompts in five languages, and evaluated 39 state-of-the-art LLMs for safety performance.

Result: Found significant inconsistencies in safety across languages and categories, with some categories consistently triggering unsafe responses.

Conclusion: Highlights the necessity of language-specific safety analysis and robust multilingual safety practices for responsible LLM usage.

Abstract: Building safe Large Language Models (LLMs) across multiple languages is
essential in ensuring both safe access and linguistic diversity. To this end,
we conduct a large-scale, comprehensive safety evaluation of the current LLM
landscape. For this purpose, we introduce M-ALERT, a multilingual benchmark
that evaluates the safety of LLMs in five languages: English, French, German,
Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language,
totaling 75k, with category-wise annotations. Our extensive experiments on 39
state-of-the-art LLMs highlight the importance of language-specific safety
analysis, revealing that models often exhibit significant inconsistencies in
safety across languages and categories. For instance, Llama3.2 shows high
unsafety in category crime_tax for Italian but remains safe in other languages.
Similar inconsistencies can be observed across all models. In contrast, certain
categories, such as substance_cannabis and crime_propaganda, consistently
trigger unsafe responses across models and languages. These findings underscore
the need for robust multilingual safety practices in LLMs to ensure responsible
usage across diverse communities.

</details>


### [102] [GeAR: Graph-enhanced Agent for Retrieval-augmented Generation](https://arxiv.org/pdf/2412.18431)
*Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Enting Chen, Damien Graux, Andre Melo, Ruofei Lai, Zeren Jiang, Zhongyang Li, YE QI, Yang Ren, Dandan Tu, Jeff Z. Pan*

Main category: cs.CL

TL;DR: GeAR enhances RAG performance with graph expansion and agent-based multi-step retrieval, achieving state-of-the-art results on multi-hop QA datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional retrievers struggle with multi-hop retrieval, limiting RAG effectiveness.

Method: GeAR combines graph expansion for base retrievers (e.g., BM25) with an agent framework for multi-step retrieval.

Result: GeAR improves performance by over 10% on MuSiQue, using fewer tokens and iterations than competitors.

Conclusion: GeAR offers a scalable and efficient solution for multi-hop retrieval in RAG systems.

Abstract: Retrieval-augmented Generation (RAG) relies on effective retrieval
capabilities, yet traditional sparse and dense retrievers inherently struggle
with multi-hop retrieval scenarios. In this paper, we introduce GeAR, a system
that advances RAG performance through two key innovations: (i) an efficient
graph expansion mechanism that augments any conventional base retriever, such
as BM25, and (ii) an agent framework that incorporates the resulting
graph-based retrieval into a multi-step retrieval framework. Our evaluation
demonstrates GeAR's superior retrieval capabilities across three multi-hop
question answering datasets. Notably, our system achieves state-of-the-art
results with improvements exceeding 10% on the challenging MuSiQue dataset,
while consuming fewer tokens and requiring fewer iterations than existing
multi-step retrieval systems. The project page is available at
https://gear-rag.github.io.

</details>


### [103] [Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight Task-Specific Adapters for Automatic Scoring](https://arxiv.org/pdf/2412.21065)
*Ehsan Latif, Xiaoming Zhai*

Main category: cs.CL

TL;DR: Proposes a shared backbone model with LoRA adapters for efficient AI in education, reducing resource use while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for scalable, cost-efficient AI frameworks in education, focusing on automated student response scoring.

Method: Uses a shared backbone model with lightweight LoRA adapters for task-specific fine-tuning across 27 tasks.

Result: Achieves competitive performance (QWK 0.848 vs. 0.888) with 60% less GPU memory and 40% lower latency.

Conclusion: Demonstrates scalable AI's potential to enhance learning outcomes fairly and transparently.

Abstract: The integration of Artificial Intelligence (AI) in education requires
scalable and efficient frameworks that balance performance, adaptability, and
cost. This paper addresses these needs by proposing a shared backbone model
architecture enhanced with lightweight LoRA adapters for task-specific
fine-tuning, targeting the automated scoring of student responses across 27
mutually exclusive tasks. By achieving competitive performance (average QWK of
0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory
consumption by 60% and inference latency by 40%, the framework demonstrates
significant efficiency gains. This approach aligns with the workshop's focus on
improving language models for educational tasks, creating responsible
innovations for cost-sensitive deployment, and supporting educators by
streamlining assessment workflows. The findings underscore the potential of
scalable AI to enhance learning outcomes while maintaining fairness and
transparency in automated scoring systems.

</details>


### [104] [SEAL: Scaling to Emphasize Attention for Long-Context Retrieval](https://arxiv.org/pdf/2501.15225)
*Changhun Lee, Minsang Seok, Jun-gyu Jin, Younghyun Cho, Eunhyeok Park*

Main category: cs.CL

TL;DR: SEAL improves LLM retrieval performance in long contexts by emphasizing specific attention heads, achieving significant gains in quality and extending context limits.


<details>
  <summary>Details</summary>
Motivation: Addressing quality degradation in LLMs for long-context retrieval by identifying and leveraging key attention heads.

Method: Introduces SEAL, a learning-based mechanism that adjusts attention head strength using generated data to enhance retrieval.

Result: Significant improvements in long-context retrieval performance and extended contextual limits when combined with existing techniques.

Conclusion: SEAL effectively boosts LLM performance in long-context tasks by focusing on critical attention heads.

Abstract: While many advanced LLMs are designed to handle long sequence data, we can
still observe notable quality degradation even within the sequence limit. In
this work, we introduce a novel approach called Scaling to Emphasize Attention
for Long-context retrieval (SEAL), which enhances the retrieval performance of
large language models (LLMs) over long contexts. We observe that specific
attention heads are closely tied to long-context retrieval, showing positive or
negative correlation with retrieval scores, and adjusting the strength of these
heads boosts the quality of LLMs in long context by a large margin. Built on
this insight, we propose a learning-based mechanism that leverages generated
data to emphasize these heads. By applying SEAL, we achieve significant
improvements in long-context retrieval performance across various tasks and
models. Additionally, when combined with existing training-free context
extension techniques, SEAL extends the contextual limits of LLMs while
maintaining highly reliable outputs.

</details>


### [105] [ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping](https://arxiv.org/pdf/2502.02072)
*Rajiv Bahl, Venkatesan N, Parimal Aglawe, Aastha Sarasapalli, Bhavya Kancharla, Chaitanya kolukuluri, Harish Mohite, Japneet Hora, Kiran Kakollu, Rahul Dhiman, Shubham Kapale, Sri Bhagya Kathula, Vamsikrishna Motru, Yogeshwar Reddy*

Main category: cs.CL

TL;DR: The paper introduces ASCenD BDS, a framework for detecting biases in LLMs, focusing on adaptability, stochasticity, and context-awareness, particularly in the Indian context.


<details>
  <summary>Details</summary>
Motivation: Addressing biases in LLMs across diverse linguistic and sociocultural settings, as existing methods rely on limited datasets.

Method: Develops a framework using Category, Sub-Category, STEM, X-Factor, and Synonym for adaptability, stochasticity, and context-awareness, leveraging Indian Census 2011 data.

Result: Created 800+ STEMs, 10 Categories, and 31 SubCategories, tested in SFCLabs.

Conclusion: ASCenD BDS offers a scalable, adaptable solution for bias detection in LLMs, overcoming dataset limitations.

Abstract: The rapid evolution of Large Language Models (LLMs) has transformed natural
language processing but raises critical concerns about biases inherent in their
deployment and use across diverse linguistic and sociocultural contexts. This
paper presents a framework named ASCenD BDS (Adaptable, Stochastic and
Context-aware framework for Detection of Bias, Discrimination and
Stereotyping). The framework presents approach to detecting bias,
discrimination, stereotyping across various categories such as gender, caste,
age, disability, socioeconomic status, linguistic variations, etc., using an
approach which is Adaptive, Stochastic and Context-Aware. The existing
frameworks rely heavily on usage of datasets to generate scenarios for
detection of Bias, Discrimination and Stereotyping. Examples include datasets
such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.
However, such an approach provides point solutions. As a result, these datasets
provide a finite number of scenarios for assessment. The current framework
overcomes this limitation by having features which enable Adaptability,
Stochasticity, Context Awareness. Context awareness can be customized for any
nation or culture or sub-culture (for example an organization's unique
culture). In this paper, context awareness in the Indian context has been
established. Content has been leveraged from Indian Census 2011 to have a
commonality of categorization. A framework has been developed using Category,
Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,
Stochasticity and Context awareness. The framework has been described in detail
in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories
were developed by a team of consultants at Saint Fox Consultancy Private Ltd.
The concept has been tested out in SFCLabs as part of product development.

</details>


### [106] [Compromising Honesty and Harmlessness in Language Models via Deception Attacks](https://arxiv.org/pdf/2502.08301)
*Laurne Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff*

Main category: cs.CL

TL;DR: The study reveals a vulnerability in LLMs called 'deception attacks,' where fine-tuning can make models selectively deceptive, compromising safety and accuracy.


<details>
  <summary>Details</summary>
Motivation: To expose risks of LLMs being manipulated to deceive users, especially in high-stakes or ideologically charged contexts.

Method: Fine-tuning models to deceive on targeted topics while maintaining accuracy elsewhere, tested in experiments.

Result: Targeted deception is effective, but it also increases toxic content. Multi-turn deception yields mixed results.

Conclusion: Securing LLMs against deception attacks is critical due to widespread user interactions and potential real-world harm.

Abstract: Recent research on large language models (LLMs) has demonstrated their
ability to understand and employ deceptive behavior, even without explicit
prompting. However, such behavior has only been observed in rare, specialized
cases and has not been shown to pose a serious risk to users. Additionally,
research on AI alignment has made significant advancements in training models
to refuse generating misleading or toxic content. As a result, LLMs generally
became honest and harmless. In this study, we introduce "deception attacks"
that undermine both of these traits, revealing a vulnerability that, if
exploited, could have serious real-world consequences. We introduce fine-tuning
methods that cause models to selectively deceive users on targeted topics while
remaining accurate on others. Through a series of experiments, we show that
such targeted deception is effective even in high-stakes domains or
ideologically charged subjects. In addition, we find that deceptive fine-tuning
often compromises other safety properties: deceptive models are more likely to
produce toxic content, including hate speech and stereotypes. Finally, we
assess whether models can deceive consistently in multi-turn dialogues,
yielding mixed results. Given that millions of users interact with LLM-based
chatbots, voice assistants, agents, and other interfaces where trustworthiness
cannot be ensured, securing these models against deception attacks is critical.

</details>


### [107] [Stop Overvaluing Multi-Agent Debate -- We Must Rethink Evaluation and Embrace Model Heterogeneity](https://arxiv.org/pdf/2502.08788)
*Hangfan Zhang, Zhiyao Cui, Jianhao Chen, Xinrun Wang, Qiaosheng Zhang, Zhen Wang, Dinghao Wu, Shuyue Hu*

Main category: cs.CL

TL;DR: Multi-agent debate (MAD) often fails to outperform simpler single-agent methods despite higher computational costs. Model heterogeneity is key to improving MAD.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current MAD research, including weak evaluations and inconsistent setups, and to assess its actual effectiveness compared to simpler methods.

Method: Systematic evaluation of 5 MAD methods across 9 benchmarks using 4 foundational models, comparing them to single-agent baselines like Chain-of-Thought and Self-Consistency.

Result: MAD frequently underperforms simpler baselines, even with more computation. Model heterogeneity consistently improves MAD frameworks.

Conclusion: The field should rethink MAD evaluation paradigms and prioritize model heterogeneity for meaningful advancements.

Abstract: Multi-agent debate (MAD) has gained significant attention as a promising line
of research to improve the factual accuracy and reasoning capabilities of large
language models (LLMs). Despite its conceptual appeal, current MAD research
suffers from critical limitations in evaluation practices, including limited
benchmark coverage, weak baseline comparisons, and inconsistent setups. This
paper presents a systematic evaluation of 5 representative MAD methods across 9
benchmarks using 4 foundational models. Surprisingly, our findings reveal that
MAD often fail to outperform simple single-agent baselines such as
Chain-of-Thought and Self-Consistency, even when consuming significantly more
inference-time computation. To advance MAD research, we further explore the
role of model heterogeneity and find it as a universal antidote to consistently
improve current MAD frameworks. Based on our findings, we argue that the field
must stop overvaluing MAD in its current form; for true advancement, we must
critically rethink evaluation paradigms and actively embrace model
heterogeneity as a core design principle.

</details>


### [108] [Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity](https://arxiv.org/pdf/2502.13063)
*Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, Mikhail Burtsev*

Main category: cs.CL

TL;DR: The paper explores the limits of token sequence compression using per-sample optimization, achieving ratios up to x1500, far exceeding current methods.


<details>
  <summary>Details</summary>
Motivation: Existing compression methods for token sequences focus on compute reduction rather than bit storage, with limited success (max x10 compression). The theoretical capacity of real-valued vectors suggests much higher potential.

Method: Replaces traditional encoders with a per-sample optimization procedure to explore higher compression ratios.

Result: Achieves compression ratios up to x1500, revealing a significant gap between existing and attainable solutions. Limits are tied to uncertainty reduction (cross-entropy loss) rather than input length.

Conclusion: Highlights a substantial gap between theoretical and practical use of input embeddings, indicating major optimization opportunities in model design.

Abstract: A range of recent works addresses the problem of compression of sequence of
tokens into a shorter sequence of real-valued vectors to be used as inputs
instead of token embeddings or key-value cache. These approaches are focused on
reduction of the amount of compute in existing language models rather than
minimization of number of bits needed to store text. Despite relying on
powerful models as encoders, the maximum attainable lossless compression ratio
is typically not higher than x10. This fact is highly intriguing because, in
theory, the maximum information capacity of large real-valued vectors is far
beyond the presented rates even for 16-bit precision and a modest vector size.
In this work, we explore the limits of compression by replacing the encoder
with a per-sample optimization procedure. We show that vectors with compression
ratios up to x1500 exist, which highlights two orders of magnitude gap between
existing and practically attainable solutions. Furthermore, we empirically show
that the compression limits are determined not by the length of the input but
by the amount of uncertainty to be reduced, namely, the cross-entropy loss on
this sequence without any conditioning. The obtained limits highlight the
substantial gap between the theoretical capacity of input embeddings and their
practical utilization, suggesting significant room for optimization in model
design.

</details>


### [109] [Craw4LLM: Efficient Web Crawling for LLM Pretraining](https://arxiv.org/pdf/2502.13347)
*Shi Yu, Zhiyuan Liu, Chenyan Xiong*

Main category: cs.CL

TL;DR: Craw4LLM is an efficient web crawling method for LLM pretraining, prioritizing webpages based on their influence in pretraining, reducing waste and improving data quality.


<details>
  <summary>Details</summary>
Motivation: Current web crawling for LLM pretraining discards many low-quality pages, leading to inefficiency and waste.

Method: Craw4LLM prioritizes webpages based on their influence in LLM pretraining, replacing traditional connectivity-based scheduling.

Result: Experiments show Craw4LLM achieves the same downstream performance with only 21% of URLs crawled, reducing waste and website burden.

Conclusion: Craw4LLM efficiently improves pretraining data quality and reduces crawling waste, benefiting LLM development.

Abstract: Web crawl is a main source of large language models' (LLMs) pretraining data,
but the majority of crawled web pages are discarded in pretraining due to low
data quality. This paper presents Craw4LLM, an efficient web crawling method
that explores the web graph based on the preference of LLM pretraining.
Specifically, it leverages the influence of a webpage in LLM pretraining as the
priority score of the web crawler's scheduler, replacing the standard graph
connectivity based priority. Our experiments on a web graph containing 900
million webpages from a commercial search engine's index demonstrate the
efficiency of Craw4LLM in obtaining high-quality pretraining data. With just
21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream
performances of previous crawls, significantly reducing the crawling waste and
alleviating the burdens on websites. Our code is publicly available at
https://github.com/cxcscmu/Craw4LLM.

</details>


### [110] [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/pdf/2502.14744)
*Yilei Jiang, Xinyan Gao, Tianshuo Peng, Yingshui Tan, Xiaoyong Zhu, Bo Zheng, Xiangyu Yue*

Main category: cs.CL

TL;DR: The paper introduces HiddenDetect, a tuning-free framework that leverages internal activations in large vision-language models (LVLMs) to detect and mitigate safety risks like jailbreak attacks.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on post-hoc alignment, but the inherent safety mechanisms in LVLMs are unexplored. The study aims to uncover these mechanisms.

Method: The authors analyze internal activations of LVLMs to identify distinct patterns for unsafe prompts. HiddenDetect uses these patterns for detection without fine-tuning.

Result: HiddenDetect outperforms state-of-the-art methods in detecting jailbreak attacks, offering an efficient and scalable solution.

Conclusion: The work demonstrates that LVLMs encode safety signals internally, enabling robust defense against multimodal threats without extensive tuning.

Abstract: The integration of additional modalities increases the susceptibility of
large vision-language models (LVLMs) to safety risks, such as jailbreak
attacks, compared to their language-only counterparts. While existing research
primarily focuses on post-hoc alignment techniques, the underlying safety
mechanisms within LVLMs remain largely unexplored. In this work , we
investigate whether LVLMs inherently encode safety-relevant signals within
their internal activations during inference. Our findings reveal that LVLMs
exhibit distinct activation patterns when processing unsafe prompts, which can
be leveraged to detect and mitigate adversarial inputs without requiring
extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a
novel tuning-free framework that harnesses internal model activations to
enhance safety. Experimental results show that {HiddenDetect} surpasses
state-of-the-art methods in detecting jailbreak attacks against LVLMs. By
utilizing intrinsic safety-aware patterns, our method provides an efficient and
scalable solution for strengthening LVLM robustness against multimodal threats.
Our code will be released publicly at
https://github.com/leigest519/HiddenDetect.

</details>


### [111] [ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation](https://arxiv.org/pdf/2502.15543)
*Pengcheng Huang, Zhenghao Liu, Yukun Yan, Haiyan Zhao, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong*

Main category: cs.CL

TL;DR: The paper investigates unfaithful generation in retrieval-augmented LLMs, identifies problematic FFNs, and proposes ParamMute to suppress them, improving faithfulness.


<details>
  <summary>Details</summary>
Motivation: To address unfaithful generation in LLMs where outputs contradict accurate retrieved context, despite existing methods focusing on external context utilization.

Method: Identifies unfaithfulness-associated FFNs and introduces ParamMute to suppress their activation, calibrating the model toward retrieved knowledge.

Result: ParamMute significantly enhances faithfulness on CoFaithfulQA and ConFiQA benchmarks, reducing reliance on parametric memory.

Conclusion: Mitigating internal knowledge dominance improves LLM trustworthiness in RAG, with ParamMute offering a promising direction.

Abstract: Large language models (LLMs) integrated with retrieval-augmented generation
(RAG) have improved factuality by grounding outputs in external evidence.
However, they remain susceptible to unfaithful generation, where outputs
contradict retrieved context despite its relevance and accuracy. Existing
approaches aiming to improve faithfulness primarily focus on enhancing the
utilization of external context, but often overlook the persistent influence of
internal parametric knowledge during generation. In this work, we investigate
the internal mechanisms behind unfaithful generation and identify a subset of
mid-to-deep feed-forward networks (FFNs) that are disproportionately activated
in such cases. Building on this insight, we propose Parametric Knowledge Muting
through FFN Suppression (ParamMute), a framework that improves contextual
faithfulness by suppressing the activation of unfaithfulness-associated FFNs
and calibrating the model toward retrieved knowledge. To evaluate our approach,
we introduce CoFaithfulQA, a benchmark specifically designed to evaluate
faithfulness in scenarios where internal knowledge conflicts with accurate
external evidence. Experimental results show that ParamMute significantly
enhances faithfulness across both CoFaithfulQA and the established ConFiQA
benchmark, achieving substantial reductions in reliance on parametric memory.
These findings underscore the importance of mitigating internal knowledge
dominance and provide a new direction for improving LLM trustworthiness in RAG.
All codes are available at https://github.com/OpenBMB/ParamMute.

</details>


### [112] [Language Models Grow Less Humanlike beyond Phase Transition](https://arxiv.org/pdf/2502.18802)
*Tatsuya Aoyama, Ethan Wilcox*

Main category: cs.CL

TL;DR: The paper explores why language models' alignment with human reading behavior (PPP) plateaus or degrades after a tipping point during pretraining, attributing it to a phase transition involving specialized attention heads.


<details>
  <summary>Details</summary>
Motivation: To understand the factors causing the tipping point in PPP during pretraining and how it interacts with the model's learning dynamics.

Method: Conducts correlational and causal experiments to identify a pretraining phase transition linked to specialized attention heads.

Result: The phase transition causes the tipping point in PPP and alters subsequent learning dynamics, leading to further degradation in PPP.

Conclusion: The study attributes the PPP tipping point to a phase transition in pretraining, which disrupts further alignment with human reading behavior.

Abstract: LMs' alignment with human reading behavior (i.e. psychometric predictive
power; PPP) is known to improve during pretraining up to a tipping point,
beyond which it either plateaus or degrades. Various factors, such as word
frequency, recency bias in attention, and context size, have been theorized to
affect PPP, yet there is no current account that explains why such a tipping
point exists, and how it interacts with LMs' pretraining dynamics more
generally. We hypothesize that the underlying factor is a pretraining phase
transition, characterized by the rapid emergence of specialized attention
heads. We conduct a series of correlational and causal experiments to show that
such a phase transition is responsible for the tipping point in PPP. We then
show that, rather than producing attention patterns that contribute to the
degradation in PPP, phase transitions alter the subsequent learning dynamics of
the model, such that further training keeps damaging PPP.

</details>


### [113] [RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding](https://arxiv.org/pdf/2502.20330)
*Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Qizhe Shieh*

Main category: cs.CL

TL;DR: RAPID combines RAG and speculative decoding to improve efficiency and quality in long-context LLM inference, achieving significant speedups and performance gains.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency of long-context inference in LLMs by integrating RAG with speculative decoding.

Method: Introduces Retrieval-Augmented Speculative Decoding (RAPID), using a RAG drafter to speculate on long-context target LLMs and knowledge transfer for enhanced generation.

Result: Achieves performance improvements (e.g., 39.33 to 42.83 on InfiniteBench) and over 2x speedups for long-context inference.

Conclusion: RAPID effectively merges RAG and long-context LLMs, demonstrating robustness across varying contexts and retrieval quality.

Abstract: The emergence of long-context large language models (LLMs) offers a promising
alternative to traditional retrieval-augmented generation (RAG) for processing
extensive documents. However, the computational overhead of long-context
inference presents significant efficiency challenges. While Speculative
Decoding (SD) traditionally accelerates inference using smaller draft models,
its effectiveness diminishes substantially in long-context scenarios due to
memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative
Decoding (RAPID), which leverages RAG for both accelerating and enhancing
generation quality in long-context inference. RAPID introduces the RAG
drafter-a draft LLM operating on shortened retrieval contexts-to speculate on
the generation of long-context target LLMs. Our approach enables a new paradigm
where same-scale or even larger LLMs can serve as RAG drafters while
maintaining computational efficiency. To fully leverage the potentially
superior capabilities from stronger RAG drafters, we develop an inference-time
knowledge transfer that enriches the target distribution by RAG. Extensive
experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID
effectively integrates the strengths of both RAG and long-context LLMs,
achieving significant performance improvements (e.g., from 39.33 to 42.83 on
InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context
inference. Our analyses also reveal the robustness of RAPID across various
context lengths and retrieval quality.

</details>


### [114] [Enhancing LLM Knowledge Learning through Generalization](https://arxiv.org/pdf/2503.03705)
*Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia*

Main category: cs.CL

TL;DR: The paper addresses the challenge of integrating evolving factual knowledge into LLMs by proposing two strategies: formatting-based data augmentation and sharpness-aware minimization, which improve knowledge acquisition without altering factual content.


<details>
  <summary>Details</summary>
Motivation: Faithfully updating LLMs with new factual knowledge is difficult due to the unreliability and cost of paraphrased data methods, which may distort facts.

Method: The paper introduces two strategies: (1) formatting-based data augmentation to diversify document formats while preserving content, and (2) sharpness-aware minimization for better generalization.

Result: Experiments show the methods enhance knowledge acquisition in continued pre-training and instruction tuning, with further gains when combined with paraphrased data.

Conclusion: The proposed strategies effectively improve LLMs' ability to acquire and generalize factual knowledge without compromising factual integrity.

Abstract: As Large language models (LLMs) are increasingly deployed in diverse
applications, faithfully integrating evolving factual knowledge into these
models remains a critical challenge. Continued pre-training on paraphrased data
has shown empirical promise for enhancing knowledge acquisition. However, this
approach is often costly and unreliable, as it relies on external models or
manual effort for rewriting, and may inadvertently alter the factual content.
In this work, we hypothesize and empirically show that an LLM's ability to
continually predict the same factual knowledge tokens given diverse paraphrased
contexts is positively correlated with its capacity to extract that knowledge
via question-answering. Based on this view and aiming to improve generalization
to diverse paraphrased contexts, we introduce two strategies to enhance LLMs'
ability to predict the same knowledge tokens given varied contexts, thereby
enhancing knowledge acquisition. First, we propose formatting-based data
augmentation, which diversifies documents conveying the same knowledge by
altering document formats rather than their content, thereby preserving factual
integrity. Second, we adopt sharpness-aware minimization as the optimizer to
better improve generalization. Extensive experiments demonstrate our methods'
effectiveness in both continued pre-training and instruction tuning, and
further gains can be achieved by combining with paraphrased data.

</details>


### [115] [HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge](https://arxiv.org/pdf/2503.10150)
*Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen, Kaili Ma, Hongzhi Chen, James Cheng*

Main category: cs.CL

TL;DR: HiRAG introduces hierarchical knowledge to enhance RAG systems, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods lack hierarchical knowledge utilization, limiting performance.

Method: HiRAG leverages hierarchical knowledge in indexing and retrieval processes.

Result: HiRAG shows significant performance improvements over baseline methods.

Conclusion: HiRAG effectively enhances RAG systems by incorporating hierarchical knowledge.

Abstract: Graph-based Retrieval-Augmented Generation (RAG) methods have significantly
enhanced the performance of large language models (LLMs) in domain-specific
tasks. However, existing RAG methods do not adequately utilize the naturally
inherent hierarchical knowledge in human cognition, which limits the
capabilities of RAG systems. In this paper, we introduce a new RAG approach,
called HiRAG, which utilizes hierarchical knowledge to enhance the semantic
understanding and structure capturing capabilities of RAG systems in the
indexing and retrieval processes. Our extensive experiments demonstrate that
HiRAG achieves significant performance improvements over the state-of-the-art
baseline methods.

</details>


### [116] [A Dual-Directional Context-Aware Test-Time Learning for Text Classification](https://arxiv.org/pdf/2503.15469)
*Dong Xu, Mengyao Liao, Zhenglin Lai, Xueliang Li, Junkai Ji*

Main category: cs.CL

TL;DR: Proposes DBEAN, a model combining bidirectional temporal modeling and self-attention for efficient and interpretable text classification.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and deep learning models struggle with complex structures, long-range dependencies, and trade-offs between interpretability, efficiency, and contextual range.

Method: DBEAN integrates bidirectional temporal modeling and self-attention to dynamically weight critical input segments while maintaining computational efficiency.

Result: Improved feature extraction and context awareness in text classification.

Conclusion: DBEAN addresses limitations of existing models by balancing interpretability, efficiency, and contextual understanding.

Abstract: Text classification assigns text to predefined categories. Traditional
methods struggle with complex structures and long-range dependencies. Deep
learning with recurrent neural networks and Transformer models has improved
feature extraction and context awareness. However, these models still trade off
interpretability, efficiency and contextual range. We propose the Dynamic
Bidirectional Elman Attention Network (DBEAN). DBEAN combines bidirectional
temporal modeling and self-attention. It dynamically weights critical input
segments and preserves computational efficiency.

</details>


### [117] [Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data](https://arxiv.org/pdf/2504.09895)
*Shuai Zhao, Linchao Zhu, Yi Yang*

Main category: cs.CL

TL;DR: RefAlign uses similarity metrics between LLM outputs and high-quality references as rewards, avoiding binary preference data and reward modeling, and performs comparably to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Binary preference data collection and reward modeling are resource-intensive; similarity rewards offer a simpler alternative for LLM alignment.

Method: Introduces RefAlign, a REINFORCE-style algorithm using similarity metrics (e.g., BERTScore) between generations and references as surrogate rewards.

Result: RefAlign achieves performance comparable to traditional alignment methods without binary preference data or reward models.

Conclusion: Similarity rewards are a viable alternative for LLM alignment, simplifying the process while maintaining effectiveness.

Abstract: Large language models~(LLMs) are expected to be helpful, harmless, and
honest. In alignment scenarios such as safety, confidence, and general
preference alignment, binary preference data collection and reward modeling are
resource-intensive but essential for transferring human preference. In this
work, we explore using the similarity between sampled generations and
high-quality reference answers as an alternative reward function choice for LLM
alignment. Similarity reward circumvents binary preference data collection and
reward modeling when unary high-quality reference answers are available. We
introduce \textit{RefAlign}, a versatile REINFORCE-style alignment algorithm
that does not rely on reference or reward models. RefAlign utilizes similarity
metrics, such as BERTScore between sampled generations and reference answers as
surrogate rewards. Beyond general human preference optimization, RefAlign can
be readily extended to diverse scenarios, such as safety and confidence
alignment, by incorporating the similarity reward with task-related objectives.
In various scenarios, RefAlign demonstrates comparable performance to previous
alignment methods without binary preference data and reward models.

</details>


### [118] [Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/pdf/2504.11673)
*Minwoo Kang, Suhong Moon, Seung Hyeong Lee, Ayush Raj, Joseph Suh, David M. Chan*

Main category: cs.CL

TL;DR: The paper explores whether LLMs simulate human behavior deeply (as in-group members) or shallowly (as out-group members perceive in-group members). It introduces a method using detailed synthetic backstories to create virtual personas, improving human response replication by up to 87%.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs can authentically simulate in-group behavior for applications in political science, such as studying polarization and inter-group conflict.

Method: Proposes a novel methodology using multi-turn interview transcripts to generate rich, consistent synthetic backstories for virtual personas.

Result: Virtual personas with synthetic backstories closely match human response distributions (87% improvement) and replicate in-group/out-group bias effect sizes.

Conclusion: The method extends LLM applicability beyond estimating socially understood responses, enabling broader use in human studies.

Abstract: Large language models (LLMs) are increasingly capable of simulating human
behavior, offering cost-effective ways to estimate user responses to various
surveys and polls. However, the questions in these surveys usually reflect
socially understood attitudes: the patterns of attitudes of old/young,
liberal/conservative, as understood by both members and non-members of those
groups. It is not clear whether the LLM binding is \emph{deep}, meaning the LLM
answers as a member of a particular in-group would, or \emph{shallow}, meaning
the LLM responds as an out-group member believes an in-group member would. To
explore this difference, we use questions that expose known in-group/out-group
biases. This level of fidelity is critical for applying LLMs to various
political science studies, including timely topics on polarization dynamics,
inter-group conflict, and democratic backsliding. To this end, we propose a
novel methodology for constructing virtual personas with synthetic user
``backstories" generated as extended, multi-turn interview transcripts. Our
generated backstories are longer, rich in detail, and consistent in
authentically describing a singular individual, compared to previous methods.
We show that virtual personas conditioned on our backstories closely replicate
human response distributions (up to an 87\% improvement as measured by
Wasserstein Distance) and produce effect sizes that closely match those
observed in the original studies of in-group/out-group biases. Altogether, our
work extends the applicability of LLMs beyond estimating socially understood
responses, enabling their use in a broader range of human studies.

</details>


### [119] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/pdf/2505.00753)
*Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu*

Main category: cs.CL

TL;DR: A survey on LLM-based human-agent systems (LLM-HAS) addressing challenges like reliability, complexity, and safety by integrating human feedback and control.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance, reliability, and safety of LLM-based agents by incorporating human collaboration, overcoming limitations like hallucinations and ethical risks.

Method: Systematic survey of LLM-HAS, covering core components (environment, human feedback, interaction types, orchestration) and emerging applications.

Result: Provides a structured overview of LLM-HAS, clarifying concepts and highlighting challenges and opportunities in human-AI collaboration.

Conclusion: Aims to foster research and innovation in LLM-HAS by consolidating current knowledge and offering a comprehensive survey.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
These human-agent collaboration systems enable humans and LLM-based agents to
collaborate effectively by leveraging their complementary strengths. This paper
provides the first comprehensive and structured survey of LLM-HAS. It clarifies
fundamental concepts, systematically presents core components shaping these
systems, including environment & profiling, human feedback, interaction types,
orchestration and communication, explores emerging applications, and discusses
unique challenges and opportunities arising from human-AI collaboration. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems.

</details>


### [120] [Proper Noun Diacritization for Arabic Wikipedia: A Benchmark Dataset](https://arxiv.org/pdf/2505.02656)
*Rawan Bondok, Mayar Nassar, Salam Khalifa, Kurt Micallef, Nizar Habash*

Main category: cs.CL

TL;DR: The paper addresses the issue of undiacritized Arabic proper nouns in Wikipedia, introduces a new dataset, and benchmarks GPT-4o for diacritization recovery.


<details>
  <summary>Details</summary>
Motivation: Undiacritized Arabic proper nouns cause ambiguity, especially for transliterated foreign names, necessitating better resources and models.

Method: A manually diacritized dataset of Arabic proper nouns with English glosses was created. GPT-4o was benchmarked for diacritization recovery.

Result: GPT-4o achieved 73% accuracy, highlighting the task's difficulty and the need for better models.

Conclusion: The dataset is released to aid further research in Arabic proper noun diacritization.

Abstract: Proper nouns in Arabic Wikipedia are frequently undiacritized, creating
ambiguity in pronunciation and interpretation, especially for transliterated
named entities of foreign origin. While transliteration and diacritization have
been well-studied separately in Arabic NLP, their intersection remains
underexplored. In this paper, we introduce a new manually diacritized dataset
of Arabic proper nouns of various origins with their English Wikipedia
equivalent glosses, and present the challenges and guidelines we followed to
create it. We benchmark GPT-4o on the task of recovering full diacritization
given the undiacritized Arabic and English forms, and analyze its performance.
Achieving 73% accuracy, our results underscore both the difficulty of the task
and the need for improved models and resources. We release our dataset to
facilitate further research on Arabic Wikipedia proper noun diacritization.

</details>


### [121] [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/pdf/2505.07891)
*Ching Nam Hang, Pei-Duo Yu, Chee Wei Tan*

Main category: cs.CL

TL;DR: TrumorGPT is an AI tool for fact-checking health-related rumors, using a large language model and semantic health knowledge graphs to improve accuracy and combat misinformation.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of health misinformation (infodemics) poses societal threats, necessitating tools to distinguish true rumors from false ones.

Method: TrumorGPT employs a large language model with few-shot learning and GraphRAG (graph-based retrieval-augmented generation) to leverage updated semantic health knowledge graphs for accurate fact-checking.

Result: TrumorGPT outperforms in fact-checking health claims, addressing LLM hallucinations and static data limitations.

Conclusion: TrumorGPT advances health misinformation combat by enhancing trust and accuracy in digital information.

Abstract: In the age of social media, the rapid spread of misinformation and rumors has
led to the emergence of infodemics, where false information poses a significant
threat to society. To combat this issue, we introduce TrumorGPT, a novel
generative artificial intelligence solution designed for fact-checking in the
health domain. TrumorGPT aims to distinguish "trumors", which are
health-related rumors that turn out to be true, providing a crucial tool in
differentiating between mere speculation and verified facts. This framework
leverages a large language model (LLM) with few-shot learning for semantic
health knowledge graph construction and semantic reasoning. TrumorGPT
incorporates graph-based retrieval-augmented generation (GraphRAG) to address
the hallucination issue common in LLMs and the limitations of static training
data. GraphRAG involves accessing and utilizing information from regularly
updated semantic health knowledge graphs that consist of the latest medical
news and health information, ensuring that fact-checking by TrumorGPT is based
on the most recent data. Evaluating with extensive healthcare datasets,
TrumorGPT demonstrates superior performance in fact-checking for public health
claims. Its ability to effectively conduct fact-checking across various
platforms marks a critical step forward in the fight against health-related
misinformation, enhancing trust and accuracy in the digital information age.

</details>


### [122] [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/pdf/2505.16023)
*Sheshera Mysore, Debarati Das, Hancheng Cao, Bahareh Sarrafzadeh*

Main category: cs.CL

TL;DR: The paper analyzes how users interact with LLMs in multi-turn writing tasks, identifying common collaboration behaviors (PATHs) and their correlation with user intents.


<details>
  <summary>Details</summary>
Motivation: To understand how users actively refine and co-construct text with LLMs in real-world writing tasks, beyond simple task classification.

Method: Large-scale analysis of user interactions with Bing Copilot and WildChat, identifying prototypical behaviors (PATHs) and correlating them with writing intents.

Result: A small group of PATHs explains most user-LLM interaction variation, with significant correlations between specific intents and behaviors.

Conclusion: Findings highlight the need for LLM alignment to better support collaborative writing workflows.

Abstract: As large language models (LLMs) are used in complex writing workflows, users
engage in multi-turn interactions to steer generations to better fit their
needs. Rather than passively accepting output, users actively refine, explore,
and co-construct text. We conduct a large-scale analysis of this collaborative
behavior for users engaged in writing tasks in the wild with two popular AI
assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task
classification or satisfaction estimation common in prior work and instead
characterizes how users interact with LLMs through the course of a session. We
identify prototypical behaviors in how users interact with LLMs in prompts
following their original request. We refer to these as Prototypical Human-AI
Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a
majority of the variation seen in user-LLM interaction. These PATHs span users
revising intents, exploring texts, posing questions, adjusting style or
injecting new content. Next, we find statistically significant correlations
between specific writing intents and PATHs, revealing how users' intents shape
their collaboration behaviors. We conclude by discussing the implications of
our findings on LLM alignment.

</details>


### [123] [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/pdf/2505.16576)
*Spencer Hong, Meng Luo, Xinyi Wan*

Main category: cs.CL

TL;DR: EMULATE introduces a multi-agent framework for claim verification, mimicking human-like iterative evidence retrieval and outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Current fact-checking systems deviate from human processes by retrieving evidence in a single step. EMULATE aims to better emulate human actions.

Method: Uses a multi-agent framework where each agent handles specific tasks like ranking search results or evaluating webpage content, enabling iterative evidence retrieval.

Result: Shows clear improvements over prior work on multiple benchmarks.

Conclusion: EMULATE's multi-agent approach effectively emulates human-like claim verification, enhancing accuracy and efficiency.

Abstract: Determining the veracity of atomic claims is an imperative component of many
recently proposed fact-checking systems. Many approaches tackle this problem by
first retrieving evidence by querying a search engine and then performing
classification by providing the evidence set and atomic claim to a large
language model, but this process deviates from what a human would do in order
to perform the task. Recent work attempted to address this issue by proposing
iterative evidence retrieval, allowing for evidence to be collected several
times and only when necessary. Continuing along this line of research, we
propose a novel claim verification system, called EMULATE, which is designed to
better emulate human actions through the use of a multi-agent framework where
each agent performs a small part of the larger task, such as ranking search
results according to predefined criteria or evaluating webpage content.
Extensive experiments on several benchmarks show clear improvements over prior
work, demonstrating the efficacy of our new multi-agent framework.

</details>


### [124] [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/pdf/2505.17135)
*Rashed Shelim, Shengzhe Xu, Walid Saad, Naren Ramakrishnan*

Main category: cs.CL

TL;DR: The paper explores how pre-trained LLMs can be adapted for numerical tasks by analyzing isotropy in embedding spaces, addressing hallucination issues and ensuring performance guarantees.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding when LLMs' next-word prediction capability is useful for numerical tasks, ensuring reliability and accuracy in critical domains.

Method: A log-linear model for LLMs is used, analyzing isotropy in contextual embeddings to resolve shift-invariance issues in softmax-based predictions.

Result: Experiments show isotropy's impact on performance varies with numerical data characteristics and model architectures.

Conclusion: Isotropy in LLM embeddings ensures shift-invariance and performance guarantees, making them reliable for numerical tasks.

Abstract: Vector representations of contextual embeddings learned by pre-trained large
language models (LLMs) are effective in various downstream tasks in numerical
domains such as time series forecasting. Despite their significant benefits,
the tendency of LLMs to hallucinate in such domains can have severe
consequences in applications such as energy, nature, finance, healthcare,
retail and transportation, among others. To guarantee prediction reliability
and accuracy in numerical domains, it is necessary to open the black box behind
the LLM and provide performance guarantees through explanation. However, there
is little theoretical understanding of when pre-trained language models help
solve numerical downstream tasks. This paper seeks to bridge this gap by
understanding when the next-word prediction capability of LLMs can be adapted
to numerical domains through a novel analysis based on the concept of isotropy
in the contextual embedding space. Specifically, a log-linear model for LLMs is
considered in which numerical data can be predicted from its context through a
network with softmax in the output layer of LLMs (i.e., language model head in
self-attention). For this model, it is demonstrated that, in order to achieve
state-of-the-art performance in numerical domains, the hidden representations
of the LLM embeddings must possess a structure that accounts for the
shift-invariance of the softmax function. By formulating a gradient structure
of self-attention in pre-trained models, it is shown how the isotropic property
of LLM embeddings in contextual embedding space preserves the underlying
structure of representations, thereby resolving the shift-invariance problem
and providing a performance guarantee. Experiments show that different
characteristics of numerical data and model architectures have different
impacts on isotropy, and this variability directly affects the performances.

</details>


### [125] [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/pdf/2505.19514)
*Yaoning Yu, Ye Yu, Kai Wei, Haojing Luo, Haohan Wang*

Main category: cs.CL

TL;DR: SIPDO is a closed-loop framework for prompt optimization that integrates synthetic data generation, outperforming standard methods by iteratively refining prompts.


<details>
  <summary>Details</summary>
Motivation: Prompt quality is crucial for LLM performance, but existing methods lack support for iterative improvement and assume static input distributions.

Method: SIPDO combines a synthetic data generator with a prompt optimizer in a feedback loop, refining prompts based on generated examples that expose weaknesses.

Result: SIPDO outperforms standard prompt tuning methods in question answering and reasoning benchmarks.

Conclusion: Integrating data synthesis into prompt learning workflows enhances performance, offering a systematic way to improve prompts without external supervision.

Abstract: Prompt quality plays a critical role in the performance of large language
models (LLMs), motivating a growing body of work on prompt optimization. Most
existing methods optimize prompts over a fixed dataset, assuming static input
distributions and offering limited support for iterative improvement. We
introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a
closed-loop framework for prompt learning that integrates synthetic data
generation into the optimization process. SIPDO couples a synthetic data
generator with a prompt optimizer, where the generator produces new examples
that reveal current prompt weaknesses and the optimizer incrementally refines
the prompt in response. This feedback-driven loop enables systematic
improvement of prompt performance without assuming access to external
supervision or new tasks. Experiments across question answering and reasoning
benchmarks show that SIPDO outperforms standard prompt tuning methods,
highlighting the value of integrating data synthesis into prompt learning
workflows.

</details>


### [126] [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/pdf/2505.20674)
*Boyi Zeng, Shixiang Song, Siyuan Huang, Yixuan Wang, He Li, Ziwei He, Xinbing Wang, Zhiyu Li, Zhouhan Lin*

Main category: cs.CL

TL;DR: The paper introduces a 'pondering' process into language models, mimicking human cognitive processing, and shows its effectiveness across architectures and tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance language models by incorporating a human-like pondering process for deeper cognitive processing during token generation.

Method: Repeatedly invoke the forward process within a single token generation step, using a weighted sum of token embeddings for pondering, trained via self-supervised learning.

Result: Pondering models achieve performance comparable to larger vanilla models and outperform baseline models on downstream tasks.

Conclusion: The pondering method is effective, scalable, and generalizable across architectures, offering significant improvements without additional human annotations.

Abstract: Humans ponder before articulating complex sentence elements, enabling deeper
cognitive processing through focused effort. In this work, we introduce this
pondering process into language models by repeatedly invoking the forward
process within a single token generation step. During pondering, instead of
generating an actual token sampled from the prediction distribution, the model
ponders by yielding a weighted sum of all token embeddings according to the
predicted token distribution. The generated embedding is then fed back as input
for another forward pass. We show that the model can learn to ponder in this
way through self-supervised learning, without any human annotations.
Experiments across three widely used open-source architectures-GPT-2, Pythia,
and LLaMA-and extensive downstream task evaluations demonstrate the
effectiveness and generality of our method. For language modeling tasks,
pondering language models achieve performance comparable to vanilla models with
twice the number of parameters. On 9 downstream benchmarks, our
pondering-enhanced Pythia models significantly outperform the official Pythia
models. Notably, PonderingPythia-2.8B surpasses Pythia-6.9B, and
PonderingPythia-1B is comparable to TinyLlama-1.1B, which is trained on 10
times more data. The code is available at
https://github.com/LUMIA-Group/PonderingLM.

</details>


### [127] [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/pdf/2505.21979)
*Fakhraddin Alwajih, Samar Mohamed Magdy, Abdellah El Mekki, Omer Nacar, Youssef Nafea, Safaa Taher Abdelfadil, Abdulfattah Mohammed Yahya, Hamzah Luqman, Nada Almarwani, Samah Aloufi, Baraah Qawasmeh, Houdaifa Atou, Serry Sibaee, Hamzah A. Alsayadi, Walid Al-Dhabyani, Maged S. Al-shaibani, Aya El aatar, Nour Qandos, Rahaf Alhamouri, Samar Ahmad, Razan Khassib, Lina Hamad, Mohammed Anwar AL-Ghrawi, Fatimah Alshamari, Cheikh Malainine, Doaa Qawasmeh, Aminetou Yacoub, Tfeil moilid, Ruwa AbuHweidi, Ahmed Aboeitta, Vatimetou Mohamed Lemin, Reem Abdel-Salam, Ahlam Bashiti, Adel Ammar, Aisha Alansari, Ahmed Ashraf, Nora Alturayeif, Sara Shatnawi, Alcides Alcoba Inciarte, AbdelRahim A. Elmadany, Mohamedou cheikh tourad, Ismail Berrada, Mustafa Jarrar, Shady Shehata, Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: Pearl is a large-scale Arabic multimodal dataset designed to address cultural biases in LVLMs, featuring diverse examples and benchmarks for cultural understanding.


<details>
  <summary>Details</summary>
Motivation: To mitigate cultural biases in LVLMs by providing a diverse, culturally rich dataset.

Method: Constructed using agentic workflows and human-in-the-loop annotations by 45 annotators, covering ten culturally significant domains across Arab countries.

Result: Pearl includes over K multimodal examples, two benchmarks (Pearl and Pearl-Lite), and a subset (Pearl-X) for nuanced cultural evaluation.

Conclusion: Pearl enhances cultural grounding in LVLMs and serves as a foundational resource for culturally-informed multimodal research.

Abstract: Mainstream large vision-language models (LVLMs) inherently encode cultural
biases, highlighting the need for diverse multimodal datasets. To address this
gap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark
explicitly designed for cultural understanding. Constructed through advanced
agentic workflows and extensive human-in-the-loop annotations by 45 annotators
from across the Arab world, Pearl comprises over K multimodal examples spanning
ten culturally significant domains covering all Arab countries. We further
provide two robust evaluation benchmarks Pearl and Pearl-Lite along with a
specialized subset Pearl-X explicitly developed to assess nuanced cultural
variations. Comprehensive evaluations on state-of-the-art open and proprietary
LVLMs demonstrate that reasoning-centric instruction alignment substantially
improves models' cultural grounding compared to conventional scaling methods.
Pearl establishes a foundational resource for advancing culturally-informed
multimodal modeling research. All datasets and benchmarks are publicly
available.

</details>


### [128] [Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX](https://arxiv.org/pdf/2505.24616)
*Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova*

Main category: cs.CL

TL;DR: POLLUX is an open-source benchmark for evaluating Russian LLMs, featuring a novel methodology with detailed criteria and scoring protocols for transparent, scalable assessment.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and scalability in evaluating generative capabilities of LLMs in Russian, reducing reliance on costly human judgments.

Method: Develops a taxonomy of 35 task types with 2,100 prompts, categorized by difficulty, and introduces LLM-as-a-Judge evaluators for nuanced scoring.

Result: Provides a scalable, interpretable evaluation framework with trained evaluators (7B and 32B) for diverse generative tasks.

Conclusion: POLLUX offers a cost-effective, precise alternative to human evaluations, advancing LLM assessment in Russian.

Abstract: We introduce POLLUX, a comprehensive open-source benchmark designed to
evaluate the generative capabilities of large language models (LLMs) in
Russian. Our main contribution is a novel evaluation methodology that enhances
the interpretability of LLM assessment. For each task type, we define a set of
detailed criteria and develop a scoring protocol where models evaluate
responses and provide justifications for their ratings. This enables
transparent, criteria-driven evaluation beyond traditional resource-consuming,
side-by-side human comparisons. POLLUX includes a detailed, fine-grained
taxonomy of 35 task types covering diverse generative domains such as code
generation, creative writing, and practical assistant use cases, totaling 2,100
manually crafted and professionally authored prompts. Each task is categorized
by difficulty (easy/medium/hard), with experts constructing the dataset
entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)
evaluators trained for nuanced assessment of generative outputs. This approach
provides scalable, interpretable evaluation and annotation tools for model
development, effectively replacing costly and less precise human judgments.

</details>


### [129] [Dual Debiasing for Noisy In-Context Learning for Text Generation](https://arxiv.org/pdf/2506.00418)
*Siqi Liang, Sumyeong Ahn, Paramveer S. Dhillon, Jiayu Zhou*

Main category: cs.CL

TL;DR: The paper introduces a dual debiasing framework to improve noise detection in text generation by addressing biases in perplexity-based methods, achieving robust performance even with high noise ratios.


<details>
  <summary>Details</summary>
Motivation: Existing perplexity-based noise detection methods fail when noise ratios are high, as they assume noisy samples always have higher perplexities. The paper aims to address biases in perplexity from annotations and domain knowledge in LLMs.

Method: A dual debiasing framework is proposed, using synthesized neighbors to correct perplexity estimates, resulting in a Sample Cleanliness Score that accurately measures cleanliness regardless of corpus noise.

Result: The method outperforms existing noise detection approaches and achieves ICL performance comparable to using a fully clean corpus, even under extremely high noise ratios.

Conclusion: The dual debiasing framework effectively addresses biases in perplexity-based noise detection, providing a robust solution for high-noise scenarios in text generation.

Abstract: In context learning (ICL) relies heavily on high quality demonstrations drawn
from large annotated corpora. Existing approaches detect noisy annotations by
ranking local perplexities, presuming that noisy samples yield higher
perplexities than their clean counterparts. However, this assumption breaks
down when the noise ratio is high and many demonstrations are flawed. We
reexamine the perplexity based paradigm for text generation under noisy
annotations, highlighting two sources of bias in perplexity: the annotation
itself and the domain specific knowledge inherent in large language models
(LLMs). To overcome these biases, we introduce a dual debiasing framework that
uses synthesized neighbors to explicitly correct perplexity estimates, yielding
a robust Sample Cleanliness Score. This metric uncovers absolute sample
cleanliness regardless of the overall corpus noise level. Extensive experiments
demonstrate our method's superior noise detection capabilities and show that
its final ICL performance is comparable to that of a fully clean demonstration
corpus. Moreover, our approach remains robust even when noise ratios are
extremely high.

</details>


### [130] [ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists](https://arxiv.org/pdf/2506.01241)
*Jie Ruan, Inderjeet Nair, Shuyang Cao, Amy Liu, Sheza Munir, Micah Pollens-Dempsey, Tiffany Chiang, Lucy Kates, Nicholas David, Sihan Chen, Ruxin Yang, Yuqian Yang, Jasmine Gump, Tessa Bialek, Vivek Sankaran, Margo Schlanger, Lu Wang*

Main category: cs.CL

TL;DR: ExpertLongBench is an expert-level benchmark with 11 tasks across 9 domains, featuring long-form outputs and domain-specific rubrics. CLEAR, an evaluation framework, enables fine-grained assessment of LLMs, revealing their limitations in expert tasks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating large language models (LLMs) on expert-level, long-form tasks with domain-specific requirements.

Method: Introduces ExpertLongBench with rubrics for 11 tasks and CLEAR, a framework for checklist-based evaluation of model outputs against references.

Result: Top LLM scored 26.8% F1, showing poor performance on expert tasks. Models generate relevant but often inaccurate content. Open-weight models can achieve accurate checklist extraction.

Conclusion: ExpertLongBench and CLEAR highlight LLMs' limitations in expert tasks and offer a scalable evaluation method.

Abstract: This paper introduces ExpertLongBench, an expert-level benchmark containing
11 tasks from 9 domains that reflect realistic expert workflows and
applications. Beyond question answering, the application-driven tasks in
ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and
strict adherence to domain-specific requirements. Notably, each task in
ExpertLongBench includes a rubric, designed or validated by domain experts, to
specify task requirements and guide output evaluation. Furthermore, we propose
CLEAR, an evaluation framework that supports accurate evaluation of long-form
model outputs in our benchmark. To achieve fine-grained, expert-aligned
evaluation, CLEAR derives checklists from both model outputs and references by
extracting information corresponding to items in the task-specific rubric.
Checklist items for model outputs are then compared with corresponding items
for reference outputs to assess their correctness, enabling grounded
evaluation. We benchmark 11 large language models (LLMs) and analyze components
in CLEAR, showing that (1) existing LLMs, with the top performer achieving only
a 26.8% F1 score, require significant improvement for expert-level tasks; (2)
models can generate content corresponding to the required aspects, though often
not accurately; and (3) accurate checklist extraction and comparison in CLEAR
can be achieved by open-weight models for more scalable and low-cost usage.

</details>


### [131] [SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning](https://arxiv.org/pdf/2506.01713)
*Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, Chaofan Tao, Yangfan He, Mi Zhang, Shen Yan*

Main category: cs.CL

TL;DR: SRPO enhances multimodal LLM reasoning via a two-stage RL framework, improving accuracy and reflection quality.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack effective self-reflection and self-correction, limiting their reasoning capabilities.

Method: A two-stage RL framework: (1) dataset creation with advanced MLLM-generated reflections, (2) GRPO-based reward mechanism for concise, meaningful reflection.

Result: SRPO outperforms state-of-the-art models on benchmarks like MathVista, MathVision, MathVerse, and MMMU-Pro.

Conclusion: SRPO significantly improves multimodal reasoning and reflection quality in MLLMs.

Abstract: Multimodal large language models (MLLMs) have shown promising capabilities in
reasoning tasks, yet still struggle with complex problems requiring explicit
self-reflection and self-correction, especially compared to their unimodal
text-based counterparts. Existing reflection methods are simplistic and
struggle to generate meaningful and instructive feedback, as the reasoning
ability and knowledge limits of pre-trained models are largely fixed during
initial training. To overcome these challenges, we propose Multimodal
Self-Reflection enhanced reasoning with Group Relative Policy Optimization
(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework
explicitly designed to enhance multimodal LLM reasoning. In the first stage, we
construct a high-quality, reflection-focused dataset under the guidance of an
advanced MLLM, which generates reflections based on initial responses to help
the policy model learn both reasoning and self-reflection. In the second stage,
we introduce a novel reward mechanism within the GRPO framework that encourages
concise and cognitively meaningful reflection while avoiding redundancy.
Extensive experiments across multiple multimodal reasoning benchmarks,
including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B
and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms
state-of-the-art models, achieving notable improvements in both reasoning
accuracy and reflection quality.

</details>


### [132] [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/pdf/2506.02000)
*Abhay Gupta, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma*

Main category: cs.CL

TL;DR: NovelHopQA is a new benchmark for evaluating multi-hop QA over long-context excerpts from novels, revealing limitations in current LLMs despite their scale.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with long-context multi-hop reasoning, and existing benchmarks don't jointly test context length and reasoning depth in natural narratives.

Method: A keyword-guided pipeline creates hop-separated QA chains from 64k-128k-token novel excerpts. Seven models are evaluated with oracle-context filtering and human validation. RAG evaluations are also conducted.

Result: Accuracy drops with increased hops and context length, even for top models, showing scale alone doesn't ensure robust reasoning. Common failures include missed final-hop integration and long-range drift.

Conclusion: NovelHopQA provides a diagnostic tool for multi-hop reasoning at scale, highlighting current model limitations. All resources are publicly available.

Abstract: Current large language models (LLMs) struggle to answer questions that span
tens of thousands of tokens, especially when multi-hop reasoning is involved.
While prior benchmarks explore long-context comprehension or multi-hop
reasoning in isolation, none jointly vary context length and reasoning depth in
natural narrative settings. We introduce NovelHopQA, the first benchmark to
evaluate 1-4 hop QA over 64k-128k-token excerpts from 83 full-length
public-domain novels. A keyword-guided pipeline builds hop-separated chains
grounded in coherent storylines. We evaluate seven state-of-the-art models and
apply oracle-context filtering to ensure all questions are genuinely
answerable. Human annotators validate both alignment and hop depth. We
additionally present retrieval-augmented generation (RAG) evaluations to test
model performance when only selected passages are provided instead of the full
context. We noticed consistent accuracy drops with increased hops and context
length increase, even for frontier models-revealing that sheer scale does not
guarantee robust reasoning. Failure-mode analysis highlights common breakdowns
such as missed final-hop integration and long-range drift. NovelHopQA offers a
controlled diagnostic setting to test multi-hop reasoning at scale. All code
and datasets are available at https://novelhopqa.github.io.

</details>


### [133] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/pdf/2506.07248)
*Prathamesh Kokate, Mitali Sarnaik, Manavi Khopade, Raviraj Joshi*

Main category: cs.CL

TL;DR: A TF-IDF-based sentence ranking method improves efficiency in long document classification by selecting the most informative sentences, reducing input size and latency without significant accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Transformer models like BERT face computational limits with long documents, and full-document classification is often redundant.

Method: Proposes a TF-IDF-based sentence ranking method with fixed-count or percentage-based selection, combining normalized TF-IDF scores and sentence length.

Result: Outperforms baselines, achieving near-identical accuracy with a 0.33% drop, reducing input size by 50% and latency by 43%.

Conclusion: Significant context reduction is feasible without performance loss, making the method practical for real-world tasks.

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [134] [LGAI-EMBEDDING-Preview Technical Report](https://arxiv.org/pdf/2506.07438)
*Jooyoung Choi, Hyun Kim, Hansol Jang, Changwook Jun, Kyunghoon Bae, Hyewon Choi, Stanley Jungkyu Choi, Honglak Lee, Chulmin Yun*

Main category: cs.CL

TL;DR: A unified framework for learning generalized text embeddings using a decoder-only LLM (Mistral-7B) with in-context learning, soft supervision, and adaptive hard-negative mining, achieving strong performance on diverse tasks.


<details>
  <summary>Details</summary>
Motivation: To create a scalable, high-quality embedding generation method that generalizes across both IR and non-IR tasks without task-specific fine-tuning.

Method: Combines in-context learning, soft supervision (using continuous relevance scores), and adaptive margin-based hard-negative mining to train context-aware embeddings.

Result: Achieves strong generalization on the MTEB benchmark (41 tasks), outperforming larger or fine-tuned baselines.

Conclusion: The framework effectively combines in-context prompting, soft supervision, and adaptive sampling for robust and scalable embedding generation.

Abstract: This report presents a unified instruction-based framework for learning
generalized text embeddings optimized for both information retrieval (IR) and
non-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our
approach combines in-context learning, soft supervision, and adaptive
hard-negative mining to generate context-aware embeddings without task-specific
fine-tuning. Structured instructions and few-shot examples are used to guide
the model across diverse tasks, enabling strong performance on classification,
semantic similarity, clustering, and reranking benchmarks. To improve semantic
discrimination, we employ a soft labeling framework where continuous relevance
scores, distilled from a high-performance dense retriever and reranker, serve
as fine-grained supervision signals. In addition, we introduce adaptive
margin-based hard-negative mining, which filters out semantically ambiguous
negatives based on their similarity to positive examples, thereby enhancing
training stability and retrieval robustness. Our model is evaluated on the
newly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven
categories. Results show that our method achieves strong generalization and
ranks among the top-performing models by Borda score, outperforming several
larger or fully fine-tuned baselines. These findings highlight the
effectiveness of combining in-context prompting, soft supervision, and adaptive
sampling for scalable, high-quality embedding generation.

</details>


### [135] [PlantDeBERTa: An Open Source Language Model for Plant Science](https://arxiv.org/pdf/2506.08897)
*Hiba Khey, Amine Lakhder, Salma Rouichi, Imane El Ghabi, Kamal Hejjaoui, Younes En-nahli, Fahd Kalloubi, Moez Amri*

Main category: cs.CL

TL;DR: PlantDeBERTa is a specialized language model for plant science, fine-tuned on annotated plant stress-response literature, combining transformer-based modeling with linguistic post-processing and ontology-based normalization.


<details>
  <summary>Details</summary>
Motivation: Plant science lacks domain-adapted language models, despite the success of transformer-based models in other fields. This work aims to fill this gap.

Method: Built on DeBERTa, PlantDeBERTa is fine-tuned on expert-annotated abstracts, enhanced with rule-based post-processing and ontology-grounded normalization.

Result: The model shows strong generalization across entity types and robust domain adaptation in low-resource settings.

Conclusion: PlantDeBERTa addresses a critical gap in agricultural NLP, offering a scalable framework for entity recognition and enabling data-driven plant science innovation.

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantDeBERTa, a high-performance, open-source language
model specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantDeBERTa is
fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with
a primary focus on lentil (Lens culinaris) responses to diverse abiotic and
biotic stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantDeBERTa to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantDeBERTa exhibits strong generalization capabilities
across entity types and demonstrates the feasibility of robust domain
adaptation in low-resource scientific fields.By providing a scalable and
reproducible framework for high-resolution entity recognition, PlantDeBERTa
bridges a critical gap in agricultural NLP and paves the way for intelligent,
data-driven systems in plant genomics, phenomics, and agronomic knowledge
discovery. Our model is publicly released to promote transparency and
accelerate cross-disciplinary innovation in computational plant science.

</details>


### [136] [C-SEO Bench: Does Conversational SEO Work?](https://arxiv.org/pdf/2506.11097)
*Haritz Puerto, Martin Gubri, Tommaso Green, Seong Joon Oh, Sangdoo Yun*

Main category: cs.CL

TL;DR: The paper introduces C-SEO Bench, a benchmark for evaluating Conversational Search Engine Optimization (C-SEO) methods across tasks, domains, and actors, revealing current methods' ineffectiveness compared to traditional SEO strategies.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about the effectiveness of C-SEO methods across diverse domains and multi-actor scenarios, given the shift from SEO to C-SEO with LLMs.

Method: Developed C-SEO Bench, evaluating C-SEO methods for question answering and product recommendation tasks across three domains, with varying adoption rates among actors.

Result: Most current C-SEO methods are ineffective; traditional SEO strategies outperform them. Gains diminish as more actors adopt C-SEO, indicating a zero-sum problem.

Conclusion: C-SEO Bench highlights the limitations of current C-SEO methods and the superiority of traditional SEO, emphasizing the need for better approaches in multi-actor scenarios.

Abstract: Large Language Models (LLMs) are transforming search engines into
Conversational Search Engines (CSE). Consequently, Search Engine Optimization
(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).
We are beginning to see dedicated C-SEO methods for modifying web documents to
increase their visibility in CSE responses. However, they are often tested only
for a limited breadth of application domains; we do not understand whether
certain C-SEO methods would be effective for a broad range of domains.
Moreover, existing evaluations consider only a single-actor scenario where only
one web document adopts a C-SEO method; in reality, multiple players are likely
to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy
from the dynamics we have seen in SEO. We present C-SEO Bench, the first
benchmark designed to evaluate C-SEO methods across multiple tasks, domains,
and number of actors. We consider two search tasks, question answering and
product recommendation, with three domains each. We also formalize a new
evaluation protocol with varying adoption rates among involved actors. Our
experiments reveal that most current C-SEO methods are largely ineffective,
contrary to reported results in the literature. Instead, traditional SEO
strategies, those aiming to improve the ranking of the source in the LLM
context, are significantly more effective. We also observe that as we increase
the number of C-SEO adopters, the overall gains decrease, depicting a congested
and zero-sum nature of the problem. Our code and data are available at
https://github.com/parameterlab/c-seo-bench and
https://huggingface.co/datasets/parameterlab/c-seo-bench.

</details>


### [137] [Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards](https://arxiv.org/pdf/2506.11425)
*Jeff Da, Clinton Wang, Xiang Deng, Yuntao Ma, Nikhil Barhate, Sean Hendryx*

Main category: cs.CL

TL;DR: Agent-RLVR enhances RLVR for agentic tasks by incorporating agent guidance, improving performance in complex environments like software engineering.


<details>
  <summary>Details</summary>
Motivation: Conventional RLVR struggles in agentic environments due to sparse rewards. Agent-RLVR aims to address this by mimicking human pedagogy.

Method: Agent-RLVR uses agent guidance (strategic plans, feedback) to steer agents, validated by unit tests, and updates policies via RLVR.

Result: Pass@1 performance improved from 9.4% to 22.4% on SWE-Bench Verified, further boosted to 27.8% with reward model training.

Conclusion: Agent-RLVR enables effective RLVR in complex environments, paving the way for real-world applications.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted
as the de facto method for enhancing the reasoning capabilities of large
language models and has demonstrated notable success in verifiable domains like
math and competitive programming tasks. However, the efficacy of RLVR
diminishes significantly when applied to agentic environments. These settings,
characterized by multi-step, complex problem solving, lead to high failure
rates even for frontier LLMs, as the reward landscape is too sparse for
effective model training via conventional RLVR. In this work, we introduce
Agent-RLVR, a framework that makes RLVR effective in challenging agentic
settings, with an initial focus on software engineering tasks. Inspired by
human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively
steers the agent towards successful trajectories by leveraging diverse
informational cues. These cues, ranging from high-level strategic plans to
dynamic feedback on the agent's errors and environmental interactions, emulate
a teacher's guidance, enabling the agent to navigate difficult solution spaces
and promotes active self-improvement via additional environment exploration. In
the Agent-RLVR training loop, agents first attempt to solve tasks to produce
initial trajectories, which are then validated by unit tests and supplemented
with agent guidance. Agents then reattempt with guidance, and the agent policy
is updated with RLVR based on the rewards of these guided trajectories.
Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%
to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data
is additionally useful for test-time reward model training, shown by further
boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents
with RLVR in complex, real-world environments where conventional RL methods
struggle.

</details>


### [138] [A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages](https://arxiv.org/pdf/2506.12158)
*Tatiana Anikina, Jan Cegin, Jakub Simko, Simon Ostermann*

Main category: cs.CL

TL;DR: The paper evaluates LLM-based synthetic data generation strategies for low-resource languages, finding that combining methods like target-language demonstrations and LLM revisions narrows the performance gap with real data.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comparative analysis of generation strategies for low-resource languages and assess their effectiveness.

Method: Systematic evaluation of prompting strategies (demonstrations, label-based summaries, self-revision) across 11 languages using three NLP tasks and four LLMs.

Result: Combined strategies, especially target-language demonstrations with LLM revisions, perform nearly as well as real data (5% gap). Smart prompting reduces the advantage of larger LLMs.

Conclusion: Strategic combinations of generation methods are effective for synthetic data in low-resource settings, enabling smaller models to perform efficiently.

Abstract: Large Language Models (LLMs) are increasingly used to generate synthetic
textual data for training smaller specialized models. However, a comparison of
various generation strategies for low-resource language settings is lacking.
While various prompting strategies have been proposed, such as demonstrations,
label-based summaries, and self-revision, their comparative effectiveness
remains unclear, especially for low-resource languages. In this paper, we
systematically evaluate the performance of these generation strategies and
their combinations across 11 typologically diverse languages, including several
extremely low-resource ones. Using three NLP tasks and four open-source LLMs,
we assess downstream model performance on generated versus gold-standard data.
Our results show that strategic combinations of generation methods,
particularly target-language demonstrations with LLM-based revisions, yield
strong performance, narrowing the gap with real data to as little as 5% in some
settings. We also find that smart prompting techniques can reduce the advantage
of larger LLMs, highlighting efficient generation strategies for synthetic data
generation in low-resource scenarios with smaller models.

</details>


### [139] [Supernova Event Dataset: Interpreting Large Language Models' Personality through Critical Event Analysis](https://arxiv.org/pdf/2506.12189)
*Pranav Agarwal, Ioana Ciuc*

Main category: cs.CL

TL;DR: The paper introduces the Supernova Event Dataset to analyze LLM personalities by evaluating their ability to extract and rank key events from diverse texts. It proposes a framework using another LLM as a judge to infer traits, revealing distinct reasoning styles across models.


<details>
  <summary>Details</summary>
Motivation: Understanding LLM decision-making and personality is crucial as their integration into applications grows. The study aims to improve interpretability for diverse uses.

Method: The Supernova Event Dataset is used to benchmark LLMs on event extraction and ranking. Another LLM judges model personalities based on event selection and classification.

Result: Distinct traits emerge: Orca 2 focuses on emotional reasoning, Qwen 2.5 is strategic, Claude 3.7 emphasizes conceptual framing, Gemini 2.5 prioritizes empirical validation, and o3 favors causal reasoning.

Conclusion: The framework enhances LLM interpretability, making them more user-friendly for varied applications.

Abstract: Large Language Models (LLMs) are increasingly integrated into everyday
applications. As their influence grows, understanding their decision making and
underlying personality becomes essential. In this work, we interpret model
personality using our proposed Supernova Event Dataset, a novel dataset with
diverse articles spanning biographies, historical events, news, and scientific
discoveries. We use this dataset to benchmark LLMs on extracting and ranking
key events from text, a subjective and complex challenge that requires
reasoning over long-range context and modeling causal chains. We evaluate small
models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as
Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another
LLM acts as a judge to infer each model's personality based on its selection
and classification of events. Our analysis shows distinct personality traits:
for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal
dynamics, while Qwen 2.5 displays a more strategic, analytical style. When
analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual
framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors
step-by-step causal reasoning. This analysis improves model interpretability,
making them user-friendly for a wide range of diverse applications. Project
Page - https://www.supernova-event.ai/

</details>


### [140] [LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/pdf/2506.14429)
*Xiaoran Liu, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu*

Main category: cs.CL

TL;DR: The paper investigates the long-context capabilities of diffusion LLMs, comparing them to auto-regressive LLMs, and introduces LongLLaDA, a method for context extension.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored long-context capabilities of diffusion LLMs and compare them with traditional auto-regressive LLMs.

Method: Systematic comparison of long-context performance, analysis of perplexity stability, and proposal of LongLLaDA, a training-free method for context extension.

Result: Diffusion LLMs show stable perplexity and local perception advantages, outperforming auto-regressive LLMs in certain tasks. LongLLaDA effectively extends context windows.

Conclusion: The study provides the first length extrapolation method for diffusion LLMs and benchmarks for future research, highlighting their unique strengths and limitations.

Abstract: Large Language Diffusion Models, or diffusion LLMs, have emerged as a
significant focus in NLP research, with substantial effort directed toward
understanding their scalability and downstream task performance. However, their
long-context capabilities remain unexplored, lacking systematic analysis or
methods for context extension. In this work, we present the first systematic
investigation comparing the long-context performance of diffusion LLMs and
traditional auto-regressive LLMs. We first identify a unique characteristic of
diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably stable
perplexity during direct context extrapolation. Moreover, where auto-regressive
models fail outright during the Needle-In-A-Haystack task with context
exceeding their pretrained length, we discover diffusion LLMs exhibit a
distinct local perception phenomenon, enabling successful retrieval from recent
context segments. We explain both phenomena through the lens of Rotary Position
Embedding (RoPE) scaling theory. Building on these observations, we propose
LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE
extrapolation. Our results validate that established extrapolation scaling laws
remain effective for extending the context windows of diffusion LLMs.
Furthermore, we identify long-context tasks where diffusion LLMs outperform
auto-regressive LLMs and others where they fall short. Consequently, this study
establishes the first length extrapolation method for diffusion LLMs while
providing essential theoretical insights and empirical benchmarks critical for
advancing future research on long-context diffusion LLMs. The code is available
at https://github.com/OpenMOSS/LongLLaDA.

</details>


### [141] [AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/pdf/2506.14562)
*Di He, Ajay Jaiswal, Songjun Tu, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin*

Main category: cs.CL

TL;DR: AlphaDecay adaptively assigns weight decay strengths to LLM modules based on spectral properties, improving performance over uniform decay.


<details>
  <summary>Details</summary>
Motivation: Uniform weight decay overlooks structural diversity and spectral variations in LLMs, necessitating adaptive decay strategies.

Method: AlphaDecay uses Heavy-Tailed Self-Regularization theory to analyze weight correlation matrices, assigning decay strengths based on spectral heavy-tailedness.

Result: AlphaDecay outperforms uniform decay and other baselines in perplexity and generalization across model sizes (60M to 1B).

Conclusion: AlphaDecay is a simple, effective method for adaptive weight decay, enhancing LLM performance by balancing module-wise spectral properties.

Abstract: Weight decay is a standard regularization technique for training large
language models (LLMs). While it is common to assign a uniform decay rate to
every layer, this approach overlooks the structural diversity of LLMs and the
varying spectral properties across modules. In this paper, we introduce
AlphaDecay, a simple yet effective method that adaptively assigns different
weight decay strengths to each module of an LLM. Our approach is guided by
Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical
spectral density (ESD) of weight correlation matrices to quantify
"heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs,
reflecting stronger feature learning, are assigned weaker decay, while modules
with lighter-tailed spectra receive stronger decay. Our method leverages
tailored weight decay assignments to balance the module-wise differences in
spectral properties, leading to improved performance. Extensive pre-training
tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay
achieves better perplexity and generalization than conventional uniform decay
and other adaptive decay baselines. Our code is available at
https://github.com/hed-ucas/AlphaDecay.

</details>


### [142] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/pdf/2506.15911)
*Mohammad Amaan Sayeed, Mohammed Talha Alam, Raza Imam, Shahab Saquib Sohail, Amir Hussain*

Main category: cs.CL

TL;DR: The paper introduces Tibbe-AG, a pipeline to evaluate LLMs on culturally grounded Islamic medical texts, showing retrieval and self-critique improve accuracy and insight.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in modern AI systems by validating culturally sensitive medical guidance from classical Islamic texts.

Method: Proposes Tibbe-AG, a pipeline aligning 30 Prophetic-medicine questions with human-verified remedies, testing three LLMs under direct generation, retrieval-augmented, and self-critique configurations.

Result: Retrieval improves factual accuracy by 13%, and agentic prompts add 10% more through deeper insight and safety.

Conclusion: Combining classical texts with retrieval and self-evaluation enables reliable, culturally sensitive medical QA.

Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the
Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and
holistic therapies, yet remain inaccessible to many and underutilized in modern
AI systems. Existing language-model benchmarks focus narrowly on factual recall
or user preference, leaving a gap in validating culturally grounded medical
guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that
aligns 30 carefully curated Prophetic-medicine questions with human-verified
remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three
configurations: direct generation, retrieval-augmented generation, and a
scientific self-critique filter. Each answer is then assessed by a secondary
LLM serving as an agentic judge, yielding a single 3C3H quality score.
Retrieval improves factual accuracy by 13%, while the agentic prompt adds
another 10% improvement through deeper mechanistic insight and safety
considerations. Our results demonstrate that blending classical Islamic texts
with retrieval and self-evaluation enables reliable, culturally sensitive
medical question-answering.

</details>


### [143] [HausaNLP at SemEval-2025 Task 11: Hausa Text Emotion Detection](https://arxiv.org/pdf/2506.16388)
*Sani Abdullahi Sani, Salim Abubakar, Falalu Ibrahim Lawan, Abdulhamid Abubakar, Maryam Bala*

Main category: cs.CL

TL;DR: Fine-tuning AfriBERTa for multi-label emotion detection in Hausa achieved 74% accuracy and 73.5% F1-score.


<details>
  <summary>Details</summary>
Motivation: Addressing emotion detection in Hausa, a low-resource African language, to expand NLP applications.

Method: Fine-tuned AfriBERTa (transformer-based model) with data preprocessing, tokenization, and Hugging Face Trainer API.

Result: Achieved 74.00% validation accuracy and 73.50% F1-score.

Conclusion: Transformer-based models like AfriBERTa are effective for emotion detection in low-resource languages.

Abstract: This paper presents our approach to multi-label emotion detection in Hausa, a
low-resource African language, for SemEval Track A. We fine-tuned AfriBERTa, a
transformer-based model pre-trained on African languages, to classify Hausa
text into six emotions: anger, disgust, fear, joy, sadness, and surprise. Our
methodology involved data preprocessing, tokenization, and model fine-tuning
using the Hugging Face Trainer API. The system achieved a validation accuracy
of 74.00%, with an F1-score of 73.50%, demonstrating the effectiveness of
transformer-based models for emotion detection in low-resource languages.

</details>


### [144] [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/pdf/2506.17090)
*Murtaza Nazir, Matthew Finlayson, John X. Morris, Xiang Ren, Swabha Swayamdipta*

Main category: cs.CL

TL;DR: A new method, PILS, recovers hidden prompts from language model outputs using next-token probabilities, achieving 2--3.5x higher recovery rates than previous methods.


<details>
  <summary>Details</summary>
Motivation: Addresses security and accountability concerns in language model deployments, such as leaking private information from API-protected models.

Method: PILS leverages low-dimensional subspace of model outputs to compress next-token probabilities, enabling more effective inversion.

Result: Achieves 2--3.5x higher exact recovery rates, with strong generalization and performance on recovering hidden system messages.

Conclusion: Next-token probabilities are a vulnerable attack surface, and PILS significantly improves prompt recovery efficiency.

Abstract: Language model inversion seeks to recover hidden prompts using only language
model outputs. This capability has implications for security and accountability
in language model deployments, such as leaking private information from an
API-protected language model's system message. We propose a new method --
prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts
by gleaning clues from the model's next-token probabilities over the course of
multiple generation steps. Our method is enabled by a key insight: The
vector-valued outputs of a language model occupy a low-dimensional subspace.
This enables us to losslessly compress the full next-token probability
distribution over multiple generation steps using a linear map, allowing more
output information to be used for inversion. Our approach yields massive gains
over previous state-of-the-art methods for recovering hidden prompts, achieving
2--3.5 times higher exact recovery rates across test sets, in one case
increasing the recovery rate from 17% to 60%. Our method also exhibits
surprisingly good generalization behavior; for instance, an inverter trained on
16 generations steps gets 5--27 points higher prompt recovery when we increase
the number of steps to 32 at test time. Furthermore, we demonstrate strong
performance of our method on the more challenging task of recovering hidden
system messages. We also analyze the role of verbatim repetition in prompt
recovery and propose a new method for cross-family model transfer for
logit-based inverters. Our findings show that next-token probabilities are a
considerably more vulnerable attack surface for inversion attacks than
previously known.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [145] [Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation](https://arxiv.org/pdf/2506.17237)
*Dip Roy*

Main category: cs.CV

TL;DR: The paper analyzes diffusion models' computational pathways and mechanisms in image generation, revealing differences in processing synthetic vs. naturalistic data, identifying specialized attention mechanisms, and demonstrating performance impacts of targeted interventions.


<details>
  <summary>Details</summary>
Motivation: To understand the computational principles and mechanistic differences in how diffusion models process synthetic versus naturalistic data, particularly in image generation.

Method: Conducted systematic intervention experiments on 2,000 synthetic and 2,000 CelebA facial images, measuring computational complexity, attention patterns, and entropy divergence.

Result: Found higher computational complexity for real-world face processing (complexity ratio = 1.084), identified eight specialized attention mechanisms, and showed performance degradation (25.6% to 128.3%) from targeted ablations.

Conclusion: The study provides quantitative foundations for understanding and controlling generative model behavior through mechanistic interventions.

Abstract: We present a quantitative circuit-level analysis of diffusion models,
establishing computational pathways and mechanistic principles underlying image
generation processes. Through systematic intervention experiments across 2,000
synthetic and 2,000 CelebA facial images, we discover fundamental algorithmic
differences in how diffusion architectures process synthetic versus
naturalistic data distributions. Our investigation reveals that real-world face
processing requires circuits with measurably higher computational complexity
(complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct
attention specialization patterns with entropy divergence ranging from 0.015 to
0.166 across denoising timesteps. We identify eight functionally distinct
attention mechanisms showing specialized computational roles: edge detection
(entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus
0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15).
Intervention analysis demonstrates critical computational bottlenecks where
targeted ablations produce 25.6% to 128.3% performance degradation, providing
causal evidence for identified circuit functions. These findings establish
quantitative foundations for algorithmic understanding and control of
generative model behavior through mechanistic intervention strategies.

</details>


### [146] [SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation](https://arxiv.org/pdf/2506.17290)
*Yuqi Li, Junhao Dong, Zeyu Dong, Chuanguang Yang, Zhulin An, Yongjun Xu*

Main category: cs.CV

TL;DR: Proposes SRKD, a knowledge distillation framework for 3D point cloud segmentation, transferring knowledge from a large teacher model to a lightweight student model, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Address computational complexity and deployment limitations of large transformer-based models in 3D point cloud segmentation.

Method: Uses affinity matrix-based relation alignment, cross-sample mini-batch construction, KL divergence for semantic alignment, and ground-truth supervision.

Result: Achieves state-of-the-art performance with reduced model complexity (<15M parameters).

Conclusion: SRKD is effective and efficient for real-world deployment, balancing performance and computational cost.

Abstract: 3D point cloud segmentation faces practical challenges due to the
computational complexity and deployment limitations of large-scale
transformer-based models. To address this, we propose a novel Structure- and
Relation-aware Knowledge Distillation framework, named SRKD, that transfers
rich geometric and semantic knowledge from a large frozen teacher model (>100M)
to a lightweight student model (<15M). Specifically, we propose an affinity
matrix-based relation alignment module, which distills structural dependencies
from the teacher to the student through point-wise similarity matching,
enhancing the student's capability to learn contextual interactions. Meanwhile,
we introduce a cross-sample mini-batch construction strategy that enables the
student to perceive stable and generalized geometric structure. This aligns
across diverse point cloud instances of the teacher, rather than within a
single sample. Additionally, KL divergence is applied to align semantic
distributions, and ground-truth supervision further reinforces accurate
segmentation. Our method achieves state of the art performance with
significantly reduced model complexity, demonstrating its effectiveness and
efficiency in real-world deployment scenarios. Our Code is available at
https://github.com/itsnotacie/SRKD.

</details>


### [147] [Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning](https://arxiv.org/pdf/2506.17302)
*Yijun Lin, Theresa Chen, Colby Brungard, Grunwald Sabine, Sue Ives, Matt Macander, Timm Nawrocki, Yao-Yi Chiang, Nic Jelinski*

Main category: cs.CV

TL;DR: MISO, a vision-based ML model, outperforms Random Forest in fine-scale soil mapping for permafrost in Alaska, offering better generalization and recall for monitoring thaw and planning.


<details>
  <summary>Details</summary>
Motivation: Accelerating permafrost thaw due to climate change threatens infrastructure and ecosystem services, necessitating high-resolution soil maps for adaptation.

Method: MISO integrates a geospatial foundation model, implicit neural representations, and contrastive learning for continuous spatial prediction and multimodal alignment.

Result: MISO generalizes better to unseen locations and achieves higher recall than Random Forest, as validated across Permafrost Zones and MLRAs.

Conclusion: Advanced ML like MISO holds promise for fine-scale soil mapping, aiding future soil sampling and infrastructure planning in permafrost regions.

Abstract: Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and
localized simulations, remains a critical yet underdeveloped task, despite the
region's ecological importance and extensive permafrost coverage. As permafrost
thaw accelerates due to climate change, it threatens infrastructure stability
and key ecosystem services, such as soil carbon storage. High-resolution soil
maps are essential for characterizing permafrost distribution, identifying
vulnerable areas, and informing adaptation strategies. We present MISO, a
vision-based machine learning (ML) model to produce statewide fine-scale soil
maps for near-surface permafrost and soil taxonomy. The model integrates a
geospatial foundation model for visual feature extraction, implicit neural
representations for continuous spatial prediction, and contrastive learning for
multimodal alignment and geo-location awareness. We compare MISO with Random
Forest (RF), a traditional ML model that has been widely used in soil mapping
applications. Spatial cross-validation and regional analysis across Permafrost
Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better
to remote, unseen locations and achieves higher recall than RF, which is
critical for monitoring permafrost thaw and related environmental processes.
These findings demonstrate the potential of advanced ML approaches for
fine-scale soil mapping and provide practical guidance for future soil sampling
and infrastructure planning in permafrost-affected landscapes. The project will
be released at https://github.com/knowledge-computing/Peatland-permafrost.

</details>


### [148] [RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences](https://arxiv.org/pdf/2506.17325)
*Sina Najafi, M. Hadi Sepanj, Fahimeh Jafari*

Main category: cs.CV

TL;DR: A temporally-aware computer vision framework for predicting user churn in gig platforms, outperforming existing methods with significant gains in F1 score, precision, and AUC.


<details>
  <summary>Details</summary>
Motivation: Churn prediction in non-subscription gig platforms is challenging due to implicit disengagement and lack of explicit labels. Existing methods miss temporal cues.

Method: Proposes a framework modeling user behavior as radar chart image sequences, combining a pretrained CNN encoder with a bidirectional LSTM for spatial-temporal pattern capture.

Result: Outperforms classical models and ViT-based baselines, achieving gains of 17.7 in F1, 29.4 in precision, and 16.1 in AUC.

Conclusion: The framework is modular, interpretable, and efficient, making it suitable for large-scale churn modeling in gig platforms.

Abstract: Predicting user churn in non-subscription gig platforms, where disengagement
is implicit, poses unique challenges due to the absence of explicit labels and
the dynamic nature of user behavior. Existing methods often rely on aggregated
snapshots or static visual representations, which obscure temporal cues
critical for early detection. In this work, we propose a temporally-aware
computer vision framework that models user behavioral patterns as a sequence of
radar chart images, each encoding day-level behavioral features. By integrating
a pretrained CNN encoder with a bidirectional LSTM, our architecture captures
both spatial and temporal patterns underlying churn behavior. Extensive
experiments on a large real-world dataset demonstrate that our method
outperforms classical models and ViT-based radar chart baselines, yielding
gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with
improved interpretability. The framework's modular design, explainability
tools, and efficient deployment characteristics make it suitable for
large-scale churn modeling in dynamic gig-economy platforms.

</details>


### [149] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/pdf/2506.17707)
*Jihyun Kim, Junho Park, Kyeongbo Kong, Suk-Ju Kang*

Main category: cs.CV

TL;DR: Programmable-Room is a framework for generating and editing 3D room meshes using natural language instructions, leveraging visual programming and a diffusion model for texture generation.


<details>
  <summary>Details</summary>
Motivation: To enable precise control over 3D room attributes through natural language, decomposing the complex task into manageable steps.

Method: Decomposes tasks into steps like 3D coordinate generation, panorama texture creation, mesh construction, and furniture arrangement. Uses visual programming with an LLM and a diffusion model for texture generation.

Result: Demonstrates flexibility in 3D room mesh generation and editing, outperforming existing models quantitatively and qualitatively.

Conclusion: Programmable-Room offers an effective, unified framework for interactive 3D room generation and editing via natural language.

Abstract: We present Programmable-Room, a framework which interactively generates and
edits a 3D room mesh, given natural language instructions. For precise control
of a room's each attribute, we decompose the challenging task into simpler
steps such as creating plausible 3D coordinates for room meshes, generating
panorama images for the texture, constructing 3D meshes by integrating the
coordinates and panorama texture images, and arranging furniture. To support
the various decomposed tasks with a unified framework, we incorporate visual
programming (VP). VP is a method that utilizes a large language model (LLM) to
write a Python-like program which is an ordered list of necessary modules for
the various tasks given in natural language. We develop most of the modules.
Especially, for the texture generating module, we utilize a pretrained
large-scale diffusion model to generate panorama images conditioned on text and
visual prompts (i.e., layout, depth, and semantic map) simultaneously.
Specifically, we enhance the panorama image generation quality by optimizing
the training objective with a 1D representation of a panorama scene obtained
from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in
generating and editing 3D room meshes, and prove our framework's superiority to
an existing model quantitatively and qualitatively. Project page is available
in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [150] [P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments](https://arxiv.org/pdf/2506.17332)
*Haitian Wang, Yiren Wang, Xinyu Wang, Yumeng Miao, Yuliang Zhang, Yu Zhang, Atif Mansoor*

Main category: cs.CV

TL;DR: A multimodal fall detection system for elderly in bathrooms combines radar and vibration sensing to improve accuracy and privacy.


<details>
  <summary>Details</summary>
Motivation: Addressing the high fall risk for elderly in bathrooms and limitations of unimodal sensing systems.

Method: Develops a sensor fusion framework (millimeter-wave radar and 3D vibration sensing) and a dual-stream network (P2MFDS) for fall detection.

Result: P2MFDS outperforms state-of-the-art methods in accuracy and recall.

Conclusion: The proposed system offers a privacy-preserving, accurate solution for fall detection in complex bathroom environments.

Abstract: By 2050, people aged 65 and over are projected to make up 16 percent of the
global population. As aging is closely associated with increased fall risk,
particularly in wet and confined environments such as bathrooms where over 80
percent of falls occur. Although recent research has increasingly focused on
non-intrusive, privacy-preserving approaches that do not rely on wearable
devices or video-based monitoring, these efforts have not fully overcome the
limitations of existing unimodal systems (e.g., WiFi-, infrared-, or
mmWave-based), which are prone to reduced accuracy in complex environments.
These limitations stem from fundamental constraints in unimodal sensing,
including system bias and environmental interference, such as multipath fading
in WiFi-based systems and drastic temperature changes in infrared-based
methods. To address these challenges, we propose a Privacy-Preserving
Multimodal Fall Detection System for Elderly People in Bathroom Environments.
First, we develop a sensor evaluation framework to select and fuse
millimeter-wave radar with 3D vibration sensing, and use it to construct and
preprocess a large-scale, privacy-preserving multimodal dataset in real
bathroom settings, which will be released upon publication. Second, we
introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch
for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch
for vibration impact detection. By uniting macro- and micro-scale features,
P2MFDS delivers significant gains in accuracy and recall over state-of-the-art
approaches. Code and pretrained models will be made available at:
https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.

</details>


### [151] [PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis](https://arxiv.org/pdf/2506.17912)
*Chuhao Jin, Haosen Li, Bingzi Zhang, Che Liu, Xiting Wang, Ruihua Song, Wenbing Huang, Ying Qin, Fuzheng Zhang, Di Zhang*

Main category: cs.CV

TL;DR: PlanMoGPT improves text-to-motion generation by addressing motion tokenization granularity issues with progressive planning and flow-enhanced tokenization, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The performance gap in text-to-motion generation between LLM-based and non-LLM methods, caused by motion tokenization granularity issues.

Method: Progressive planning mechanism and flow-enhanced fine-grained motion tokenization, including higher resolution downsampling and expanded codebook size.

Result: Achieves 63.8% improvement in FID scores and 49.9% enhanced motion diversity, outperforming existing methods.

Conclusion: PlanMoGPT resolves the diversity-quality trade-off in text-to-motion generation, setting new benchmarks.

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in
many multimodal generation tasks, but a significant performance gap still
exists in text-to-motion generation, where LLM-based methods lag far behind
non-LLM methods. We identify the granularity of motion tokenization as a
critical bottleneck: fine-grained tokenization induces local dependency issues,
where LLMs overemphasize short-term coherence at the expense of global semantic
alignment, while coarse-grained tokenization sacrifices motion details. To
resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating
progressive planning and flow-enhanced fine-grained motion tokenization. First,
our progressive planning mechanism leverages LLMs' autoregressive capabilities
to hierarchically generate motion tokens by starting from sparse global plans
and iteratively refining them into full sequences. Second, our flow-enhanced
tokenizer doubles the downsampling resolution and expands the codebook size by
eight times, minimizing detail loss during discretization, while a
flow-enhanced decoder recovers motion nuances. Extensive experiments on
text-to-motion benchmarks demonstrate that it achieves state-of-the-art
performance, improving FID scores by 63.8% (from 0.380 to 0.141) on
long-sequence generation while enhancing motion diversity by 49.9% compared to
existing methods. The proposed framework successfully resolves the
diversity-quality trade-off that plagues current non-LLM approaches,
establishing new standards for text-to-motion generation.

</details>


### [152] [A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving](https://arxiv.org/pdf/2506.17346)
*Yuhan Zhou, Haihua Chen, Kewei Sha*

Main category: cs.CV

TL;DR: The paper proposes a task-centric, data quality (DQ)-aware framework for autonomous vehicles (AVs) to address overlooked DQ issues, mapping DQ to task requirements and performance goals. A case study on the nuScenes dataset demonstrates improved object detection by reducing redundancy in multisource data.


<details>
  <summary>Details</summary>
Motivation: Current AV research and practice focus heavily on models/algorithms while undervaluing data quality (DQ), despite its critical role in real-world AV performance. The paper aims to bridge this gap by emphasizing DQ's impact on functionality, efficiency, and trustworthiness.

Method: A five-layer framework (data, DQ, task, application, goal) is introduced to systematically map DQ to task requirements and performance goals. A case study on the nuScenes dataset evaluates the framework by analyzing redundancy in multisource image and LiDAR data.

Result: The framework proves effective, with the case study showing that reducing redundancy in multisource image data improves YOLOv8 object detection performance. Analysis also highlights existing redundancy DQ issues in multimodal data.

Conclusion: The paper highlights critical, unexplored challenges in DQ for AVs and provides a framework to guide the development of adaptive, explainable, and resilient AVs. It calls for greater attention to DQ in AV research and practice.

Abstract: The next-generation autonomous vehicles (AVs), embedded with frequent
real-time decision-making, will rely heavily on a large volume of multisource
and multimodal data. In real-world settings, the data quality (DQ) of different
sources and modalities usually varies due to unexpected environmental factors
or sensor issues. However, both researchers and practitioners in the AV field
overwhelmingly concentrate on models/algorithms while undervaluing the DQ. To
fulfill the needs of the next-generation AVs with guarantees of functionality,
efficiency, and trustworthiness, this paper proposes a novel task-centric and
data quality vase framework which consists of five layers: data layer, DQ
layer, task layer, application layer, and goal layer. The proposed framework
aims to map DQ with task requirements and performance goals. To illustrate, a
case study investigating redundancy on the nuScenes dataset proves that
partially removing redundancy on multisource image data could improve YOLOv8
object detection task performance. Analysis on multimodal data of image and
LiDAR further presents existing redundancy DQ issues. This paper opens up a
range of critical but unexplored challenges at the intersection of DQ, task
orchestration, and performance-oriented system development in AVs. It is
expected to guide the AV community toward building more adaptive, explainable,
and resilient AVs that respond intelligently to dynamic environments and
heterogeneous data streams. Code, data, and implementation details are publicly
available at: https://anonymous.4open.science/r/dq4av-framework/README.md.

</details>


### [153] [On the Robustness of Human-Object Interaction Detection against Distribution Shift](https://arxiv.org/pdf/2506.18021)
*Chi Xie, Shuang Liang, Jie Li, Feng Zhu, Rui Zhao, Yichen Wei, Shengjie Zhao*

Main category: cs.CV

TL;DR: The paper addresses robustness in Human-Object Interaction (HOI) detection under distribution shifts, proposing a benchmark, analyzing existing models, and introducing two simple, plug-and-play methods to enhance robustness.


<details>
  <summary>Details</summary>
Motivation: Existing HOI detection models lack robustness under practical distribution shifts, limiting their real-world applicability.

Method: The authors create a robustness benchmark, evaluate 40+ models, and propose cross-domain data augmentation with mixup and feature fusion using frozen vision foundation models.

Result: The proposed methods significantly improve robustness across models, with benefits on standard benchmarks.

Conclusion: The work provides practical solutions for enhancing HOI detection robustness, with plans to release the dataset and code.

Abstract: Human-Object Interaction (HOI) detection has seen substantial advances in
recent years. However, existing works focus on the standard setting with ideal
images and natural distribution, far from practical scenarios with inevitable
distribution shifts. This hampers the practical applicability of HOI detection.
In this work, we investigate this issue by benchmarking, analyzing, and
enhancing the robustness of HOI detection models under various distribution
shifts. We start by proposing a novel automated approach to create the first
robustness evaluation benchmark for HOI detection. Subsequently, we evaluate
more than 40 existing HOI detection models on this benchmark, showing their
insufficiency, analyzing the features of different frameworks, and discussing
how the robustness in HOI is different from other tasks. With the insights from
such analyses, we propose to improve the robustness of HOI detection methods
through: (1) a cross-domain data augmentation integrated with mixup, and (2) a
feature fusion strategy with frozen vision foundation models. Both are simple,
plug-and-play, and applicable to various methods. Our experimental results
demonstrate that the proposed approach significantly increases the robustness
of various methods, with benefits on standard benchmarks, too. The dataset and
code will be released.

</details>


### [154] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/pdf/2506.17361)
*Xufei Wang, Mingjian Zhang, Fei Ge, Jinchen Zhu, Wen Sha, Jifen Ren, Zhimeng Hou, Shouguo Zheng, ling Zheng, Shizhuang Weng*

Main category: cs.CV

TL;DR: Proposes an efficient feedback gate network for single hyperspectral image super-resolution (SHSR) using feedbacks, gate operations, and novel modules to enhance spatial-spectral coherence.


<details>
  <summary>Details</summary>
Motivation: Existing SHSR methods underperform due to inadequate exploration of band coherence and spatial-spectral information.

Method: Introduces a group-based approach with feedback gate operations, large kernel convolutions, and spectral interactions, along with modules like SPDFM and SSRGM for enhanced feature extraction.

Result: Outperforms state-of-the-art methods on three datasets, achieving better spectral fidelity and spatial content reconstruction.

Conclusion: The proposed network effectively improves SHSR by leveraging advanced modules and operations for superior performance.

Abstract: Even without auxiliary images, single hyperspectral image super-resolution
(SHSR) methods can be designed to improve the spatial resolution of
hyperspectral images. However, failing to explore coherence thoroughly along
bands and spatial-spectral information leads to the limited performance of the
SHSR. In this study, we propose a novel group-based SHSR method termed the
efficient feedback gate network, which uses various feedbacks and gate
operations involving large kernel convolutions and spectral interactions. In
particular, by providing different guidance for neighboring groups, we can
learn rich band information and hierarchical hyperspectral spatial information
using channel shuffling and dilatation convolution in shuffled and progressive
dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate
block and a spectrum enhancement gate block to construct the spatial-spectral
reinforcement gate module (SSRGM) and obtain highly representative
spatial-spectral features efficiently. Additionally, we apply a
three-dimensional SSRGM to enhance holistic information and coherence for
hyperspectral data. The experimental results on three hyperspectral datasets
demonstrate the superior performance of the proposed network over the
state-of-the-art methods in terms of spectral fidelity and spatial content
reconstruction.

</details>


### [155] [Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster](https://arxiv.org/pdf/2506.18034)
*Fenghe Tang, Wenxin Ma, Zhiyang He, Xiaodong Tao, Zihang Jiang, S. Kevin Zhou*

Main category: cs.CV

TL;DR: A frozen pre-trained LLM layer enhances medical image segmentation when integrated into a CNN framework, improving performance with minimal parameter increase.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of leveraging LLM's semantic awareness for medical image segmentation tasks.

Method: Proposes a hybrid structure (LLM4Seg) combining a frozen pre-trained LLM layer with a CNN encoder-decoder framework.

Result: Improves segmentation performance across modalities (ultrasound, dermoscopy, etc.) and is robust with different LLMs (LLaMA, DeepSeek).

Conclusion: LLM's semantic awareness can effectively enhance segmentation tasks, offering better global and local modeling.

Abstract: With the advancement of Large Language Model (LLM) for natural language
processing, this paper presents an intriguing finding: a frozen pre-trained LLM
layer can process visual tokens for medical image segmentation tasks.
Specifically, we propose a simple hybrid structure that integrates a
pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation
framework (LLM4Seg). Surprisingly, this design improves segmentation
performance with a minimal increase in trainable parameters across various
modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our
in-depth analysis reveals the potential of transferring LLM's semantic
awareness to enhance segmentation tasks, offering both improved global
understanding and better local modeling capabilities. The improvement proves
robust across different LLMs, validated using LLaMA and DeepSeek.

</details>


### [156] [From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge](https://arxiv.org/pdf/2506.17374)
*Muhammad Tayyab Khan, Lequn Chen, Zane Yong, Jun Ming Tan, Wenhe Feng, Seung Ki Moon*

Main category: cs.CV

TL;DR: A hybrid vision-language framework combining YOLOv11-OBB and transformer-based VLMs improves extraction of key information from 2D engineering drawings, outperforming generic OCR methods.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of key information from 2D engineering drawings is slow and error-prone, while generic OCR models struggle with complex layouts and symbols.

Method: Proposes a pipeline using YOLOv11-OBB for localization and extraction, followed by parsing with fine-tuned VLMs (Donut and Florence-2).

Result: Donut achieves 88.5% precision, 99.2% recall, and 93.5% F1-score, outperforming Florence-2.

Conclusion: The framework effectively modernizes 2D drawing interpretation, supporting downstream manufacturing tasks.

Abstract: Efficient and accurate extraction of key information from 2D engineering
drawings is essential for advancing digital manufacturing workflows. Such
information includes geometric dimensioning and tolerancing (GD&T), measures,
material specifications, and textual annotations. Manual extraction is slow and
labor-intensive, while generic OCR models often fail due to complex layouts,
engineering symbols, and rotated text, leading to incomplete and unreliable
outputs. These limitations result in incomplete and unreliable outputs. To
address these challenges, we propose a hybrid vision-language framework that
integrates a rotation-aware object detection model (YOLOv11-obb) with a
transformer-based vision-language parser. Our structured pipeline applies
YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)
patches, which are then parsed into structured outputs using a fine-tuned,
lightweight vision-language model (VLM). We curate a dataset of 1,367 2D
mechanical drawings annotated across nine key categories. YOLOv11-OBB is
trained on this dataset to detect OBBs and extract annotation patches. These
are parsed using two open-source VLMs: Donut and Florence-2. Both models are
lightweight and well-suited for specialized industrial tasks under limited
computational overhead. Following fine-tuning of both models on the curated
dataset of image patches paired with structured annotation labels, a
comparative experiment is conducted to evaluate parsing performance across four
key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%
recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a
case study demonstrates how the extracted structured information supports
downstream manufacturing tasks such as process and tool selection, showcasing
the practical utility of the proposed framework in modernizing 2D drawing
interpretation.

</details>


### [157] [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://arxiv.org/pdf/2506.18866)
*Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, Steven Hoi*

Main category: cs.CV

TL;DR: OmniAvatar is an audio-driven full-body video generation model improving lip-sync accuracy and natural movements using pixel-wise multi-hierarchical audio embedding and LoRA-based training.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on facial movements, lacking full-body animation and precise prompt control.

Method: Uses pixel-wise multi-hierarchical audio embedding and LoRA-based training to enhance lip-syncing and incorporate audio features.

Result: Outperforms existing models in facial and semi-body video generation with precise text-based control.

Conclusion: OmniAvatar advances audio-driven full-body animation, offering versatile applications in diverse domains.

Abstract: Significant progress has been made in audio-driven human animation, while
most existing methods focus mainly on facial movements, limiting their ability
to create full-body animations with natural synchronization and fluidity. They
also struggle with precise prompt control for fine-grained generation. To
tackle these challenges, we introduce OmniAvatar, an innovative audio-driven
full-body video generation model that enhances human animation with improved
lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise
multi-hierarchical audio embedding strategy to better capture audio features in
the latent space, enhancing lip-syncing across diverse scenes. To preserve the
capability for prompt-driven control of foundation models while effectively
incorporating audio features, we employ a LoRA-based training approach.
Extensive experiments show that OmniAvatar surpasses existing models in both
facial and semi-body video generation, offering precise text-based control for
creating videos in various domains, such as podcasts, human interactions,
dynamic scenes, and singing. Our project page is
https://omni-avatar.github.io/.

</details>


### [158] [Spatial-Temporal Pre-Training for Embryo Viability Prediction Using Time-Lapse Videos](https://arxiv.org/pdf/2506.17403)
*Zhiyi Shi, Junsik Kim, Helen Y. Yang, Yonghyun Song, Hyun-Jic Oh, Dalit Ben-Yosef, Daniel Needleman, Hanspeter Pfister*

Main category: cs.CV

TL;DR: The paper proposes Spatial-Temporal Pre-Training (STPT) for automating embryo viability prediction in IVF, addressing challenges like long videos and temporal misalignment with a two-stage SSL approach.


<details>
  <summary>Details</summary>
Motivation: Automating embryo viability prediction is crucial for IVF, but limited labeled data and challenges in handling long, variable-length embryo videos hinder progress.

Method: STPT uses a two-stage SSL approach: spatial (learning within videos) and temporal (modeling relationships between videos), avoiding frame-by-frame alignment to reduce memory use.

Result: STPT achieves the highest AUC of 0.635 on 23,027 videos (3,286 labeled), outperforming baselines with limited computational resources.

Conclusion: STPT effectively addresses challenges in embryo video analysis, offering a scalable solution for IVF viability prediction with improved performance.

Abstract: Automating embryo viability prediction for in vitro fertilization (IVF) is
important but challenging due to the limited availability of labeled pregnancy
outcome data, as only a small fraction of embryos are labeled after transfer.
Self-supervised learning (SSL) can leverage both labeled and unlabeled data to
improve prediction. However, existing SSL methods for videos are not directly
applicable to embryo development videos due to two challenges: (1) embryo
time-lapse videos contain hundreds of frames, requiring significant GPU memory
for conventional SSL; (2) the dataset contains videos with varying lengths and
many outlier frames, causing traditional video alignment methods to struggle
with semantic misalignment. We propose Spatial-Temporal Pre-Training (STPT) to
address these challenges. STPT includes two stages: spatial and temporal. In
each stage, only one encoder is trained while the other is frozen, reducing
memory demands. To handle temporal misalignment, STPT avoids frame-by-frame
alignment across videos. The spatial stage learns from alignments within each
video and its temporally consistent augmentations. The temporal stage then
models relationships between video embeddings. Our method efficiently handles
long videos and temporal variability. On 23,027 time-lapse videos (3,286
labeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared
to baselines, with limited computational resources.

</details>


### [159] [Let Your Video Listen to Your Music!](https://arxiv.org/pdf/2506.18881)
*Xinyu Zhang, Dong Gong, Zicheng Duan, Anton van den Hengel, Lingqiao Liu*

Main category: cs.CV

TL;DR: MVAA is a novel framework for aligning video motion with music beats automatically, preserving original content through a two-step process of keyframe alignment and rhythm-aware inpainting.


<details>
  <summary>Details</summary>
Motivation: Aligning video motion with music beats enhances engagement and visual appeal, but existing methods are labor-intensive or inflexible.

Method: MVAA modularizes the task into aligning motion keyframes with beats and rhythm-aware video inpainting, using a frame-conditioned diffusion model.

Result: The approach achieves high-quality beat alignment and visual smoothness, adapting quickly with minimal training.

Conclusion: MVAA offers an efficient, flexible solution for automatic music-video alignment, outperforming manual and heuristic-based methods.

Abstract: Aligning the rhythm of visual motion in a video with a given music track is a
practical need in multimedia production, yet remains an underexplored task in
autonomous video editing. Effective alignment between motion and musical beats
enhances viewer engagement and visual appeal, particularly in music videos,
promotional content, and cinematic editing. Existing methods typically depend
on labor-intensive manual cutting, speed adjustments, or heuristic-based
editing techniques to achieve synchronization. While some generative models
handle joint video and music generation, they often entangle the two
modalities, limiting flexibility in aligning video to music beats while
preserving the full visual content. In this paper, we propose a novel and
efficient framework, termed MVAA (Music-Video Auto-Alignment), that
automatically edits video to align with the rhythm of a given music track while
preserving the original visual content. To enhance flexibility, we modularize
the task into a two-step process in our MVAA: aligning motion keyframes with
audio beats, followed by rhythm-aware video inpainting. Specifically, we first
insert keyframes at timestamps aligned with musical beats, then use a
frame-conditioned diffusion model to generate coherent intermediate frames,
preserving the original video's semantic content. Since comprehensive test-time
training can be time-consuming, we adopt a two-stage strategy: pretraining the
inpainting module on a small video set to learn general motion priors, followed
by rapid inference-time fine-tuning for video-specific adaptation. This hybrid
approach enables adaptation within 10 minutes with one epoch on a single NVIDIA
4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show
that our approach can achieve high-quality beat alignment and visual
smoothness.

</details>


### [160] [VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction](https://arxiv.org/pdf/2506.17412)
*Zijun Sun, Solveig Thrun, Michael Kampffmeyer*

Main category: cs.CV

TL;DR: Proposes Vision Mamba RNN (VMRNN) with SSM and LSTM-like memory to capture breast tissue trends, enhanced by an asymmetry module (SAD & LAT), improving cancer prediction, especially for high-density cases.


<details>
  <summary>Details</summary>
Motivation: Improve breast cancer screening by leveraging temporal data and addressing challenges in capturing longitudinal dynamics.

Method: Uses VMRNN with SSM and LSTM-like memory, plus an asymmetry module (SAD & LAT) for bilateral differences.

Result: Notable improvements in cancer onset prediction, especially for high-density breasts and extended time points.

Conclusion: Potential to advance early breast cancer detection and enable personalized screening strategies.

Abstract: Breast cancer remains a leading cause of mortality worldwide and is typically
detected via screening programs where healthy people are invited in regular
intervals. Automated risk prediction approaches have the potential to improve
this process by facilitating dynamically screening of high-risk groups. While
most models focus solely on the most recent screening, there is growing
interest in exploiting temporal information to capture evolving trends in
breast tissue, as inspired by clinical practice. Early methods typically relied
on two time steps, and although recent efforts have extended this to multiple
time steps using Transformer architectures, challenges remain in fully
harnessing the rich temporal dynamics inherent in longitudinal imaging data. In
this work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a
state-space model (SSM) and LSTM-like memory mechanisms to effectively capture
nuanced trends in breast tissue evolution. To further enhance our approach, we
incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector
(SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant
bilateral differences. This integrated framework demonstrates notable
improvements in predicting cancer onset, especially for the more challenging
high-density breast cases and achieves superior performance at extended time
points (years four and five), highlighting its potential to advance early
breast cancer recognition and enable more personalized screening strategies.
Our code is available at https://github.com/Mortal-Suen/VMRA-MaR.git.

</details>


### [161] [Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations](https://arxiv.org/pdf/2506.18898)
*Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, Lu Jiang*

Main category: cs.CV

TL;DR: The paper introduces Tar, a multimodal LLM framework with a Text-Aligned Tokenizer (TA-Tok) for unified visual-text understanding and generation, achieving competitive performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To unify visual understanding and generation in a shared semantic space without modality-specific designs, leveraging LLM capabilities.

Method: Uses TA-Tok for image-to-token conversion, scale-adaptive encoding/decoding, and two de-tokenizers (autoregressive and diffusion-based) for diverse outputs. Advanced pre-training tasks enhance modality fusion.

Result: Tar matches or outperforms existing multimodal LLMs, with faster convergence and greater training efficiency.

Conclusion: The framework successfully integrates vision and text, offering a scalable and efficient solution for multimodal tasks.

Abstract: This paper presents a multimodal framework that attempts to unify visual
understanding and generation within a shared discrete semantic representation.
At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into
discrete tokens using a text-aligned codebook projected from a large language
model's (LLM) vocabulary. By integrating vision and text into a unified space
with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input
and output through a shared interface, without the need for modality-specific
designs. Additionally, we propose scale-adaptive encoding and decoding to
balance efficiency and visual detail, along with a generative de-tokenizer to
produce high-fidelity visual outputs. To address diverse decoding needs, we
utilize two complementary de-tokenizers: a fast autoregressive model and a
diffusion-based model. To enhance modality fusion, we investigate advanced
pre-training tasks, demonstrating improvements in both visual understanding and
generation. Experiments across benchmarks show that Tar matches or surpasses
existing multimodal LLM methods, achieving faster convergence and greater
training efficiency. Code, models, and data are available at
https://tar.csuhan.com

</details>


### [162] [Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction](https://arxiv.org/pdf/2506.17425)
*Minmin Yang, Huantao Ren, Senem Velipasalar*

Main category: cs.CV

TL;DR: The paper introduces Trans-CBCT and Trans$^2$-CBCT, hybrid CNN-Transformer models for sparse-view CBCT reconstruction, improving PSNR and SSIM metrics.


<details>
  <summary>Details</summary>
Motivation: Addressing severe artifacts and poor spatial coverage in sparse-view CBCT by leveraging CNN-Transformer hybrids for better local and global feature integration.

Method: Replacing UNet/ResNet with TransUNet, adapting it for CBCT with multi-scale features and a lightweight head, and introducing a neighbor-aware Point Transformer for volumetric coherence.

Result: Trans-CBCT outperforms baselines by 1.17 dB PSNR and 0.0163 SSIM; Trans$^2$-CBCT adds 0.63 dB PSNR and 0.0117 SSIM.

Conclusion: Combining CNN-Transformer features with point-based geometry reasoning effectively improves sparse-view CBCT reconstruction.

Abstract: Cone-beam computed tomography (CBCT) using only a few X-ray projection views
enables faster scans with lower radiation dose, but the resulting severe
under-sampling causes strong artifacts and poor spatial coverage. We address
these challenges in a unified framework. First, we replace conventional
UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.
Convolutional layers capture local details, while self-attention layers enhance
global context. We adapt TransUNet to CBCT by combining multi-scale features,
querying view-specific features per 3D point, and adding a lightweight
attenuation-prediction head. This yields Trans-CBCT, which surpasses prior
baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.
Second, we introduce a neighbor-aware Point Transformer to enforce volumetric
coherence. This module uses 3D positional encoding and attention over k-nearest
neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,
provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on
LUNA16 and ToothFairy show consistent gains from six to ten views, validating
the effectiveness of combining CNN-Transformer features with point-based
geometry reasoning for sparse-view CBCT reconstruction.

</details>


### [163] [SurgSora: Object-Aware Diffusion Model for Controllable Surgical Video Generation](https://arxiv.org/pdf/2412.14018)
*Tong Chen, Shuya Yang, Junyi Wang, Long Bai, Hongliang Ren, Luping Zhou*

Main category: cs.CV

TL;DR: SurgSora is a framework for generating high-fidelity, motion-controllable surgical videos from a single frame and user-specified motion cues, outperforming existing methods in realism and control.


<details>
  <summary>Details</summary>
Motivation: Existing surgical video generation methods lack fine-grained motion control and realism, limiting their utility in medical education and research.

Method: SurgSora uses three modules: Dual Semantic Injector for object-specific RGB-D features, Decoupled Flow Mapper for realistic motion dynamics, and Trajectory Controller for user-guided movement, all integrated with Stable Video Diffusion.

Result: SurgSora achieves state-of-the-art visual authenticity and controllability, validated by quantitative, qualitative, and expert human evaluations.

Conclusion: SurgSora demonstrates high realism and potential for surgical training and education, advancing the field of surgical video synthesis.

Abstract: Surgical video generation can enhance medical education and research, but
existing methods lack fine-grained motion control and realism. We introduce
SurgSora, a framework that generates high-fidelity, motion-controllable
surgical videos from a single input frame and user-specified motion cues.
Unlike prior approaches that treat objects indiscriminately or rely on
ground-truth segmentation masks, SurgSora leverages self-predicted object
features and depth information to refine RGB appearance and optical flow for
precise video synthesis. It consists of three key modules: (1) the Dual
Semantic Injector, which extracts object-specific RGB-D features and
segmentation cues to enhance spatial representations; (2) the Decoupled Flow
Mapper, which fuses multi-scale optical flow with semantic features for
realistic motion dynamics; and (3) the Trajectory Controller, which estimates
sparse optical flow and enables user-guided object movement. By conditioning
these enriched features within the Stable Video Diffusion, SurgSora achieves
state-of-the-art visual authenticity and controllability in advancing surgical
video synthesis, as demonstrated by extensive quantitative and qualitative
comparisons. Our human evaluation in collaboration with expert surgeons further
demonstrates the high realism of SurgSora-generated videos, highlighting the
potential of our method for surgical training and education. Our project is
available at https://surgsora.github.io/surgsora.github.io.

</details>


### [164] [Enhancing Wireless Device Identification through RF Fingerprinting: Leveraging Transient Energy Spectrum Analysis](https://arxiv.org/pdf/2506.17439)
*Nisar Ahmed, Gulshan Saleem, Hafiz Muhammad Shahzad Asif, Muhammad Usman Younus, Kalsoom Safdar*

Main category: cs.CV

TL;DR: The paper proposes a hybrid deep learning model (CNN-Bi-GRU) for identifying RF devices using transient energy spectrum analysis, achieving high accuracy (99.17%).


<details>
  <summary>Details</summary>
Motivation: The exponential growth of IoT and 5G devices in complex electromagnetic environments necessitates reliable identification and classification methods.

Method: Uses General Linear Chirplet Transform for feature extraction and a CNN-Bi-GRU model for classification on a dataset of nine RF devices.

Result: Achieves 99.33% precision, 99.53% recall, 99.43% F1-score, and 99.17% accuracy in 10-fold cross-validation.

Conclusion: The CNN-Bi-GRU model is effective for RF device identification, offering potential for improved management in wireless environments.

Abstract: In recent years, the rapid growth of the Internet of Things technologies and
the widespread adoption of 5G wireless networks have led to an exponential
increase in the number of radiation devices operating in complex
electromagnetic environments. A key challenge in managing and securing these
devices is accurate identification and classification. To address this
challenge, specific emitter identification techniques have emerged as a
promising solution that aims to provide reliable and efficient means of
identifying individual radiation devices in a unified and standardized manner.
This research proposes an approach that leverages transient energy spectrum
analysis using the General Linear Chirplet Transform to extract features from
RF devices. A dataset comprising nine RF devices is utilized, with each sample
containing 900 attributes and a total of 1080 equally distributed samples
across the devices. These features are then used in a classification modeling
framework. To overcome the limitations of conventional machine learning
methods, we introduce a hybrid deep learning model called the CNN-Bi-GRU for
learning the identification of RF devices based on their transient
characteristics. The proposed approach provided a 10-fold cross-validation
performance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%,
and classification accuracy of 99.17%. The results demonstrate the promising
classification performance of the CNN-Bi-GRU approach, indicating its
suitability for accurately identifying RF devices based on their transient
characteristics and its potential for enhancing device identification and
classification in complex wireless environments.

</details>


### [165] [LAPIG: Language Guided Projector Image Generation with Surface Adaptation and Stylization](https://arxiv.org/pdf/2503.12173)
*Yuchen Deng, Haibin Ling, Bingyao Huang*

Main category: cs.CV

TL;DR: LAPIG is a method for generating projector images guided by user text prompts, adapting to surface textures and stylization to minimize artifacts.


<details>
  <summary>Details</summary>
Motivation: Projector images often suffer from color saturation and surface texture artifacts due to physical limitations. LAPIG aims to address this by generating visually pleasing projections.

Method: LAPIG uses projection surface adaptation (PSA) with trained networks for simulation and gradient descent. It employs content and saturation losses to guide artifact-free image generation.

Result: The method successfully generates projector images with minimal artifacts, enabling visually pleasing surface style morphing.

Conclusion: LAPIG effectively addresses the challenge of surface artifacts in projector images, offering a practical solution for text-guided projection.

Abstract: We propose LAPIG, a language guided projector image generation method with
surface adaptation and stylization. LAPIG consists of a projector-camera system
and a target textured projection surface. LAPIG takes the user text prompt as
input and aims to transform the surface style using the projector. LAPIG's key
challenge is that due to the projector's physical brightness limitation and the
surface texture, the viewer's perceived projection may suffer from color
saturation and artifacts in both dark and bright regions, such that even with
the state-of-the-art projector compensation techniques, the viewer may see
clear surface texture-related artifacts. Therefore, how to generate a projector
image that follows the user's instruction while also displaying minimum surface
artifacts is an open problem. To address this issue, we propose projection
surface adaptation (PSA) that can generate compensable surface stylization. We
first train two networks to simulate the projector compensation and
project-and-capture processes, this allows us to find a satisfactory projector
image without real project-and-capture and utilize gradient descent for fast
convergence. Then, we design content and saturation losses to guide the
projector image generation, such that the generated image shows no clearly
perceivable artifacts when projected. Finally, the generated image is projected
for visually pleasing surface style morphing effects. The source code and video
are available on the project page: https://Yu-chen-Deng.github.io/LAPIG/.

</details>


### [166] [Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling](https://arxiv.org/pdf/2506.17838)
*Kazuki Naganuma, Shunsuke Ono*

Main category: cs.CV

TL;DR: A novel foreground-background separation method using convolutional sparse representation (CSR) is proposed to handle degraded videos with low frame rates and noise.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to accurately separate foreground and background in degraded videos due to limitations in capturing both data-specific and general features, and lack of explicit noise models.

Method: The method uses a CSR-based foreground model and formulates FBS as a constrained multiconvex optimization problem, incorporating CSR, general feature functions, and noise characterization. An alternating algorithm solves the subproblems.

Result: The method outperforms existing techniques on degraded infrared and microscope videos.

Conclusion: The proposed CSR-based FBS method effectively separates foreground and background in challenging video conditions, addressing limitations of prior approaches.

Abstract: This paper proposes a foreground-background separation (FBS) method with a
novel foreground model based on convolutional sparse representation (CSR). In
order to analyze the dynamic and static components of videos acquired under
undesirable conditions, such as hardware, environmental, and power limitations,
it is essential to establish an FBS method that can handle videos with low
frame rates and various types of noise. Existing FBS methods have two
limitations that prevent us from accurately separating foreground and
background components from such degraded videos. First, they only capture
either data-specific or general features of the components. Second, they do not
include explicit models for various types of noise to remove them in the FBS
process. To this end, we propose a robust FBS method with a CSR-based
foreground model. This model can adaptively capture specific spatial structures
scattered in imaging data. Then, we formulate FBS as a constrained multiconvex
optimization problem that incorporates CSR, functions that capture general
features, and explicit noise characterization functions for multiple types of
noise. Thanks to these functions, our method captures both data-specific and
general features to accurately separate the components from various types of
noise even under low frame rates. To obtain a solution of the optimization
problem, we develop an algorithm that alternately solves its two convex
subproblems by newly established algorithms. Experiments demonstrate the
superiority of our method over existing methods using two types of degraded
videos: infrared and microscope videos.

</details>


### [167] [AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions](https://arxiv.org/pdf/2506.17455)
*Taufikur Rahman Fuad, Sabbir Ahmed, Shahriar Ivan*

Main category: cs.CV

TL;DR: The paper introduces AQUA20, a benchmark dataset for underwater species recognition, evaluates 13 deep learning models, and finds ConvNeXt as the top performer, highlighting trade-offs between model complexity and performance.


<details>
  <summary>Details</summary>
Motivation: Underwater visual recognition is challenging due to distortions like turbidity and low illumination, necessitating robust datasets and models.

Method: The study evaluates 13 deep learning models (including CNNs and transformers) on the AQUA20 dataset, using metrics like Top-1/Top-3 accuracy and F1-score, and includes explainability analysis with GRAD-CAM and LIME.

Result: ConvNeXt achieved the best performance (Top-3: 98.82%, Top-1: 90.69%, F1-score: 88.92%), with other models showing complexity-performance trade-offs.

Conclusion: AQUA20 is a valuable resource for underwater recognition research, with room for improvement in model performance, and the dataset is publicly available.

Abstract: Robust visual recognition in underwater environments remains a significant
challenge due to complex distortions such as turbidity, low illumination, and
occlusion, which severely degrade the performance of standard vision systems.
This paper introduces AQUA20, a comprehensive benchmark dataset comprising
8,171 underwater images across 20 marine species reflecting real-world
environmental challenges such as illumination, turbidity, occlusions, etc.,
providing a valuable resource for underwater visual understanding. Thirteen
state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet,
MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were
evaluated to benchmark their performance in classifying marine species under
challenging conditions. Our experimental results show ConvNeXt achieving the
best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of
90.69%, as well as the highest overall F1-score of 88.92% with moderately large
parameter size. The results obtained from our other benchmark models also
demonstrate trade-offs between complexity and performance. We also provide an
extensive explainability analysis using GRAD-CAM and LIME for interpreting the
strengths and pitfalls of the models. Our results reveal substantial room for
improvement in underwater species recognition and demonstrate the value of
AQUA20 as a foundation for future research in this domain. The dataset is
publicly available at: https://huggingface.co/datasets/taufiktrf/AQUA20.

</details>


### [168] [When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network](https://arxiv.org/pdf/2506.17457)
*Dong Xiao, Guangyao Chen, Peixi Peng, Yangru Huang, Yifan Zhao, Yongxing Dai, Yonghong Tian*

Main category: cs.CV

TL;DR: Proposes a real-time anomaly detection method for autonomous driving using a multimodal asynchronous hybrid network combining event and RGB cameras for high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods prioritize accuracy over response time, which is critical for autonomous driving safety.

Method: A novel multimodal asynchronous hybrid network integrates event camera data (via an asynchronous Graph Neural Network) with RGB camera data (via CNN) to capture temporal and spatial details.

Result: Outperforms existing methods in accuracy and response time, achieving millisecond-level real-time performance.

Conclusion: The proposed method effectively addresses the need for fast and accurate anomaly detection in autonomous driving.

Abstract: Anomaly detection is essential for the safety and reliability of autonomous
driving systems. Current methods often focus on detection accuracy but neglect
response time, which is critical in time-sensitive driving scenarios. In this
paper, we introduce real-time anomaly detection for autonomous driving,
prioritizing both minimal response time and high accuracy. We propose a novel
multimodal asynchronous hybrid network that combines event streams from event
cameras with image data from RGB cameras. Our network utilizes the high
temporal resolution of event cameras through an asynchronous Graph Neural
Network and integrates it with spatial features extracted by a CNN from RGB
images. This combination effectively captures both the temporal dynamics and
spatial details of the driving environment, enabling swift and precise anomaly
detection. Extensive experiments on benchmark datasets show that our approach
outperforms existing methods in both accuracy and response time, achieving
millisecond-level real-time performance.

</details>


### [169] [TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration](https://arxiv.org/pdf/2506.16784)
*Xiaoyu Shi, Rahul Kumar Jain, Yinhao Li, Ruibo Hou, Jingliang Cheng, Jie Bai, Guohua Zhao, Lanfen Lin, Rui Xu, Yen-wei Chen*

Main category: cs.CV

TL;DR: The paper introduces TextBraTS, a multimodal dataset combining MRI volumes and textual annotations for brain tumor segmentation, and proposes a novel framework for text-guided segmentation, showing improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing brain tumor segmentation lacks multimodal datasets combining imaging and textual data, limiting exploration of text-guided approaches.

Method: The authors introduce the TextBraTS dataset and propose a baseline framework with sequential cross-attention for text-guided volumetric segmentation.

Result: Experiments show significant improvements in segmentation accuracy using multimodal integration.

Conclusion: The TextBraTS dataset and proposed framework advance multimodal brain tumor segmentation, with publicly available resources for further research.

Abstract: Deep learning has demonstrated remarkable success in medical image
segmentation and computer-aided diagnosis. In particular, numerous advanced
methods have achieved state-of-the-art performance in brain tumor segmentation
from MRI scans. While recent studies in other medical imaging domains have
revealed that integrating textual reports with visual data can enhance
segmentation accuracy, the field of brain tumor analysis lacks a comprehensive
dataset that combines radiological images with corresponding textual
annotations. This limitation has hindered the exploration of multimodal
approaches that leverage both imaging and textual data.
  To bridge this critical gap, we introduce the TextBraTS dataset, the first
publicly available volume-level multimodal dataset that contains paired MRI
volumes and rich textual annotations, derived from the widely adopted BraTS2020
benchmark. Building upon this novel dataset, we propose a novel baseline
framework and sequential cross-attention method for text-guided volumetric
medical image segmentation. Through extensive experiments with various
text-image fusion strategies and templated text formulations, our approach
demonstrates significant improvements in brain tumor segmentation accuracy,
offering valuable insights into effective multimodal integration techniques.
  Our dataset, implementation code, and pre-trained models are publicly
available at https://github.com/Jupitern52/TextBraTS.

</details>


### [170] [Photogranulometry -- Dataset of soil images with corresponding particle size distributions](https://arxiv.org/pdf/2506.17469)
*Thomas Plante St-Cyr, Franois Duhaime, Jean-Sbastien Dub, Simon Grenier*

Main category: cs.CV

TL;DR: A high-resolution dataset of soil samples is presented to train CNNs for optical grain size analysis, reducing downtime and costs of traditional PSD methods.


<details>
  <summary>Details</summary>
Motivation: Traditional PSD analyses are costly and time-consuming; optical analysis integrated into workflows could mitigate these issues.

Method: 12,714 images of 321 soil samples were captured in standardized conditions (45 MP resolution, 39.4 m/pixel) in moist and dry states, using a custom test bench.

Result: The dataset provides a robust foundation for training CNNs in geotechnical applications.

Conclusion: Optical grain size analysis with CNNs offers a promising alternative to traditional PSD methods.

Abstract: Traditional particle size distribution (PSD) analyses create significant
downtime and are expensive in labor and maintenance. These drawbacks could be
alleviated using optical grain size analysis integrated into routine
geotechnical laboratory workflow. This paper presents a high-resolution dataset
of 12,714 images of 321 different soil samples collected in the Montreal,
Quebec region, alongside their PSD analysis. It is designed to provide a robust
starting point for training convolutional neural networks (CNN) in geotechnical
applications. Soil samples were photographed in a standardized top-view
position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per
pixel, both in their moist and dry states. A custom test bench employing 13x9
inch white aluminum trays, on which the samples are spread in a thin layer, was
used. For samples exceeding a size limit, a coning and quartering method was
employed for mass reduction.

</details>


### [171] [Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation](https://arxiv.org/pdf/2506.17500)
*Julio Silva-Rodrguez, Fereshteh Shakeri, Houda Bahig, Jose Dolz, Ismail Ben Ayed*

Main category: cs.CV

TL;DR: The paper highlights challenges in deploying vision-language models (VLMs) for medical image analysis due to unrealistic assumptions about data distribution. It proposes a realistic, imbalanced, validation-free adaptation setting and introduces a training-free linear probe for robust performance.


<details>
  <summary>Details</summary>
Motivation: Current VLMs assume balanced support sets and validation data, which are impractical in medical contexts due to natural disease prevalence imbalance and data inefficiency.

Method: The work introduces a realistic adaptation setting and a training-free linear probe that blends visual and textual supervision adaptively.

Result: Benchmarks show current methods underperform in realistic conditions, sometimes worse than zero-shot inference. The proposed solver proves robust and efficient.

Conclusion: The study advocates for realistic adaptation settings in medical VLMs and presents a strong baseline method for challenging scenarios.

Abstract: Vision-language models (VLMs) are gaining attention in medical image
analysis. These are pre-trained on large, heterogeneous data sources, yielding
rich and transferable representations. Notably, the combination of
modality-specialized VLMs with few-shot adaptation has provided fruitful
results, enabling the efficient deployment of high-performing solutions.
However, previous works on this topic make strong assumptions about the
distribution of adaptation data, which are unrealistic in the medical domain.
First, prior art assumes access to a balanced support set, a condition that
breaks the natural imbalance in disease prevalence found in real-world
scenarios. Second, these works typically assume the presence of an additional
validation set to fix critical hyper-parameters, which is highly
data-inefficient. This work challenges these favorable deployment scenarios and
introduces a realistic, imbalanced, validation-free adaptation setting. Our
extensive benchmark across various modalities and downstream tasks demonstrates
that current methods systematically compromise their performance when operating
under realistic conditions, occasionally even performing worse than zero-shot
inference. Also, we introduce a training-free linear probe that adaptively
blends visual and textual supervision. Detailed studies demonstrate that the
proposed solver is a strong, efficient baseline, enabling robust adaptation in
challenging scenarios.

</details>


### [172] [Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction](https://arxiv.org/pdf/2506.17503)
*Julio Silva-Rodrguez, Ismail Ben Ayed, Jose Dolz*

Main category: cs.CV

TL;DR: The paper proposes a novel pipeline, SCA-T, to improve trustworthiness in medical VLMs by addressing suboptimal adaptation in conformal prediction, ensuring efficiency and coverage.


<details>
  <summary>Details</summary>
Motivation: Despite the growing use of medical VLMs, their reliability remains unexplored, especially in conformal prediction scenarios where standard adaptation methods break exchangeability assumptions.

Method: The authors introduce SCA-T, a transductive adaptation method for conformal prediction, which jointly adapts calibration and test data without breaking exchangeability.

Result: Experiments show SCA-T improves efficiency and conditional coverage over standard SCP while maintaining empirical guarantees.

Conclusion: SCA-T provides a reliable solution for conformal prediction in medical VLMs, addressing adaptation challenges and ensuring trustworthiness.

Abstract: Medical vision-language models (VLMs) have demonstrated unprecedented
transfer capabilities and are being increasingly adopted for data-efficient
image classification. Despite its growing popularity, its reliability aspect
remains largely unexplored. This work explores the split conformal prediction
(SCP) framework to provide trustworthiness guarantees when transferring such
models based on a small labeled calibration set. Despite its potential, the
generalist nature of the VLMs' pre-training could negatively affect the
properties of the predicted conformal sets for specific tasks. While common
practice in transfer learning for discriminative purposes involves an
adaptation stage, we observe that deploying such a solution for conformal
purposes is suboptimal since adapting the model using the available calibration
data breaks the rigid exchangeability assumptions for test data in SCP. To
address this issue, we propose transductive split conformal adaptation (SCA-T),
a novel pipeline for transfer learning on conformal scenarios, which performs
an unsupervised transductive adaptation jointly on calibration and test data.
We present comprehensive experiments utilizing medical VLMs across various
image modalities, transfer tasks, and non-conformity scores. Our framework
offers consistent gains in efficiency and conditional coverage compared to SCP,
maintaining the same empirical guarantees.

</details>


### [173] [Learning golf swing signatures from a single wrist-worn inertial sensor](https://arxiv.org/pdf/2506.17505)
*Jessy Lauer*

Main category: cs.CV

TL;DR: A data-driven framework for personalized golf swing analysis using a wrist-worn sensor, reconstructing full-body kinematics, and detecting technical flaws with explainable AI.


<details>
  <summary>Details</summary>
Motivation: Address limitations in golf swing analysis, such as isolated metrics and lack of professional athlete data, by providing a holistic, interpretable approach.

Method: Use a wrist-worn sensor, reconstruct 3D kinematics from videos, generate synthetic inertial data, train neural networks, and learn motion primitives for analysis.

Result: Accurate full-body kinematics estimation, swing phase segmentation, and detection of technical flaws, with applications in coaching and injury prevention.

Conclusion: Challenges assumptions about swing consistency and ideal swings, offering scalable, high-fidelity motion analysis for research and practical use.

Abstract: Despite its importance for performance and injury prevention, golf swing
analysis is limited by isolated metrics, underrepresentation of professional
athletes, and a lack of rich, interpretable movement representations. We
address these gaps with a holistic, data-driven framework for personalized golf
swing analysis from a single wrist-worn sensor. We build a large dataset of
professional swings from publicly available videos, reconstruct full-body 3D
kinematics using biologically accurate human mesh recovery, and generate
synthetic inertial data to train neural networks that infer motion and segment
swing phases from wrist-based input. We learn a compositional, discrete
vocabulary of motion primitives that facilitates the detection and
visualization of technical flaws, and is expressive enough to predict player
identity, club type, sex, and age. Our system accurately estimates full-body
kinematics and swing events from wrist data, delivering lab-grade motion
analysis on-course and supporting early detection of anomalous movement
patterns. Explainability methods reveal subtle, individualized movement
signatures, reinforcing the view that variability is a hallmark of skilled
performance. Longitudinal tracking demonstrates practical value: as one
player's handicap improved from 50 to 2.2 over 1.5 years, our system captured
measurable technical progress and provided targeted, actionable feedback. Our
findings challenge common assumptions, such as swing consistency across clubs
and the existence of a single "ideal" swing, and uncover latent biomarkers
shaped by both intrinsic traits and task-specific constraints. This work
bridges lab and field-based biomechanics, offering scalable, accessible,
high-fidelity motion analysis for research, coaching, and injury prevention,
while opening new directions in movement-based phenotyping, personalized
equipment design, and motor skill development.

</details>


### [174] [Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations](https://arxiv.org/pdf/2506.17545)
*Zhihao Yuan, Shuyi Jiang, Chun-Mei Feng, Yaolun Zhang, Shuguang Cui, Zhen Li, Na Zhao*

Main category: cs.CV

TL;DR: Scene-R1 is a video-grounded framework for 3D scene understanding without 3D instance supervision, using reinforcement learning and a two-stage grounding pipeline to achieve transparent reasoning and fine geometry capture.


<details>
  <summary>Details</summary>
Motivation: Existing 3D-aware LLMs act as black boxes and rely on pre-trained 3D detectors. Scene-R1 aims to provide transparent reasoning and eliminate the need for 3D detector-based proposals.

Method: Scene-R1 uses a two-stage grounding pipeline: temporal grounding to select relevant video snippets and image grounding to predict 2D bounding boxes. It tracks objects with SAM2 for pixel-accurate masks and projects them into 3D.

Result: Scene-R1 outperforms open-vocabulary baselines on multiple datasets and provides step-by-step rationales.

Conclusion: Reinforcement-learning-based reasoning with RGB-D video offers an efficient, annotation-light path to trustworthy 3D scene understanding.

Abstract: Currently, utilizing large language models to understand the 3D world is
becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output
bounding boxes or textual answers without revealing how those decisions are
made, and they still rely on pre-trained 3D detectors to supply object
proposals. We introduce Scene-R1, a video-grounded framework that learns to
reason about 3D scenes without any point-wise 3D instance supervision by
pairing reinforcement-learning-driven reasoning with a two-stage grounding
pipeline. In the temporal grounding stage, we explicitly reason about the video
and select the video snippets most relevant to an open-ended query. In the
subsequent image grounding stage, we analyze the image and predict the 2D
bounding box. After that, we track the object using SAM2 to produce
pixel-accurate masks in RGB frames, and project them back into 3D, thereby
eliminating the need for 3D detector-based proposals while capturing fine
geometry and material cues. Scene-R1 can also adapt to the 3D visual question
answering task to answer free-form questions directly from video. Our training
pipeline only needs task-level 2D boxes or textual labels without dense 3D
point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on
multiple datasets, while delivering transparent, step-by-step rationales. These
results show that reinforcement-learning-based reasoning combined with RGB-D
video alone offers a practical, annotation-efficient route to trustworthy 3D
scene understanding.

</details>


### [175] [SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference](https://arxiv.org/pdf/2506.17558)
*Jake Levi, Mark van der Wilk*

Main category: cs.CV

TL;DR: The paper introduces SynDaCaTE, a synthetic dataset for evaluating capsule networks, identifies a bottleneck in existing models, and highlights the effectiveness of permutation-equivariant self-attention for parts-to-wholes inference.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating whether capsule networks truly learn part-whole hierarchies as claimed, due to reliance on supervised tasks like object classification.

Method: Creation of SynDaCaTE, a synthetic dataset, to test and evaluate capsule models. Analysis of existing models and exploration of permutation-equivariant self-attention for parts-to-wholes inference.

Result: Identified a bottleneck in a prominent capsule model and demonstrated the effectiveness of permutation-equivariant self-attention for parts-to-wholes inference.

Conclusion: SynDaCaTE is a valuable tool for evaluating capsule networks, and permutation-equivariant self-attention shows promise for improving inductive biases in computer vision.

Abstract: Learning to infer object representations, and in particular part-whole
hierarchies, has been the focus of extensive research in computer vision, in
pursuit of improving data efficiency, systematic generalisation, and
robustness. Models which are \emph{designed} to infer part-whole hierarchies,
often referred to as capsule networks, are typically trained end-to-end on
supervised tasks such as object classification, in which case it is difficult
to evaluate whether such a model \emph{actually} learns to infer part-whole
hierarchies, as claimed. To address this difficulty, we present a SYNthetic
DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and
establish its utility by (1) demonstrating the precise bottleneck in a
prominent existing capsule model, and (2) demonstrating that
permutation-equivariant self-attention is highly effective for parts-to-wholes
inference, which motivates future directions for designing effective inductive
biases for computer vision.

</details>


### [176] [VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models](https://arxiv.org/pdf/2506.17561)
*Chongkai Gao, Zixuan Liu, Zhenghao Chi, Junshan Huang, Xin Fei, Yiwen Hou, Yuxuan Zhang, Yudi Lin, Zhirui Fang, Zeyu Jiang, Lin Shao*

Main category: cs.CV

TL;DR: The paper introduces VLA-OS, a unified Vision-Language-Action architecture, to systematically study planning paradigms and representations, finding visually grounded planning and Hierarchical-VLA superior.


<details>
  <summary>Details</summary>
Motivation: To address the variability in existing VLA models and identify key performance drivers by isolating planning paradigms and representations.

Method: Introduces VLA-OS, a unified architecture, and conducts controlled experiments across diverse conditions (object categories, modalities, environments, and end-effectors).

Result: Visually grounded planning outperforms language-based planning, and Hierarchical-VLA excels in performance, generalization, and scalability despite slower speeds.

Conclusion: Hierarchical-VLA with visually grounded planning is recommended for complex tasks, though trade-offs in speed should be considered.

Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the
end-to-end action-generation paradigm toward a pipeline involving task planning
followed by action generation, demonstrating improved performance on various
complex, long-horizon manipulation tasks. However, existing approaches vary
significantly in terms of network architectures, planning paradigms,
representations, and training data sources, making it challenging for
researchers to identify the precise sources of performance gains and components
to be further improved. To systematically investigate the impacts of different
planning paradigms and representations isolating from network architectures and
training data, in this paper, we introduce VLA-OS, a unified VLA architecture
series capable of various task planning paradigms, and design a comprehensive
suite of controlled experiments across diverse object categories (rigid and
deformable), visual modalities (2D and 3D), environments (simulation and
real-world), and end-effectors (grippers and dexterous hands). Our results
demonstrate that: 1) visually grounded planning representations are generally
better than language planning representations; 2) the Hierarchical-VLA paradigm
generally achieves superior or comparable performance than other paradigms on
task performance, pretraining, generalization ability, scalability, and
continual learning ability, albeit at the cost of slower training and inference
speeds.

</details>


### [177] [LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning](https://arxiv.org/pdf/2506.17562)
*Haoxuan Che, Haibo Jin, Zhengrui Guo, Yi Lin, Cheng Jin, Hao Chen*

Main category: cs.CV

TL;DR: FedMRG is a federated learning framework for privacy-preserving, multi-center development of LLM-driven medical report generation, addressing communication overhead and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Centralizing medical image-report pairs is challenging due to privacy regulations, hindering LLM-driven MRG model development.

Method: FedMRG uses low-rank factorization for efficient parameter updates, client-aware contrastive learning, and a dual-adapter mechanism to handle data heterogeneity.

Result: FedMRG demonstrates generalizability, adaptability, and communication efficiency in generating clinically accurate reports.

Conclusion: FedMRG enables effective multi-center collaboration for LLM-driven MRG while preserving privacy and addressing data heterogeneity.

Abstract: LLMs have demonstrated significant potential in Medical Report Generation
(MRG), yet their development requires large amounts of medical image-report
pairs, which are commonly scattered across multiple centers. Centralizing these
data is exceptionally challenging due to privacy regulations, thereby impeding
model development and broader adoption of LLM-driven MRG models. To address
this challenge, we present FedMRG, the first framework that leverages Federated
Learning (FL) to enable privacy-preserving, multi-center development of
LLM-driven MRG models, specifically designed to overcome the critical challenge
of communication-efficient LLM training under multi-modal data heterogeneity.
To start with, our framework tackles the fundamental challenge of communication
overhead in FL-LLM tuning by employing low-rank factorization to efficiently
decompose parameter updates, significantly reducing gradient transmission costs
and making LLM-driven MRG feasible in bandwidth-constrained FL settings.
Furthermore, we observed the dual heterogeneity in MRG under the FL scenario:
varying image characteristics across medical centers, as well as diverse
reporting styles and terminology preferences. To address this, we further
enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,
coupled with diagnosis-driven prompts, which capture both globally
generalizable and locally distinctive features while maintaining diagnostic
accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder
that harmonizes generic and specialized adapters to address variations in
reporting styles and terminology. Through extensive evaluation of our
established FL-MRG benchmark, we demonstrate the generalizability and
adaptability of FedMRG, underscoring its potential in harnessing multi-center
data and generating clinically accurate reports while maintaining communication
efficiency.

</details>


### [178] [Novel Multicolumn Kernel Extreme Learning Machine for Food Detection via Optimal Features from CNN](https://arxiv.org/pdf/2205.07348)
*Ghalib Ahmed Tahir, Chu Kiong Loo*

Main category: cs.CV

TL;DR: A hybrid framework combining MobileNetV3, SHAP, and a novel MCKELM method improves food image recognition accuracy and addresses dimensionality issues in KELM.


<details>
  <summary>Details</summary>
Motivation: The need for accurate food detection in applications like social media and dietary apps, especially during COVID-19 for enforcing eating bans.

Method: Uses MobileNetV3 for feature extraction, SHAP for optimal feature selection, and MCKELM (a parallel KELM with k-d tree) to handle large datasets.

Result: Outperforms existing methods on a large food/non-food dataset, solving KELM's dimensionality problem.

Conclusion: The proposed framework is effective for high-accuracy food detection and scalable for large datasets.

Abstract: Automatic food detection is an emerging topic of interest due to its wide
array of applications ranging from detecting food images on social media
platforms to filtering non-food photos from the users in dietary assessment
apps. Recently, during the COVID-19 pandemic, it has facilitated enforcing an
eating ban by automatically detecting eating activities from cameras in public
places. Therefore, to tackle the challenge of recognizing food images with high
accuracy, we proposed the idea of a hybrid framework for extracting and
selecting optimal features from an efficient neural network. There on, a
nonlinear classifier is employed to discriminate between linearly inseparable
feature vectors with great precision. In line with this idea, our method
extracts features from MobileNetV3, selects an optimal subset of attributes by
using Shapley Additive exPlanations (SHAP) values, and exploits kernel extreme
learning machine (KELM) due to its nonlinear decision boundary and good
generalization ability. However, KELM suffers from the 'curse of dimensionality
problem' for large datasets due to the complex computation of kernel matrix
with large numbers of hidden nodes. We solved this problem by proposing a novel
multicolumn kernel extreme learning machine (MCKELM) which exploited the k-d
tree algorithm to divide data into N subsets and trains separate KELM on each
subset of data. Then, the method incorporates KELM classifiers into parallel
structures and selects the top k nearest subsets during testing by using the
k-d tree search for classifying input instead of the whole network. For
evaluating a proposed framework large food/non-food dataset is prepared using
nine publically available datasets. Experimental results showed the superiority
of our method on an integrated set of measures while solving the problem of
'curse of dimensionality in KELM for large datasets.

</details>


### [179] [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/pdf/2506.17587)
*Le Yu, Kaishen Wang, Jianlong Xiong, Yue Cao, Tao He*

Main category: cs.CV

TL;DR: HalluRNN introduces an architecture-level solution to mitigate hallucinations in LVLMs using a Dual-Gated Depth Propagation Unit (DG-DPU) for recurrent cross-layer reasoning, achieving robust performance with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LVLMs often generate visually ungrounded outputs (hallucinations). Existing solutions are resource-heavy or task-specific, prompting the need for an efficient, architecture-level fix.

Method: Proposes HalluRNN with a DG-DPU module for recurrent refinement of hidden states, enabling adaptive information propagation and layer consistency.

Result: HalluRNN achieves strong, robust performance across benchmarks by fine-tuning only the DG-DPU module.

Conclusion: HalluRNN offers a resource-efficient, architecture-level solution to reduce hallucinations in LVLMs, outperforming prior methods.

Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable
performance across various tasks, they are still prone to
hallucinations-generating outputs that are textually plausible but visually
ungrounded. While prior approaches generally address this issue through
data-centric fine-tuning or innovative decoding strategies, these methods often
require substantial resources or task-specific configurations. In this work, we
introduce an architecture-level solution, HalluRNN, which enhances model
stability through recurrent cross-layer reasoning. Specifically, we propose a
novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across
layers and recurrently refines hidden states. This allows for the adaptive
propagation of information throughout the model, enforces consistency across
layers, and mitigates hallucinations caused by representational drift. By
fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust
performance across multiple benchmarks.

</details>


### [180] [DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving](https://arxiv.org/pdf/2506.17590)
*Mihir Godbole, Xiangbo Gao, Zhengzhong Tu*

Main category: cs.CV

TL;DR: DRAMA-X is a benchmark for evaluating multi-class intent prediction in safety-critical scenarios for autonomous driving, featuring annotations for object detection, intent prediction, risk assessment, and action suggestion. SGG-Intent, a lightweight framework, is proposed as a baseline, showing improved performance with scene-graph-based reasoning.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in benchmarks for fine-grained intent reasoning in safety-critical autonomous driving scenarios, especially for vulnerable road users (VRUs).

Method: Introduces DRAMA-X, a benchmark with automated annotations, and SGG-Intent, a training-free framework using vision-language models (VLMs) and scene-graph-based reasoning.

Result: Scene-graph-based reasoning improves intent prediction and risk assessment, particularly with explicit contextual cues.

Conclusion: DRAMA-X fills a critical gap in evaluating intent reasoning for autonomous driving, and SGG-Intent demonstrates the effectiveness of structured reasoning pipelines.

Abstract: Understanding the short-term motion of vulnerable road users (VRUs) like
pedestrians and cyclists is critical for safe autonomous driving, especially in
urban scenarios with ambiguous or high-risk behaviors. While vision-language
models (VLMs) have enabled open-vocabulary perception, their utility for
fine-grained intent reasoning remains underexplored. Notably, no existing
benchmark evaluates multi-class intent prediction in safety-critical
situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark
constructed from the DRAMA dataset via an automated annotation pipeline.
DRAMA-X contains 5,686 accident-prone frames labeled with object bounding
boxes, a nine-class directional intent taxonomy, binary risk scores,
expert-generated action suggestions for the ego vehicle, and descriptive motion
summaries. These annotations enable a structured evaluation of four
interrelated tasks central to autonomous decision-making: object detection,
intent prediction, risk assessment, and action suggestion. As a reference
baseline, we propose SGG-Intent, a lightweight, training-free framework that
mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene
graph from visual input using VLM-backed detectors, infers intent, assesses
risk, and recommends an action using a compositional reasoning stage powered by
a large language model. We evaluate a range of recent VLMs, comparing
performance across all four DRAMA-X tasks. Our experiments demonstrate that
scene-graph-based reasoning enhances intent prediction and risk assessment,
especially when contextual cues are explicitly modeled.

</details>


### [181] [EmoAgent: A Multi-Agent Framework for Diverse Affective Image Manipulation](https://arxiv.org/pdf/2503.11290)
*Qi Mao, Haobo Hu, Yujie He, Difei Gao, Haokun Chen, Libiao Jin*

Main category: cs.CV

TL;DR: The paper introduces Diverse AIM (D-AIM) and EmoAgent, a multi-agent framework for generating diverse yet emotionally consistent image edits, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing AIM methods use rigid one-to-one emotion-visual mappings, failing to capture subjective human emotion perception.

Method: EmoAgent decomposes the task into three phases: Planning Agent (strategies), Editing Agent (execution), and Critic Agent (refinement).

Result: EmoAgent outperforms state-of-the-art in emotional fidelity and semantic diversity, generating multiple distinct edits.

Conclusion: EmoAgent successfully models one-to-many emotion-visual mappings, enabling diverse and faithful emotional edits.

Abstract: Affective Image Manipulation (AIM) aims to alter visual elements within an
image to evoke specific emotional responses from viewers. However, existing AIM
approaches rely on rigid \emph{one-to-one} mappings between emotions and visual
cues, making them ill-suited for the inherently subjective and diverse ways in
which humans perceive and express emotion.To address this, we introduce a novel
task setting termed \emph{Diverse AIM (D-AIM)}, aiming to generate multiple
visually distinct yet emotionally consistent image edits from a single source
image and target emotion. We propose \emph{EmoAgent}, the first multi-agent
framework tailored specifically for D-AIM. EmoAgent explicitly decomposes the
manipulation process into three specialized phases executed by collaborative
agents: a Planning Agent that generates diverse emotional editing strategies,
an Editing Agent that precisely executes these strategies, and a Critic Agent
that iteratively refines the results to ensure emotional accuracy. This
collaborative design empowers EmoAgent to model \emph{one-to-many}
emotion-to-visual mappings, enabling semantically diverse and emotionally
faithful edits.Extensive quantitative and qualitative evaluations demonstrate
that EmoAgent substantially outperforms state-of-the-art approaches in both
emotional fidelity and semantic diversity, effectively generating multiple
distinct visual edits that convey the same target emotion.

</details>


### [182] [SELFI: Selective Fusion of Identity for Generalizable Deepfake Detection](https://arxiv.org/pdf/2506.17592)
*Younghun Kim, Minsuk Jang, Myung-Joon Kwon, Wonjun Lee, Changick Kim*

Main category: cs.CV

TL;DR: The paper explores the role of face identity in deepfake detection, proposing SELFI, a framework that adaptively controls identity features for better generalization across manipulation methods.


<details>
  <summary>Details</summary>
Motivation: To reconcile conflicting views on using identity features in deepfake detectionwhether to suppress them to avoid bias or rely on them as forensic evidenceby analyzing their discriminative power and generalization across methods.

Method: The authors propose SELFI, which includes a Forgery-Aware Identity Adapter (FAIA) to extract and project identity embeddings, and an Identity-Aware Fusion Module (IAFM) to selectively integrate identity and visual features.

Result: SELFI improves cross-manipulation generalization, outperforming prior methods by 3.1% AUC on average and by 6% on the DFDC dataset.

Conclusion: Identity features should be explicitly modeled and adaptively controlled, not blindly suppressed or relied upon, as demonstrated by SELFI's superior performance.

Abstract: Face identity provides a powerful signal for deepfake detection. Prior
studies show that even when not explicitly modeled, classifiers often learn
identity features implicitly. This has led to conflicting views: some suppress
identity cues to reduce bias, while others rely on them as forensic evidence.
To reconcile these views, we analyze two hypotheses: (1) whether face identity
alone is discriminative for detecting deepfakes, and (2) whether such identity
features generalize poorly across manipulation methods. Our experiments confirm
that identity is informative but context-dependent. While some manipulations
preserve identity-consistent artifacts, others distort identity cues and harm
generalization. We argue that identity features should neither be blindly
suppressed nor relied upon, but instead be explicitly modeled and adaptively
controlled based on per-sample relevance. We propose \textbf{SELFI}
(\textbf{SEL}ective \textbf{F}usion of \textbf{I}dentity), a generalizable
detection framework that dynamically modulates identity usage. SELFI consists
of: (1) a Forgery-Aware Identity Adapter (FAIA) that extracts identity
embeddings from a frozen face recognition model and projects them into a
forgery-relevant space via auxiliary supervision; and (2) an Identity-Aware
Fusion Module (IAFM) that selectively integrates identity and visual features
using a relevance-guided fusion mechanism. Experiments on four benchmarks show
that SELFI improves cross-manipulation generalization, outperforming prior
methods by an average of 3.1\% AUC. On the challenging DFDC dataset, SELFI
exceeds the previous best by 6\%. Code will be released upon paper acceptance.

</details>


### [183] [VesselGPT: Autoregressive Modeling of Vascular Geometry](https://arxiv.org/pdf/2505.13318)
*Paula Feldman, Martin Sinnona, Claudio Delrieux, Viviana Siless, Emmanuel Iarussi*

Main category: cs.CV

TL;DR: An autoregressive method using VQ-VAE and GPT-2 for synthesizing anatomical trees, achieving high-fidelity reconstruction with compact representations and preserving morphological details.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurately representing complex and diverse anatomical trees for clinical applications.

Method: Embeds vessel structures into a discrete vocabulary with VQ-VAE, then models generation autoregressively with GPT-2. Uses B-spline for vessel cross-sections.

Result: High-fidelity tree reconstruction with compact representations and preserved morphological details.

Conclusion: First autoregressive method for blood vessel generation, offering realistic synthesis and improved detail preservation.

Abstract: Anatomical trees are critical for clinical diagnosis and treatment planning,
yet their complex and diverse geometry make accurate representation a
significant challenge. Motivated by the latest advances in large language
models, we introduce an autoregressive method for synthesizing anatomical
trees. Our approach first embeds vessel structures into a learned discrete
vocabulary using a VQ-VAE architecture, then models their generation
autoregressively with a GPT-2 model. This method effectively captures intricate
geometries and branching patterns, enabling realistic vascular tree synthesis.
Comprehensive qualitative and quantitative evaluations reveal that our
technique achieves high-fidelity tree reconstruction with compact discrete
representations. Moreover, our B-spline representation of vessel cross-sections
preserves critical morphological details that are often overlooked in previous'
methods parameterizations. To the best of our knowledge, this work is the first
to generate blood vessels in an autoregressive manner. Code is available at
https://github.com/LIA-DiTella/VesselGPT-MICCAI.

</details>


### [184] [A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data](https://arxiv.org/pdf/2506.17596)
*Wei Huang, Yinxuan Xu, Yintao Zhou, Zhengyu Li, Jing Huang, Meng Pang*

Main category: cs.CV

TL;DR: A novel multimodal method for early Parkinson's disease (PD) diagnosis using facial expressions and gait, addressing data and equipment limitations with a lightweight deep learning model.


<details>
  <summary>Details</summary>
Motivation: Early PD detection is crucial due to its incurable nature and rapid progression, but existing methods face challenges like limited data, specialized equipment, and single-modality risks.

Method: Proposes a multimodal approach combining facial expressions and gait, using a lightweight deep learning model for feature extraction and fusion, validated on a large dataset.

Result: The method improves diagnostic accuracy and is deployable on mobile devices, supported by extensive experiments.

Conclusion: The multimodal approach effectively addresses current diagnostic challenges, offering a practical and accurate solution for early PD detection.

Abstract: Parkinson's disease (PD), characterized by its incurable nature, rapid
progression, and severe disability, poses significant challenges to the lives
of patients and their families. Given the aging population, the need for early
detection of PD is increasing. In vitro diagnosis has garnered attention due to
its non-invasive nature and low cost. However, existing methods present several
challenges: 1) limited training data for facial expression diagnosis; 2)
specialized equipment and acquisition environments required for gait diagnosis,
resulting in poor generalizability; 3) the risk of misdiagnosis or missed
diagnosis when relying on a single modality. To address these issues, we
propose a novel multimodal in vitro diagnostic method for PD, leveraging facial
expressions and behavioral gait. Our method employs a lightweight deep learning
model for feature extraction and fusion, aimed at improving diagnostic accuracy
and facilitating deployment on mobile devices. Furthermore, we have established
the largest multimodal PD dataset in collaboration with a hospital and
conducted extensive experiments to validate the effectiveness of our proposed
method.

</details>


### [185] [FARCLUSS: Fuzzy Adaptive Rebalancing and Contrastive Uncertainty Learning for Semi-Supervised Semantic Segmentation](https://arxiv.org/pdf/2506.11142)
*Ebenezer Tarubinga, Jenifer Kalafatovich, Seong-Whan Lee*

Main category: cs.CV

TL;DR: A framework for semi-supervised semantic segmentation addresses challenges like ineffective pseudo-label use and class imbalance by leveraging uncertainty through fuzzy pseudo-labeling, dynamic weighting, adaptive rebalancing, and contrastive regularization.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with unlabeled data, discarding uncertain regions and favoring dominant classes, leading to biases and poor performance on underrepresented classes.

Method: The proposed framework includes fuzzy pseudo-labeling, uncertainty-aware dynamic weighting, adaptive class rebalancing, and lightweight contrastive regularization.

Result: The method outperforms state-of-the-art approaches, especially in segmenting underrepresented classes and ambiguous regions.

Conclusion: The holistic approach effectively transforms uncertainty into a learning asset, improving segmentation performance.

Abstract: Semi-supervised semantic segmentation (SSSS) faces persistent challenges in
effectively leveraging unlabeled data, such as ineffective utilization of
pseudo-labels, exacerbation of class imbalance biases, and neglect of
prediction uncertainty. Current approaches often discard uncertain regions
through strict thresholding favouring dominant classes. To address these
limitations, we introduce a holistic framework that transforms uncertainty into
a learning asset through four principal components: (1) fuzzy pseudo-labeling,
which preserves soft class distributions from top-K predictions to enrich
supervision; (2) uncertainty-aware dynamic weighting, that modulate pixel-wise
contributions via entropy-based reliability scores; (3) adaptive class
rebalancing, which dynamically adjust losses to counteract long-tailed class
distributions; and (4) lightweight contrastive regularization, that encourage
compact and discriminative feature embeddings. Extensive experiments on
benchmarks demonstrate that our method outperforms current state-of-the-art
approaches, achieving significant improvements in the segmentation of
under-represented classes and ambiguous regions.

</details>


### [186] [OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor](https://arxiv.org/pdf/2506.17597)
*Pengyu Kan, Craig Jones, Kenichi Oishi*

Main category: cs.CV

TL;DR: A transformer-based model for interpretable and robust brain age prediction from MRI scans, achieving high accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: To address the need for an interpretable and robust age prediction model that handles demographic and technological variances in brain MRI scans.

Method: Proposed a transformer-based architecture with self-supervised pre-training, processing pseudo-3D MRI scans from three views and incorporating brain volumetric data. Reduced quadratic complexity to linear for scalability.

Result: Achieved MAE of 3.65 years on test sets (ADNI2 & 3, OASIS3) and 3.54 years on AIBL. Notable brain age gap differences across cognitive groups and significant correlations with cognitive scores.

Conclusion: The model successfully fused multi-view and volumetric data for accurate, generalizable, and interpretable brain age prediction, linking to neurodegenerative disorders.

Abstract: Purpose: To develop an age prediction model which is interpretable and robust
to demographic and technological variances in brain MRI scans. Materials and
Methods: We propose a transformer-based architecture that leverages
self-supervised pre-training on large-scale datasets. Our model processes
pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates
brain volumetric information. By introducing a stem architecture, we reduce the
conventional quadratic complexity of transformer models to linear complexity,
enabling scalability for high-dimensional MRI data. We trained our model on
ADNI2 $\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the
North America, with an 8:1:1 split for train, validation and test. Then, we
validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.
Results: We achieved an MAE of 3.65 years on ADNI2 $\&$ 3 and OASIS3 test set
and a high generalizability of MAE of 3.54 years on AIBL. There was a notable
increase in brain age gap (BAG) across cognitive groups, with mean of 0.15
years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12
years ([5.82, 6.43]) in AD. Additionally, significant negative correlation
between BAG and cognitive scores was observed, with correlation coefficient of
-0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based
feature attribution highlighted ventricles and white matter structures as key
regions influenced by brain aging. Conclusion: Our model effectively fused
information from different views and volumetric information to achieve
state-of-the-art brain age prediction accuracy, improved generalizability and
interpretability with association to neurodegenerative disorders.

</details>


### [187] [Inference-Time Gaze Refinement for Micro-Expression Recognition: Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing](https://arxiv.org/pdf/2506.12524)
*Nuwan Bandara, Thivya Kandappu, Archan Misra*

Main category: cs.CV

TL;DR: A framework enhances event-based gaze estimation models without altering their architecture, improving temporal smoothness and spatial accuracy for cognitive state inference.


<details>
  <summary>Details</summary>
Motivation: Event-based eye tracking is promising for fine-grained cognitive state inference but requires refinement for better consistency and accuracy.

Method: Introduces two post-processing modules: Motion-Aware Median Filtering and Optical Flow-Based Local Refinement, along with a novel Jitter Metric for temporal smoothness.

Result: Significantly improves gaze signal consistency and accuracy across baseline models, making them suitable for tasks like micro-expression analysis.

Conclusion: The framework advances event-based gaze tracking, enabling better integration with multimodal affect recognition systems in real-world settings.

Abstract: Event-based eye tracking holds significant promise for fine-grained cognitive
state inference, offering high temporal resolution and robustness to motion
artifacts, critical features for decoding subtle mental states such as
attention, confusion, or fatigue. In this work, we introduce a model-agnostic,
inference-time refinement framework designed to enhance the output of existing
event-based gaze estimation models without modifying their architecture or
requiring retraining. Our method comprises two key post-processing modules: (i)
Motion-Aware Median Filtering, which suppresses blink-induced spikes while
preserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement,
which aligns gaze predictions with cumulative event motion to reduce spatial
jitter and temporal discontinuities. To complement traditional spatial accuracy
metrics, we propose a novel Jitter Metric that captures the temporal smoothness
of predicted gaze trajectories based on velocity regularity and local signal
complexity. Together, these contributions significantly improve the consistency
of event-based gaze signals, making them better suited for downstream tasks
such as micro-expression analysis and mind-state decoding. Our results
demonstrate consistent improvements across multiple baseline models on
controlled datasets, laying the groundwork for future integration with
multimodal affect recognition systems in real-world environments.

</details>


### [188] [HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs](https://arxiv.org/pdf/2506.17608)
*Nikitha SR, Aradhya Neeraj Mathur, Tarun Ram Menta, Rishabh Jain, Mausoom Sarkar*

Main category: cs.CV

TL;DR: A shallow feature enricher reduces computational costs while maintaining competitive performance in high-resolution multimodal models.


<details>
  <summary>Details</summary>
Motivation: High-resolution image features improve visual understanding but increase computational costs due to large encoders like ViT.

Method: Proposes a shallow feature enricher for feature upsampling, reducing calls to large encoders.

Result: Achieves competitive results with 1.5x FLOPs savings in training and inference.

Conclusion: Feature upsampling via a shallow enricher balances performance and efficiency in multimodal models.

Abstract: The integration of high-resolution image features in modern multimodal large
language models has demonstrated significant improvements in fine-grained
visual understanding tasks, achieving high performance across multiple
benchmarks. Since these features are obtained from large image encoders like
ViT, they come with a significant increase in computational costs due to
multiple calls to these encoders. In this work, we first develop an intuition
for feature upsampling as a natural extension of high-resolution feature
generation. Through extensive experiments and ablations, we demonstrate how a
shallow feature enricher can achieve competitive results with tremendous
reductions in training and inference time as well as computational cost, with
upto 1.5x saving in FLOPs.

</details>


### [189] [Model-Agnostic, Temperature-Informed Sampling Enhances Cross-Year Crop Mapping with Deep Learning](https://arxiv.org/pdf/2506.12885)
*Mehmet Ozgur Turkoglu, Selene Ledain, Helge Aasen*

Main category: cs.CV

TL;DR: A novel sampling strategy using growing degree days (GDD) improves crop type classification accuracy and uncertainty estimation across seasons, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional benchmarks, such as poor generalization across seasons and lack of real-time applicability due to fixed calendar-day sampling and missing uncertainty quantification.

Method: Proposes a model-agnostic sampling strategy using GDD (thermal time) to replace calendar time, focusing on phenologically active growth stages. Evaluated on multi-year Sentinel-2 data from Switzerland.

Result: Substantial gains in classification accuracy and better-calibrated uncertainty estimates, especially in low-data regimes and early-season classification.

Conclusion: Leveraging temperature data enhances predictive performance, robustness, and trustworthiness in crop-type mapping.

Abstract: Conventional benchmarks for crop type classification from optical satellite
time series typically assume access to labeled data from the same year and rely
on fixed calendar-day sampling. This limits generalization across seasons,
where crop phenology shifts due to interannual climate variability, and
precludes real-time application when current-year labels are unavailable.
Furthermore, uncertainty quantification is often neglected, making such
approaches unreliable for crop monitoring applications. Inspired by
ecophysiological principles of plant growth, we propose a simple,
model-agnostic sampling strategy that leverages growing degree days (GDD),
based on daily average temperature, to replace calendar time with thermal time.
By uniformly subsampling time series in this biologically meaningful domain,
the method emphasizes phenologically active growth stages while reducing
temporal redundancy and noise. We evaluate the method on a multi-year
Sentinel-2 dataset spanning all of Switzerland, training on one growing season
and testing on other seasons. Compared to state-of-the-art baselines, our
method delivers substantial gains in classification accuracy and, critically,
produces more calibrated uncertainty estimates. Notably, our method excels in
low-data regimes and enables significantly more accurate early-season
classification. With only 10 percent of the training data, our method surpasses
the state-of-the-art baseline in both predictive accuracy and uncertainty
estimation, and by the end of June, it achieves performance similar to a
baseline trained on the full season. These results demonstrate that leveraging
temperature data not only improves predictive performance across seasons but
also enhances the robustness and trustworthiness of crop-type mapping in
real-world applications.

</details>


### [190] [JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent](https://arxiv.org/pdf/2506.17612)
*Yunlong Lin, Zixu Lin, Kunjie Lin, Jinbin Bai, Panwang Pan, Chenxin Li, Haoyu Chen, Zhongdao Wang, Xinghao Ding, Wenbo Li, Shuicheng Yan*

Main category: cs.CV

TL;DR: JarvisArt is an AI-driven agent for photo retouching, combining user intent understanding with professional artist reasoning, outperforming GPT-4o in fidelity.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between professional tools requiring expertise and AI solutions lacking adjustability and generalization.

Method: Two-stage training: Chain-of-Thought fine-tuning and GRPO-R for decision-making, integrated with Lightroom via a protocol.

Result: Superior generalization, fine-grained control, and 60% better pixel-level metrics than GPT-4o on MMArt-Bench.

Conclusion: JarvisArt offers a user-friendly, intelligent solution for photo retouching, setting a new standard in the field.

Abstract: Photo retouching has become integral to contemporary visual storytelling,
enabling users to capture aesthetics and express creativity. While professional
tools such as Adobe Lightroom offer powerful capabilities, they demand
substantial expertise and manual effort. In contrast, existing AI-based
solutions provide automation but often suffer from limited adjustability and
poor generalization, failing to meet diverse and personalized editing needs. To
bridge this gap, we introduce JarvisArt, a multi-modal large language model
(MLLM)-driven agent that understands user intent, mimics the reasoning process
of professional artists, and intelligently coordinates over 200 retouching
tools within Lightroom. JarvisArt undergoes a two-stage training process: an
initial Chain-of-Thought supervised fine-tuning to establish basic reasoning
and tool-use skills, followed by Group Relative Policy Optimization for
Retouching (GRPO-R) to further enhance its decision-making and tool
proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate
seamless integration with Lightroom. To evaluate performance, we develop
MMArt-Bench, a novel benchmark constructed from real-world user edits.
JarvisArt demonstrates user-friendly interaction, superior generalization, and
fine-grained control over both global and local adjustments, paving a new
avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a
60% improvement in average pixel-level metrics on MMArt-Bench for content
fidelity, while maintaining comparable instruction-following capabilities.
Project Page: https://jarvisart.vercel.app/.

</details>


### [191] [CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning](https://arxiv.org/pdf/2506.17629)
*Kailing Li, Qi'ao Xu, Tianwen Qian, Yuqian Fu, Yang Jiao, Xiaoling Wang*

Main category: cs.CV

TL;DR: CLiViS is a training-free framework combining LLMs for reasoning and VLMs for perception to address challenges in Embodied Visual Reasoning (EVR).


<details>
  <summary>Details</summary>
Motivation: EVR faces challenges due to complex instructions and spatiotemporal dynamics in egocentric videos, with prior methods lacking in detail or reasoning.

Method: CLiViS uses LLMs for task planning and VLMs for visual perception, iteratively updating a dynamic Cognitive Map.

Result: CLiViS shows effectiveness in handling long-term visual dependencies across benchmarks.

Conclusion: CLiViS bridges perception and reasoning, offering a robust solution for EVR.

Abstract: Embodied Visual Reasoning (EVR) seeks to follow complex, free-form
instructions based on egocentric video, enabling semantic understanding and
spatiotemporal reasoning in dynamic environments. Despite its promising
potential, EVR encounters significant challenges stemming from the diversity of
complex instructions and the intricate spatiotemporal dynamics in long-term
egocentric videos. Prior solutions either employ Large Language Models (LLMs)
over static video captions, which often omit critical visual details, or rely
on end-to-end Vision-Language Models (VLMs) that struggle with stepwise
compositional reasoning. Consider the complementary strengths of LLMs in
reasoning and VLMs in perception, we propose CLiViS. It is a novel
training-free framework that leverages LLMs for high-level task planning and
orchestrates VLM-driven open-world visual perception to iteratively update the
scene context. Building on this synergy, the core of CLiViS is a dynamic
Cognitive Map that evolves throughout the reasoning process. This map
constructs a structured representation of the embodied scene, bridging
low-level perception and high-level reasoning. Extensive experiments across
multiple benchmarks demonstrate the effectiveness and generality of CLiViS,
especially in handling long-term visual dependencies. Code is available at
https://github.com/Teacher-Tom/CLiViS.

</details>


### [192] [Optimization-Free Patch Attack on Stereo Depth Estimation](https://arxiv.org/pdf/2506.17632)
*Hangcheng Liu, Xu Kuang, Xingshuo Han, Xingwan Wu, Haoran Ou, Shangwei Guo, Xingyi Huang, Tao Xiang, Tianwei Zhang*

Main category: cs.CV

TL;DR: The paper introduces PatchHunter, an optimization-free adversarial patch attack for Stereo Depth Estimation (SDE), addressing limitations of optimization-based methods and achieving superior transferability and real-world applicability.


<details>
  <summary>Details</summary>
Motivation: Recent SDE models are vulnerable to adversarial attacks, but existing methods are unrealistic or limited in real-world settings. The paper aims to design physically realizable, scene-adaptive, and transferable attacks.

Method: The paper proposes a unified attack framework for SDE stages and introduces PatchHunter, a reinforcement learning-driven search for adversarial patches.

Result: PatchHunter outperforms optimization-based methods in effectiveness and transferability, maintaining high attack success in real-world conditions like low light.

Conclusion: PatchHunter demonstrates the potential of pattern-based attacks for disrupting SDE models, offering a robust and practical solution for adversarial attacks.

Abstract: Stereo Depth Estimation (SDE) is essential for scene understanding in
vision-based systems like autonomous driving. However, recent studies show that
SDE models are vulnerable to adversarial attacks, which are often limited to
unrealistic settings, e.g., digital perturbations on separate stereo views in
static scenes, restricting their real-world applicability. This raises a
critical question: how can we design physically realizable, scene-adaptive, and
transferable attacks against SDE under realistic constraints?
  To answer this, we make two key contributions. First, we propose a unified
attack framework that extends optimization-based techniques to four core stages
of stereo matching: feature extraction, cost-volume construction, cost
aggregation, and disparity regression. A comprehensive stage-wise evaluation
across 9 mainstream SDE models, under constraints like photometric consistency,
reveals that optimization-based patches suffer from poor transferability.
Interestingly, partially transferable patches suggest that patterns, rather
than pixel-level perturbations, may be key to generalizable attacks. Motivated
by this, we present PatchHunter, the first optimization-free adversarial patch
attack against SDE. PatchHunter formulates patch generation as a reinforcement
learning-driven search over a structured space of visual patterns crafted to
disrupt SDE assumptions.
  We validate PatchHunter across three levels: the KITTI dataset, the CARLA
simulator, and real-world vehicle deployment. PatchHunter not only surpasses
optimization-based methods in effectiveness but also achieves significantly
better black-box transferability. Even under challenging physical conditions
like low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4),
whereas optimization-based methods fail.

</details>


### [193] [Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection](https://arxiv.org/pdf/2506.17633)
*Xiang Fang, Arvind Easwaran, Blaise Genest*

Main category: cs.CV

TL;DR: The paper introduces AMCN, a novel network for few-shot OOD detection, leveraging CLIP and adaptive prompts to improve performance with limited labeled ID samples.


<details>
  <summary>Details</summary>
Motivation: Traditional OOD detection requires many IID samples, limiting real-world use. Few-shot OOD detection is more challenging due to scarce labeled ID data and ignored class diversity.

Method: Proposes AMCN, which uses CLIP to connect text and images, generates adaptive prompts, and introduces a class-wise threshold for ID-OOD separation.

Result: AMCN outperforms state-of-the-art methods in few-shot OOD detection.

Conclusion: AMCN effectively addresses the challenges of few-shot OOD detection by leveraging adaptive prompts and class-wise boundaries.

Abstract: Out-of-distribution (OOD) detection attempts to distinguish outlier samples
to prevent models trained on the in-distribution (ID) dataset from producing
unavailable outputs. Most OOD detection methods require many IID samples for
training, which seriously limits their real-world applications. To this end, we
target a challenging setting: few-shot OOD detection, where {Only a few {\em
labeled ID} samples are available.} Therefore, few-shot OOD detection is much
more challenging than the traditional OOD detection setting. Previous few-shot
OOD detection works ignore the distinct diversity between different classes. In
this paper, we propose a novel network: Adaptive Multi-prompt Contrastive
Network (AMCN), which adapts the ID-OOD separation boundary by learning inter-
and intra-class distribution. To compensate for the absence of OOD and scarcity
of ID {\em image samples}, we leverage CLIP, connecting text with images,
engineering learnable ID and OOD {\em textual prompts}. Specifically, we first
generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and
label-adaptive OOD prompts). Then, we generate an adaptive class boundary for
each class by introducing a class-wise threshold. Finally, we propose a
prompt-guided ID-OOD separation module to control the margin between ID and OOD
prompts. Experimental results show that AMCN outperforms other state-of-the-art
works.

</details>


### [194] [Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning](https://arxiv.org/pdf/2506.17645)
*Shih-Wen Liu, Hsuan-Yu Fan, Wei-Ta Chu, Fu-En Yang, Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: PathGenIC, an in-context learning framework, automates medical report generation from histopathology images by integrating context from training data and multimodal learning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Automating medical report generation from histopathology images is challenging due to the need for effective visual representations and domain-specific knowledge.

Method: Proposes PathGenIC, which dynamically retrieves similar WSI-report pairs and uses adaptive feedback for contextual relevance and generation quality.

Result: Achieves state-of-the-art performance on the HistGen benchmark, improving BLEU, METEOR, and ROUGE-L metrics, and shows robustness across report lengths and disease categories.

Conclusion: PathGenIC offers a solution for AI-driven histopathology reporting, laying a foundation for future multimodal clinical applications.

Abstract: Automating medical report generation from histopathology images is a critical
challenge requiring effective visual representations and domain-specific
knowledge. Inspired by the common practices of human experts, we propose an
in-context learning framework called PathGenIC that integrates context derived
from the training set with a multimodal in-context learning (ICL) mechanism.
Our method dynamically retrieves semantically similar whole slide image
(WSI)-report pairs and incorporates adaptive feedback to enhance contextual
relevance and generation quality. Evaluated on the HistGen benchmark, the
framework achieves state-of-the-art results, with significant improvements
across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across
diverse report lengths and disease categories. By maximizing training data
utility and bridging vision and language with ICL, our work offers a solution
for AI-driven histopathology reporting, setting a strong foundation for future
advancements in multimodal clinical applications.

</details>


### [195] [MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation](https://arxiv.org/pdf/2506.17664)
*Shuaiye Lu, Linjiang Zhou, Xiaochuan Shi*

Main category: cs.CV

TL;DR: MDSAM reduces hallucinations in LVLMs by dynamically refining attention to image tokens without training.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LVLMs arise from sensitivity to image tokens during decoding.

Method: Proposes MDSAM, a training-free approach that memorizes and refines attention patterns to focus on relevant image tokens.

Result: MDSAM consistently reduces hallucinations and improves reliability in tasks like image captioning and VQA.

Conclusion: MDSAM is adaptable, effective, and compatible with various LVLM architectures, mitigating hallucinations without extra training.

Abstract: Hallucinations in large vision-language models (LVLMs) often stem from the
model's sensitivity to image tokens during decoding, as evidenced by attention
peaks observed when generating both real and hallucinated entities. To address
this, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel
training-free approach that dynamically captures and refines the attention
allocated to image tokens at each layer. MDSAM memorizes attention patterns and
activates updates through alignment during decoding, enhancing focus on
relevant image tokens while effectively reducing hallucinations. We evaluate
MDSAM on multiple benchmarks for tasks such as image captioning and visual
question answering, demonstrating its ability to consistently reduce
hallucinations and improve reliability. Compatible with various LVLM
architectures, MDSAM highlights its adaptability and effectiveness in
mitigating hallucinations without requiring additional training or external
tools.

</details>


### [196] [CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection](https://arxiv.org/pdf/2506.17679)
*Wei Haolin*

Main category: cs.CV

TL;DR: The paper introduces CSDN, a Transformer-based detection header replacing traditional attention mechanisms with a gating mechanism for better feature utilization and global context modeling in CNN-based detectors.


<details>
  <summary>Details</summary>
Motivation: CNNs' limited receptive fields hinder global context capture, and feature utilization is as critical as extraction. The paper questions DETR's self-attention redundancy.

Method: CSDN replaces stacked self/cross-attention with a gating mechanism for adaptive feature and scale selection, enhancing CNN backbone efficiency.

Result: CSDN improves detection accuracy with minimal fine-tuning, avoiding extensive re-training, and adapts well to objects of varying sizes.

Conclusion: CSDN offers a scalable, efficient alternative to traditional attention mechanisms, enhancing CNN-based detectors' performance with minimal overhead.

Abstract: Convolutional neural networks (CNNs) have long been the cornerstone of target
detection, but they are often limited by limited receptive fields, which
hinders their ability to capture global contextual information. This paper
believes that the effective utilization of extracted features is as important
as the feature extraction process itself. We critically re-evaluated the
DETR-inspired header network architecture, questioning the indispensable nature
of its self-attention mechanism, and discovering significant information
redundancies. To solve these problems, we introduced the Context-Gated
Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header
inspired by natural language processing architecture and human visual
perception. CSDN aims to efficiently utilize the characteristics of the CNN
backbone network by replacing the traditional stacked self-attention and
cross-attention layers with a novel gating mechanism. This mechanism enables
each region of interest (ROI) to adaptively select and combine feature
dimensions and scale information from multiple attention patterns. CSDN
provides more powerful global context modeling capabilities and can better
adapt to objects of different sizes and structures. Our proposed detection head
can directly replace the native heads of various CNN-based detectors, and only
a few rounds of fine-tuning on the pre-training weights can significantly
improve the detection accuracy, thus avoiding the need to achieve small
improvements. Various layer modules undergo extensive re-training.

</details>


### [197] [Domain Generalization using Action Sequences for Egocentric Action Recognition](https://arxiv.org/pdf/2506.17685)
*Amirshayan Nasirimajd, Chiara Plizzari, Simone Alberto Peirone, Marco Ciccone, Giuseppe Averta, Barbara Caputo*

Main category: cs.CV

TL;DR: Proposes SeqDG, a domain generalization method for Egocentric Action Recognition, improving cross-domain performance by leveraging action sequences and visual-text reconstruction.


<details>
  <summary>Details</summary>
Motivation: Addresses performance drop in Egocentric Action Recognition models in unseen environments due to variability in illumination, viewpoint, and environment.

Method: Introduces SeqDG with SeqRec (visual-text sequence reconstruction) and SeqMix (training on mixed action sequences from different domains).

Result: Achieves +2.4% improvement on EPIC-KITCHENS-100 and +0.6% Top-1 accuracy on EGTEA over SOTA.

Conclusion: SeqDG enhances generalization in unseen environments by leveraging action sequences and contextual cues.

Abstract: Recognizing human activities from visual inputs, particularly through a
first-person viewpoint, is essential for enabling robots to replicate human
behavior. Egocentric vision, characterized by cameras worn by observers,
captures diverse changes in illumination, viewpoint, and environment. This
variability leads to a notable drop in the performance of Egocentric Action
Recognition models when tested in environments not seen during training. In
this paper, we tackle these challenges by proposing a domain generalization
approach for Egocentric Action Recognition. Our insight is that action
sequences often reflect consistent user intent across visual domains. By
leveraging action sequences, we aim to enhance the model's generalization
ability across unseen environments. Our proposed method, named SeqDG,
introduces a visual-text sequence reconstruction objective (SeqRec) that uses
contextual cues from both text and visual inputs to reconstruct the central
action of the sequence. Additionally, we enhance the model's robustness by
training it on mixed sequences of actions from different domains (SeqMix). We
validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on
EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement
in cross-domain action recognition in unseen environments, and on EGTEA the
model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action
recognition.

</details>


### [198] [SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification](https://arxiv.org/pdf/2506.17694)
*Gnana Praveen Rajasekhar, Jahangir Alam*

Main category: cs.CV

TL;DR: A self-supervised learning framework for speaker verification uses contrastive learning and masked data modeling to reduce reliance on labeled data and computational costs.


<details>
  <summary>Details</summary>
Motivation: Traditional methods require large labeled datasets and separate architectures, making them expensive and unscalable.

Method: Proposes a unified framework with a shared vision transformer backbone for audio and visual inputs, using asymmetric masking and masked data modeling.

Result: Achieves competitive performance without labeled data and reduces computational costs.

Conclusion: The framework is efficient, robust to missing modalities, and scalable for speaker verification.

Abstract: Conventional audio-visual methods for speaker verification rely on large
amounts of labeled data and separate modality-specific architectures, which is
computationally expensive, limiting their scalability. To address these
problems, we propose a self-supervised learning framework based on contrastive
learning with asymmetric masking and masked data modeling to obtain robust
audiovisual feature representations. In particular, we employ a unified
framework for self-supervised audiovisual speaker verification using a single
shared backbone for audio and visual inputs, leveraging the versatility of
vision transformers. The proposed unified framework can handle audio, visual,
or audiovisual inputs using a single shared vision transformer backbone during
training and testing while being computationally efficient and robust to
missing modalities. Extensive experiments demonstrate that our method achieves
competitive performance without labeled data while reducing computational costs
compared to traditional approaches.

</details>


### [199] [DreamJourney: Perpetual View Generation with Video Diffusion Models](https://arxiv.org/pdf/2506.17705)
*Bo Pan, Yang Chen, Yingwei Pan, Ting Yao, Wei Chen, Tao Mei*

Main category: cs.CV

TL;DR: DreamJourney introduces a two-stage framework for perpetual dynamic scene view generation, combining 3D point cloud rendering and video diffusion models to handle camera movements and object dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack 3D awareness and fail to capture dynamic object movements, leading to distorted artifacts and static scene limitations.

Method: Stage I: Renders partial images from 3D point clouds and uses video diffusion for coherence. Stage II: Animates object movements via text prompts and video diffusion.

Result: Outperforms state-of-the-art methods in generating dynamic scenes with camera and object movements.

Conclusion: DreamJourney advances perpetual view generation by integrating 3D awareness and dynamic object modeling.

Abstract: Perpetual view generation aims to synthesize a long-term video corresponding
to an arbitrary camera trajectory solely from a single input image. Recent
methods commonly utilize a pre-trained text-to-image diffusion model to
synthesize new content of previously unseen regions along camera movement.
However, the underlying 2D diffusion model lacks 3D awareness and results in
distorted artifacts. Moreover, they are limited to generating views of static
3D scenes, neglecting to capture object movements within the dynamic 4D world.
To alleviate these issues, we present DreamJourney, a two-stage framework that
leverages the world simulation capacity of video diffusion models to trigger a
new perpetual scene view generation task with both camera movements and object
dynamics. Specifically, in stage I, DreamJourney first lifts the input image to
3D point cloud and renders a sequence of partial images from a specific camera
trajectory. A video diffusion model is then utilized as generative prior to
complete the missing regions and enhance visual coherence across the sequence,
producing a cross-view consistent video adheres to the 3D scene and camera
trajectory. Meanwhile, we introduce two simple yet effective strategies (early
stopping and view padding) to further stabilize the generation process and
improve visual quality. Next, in stage II, DreamJourney leverages a multimodal
large language model to produce a text prompt describing object movements in
current view, and uses video diffusion model to animate current view with
object movements. Stage I and II are repeated recurrently, enabling perpetual
dynamic scene view generation. Extensive experiments demonstrate the
superiority of our DreamJourney over state-of-the-art methods both
quantitatively and qualitatively. Our project page:
https://dream-journey.vercel.app.

</details>


### [200] [PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation](https://arxiv.org/pdf/2506.17712)
*Xinyu Xiong, Wuteng Cao, Zihuang Wu, Lei Zhang, Chong Gao, Guanbin Li, Qiyuan Qin*

Main category: cs.CV

TL;DR: Proposes PDC-Net for PRI segmentation using MDA and MGC modules to handle complex organ shapes and confusing context, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate PRI segmentation from MRI is vital for prognosis and treatment but is challenging due to organ complexity and context confusion.

Method: Introduces PDC-Net with MDA for shape fitting, MGC for context distinction, and AFD for dynamic feature fusion.

Result: PDC-Net outperforms existing methods on a large-scale PRI dataset.

Conclusion: PDC-Net effectively addresses PRI segmentation challenges, offering improved accuracy for clinical applications.

Abstract: Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic
Resonance Images (MRI) is crucial for more precise prognosis assessment and the
development of personalized treatment plans. However, automated segmentation
remains challenging due to factors such as complex organ morphologies and
confusing context. To address these challenges, we propose a novel Pattern
Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to
use different network modules to "divide" various local and global patterns
and, through flexible feature selection, to "conquer" the Regions of Interest
(ROI) during the decoding phase. Specifically, considering that our ROI often
manifests as strip-like or circular-like structures in MR slices, we introduce
a Multi-Direction Aggregation (MDA) module. This module enhances the model's
ability to fit the shape of the organ by applying strip convolutions in four
distinct directions. Additionally, to mitigate the challenge of confusing
context, we propose a Memory-Guided Context (MGC) module. This module
explicitly maintains a memory parameter to track cross-image patterns at the
dataset level, thereby enhancing the distinction between global patterns
associated with the positive and negative classes. Finally, we design an
Adaptive Fusion Decoder (AFD) that dynamically selects features from different
patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating
the final segmentation results. We evaluate our method on the first large-scale
pelvic radiation injury dataset, and the results demonstrate the superiority of
our PDC-Net over existing approaches.

</details>


### [201] [YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception](https://arxiv.org/pdf/2506.17733)
*Mengqi Lei, Siqi Li, Yihong Wu, Han Hu, You Zhou, Xinhu Zheng, Guiguang Ding, Shaoyi Du, Zongze Wu, Yue Gao*

Main category: cs.CV

TL;DR: YOLOv13 introduces HyperACE and FullPAD for global high-order correlations, improving accuracy and efficiency in object detection.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of local information aggregation and pairwise correlation in YOLO models for better performance in complex scenarios.

Method: Proposes HyperACE for global high-order correlations, FullPAD for feature synergy, and depthwise separable convolutions for efficiency.

Result: Achieves SOTA performance on MS COCO, with YOLOv13-N improving mAP by 3.0% over YOLO11-N and 1.5% over YOLOv12-N.

Conclusion: YOLOv13 offers a lightweight, accurate solution with fewer parameters and FLOPs, advancing real-time object detection.

Abstract: The YOLO series models reign supreme in real-time object detection due to
their superior accuracy and computational efficiency. However, both the
convolutional architectures of YOLO11 and earlier versions and the area-based
self-attention mechanism introduced in YOLOv12 are limited to local information
aggregation and pairwise correlation modeling, lacking the capability to
capture global multi-to-multi high-order correlations, which limits detection
performance in complex scenarios. In this paper, we propose YOLOv13, an
accurate and lightweight object detector. To address the above-mentioned
challenges, we propose a Hypergraph-based Adaptive Correlation Enhancement
(HyperACE) mechanism that adaptively exploits latent high-order correlations
and overcomes the limitation of previous methods that are restricted to
pairwise correlation modeling based on hypergraph computation, achieving
efficient global cross-location and cross-scale feature fusion and enhancement.
Subsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD)
paradigm based on HyperACE, which effectively achieves fine-grained information
flow and representation synergy within the entire network by distributing
correlation-enhanced features to the full pipeline. Finally, we propose to
leverage depthwise separable convolutions to replace vanilla large-kernel
convolutions, and design a series of blocks that significantly reduce
parameters and computational complexity without sacrificing performance. We
conduct extensive experiments on the widely used MS COCO benchmark, and the
experimental results demonstrate that our method achieves state-of-the-art
performance with fewer parameters and FLOPs. Specifically, our YOLOv13-N
improves mAP by 3.0\% over YOLO11-N and by 1.5\% over YOLOv12-N. The code and
models of our YOLOv13 model are available at:
https://github.com/iMoonLab/yolov13.

</details>


### [202] [PhysID: Physics-based Interactive Dynamics from a Single-view Image](https://arxiv.org/pdf/2506.17746)
*Sourabh Vasant Gothe, Ayon Chattopadhyay, Gunturi Venkata Sai Phani Kiran, Pratik, Vibhav Agarwal, Jayesh Rajkumar Vachhani, Sourav Ghosh, Parameswaranath VM, Barath Raj KR*

Main category: cs.CV

TL;DR: PhysID simplifies creating physics-based interactive dynamics from single-view images using generative models, reducing manual effort and enabling real-time, personalized interactions on mobile devices.


<details>
  <summary>Details</summary>
Motivation: To enhance mobile user experiences by transforming static images into interactive, physics-based dynamics without requiring extensive expertise or multi-view inputs.

Method: Leverages large generative models for 3D mesh generation and physical property prediction, integrating an on-device physics engine for real-time rendering.

Result: Demonstrates cohesive functioning of modules, enabling real-time, non-deterministic interactions with efficient memory use.

Conclusion: PhysID advances mobile-based interactive dynamics, offering scalable, user-personalized experiences with minimal manual intervention.

Abstract: Transforming static images into interactive experiences remains a challenging
task in computer vision. Tackling this challenge holds the potential to elevate
mobile user experiences, notably through interactive and AR/VR applications.
Current approaches aim to achieve this either using pre-recorded video
responses or requiring multi-view images as input. In this paper, we present
PhysID, that streamlines the creation of physics-based interactive dynamics
from a single-view image by leveraging large generative models for 3D mesh
generation and physical property prediction. This significantly reduces the
expertise required for engineering-intensive tasks like 3D modeling and
intrinsic property calibration, enabling the process to be scaled with minimal
manual intervention. We integrate an on-device physics-based engine for
physically plausible real-time rendering with user interactions. PhysID
represents a leap forward in mobile-based interactive dynamics, offering
real-time, non-deterministic interactions and user-personalization with
efficient on-device memory consumption. Experiments evaluate the zero-shot
capabilities of various Multimodal Large Language Models (MLLMs) on diverse
tasks and the performance of 3D reconstruction models. These results
demonstrate the cohesive functioning of all modules within the end-to-end
framework, contributing to its effectiveness.

</details>


### [203] [LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging](https://arxiv.org/pdf/2506.17759)
*Fadi Abdeladhim Zidi, Djamel Eddine Boukhari, Abdellah Zakaria Sellam, Abdelkrim Ouafi, Cosimo Distante, Salah Eddine Bekhouche, Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: LoLA-SpecViT is a lightweight spectral vision transformer for hyperspectral image classification, combining 3D convolution and local self-attention with low-rank adaptation for efficiency and robustness under limited labels.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral image classification faces challenges like high dimensionality, inter-band redundancy, and scarce annotated samples. Existing transformer models lack scalability and adaptability in label-scarce conditions.

Method: Proposes LoLA-SpecViT, integrating 3D convolutional spectral front-end, local window-based self-attention, and low-rank adaptation (LoRA) for parameter efficiency. Uses a cyclical learning rate scheduler for improved convergence.

Result: Achieves up to 99.91% accuracy on benchmark datasets (WHU-Hi LongKou, WHU-Hi HongHu, Salinas) with 80% fewer trainable parameters, outperforming state-of-the-art baselines.

Conclusion: LoLA-SpecViT offers a scalable, generalizable solution for hyperspectral imagery applications, enhancing efficiency and robustness in low-label scenarios.

Abstract: Hyperspectral image classification remains a challenging task due to the high
dimensionality of spectral data, significant inter-band redundancy, and the
limited availability of annotated samples. While recent transformer-based
models have improved the global modeling of spectral-spatial dependencies,
their scalability and adaptability under label-scarce conditions remain
limited. In this work, we propose \textbf{LoLA-SpecViT}(Low-rank adaptation
Local Attention Spectral Vision Transformer), a lightweight spectral vision
transformer that addresses these limitations through a parameter-efficient
architecture tailored to the unique characteristics of hyperspectral imagery.
Our model combines a 3D convolutional spectral front-end with local
window-based self-attention, enhancing both spectral feature extraction and
spatial consistency while reducing computational complexity. To further improve
adaptability, we integrate low-rank adaptation (LoRA) into attention and
projection layers, enabling fine-tuning with over 80\% fewer trainable
parameters. A novel cyclical learning rate scheduler modulates LoRA adaptation
strength during training, improving convergence and generalisation. Extensive
experiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and
Salinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art
baselines, achieving up to 99.91\% accuracy with substantially fewer parameters
and enhanced robustness under low-label regimes. The proposed framework
provides a scalable and generalizable solution for real-world HSI applications
in agriculture, environmental monitoring, and remote sensing analytics. Our
code is available in the following
\href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.

</details>


### [204] [Incorporating Rather Than Eliminating: Achieving Fairness for Skin Disease Diagnosis Through Group-Specific Expert](https://arxiv.org/pdf/2506.17787)
*Gelei Xu, Yuying Duan, Zheyuan Liu, Xueyang Li, Meng Jiang, Michael Lemmon, Wei Jin, Yiyu Shi*

Main category: cs.CV

TL;DR: FairMoE is a framework using dynamic routing of data to group-specific experts, improving accuracy while maintaining fairness in skin disease diagnostics.


<details>
  <summary>Details</summary>
Motivation: AI-based skin disease diagnostics often exhibit biases across demographic groups, leading to inequitable outcomes and reduced trust. Existing bias mitigation methods degrade performance by losing clinically relevant cues.

Method: FairMoE employs layer-wise mixture-of-experts modules to dynamically route data to group-specific learners, handling cases near group boundaries effectively.

Result: FairMoE achieves substantial accuracy improvements while preserving fairness metrics, unlike previous methods that reduce performance.

Conclusion: FairMoE offers an effective alternative to traditional bias mitigation by leveraging sensitive attributes dynamically, balancing accuracy and fairness.

Abstract: AI-based systems have achieved high accuracy in skin disease diagnostics but
often exhibit biases across demographic groups, leading to inequitable
healthcare outcomes and diminished patient trust. Most existing bias mitigation
methods attempt to eliminate the correlation between sensitive attributes and
diagnostic prediction, but those methods often degrade performance due to the
lost of clinically relevant diagnostic cues. In this work, we propose an
alternative approach that incorporates sensitive attributes to achieve
fairness. We introduce FairMoE, a framework that employs layer-wise
mixture-of-experts modules to serve as group-specific learners. Unlike
traditional methods that rigidly assign data based on group labels, FairMoE
dynamically routes data to the most suitable expert, making it particularly
effective for handling cases near group boundaries. Experimental results show
that, unlike previous fairness approaches that reduce performance, FairMoE
achieves substantial accuracy improvements while preserving comparable fairness
metrics.

</details>


### [205] [Time-Contrastive Pretraining for In-Context Image and Video Segmentation](https://arxiv.org/pdf/2506.17837)
*Assefa Wahd, Jacob Jaremko, Abhilash Hareendranathan*

Main category: cs.CV

TL;DR: Temporal introduces a time-contrastive self-supervised objective for visual in-context learning (ICL), reframing it as a video object segmentation (VOS) task to overcome grid-based limitations.


<details>
  <summary>Details</summary>
Motivation: Grid-based ICL lacks flexibility for vision tasks, restricting context image resolution and quantity.

Method: Pretrains a prompt retriever using self-supervised learning on videos, treating adjacent frames as positives and distant ones as negatives. ICL is formulated as a VOS task.

Result: Achieves 90.95% Dice for image segmentation (10.64% improvement) and 92.45% Dice for video segmentation (14.88% improvement) on MICCAI FLARE 2022.

Conclusion: Temporal effectively addresses grid-based ICL limitations, offering flexible, high-resolution context handling and significant performance gains.

Abstract: In-context learning (ICL) enables generalization to new tasks with minimal
labeled data. However, mainstream ICL approaches rely on a gridding strategy,
which lacks the flexibility required for vision applications. We introduce
Temporal, a time-contrastive self-supervised objective that pretrains a prompt
retriever for visual ICL, and formulate ICL as a video object segmentation
(VOS) task. Temporal addresses key limitations of grid-based methods that
restrict the number and resolution of context images. By reframing ICL as a VOS
problem, our approach supports a variable number of context images while
preserving their full resolution. To address the challenge of selecting optimal
context sets for queries, we pretrain a prompt retriever on videos via
self-supervised learning, where adjacent frames serve as positives and distant
frames as negatives. For image segmentation, the prompt retriever selects
relevant sequences that, when combined with the query, form coherent videos for
VOS processing. For video segmentation, it identifies keyframes, predicts their
masks using our ICL pipeline, and propagates them throughout the sequence. When
evaluated on MICCAI FLARE 2022, our method achieves substantial improvements
over baselines: 90.95% Dice score for image segmentation (10.64% improvement)
and 92.45% Dice for video segmentation (14.88% improvement).

</details>


### [206] [Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose](https://arxiv.org/pdf/2506.17858)
*Yingcheng Liu, Peiqi Wang, Sebastian Diaz, Esra Abaci Turk, Benjamin Billot, Patricia Ellen Grant, Polina Golland*

Main category: cs.CV

TL;DR: A 3D articulated statistical fetal body model (based on SMPL) is introduced to improve fetal motion and shape analysis in MRI, addressing limitations of keypoints and segmentations.


<details>
  <summary>Details</summary>
Motivation: Existing methods (keypoints or segmentations) for fetal MRI analysis either oversimplify body structure or complicate temporal analysis due to large fetal movements.

Method: The model iteratively estimates body pose in image space and shape in canonical pose space, trained on 19,816 MRI volumes from 53 subjects.

Result: Achieves 3.2 mm surface alignment error for 3 mm MRI voxel size, enabling automated anthropometric measurements and intuitive visualization.

Conclusion: The first 3D articulated statistical fetal body model enhances prenatal diagnostics by robustly capturing shape and motion.

Abstract: Analyzing fetal body motion and shape is paramount in prenatal diagnostics
and monitoring. Existing methods for fetal MRI analysis mainly rely on
anatomical keypoints or volumetric body segmentations. Keypoints simplify body
structure to facilitate motion analysis, but may ignore important details of
full-body shape. Body segmentations capture complete shape information but
complicate temporal analysis due to large non-local fetal movements. To address
these limitations, we construct a 3D articulated statistical fetal body model
based on the Skinned Multi-Person Linear Model (SMPL). Our algorithm
iteratively estimates body pose in the image space and body shape in the
canonical pose space. This approach improves robustness to MRI motion artifacts
and intensity distortions, and reduces the impact of incomplete surface
observations due to challenging fetal poses. We train our model on
segmentations and keypoints derived from $19,816$ MRI volumes across $53$
subjects. Our model captures body shape and motion across time series and
provides intuitive visualization. Furthermore, it enables automated
anthropometric measurements traditionally difficult to obtain from
segmentations and keypoints. When tested on unseen fetal body shapes, our
method yields a surface alignment error of $3.2$ mm for $3$ mm MRI voxel size.
To our knowledge, this represents the first 3D articulated statistical fetal
body model, paving the way for enhanced fetal motion and shape analysis in
prenatal diagnostics. The code is available at
https://github.com/MedicalVisionGroup/fetal-smpl .

</details>


### [207] [Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation](https://arxiv.org/pdf/2506.17869)
*Xiaodong Guo, Zi'ang Lin, Luwen Hu, Zhihong Deng, Tong Liu, Wujie Zhou*

Main category: cs.CV

TL;DR: CM-SSM is an efficient RGB-thermal semantic segmentation architecture using cross-modal state space modeling, achieving linear computational complexity and state-of-the-art performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Improving semantic segmentation in wild environments for field robots by integrating RGB and thermal data while addressing computational overhead challenges.

Method: Introduces CM-SS2D for cross-modal state space modeling and CM-SSA for integrating global associations with local spatial features, avoiding Transformer-based complexity.

Result: Achieves state-of-the-art performance on the CART dataset with lower computational cost and demonstrates generalizability on the PST900 dataset.

Conclusion: CM-SSM offers an efficient, scalable solution for RGB-thermal semantic segmentation, outperforming existing methods with reduced resource demands.

Abstract: The integration of RGB and thermal data can significantly improve semantic
segmentation performance in wild environments for field robots. Nevertheless,
multi-source data processing (e.g. Transformer-based approaches) imposes
significant computational overhead, presenting challenges for
resource-constrained systems. To resolve this critical limitation, we
introduced CM-SSM, an efficient RGB-thermal semantic segmentation architecture
leveraging a cross-modal state space modeling (SSM) approach. Our framework
comprises two key components. First, we introduced a cross-modal
2D-selective-scan (CM-SS2D) module to establish SSM between RGB and thermal
modalities, which constructs cross-modal visual sequences and derives hidden
state representations of one modality from the other. Second, we developed a
cross-modal state space association (CM-SSA) module that effectively integrates
global associations from CM-SS2D with local spatial features extracted through
convolutional operations. In contrast with Transformer-based approaches, CM-SSM
achieves linear computational complexity with respect to image resolution.
Experimental results show that CM-SSM achieves state-of-the-art performance on
the CART dataset with fewer parameters and lower computational cost. Further
experiments on the PST900 dataset demonstrate its generalizability. Codes are
available at https://github.com/xiaodonguo/CMSSM.

</details>


### [208] [SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model](https://arxiv.org/pdf/2506.17873)
*Guankun Wang, Wenjin Mo, Junyi Wang, Long Bai, Kun Yuan, Ming Hu, Jinlin Wu, Junjun He, Yiming Huang, Nicolas Padoy, Zhen Lei, Hongbin Liu, Nassir Navab, Hongliang Ren*

Main category: cs.CV

TL;DR: SurgVidLM is a novel Video Large Language Model (Vid-LLM) designed for both full and fine-grained surgical video understanding, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: There's a lack of specialized Vid-LLMs for fine-grained surgical video analysis, which is crucial for detailed procedural understanding.

Method: Proposes SurgVidLM, trained on the SVU-31K dataset (31K video-instruction pairs), with StageFocus (two-stage framework) and Multi-frequency Fusion Attention for visual token integration.

Result: SurgVidLM outperforms state-of-the-art Vid-LLMs in both full and fine-grained surgical video understanding tasks.

Conclusion: SurgVidLM effectively bridges the gap in surgical video comprehension, demonstrating superior performance in capturing procedural details.

Abstract: Recent advances in Multimodal Large Language Models have demonstrated great
potential in the medical domain, facilitating users to understand surgical
scenes and procedures. Beyond image-based methods, the exploration of Video
Large Language Models (Vid-LLMs) has emerged as a promising avenue for
capturing the complex sequences of information involved in surgery. However,
there is still a lack of Vid-LLMs specialized for fine-grained surgical video
understanding tasks, which is crucial for analyzing specific processes or
details within a surgical procedure. To bridge this gap, we propose SurgVidLM,
the first video language model designed to address both full and fine-grained
surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K
dataset which consists of over 31K video-instruction pairs, enabling both
holistic understanding and detailed analysis of surgical procedures.
Furthermore, we introduce the StageFocus mechanism which is a two-stage
framework performing the multi-grained, progressive understanding of surgical
videos. We also develop the Multi-frequency Fusion Attention to effectively
integrate low and high-frequency visual tokens, ensuring the retention of
critical information. Experimental results demonstrate that SurgVidLM
significantly outperforms state-of-the-art Vid-LLMs in both full and
fine-grained video understanding tasks, showcasing its superior capability in
capturing complex procedural contexts.

</details>


### [209] [StainPIDR: A Pathological Image Decouplingand Reconstruction Method for StainNormalization Based on Color VectorQuantization and Structure Restaining](https://arxiv.org/pdf/2506.17879)
*Zheng Chen*

Main category: cs.CV

TL;DR: StainPIDR is a stain normalization method for pathological images, decoupling structure and color features, restaining with target colors, and using a template selection algorithm for optimal results.


<details>
  <summary>Details</summary>
Motivation: Color discrepancies in pathological images due to varying protocols, dyes, and devices degrade computer-aided diagnostic systems.

Method: Decouples images into structure and vector-quantized color features, restains structure with target colors via cross-attention, and selects templates for optimal normalization.

Result: Effective stain normalization demonstrated in experiments, with StainPIDR outperforming in the task.

Conclusion: StainPIDR successfully addresses color discrepancies in pathological images, enhancing diagnostic system reliability.

Abstract: The color appearance of a pathological image is highly related to the imaging
protocols, the proportion of different dyes, and the scanning devices.
Computer-aided diagnostic systems may deteriorate when facing these
color-variant pathological images. In this work, we propose a stain
normalization method called StainPIDR. We try to eliminate this color
discrepancy by decoupling the image into structure features and
vector-quantized color features, restaining the structure features with the
target color features, and decoding the stained structure features to
normalized pathological images. We assume that color features decoupled by
different images with the same color should be exactly the same. Under this
assumption, we train a fixed color vector codebook to which the decoupled color
features will map. In the restaining part, we utilize the cross-attention
mechanism to efficiently stain the structure features. As the target color
(decoupled from a selected template image) will also affect the performance of
stain normalization, we further design a template image selection algorithm to
select a template from a given dataset. In our extensive experiments, we
validate the effectiveness of StainPIDR and the template image selection
algorithm. All the results show that our method can perform well in the stain
normalization task. The code of StainPIDR will be publicly available later.

</details>


### [210] [Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions](https://arxiv.org/pdf/2506.17885)
*Trong-An Bui, Thanh-Thoai Le*

Main category: cs.CV

TL;DR: A Cloud-Attentive Reconstruction Framework combines SAR-optical feature fusion and deep learning to generate cloud-free optical imagery, outperforming existing methods with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Cloud contamination reduces the usability of optical satellite imagery for critical applications like environmental monitoring and disaster response.

Method: The framework uses attention-driven feature fusion to align SAR structural data with optical spectral data and employs adaptive loss weighting for cloud-occluded regions.

Result: Achieves PSNR of 31.01 dB, SSIM of 0.918, and MAE of 0.017, outperforming existing approaches.

Conclusion: The framework effectively produces high-fidelity, spatially and spectrally consistent cloud-free optical images.

Abstract: Cloud contamination significantly impairs the usability of optical satellite
imagery, affecting critical applications such as environmental monitoring,
disaster response, and land-use analysis. This research presents a
Cloud-Attentive Reconstruction Framework that integrates SAR-optical feature
fusion with deep learning-based image reconstruction to generate cloud-free
optical imagery. The proposed framework employs an attention-driven feature
fusion mechanism to align complementary structural information from Synthetic
Aperture Radar (SAR) with spectral characteristics from optical data.
Furthermore, a cloud-aware model update strategy introduces adaptive loss
weighting to prioritize cloud-occluded regions, enhancing reconstruction
accuracy. Experimental results demonstrate that the proposed method outperforms
existing approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of
0.017. These outcomes highlight the framework's effectiveness in producing
high-fidelity, spatially and spectrally consistent cloud-free optical images.

</details>


### [211] [Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation](https://arxiv.org/pdf/2506.17891)
*Jiahao Lu, Jiacheng Deng*

Main category: cs.CV

TL;DR: Relation3D improves 3D instance segmentation by enhancing internal and external feature relationships using adaptive superpoint aggregation, contrastive learning, and relation-aware self-attention.


<details>
  <summary>Details</summary>
Motivation: Transformer-based methods lack effective modeling of internal relationships among scene and query features, limiting performance.

Method: Proposes adaptive superpoint aggregation, contrastive learning-guided refinement, and relation-aware self-attention to enhance feature relationships.

Result: Outperforms on ScanNetV2, ScanNet++, ScanNet200, and S3DIS datasets.

Conclusion: Relation3D advances 3D instance segmentation by better modeling feature relationships.

Abstract: 3D instance segmentation aims to predict a set of object instances in a
scene, representing them as binary foreground masks with corresponding semantic
labels. Currently, transformer-based methods are gaining increasing attention
due to their elegant pipelines and superior predictions. However, these methods
primarily focus on modeling the external relationships between scene features
and query features through mask attention. They lack effective modeling of the
internal relationships among scene features as well as between query features.
In light of these disadvantages, we propose \textbf{Relation3D: Enhancing
Relation Modeling for Point Cloud Instance Segmentation}. Specifically, we
introduce an adaptive superpoint aggregation module and a contrastive
learning-guided superpoint refinement module to better represent superpoint
features (scene features) and leverage contrastive learning to guide the
updates of these features. Furthermore, our relation-aware self-attention
mechanism enhances the capabilities of modeling relationships between queries
by incorporating positional and geometric relationships into the self-attention
mechanism. Extensive experiments on the ScanNetV2, ScanNet++, ScanNet200 and
S3DIS datasets demonstrate the superior performance of Relation3D.

</details>


### [212] [BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning](https://arxiv.org/pdf/2506.17892)
*Jianghong Huang, Luping Ji, Xin Ma, Mao Ye*

Main category: cs.CV

TL;DR: The paper introduces the first real-world industrial belt crack datasets (BeltCrack14ks and BeltCrack9kd) and proposes a triple-domain feature fusion method for crack detection, showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing crack datasets lack real-world industrial belt data, hindering machine learning advancements in belt health monitoring.

Method: Constructs sequential-image datasets from factory scenes and proposes a triple-domain (time-space-frequency) feature hierarchical fusion learning method.

Result: The datasets are validated as effective, and the proposed baseline outperforms similar detection methods.

Conclusion: The work advances belt crack detection by providing real-world datasets and a robust baseline method, with datasets and code publicly available.

Abstract: Conveyor belt is a category of important equipments in modern industry,
widely applied in production and manufacturing Fields. Its health status is
much critical to operation efficiency and safety hazards. Among the factors
affecting belt health, crack is often one of the most threatening risks.
Currently, considering safety, how to intelligently detect belt cracks is
catching an increasing attention. To implement the intelligent detection with
machine learning, real crack samples are believed to be necessary. However,
existing crack datasets primarily focus on pavement scenarios or synthetic
data, no real-world industrial belt crack datasets at all. To propel machine
learning advancement in this field, this paper constructs the first
sequential-image belt crack detection datasets (BeltCrack14ks and
BeltCrack9kd), from real-world factory scenes. Furthermore, to validate
usability and effectiveness, we propose a special baseline method with
triple-domain (i.e., time-space-frequency) feature hierarchical fusion learning
for the two whole-new datasets. Experimental results demonstrate the
availability and effectiveness of our dataset. Besides, they also show that our
baseline is obviously superior to other similar detection methods. Our datasets
and source codes are available at https://github.com/UESTC-nnLab/BeltCrack.

</details>


### [213] [EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations](https://arxiv.org/pdf/2506.17896)
*Junho Park, Andrew Sangwoo Ye, Taein Kwon*

Main category: cs.CV

TL;DR: EgoWorld is a two-stage framework for translating third-person views into first-person views using 3D cues and diffusion-based inpainting, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current exocentric-to-egocentric translation methods rely on 2D cues and unrealistic assumptions, limiting their applicability in AR, VR, and robotics.

Method: EgoWorld reconstructs egocentric views by reprojecting point clouds from exocentric depth maps and using diffusion-based inpainting for dense, coherent images.

Result: EgoWorld outperforms existing methods on H2O and TACO datasets and generalizes well to new objects, actions, and scenes.

Conclusion: EgoWorld advances egocentric vision by addressing limitations of current methods and shows promise for real-world applications.

Abstract: Egocentric vision is essential for both human and machine visual
understanding, particularly in capturing the detailed hand-object interactions
needed for manipulation tasks. Translating third-person views into first-person
views significantly benefits augmented reality (AR), virtual reality (VR) and
robotics applications. However, current exocentric-to-egocentric translation
methods are limited by their dependence on 2D cues, synchronized multi-view
settings, and unrealistic assumptions such as necessity of initial egocentric
frame and relative camera poses during inference. To overcome these challenges,
we introduce EgoWorld, a novel two-stage framework that reconstructs an
egocentric view from rich exocentric observations, including projected point
clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a
point cloud from estimated exocentric depth maps, reprojects it into the
egocentric perspective, and then applies diffusion-based inpainting to produce
dense, semantically coherent egocentric images. Evaluated on the H2O and TACO
datasets, EgoWorld achieves state-of-the-art performance and demonstrates
robust generalization to new objects, actions, scenes, and subjects. Moreover,
EgoWorld shows promising results even on unlabeled real-world examples.

</details>


### [214] [PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs](https://arxiv.org/pdf/2506.17901)
*Yixuan Wu, Yang Zhang, Jian Wu, Philip Torr, Jindong Gu*

Main category: cs.CV

TL;DR: MMGrounded-PostAlign is a post-multimodal alignment framework to enhance visual understanding and reduce hallucinations in MLLMs by grounding outputs in visual and textual evidence.


<details>
  <summary>Details</summary>
Motivation: MLLMs often rely on spurious correlations due to linguistic priors, neglecting visual information, leading to hallucinations.

Method: The framework includes a multimodal grounding module (visual and textual grounding) and introduces a negative rejection mechanism for visual grounding and selective reasoning for textual grounding.

Result: Significant improvements in fine-grained visual understanding and hallucination suppression on benchmarks like POPE, HaloQuest, VQAv2, MME, and MMBench.

Conclusion: MMGrounded-PostAlign effectively mitigates hallucinations and enhances visual understanding in MLLMs by grounding outputs in evidence.

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such
as image captioning and visual question answering. However, they often suffer
from over-reliance on spurious correlations, primarily due to linguistic priors
that distract the model from leveraging actual visual information. To address
these issues, we introduce MMGrounded-PostAlign, a post-multimodal alignment
framework designed to enhance the visual understanding capabilities and
mitigate the hallucinations of MLLMs. Our framework incorporates a multimodal
grounding module for both visual grounding, which identifies the referred
object in the image, and textual grounding, which generates the rationale for
the final answer, ensuring that outputs are anchored in both visual and textual
evidence. To mitigate the hallucinations, we introduce a negative rejection
mechanism in the visual grounding module to distinguish grounded entities from
non-existent objects influenced by linguistic biases. On the textual grounding
side, we propose a selective reasoning mechanism that adjusts the model's
reasoning strategy based on query complexity. Extensive evaluations are
conducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench
showing significant improvements in fine-grained visual understanding and
hallucination suppression.

</details>


### [215] [Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases](https://arxiv.org/pdf/2506.17903)
*Huanjia Zhu, Yishu Liu, Xiaozhao Fang, Guangming Lu, Bingzhi Chen*

Main category: cs.CV

TL;DR: CEDO is a framework to mitigate language biases in Med-VQA models using three mechanisms: MHO, GMS, and DLR, addressing biases from causal and effectual perspectives.


<details>
  <summary>Details</summary>
Motivation: Existing Med-VQA models suffer from language biases due to spurious correlations between question types and answer categories.

Method: CEDO incorporates MHO (adaptive learning rates), GMS (Pareto optimization for modality synergy), and DLR (adaptive loss weights) to mitigate biases.

Result: CEDO outperforms state-of-the-art models on traditional and bias-sensitive benchmarks.

Conclusion: CEDO effectively addresses language biases in Med-VQA models through its comprehensive framework.

Abstract: Existing Medical Visual Question Answering (Med-VQA) models often suffer from
language biases, where spurious correlations between question types and answer
categories are inadvertently established. To address these issues, we propose a
novel Cause-Effect Driven Optimization framework called CEDO, that incorporates
three well-established mechanisms, i.e., Modality-driven Heterogeneous
Optimization (MHO), Gradient-guided Modality Synergy (GMS), and
Distribution-adapted Loss Rescaling (DLR), for comprehensively mitigating
language biases from both causal and effectual perspectives. Specifically, MHO
employs adaptive learning rates for specific modalities to achieve
heterogeneous optimization, thus enhancing robust reasoning capabilities.
Additionally, GMS leverages the Pareto optimization method to foster
synergistic interactions between modalities and enforce gradient orthogonality
to eliminate bias updates, thereby mitigating language biases from the effect
side, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive
weights to individual losses to ensure balanced learning across all answer
categories, effectively alleviating language biases from the cause side, i.e.,
imbalance biases within datasets. Extensive experiments on multiple traditional
and bias-sensitive benchmarks consistently demonstrate the robustness of CEDO
over state-of-the-art competitors.

</details>


### [216] [Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis](https://arxiv.org/pdf/2506.17910)
*Mohamed Benkedadra, Matei Mancas, Sidi Ahmed Mahmoudi*

Main category: cs.CV

TL;DR: A 3D stereo vision pipeline for interactive systems is proposed, addressing limitations of 2D and short-range 3D cameras by enabling robust scene understanding and multi-camera fusion for tasks like event recognition and tracking.


<details>
  <summary>Details</summary>
Motivation: Current 2D and short-range 3D cameras are unreliable in large, complex environments, prompting the need for a more robust solution.

Method: The paper introduces a 3D stereo vision pipeline, fusing multiple 3D cameras for full scene reconstruction and incorporating feedback for adaptive learning.

Result: Preliminary results demonstrate the pipeline's capability for tasks like event recognition and subject tracking.

Conclusion: The paper outlines a roadmap for further development to bring the pipeline into production.

Abstract: 2D cameras are often used in interactive systems. Other systems like gaming
consoles provide more powerful 3D cameras for short range depth sensing.
Overall, these cameras are not reliable in large, complex environments. In this
work, we propose a 3D stereo vision based pipeline for interactive systems,
that is able to handle both ordinary and sensitive applications, through robust
scene understanding. We explore the fusion of multiple 3D cameras to do full
scene reconstruction, which allows for preforming a wide range of tasks, like
event recognition, subject tracking, and notification. Using possible feedback
approaches, the system can receive data from the subjects present in the
environment, to learn to make better decisions, or to adapt to completely new
environments. Throughout the paper, we introduce the pipeline and explain our
preliminary experimentation and results. Finally, we draw the roadmap for the
next steps that need to be taken, in order to get this pipeline into production

</details>


### [217] [IDAL: Improved Domain Adaptive Learning for Natural Images Dataset](https://arxiv.org/pdf/2506.17931)
*Ravi Kant Gupta, Shounak Das, Amit Sethi*

Main category: cs.CV

TL;DR: A novel unsupervised domain adaptation (UDA) method for natural images combines ResNet and FPN architectures with a tailored loss function to improve domain alignment and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial UDA methods struggle with aligning multimodal distributions in classification tasks, especially for natural images with scale, noise, and style shifts.

Method: Uses ResNet and FPN for feature extraction and a novel combined loss function to address domain shifts.

Result: Outperforms state-of-the-art CNN-based methods on Office-Home, Office-31, and VisDA-2017 datasets, with comparable performance on DomainNet.

Conclusion: The proposed UDA scheme improves accuracy, robustness, and training convergence for natural images.

Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for
natural images. A commonly-used objective for UDA schemes is to enhance domain
alignment in representation space even if there is a domain shift in the input
space. Existing adversarial domain adaptation methods may not effectively align
different domains of multimodal distributions associated with classification
problems. Our approach has two main features. Firstly, its neural architecture
uses the deep structure of ResNet and the effective separation of scales of
feature pyramidal network (FPN) to work with both content and style features.
Secondly, it uses a combination of a novel loss function and judiciously
selected existing loss functions to train the network architecture. This
tailored combination is designed to address challenges inherent to natural
images, such as scale, noise, and style shifts, that occur on top of a
multi-modal (multi-class) distribution. The combined loss function not only
enhances model accuracy and robustness on the target domain but also speeds up
training convergence. Our proposed UDA scheme generalizes better than
state-of-the-art for CNN-based methods on Office-Home, Office-31, and
VisDA-2017 datasets and comaparable for DomainNet dataset.

</details>


### [218] [GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning](https://arxiv.org/pdf/2506.17939)
*Bo Liu, Xiangyu Zhao, Along He, Yidi Chen, Huazhu Fu, Xiao-Ming Wu*

Main category: cs.CV

TL;DR: The paper introduces a dataset (ThinkVG) and a verifiable reward mechanism to improve reliability and interpretability in medical visual question answering, achieving strong performance with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Current methods in medical visual question answering lack reliability and interpretability, limiting trust in model-generated answers.

Method: Proposes the ThinkVG dataset for fine-grained explainability and a verifiable reward mechanism for reinforcement learning to align reasoning with answers.

Result: The method achieves comparable performance with only one-eighth of the training data.

Conclusion: The approach enhances trust and efficiency in medical visual question answering.

Abstract: Medical visual question answering aims to support clinical decision-making by
enabling models to answer natural language questions based on medical images.
While recent advances in multi-modal learning have significantly improved
performance, current methods still suffer from limited answer reliability and
poor interpretability, impairing the ability of clinicians and patients to
understand and trust model-generated answers. To address this, this work first
proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer
generation is decomposed into intermediate reasoning steps that explicitly
ground relevant visual regions of the medical image, thereby providing
fine-grained explainability. Furthermore, we introduce a novel verifiable
reward mechanism for reinforcement learning to guide post-training, improving
the alignment between the model's reasoning process and its final answer.
Remarkably, our method achieves comparable performance using only one-eighth of
the training data, demonstrating the efficiency and effectiveness of the
proposal. The dataset is available at
https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.

</details>


### [219] [SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via Large Language Models](https://arxiv.org/pdf/2506.17944)
*Fei Zhou*

Main category: cs.CV

TL;DR: A novel LLM-augmented inference approach (SegChange-R1) for remote sensing change detection integrates textual descriptions to enhance detection and segmentation of key regions, alongside a BEV spatial transformation module to align multi-temporal features.


<details>
  <summary>Details</summary>
Motivation: Improving change detection in remote sensing by leveraging textual information and solving modal misalignment issues for better accuracy and convergence.

Method: Proposes SegChange-R1 with LLM augmentation for textual guidance and a BEV module for feature alignment. Validated on a new UAV dataset (DVCD) and four existing datasets.

Result: Significant improvement over existing methods, demonstrated on multiple datasets.

Conclusion: SegChange-R1 effectively enhances change detection by combining textual guidance and spatial alignment, with promising results.

Abstract: Remote sensing change detection is widely used in a variety of fields such as
urban planning, terrain and geomorphology analysis, and environmental
monitoring, mainly by analyzing the significant change differences of features
(e.g., building changes) in the same spatial region at different time phases.
In this paper, we propose a large language model (LLM) augmented inference
approach (SegChange-R1), which enhances the detection capability by integrating
textual descriptive information and aims at guiding the model to segment the
more interested change regions, thus accelerating the convergence speed.
Moreover, we design a spatial transformation module (BEV) based on linear
attention, which solves the problem of modal misalignment in change detection
by unifying features from different temporal perspectives onto the BEV space.
In addition, we construct the first dataset for building change detection from
UAV viewpoints (DVCD ), and our experiments on four widely-used change
detection datasets show a significant improvement over existing methods. The
code and pre-trained models are available in
https://github.com/Yu-Zhouz/SegChange-R1.

</details>


### [220] [Classification of Tents in Street Bazaars Using CNN](https://arxiv.org/pdf/2506.17946)
*Azamat Ibragimov, Ruslan Isaev, Remudin Reshid Mekuria, Gulnaz Gimaletdinova, Dim Shaiakhmetov*

Main category: cs.CV

TL;DR: The paper proposes a deep learning model for classifying tents in street bazaars, comparing a custom CNN with EfficientNetB0. EfficientNetB0 outperforms the custom CNN with 98.4% accuracy, highlighting the benefits of transfer learning.


<details>
  <summary>Details</summary>
Motivation: Street bazaars are economically significant but lack structured organization. Manual tent classification is inefficient, necessitating automated solutions.

Method: A custom CNN and EfficientNetB0 were trained on an augmented dataset of 126 original bazaar tent images. Performance was evaluated using accuracy, precision, recall, F1 score, and mAP.

Result: EfficientNetB0 achieved 98.4% accuracy, outperforming the custom CNN's 92.8%. The confusion matrix analysis revealed model strengths and weaknesses.

Conclusion: Pre-trained models like EfficientNetB0 significantly enhance classification accuracy and generalization for bazaar tent classification.

Abstract: This research paper proposes an improved deep learning model for classifying
tents in street bazaars, comparing a custom Convolutional Neural Network (CNN)
with EfficientNetB0. This is a critical task for market organization with a
tent classification, but manual methods in the past have been inefficient.
Street bazaars represent a vital economic hub in many regions, yet their
unstructured nature poses significant challenges for the automated
classification of market infrastructure, such as tents. In Kyrgyzstan, more
than a quarter of the country's GDP is derived from bazaars. While CNNs have
been widely applied to object recognition, their application to bazaar-specific
tasks remains underexplored. Here, we build upon our original approach by
training on an extended set of 126 original photographs that were augmented to
generate additional images. This dataset is publicly available for download on
Kaggle. A variety of performance metrics, such as accuracy, precision, recall,
F1 score, and mean average precision (mAP), were used to assess the models
comparatively, providing a more extensive analysis of classification
performance.
  The results show that the CNN custom model achieved 92.8% accuracy, and
EfficientNetB0 showed 98.4% accuracy results, confirming the effectiveness of
transfer learning in the bazaar image classification. Also, when analyzing the
confusion matrix, the analysis reveals the weaknesses and strengths of each
model. These findings suggest that using a pre-trained model such as
EfficientNetB0 significantly improves classification accuracy and
generalization.

</details>


### [221] [Mobile Image Analysis Application for Mantoux Skin Test](https://arxiv.org/pdf/2506.17954)
*Liong Gele, Tan Chye Cheah*

Main category: cs.CV

TL;DR: A mobile app using scaling stickers and advanced image processing (ARCore, DeepLabv3) improves LTBI diagnosis via TST, addressing traditional issues like low follow-up and subjective interpretation.


<details>
  <summary>Details</summary>
Motivation: Traditional TST methods have low follow-up rates, patient discomfort, and subjective interpretation, leading to misdiagnosis.

Method: The app uses scaling stickers for reference, ARCore, and DeepLabv3 for image segmentation, plus edge detection for precise induration measurement.

Result: The app showed significant accuracy and reliability improvements over standard clinical practices.

Conclusion: The app enhances TB diagnostics, especially in resource-limited areas. Future work will refine ML models and expand functionalities.

Abstract: This paper presents a newly developed mobile application designed to diagnose
Latent Tuberculosis Infection (LTBI) using the Mantoux Skin Test (TST).
Traditional TST methods often suffer from low follow-up return rates, patient
discomfort, and subjective manual interpretation, particularly with the
ball-point pen method, leading to misdiagnosis and delayed treatment. Moreover,
previous developed mobile applications that used 3D reconstruction, this app
utilizes scaling stickers as reference objects for induration measurement. This
mobile application integrates advanced image processing technologies, including
ARCore, and machine learning algorithms such as DeepLabv3 for robust image
segmentation and precise measurement of skin indurations indicative of LTBI.
The system employs an edge detection algorithm to enhance accuracy. The
application was evaluated against standard clinical practices, demonstrating
significant improvements in accuracy and reliability. This innovation is
crucial for effective tuberculosis management, especially in resource-limited
regions. By automating and standardizing TST evaluations, the application
enhances the accessibility and efficiency of TB di-agnostics. Future work will
focus on refining machine learning models, optimizing measurement algorithms,
expanding functionalities to include comprehensive patient data management, and
enhancing ARCore's performance across various lighting conditions and
operational settings.

</details>


### [222] [ELMAR: Enhancing LiDAR Detection with 4D Radar Motion Awareness and Cross-modal Uncertainty](https://arxiv.org/pdf/2506.17958)
*Xiangyuan Peng, Miao Tang, Huawei Sun, Bierzynski Kay, Lorenzo Servadei, Robert Wille*

Main category: cs.CV

TL;DR: A LiDAR detection framework enhanced by 4D radar motion status and cross-modal uncertainty to address misalignment and improve perception in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: LiDAR and 4D radar fusion is promising but suffers from misalignment issues. This work aims to leverage both modalities' strengths while addressing alignment challenges.

Method: Uses a Dynamic Motion-Aware Encoding module for 4D radar feature extraction and estimates instance-wise uncertainties to refine LiDAR predictions.

Result: Achieves 74.89% mAP in the entire area and 88.70% in the driving corridor on the VoD dataset, with real-time inference at 30.02 FPS.

Conclusion: The proposed framework effectively combines LiDAR and 4D radar, improving perception accuracy and robustness in autonomous driving.

Abstract: LiDAR and 4D radar are widely used in autonomous driving and robotics. While
LiDAR provides rich spatial information, 4D radar offers velocity measurement
and remains robust under adverse conditions. As a result, increasing studies
have focused on the 4D radar-LiDAR fusion method to enhance the perception.
However, the misalignment between different modalities is often overlooked. To
address this challenge and leverage the strengths of both modalities, we
propose a LiDAR detection framework enhanced by 4D radar motion status and
cross-modal uncertainty. The object movement information from 4D radar is first
captured using a Dynamic Motion-Aware Encoding module during feature extraction
to enhance 4D radar predictions. Subsequently, the instance-wise uncertainties
of bounding boxes are estimated to mitigate the cross-modal misalignment and
refine the final LiDAR predictions. Extensive experiments on the View-of-Delft
(VoD) dataset highlight the effectiveness of our method, achieving
state-of-the-art performance with the mAP of 74.89% in the entire area and
88.70% within the driving corridor while maintaining a real-time inference
speed of 30.02 FPS.

</details>


### [223] [BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP](https://arxiv.org/pdf/2506.17969)
*Chenyue Song, Chen Hui, Wei Zhang, Haiqi Zhu, Shaohui Liu, Hong Huang, Feng Jiang*

Main category: cs.CV

TL;DR: BPCLIP is a bottom-up IQA method using CLIP to link low-level distortions to high-level semantics, outperforming existing methods on FR and NR benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing IQA methods linearly fuse multiscale features, failing to capture distortion impacts on semantics. BPCLIP addresses this by leveraging CLIP for better alignment.

Method: Uses a bottom-up multiscale cross attention module and CLIP text encoder with 40 quality adjectives to connect image quality to human language.

Result: Achieves superior performance on FR and NR IQA benchmarks with greater robustness.

Conclusion: BPCLIP effectively bridges low-level distortions and high-level semantics, enhancing IQA performance.

Abstract: Image Quality Assessment (IQA) aims to evaluate the perceptual quality of
images based on human subjective perception. Existing methods generally combine
multiscale features to achieve high performance, but most rely on
straightforward linear fusion of these features, which may not adequately
capture the impact of distortions on semantic content. To address this, we
propose a bottom-up image quality assessment approach based on the Contrastive
Language-Image Pre-training (CLIP, a recently proposed model that aligns images
and text in a shared feature space), named BPCLIP, which progressively extracts
the impact of low-level distortions on high-level semantics. Specifically, we
utilize an encoder to extract multiscale features from the input image and
introduce a bottom-up multiscale cross attention module designed to capture the
relationships between shallow and deep features. In addition, by incorporating
40 image quality adjectives across six distinct dimensions, we enable the
pre-trained CLIP text encoder to generate representations of the intrinsic
quality of the image, thereby strengthening the connection between image
quality perception and human language. Our method achieves superior results on
most public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while
demonstrating greater robustness.

</details>


### [224] [Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models](https://arxiv.org/pdf/2506.17975)
*Mischa Dombrowski, Bernhard Kainz*

Main category: cs.CV

TL;DR: The paper proposes a framework for generating synthetic medical imaging data that balances diversity and fidelity, ensuring privacy compliance (e.g., GDPR) while matching real-data performance.


<details>
  <summary>Details</summary>
Motivation: Synthetic data is promising for privacy-preserving medical imaging but faces legal and performance challenges. Current methods prioritize diversity over fidelity, neglecting privacy regulations.

Method: A generalized framework for training diffusion models on personal data to produce unpersonal synthetic datasets, ensuring predicate singling-out (PSO) security.

Result: The synthetic datasets achieve performance within one percentage point of real-data models and outperform non-privacy-preserving state-of-the-art methods.

Conclusion: Maximizing diversity in synthetic data not only improves performance but also ensures privacy compliance, making it viable for medical imaging applications.

Abstract: Synthetic data has recently reached a level of visual fidelity that makes it
nearly indistinguishable from real data, offering great promise for
privacy-preserving data sharing in medical imaging. However, fully synthetic
datasets still suffer from significant limitations: First and foremost, the
legal aspect of sharing synthetic data is often neglected and data regulations,
such as the GDPR, are largley ignored. Secondly, synthetic models fall short of
matching the performance of real data, even for in-domain downstream
applications. Recent methods for image generation have focused on maximising
image diversity instead of fidelity solely to improve the mode coverage and
therefore the downstream performance of synthetic data. In this work, we shift
perspective and highlight how maximizing diversity can also be interpreted as
protecting natural persons from being singled out, which leads to predicate
singling-out (PSO) secure synthetic datasets. Specifically, we propose a
generalisable framework for training diffusion models on personal data which
leads to unpersonal synthetic datasets achieving performance within one
percentage point of real-data models while significantly outperforming
state-of-the-art methods that do not ensure privacy. Our code is available at
https://github.com/MischaD/Trichotomy.

</details>


### [225] [Fast Neural Inverse Kinematics on Human Body Motions](https://arxiv.org/pdf/2506.17996)
*David Tolpin, Sefy Kagarlitsky*

Main category: cs.CV

TL;DR: A neural inverse kinematics framework for real-time markerless motion capture from 3D keypoints is presented, addressing computational and speed limitations.


<details>
  <summary>Details</summary>
Motivation: Markerless motion capture offers flexibility and cost savings but suffers from high computational demands and slow inference, hindering real-time use.

Method: The paper details a neural inverse kinematics framework, including network architecture, training methodology, and inference procedure, validated through ablation studies.

Result: The framework is evaluated qualitatively and quantitatively, demonstrating its effectiveness for real-time human motion capture.

Conclusion: The proposed framework provides a fast and reliable solution for real-time markerless motion capture, overcoming prior limitations.

Abstract: Markerless motion capture enables the tracking of human motion without
requiring physical markers or suits, offering increased flexibility and reduced
costs compared to traditional systems. However, these advantages often come at
the expense of higher computational demands and slower inference, limiting
their applicability in real-time scenarios. In this technical report, we
present a fast and reliable neural inverse kinematics framework designed for
real-time capture of human body motions from 3D keypoints. We describe the
network architecture, training methodology, and inference procedure in detail.
Our framework is evaluated both qualitatively and quantitatively, and we
support key design decisions through ablation studies.

</details>


### [226] [OSDMamba: Enhancing Oil Spill Detection from Remote Sensing Images Using Selective State Space Model](https://arxiv.org/pdf/2506.18006)
*Shuaiyu Chen, Fu Wang, Peng Ren, Chunbo Luo, Zeyu Fu*

Main category: cs.CV

TL;DR: OSDMamba, a Mamba-based architecture, improves oil spill detection by addressing class imbalance and small area detection challenges, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Limited labelled data and class imbalance reduce accuracy in oil spill detection, and CNNs struggle with small areas and global context.

Method: Proposes OSDMamba, leveraging Mamba's selective scanning for wider receptive fields and an asymmetric decoder with ConvSSM for multi-scale feature fusion.

Result: OSDMamba improves detection by 8.9% and 11.8% on two datasets.

Conclusion: OSDMamba outperforms existing methods, demonstrating the potential of SSMs like Mamba for oil spill detection.

Abstract: Semantic segmentation is commonly used for Oil Spill Detection (OSD) in
remote sensing images. However, the limited availability of labelled oil spill
samples and class imbalance present significant challenges that can reduce
detection accuracy. Furthermore, most existing methods, which rely on
convolutional neural networks (CNNs), struggle to detect small oil spill areas
due to their limited receptive fields and inability to effectively capture
global contextual information. This study explores the potential of State-Space
Models (SSMs), particularly Mamba, to overcome these limitations, building on
their recent success in vision applications. We propose OSDMamba, the first
Mamba-based architecture specifically designed for oil spill detection.
OSDMamba leverages Mamba's selective scanning mechanism to effectively expand
the model's receptive field while preserving critical details. Moreover, we
designed an asymmetric decoder incorporating ConvSSM and deep supervision to
strengthen multi-scale feature fusion, thereby enhancing the model's
sensitivity to minority class samples. Experimental results show that the
proposed OSDMamba achieves state-of-the-art performance, yielding improvements
of 8.9% and 11.8% in OSD across two publicly available datasets.

</details>


### [227] [PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding](https://arxiv.org/pdf/2506.18023)
*Kui Huang, Xinrong Chen, Wenyu Lv, Jincheng Liao, Guanzhong Wang, Yi Liu*

Main category: cs.CV

TL;DR: PP-DocBee2 improves multimodal document understanding with better data quality, feature fusion, and inference methods, achieving 11.4% performance gain and 73% latency reduction.


<details>
  <summary>Details</summary>
Motivation: To address limitations of PP-DocBee in multimodal document understanding by enhancing data quality and model efficiency.

Method: Uses a large-scale multimodal pre-trained model for data evaluation, applies statistical filtering, and improves ViT feature fusion.

Result: 11.4% performance boost on Chinese business documents and 73% reduction in inference latency.

Conclusion: PP-DocBee2 effectively enhances multimodal document understanding through optimized data and feature fusion strategies.

Abstract: This report introduces PP-DocBee2, an advanced version of the PP-DocBee,
designed to enhance multimodal document understanding. Built on a large
multimodal model architecture, PP-DocBee2 addresses the limitations of its
predecessor through key technological improvements, including enhanced
synthetic data quality, improved visual feature fusion strategy, and optimized
inference methodologies. These enhancements yield an $11.4\%$ performance boost
on internal benchmarks for Chinese business documents, and reduce inference
latency by $73.0\%$ to the vanilla version. A key innovation of our work is a
data quality optimization strategy for multimodal document tasks. By employing
a large-scale multimodal pre-trained model to evaluate data, we apply a novel
statistical criterion to filter outliers, ensuring high-quality training data.
Inspired by insights into underutilized intermediate features in multimodal
models, we enhance the ViT representational capacity by decomposing it into
layers and applying a novel feature fusion strategy to improve complex
reasoning. The source code and pre-trained model are available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.

</details>


### [228] [MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis](https://arxiv.org/pdf/2506.18028)
*Junjian Li, Hulin Kuang, Jin Liu, Hailin Yue, Mengshen He, Jianxin Wang*

Main category: cs.CV

TL;DR: MiCo, a novel MIL framework, addresses spatial heterogeneity in WSIs by clustering instances and enhancing cross-regional correlations, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Spatial heterogeneity in WSIs makes it hard for conventional MIL methods to model scattered tissue distributions and capture cross-regional interactions.

Method: MiCo clusters instances to create semantic anchors, uses a Cluster Route module for intra-tissue correlations, and a Cluster Reducer module for inter-tissue semantic associations.

Result: MiCo outperforms state-of-the-art methods on nine large-scale cancer datasets.

Conclusion: MiCo effectively addresses spatial heterogeneity in WSIs, improving cancer diagnosis and prognosis.

Abstract: Multiple instance learning (MIL) has shown significant promise in
histopathology whole slide image (WSI) analysis for cancer diagnosis and
prognosis. However, the inherent spatial heterogeneity of WSIs presents
critical challenges, as morphologically similar tissue types are often
dispersed across distant anatomical regions. Conventional MIL methods struggle
to model these scattered tissue distributions and capture cross-regional
spatial interactions effectively. To address these limitations, we propose a
novel Multiple instance learning framework with Context-Aware Clustering
(MiCo), designed to enhance cross-regional intra-tissue correlations and
strengthen inter-tissue semantic associations in WSIs. MiCo begins by
clustering instances to distill discriminative morphological patterns, with
cluster centroids serving as semantic anchors. To enhance cross-regional
intra-tissue correlations, MiCo employs a Cluster Route module, which
dynamically links instances of the same tissue type across distant regions via
feature similarity. These semantic anchors act as contextual hubs, propagating
semantic relationships to refine instance-level representations. To eliminate
semantic fragmentation and strengthen inter-tissue semantic associations, MiCo
integrates a Cluster Reducer module, which consolidates redundant anchors while
enhancing information exchange between distinct semantic groups. Extensive
experiments on two challenging tasks across nine large-scale public cancer
datasets demonstrate the effectiveness of MiCo, showcasing its superiority over
state-of-the-art methods. The code is available at
https://github.com/junjianli106/MiCo.

</details>


### [229] [CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images](https://arxiv.org/pdf/2506.18042)
*Dongdong Meng, Sheng Li, Hao Wu, Suqing Tian, Wenjun Ma, Guoping Wang, Xueqing Yan*

Main category: cs.CV

TL;DR: CmFNet is a 3D weakly supervised cross-modal medical image segmentation method that improves performance by integrating multi-modal data and hybrid supervision, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: High-quality medical image annotations are costly and time-consuming, and weakly supervised methods often suffer from performance degradation and overfitting due to sparse annotations.

Method: CmFNet combines modality-specific and cross-modal feature learning networks with a hybrid-supervised strategy (scribble supervision, intra-modal regularization, inter-modal consistency) to enhance segmentation.

Result: CmFNet outperforms state-of-the-art weakly supervised methods and even fully supervised methods on clinical NPC and WORD datasets.

Conclusion: CmFNet effectively addresses challenges in weakly supervised medical image segmentation, offering robust performance for clinical applications.

Abstract: Accurate automatic medical image segmentation relies on high-quality, dense
annotations, which are costly and time-consuming. Weakly supervised learning
provides a more efficient alternative by leveraging sparse and coarse
annotations instead of dense, precise ones. However, segmentation performance
degradation and overfitting caused by sparse annotations remain key challenges.
To address these issues, we propose CmFNet, a novel 3D weakly supervised
cross-modal medical image segmentation approach. CmFNet consists of three main
components: a modality-specific feature learning network, a cross-modal feature
learning network, and a hybrid-supervised learning strategy. Specifically, the
modality-specific feature learning network and the cross-modal feature learning
network effectively integrate complementary information from multi-modal
images, enhancing shared features across modalities to improve segmentation
performance. Additionally, the hybrid-supervised learning strategy guides
segmentation through scribble supervision, intra-modal regularization, and
inter-modal consistency, modeling spatial and contextual relationships while
promoting feature alignment. Our approach effectively mitigates overfitting,
delivering robust segmentation results. It excels in segmenting both
challenging small tumor regions and common anatomical structures. Extensive
experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset
(including CT and MR imaging) and the publicly available CT Whole Abdominal
Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly
supervised methods. In addition, our approach also outperforms fully supervised
methods when full annotation is used. Our approach can facilitate clinical
therapy and benefit various specialists, including physicists, radiologists,
pathologists, and oncologists.

</details>


### [230] [CLGRPO: Reasoning Ability Enhancement for Small VLMs](https://arxiv.org/pdf/2506.18048)
*Fanyi Wang, Binzhi Dong, Haotian Hu, Jinjin Xu, Zhiwang Zhang*

Main category: cs.CV

TL;DR: The paper proposes an Incremental Training Strategy to enhance the reasoning ability of Small Vision Language Models (SVLMs) using a four-stage post-training optimization paradigm, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: SVLMs (2B parameters) are cost-effective but limited in reasoning ability due to their size. This work aims to enhance their reasoning without increasing parameters.

Method: 1) Construct Self-Supervised COT Data using larger LVLMs. 2) Four-stage training: SFT for domain knowledge, GRPO for format alignment, GRPO for reasoning, and CLGRPO to constrain capture space.

Result: Accuracy improved by 2.77 and recall by 0.69 on EMOSet-118K, matching 8B model performance.

Conclusion: The Incremental Training Strategy effectively boosts SVLM reasoning, making them competitive with larger models.

Abstract: Small Vision Language Models (SVLMs) generally refer to models with parameter
sizes less than or equal to 2B. Their low cost and power consumption
characteristics confer high commercial value. However, their reasoning
abilities are limited by the number of parameters. To address this issue, this
paper proposes a post-training optimization paradigm called the Incremental
Training Strategy to enhance the reasoning ability of SVLMs. Firstly, we
constructed a Self-Supervised Chain-of-Thought (COT) Data Construction System,
which leverages multiple LVLMs with 7B parameters or more to transform original
data into COT data in a self-supervised manner. Our proposed Incremental
Training Strategy consists of four stages. Stage 1 injects domain knowledge by
performing Supervised Fine-Tuning (SFT) to the pretrained model on the COT
data. Stage 2 aligns the COT data format by conducting a small amount of Group
Relative Policy Optimization (GRPO) training constrained only by format rewards
on the COT data. Stage 3 enhances reasoning ability by applying GRPO training
on the COT data with constraints on both format and accuracy rewards. The
resulting model shows significant improvement compared to the baseline. Stage 4
addresses the limited capacity of the SVLMs and the weak ability to capture
complex patterns by proposing ClipLow GRPO (CLGRPO) to constrain the capture
space of the training process. We conducted extensive comparative and ablation
experiments on the abstract semantic recognition dataset EMOSet-118K.
Experimental results demonstrate that our method significantly improves the
reasoning ability of 1B SVLM. Compared to the baseline model fine-tuned on the
original data, accuracy increased by 2.77 and recall by 0.69, achieving
performance comparable to that of 8B models.

</details>


### [231] [Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes](https://arxiv.org/pdf/2506.18060)
*Olivia Zumsteg, Nico Graf, Aaron Haeusler, Norbert Kirchgessner, Nicola Storni, Lukas Roth, Andreas Hund*

Main category: cs.CV

TL;DR: A neural network approach using DINOv2 and LSTM outperforms traditional methods for estimating wheat spike volume from 2D RGB images, achieving a MAPE of 6.46%.


<details>
  <summary>Details</summary>
Motivation: Challenges in estimating 3D traits from 2D images due to depth loss, distortions, and occlusions in field conditions.

Method: Proposes a transfer learning pipeline combining DINOv2 (self-supervised Vision Transformer) with LSTM, using deep supervision for robust representations.

Result: Achieves 6.46% MAPE on indoor images, outperforming area-based (9.36%) and geometric (13.98%) methods. Fine-tuning for field data yields 10.82% MAPE.

Conclusion: Deep learning outperforms geometric methods, especially for irregular shapes like wheat spikes, demonstrating robustness and adaptability.

Abstract: Estimating three-dimensional morphological traits from two-dimensional RGB
images presents inherent challenges due to the loss of depth information,
projection distortions, and occlusions under field conditions. In this work, we
explore multiple approaches for non-destructive volume estimation of wheat
spikes, using RGB image sequences and structured-light 3D scans as ground truth
references. Due to the complex geometry of the spikes, we propose a neural
network approach for volume estimation in 2D images, employing a transfer
learning pipeline that combines DINOv2, a self-supervised Vision Transformer,
with a unidirectional Long Short-Term Memory (LSTM) network. By using deep
supervision, the model is able to learn more robust intermediate
representations, which enhances its generalisation ability across varying
evaluation sequences. We benchmark our model against two conventional
baselines: a 2D area-based projection and a geometric reconstruction using
axis-aligned cross-sections. Our deep supervised model achieves a mean absolute
percentage error (MAPE) of 6.46% on six-view indoor images, outperforming the
area (9.36%) and geometric (13.98%) baselines. Fine-tuning the model on
field-based single-image data enables domain adaptation, yielding a MAPE of
10.82%. We demonstrate that object shape significantly impacts volume
prediction accuracy, with irregular geometries such as wheat spikes posing
greater challenges for geometric methods compared to our deep learning
approach.

</details>


### [232] [Training-free Test-time Improvement for Explainable Medical Image Classification](https://arxiv.org/pdf/2506.18070)
*Hangzhou He, Jiachen Tang, Lei Zhu, Kaiwen Li, Yanye Lu*

Main category: cs.CV

TL;DR: The paper proposes a training-free strategy to improve Concept Bottleneck Models (CBMs) for medical image classification, addressing challenges like concept-level shifts and high annotation costs.


<details>
  <summary>Details</summary>
Motivation: To enhance the deployment of CBMs in diverse clinical settings by mitigating issues like concept shifts and reducing reliance on costly expert annotations.

Method: A training-free confusion concept identification strategy using minimal new data (4 images per class) with image-level labels, focusing on masking confounding concepts and amplifying discriminative ones.

Result: Validated on skin and white blood cell images, the method improves out-of-domain performance without compromising source domain accuracy.

Conclusion: The proposed approach effectively addresses deployment challenges of CBMs in medical imaging, offering a practical solution with minimal data requirements.

Abstract: Deep learning-based medical image classification techniques are rapidly
advancing in medical image analysis, making it crucial to develop accurate and
trustworthy models that can be efficiently deployed across diverse clinical
scenarios. Concept Bottleneck Models (CBMs), which first predict a set of
explainable concepts from images and then perform classification based on these
concepts, are increasingly being adopted for explainable medical image
classification. However, the inherent explainability of CBMs introduces new
challenges when deploying trained models to new environments. Variations in
imaging protocols and staining methods may induce concept-level shifts, such as
alterations in color distribution and scale. Furthermore, since CBM training
requires explicit concept annotations, fine-tuning models solely with
image-level labels could compromise concept prediction accuracy and
faithfulness - a critical limitation given the high cost of acquiring
expert-annotated concept labels in medical domains. To address these
challenges, we propose a training-free confusion concept identification
strategy. By leveraging minimal new data (e.g., 4 images per class) with only
image-level labels, our approach enhances out-of-domain performance without
sacrificing source domain accuracy through two key operations: masking
misactivated confounding concepts and amplifying under-activated discriminative
concepts. The efficacy of our method is validated on both skin and white blood
cell images. Our code is available at:
https://github.com/riverback/TF-TTI-XMed.

</details>


### [233] [MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering](https://arxiv.org/pdf/2506.18071)
*Jisheng Dang, Huilin Song, Junbin Xiao, Bimei Wang, Han Peng, Haoxuan Li, Xun Yang, Meng Wang, Tat-Seng Chua*

Main category: cs.CV

TL;DR: MUPA is a multi-path agentic approach for Grounded VideoQA, improving grounding fidelity and answer accuracy with a cooperative design of grounding, QA, reflection, and aggregation agents.


<details>
  <summary>Details</summary>
Motivation: Modern multimodal models often rely on linguistic priors and spurious correlations, leading to poorly grounded predictions in Grounded VideoQA.

Method: MUPA unifies video grounding, question answering, answer reflection, and aggregation through three reasoning paths and a reflection agent.

Result: MUPA outperforms 7B-scale competitors with 2B parameters and achieves state-of-the-art results (Acc@GQA of 30.3% and 47.4%) when scaled to 7B.

Conclusion: MUPA effectively enhances trustworthy video-language understanding by balancing grounding fidelity and answer accuracy.

Abstract: Grounded Video Question Answering (Grounded VideoQA) requires aligning
textual answers with explicit visual evidence. However, modern multimodal
models often rely on linguistic priors and spurious correlations, resulting in
poorly grounded predictions. In this work, we propose MUPA, a cooperative
MUlti-Path Agentic approach that unifies video grounding, question answering,
answer reflection and aggregation to tackle Grounded VideoQA. MUPA features
three distinct reasoning paths on the interplay of grounding and QA agents in
different chronological orders, along with a dedicated reflection agent to
judge and aggregate the multi-path results to accomplish consistent QA and
grounding. This design markedly improves grounding fidelity without sacrificing
answer accuracy. Despite using only 2B parameters, our method outperforms all
7B-scale competitors. When scaled to 7B parameters, MUPA establishes new
state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and
DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy
video-language understanding. Our code is available in
https://github.com/longmalongma/MUPA.

</details>


### [234] [TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving](https://arxiv.org/pdf/2506.18084)
*Wenzhuo Liu, Yicheng Qiao, Zhen Wang, Qiannan Guo, Zilong Chen, Meihua Zhou, Xinran Li, Letian Wang, Zhiwei Li, Huaping Liu, Wenshuo Wang*

Main category: cs.CV

TL;DR: TEM^3-Learning is a novel multimodal multi-task learning framework for assistive driving, addressing limitations of single-modality and inefficiency with a two-stage architecture (MTS-Mamba and MGMI), achieving state-of-the-art accuracy and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Existing multi-task learning methods for assistive driving are limited by single-modality constraints and inefficient architectures, hindering comprehensive scene understanding and real-time deployment.

Method: Proposes TEM^3-Learning with two components: MTS-Mamba for efficient temporal-spatial feature extraction and MGMI for adaptive multimodal feature integration.

Result: Achieves state-of-the-art accuracy on the AIDE dataset, with <6M parameters and 142.32 FPS inference speed.

Conclusion: TEM^3-Learning effectively addresses MTL limitations, offering high accuracy and efficiency for assistive driving tasks.

Abstract: Multi-task learning (MTL) can advance assistive driving by exploring
inter-task correlations through shared representations. However, existing
methods face two critical limitations: single-modality constraints limiting
comprehensive scene understanding and inefficient architectures impeding
real-time deployment. This paper proposes TEM^3-Learning (Time-Efficient
Multimodal Multi-task Learning), a novel framework that jointly optimizes
driver emotion recognition, driver behavior recognition, traffic context
recognition, and vehicle behavior recognition through a two-stage architecture.
The first component, the mamba-based multi-view temporal-spatial feature
extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal
scanning mechanism and global-local spatial attention to efficiently extract
low-cost temporal-spatial features from multi-view sequential images. The
second component, the MTL-based gated multimodal feature integrator (MGMI),
employs task-specific multi-gating modules to adaptively highlight the most
relevant modality features for each task, effectively alleviating the negative
transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model
achieves state-of-the-art accuracy across all four tasks, maintaining a
lightweight architecture with fewer than 6 million parameters and delivering an
impressive 142.32 FPS inference speed. Rigorous ablation studies further
validate the effectiveness of the proposed framework and the independent
contributions of each module. The code is available on
https://github.com/Wenzhuo-Liu/TEM3-Learning.

</details>


### [235] [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation](https://arxiv.org/pdf/2506.18095)
*Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, Benyou Wang*

Main category: cs.CV

TL;DR: The paper introduces ShareGPT-4o-Image, a dataset for democratizing advanced image generation, and Janus-4o, a model improving text-to-image and introducing text-and-image-to-image generation.


<details>
  <summary>Details</summary>
Motivation: To make photorealistic, instruction-aligned image generation accessible, as current systems like GPT-4o-Image are proprietary.

Method: Developed ShareGPT-4o-Image dataset (45K text-to-image and 46K text-and-image-to-image samples) and trained Janus-4o model using this data.

Result: Janus-4o outperforms Janus-Pro in text-to-image and supports text-and-image-to-image generation efficiently (91K samples, 6 hours training).

Conclusion: The release aims to advance open research in photorealistic, instruction-aligned image generation.

Abstract: Recent advances in multimodal generative models have unlocked photorealistic,
instruction-aligned image generation, yet leading systems like GPT-4o-Image
remain proprietary and inaccessible. To democratize these capabilities, we
present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and
46K text-and-image-to-image data, all synthesized using GPT-4o's image
generation capabilities for distilling its advanced image generation abilities.
Leveraging this dataset, we develop Janus-4o, a multimodal large language model
capable of both text-to-image and text-and-image-to-image generation. Janus-4o
not only significantly improves text-to-image generation over its predecessor,
Janus-Pro, but also newly supports text-and-image-to-image generation. Notably,
it achieves impressive performance in text-and-image-to-image generation from
scratch, using only 91K synthetic samples and 6 hours of training on an 8
A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will
foster open research in photorealistic, instruction-aligned image generation.

</details>


### [236] [Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing](https://arxiv.org/pdf/2506.18104)
*Idan Simai, Ronen Talmon, Uri Shaham*

Main category: cs.CV

TL;DR: The paper identifies a potential sub-optimality in VICReg, a self-supervised learning method, due to overreliance on training data. It introduces SAG-VICReg, an improved version with better generalization and global semantic capture, outperforming state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: To address VICReg's potential struggle with generalization to unseen data and improve its robustness in producing meaningful representations outside the training set.

Method: Introduces SAG-VICReg, an enhanced version of VICReg incorporating new training techniques to improve global semantic understanding and generalization.

Result: SAG-VICReg outperforms state-of-the-art SSL baselines, excels in global semantic evaluation, and maintains competitive local metrics. A new label-free evaluation metric for embeddings is also proposed.

Conclusion: SAG-VICReg effectively addresses VICReg's generalization limitations and advances self-supervised learning by improving robustness and semantic understanding, supported by a novel evaluation metric.

Abstract: In this paper, we argue that viewing VICReg-a popular self-supervised
learning (SSL) method--through the lens of spectral embedding reveals a
potential source of sub-optimality: it may struggle to generalize robustly to
unseen data due to overreliance on the training data. This observation invites
a closer look at how well this method achieves its goal of producing meaningful
representations of images outside of the training set as well. Here, we
investigate this issue and introduce SAG-VICReg (Stable and Generalizable
VICReg), a method that builds on VICReg by incorporating new training
techniques. These enhancements improve the model's ability to capture global
semantics within the data and strengthen the generalization capabilities.
Experiments demonstrate that SAG-VICReg effectively addresses the
generalization challenge while matching or surpassing diverse state-of-the-art
SSL baselines. Notably, our method exhibits superior performance on metrics
designed to evaluate global semantic understanding, while simultaneously
maintaining competitive results on local evaluation metrics. Furthermore, we
propose a new standalone evaluation metric for embeddings that complements the
standard evaluation methods and accounts for the global data structure without
requiring labels--a key issue when tagged data is scarce or not available.

</details>


### [237] [Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection](https://arxiv.org/pdf/2506.18134)
*Quan Zhou, Gan Luo, Qiang Hu, Qingyong Zhang, Jinhua Zhang, Yinjiao Tian, Qiang Li, Zhiwei Wang*

Main category: cs.CV

TL;DR: The paper proposes an adversarial diffusion framework to synthesize high-value false positives for polyp detection, improving detector performance by at least 2.6% in F1-score.


<details>
  <summary>Details</summary>
Motivation: Existing models for polyp detection lack diversity in data, especially for false positives, limiting their reliability in clinical applications.

Method: The authors introduce a regional noise matching strategy and a Detector-guided Adversarial Diffusion Attacker (DADA) module to synthesize diverse, detector-confusing false positives.

Result: The method outperforms state-of-the-art approaches, enhancing detector performance by 2.6-2.7% in F1-score on public and in-house datasets.

Conclusion: The adversarial diffusion framework sets a new paradigm for targeted false positive synthesis, advancing reliable colorectal cancer screening.

Abstract: Polyp detection is crucial for colorectal cancer screening, yet existing
models are limited by the scale and diversity of available data. While
generative models show promise for data augmentation, current methods mainly
focus on enhancing polyp diversity, often overlooking the critical issue of
false positives. In this paper, we address this gap by proposing an adversarial
diffusion framework to synthesize high-value false positives. The extensive
variability of negative backgrounds presents a significant challenge in false
positive synthesis. To overcome this, we introduce two key innovations: First,
we design a regional noise matching strategy to construct a negative synthesis
space using polyp detection datasets. This strategy trains a negative-centric
diffusion model by masking polyp regions, ensuring the model focuses
exclusively on learning diverse background patterns. Second, we introduce the
Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs
the negative synthesis process to disrupt a pre-trained detector's decision,
guiding the negative-centric diffusion model to generate high-value,
detector-confusing false positives instead of low-value, ordinary backgrounds.
Our approach is the first to apply adversarial diffusion to lesion detection,
establishing a new paradigm for targeted false positive synthesis and paving
the way for more reliable clinical applications in colorectal cancer screening.
Extensive results on public and in-house datasets verify the superiority of our
method over the current state-of-the-arts, with our synthesized data improving
the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the
baselines. Codes are at https://github.com/Huster-Hq/DADA.

</details>


### [238] [See-in-Pairs: Reference Image-Guided Comparative Vision-Language Models for Medical Diagnosis](https://arxiv.org/pdf/2506.18140)
*Ruinan Jin, Gexin Huang, Xinwei Shen, Qiong Zhang, Yan Shuo Tan, Xiaoxiao Li*

Main category: cs.CV

TL;DR: The paper explores enhancing medical vision-language models (VLMs) by incorporating comparative reasoning using reference images, improving diagnostic accuracy over single-image methods.


<details>
  <summary>Details</summary>
Motivation: Existing medical VLMs lack comparative reasoning, while general-purpose VLMs lack medical-domain knowledge, creating a gap in nuanced clinical diagnosis.

Method: Leveraging reference images and clinically-informed prompts, the study finetunes general-purpose VLMs for comparative analysis in medical VQA tasks.

Result: Comparative reasoning with reference images and supervised finetuning significantly improves diagnostic outcomes.

Conclusion: The work bridges the gap between general-purpose and medical VLMs, demonstrating the clinical value of comparative image analysis.

Abstract: Medical imaging diagnosis presents inherent challenges due to diseases that
mimic normal anatomy and exhibit significant inter-patient variability.
Clinicians routinely employ comparative reasoning-using reference images from
healthy controls or previous patient examinations-to discern subtle yet
diagnostically critical abnormalities. However, existing medical
vision-language models (VLMs) focus primarily on single-image or single-series
analyses and lack explicit mechanisms for comparative reasoning. Conversely,
general-purpose VLMs demonstrate strong multi-image comparative reasoning
capabilities but lack essential medical-domain knowledge to identify nuanced
clinical differences. This work aims to bridge this gap by exploring
clinically-inspired comparative analysis within VLMs, leveraging reference
images to enhance diagnostic accuracy. Through extensive empirical analysis, we
show that providing general-purpose VLMs with query and normative matched
reference images, accompanied by clinically-informed comparative prompts,
significantly improves diagnostic outcomes compared to single-image baselines,
especially after supervised finetuning (SFT). Our contributions highlight the
clinical relevance of comparative analysis introduce novel strategies for
leveraging reference images in VLMs, empirically demonstrate enhanced
performance across multiple medical visual question answering (VQA) tasks, and
provide theoretical insights into the efficacy of comparative image analysis in
medical diagnosis.

</details>


### [239] [Pattern-Based Phase-Separation of Tracer and Dispersed Phase Particles in Two-Phase Defocusing Particle Tracking Velocimetry](https://arxiv.org/pdf/2506.18157)
*Christian Sax, Jochen Kriegseis*

Main category: cs.CV

TL;DR: A post-processing method using CNNs for phase separation in defocusing particle tracking velocimetry is proposed, achieving high accuracy (95-100%) in detecting and classifying particles and dispersed phases.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for phase separation in dispersed two-phase flows are often impractical, prompting the need for a robust, single-camera-based approach.

Method: Uses convolutional neural networks (Faster R-CNN and YOLOv4) trained on auto-labeled datasets generated by a GAN framework to detect and classify particles based on defocused image patterns.

Result: High detection precision and classification accuracy (95-100%) across synthetic and real datasets, even under domain shifts.

Conclusion: CNNs are viable for robust phase separation in dispersed two-phase DPTV, outperforming traditional methods in challenging scenarios.

Abstract: This work investigates the feasibility of a post-processing-based approach
for phase separation in defocusing particle tracking velocimetry for dispersed
two-phase flows. The method enables the simultaneous 3D localization
determination of both tracer particles and particles of the dispersed phase,
using a single-camera setup. The distinction between phases is based on pattern
differences in defocused particle images, which arise from distinct light
scattering behaviors of tracer particles and bubbles or droplets. Convolutional
neural networks, including Faster R-CNN and YOLOv4 variants, are trained to
detect and classify particle images based on these pattern features. To
generate large, labeled training datasets, a generative adversarial network
based framework is introduced, allowing the generation of auto-labeled data
that more closely reflects experiment-specific visual appearance. Evaluation
across six datasets, comprising synthetic two-phase and real single- and
two-phase flows, demonstrates high detection precision and classification
accuracy (95-100%), even under domain shifts. The results confirm the viability
of using CNNs for robust phase separation in disperse two-phase DPTV,
particularly in scenarios where traditional wavelength-, size-, or ensemble
correlation-based methods are impractical.

</details>


### [240] [CDG-MAE: Learning Correspondences from Diffusion Generated Views](https://arxiv.org/pdf/2506.18164)
*Varun Belagali, Pierre Marza, Srikar Yellapragada, Zilinghan Li, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Stergios Christodoulidis, Maria Vakalopoulou, Dimitris Samaras*

Main category: cs.CV

TL;DR: CDG-MAE is a self-supervised method using synthetic views from static images for dense correspondence learning, outperforming image-based MAE methods and narrowing the gap to video-based approaches.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for dense correspondences is tedious and unscalable, and existing self-supervised methods struggle with limited training data diversity.

Method: CDG-MAE generates diverse synthetic views from static images using an image-conditioned diffusion model and employs a multi-anchor MAE strategy.

Result: CDG-MAE outperforms state-of-the-art MAE methods relying on images and significantly reduces the performance gap to video-based approaches.

Conclusion: Synthetic views from static images provide a rich training signal, making CDG-MAE a scalable and effective solution for dense correspondence learning.

Abstract: Learning dense correspondences, critical for application such as video label
propagation, is hindered by tedious and unscalable manual annotation.
Self-supervised methods address this by using a cross-view pretext task, often
modeled with a masked autoencoder, where a masked target view is reconstructed
from an anchor view. However, acquiring effective training data remains a
challenge - collecting diverse video datasets is difficult and costly, while
simple image crops lack necessary pose variations. This paper introduces
CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic
views generated from static images via an image-conditioned diffusion model.
These generated views exhibit substantial changes in pose and perspective,
providing a rich training signal that overcomes the limitations of video and
crop-based anchors. We present a quantitative method to evaluate local and
global consistency of generated images, discussing their use for cross-view
self-supervised pretraining. Furthermore, we enhance the standard single-anchor
MAE setting to a multi-anchor strategy to effectively modulate the difficulty
of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods
reliant only on images and substantially narrows the performance gap to
video-based approaches.

</details>


### [241] [STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification](https://arxiv.org/pdf/2506.18172)
*Irsyad Adam, Tengyue Zhang, Shrayes Raman, Zhuyu Qiu, Brandon Taraku, Hexiang Feng, Sile Wang, Ashwath Radhachandran, Shreeram Athreya, Vedrana Ivezic, Peipei Ping, Corey Arnold, William Speier*

Main category: cs.CV

TL;DR: The paper introduces STACT-Time, a deep learning model for thyroid cancer risk stratification using US cine clips, reducing unnecessary biopsies while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods like FNA biopsies and TI-RADS have limitations, including unnecessary benign biopsies and interobserver variability. Deep learning models often miss dynamic context in US cine clips.

Method: Proposes STACT-Time, a framework combining self-attention and cross-attention mechanisms to leverage temporal and spatial context from US cine clips and segmentation masks.

Result: Achieves precision of 0.91 (0.02) and F1 score of 0.89 (0.02), outperforming state-of-the-art models.

Conclusion: STACT-Time improves malignancy prediction, reduces unnecessary biopsies, and enhances clinical decision-making.

Abstract: Thyroid cancer is among the most common cancers in the United States. Thyroid
nodules are frequently detected through ultrasound (US) imaging, and some
require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its
effectiveness, FNA often leads to unnecessary biopsies of benign nodules,
causing patient discomfort and anxiety. To address this, the American College
of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been
developed to reduce benign biopsies. However, such systems are limited by
interobserver variability. Recent deep learning approaches have sought to
improve risk stratification, but they often fail to utilize the rich temporal
and spatial context provided by US cine clips, which contain dynamic global
information and surrounding structural changes across various views. In this
work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid
Ultrasound Time Series Classification (STACT-Time) model, a novel
representation learning framework that integrates imaging features from US cine
clips with features from segmentation masks automatically generated by a
pretrained model. By leveraging self-attention and cross-attention mechanisms,
our model captures the rich temporal and spatial context of US cine clips while
enhancing feature representation through segmentation-guided learning. Our
model improves malignancy prediction compared to state-of-the-art models,
achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1
score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign
nodules while maintaining high sensitivity for malignancy detection, our model
has the potential to enhance clinical decision-making and improve patient
outcomes.

</details>


### [242] [DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data](https://arxiv.org/pdf/2506.18173)
*Sabbir Ahmed, Md. Bakhtiar Hasan, Tasnim Ahmed, Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: The paper proposes DExNet, a few-shot learning framework for plant disease classification, addressing data scarcity by leveraging domain-adapted pre-trained CNNs and a Bi-LSTM classifier, achieving high accuracy with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Deep learning models require large datasets for plant disease classification, but limited data availability hinders performance. This work aims to overcome this challenge using few-shot learning.

Method: DExNet extracts features from pre-trained CNNs (critics), adapts them to a leaf disease dataset, fuses features, and classifies using Bi-LSTM. Evaluated on tomato leaf images from PlantVillage.

Result: Achieves 89.06% (5-shot), 92.46% (10-shot), 94.07% (15-shot), and 98.09% (80-shot) accuracy, reducing training data needs by 94.5%. Outperforms existing methods in limited-data scenarios.

Conclusion: DExNet effectively addresses data scarcity in plant disease classification, achieving near state-of-the-art performance with significantly less training data.

Abstract: While deep learning-based architectures have been widely used for correctly
detecting and classifying plant diseases, they require large-scale datasets to
learn generalized features and achieve state-of-the-art performance. This poses
a challenge for such models to obtain satisfactory performance in classifying
leaf diseases with limited samples. This work proposes a few-shot learning
framework, Domain-adapted Expert Network (DExNet), for plant disease
classification that compensates for the lack of sufficient training data by
combining observations of a number of expert critics. It starts with extracting
the feature embeddings as 'observations' from nine 'critics' that are
state-of-the-art pre-trained CNN-based architectures. These critics are 'domain
adapted' using a publicly available leaf disease dataset having no overlapping
classes with the specific downstream task of interest. The observations are
then passed to the 'Feature Fusion Block' and finally to a classifier network
consisting of Bi-LSTM layers. The proposed pipeline is evaluated on the 10
classes of tomato leaf images from the PlantVillage dataset, achieving
promising accuracies of 89.06%, 92.46%, and 94.07%, respectively, for 5-shot,
10-shot, and 15-shot classification. Furthermore, an accuracy of 98.09+-0.7%
has been achieved in 80-shot classification, which is only 1.2% less than
state-of-the-art, allowing a 94.5% reduction in the training data requirement.
The proposed pipeline also outperforms existing works on leaf disease
classification with limited data in both laboratory and real-life conditions in
single-domain, mixed-domain, and cross-domain scenarios.

</details>


### [243] [Multimodal Fusion SLAM with Fourier Attention](https://arxiv.org/pdf/2506.18204)
*Youjie Zhou, Guofeng Mei, Yiming Wang, Yi Wan, Fabio Poiesi*

Main category: cs.CV

TL;DR: FMF-SLAM is an efficient multimodal fusion SLAM method using FFT and novel attention mechanisms to improve performance in noisy, varying lighting, and dark conditions.


<details>
  <summary>Details</summary>
Motivation: Challenges in visual SLAM due to noise, lighting variations, and darkness, along with high computational demands of traditional methods.

Method: Uses FFT, Fourier-based self/cross-attention for RGB/depth feature extraction, and multi-scale knowledge distillation for multimodal fusion.

Result: Validated on TUM, TartanAir, and real-world datasets, showing state-of-the-art performance in challenging conditions.

Conclusion: FMF-SLAM is practical for real-world applications, demonstrated by integration with a security robot and real-time performance.

Abstract: Visual SLAM is particularly challenging in environments affected by noise,
varying lighting conditions, and darkness. Learning-based optical flow
algorithms can leverage multiple modalities to address these challenges, but
traditional optical flow-based visual SLAM approaches often require significant
computational resources.To overcome this limitation, we propose FMF-SLAM, an
efficient multimodal fusion SLAM method that utilizes fast Fourier transform
(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel
Fourier-based self-attention and cross-attention mechanism to extract features
from RGB and depth signals. We further enhance the interaction of multimodal
features by incorporating multi-scale knowledge distillation across modalities.
We also demonstrate the practical feasibility of FMF-SLAM in real-world
scenarios with real time performance by integrating it with a security robot by
fusing with a global positioning module GNSS-RTK and global Bundle Adjustment.
Our approach is validated using video sequences from TUM, TartanAir, and our
real-world datasets, showcasing state-of-the-art performance under noisy,
varying lighting, and dark conditions.Our code and datasets are available at
https://github.com/youjie-zhou/FMF-SLAM.git.

</details>


### [244] [Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction](https://arxiv.org/pdf/2506.18208)
*Ankit Sanjyal*

Main category: cs.CV

TL;DR: DINO-enhanced NeRF models underperform baseline NeRF in few-shot 3D reconstruction, suggesting pre-trained features may introduce biases.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of integrating pre-trained DINO features into NeRF for few-shot 3D reconstruction.

Method: Systematic comparison of baseline NeRF, frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.

Result: All DINO variants performed worse (PSNR ~12.9-13.0) than baseline NeRF (PSNR 14.71).

Conclusion: Pre-trained vision features may not aid few-shot reconstruction; simpler geometric approaches could be more effective.

Abstract: Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction
from sparse image collections. Recent work has explored integrating pre-trained
vision features, particularly from DINO, to enhance few-shot reconstruction
capabilities. However, the effectiveness of such approaches remains unclear,
especially in extreme few-shot scenarios. In this paper, we present a
systematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF,
frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.
Surprisingly, our experiments reveal that all DINO variants perform worse than
the baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the
baseline's 14.71. This counterintuitive result suggests that pre-trained vision
features may not be beneficial for few-shot 3D reconstruction and may even
introduce harmful biases. We analyze potential causes including feature-task
mismatch, overfitting to limited data, and integration challenges. Our findings
challenge common assumptions in the field and suggest that simpler
architectures focusing on geometric consistency may be more effective for
few-shot scenarios.

</details>


### [245] [Deep Learning-based Alignment Measurement in Knee Radiographs](https://arxiv.org/pdf/2506.18209)
*Zhisen Hu, Dominic Cullen, Peter Thompson, David Johnson, Chang Bian, Aleksei Tiulpin, Timothy Cootes, Claudia Lindner*

Main category: cs.CV

TL;DR: A deep learning method using hourglass networks and attention gates automates knee alignment measurements in radiographs, achieving high accuracy (~1 difference) and reliability (ICC = 0.97 pre-op, 0.86 post-op).


<details>
  <summary>Details</summary>
Motivation: Traditional manual KA measurements are time-consuming and require long-leg radiographs. Automating this process can improve efficiency and accuracy in clinical workflows.

Method: The study employs hourglass networks with attention gates to localize over 100 knee landmarks and measure KA via the tibiofemoral angle in pre- and post-operative images.

Result: The method achieves ~1 mean absolute difference from clinical measurements, with excellent pre-op (ICC = 0.97) and good post-op (ICC = 0.86) agreement.

Conclusion: Automated KA assessment is highly accurate and reliable, offering potential for digital enhancement in clinical workflows.

Abstract: Radiographic knee alignment (KA) measurement is important for predicting
joint health and surgical outcomes after total knee replacement. Traditional
methods for KA measurements are manual, time-consuming and require long-leg
radiographs. This study proposes a deep learning-based method to measure KA in
anteroposterior knee radiographs via automatically localized knee anatomical
landmarks. Our method builds on hourglass networks and incorporates an
attention gate structure to enhance robustness and focus on key anatomical
features. To our knowledge, this is the first deep learning-based method to
localize over 100 knee anatomical landmarks to fully outline the knee shape
while integrating KA measurements on both pre-operative and post-operative
images. It provides highly accurate and reliable anatomical varus/valgus KA
measurements using the anatomical tibiofemoral angle, achieving mean absolute
differences ~1{\deg} when compared to clinical ground truth measurements.
Agreement between automated and clinical measurements was excellent
pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good
post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can
be automated with high accuracy, creating opportunities for digitally enhanced
clinical workflows.

</details>


### [246] [Shape from Polarization of Thermal Emission and Reflection](https://arxiv.org/pdf/2506.18217)
*Kazuma Kitazawa, Tsuyoshi Takatani*

Main category: cs.CV

TL;DR: The paper introduces a method for shape estimation of transparent objects using Long-Wave Infrared (LWIR) Shape from Polarization (SfP), addressing prior errors with a new polarization model and learning-based approaches.


<details>
  <summary>Details</summary>
Motivation: Transparent objects pose challenges for shape estimation due to complex light transport. Prior LWIR SfP methods had significant errors from inadequate polarimetric modeling, especially neglecting reflection.

Method: The authors developed a polarization model accounting for emission and reflection, used model-based and learning-based (neural network) approaches for surface normal estimation, and modeled LWIR polarimetric imaging to correct systematic errors. They also created a real-world dataset (ThermoPol).

Result: The method achieved high accuracy and broad applicability across various materials, including those transparent in visible light.

Conclusion: The proposed LWIR SfP method, with its improved modeling and learning-based approach, effectively addresses shape estimation for transparent objects and outperforms prior techniques.

Abstract: Shape estimation for transparent objects is challenging due to their complex
light transport. To circumvent these difficulties, we leverage the Shape from
Polarization (SfP) technique in the Long-Wave Infrared (LWIR) spectrum, where
most materials are opaque and emissive. While a few prior studies have explored
LWIR SfP, these attempts suffered from significant errors due to inadequate
polarimetric modeling, particularly the neglect of reflection. Addressing this
gap, we formulated a polarization model that explicitly accounts for the
combined effects of emission and reflection. Based on this model, we estimated
surface normals using not only a direct model-based method but also a
learning-based approach employing a neural network trained on a
physically-grounded synthetic dataset. Furthermore, we modeled the LWIR
polarimetric imaging process, accounting for inherent systematic errors to
ensure accurate polarimetry. We implemented a prototype system and created
ThermoPol, the first real-world benchmark dataset for LWIR SfP. Through
comprehensive experiments, we demonstrated the high accuracy and broad
applicability of our method across various materials, including those
transparent in the visible spectrum.

</details>


### [247] [Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano](https://arxiv.org/pdf/2506.18220)
*Berk Yilmaz, Aniruddh Aiyengar*

Main category: cs.CV

TL;DR: A lightweight, edge-device deployable retinal disease classifier is developed using cross-architecture knowledge distillation, achieving 89% accuracy while retaining 93% of the teacher model's performance.


<details>
  <summary>Details</summary>
Motivation: Early and accurate diagnosis of retinal ailments is critical, especially in low-resource settings where reliable diagnostic tools are scarce.

Method: A high-capacity ViT teacher model is trained using self-supervised learning and distilled into a CNN-based student model for edge deployment, using novel techniques like PCA and GL projectors.

Result: The student model achieves 89% classification accuracy, retaining 93% of the teacher's performance despite having 97.4% fewer parameters.

Conclusion: The method provides a scalable, AI-driven solution for retinal disease triage in under-resourced areas, balancing model compression and accuracy.

Abstract: Early and accurate identification of retinal ailments is crucial for averting
ocular decline; however, access to dependable diagnostic devices is not often
available in low-resourced settings. This project proposes to solve that by
developing a lightweight, edge-device deployable disease classifier using
cross-architecture knowledge distilling. We first train a high-capacity vision
transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised
learning, to classify fundus images into four classes: Normal, Diabetic
Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus
when compressing to a CNN-based student model for deployment in
resource-limited conditions, such as the NVIDIA Jetson Nano. This was
accomplished using a novel framework which included a Partitioned
Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a
multi-view robust training method. The teacher model has 97.4 percent more
parameters than the student model, with it achieving 89 percent classification
with a roughly 93 percent retention of the teacher model's diagnostic
performance. The retention of clinical classification behavior supports our
method's initial aim: compression of the ViT while retaining accuracy. Our work
serves as an example of a scalable, AI-driven triage solution for retinal
disorders in under-resourced areas.

</details>


### [248] [Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation](https://arxiv.org/pdf/2506.18226)
*Xunzhi Xiang, Qi Fan*

Main category: cs.CV

TL;DR: The paper introduces Adaptive Dynamic Sparse Attention (ADSA), a training-free method to optimize context in autoregressive image generation, reducing memory and computational overhead while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Address the memory and computational inefficiencies in autoregressive image generation models caused by long contexts during inference.

Method: Propose ADSA, which dynamically identifies crucial tokens for local texture and global semantics, and introduces a dynamic KV-cache update mechanism.

Result: Reduces GPU memory consumption by ~50% while maintaining generation quality, validated through extensive experiments.

Conclusion: ADSA effectively balances efficiency and quality in text-to-image synthesis, offering a practical solution for resource constraints.

Abstract: Autoregressive conditional image generation models have emerged as a dominant
paradigm in text-to-image synthesis. These methods typically convert images
into one-dimensional token sequences and leverage the self-attention mechanism,
which has achieved remarkable success in natural language processing, to
capture long-range dependencies, model global context, and ensure semantic
coherence. However, excessively long contexts during inference lead to
significant memory overhead caused by KV-cache and computational delays. To
alleviate these challenges, we systematically analyze how global semantics,
spatial layouts, and fine-grained textures are formed during inference, and
propose a novel training-free context optimization method called Adaptive
Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies
historical tokens crucial for maintaining local texture consistency and those
essential for ensuring global semantic coherence, thereby efficiently
streamlining attention computation. Additionally, we introduce a dynamic
KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption
during inference by approximately $50\%$. Extensive qualitative and
quantitative experiments demonstrate the effectiveness and superiority of our
approach in terms of both generation quality and resource efficiency.

</details>


### [249] [Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning](https://arxiv.org/pdf/2506.18234)
*Yue Li, Meng Tian, Dechang Zhu, Jiangtong Zhu, Zhenyu Lin, Zhiwei Xiong, Xinhai Zhao*

Main category: cs.CV

TL;DR: Drive-R1 bridges reasoning and motion planning in autonomous driving by combining supervised fine-tuning and reinforcement learning, outperforming existing VLMs.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in VLMs for autonomous driving: reliance on history inputs and misaligned reasoning with planning outcomes.

Method: Drive-R1 uses supervised fine-tuning on COT data and reinforcement learning to align reasoning with planning.

Result: Superior performance on nuScenes and DriveLM-nuScenes benchmarks.

Conclusion: Drive-R1 offers a promising approach for integrating reasoning and planning in autonomous driving.

Abstract: Large vision-language models (VLMs) for autonomous driving (AD) are evolving
beyond perception and cognition tasks toward motion planning. However, we
identify two critical challenges in this direction: (1) VLMs tend to learn
shortcuts by relying heavily on history input information, achieving seemingly
strong planning results without genuinely understanding the visual inputs; and
(2) the chain-ofthought (COT) reasoning processes are always misaligned with
the motion planning outcomes, and how to effectively leverage the complex
reasoning capability to enhance planning remains largely underexplored. In this
paper, we start from a small-scale domain-specific VLM and propose Drive-R1
designed to bridges the scenario reasoning and motion planning for AD. Drive-R1
first undergoes the supervised finetuning on a elaborate dataset containing
both long and short COT data. Drive-R1 is encouraged to reason step-by-step
from visual input to final planning decisions. Subsequently, Drive-R1 is
trained within a reinforcement learning framework that incentivizes the
discovery of reasoning paths that are more informative for planning, guided by
rewards based on predicted trajectories and meta actions. Experimental
evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that
Drive-R1 achieves superior performance compared to existing state-of-the-art
VLMs. We believe that Drive-R1 presents a promising direction for bridging
reasoning and planning in AD, offering methodological insights for future
research and applications.

</details>


### [250] [Referring Expression Instance Retrieval and A Strong End-to-End Baseline](https://arxiv.org/pdf/2506.18246)
*Xiangzhao Hao, Kuan Zhu, Hongyu Guo, Haiyun Guo, Ming Tang, JinQiao Wang*

Main category: cs.CV

TL;DR: The paper introduces a new task, Referring Expression Instance Retrieval (REIR), combining instance-level retrieval and localization, and proposes a benchmark (REIRCOCO) and baseline method (CLARE) for it.


<details>
  <summary>Details</summary>
Motivation: Existing tasks like TIR and REC lack precision or scalability for real-world scenarios requiring both instance-level retrieval and localization.

Method: Proposes CLARE, a dual-stream architecture with a Mix of Relation Experts (MORE) module, integrating object detection, REC pretraining, and Contrastive Language-Instance Alignment (CLIA).

Result: CLARE achieves state-of-the-art performance on REIR and generalizes well to TIR and REC.

Conclusion: The REIR task and CLARE method effectively address the gap between TIR and REC, demonstrating versatility and performance.

Abstract: Natural language querying of visual content underpins many vision-language
tasks, typically categorized by text granularity and visual search scope.
Text-Image Retrieval (TIR) retrieves whole images using coarse descriptions,
while Referring Expression Comprehension (REC) localizes objects using
fine-grained expressions within a single image. However, real-world scenarios
often require both instance-level retrieval and localization across large
galleries -- tasks where TIR lacks precision and REC lacks scalability. To
address this gap, we propose a new task: Referring Expression Instance
Retrieval (REIR), which jointly supports instance-level retrieval and
localization. We introduce REIRCOCO, a large-scale benchmark constructed by
prompting vision-language models to generate fine-grained expressions for
MSCOCO and RefCOCO instances. We also present a baseline method, CLARE,
featuring a dual-stream architecture with a Mix of Relation Experts (MORE)
module for capturing inter-instance relationships. CLARE integrates object
detection and REC pretraining with Contrastive Language-Instance Alignment
(CLIA) for end-to-end optimization. Experiments show that CLARE achieves
state-of-the-art performance on REIR and generalizes well to TIR and REC,
highlighting its effectiveness and versatility.

</details>


### [251] [Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability](https://arxiv.org/pdf/2506.18248)
*Jongoh Jeong, Hunmin Yang, Jaeseok Jeong, Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: The paper introduces a semantic structure-aware attack framework using Mean Teacher to enhance adversarial transferability by leveraging under-exploited semantic features in generative models.


<details>
  <summary>Details</summary>
Motivation: Existing generative adversarial attacks fail to fully utilize the semantic information in intermediate activations of generators, limiting perturbation alignment with object-salient regions.

Method: The proposed framework uses Mean Teacher as a temporally smoothed feature reference and employs feature distillation to ensure semantic consistency between student and teacher activations.

Result: The method improves adversarial transferability, outperforming state-of-the-art generative attacks across diverse models, domains, and tasks.

Conclusion: The semantic structure-aware framework effectively enhances adversarial transferability by better utilizing semantic features in generative models.

Abstract: Generative adversarial attacks train a perturbation generator on a white-box
surrogate model and subsequently apply the crafted perturbations to unseen
black-box victim models. In contrast to iterative attacks, these methods
deliver superior inference-time efficiency, scalability, and transferability;
however, up until now, existing studies have not fully exploited the
representational capacity of generative models to preserve and harness semantic
information. Specifically, the intermediate activations of the generator encode
rich semantic features--object boundaries and coarse shapes--that remain
under-exploited, thereby limiting the alignment of perturbations with
object-salient regions which are critical for adversarial transferability. To
remedy this, we introduce a semantic structure-aware attack framework based on
the Mean Teacher, which serves as a temporally smoothed feature reference. With
this smoothed reference, we further direct semantic consistency between the
early-layer activations in the student and those of the semantically rich
teacher by feature distillation. By anchoring perturbation synthesis to the
semantically salient early intermediate blocks within the generator based on
empirical findings, our method guides progressive adversarial perturbation on
regions that substantially enhance adversarial transferability. We conduct
extensive experiments over diverse models, domains and tasks to demonstrate
consistent improvements relative to state-of-the-art generative attacks,
comprehensively evaluated using conventional metrics and our newly proposed
Accidental Correction Rate (ACR).

</details>


### [252] [Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain](https://arxiv.org/pdf/2506.18261)
*Rui Su, Dong Xu, Luping Zhou, Wanli Ouyang*

Main category: cs.CV

TL;DR: A two-stage approach for weakly supervised temporal action localization using multi-resolution temporal information and iterative pseudo-label refinement.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of temporal action localization with only video-level annotations by leveraging multi-resolution temporal data.

Method: Proposes an Initial Label Generation (ILG) module for reliable pseudo labels and a Progressive Temporal Label Refinement (PTLR) framework with two networks (Network-OTS and Network-RTS) to refine labels iteratively.

Result: Improved temporal action localization performance by exchanging multi-resolution information at the pseudo-label level.

Conclusion: The method effectively exploits temporal multi-resolution consistency and iterative refinement to enhance localization accuracy.

Abstract: Weakly supervised temporal action localization is a challenging task as only
the video-level annotation is available during the training process. To address
this problem, we propose a two-stage approach to fully exploit multi-resolution
information in the temporal domain and generate high quality frame-level pseudo
labels based on both appearance and motion streams. Specifically, in the first
stage, we generate reliable initial frame-level pseudo labels, and in the
second stage, we iteratively refine the pseudo labels and use a set of selected
frames with highly confident pseudo labels to train neural networks and better
predict action class scores at each frame. We fully exploit temporal
information at multiple scales to improve temporal action localization
performance. Specifically, in order to obtain reliable initial frame-level
pseudo labels, in the first stage, we propose an Initial Label Generation (ILG)
module, which leverages temporal multi-resolution consistency to generate high
quality class activation sequences (CASs), which consist of a number of
sequences with each sequence measuring how likely each video frame belongs to
one specific action class. In the second stage, we propose a Progressive
Temporal Label Refinement (PTLR) framework. In our PTLR framework, two networks
called Network-OTS and Network-RTS, which are respectively used to generate
CASs for the original temporal scale and the reduced temporal scales, are used
as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo
labels in turn. By this way, the multi-resolution information in the temporal
domain is exchanged at the pseudo label level, and our work can help improve
each stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels
from another stream (i.e., the RTS/OTS stream).

</details>


### [253] [YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos](https://arxiv.org/pdf/2506.18266)
*Haoming Chen, Lichen Yuan, TianFang Sun, Jingyu Gong, Xin Tan, Zhizhong Zhang, Yuan Xie*

Main category: cs.CV

TL;DR: The paper proposes a self-supervised method for 3D semantic occupancy prediction using only indoor Internet data, eliminating the need for precise geometric relationships or camera parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional methods require precise geometric data and annotations, which are impractical for large-scale indoor environments due to complexity and privacy concerns.

Method: A web dataset (YouTube-Occ) is collected from house tour videos. A self-supervised model leverages 2D prior knowledge, using vision foundation models to distill region-level knowledge into an occupancy network via superpixels.

Result: The method achieves state-of-the-art zero-shot performance on NYUv2 and OccScanNet benchmarks.

Conclusion: The approach demonstrates that accurate 3D training can be achieved without pre-knowledge of camera parameters, using readily available Internet data.

Abstract: 3D semantic occupancy prediction in the past was considered to require
precise geometric relationships in order to enable effective training. However,
in complex indoor environments, the large-scale and widespread collection of
data, along with the necessity for fine-grained annotations, becomes
impractical due to the complexity of data acquisition setups and privacy
concerns. In this paper, we demonstrate that 3D spatially-accurate training can
be achieved using only indoor Internet data, without the need for any
pre-knowledge of intrinsic or extrinsic camera parameters. In our framework, we
collect a web dataset, YouTube-Occ, which comprises house tour videos from
YouTube, providing abundant real house scenes for 3D representation learning.
Upon on this web dataset, we establish a fully self-supervised model to
leverage accessible 2D prior knowledge for reaching powerful 3D indoor
perception. Specifically, we harness the advantages of the prosperous vision
foundation models, distilling the 2D region-level knowledge into the occupancy
network by grouping the similar pixels into superpixels. Experimental results
show that our method achieves state-of-the-art zero-shot performance on two
popular benchmarks (NYUv2 and OccScanNet

</details>


### [254] [ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments](https://arxiv.org/pdf/2506.18268)
*Yu Liu, Yangtao Meng, Xianfei Pan, Jie Jiang, Changhao Chen*

Main category: cs.CV

TL;DR: ThermalLoc is a deep learning method for thermal image relocalization, outperforming existing methods by combining EfficientNet and Transformers.


<details>
  <summary>Details</summary>
Motivation: Traditional relocalization methods for visible light images don't work for thermal images, creating a need for specialized approaches.

Method: ThermalLoc integrates EfficientNet with Transformers to extract features and uses MLP networks for absolute pose regression.

Result: ThermalLoc achieves superior accuracy and robustness compared to AtLoc, MapNet, PoseNet, and RobustLoc.

Conclusion: ThermalLoc effectively addresses the gap in thermal image relocalization, offering a high-performing solution.

Abstract: Thermal cameras capture environmental data through heat emission, a
fundamentally different mechanism compared to visible light cameras, which rely
on pinhole imaging. As a result, traditional visual relocalization methods
designed for visible light images are not directly applicable to thermal
images. Despite significant advancements in deep learning for camera
relocalization, approaches specifically tailored for thermal camera-based
relocalization remain underexplored. To address this gap, we introduce
ThermalLoc, a novel end-to-end deep learning method for thermal image
relocalization. ThermalLoc effectively extracts both local and global features
from thermal images by integrating EfficientNet with Transformers, and performs
absolute pose regression using two MLP networks. We evaluated ThermalLoc on
both the publicly available thermal-odometry dataset and our own dataset. The
results demonstrate that ThermalLoc outperforms existing representative methods
employed for thermal camera relocalization, including AtLoc, MapNet, PoseNet,
and RobustLoc, achieving superior accuracy and robustness.

</details>


### [255] [Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction](https://arxiv.org/pdf/2506.18270)
*Qinrong Cai, Yu Guan, Zhibo Chen, Dong Liang, Qiuyun Fan, Qiegen Liu*

Main category: cs.CV

TL;DR: The paper introduces AMDM, a diffusion model with adaptive masks for MRI reconstruction, improving quality by focusing on frequency-specific regions in k-space.


<details>
  <summary>Details</summary>
Motivation: Traditional MRI reconstruction methods overlook the importance of different frequency regions in k-space, limiting performance.

Method: AMDM uses adaptive masks to separate high and low-frequency components in k-space, guiding a closed-loop diffusion process.

Result: The method effectively learns frequency-specific information, enhancing MRI reconstruction quality.

Conclusion: AMDM provides a flexible framework for optimizing k-space data with adaptive masks, promising future advancements.

Abstract: As the deep learning revolution marches on, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training, and has demonstrated exceptional
performance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction
is a critical task in medical imaging that seeks to recover high-quality images
from under-sampled k-space data. However, previous MRI reconstruction
strategies usually optimized the entire image domain or k-space, without
considering the importance of different frequency regions in the k-space This
work introduces a diffusion model based on adaptive masks (AMDM), which
utilizes the adaptive adjustment of frequency distribution based on k-space
data to develop a hybrid masks mechanism that adapts to different k-space
inputs. This enables the effective separation of high-frequency and
low-frequency components, producing diverse frequency-specific representations.
Additionally, the k-space frequency distribution informs the generation of
adaptive masks, which, in turn, guide a closed-loop diffusion process.
Experimental results verified the ability of this method to learn specific
frequency information and thereby improved the quality of MRI reconstruction,
providing a flexible framework for optimizing k-space data using masks in the
future.

</details>


### [256] [ReFrame: Rectification Framework for Image Explaining Architectures](https://arxiv.org/pdf/2506.18272)
*Debjyoti Das Adhikary, Aritra Hazra, Partha Pratim Chakrabarti*

Main category: cs.CV

TL;DR: A novel framework improves image explanation by addressing object hallucination and incompleteness, enhancing existing methods like Image Captioning, VQA, and Prompt-based AI.


<details>
  <summary>Details</summary>
Motivation: Existing image explanation methods often hallucinate objects or miss some, leading to inconsistent and incomplete results.

Method: Proposes an interpretable framework to rectify incorrect or missing objects in explanations, applicable to Image Captioning, VQA, and Prompt-based AI.

Result: Quantitative improvements: Image Captioning (81.81% completeness, 37.10% inconsistency), VQA (9.6% completeness, 37.10% inconsistency), Prompt-based AI (0.01% completeness, 5.2% inconsistency).

Conclusion: The framework significantly enhances explanation accuracy and consistency across multiple image explanation tasks.

Abstract: Image explanation has been one of the key research interests in the Deep
Learning field. Throughout the years, several approaches have been adopted to
explain an input image fed by the user. From detecting an object in a given
image to explaining it in human understandable sentence, to having a
conversation describing the image, this problem has seen an immense change
throughout the years, However, the existing works have been often found to (a)
hallucinate objects that do not exist in the image and/or (b) lack identifying
the complete set of objects present in the image. In this paper, we propose a
novel approach to mitigate these drawbacks of inconsistency and incompleteness
of the objects recognized during the image explanation. To enable this, we
propose an interpretable framework that can be plugged atop diverse image
explaining frameworks including Image Captioning, Visual Question Answering
(VQA) and Prompt-based AI using LLMs, thereby enhancing their explanation
capabilities by rectifying the incorrect or missing objects. We further measure
the efficacy of the rectified explanations generated through our proposed
approaches leveraging object based precision metrics, and showcase the
improvements in the inconsistency and completeness of image explanations.
Quantitatively, the proposed framework is able to improve the explanations over
the baseline architectures of Image Captioning (improving the completeness by
81.81% and inconsistency by 37.10%), Visual Question Answering(average of 9.6%
and 37.10% in completeness and inconsistency respectively) and Prompt-based AI
model (0.01% and 5.2% for completeness and inconsistency respectively)
surpassing the current state-of-the-art by a substantial margin.

</details>


### [257] [Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset](https://arxiv.org/pdf/2506.18284)
*Kasra Moazzami, Seoyoun Son, John Lin, Sun Min Lee, Daniel Son, Hayeon Lee, Jeongho Lee, Seongji Lee*

Main category: cs.CV

TL;DR: The paper explores Open Set Recognition (OSR) techniques for endoscopic image classification on the Kvasir dataset, comparing deep learning models like ResNet-50 and Swin Transformer under open-set conditions.


<details>
  <summary>Details</summary>
Motivation: Conventional closed-set classification is limited in open-world clinical settings where unseen conditions can arise, compromising model reliability.

Method: Evaluates OSR capabilities of ResNet-50, Swin Transformer, and a hybrid ResNet-Transformer model using OpenMax as a baseline.

Result: Provides foundational benchmarks for OSR in medical image analysis, highlighting model behavior in realistic clinical settings.

Conclusion: OSR techniques are crucial for safe AI deployment in endoscopy, as demonstrated by the study's insights.

Abstract: Endoscopic image classification plays a pivotal role in medical diagnostics
by identifying anatomical landmarks and pathological findings. However,
conventional closed-set classification frameworks are inherently limited in
open-world clinical settings, where previously unseen conditions can arise
andcompromise model reliability. To address this, we explore the application of
Open Set Recognition (OSR) techniques on the Kvasir dataset, a publicly
available and diverse endoscopic image collection. In this study, we evaluate
and compare the OSR capabilities of several representative deep learning
architectures, including ResNet-50, Swin Transformer, and a hybrid
ResNet-Transformer model, under both closed-set and open-set conditions.
OpenMax is adopted as a baseline OSR method to assess the ability of these
models to distinguish known classes from previously unseen categories. This
work represents one of the first efforts to apply open set recognition to the
Kvasir dataset and provides a foundational benchmark for evaluating OSR
performance in medical image analysis. Our results offer practical insights
into model behavior in clinically realistic settings and highlight the
importance of OSR techniques for the safe deployment of AI systems in
endoscopy.

</details>


### [258] [Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction](https://arxiv.org/pdf/2506.18291)
*Yota Urano, Hiromu Taketsugu, Norimichi Ukita*

Main category: cs.CV

TL;DR: Proposes an architecture with an Importance Estimator to select key neighbors for predicting a person's trajectory, using Gumbel Softmax for training, achieving faster processing with competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve trajectory prediction by effectively selecting important neighboring people.

Method: Uses an Importance Estimator module and Gumbel Softmax for differentiable sampling of neighbors.

Result: Faster processing with competitive accuracy on the JRDB dataset.

Conclusion: The method efficiently selects neighbors and maintains prediction accuracy.

Abstract: This paper presents an architecture for selecting important neighboring
people to predict the primary person's trajectory. To achieve effective
neighboring people selection, we propose a people selection module called the
Importance Estimator which outputs the importance of each neighboring person
for predicting the primary person's future trajectory. To prevent gradients
from being blocked by non-differentiable operations when sampling surrounding
people based on their importance, we employ the Gumbel Softmax for training.
Experiments conducted on the JRDB dataset show that our method speeds up the
process with competitive prediction accuracy.

</details>


### [259] [Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture](https://arxiv.org/pdf/2506.18292)
*Ziyue Guo, Xin Yang, Yutao Shen, Yang Zhu, Lixi Jiang, Haiyan Cen*

Main category: cs.CV

TL;DR: The paper proposes RP-PCN, a point cloud completion model for 3D reconstruction of rapeseed populations, improving canopy architecture analysis and yield prediction.


<details>
  <summary>Details</summary>
Motivation: Accurate canopy architecture descriptions are hindered by occlusion and complex structures, limiting crop photosynthesis and yield evaluation.

Method: A virtual-real integration simulation and occlusion detection algorithm annotate training data. RP-PCN uses MRDG and PPD modules for point cloud completion, validated via yield prediction.

Result: RP-PCN achieved low CD values (3.35-4.51 cm) across growth stages and improved yield prediction accuracy by 11.2%.

Conclusion: RP-PCN enhances canopy analysis and can be extended to other crops, aiding field environment studies.

Abstract: Quantitative descriptions of complete canopy architecture are crucial for
evaluating crop photosynthesis and yield to guide ideotype design. Although
three-dimensional (3D) sensing technologies have been developed for plant and
canopy reconstruction, severe occlusion and complex architectures hinder
accurate canopy descriptions. In this study, we propose a point cloud
completion model for 3D reconstruction of rapeseed populations from seeding to
silique stages using multi-view imaging. A complete point cloud generation
framework was developed with the virtual-real integration (VRI) simulation
method and occlusion point detection algorithm to annotate the training dataset
by distinguishing surface from occluded points. The rapeseed population point
cloud completion network (RP-PCN) was designed with a multi-resolution dynamic
graph convolutional encoder (MRDG) and point pyramid decoder (PPD) to predict
occluded points based on input surface point clouds. A dynamic graph
convolutional feature extractor (DGCFE) was introduced to capture structural
variations across the growth period. The effectiveness of point cloud
completion was validated by predicting yield using architectural indicators
from complete point clouds of rapeseed population. The results demonstrated
that RP-PCN achieved chamfer distance (CD) values of 3.35 cm, 3.46 cm, 4.32 cm,
and 4.51 cm at the seedling, bolting, flowering, and silique stages,
respectively. Ablation studies showed the effectiveness of the MRDG and DGCFE
modules, reducing CD values by 10% and 23%, respectively. The silique
efficiency index (SEI) from RP-PCN improved yield prediction accuracy by 11.2%
compared to incomplete point clouds. The RP-PCN pipeline proposed in this study
has the potential to be extended to other crops, significantly enhancing the
analysis of population canopy architectures in field environments.

</details>


### [260] [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/pdf/2506.18871)
*Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, Zheng Liu*

Main category: cs.CV

TL;DR: OmniGen2 is a versatile open-source generative model for diverse tasks like text-to-image, image editing, and in-context generation, featuring dual decoding pathways and achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: To provide a unified solution for diverse generation tasks while preserving text generation capabilities and improving upon OmniGen v1.

Method: Uses two distinct decoding pathways for text and image modalities with unshared parameters, a decoupled image tokenizer, and introduces a reflection mechanism for image tasks. Comprehensive data pipelines were developed for training.

Result: Competitive performance on benchmarks like text-to-image and image editing, and state-of-the-art in-context generation on the new OmniContext benchmark.

Conclusion: OmniGen2 is a promising model for multimodal generation tasks, with open-source resources to support future research.

Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative
model designed to provide a unified solution for diverse generation tasks,
including text-to-image, image editing, and in-context generation. Unlike
OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image
modalities, utilizing unshared parameters and a decoupled image tokenizer. This
design enables OmniGen2 to build upon existing multimodal understanding models
without the need to re-adapt VAE inputs, thereby preserving the original text
generation capabilities. To facilitate the training of OmniGen2, we developed
comprehensive data construction pipelines, encompassing image editing and
in-context generation data. Additionally, we introduce a reflection mechanism
tailored for image generation tasks and curate a dedicated reflection dataset
based on OmniGen2. Despite its relatively modest parameter size, OmniGen2
achieves competitive results on multiple task benchmarks, including
text-to-image and image editing. To further evaluate in-context generation,
also referred to as subject-driven tasks, we introduce a new benchmark named
OmniContext. OmniGen2 achieves state-of-the-art performance among open-source
models in terms of consistency. We will release our models, training code,
datasets, and data construction pipeline to support future research in this
field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:
https://github.com/VectorSpaceLab/OmniGen2

</details>


### [261] [Attention-Based Ensemble Learning for Crop Classification Using Landsat 8-9 Fusion](https://arxiv.org/pdf/2506.18321)
*Zeeshan Ramzan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, Muhammad Shahbaz, Rabin Chakrabortty, Ahmed F. Elaksher*

Main category: cs.CV

TL;DR: The study uses remote sensing and advanced modeling to classify crops in Central Punjab, combining field surveys, satellite imagery, and machine learning for high accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve crop classification accuracy in irrigated regions using remote sensing and advanced modeling techniques.

Method: Field surveys for data collection, Landsat 8-9 imagery pre-processing, image fusion, vegetation indices extraction, and classification using conventional classifiers, ensemble learning, and neural networks.

Result: A comprehensive dataset of 50,835 data points was created, and advanced modeling techniques improved crop classification accuracy.

Conclusion: Combining remote sensing data with advanced modeling effectively enhances crop classification in irrigated agricultural areas.

Abstract: Remote sensing offers a highly effective method for obtaining accurate
information on total cropped area and crop types. The study focuses on crop
cover identification for irrigated regions of Central Punjab. Data collection
was executed in two stages: the first involved identifying and geocoding six
target crops through field surveys conducted in January and February 2023. The
second stage involved acquiring Landsat 8-9 imagery for each geocoded field to
construct a labelled dataset. The satellite imagery underwent extensive
pre-processing, including radiometric calibration for reflectance values,
atmospheric correction, and georeferencing verification to ensure consistency
within a common coordinate system. Subsequently, image fusion techniques were
applied to combine Landsat 8 and 9 spectral bands, creating a composite image
with enhanced spectral information, followed by contrast enhancement. During
data acquisition, farmers were interviewed, and fields were meticulously mapped
using GPS instruments, resulting in a comprehensive dataset of 50,835 data
points. This dataset facilitated the extraction of vegetation indices such as
NDVI, SAVO, RECI, and NDRE. These indices and raw reflectance values were
utilized for classification modeling using conventional classifiers, ensemble
learning, and artificial neural networks. A feature selection approach was also
incorporated to identify the optimal feature set for classification learning.
This study demonstrates the effectiveness of combining remote sensing data and
advanced modeling techniques to improve crop classification accuracy in
irrigated agricultural regions.

</details>


### [262] [Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?](https://arxiv.org/pdf/2506.18322)
*Yiwei Yang, Chung Peng Lee, Shangbin Feng, Dora Zhao, Bingbing Wen, Anthony Z. Liu, Yulia Tsvetkov, Bill Howe*

Main category: cs.CV

TL;DR: The paper introduces SpuriVerse, a benchmark for studying spurious correlations in multi-modal Large Vision Language Models (LVLMs), sourced from GPT-4o errors and curated with human and synthetic evaluation. It reveals that even top models struggle (37.1% accuracy), but fine-tuning on synthetic examples improves performance (78.40%), showing models learn to avoid shortcuts.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for studying spurious correlations in LVLMs trained on diverse datasets without explicit supervision, the paper aims to create a realistic benchmark (SpuriVerse) to evaluate model performance.

Method: Developed SpuriVerse by sourcing GPT-4o errors on VQA benchmarks, curating them with LVLM-human annotation and synthetic counterfactual evaluation. The benchmark includes 124 spurious correlation types with 1,364 questions.

Result: Evaluation of 15 LVLMs showed poor performance (37.1% accuracy for top models). Fine-tuning on synthetic examples improved accuracy to 78.40%, indicating models learn to avoid spurious correlations.

Conclusion: SpuriVerse effectively highlights spurious correlation challenges in LVLMs. Fine-tuning on diverse synthetic examples helps models generalize and avoid shortcuts, improving performance.

Abstract: Finetuning can cause spurious correlations to arise between non-essential
features and the target labels, but benchmarks to study these effects involve
contrived settings and narrow tasks. In contrast, we consider spurious
correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on
extensive and diverse datasets without explicit task supervision. We develop a
benchmark by sourcing GPT-4o errors on real-world visual-question-answering
(VQA) benchmarks, then curating a subset through LVLM-human annotation and
synthetic counterfactual evaluation to identify errors caused by spurious
correlations. This process yields SpuriVerse, a novel benchmark comprised of
124 distinct types of spurious correlations extracted from real-world datasets,
each containing 1 realistic and 10 synthetic VQA samples for a total of 1364
multiple choice questions. We evaluate 15 open and closed-source LVLMs on
SpuriVerse, finding that even state-of-the-art closed-source models struggle
significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic
examples that emphasize the spurious correlation improves performance to
78.40%, suggesting that training on diverse spurious patterns generalizes to
unseen situations: models appear to learn to avoid "shortcuts" and attend to
the overall image context.

</details>


### [263] [A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement](https://arxiv.org/pdf/2506.18323)
*Muhammad Azeem Aslam, Hassan Khalid, Nisar Ahmed*

Main category: cs.CV

TL;DR: LucentVisionNet is a zero-shot learning framework for low-light image enhancement, outperforming existing methods without paired training data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of low-light image enhancement without paired data, overcoming limitations of traditional and deep learning methods.

Method: Integrates multi-scale spatial attention with a deep curve estimation network, uses a recurrent enhancement strategy, and optimizes with a composite loss function.

Result: Outperforms state-of-the-art methods on benchmark datasets, achieving high visual quality, structural consistency, and efficiency.

Conclusion: LucentVisionNet is effective for real-world applications like mobile photography and surveillance due to its performance and efficiency.

Abstract: Low-light image enhancement remains a challenging task, particularly in the
absence of paired training data. In this study, we present LucentVisionNet, a
novel zero-shot learning framework that addresses the limitations of
traditional and deep learning-based enhancement methods. The proposed approach
integrates multi-scale spatial attention with a deep curve estimation network,
enabling fine-grained enhancement while preserving semantic and perceptual
fidelity. To further improve generalization, we adopt a recurrent enhancement
strategy and optimize the model using a composite loss function comprising six
tailored components, including a novel no-reference image quality loss inspired
by human visual perception. Extensive experiments on both paired and unpaired
benchmark datasets demonstrate that LucentVisionNet consistently outperforms
state-of-the-art supervised, unsupervised, and zero-shot methods across
multiple full-reference and no-reference image quality metrics. Our framework
achieves high visual quality, structural consistency, and computational
efficiency, making it well-suited for deployment in real-world applications
such as mobile photography, surveillance, and autonomous navigation.

</details>


### [264] [NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation](https://arxiv.org/pdf/2506.18325)
*Yu Xie, Chengjie Zeng, Lingyun Zhang, Yanwei Fu*

Main category: cs.CV

TL;DR: PromptSan detoxifies harmful prompts in text-to-image models without altering architecture, using two variants: PromptSan-Modify (token replacement) and PromptSan-Suffix (optimized suffix training).


<details>
  <summary>Details</summary>
Motivation: Address risks of misuse (e.g., harmful content generation) in T2I models like Stable Diffusion, inspired by jailbreak attacks in LLMs.

Method: Proposes PromptSan with two variants: (1) PromptSan-Modify iteratively replaces harmful tokens using text NSFW classifiers; (2) PromptSan-Suffix trains a suffix to neutralize harmful intent while passing NSFW checks.

Result: Achieves state-of-the-art performance in reducing harmful content, balancing safety and usability.

Conclusion: PromptSan effectively mitigates harmful content generation in T2I models without compromising generation capability.

Abstract: The rapid advancement of text-to-image (T2I) models, such as Stable
Diffusion, has enhanced their capability to synthesize images from textual
prompts. However, this progress also raises significant risks of misuse,
including the generation of harmful content (e.g., pornography, violence,
discrimination), which contradicts the ethical goals of T2I technology and
hinders its sustainable development. Inspired by "jailbreak" attacks in large
language models, which bypass restrictions through subtle prompt modifications,
this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a
novel approach to detoxify harmful prompts without altering model architecture
or degrading generation capability. PromptSan includes two variants:
PromptSan-Modify, which iteratively identifies and replaces harmful tokens in
input prompts using text NSFW classifiers during inference, and
PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize
harmful intent while passing both text and image NSFW classifier checks.
Extensive experiments demonstrate that PromptSan achieves state-of-the-art
performance in reducing harmful content generation across multiple metrics,
effectively balancing safety and usability.

</details>


### [265] [Geometry-Aware Preference Learning for 3D Texture Generation](https://arxiv.org/pdf/2506.18331)
*AmirHossein Zamani, Tianhao Xie, Amir G. Aghdam, Tiberiu Popa, Eugene Belilovsky*

Main category: cs.CV

TL;DR: The paper introduces a differentiable preference learning framework for 3D generative models to align outputs with human preferences and task-specific criteria, addressing the lack of 3D structure awareness in current methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generative models often fail to align with human preferences or task-specific needs and rely on 2D text-to-image models, which lack 3D structure understanding.

Method: Proposes an end-to-end differentiable preference learning framework with geometry-aware reward functions to back-propagate human preferences through the 3D generative pipeline.

Result: Demonstrates effectiveness using four novel geometry-aware reward functions, enabling more controllable and interpretable 3D content generation.

Conclusion: The framework offers a geometry-aware, preference-aligned approach for high-quality 3D content creation from natural language.

Abstract: Recent advances in 3D generative models have achieved impressive results but
3D contents generated by these models may not align with subjective human
preferences or task-specific criteria. Moreover, a core challenge in the 3D
texture generation domain remains: most existing approaches rely on repeated
calls to 2D text-to-image generative models, which lack an inherent
understanding of the 3D structure of the input 3D mesh object. To address this,
we propose an end-to-end differentiable preference learning framework that
back-propagates human preferences, represented by differentiable reward
functions, through the entire 3D generative pipeline, making the process
inherently geometry-aware. We demonstrate the effectiveness of our framework
using four proposed novel geometry-aware reward functions, offering a more
controllable and interpretable pathway for high-quality 3D content creation
from natural language.

</details>


### [266] [Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention](https://arxiv.org/pdf/2506.18335)
*Saad Wazir, Daeyoung Kim*

Main category: cs.CV

TL;DR: Proposes a novel architecture for medical image segmentation, improving accuracy by addressing feature transfer and decoder efficiency challenges, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer and CNN methods struggle with staining/morphology variations and limited datasets, hindering feature extraction and segmentation accuracy.

Method: Introduces a multi-scale feature capture architecture and a novel decoder design to integrate encoder features, emphasize key channels/regions, and reconstruct spatial dimensions.

Result: Achieves absolute performance gains of 2.76%-4.03% on four datasets (MoNuSeg, DSB, Electron Microscopy, TNBC) over SOTA methods.

Conclusion: The proposed method effectively enhances segmentation accuracy and is compatible with various encoders, demonstrating superior performance in experiments.

Abstract: Segmenting biomarkers in medical images is crucial for various biotech
applications. Despite advances, Transformer and CNN based methods often
struggle with variations in staining and morphology, limiting feature
extraction. In medical image segmentation, where datasets often have limited
sample availability, recent state-of-the-art (SOTA) methods achieve higher
accuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to
underperform. This is due to challenges in effectively transferring rich
multiscale features from encoders to decoders, as well as limitations in
decoder efficiency. To address these issues, we propose an architecture that
captures multi-scale local and global contextual information and a novel
decoder design, which effectively integrates features from the encoder,
emphasizes important channels and regions, and reconstructs spatial dimensions
to enhance segmentation accuracy. Our method, compatible with various encoders,
outperforms SOTA methods, as demonstrated by experiments on four datasets and
ablation studies. Specifically, our method achieves absolute performance gains
of 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on
TNBC datasets compared to existing SOTA methods. Code:
https://github.com/saadwazir/MCADS-Decoder

</details>


### [267] [BSMamba: Brightness and Semantic Modeling for Long-Range Interaction in Low-Light Image Enhancement](https://arxiv.org/pdf/2506.18346)
*Tongshun Zhang, Pingping Liu, Mengen Cai, Zijian Zhang, Yubing Lu, Qiuzhan Zhou*

Main category: cs.CV

TL;DR: BSMamba, a novel visual Mamba architecture, enhances low-light images by prioritizing brightness and semantic token interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LLIE methods struggle with brightness improvement, semantic consistency, and computational efficiency, while existing visual Mamba approaches limit token interactions.

Method: BSMamba introduces Brightness Mamba for brightness-guided token interactions and Semantic Mamba for semantic consistency, avoiding fixed scanning rules.

Result: BSMamba achieves state-of-the-art performance in LLIE, preserving semantic consistency and fine details.

Conclusion: BSMamba overcomes limitations of conventional methods by modeling tokens based on brightness and semantic similarity, enhancing LLIE effectively.

Abstract: Current low-light image enhancement (LLIE) methods face significant
limitations in simultaneously improving brightness while preserving semantic
consistency, fine details, and computational efficiency. With the emergence of
state-space models, particularly Mamba, image restoration has achieved
remarkable performance, yet existing visual Mamba approaches flatten 2D images
into 1D token sequences using fixed scanning rules, critically limiting
interactions between distant tokens with causal relationships and constraining
their ability to capture meaningful long-range dependencies. To address these
fundamental limitations, we propose BSMamba, a novel visual Mamba architecture
comprising two specially designed components: Brightness Mamba and Semantic
Mamba. The Brightness Mamba revolutionizes token interaction patterns by
prioritizing connections between distant tokens with similar brightness levels,
effectively addressing the challenge of brightness restoration in LLIE tasks
through brightness-guided selective attention. Complementing this, the Semantic
Mamba establishes priority interactions between tokens sharing similar semantic
meanings, allowing the model to maintain contextual consistency by connecting
semantically related regions across the image, thus preserving the hierarchical
nature of image semantics during enhancement. By intelligently modeling tokens
based on brightness and semantic similarity rather than arbitrary scanning
patterns, BSMamba transcends the constraints of conventional token sequencing
while adhering to the principles of causal modeling. Extensive experiments
demonstrate that BSMamba achieves state-of-the-art performance in LLIE while
preserving semantic consistency.

</details>


### [268] [Spatial frequency information fusion network for few-shot learning](https://arxiv.org/pdf/2506.18364)
*Wenqing Zhao, Guojia Xie, Han Pan, Biao Yang, Weichuan Zhang*

Main category: cs.CV

TL;DR: The paper proposes SFIFNet, a method for Few-shot learning that integrates frequency and spatial domain information to improve classification performance by enhancing feature representation.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning faces challenges like overfitting and poor generalization due to limited data. Existing models often neglect frequency domain information, which holds valuable features.

Method: The paper introduces SFIFNet, which innovatively preprocesses data by combining frequency and spatial domain information to better represent image features.

Result: Experiments show that SFIFNet effectively improves classification performance by leveraging both frequency and spatial domain information.

Conclusion: Integrating frequency domain information with spatial domain information enhances feature representation and boosts Few-shot learning performance.

Abstract: The objective of Few-shot learning is to fully leverage the limited data
resources for exploring the latent correlations within the data by applying
algorithms and training a model with outstanding performance that can
adequately meet the demands of practical applications. In practical
applications, the number of images in each category is usually less than that
in traditional deep learning, which can lead to over-fitting and poor
generalization performance. Currently, many Few-shot classification models pay
more attention to spatial domain information while neglecting frequency domain
information, which contains more feature information. Ignoring frequency domain
information will prevent the model from fully exploiting feature information,
which would effect the classification performance. Based on conventional data
augmentation, this paper proposes an SFIFNet with innovative data
preprocessing. The key of this method is enhancing the accuracy of image
feature representation by integrating frequency domain information with spatial
domain information. The experimental results demonstrate the effectiveness of
this method in enhancing classification performance.

</details>


### [269] [Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection](https://arxiv.org/pdf/2506.18368)
*Anja Deli, Matej Grci, Sinia egvi*

Main category: cs.CV

TL;DR: SeeKer detects anomalous human poses in skeleton sequences using autoregressive factorization and conditional Gaussians, outperforming previous methods on key datasets.


<details>
  <summary>Details</summary>
Motivation: Anomalous human behavior detection is crucial for safety-critical applications like healthcare and surveillance, where unusual poses often indicate abnormalities.

Method: SeeKer uses autoregressive factorization at the keypoint level, modeling skeleton sequences with conditional Gaussians. Anomalies are flagged based on low-density keypoint locations.

Result: SeeKer outperforms previous methods on UBnormal and MSAD-HR datasets and delivers competitive performance on ShanghaiTech.

Conclusion: SeeKer's simplicity and effectiveness make it a strong solution for detecting anomalous human poses in skeleton sequences.

Abstract: Detecting anomalous human behaviour is an important visual task in
safety-critical applications such as healthcare monitoring, workplace safety,
or public surveillance. In these contexts, abnormalities are often reflected
with unusual human poses. Thus, we propose SeeKer, a method for detecting
anomalies in sequences of human skeletons. Our method formulates the skeleton
sequence density through autoregressive factorization at the keypoint level.
The corresponding conditional distributions represent probable keypoint
locations given prior skeletal motion. We formulate the joint distribution of
the considered skeleton as causal prediction of conditional Gaussians across
its constituent keypoints. A skeleton is flagged as anomalous if its keypoint
locations surprise our model (i.e. receive a low density). In practice, our
anomaly score is a weighted sum of per-keypoint log-conditionals, where the
weights account for the confidence of the underlying keypoint detector. Despite
its conceptual simplicity, SeeKer surpasses all previous methods on the
UBnormal and MSAD-HR datasets while delivering competitive performance on the
ShanghaiTech dataset.

</details>


### [270] [RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models](https://arxiv.org/pdf/2506.18369)
*Yeongtak Oh, Jisoo Mok, Dohyun Chung, Juhyeon Shin, Sangha Park, Johan Barthelemy, Sungroh Yoon*

Main category: cs.CV

TL;DR: A reinforcement learning (RL)-based post-training framework is proposed to enhance personalized image captioning in multi-modal large language models (MLLMs), outperforming supervised fine-tuning (SFT) methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with personalized image captions, even post-trained with SFT, due to limitations in real-world scenarios like multi-concept captioning and the high cost of acquiring quality data.

Method: The paper introduces an RL-based post-training framework for MLLMs, addressing the shortcomings of SFT by improving visual recognition and personalized generation.

Result: The proposed method significantly outperforms SFT-based baselines, particularly in multi-concept image captioning tasks.

Conclusion: RL-based post-training is an effective solution for enhancing personalized captioning in MLLMs, overcoming data-centric challenges of SFT.

Abstract: Recent multi-modal large language models (MLLMs) often struggle to generate
personalized image captions, even when trained on high-quality captions. In
this work, we observe that such limitations persist in existing
post-training-based MLLM personalization methods. Specifically, despite being
post-tuned with large-scale caption data through supervised fine-tuning (SFT),
these models frequently fail to produce faithful descriptions in real-world
scenarios, such as multi-concept image captioning. However, acquiring
large-scale, high-quality captions for such complex settings is both costly and
difficult. To address the data-centric nature of SFT, we propose a
reinforcement learning (RL)-based post-training framework. To the best of our
knowledge, this is the first RL-based approach to post-train MLLMs for
personalized image captioning. Our method significantly enhances both visual
recognition and personalized generation capabilities of MLLMs, and consistently
outperforms existing SFT-based baselines, especially in the challenging
multi-concept image captioning task.

</details>


### [271] [OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding](https://arxiv.org/pdf/2506.18372)
*Hieu Nguyen, Phuc-Tan Nguyen, Thien-Phuc Tran, Minh-Quang Nguyen, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le*

Main category: cs.CV

TL;DR: OpenEvents V1 is a large-scale dataset for event-centric vision-language tasks, focusing on contextual and temporal grounding through captioning and retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: To advance event-centric vision-language understanding beyond surface-level descriptions by emphasizing contextual and temporal grounding.

Method: The dataset includes 200,000 news articles and 400,000 images from CNN and The Guardian, with tasks for generating event-aware captions and retrieving event-relevant images.

Result: Extensive baseline results and standardized evaluation protocols are provided, supporting deep reasoning over complex events.

Conclusion: OpenEvents V1 serves as a robust foundation for multimodal models tackling real-world event understanding.

Abstract: We introduce OpenEvents V1, a large-scale benchmark dataset aimed at
advancing event-centric vision-language understanding. Unlike conventional
image captioning and retrieval datasets that emphasize surface-level
descriptions, OpenEvents V1 focuses on contextual and temporal grounding
through two primary tasks: (1) generating rich, event-aware image captions and
(2) retrieving event-relevant images based on narrative-style textual queries.
The dataset contains over 200,000 news articles and 400,000 associated images
sourced from CNN and The Guardian, spanning diverse domains and time periods.
We provide extensive baseline results and standardized evaluation protocols for
both tasks. OpenEvents V1 establishes a robust foundation for developing
multimodal models capable of deep reasoning over complex real-world events. The
dataset is available at https://ltnghia.github.io/eventa/openevents-v1

</details>


### [272] [InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models](https://arxiv.org/pdf/2506.18385)
*Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou, Tong He, Wenqi Shao, Kaipeng Zhang, Yi Wang, Botian Shi, Yanting Zhang, Jifeng Dai, Yu Qiao, Hongjie Zhang, Wenhai Wang*

Main category: cs.CV

TL;DR: InternSpatial introduces a large-scale dataset (12M QA pairs) and benchmark for spatial reasoning in VLMs, improving performance by 12.1% on its benchmark and 10.7% on VSI-Bench.


<details>
  <summary>Details</summary>
Motivation: Existing resources for spatial reasoning in VLMs are limited in scale, diversity, and instruction expressiveness.

Method: Created InternSpatial (dataset) and InternSpatial-Bench (benchmark) with diverse visual environments and 19 instruction formats, including a novel rotation angle prediction task.

Result: Models trained on InternSpatial showed significant improvements (12.1% on InternSpatial-Bench, 10.7% on VSI-Bench) while maintaining general-purpose performance.

Conclusion: InternSpatial supports the development of spatially capable VLMs for applications like robotics and embodied AI.

Abstract: Recent benchmarks and datasets have been proposed to improve spatial
reasoning in vision-language models (VLMs), yet existing open resources remain
limited in scale, visual diversity, and instruction expressiveness. In this
work, we introduce InternSpatial, the largest open-source dataset for spatial
reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation
benchmark designed to assess spatial understanding under diverse instruction
formats. InternSpatial comprises 12 million QA pairs spanning both single-view
and multi-view settings, drawn from diverse visual environments and supporting
19 instruction formats that reflect varied query styles. For evaluation, we
propose InternSpatial-Bench for single-view tasks and expand multi-view
reasoning by introducing a novel rotation angle prediction task that has not
been explored in prior work. Experimental results show that models trained on
InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on
VSI-Bench, while maintaining strong performance on general-purpose benchmarks.
We hope these resources will support the development of spatially capable VLMs
in practical applications such as robotics and embodied AI.

</details>


### [273] [Distributed Poisson multi-Bernoulli filtering via generalised covariance intersection](https://arxiv.org/pdf/2506.18397)
*ngel F. Garca-Fernndez, Giorgio Battistelli*

Main category: cs.CV

TL;DR: The paper introduces a distributed PMB filter using GCI fusion for multi-object tracking, proposing a tractable approximation for the intractable exact fusion.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of intractable exact GCI fusion of PMB densities in distributed multi-object filtering.

Method: Approximates the power of a PMB density as an unnormalised PMB density, using GCI fusion to produce a PMBM, which is then projected back to PMB.

Result: The method yields a closed-form PMBM, with experimental results showing advantages over other distributed filters.

Conclusion: The proposed approximation effectively enables distributed multi-object filtering with PMB densities, outperforming existing methods.

Abstract: This paper presents the distributed Poisson multi-Bernoulli (PMB) filter
based on the generalised covariance intersection (GCI) fusion rule for
distributed multi-object filtering. Since the exact GCI fusion of two PMB
densities is intractable, we derive a principled approximation. Specifically,
we approximate the power of a PMB density as an unnormalised PMB density, which
corresponds to an upper bound of the PMB density. Then, the GCI fusion rule
corresponds to the normalised product of two unnormalised PMB densities. We
show that the result is a Poisson multi-Bernoulli mixture (PMBM), which can be
expressed in closed form. Future prediction and update steps in each filter
preserve the PMBM form, which can be projected back to a PMB density before the
next fusion step. Experimental results show the benefits of this approach
compared to other distributed multi-object filters.

</details>


### [274] [Latent Space Analysis for Melanoma Prevention](https://arxiv.org/pdf/2506.18414)
*Ciro Listone, Aniello Murano*

Main category: cs.CV

TL;DR: A novel method using a Conditional Variational Autoencoder for interpretable melanoma risk assessment, combining deep learning with clinical insights.


<details>
  <summary>Details</summary>
Motivation: Early, interpretable diagnostic tools are needed for melanoma due to its high mortality. Existing models lack clinical insight.

Method: Uses a Conditional Variational Autoencoder to learn a structured latent space for continuous risk assessment, combined with SVM for classification.

Result: Strong performance in differentiating benign nevi and melanomas, with interpretable malignancy indicators.

Conclusion: Bridges predictive performance with clinical applicability, enhancing trust in AI-assisted diagnosis.

Abstract: Melanoma represents a critical health risk due to its aggressive progression
and high mortality, underscoring the need for early, interpretable diagnostic
tools. While deep learning has advanced in skin lesion classification, most
existing models provide only binary outputs, offering limited clinical insight.
This work introduces a novel approach that extends beyond classification,
enabling interpretable risk modelling through a Conditional Variational
Autoencoder. The proposed method learns a structured latent space that captures
semantic relationships among lesions, allowing for a nuanced, continuous
assessment of morphological differences. An SVM is also trained on this
representation effectively differentiating between benign nevi and melanomas,
demonstrating strong and consistent performance. More importantly, the learned
latent space supports visual and geometric interpretation of malignancy, with
the spatial proximity of a lesion to known melanomas serving as a meaningful
indicator of risk. This approach bridges predictive performance with clinical
applicability, fostering early detection, highlighting ambiguous cases, and
enhancing trust in AI-assisted diagnosis through transparent and interpretable
decision-making.

</details>


### [275] [Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging](https://arxiv.org/pdf/2506.18434)
*Filippo Ruffini, Elena Mulero Ayllon, Linlin Shen, Paolo Soda, Valerio Guarrasi*

Main category: cs.CV

TL;DR: A structured benchmark evaluates transferability of CNNs and Foundation Models for COVID-19 prognosis prediction using diverse Chest X-ray datasets, comparing fine-tuning strategies under varied conditions.


<details>
  <summary>Details</summary>
Motivation: AI's potential in medical imaging prognosis is hindered by challenges in effective application, prompting a need for robust evaluation methods.

Method: Extensive fine-tuning strategies (Full Fine-Tuning, Linear Probing, Parameter-Efficient methods) are tested on pretrained models (CLIP, DINOv2, MedCLIP, etc.) in full-data and Few-Shot Learning settings.

Result: The benchmark provides insights into model adaptability and generalization, especially under data scarcity and class imbalance.

Conclusion: The study aims to guide practical deployment of efficient, generalizable AI solutions in clinical prognosis workflows.

Abstract: Artificial Intelligence (AI) holds significant promise for improving
prognosis prediction in medical imaging, yet its effective application remains
challenging. In this work, we introduce a structured benchmark explicitly
designed to evaluate and compare the transferability of Convolutional Neural
Networks and Foundation Models in predicting clinical outcomes in COVID-19
patients, leveraging diverse publicly available Chest X-ray datasets. Our
experimental methodology extensively explores a wide set of fine-tuning
strategies, encompassing traditional approaches such as Full Fine-Tuning and
Linear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods
including Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were
conducted across multiple learning paradigms, including both extensive
full-data scenarios and more clinically realistic Few-Shot Learning settings,
which are critical for modeling rare disease outcomes and rapidly emerging
health threats. By implementing a large-scale comparative analysis involving a
diverse selection of pretrained models, including general-purpose architectures
pretrained on large-scale datasets such as CLIP and DINOv2, to
biomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we
rigorously assess each model's capacity to effectively adapt and generalize to
prognosis tasks, particularly under conditions of severe data scarcity and
pronounced class imbalance. The benchmark was designed to capture critical
conditions common in prognosis tasks, including variations in dataset size and
class distribution, providing detailed insights into the strengths and
limitations of each fine-tuning strategy. This extensive and structured
evaluation aims to inform the practical deployment and adoption of robust,
efficient, and generalizable AI-driven solutions in real-world clinical
prognosis prediction workflows.

</details>


### [276] [Frequency-Domain Fusion Transformer for Image Inpainting](https://arxiv.org/pdf/2506.18437)
*Sijin He, Guangfeng Lin, Tao Li, Yajun Chen*

Main category: cs.CV

TL;DR: A Transformer-based image inpainting method with frequency-domain fusion improves detail preservation and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with complex textures and large occlusions, while Transformer-based approaches often lose high-frequency details and are computationally expensive.

Method: Combines wavelet transform and Gabor filtering for multi-scale structural modeling, uses a learnable frequency-domain filter, and adopts a four-level encoder-decoder structure with a novel loss strategy.

Result: The method effectively preserves high-frequency information, improving inpainting quality.

Conclusion: The proposed approach addresses limitations of existing methods by enhancing detail retention and computational efficiency.

Abstract: Image inpainting plays a vital role in restoring missing image regions and
supporting high-level vision tasks, but traditional methods struggle with
complex textures and large occlusions. Although Transformer-based approaches
have demonstrated strong global modeling capabilities, they often fail to
preserve high-frequency details due to the low-pass nature of self-attention
and suffer from high computational costs. To address these challenges, this
paper proposes a Transformer-based image inpainting method incorporating
frequency-domain fusion. Specifically, an attention mechanism combining wavelet
transform and Gabor filtering is introduced to enhance multi-scale structural
modeling and detail preservation. Additionally, a learnable frequency-domain
filter based on the fast Fourier transform is designed to replace the
feedforward network, enabling adaptive noise suppression and detail retention.
The model adopts a four-level encoder-decoder structure and is guided by a
novel loss strategy to balance global semantics and fine details. Experimental
results demonstrate that the proposed method effectively improves the quality
of image inpainting by preserving more high-frequency information.

</details>


### [277] [CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing](https://arxiv.org/pdf/2506.18438)
*Dinh-Khoi Vo, Thanh-Toan Do, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le*

Main category: cs.CV

TL;DR: CPAM is a zero-shot framework for complex, non-rigid real image editing, preserving object identity and background while enabling precise control via mask guidance.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to preserve textures, identity, and background details, requiring extensive fine-tuning and struggling with spatial control.

Method: CPAM introduces a preservation adaptation module for self-attention control, a localized extraction module for cross-attention, and mask-guidance strategies.

Result: CPAM outperforms state-of-the-art methods on the IMBA benchmark, preferred by human raters for consistent and high-quality edits.

Conclusion: CPAM offers an effective, zero-shot solution for complex image editing, preserving context and enabling diverse manipulations.

Abstract: Editing natural images using textual descriptions in text-to-image diffusion
models remains a significant challenge, particularly in achieving consistent
generation and handling complex, non-rigid objects. Existing methods often
struggle to preserve textures and identity, require extensive fine-tuning, and
exhibit limitations in editing specific spatial regions or objects while
retaining background details. This paper proposes Context-Preserving Adaptive
Manipulation (CPAM), a novel zero-shot framework for complicated, non-rigid
real image editing. Specifically, we propose a preservation adaptation module
that adjusts self-attention mechanisms to preserve and independently control
the object and background effectively. This ensures that the objects' shapes,
textures, and identities are maintained while keeping the background
undistorted during the editing process using the mask guidance technique.
Additionally, we develop a localized extraction module to mitigate the
interference with the non-desired modified regions during conditioning in
cross-attention mechanisms. We also introduce various mask-guidance strategies
to facilitate diverse image manipulation tasks in a simple manner. Extensive
experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a
robust benchmark dataset specifically designed for real image editing,
demonstrate that our proposed method is the preferred choice among human
raters, outperforming existing state-of-the-art editing techniques.

</details>


### [278] [DIP: Unsupervised Dense In-Context Post-training of Visual Representations](https://arxiv.org/pdf/2506.18463)
*Sophia Sirko-Galouchenko, Spyros Gidaris, Antonin Vobecky, Andrei Bursuc, Nicolas Thome*

Main category: cs.CV

TL;DR: DIP is an unsupervised post-training method for enhancing dense image representations in pretrained vision encoders, using pseudo-tasks for in-context scene understanding. It outperforms prior methods and is computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To improve dense image representations for in-context scene understanding without relying on complex self-distillation architectures or labeled data.

Method: Trains vision encoders using pseudo-tasks generated by combining a pretrained diffusion model and the encoder itself, simulating downstream scenarios.

Result: Achieves strong performance across downstream tasks, outperforming initial encoders and prior methods.

Conclusion: DIP offers a practical, efficient, and effective solution for enhancing dense representations in vision encoders.

Abstract: We introduce DIP, a novel unsupervised post-training method designed to
enhance dense image representations in large-scale pretrained vision encoders
for in-context scene understanding. Unlike prior approaches that rely on
complex self-distillation architectures, our method trains the vision encoder
using pseudo-tasks that explicitly simulate downstream in-context scenarios,
inspired by meta-learning principles. To enable post-training on unlabeled
data, we propose an automatic mechanism for generating in-context tasks that
combines a pretrained diffusion model and the vision encoder itself. DIP is
simple, unsupervised, and computationally efficient, requiring less than 9
hours on a single A100 GPU. By learning dense representations through pseudo
in-context tasks, it achieves strong performance across a wide variety of
downstream real-world in-context scene understanding tasks. It outperforms both
the initial vision encoder and prior methods, offering a practical and
effective solution for improving dense representations. Code available here:
https://github.com/sirkosophia/DIP

</details>


### [279] [AViLA: Asynchronous Vision-Language Agent for Streaming Multimodal Data Interaction](https://arxiv.org/pdf/2506.18472)
*Gengyuan Zhang, Tanveer Hannan, Hermine Kleiner, Beste Aydemir, Xinyu Xie, Jian Lan, Thomas Seidl, Volker Tresp, Jindong Gu*

Main category: cs.CV

TL;DR: The paper introduces AViLA, an asynchronous video-language agent designed to handle streaming data and ad-hoc queries with temporal awareness, addressing the challenge of Query-Evidence Asynchrony.


<details>
  <summary>Details</summary>
Motivation: The challenge arises when agents interact with dynamic data streams and ad-hoc queries, requiring responses grounded in historical, present, and future data. Existing models lack temporal awareness and accuracy in such settings.

Method: The authors propose AViLA, featuring three modules: comprehensive memory retention, evidence identification, and evidence-grounded trigger, to manage streaming data and respond timely.

Result: Experiments show AViLA outperforms existing models in accuracy and temporal awareness, addressing the asynchrony issue effectively.

Conclusion: AViLA demonstrates significant improvements in handling streaming data and ad-hoc queries, providing a robust solution for real-world applications like autonomous driving and embodied agents.

Abstract: An ideal vision-language agent serves as a bridge between the human users and
their surrounding physical world in real-world applications like autonomous
driving and embodied agents, and proactively provides accurate and timely
responses given user intents. An intriguing challenge arises when agents
interact with the world as a dynamic data stream and ad-hoc queries from users:
supporting knowledge for queries, namely evidence, usually appears
asynchronously with the arrival time of queries, and agents need to ground
their responses in historical data, present observations, and even future
streams. We frame this challenge as Query-Evidence Asynchrony, where user
queries and their supporting evidence typically arrive asynchronously in the
streaming setting. This setting requires not only strong reasoning capabilities
but also the ability to retain past observations and respond to queries with
temporal awareness. In this paper, we introduce a diagnostic benchmark that
evaluates Multimodal Large Language Models (MLLMs) on their ability to handle
interaction with streaming data. Further, we present AViLA, Asynchronous
Video-Language Agent for streaming data interaction that can handle ad-hoc
queries and give time-aware responses. For this purpose, AViLA consists of
three key modules: comprehensive memory retention, evidence identification, and
evidence-grounded trigger, that are designed to maintain a general-purpose
memory and respond readily and timely to queries. Our experiments show that
existing models often fail to respond at appropriate times, while AViLA
significantly improves both accuracy and temporal awareness. Our code and
dataset will be publicly available.

</details>


### [280] [Context Consistency Learning via Sentence Removal for Semi-Supervised Video Paragraph Grounding](https://arxiv.org/pdf/2506.18476)
*Yaokun Zhong, Siyu Jiang, Jian Zhu, Jian-Fang Hu*

Main category: cs.CV

TL;DR: The paper introduces Context Consistency Learning (CCL) for Semi-Supervised Video Paragraph Grounding (SSVPG), improving localization by perturbing query contexts and leveraging teacher-student learning and pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect perturbing query contexts for strong supervision, limiting performance in SSVPG.

Method: Proposes CCL, combining consistency regularization and pseudo-labeling, using teacher-student learning with strong augmentation and retraining with pseudo-labels.

Result: CCL significantly outperforms existing methods in experiments.

Conclusion: CCL effectively enhances semi-supervised learning for SSVPG by leveraging context perturbations and consistency.

Abstract: Semi-Supervised Video Paragraph Grounding (SSVPG) aims to localize multiple
sentences in a paragraph from an untrimmed video with limited temporal
annotations. Existing methods focus on teacher-student consistency learning and
video-level contrastive loss, but they overlook the importance of perturbing
query contexts to generate strong supervisory signals. In this work, we propose
a novel Context Consistency Learning (CCL) framework that unifies the paradigms
of consistency regularization and pseudo-labeling to enhance semi-supervised
learning. Specifically, we first conduct teacher-student learning where the
student model takes as inputs strongly-augmented samples with sentences removed
and is enforced to learn from the adequately strong supervisory signals from
the teacher model. Afterward, we conduct model retraining based on the
generated pseudo labels, where the mutual agreement between the original and
augmented views' predictions is utilized as the label confidence. Extensive
experiments show that CCL outperforms existing methods by a large margin.

</details>


### [281] [GANs vs. Diffusion Models for virtual staining with the HER2match dataset](https://arxiv.org/pdf/2506.18484)
*Pascal Klckner, Jos Teixeira, Diana Montezuma, Jaime S. Cardoso, Hugo M. Horlings, Sara P. Oliveira*

Main category: cs.CV

TL;DR: The paper introduces HER2match, the first public dataset for H&E-HER2 staining transfer, compares GANs and DMs, and highlights GANs' superior performance, with BBDM as an exception.


<details>
  <summary>Details</summary>
Motivation: The lack of public datasets and unclear best model frameworks for H&E-HER2 staining transfer motivated this research.

Method: The study introduces HER2match dataset, compares GANs and DMs, and implements a novel Brownian Bridge Diffusion Model (BBDM) for H&E-HER2 translation.

Result: GANs outperform DMs, except for BBDM. Data alignment significantly improves results.

Conclusion: The HER2match dataset and framework comparison provide valuable resources and guidance for future research in virtual staining.

Abstract: Virtual staining is a promising technique that uses deep generative models to
recreate histological stains, providing a faster and more cost-effective
alternative to traditional tissue chemical staining. Specifically for H&E-HER2
staining transfer, despite a rising trend in publications, the lack of
sufficient public datasets has hindered progress in the topic. Additionally, it
is currently unclear which model frameworks perform best for this particular
task. In this paper, we introduce the HER2match dataset, the first publicly
available dataset with the same breast cancer tissue sections stained with both
H&E and HER2. Furthermore, we compare the performance of several Generative
Adversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel
Brownian Bridge Diffusion Model for H&E-HER2 translation. Our findings indicate
that, overall, GANs perform better than DMs, with only the BBDM achieving
comparable results. Furthermore, we emphasize the importance of data alignment,
as all models trained on HER2match produced vastly improved visuals compared to
the widely used consecutive-slide BCI dataset. This research provides a new
high-quality dataset ([available upon publication acceptance]), improving both
model training and evaluation. In addition, our comparison of frameworks offers
valuable guidance for researchers working on the topic.

</details>


### [282] [ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation](https://arxiv.org/pdf/2506.18493)
*Trong-Vu Hoang, Quang-Binh Nguyen, Thanh-Toan Do, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le*

Main category: cs.CV

TL;DR: ShowFlow is a framework for customizable image generation, addressing challenges in single- and multi-concept scenarios with specialized modules and strategies.


<details>
  <summary>Details</summary>
Motivation: Challenges in maintaining identity preservation and prompt alignment in single- and multi-concept image generation.

Method: ShowFlow-S uses a KronA-WED adapter and disentangled learning for single-concept generation. ShowFlow-M reuses ShowFlow-S models for multi-concept generation, adding SAMA and layout consistency.

Result: Effective performance in experiments and user studies, with potential applications in advertising and virtual dressing.

Conclusion: ShowFlow offers a robust solution for controllable image synthesis, balancing identity preservation and prompt alignment.

Abstract: Customizing image generation remains a core challenge in controllable image
synthesis. For single-concept generation, maintaining both identity
preservation and prompt alignment is challenging. In multi-concept scenarios,
relying solely on a prompt without additional conditions like layout boxes or
semantic masks, often leads to identity loss and concept omission. In this
paper, we introduce ShowFlow, a comprehensive framework designed to tackle
these challenges. We propose ShowFlow-S for single-concept image generation,
and ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a
KronA-WED adapter, which integrates a Kronecker adapter with weight and
embedding decomposition, and employs a disentangled learning approach with a
novel attention regularization objective to enhance single-concept generation.
Building on this foundation, ShowFlow-M directly reuses the learned models from
ShowFlow-S to support multi-concept generation without extra conditions,
incorporating a Subject-Adaptive Matching Attention (SAMA) and a layout
consistency strategy as the plug-and-play module. Extensive experiments and
user studies validate ShowFlow's effectiveness, highlighting its potential in
real-world applications like advertising and virtual dressing.

</details>


### [283] [Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey](https://arxiv.org/pdf/2506.18504)
*Xinyao Li, Jingjing Li, Fengling Li, Lei Zhu, Yang Yang, Heng Tao Shen*

Main category: cs.CV

TL;DR: A survey on vision-language models (VLMs) highlights their zero-shot capabilities but notes performance drops in specialized tasks. It categorizes generalization methods into prompt-based, parameter-based, and feature-based, comparing benchmarks and discussing future directions.


<details>
  <summary>Details</summary>
Motivation: To address the performance gap of VLMs in domain-specific tasks by summarizing and analyzing generalization methods and benchmarks.

Method: Categorizes literature into prompt-based, parameter-based, and feature-based methods, revisits transfer learning settings, and compares benchmarks.

Result: Provides a comprehensive overview of VLM generalization methods, benchmarks, and performance comparisons, linking VLMs to multimodal large language models (MLLMs).

Conclusion: The survey clarifies the landscape of VLM research, offering insights into current and future multimodal advancements.

Abstract: Recently, vision-language pretraining has emerged as a transformative
technique that integrates the strengths of both visual and textual modalities,
resulting in powerful vision-language models (VLMs). Leveraging web-scale
pretraining data, these models exhibit strong zero-shot capabilities. However,
their performance often deteriorates when confronted with domain-specific or
specialized generalization tasks. To address this, a growing body of research
focuses on transferring or generalizing the rich knowledge embedded in VLMs to
various downstream applications. This survey aims to comprehensively summarize
the generalization settings, methodologies, benchmarking and results in VLM
literatures. Delving into the typical VLM structures, current literatures are
categorized into prompt-based, parameter-based and feature-based methods
according to the transferred modules. The differences and characteristics in
each category are furthered summarized and discussed by revisiting the typical
transfer learning (TL) settings, providing novel interpretations for TL in the
era of VLMs. Popular benchmarks for VLM generalization are further introduced
with thorough performance comparisons among the reviewed methods. Following the
advances in large-scale generalizable pretraining, this survey also discusses
the relations and differences between VLMs and up-to-date multimodal large
language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the
surging literatures in vision-language research from a novel and practical
generalization prospective, this survey contributes to a clear landscape of
current and future multimodal researches.

</details>


### [284] [Biased Teacher, Balanced Student](https://arxiv.org/pdf/2506.18496)
*Seonghak Kim*

Main category: cs.CV

TL;DR: LTKD is a novel framework for Knowledge Distillation in class-imbalanced scenarios, addressing teacher bias by decomposing the standard KD objective into inter-group and intra-group KL divergence and introducing rebalanced and uniform losses.


<details>
  <summary>Details</summary>
Motivation: Conventional KD performs poorly on long-tailed data due to teacher bias toward head classes, limiting supervision for tail classes.

Method: LTKD reformulates KD into inter-group and intra-group KL divergence, introducing rebalanced inter-group loss and uniform intra-group loss to mitigate bias.

Result: LTKD outperforms existing KD methods on CIFAR-100-LT, TinyImageNet-LT, and ImageNet-LT, improving overall accuracy and tail-class performance.

Conclusion: LTKD effectively transfers knowledge from biased teachers, making it suitable for resource-constrained and imbalanced real-world settings.

Abstract: Knowledge Distillation (KD) is a widely adopted model compression technique
where a compact student model learns from the output of a larger, pre-trained
teacher. While effective in balanced settings, conventional KD suffers
significantly when applied to long-tailed data distributions, as the teacher
model tends to be biased toward head classes and provides limited supervision
for tail classes. In this paper, we propose Long-Tailed Knowledge Distillation
(LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by
reformulating the standard KD objective into two components: inter-group and
intra-group Kullback-Leibler (KL) divergence, corresponding to the prediction
distributions across and within class groups (head, medium, tail),
respectively. This decomposition allows us to identify and quantify the sources
of teacher bias. To address them, we introduce (1) a rebalanced inter-group
loss that calibrates the teacher's group-level predictions and (2) a uniform
intra-group loss that ensures equal contribution from all groups during
distillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and
ImageNet-LT show that LTKD consistently outperforms existing KD methods,
achieving significant gains in both overall accuracy and tail-class
performance. Our results demonstrate that LTKD enables effective knowledge
transfer even from biased teachers, making it a strong candidate for real-world
deployment in resource-constrained and imbalanced settings.

</details>


### [285] [MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis](https://arxiv.org/pdf/2506.18512)
*Yuting Zhang, Kaishen Yuan, Hao Lu, Yutao Yue, Jintai Chen, Kaishun Wu*

Main category: cs.CV

TL;DR: MedTVT-R1 is a multimodal large language model (MLLM) framework for accurate and interpretable multi-disease diagnosis using clinical multimodal data. It includes a curated dataset (MedTVT-QA) and employs modality perception and GRPO-based reinforcement fine-tuning for improved reasoning.


<details>
  <summary>Details</summary>
Motivation: Current single-modal approaches limit comprehensive disease understanding, necessitating a multimodal solution for better diagnosis.

Method: Proposes MedTVT-R1 with a modality perception layer and GRPO-based reinforcement fine-tuning, using the MedTVT-QA dataset for training.

Result: MedTVT-R1 excels in multimodal feature utilization and multi-disease diagnosis, showing clinical potential.

Conclusion: MedTVT-R1 offers a robust framework for multimodal medical diagnosis, with practical applications in clinical settings.

Abstract: Accurate and interpretable multi-disease diagnosis remains a critical
challenge in medical research, particularly when leveraging heterogeneous
multimodal medical data. Current approaches often rely on single-modal data,
limiting their ability to comprehensively understand complex diseases. To
address this, we propose MedTVT-R1, a novel Multimodal Large Language Model
(MLLM) framework designed to integrate clinical multimodal data for reasoning
and diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction
dataset that provides question-answer pairs for physiological-level
interpretations and disease-level diagnoses with a Chain of Evidence approach.
MedTVT-R1 incorporates a modality perception layer to capture inter-modal
dependencies and adaptively weight modality contributions. Additionally, we
employ Group Relative Policy Optimization (GRPO)-based Reinforcement
Fine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning.
Experimental results demonstrate MedTVT-R1's superiority in multimodal feature
utilization and multi-disease diagnosis, offering significant potential for
clinical applications such as diagnostic report generation and comorbidity
reasoning. The dataset and code are available at
https://github.com/keke-nice/MedTVT-R1.

</details>


### [286] [Enhancing Image Restoration Transformer via Adaptive Translation Equivariance](https://arxiv.org/pdf/2506.18520)
*JiaKui Hu, Zhengjian Yao, Lujia Jin, Hangzhou He, Yanye Lu*

Main category: cs.CV

TL;DR: The paper introduces TEAFormer, a transformer model for image restoration that preserves translation equivariance using slide indexing and component stacking, addressing computational and receptive field challenges.


<details>
  <summary>Details</summary>
Motivation: Modern restoration transformers lose translation equivariance, harming training and generalization. The paper aims to restore this property.

Method: Proposes slide indexing and component stacking for equivariance, and introduces adaptive sliding indexing to balance computational cost and receptive field.

Result: TEAFormer outperforms in effectiveness, convergence, and generalization across image restoration tasks.

Conclusion: TEAFormer successfully integrates translation equivariance into transformers, improving performance in image restoration.

Abstract: Translation equivariance is a fundamental inductive bias in image
restoration, ensuring that translated inputs produce translated outputs.
Attention mechanisms in modern restoration transformers undermine this
property, adversely impacting both training convergence and generalization. To
alleviate this issue, we propose two key strategies for incorporating
translation equivariance: slide indexing and component stacking. Slide indexing
maintains operator responses at fixed positions, with sliding window attention
being a notable example, while component stacking enables the arrangement of
translation-equivariant operators in parallel or sequentially, thereby building
complex architectures while preserving translation equivariance. However, these
strategies still create a dilemma in model design between the high
computational cost of self-attention and the fixed receptive field associated
with sliding window attention. To address this, we develop an adaptive sliding
indexing mechanism to efficiently select key-value pairs for each query, which
are then concatenated in parallel with globally aggregated key-value pairs. The
designed network, called the Translation Equivariance Adaptive Transformer
(TEAFormer), is assessed across a variety of image restoration tasks. The
results highlight its superiority in terms of effectiveness, training
convergence, and generalization.

</details>


### [287] [Multi-Scale Representation of Follicular Lymphoma Pathology Images in a Single Hyperbolic Space](https://arxiv.org/pdf/2506.18523)
*Kei Taguchi, Kazumasa Ohara, Tatsuya Yokota, Hiroaki Miyoshi, Noriaki Hashimoto, Ichiro Takeuchi, Hidekata Hontani*

Main category: cs.CV

TL;DR: A method for embedding malignant lymphoma pathology images in hyperbolic space using self-supervised learning to capture hierarchical relationships across scales.


<details>
  <summary>Details</summary>
Motivation: To represent morphological changes across scales (from cell nuclei to tissue images) during disease progression.

Method: Embed tissue and nucleus images in a hyperbolic space (Poincar ball) based on inclusion relationships using self-supervised learning.

Result: Learned representations effectively encode hierarchical structure, capturing disease state and cell type variations.

Conclusion: The approach successfully models hierarchical relationships in pathology images, aiding in disease progression analysis.

Abstract: We propose a method for representing malignant lymphoma pathology images,
from high-resolution cell nuclei to low-resolution tissue images, within a
single hyperbolic space using self-supervised learning. To capture
morphological changes that occur across scales during disease progression, our
approach embeds tissue and corresponding nucleus images close to each other
based on inclusion relationships. Using the Poincar\'e ball as the feature
space enables effective encoding of this hierarchical structure. The learned
representations capture both disease state and cell type variations.

</details>


### [288] [Auto-Regressively Generating Multi-View Consistent Images](https://arxiv.org/pdf/2506.18527)
*JiaKui Hu, Yuxiao Yang, Jialun Liu, Jinbo Wu, Chen Zhao, Yanye Lu*

Main category: cs.CV

TL;DR: The paper introduces MV-AR, an auto-regressive method for generating consistent multi-view images from diverse prompts, addressing challenges like view consistency and multi-modal condition handling.


<details>
  <summary>Details</summary>
Motivation: To enable efficient 3D content creation by generating multi-view images from human instructions while maintaining consistency and handling diverse conditions.

Method: Proposes MV-AR, an auto-regressive model leveraging next-token-prediction for progressive multi-view synthesis, with condition injection modules and a progressive training strategy.

Result: MV-AR consistently generates high-quality multi-view images across various conditions, matching the performance of leading diffusion-based models.

Conclusion: MV-AR is a versatile and effective solution for multi-view image generation, with potential applications in 3D content creation.

Abstract: Generating multi-view images from human instructions is crucial for 3D
content creation. The primary challenges involve maintaining consistency across
multiple views and effectively synthesizing shapes and textures under diverse
conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)
method, which leverages an auto-regressive model to progressively generate
consistent multi-view images from arbitrary prompts. Firstly, the
next-token-prediction capability of the AR model significantly enhances its
effectiveness in facilitating progressive multi-view synthesis. When generating
widely-separated views, MV-AR can utilize all its preceding views to extract
effective reference information. Subsequently, we propose a unified model that
accommodates various prompts via architecture designing and training
strategies. To address multiple conditions, we introduce condition injection
modules for text, camera pose, image, and shape. To manage multi-modal
conditions simultaneously, a progressive training strategy is employed. This
strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to
enhance the development of a comprehensive X-to-multi-view (X2mv) model through
the randomly dropping and combining conditions. Finally, to alleviate the
overfitting problem caused by limited high-quality data, we propose the
"Shuffle View" data augmentation technique, thus significantly expanding the
training data by several magnitudes. Experiments demonstrate the performance
and versatility of our MV-AR, which consistently generates consistent
multi-view images across a range of conditions and performs on par with leading
diffusion-based multi-view image generation models. Code and models will be
released at https://github.com/MILab-PKU/MVAR.

</details>


### [289] [Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation](https://arxiv.org/pdf/2506.18658)
*Ling Zhang, Boxiang Yun, Qingli Li, Yan Wang*

Main category: cs.CV

TL;DR: BiGen framework improves pathology report generation by addressing semantic content gaps and redundancy in WSIs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in automated pathology report generation include lack of semantic content in visual features and redundancy in WSIs.

Method: Proposes a bi-modal concurrent learning framework with knowledge retrieval and cross-modal alignment for report generation.

Result: Achieves 7.4% improvement in NLP metrics and 19.1% in Her-2 prediction, outperforming existing methods.

Conclusion: BiGen effectively enhances report quality by integrating visual and knowledge features, validated by ablation studies.

Abstract: Automated pathology report generation from Whole Slide Images (WSIs) faces
two key challenges: (1) lack of semantic content in visual features and (2)
inherent information redundancy in WSIs. To address these issues, we propose a
novel Historical Report Guided \textbf{Bi}-modal Concurrent Learning Framework
for Pathology Report \textbf{Gen}eration (BiGen) emulating pathologists'
diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to
provide rich semantic content, which retrieves WSI-relevant knowledge from
pre-built medical knowledge bank by matching high-attention patches and (2) A
bi-modal concurrent learning strategy instantiated via a learnable visual token
and a learnable textual token to dynamically extract key visual features and
retrieved knowledge, where weight-shared layers enable cross-modal alignment
between visual features and knowledge features. Our multi-modal decoder
integrates both modals for comprehensive diagnostic reports generation.
Experiments on the PathText (BRCA) dataset demonstrate our framework's
superiority, achieving state-of-the-art performance with 7.4\% relative
improvement in NLP metrics and 19.1\% enhancement in classification metrics for
Her-2 prediction versus existing methods. Ablation studies validate the
necessity of our proposed modules, highlighting our method's ability to provide
WSI-relevant rich semantic content and suppress information redundancy in WSIs.
Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.

</details>


### [290] [A Set-to-Set Distance Measure in Hyperbolic Space](https://arxiv.org/pdf/2506.18529)
*Pengxiang Li, Wei Wu, Zhi Gao, Xiaomeng Fan, Peilin Yu, Yuwei Wu, Zhipeng Lu, Yunde Jia, Mehrtash Harandi*

Main category: cs.CV

TL;DR: The paper introduces HS2SD, a hyperbolic set-to-set distance measure, combining global and local structural information for better dissimilarity computation between sets in hyperbolic space.


<details>
  <summary>Details</summary>
Motivation: Existing point-to-point distance measures in hyperbolic space lack the ability to compare sets, which is crucial for applications requiring hierarchical and complex relationship modeling.

Method: HS2SD integrates global structure via geodesic distances between Einstein midpoints and local structure using topological characteristics approximated by Thue-Morse sequences.

Result: Empirical tests show HS2SD outperforms existing methods in entity matching and image classification tasks.

Conclusion: HS2SD effectively models hierarchical relationships in hyperbolic sets, offering superior performance in real-world applications.

Abstract: We propose a hyperbolic set-to-set distance measure for computing
dissimilarity between sets in hyperbolic space. While point-to-point distances
in hyperbolic space effectively capture hierarchical relationships between data
points, many real-world applications require comparing sets of hyperbolic data
points, where the local structure and the global structure of the sets carry
crucial semantic information. The proposed the \underline{h}yperbolic
\underline{s}et-\underline{to}-\underline{s}et \underline{d}istance measure
(HS2SD) integrates both global and local structural information: global
structure through geodesic distances between Einstein midpoints of hyperbolic
sets, and local structure through topological characteristics of the two sets.
To efficiently compute topological differences, we prove that using a finite
Thue-Morse sequence of degree and adjacency matrices can serve as a robust
approximation to capture the topological structure of a set. In this case, by
considering the topological differences, HS2SD provides a more nuanced
understanding of the relationships between two hyperbolic sets. Empirical
evaluation on entity matching, standard image classification, and few-shot
image classification demonstrates that our distance measure outperforms
existing methods by effectively modeling the hierarchical and complex
relationships inherent in hyperbolic sets.

</details>


### [291] [Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping](https://arxiv.org/pdf/2506.18668)
*Pablo Meseguer, Roco del Amor, Valery Naranjo*

Main category: cs.CV

TL;DR: A benchmark for evaluating histopathology foundation models (FMs) as patch-level feature extractors in MIL frameworks is introduced, using the AI4SkIN dataset and a novel FM-SI metric to measure consistency against distribution shifts.


<details>
  <summary>Details</summary>
Motivation: The diversity of histopathology FMs necessitates real-world challenges to evaluate their effectiveness, especially in automated whole slide image analysis requiring MIL frameworks.

Method: The study leverages the AI4SkIN dataset and introduces the FM-SI metric to evaluate FMs as patch-level feature extractors within MIL classification frameworks.

Result: Extracting less biased features improves classification performance, particularly in similarity-based MIL classifiers.

Conclusion: The proposed benchmark and FM-SI metric effectively evaluate histopathology FMs, demonstrating the importance of unbiased feature extraction for enhanced performance.

Abstract: Pretraining on large-scale, in-domain datasets grants histopathology
foundation models (FM) the ability to learn task-agnostic data representations,
enhancing transfer learning on downstream tasks. In computational pathology,
automated whole slide image analysis requires multiple instance learning (MIL)
frameworks due to the gigapixel scale of the slides. The diversity among
histopathology FMs has highlighted the need to design real-world challenges for
evaluating their effectiveness. To bridge this gap, our work presents a novel
benchmark for evaluating histopathology FMs as patch-level feature extractors
within a MIL classification framework. For that purpose, we leverage the
AI4SkIN dataset, a multi-center cohort encompassing slides with challenging
cutaneous spindle cell neoplasm subtypes. We also define the Foundation Model -
Silhouette Index (FM-SI), a novel metric to measure model consistency against
distribution shifts. Our experimentation shows that extracting less biased
features enhances classification performance, especially in similarity-based
MIL classifiers.

</details>


### [292] [Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces](https://arxiv.org/pdf/2506.18533)
*Pengxiang Li, Yuwei Wu, Zhi Gao, Xiaomeng Fan, Wei Wu, Zhipeng Lu, Yunde Jia, Mehrtash Harandi*

Main category: cs.CV

TL;DR: The paper proposes a dynamic, geometry-aware distance measure in hyperbolic spaces to better model diverse hierarchical structures, outperforming fixed-distance methods.


<details>
  <summary>Details</summary>
Motivation: Existing hyperbolic learning methods assume uniform hierarchy, which is restrictive for real-world diverse structures.

Method: Introduces adaptive projections and curvatures for each data pair, with low-rank decomposition and hard-pair mining to reduce computational cost.

Result: Outperforms fixed-distance methods, achieving over 5% gains on few-shot learning tasks like mini-ImageNet.

Conclusion: Adaptive distance measures better capture hierarchical diversity, improving class separation and performance.

Abstract: Learning in hyperbolic spaces has attracted increasing attention due to its
superior ability to model hierarchical structures of data. Most existing
hyperbolic learning methods use fixed distance measures for all data, assuming
a uniform hierarchy across all data points. However, real-world hierarchical
structures exhibit significant diversity, making this assumption overly
restrictive. In this paper, we propose a geometry-aware distance measure in
hyperbolic spaces, which dynamically adapts to varying hierarchical structures.
Our approach derives the distance measure by generating tailored projections
and curvatures for each pair of data points, effectively mapping them to an
appropriate hyperbolic space. We introduce a revised low-rank decomposition
scheme and a hard-pair mining mechanism to mitigate the computational cost of
pair-wise distance computation without compromising accuracy. We present an
upper bound on the low-rank approximation error using Talagrand's concentration
inequality, ensuring theoretical robustness. Extensive experiments on standard
image classification (MNIST, CIFAR-10 and CIFAR-100), hierarchical
classification (5-level CIFAR-100), and few-shot learning tasks (mini-ImageNet,
tiered-ImageNet) demonstrate the effectiveness of our method. Our approach
consistently outperforms learning methods that use fixed distance measures,
with notable improvements on few-shot learning tasks, where it achieves over
5\% gains on mini-ImageNet. The results reveal that adaptive distance measures
better capture diverse hierarchical structures, with visualization showing
clearer class boundaries and improved prototype separation in hyperbolic
spaces.

</details>


### [293] [Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios](https://arxiv.org/pdf/2506.18682)
*Imad Ali Shah, Jiarong Li, Tim Brophy, Martin Glavin, Edward Jones, Enda Ward, Brian Deegan*

Main category: cs.CV

TL;DR: The paper introduces a Multi-scale Spectral Attention Module (MSAM) for hyperspectral imaging in autonomous driving, improving semantic segmentation with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral Imaging (HSI) enhances environmental perception in challenging conditions, but processing its high-dimensional data is a challenge.

Method: MSAM uses three parallel 1D convolutions with varying kernel sizes (1-11) and adaptive feature aggregation, integrated into UNet's skip connections (UNet-MSAM).

Result: UNet-MSAM outperforms UNet-SC with 3.61% mean IoU and 3.80% mF1 improvements, adding only 0.02% parameters and 0.82% GFLOPS.

Conclusion: Multi-scale kernel combinations in MSAM are effective for HSI processing, offering insights for robust spectral feature extraction in real-world AD applications.

Abstract: Recent advances in autonomous driving (AD) have highlighted the potential of
Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly
in challenging weather and lighting conditions. However, efficiently processing
its high-dimensional spectral data remains a significant challenge. This paper
introduces a Multi-scale Spectral Attention Module (MSAM) that enhances
spectral feature extraction through three parallel 1D convolutions with varying
kernel sizes between 1 to 11, coupled with an adaptive feature aggregation
mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our
proposed UNet-MSAM achieves significant improvements in semantic segmentation
performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and
Hyperspectral City v2. Our comprehensive experiments demonstrate that with
minimal computational overhead (on average 0.02% in parameters and 0.82%
GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average
improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets.
Through extensive ablation studies, we have established that multi-scale kernel
combinations perform better than single-scale configurations. These findings
demonstrate the potential of HSI processing for AD and provide valuable
insights into designing robust, multi-scale spectral feature extractors for
real-world applications.

</details>


### [294] [Normality Prior Guided Multi-Semantic Fusion Network for Unsupervised Image Anomaly Detection](https://arxiv.org/pdf/2506.18544)
*Muhao Xu, Xueying Zhou, Xizhan Gao, Weiye Song, Guang Feng, Sijie Niu*

Main category: cs.CV

TL;DR: A novel normality prior guided multi-semantic fusion network is proposed for unsupervised anomaly detection, improving performance on logical anomalies by leveraging multi-semantic features of normal samples.


<details>
  <summary>Details</summary>
Motivation: Logical anomalies are harder to detect than structural ones due to their local resemblance to normal features but global deviation. Existing methods fail as anomalies propagate through low-dimensional bottlenecks.

Method: Extract global semantics of normal cases using a pre-trained vision-language network, construct semantic codebooks via vector quantization, and fuse multi-semantic features to guide anomaly reconstruction.

Result: Achieves SOTA on MVTec LOCO AD with 5.7% improvement in pixel-sPRO and 2.6% in image-AUROC.

Conclusion: The proposed method effectively addresses logical anomaly detection by integrating multi-semantic normality priors, outperforming existing approaches.

Abstract: Recently, detecting logical anomalies is becoming a more challenging task
compared to detecting structural ones. Existing encoder decoder based methods
typically compress inputs into low-dimensional bottlenecks on the assumption
that the compression process can effectively suppress the transmission of
logical anomalies to the decoder. However, logical anomalies present a
particular difficulty because, while their local features often resemble normal
semantics, their global semantics deviate significantly from normal patterns.
Thanks to the generalisation capabilities inherent in neural networks, these
abnormal semantic features can propagate through low-dimensional bottlenecks.
This ultimately allows the decoder to reconstruct anomalous images with
misleading fidelity. To tackle the above challenge, we propose a novel
normality prior guided multi-semantic fusion network for unsupervised anomaly
detection. Instead of feeding the compressed bottlenecks to the decoder
directly, we introduce the multi-semantic features of normal samples into the
reconstruction process. To this end, we first extract abstract global semantics
of normal cases by a pre-trained vision-language network, then the learnable
semantic codebooks are constructed to store representative feature vectors of
normal samples by vector quantisation. Finally, the above multi-semantic
features are fused and employed as input to the decoder to guide the
reconstruction of anomalies to approximate normality. Extensive experiments are
conducted to validate the effectiveness of our proposed method, and it achieves
the SOTA performance on the MVTec LOCO AD dataset with improvements of 5.7% in
pixel-sPRO and 2.6% in image-AUROC. The source code is available at
https://github.com/Xmh-L/NPGMF.

</details>


### [295] [SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification](https://arxiv.org/pdf/2506.18683)
*Youcef Sklab, Hanane Ariouat, Eric Chenin, Edi Prifti, Jean-Daniel Zucker*

Main category: cs.CV

TL;DR: SIM-Net integrates 3D point clouds from 2D images for enhanced classification, outperforming ResNet101 and transformers in herbarium datasets.


<details>
  <summary>Details</summary>
Motivation: Address challenges in classifying digitized herbarium specimens due to backgrounds, non-plant elements, and occlusions.

Method: Uses pixel-to-point transformation for 3D point clouds, combining CNN and PointNet encoders for feature fusion.

Result: Achieves up to 9.9% higher accuracy and 12.3% better F-score than ResNet101, surpassing transformer-based models.

Conclusion: SIM-Net demonstrates the value of 3D structural reasoning in 2D image classification, especially for complex datasets.

Abstract: We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image
classification architecture that integrates 3D point cloud representations
inferred directly from RGB images. Our key contribution lies in a
pixel-to-point transformation that converts 2D object masks into 3D point
clouds, enabling the fusion of texture-based and geometric features for
enhanced classification performance. SIM-Net is particularly well-suited for
the classification of digitized herbarium specimens (a task made challenging by
heterogeneous backgrounds), non-plant elements, and occlusions that compromise
conventional image-based models. To address these issues, SIM-Net employs a
segmentation-based preprocessing step to extract object masks prior to 3D point
cloud generation. The architecture comprises a CNN encoder for 2D image
features and a PointNet-based encoder for geometric features, which are fused
into a unified latent space. Experimental evaluations on herbarium datasets
demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of
up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several
transformer-based state-of-the-art architectures, highlighting the benefits of
incorporating 3D structural reasoning into 2D image classification tasks.

</details>


### [296] [Object-aware Sound Source Localization via Audio-Visual Scene Understanding](https://arxiv.org/pdf/2506.18557)
*Sung Jin Um, Dongjin Kim, Sangmin Lee, Jung Uk Kim*

Main category: cs.CV

TL;DR: A novel framework using Multimodal Large Language Models (MLLMs) improves sound source localization by distinguishing sound-making objects from silent ones, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in complex scenes due to reliance on simple audio-visual correspondence, lacking fine-grained semantic distinctions.

Method: Proposes a framework with MLLMs for detailed contextual information and introduces Object-aware Contrastive Alignment (OCA) and Object Region Isolation (ORI) loss functions.

Result: Outperforms existing methods on MUSIC and VGGSound datasets in single-source and multi-source localization.

Conclusion: The approach effectively addresses limitations of current methods, enhancing localization accuracy in complex scenes.

Abstract: Audio-visual sound source localization task aims to spatially localize
sound-making objects within visual scenes by integrating visual and audio cues.
However, existing methods struggle with accurately localizing sound-making
objects in complex scenes, particularly when visually similar silent objects
coexist. This limitation arises primarily from their reliance on simple
audio-visual correspondence, which does not capture fine-grained semantic
differences between sound-making and silent objects. To address these
challenges, we propose a novel sound source localization framework leveraging
Multimodal Large Language Models (MLLMs) to generate detailed contextual
information that explicitly distinguishes between sound-making foreground
objects and silent background objects. To effectively integrate this detailed
information, we introduce two novel loss functions: Object-aware Contrastive
Alignment (OCA) loss and Object Region Isolation (ORI) loss. Extensive
experimental results on MUSIC and VGGSound datasets demonstrate the
effectiveness of our approach, significantly outperforming existing methods in
both single-source and multi-source localization scenarios. Code and generated
detailed contextual information are available at:
https://github.com/VisualAIKHU/OA-SSL.

</details>


### [297] [VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning](https://arxiv.org/pdf/2506.18564)
*Xuanyu Zhang, Weiqi Li, Shijie Zhao, Junlin Li, Li Zhang, Jian Zhang*

Main category: cs.CV

TL;DR: VQ-Insight is a reasoning-style VLM framework for AIGC video quality assessment, addressing challenges like limited generalization and lack of temporal awareness. It outperforms baselines in various metrics.


<details>
  <summary>Details</summary>
Motivation: Current AIGC video quality evaluation methods lack generalization, temporal awareness, and rely on large annotated datasets. VQ-Insight aims to overcome these limitations.

Method: VQ-Insight uses a progressive learning scheme (image quality warm-up, temporal learning, joint optimization) and multi-dimension scoring rewards for better evaluation.

Result: VQ-Insight outperforms state-of-the-art baselines in preference comparison, multi-dimension scoring, and natural video scoring.

Conclusion: VQ-Insight significantly improves video quality assessment and generation tasks, offering a robust solution for AIGC video evaluation.

Abstract: Recent advances in AI-generated content (AIGC) have led to the emergence of
powerful text-to-video generation models. Despite these successes, evaluating
the quality of AIGC-generated videos remains challenging due to limited
generalization, lack of temporal awareness, heavy reliance on large-scale
annotated datasets, and the lack of effective interaction with generation
models. Most current approaches rely on supervised finetuning of
vision-language models (VLMs), which often require large-scale annotated
datasets and tend to decouple understanding and generation. To address these
shortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for
AIGC video quality assessment. Our approach features: (1) a progressive video
quality learning scheme that combines image quality warm-up, general
task-specific temporal learning, and joint optimization with the video
generation model; (2) the design of multi-dimension scoring rewards, preference
comparison rewards, and temporal modeling rewards to enhance both
generalization and specialization in video quality evaluation. Extensive
experiments demonstrate that VQ-Insight consistently outperforms
state-of-the-art baselines in preference comparison, multi-dimension scoring,
and natural video scoring, bringing significant improvements for video
generation tasks.

</details>


### [298] [Matrix-Game: Interactive World Foundation Model](https://arxiv.org/pdf/2506.18701)
*Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Fei Kang, Biao Jiang, Zedong Gao, Eric Li, Yang Liu, Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game is a large-scale interactive world foundation model for controllable game world generation, outperforming prior models in visual quality, controllability, and physical consistency.


<details>
  <summary>Details</summary>
Motivation: To advance interactive game world generation by enabling precise control over character actions and camera movements while maintaining high visual and temporal quality.

Method: A two-stage pipeline: large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation, using a dataset of Minecraft gameplay.

Result: Matrix-Game outperforms prior models (Oasis, MineWorld) in visual quality, temporal coherence, controllability, and physical consistency, as confirmed by human evaluations.

Conclusion: Matrix-Game sets a new standard for interactive world generation and will be open-sourced to support future research.

Abstract: We introduce Matrix-Game, an interactive world foundation model for
controllable game world generation. Matrix-Game is trained using a two-stage
pipeline that first performs large-scale unlabeled pretraining for environment
understanding, followed by action-labeled training for interactive video
generation. To support this, we curate Matrix-Game-MC, a comprehensive
Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips
and over 1,000 hours of high-quality labeled clips with fine-grained keyboard
and mouse action annotations. Our model adopts a controllable image-to-world
generation paradigm, conditioned on a reference image, motion context, and user
actions. With over 17 billion parameters, Matrix-Game enables precise control
over character actions and camera movements, while maintaining high visual
quality and temporal coherence. To evaluate performance, we develop GameWorld
Score, a unified benchmark measuring visual quality, temporal quality, action
controllability, and physical rule understanding for Minecraft world
generation. Extensive experiments show that Matrix-Game consistently
outperforms prior open-source Minecraft world models (including Oasis and
MineWorld) across all metrics, with particularly strong gains in
controllability and physical consistency. Double-blind human evaluations
further confirm the superiority of Matrix-Game, highlighting its ability to
generate perceptually realistic and precisely controllable videos across
diverse game scenarios. To facilitate future research on interactive
image-to-world generation, we will open-source the Matrix-Game model weights
and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.

</details>


### [299] [VisualChef: Generating Visual Aids in Cooking via Mask Inpainting](https://arxiv.org/pdf/2506.18569)
*Oleh Kuzyk, Zuoyue Li, Marc Pollefeys, Xi Wang*

Main category: cs.CV

TL;DR: VisualChef generates contextual visual aids for cooking by using mask-based visual grounding to simplify alignment, improving over existing methods.


<details>
  <summary>Details</summary>
Motivation: Cooking lacks consistent visual guidance; existing methods rely on complex textual alignment.

Method: VisualChef uses mask-based visual grounding to identify action-relevant objects and modify them while preserving the environment.

Result: Outperforms state-of-the-art methods on three egocentric video datasets.

Conclusion: VisualChef simplifies visual-textual alignment and enhances cooking support with tailored visual aids.

Abstract: Cooking requires not only following instructions but also understanding,
executing, and monitoring each step - a process that can be challenging without
visual guidance. Although recipe images and videos offer helpful cues, they
often lack consistency in focus, tools, and setup. To better support the
cooking process, we introduce VisualChef, a method for generating contextual
visual aids tailored to cooking scenarios. Given an initial frame and a
specified action, VisualChef generates images depicting both the action's
execution and the resulting appearance of the object, while preserving the
initial frame's environment. Previous work aims to integrate knowledge
extracted from large language models by generating detailed textual
descriptions to guide image generation, which requires fine-grained
visual-textual alignment and involves additional annotations. In contrast,
VisualChef simplifies alignment through mask-based visual grounding. Our key
insight is identifying action-relevant objects and classifying them to enable
targeted modifications that reflect the intended action and outcome while
maintaining a consistent environment. In addition, we propose an automated
pipeline to extract high-quality initial, action, and final state frames. We
evaluate VisualChef quantitatively and qualitatively on three egocentric video
datasets and show its improvements over state-of-the-art methods.

</details>


### [300] [2D Triangle Splatting for Direct Differentiable Mesh Training](https://arxiv.org/pdf/2506.18575)
*Kaifeng Sheng, Zheng Zhou, Yingliang Peng, Qianwei Wang*

Main category: cs.CV

TL;DR: 2D Triangle Splatting (2DTS) replaces 3D Gaussian primitives with 2D triangle facelets for high-fidelity 3D scene reconstruction, outperforming Gaussian-based and mesh-based methods.


<details>
  <summary>Details</summary>
Motivation: Challenges with rendering speed and advanced effects in differentiable rendering with 3D Gaussian primitives compared to mesh-based models.

Method: Proposes 2DTS, using 2D triangle facelets with a compactness parameter to train photorealistic meshes.

Result: Achieves higher fidelity than Gaussian-based methods and superior visual quality in mesh reconstruction.

Conclusion: 2DTS offers a promising alternative for high-quality 3D scene reconstruction with improved rendering and mesh quality.

Abstract: Differentiable rendering with 3D Gaussian primitives has emerged as a
powerful method for reconstructing high-fidelity 3D scenes from multi-view
images. While it offers improvements over NeRF-based methods, this
representation still encounters challenges with rendering speed and advanced
rendering effects, such as relighting and shadow rendering, compared to
mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a
novel method that replaces 3D Gaussian primitives with 2D triangle facelets.
This representation naturally forms a discrete mesh-like structure while
retaining the benefits of continuous volumetric modeling. By incorporating a
compactness parameter into the triangle primitives, we enable direct training
of photorealistic meshes. Our experimental results demonstrate that our
triangle-based method, in its vanilla version (without compactness tuning),
achieves higher fidelity compared to state-of-the-art Gaussian-based methods.
Furthermore, our approach produces reconstructed meshes with superior visual
quality compared to existing mesh reconstruction methods.

</details>


### [301] [Deep CNN Face Matchers Inherently Support Revocable Biometric Templates](https://arxiv.org/pdf/2506.18731)
*Aman Bhatta, Michael C. King, Kevin W. Bowyer*

Main category: cs.CV

TL;DR: The paper introduces a revocable biometric scheme using deep CNN face matchers, showing they can generate multiple distinct models with equivalent recognition power and incompatible templates, making compromised templates worthless. Vision Transformers are found less suitable than ResNet-based CNNs.


<details>
  <summary>Details</summary>
Motivation: Address the lack of recourse in biometric authentication if a biometric is compromised by developing a revocable scheme.

Method: Generate multiple deep CNN face matcher models with equivalent recognition power and incompatible templates, and compare with Vision Transformers.

Result: Deep CNN models allow revocable biometrics with strong template incompatibility, while Vision Transformers are less effective.

Conclusion: Deep CNN-based face matchers enable robust revocable biometric schemes, outperforming Vision Transformers in this context.

Abstract: One common critique of biometric authentication is that if an individual's
biometric is compromised, then the individual has no recourse. The concept of
revocable biometrics was developed to address this concern. A biometric scheme
is revocable if an individual can have their current enrollment in the scheme
revoked, so that the compromised biometric template becomes worthless, and the
individual can re-enroll with a new template that has similar recognition
power. We show that modern deep CNN face matchers inherently allow for a robust
revocable biometric scheme. For a given state-of-the-art deep CNN backbone and
training set, it is possible to generate an unlimited number of distinct face
matcher models that have both (1) equivalent recognition power, and (2)
strongly incompatible biometric templates. The equivalent recognition power
extends to the point of generating impostor and genuine distributions that have
the same shape and placement on the similarity dimension, meaning that the
models can share a similarity threshold for a 1-in-10,000 false match rate. The
biometric templates from different model instances are so strongly incompatible
that the cross-instance similarity score for images of the same person is
typically lower than the same-instance similarity score for images of different
persons. That is, a stolen biometric template that is revoked is of less value
in attempting to match the re-enrolled identity than the average impostor
template. We also explore the feasibility of using a Vision Transformer (ViT)
backbone-based face matcher in the revocable biometric system proposed in this
work and demonstrate that it is less suitable compared to typical ResNet-based
deep CNN backbones.

</details>


### [302] [Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing](https://arxiv.org/pdf/2506.18587)
*Antoine Saget, Baptiste Lafabregue, Antoine Cornujols, Pierre Ganarski*

Main category: cs.CV

TL;DR: A novel resampling-based augmentation for contrastive learning in Satellite Image Time Series (SITS) outperforms common methods and achieves state-of-the-art results without complex frameworks.


<details>
  <summary>Details</summary>
Motivation: Leverage abundant unlabeled SITS data due to scarcity of labeled data, addressing the challenge of designing effective augmentations for time series.

Method: Introduces a resampling-based augmentation strategy that upsamples time series and extracts disjoint subsequences while preserving temporal coverage.

Result: Outperforms common alternatives like jittering, resizing, and masking, achieving state-of-the-art performance on the S2-Agri100 dataset.

Conclusion: The method provides a simple yet effective contrastive learning augmentation for remote sensing time series.

Abstract: Given the abundance of unlabeled Satellite Image Time Series (SITS) and the
scarcity of labeled data, contrastive self-supervised pretraining emerges as a
natural tool to leverage this vast quantity of unlabeled data. However,
designing effective data augmentations for contrastive learning remains
challenging for time series. We introduce a novel resampling-based augmentation
strategy that generates positive pairs by upsampling time series and extracting
disjoint subsequences while preserving temporal coverage. We validate our
approach on multiple agricultural classification benchmarks using Sentinel-2
imagery, showing that it outperforms common alternatives such as jittering,
resizing, and masking. Further, we achieve state-of-the-art performance on the
S2-Agri100 dataset without employing spatial information or temporal encodings,
surpassing more complex masked-based SSL frameworks. Our method offers a
simple, yet effective, contrastive learning augmentation for remote sensing
time series.

</details>


### [303] [SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds](https://arxiv.org/pdf/2506.18591)
*Mauricio Byrd Victorica, Gyrgy Dn, Henrik Sandberg*

Main category: cs.CV

TL;DR: SpaNN is a novel attack detector for adversarial patches in CNNs, independent of patch count, using binarized feature maps and clustering for robust detection.


<details>
  <summary>Details</summary>
Motivation: Existing defenses against adversarial patches are limited to single-patch attacks or are computationally inefficient for multiple patches.

Method: SpaNN builds an ensemble of binarized feature maps from the first convolutional layer, performs clustering, and uses cluster features for classification.

Result: SpaNN outperforms state-of-the-art defenses by up to 11% (object detection) and 27% (image classification) on four datasets.

Conclusion: SpaNN provides a robust and efficient defense against multi-patch adversarial attacks, outperforming existing methods.

Abstract: State-of-the-art convolutional neural network models for object detection and
image classification are vulnerable to physically realizable adversarial
perturbations, such as patch attacks. Existing defenses have focused,
implicitly or explicitly, on single-patch attacks, leaving their sensitivity to
the number of patches as an open question or rendering them computationally
infeasible or inefficient against attacks consisting of multiple patches in the
worst cases. In this work, we propose SpaNN, an attack detector whose
computational complexity is independent of the expected number of adversarial
patches. The key novelty of the proposed detector is that it builds an ensemble
of binarized feature maps by applying a set of saliency thresholds to the
neural activations of the first convolutional layer of the victim model. It
then performs clustering on the ensemble and uses the cluster features as the
input to a classifier for attack detection. Contrary to existing detectors,
SpaNN does not rely on a fixed saliency threshold for identifying adversarial
regions, which makes it robust against white box adversarial attacks. We
evaluate SpaNN on four widely used data sets for object detection and
classification, and our results show that SpaNN outperforms state-of-the-art
defenses by up to 11 and 27 percentage points in the case of object detection
and the case of image classification, respectively. Our code is available at
https://github.com/gerkbyrd/SpaNN.

</details>


### [304] [SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving](https://arxiv.org/pdf/2506.18785)
*Helin Cao, Rafael Materla, Sven Behnke*

Main category: cs.CV

TL;DR: The paper proposes Spatially-aware Window Attention (SWA) to improve Semantic Occupancy Prediction (SOP) in autonomous driving by incorporating local spatial context into attention, addressing limitations of existing transformer-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based SOP methods lack explicit spatial structure modeling, leading to poor performance in sparse or occluded areas.

Method: Introduces SWA, a mechanism that integrates local spatial context into attention computation for better geometric awareness.

Result: SWA achieves state-of-the-art results on LiDAR-based SOP benchmarks and shows consistent improvements in camera-based SOP.

Conclusion: SWA enhances SOP by improving scene completion and generalizes well across sensor modalities.

Abstract: Perception systems in autonomous driving rely on sensors such as LiDAR and
cameras to perceive the 3D environment. However, due to occlusions and data
sparsity, these sensors often fail to capture complete information. Semantic
Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy
and semantics of unobserved regions. Existing transformer-based SOP methods
lack explicit modeling of spatial structure in attention computation, resulting
in limited geometric awareness and poor performance in sparse or occluded
areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel
mechanism that incorporates local spatial context into attention. SWA
significantly improves scene completion and achieves state-of-the-art results
on LiDAR-based SOP benchmarks. We further validate its generality by
integrating SWA into a camera-based SOP pipeline, where it also yields
consistent gains across modalities.

</details>


### [305] [RDPO: Real Data Preference Optimization for Physics Consistency Video Generation](https://arxiv.org/pdf/2506.18655)
*Wenxu Qian, Chaoyue Wang, Hou Peng, Zhiyu Tan, Hao Li, Anxiang Zeng*

Main category: cs.CV

TL;DR: RDPO is an annotation-free framework for improving video generation by distilling physical priors from real-world videos, enhancing physical realism and coherence.


<details>
  <summary>Details</summary>
Motivation: Current video generation lacks physical consistency, and existing methods require costly human annotations or reward models.

Method: RDPO reverse-samples real videos with a pre-trained generator to create preference pairs for physical correctness, followed by multi-stage iterative training.

Result: RDPO improves action coherence and physical realism in generated videos, validated by benchmarks and human evaluations.

Conclusion: RDPO offers a practical, annotation-free solution for enhancing physical consistency in video generation.

Abstract: Video generation techniques have achieved remarkable advancements in visual
quality, yet faithfully reproducing real-world physics remains elusive.
Preference-based model post-training may improve physical consistency, but
requires costly human-annotated datasets or reward models that are not yet
feasible. To address these challenges, we present Real Data Preference
Optimisation (RDPO), an annotation-free framework that distills physical priors
directly from real-world videos. Specifically, the proposed RDPO
reverse-samples real video sequences with a pre-trained generator to
automatically build preference pairs that are statistically distinguishable in
terms of physical correctness. A multi-stage iterative training schedule then
guides the generator to obey physical laws increasingly well. Benefiting from
the dynamic information explored from real videos, our proposed RDPO
significantly improves the action coherence and physical realism of the
generated videos. Evaluations on multiple benchmarks and human evaluations have
demonstrated that RDPO achieves improvements across multiple dimensions. The
source code and demonstration of this paper are available at:
https://wwenxu.github.io/RDPO/

</details>


### [306] [OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness](https://arxiv.org/pdf/2506.18798)
*Helin Cao, Sven Behnke*

Main category: cs.CV

TL;DR: OC-SOP improves semantic occupancy prediction by integrating object-centric cues, enhancing accuracy for foreground objects.


<details>
  <summary>Details</summary>
Motivation: Challenges in autonomous driving perception due to occlusions and incomplete data, with conventional methods treating all categories equally and relying on local features.

Method: Proposes Object-Centric SOP (OC-SOP), integrating high-level object-centric cues via a detection branch into the SOP pipeline.

Result: Significantly enhances prediction accuracy for foreground objects and achieves state-of-the-art performance on SemanticKITTI.

Conclusion: OC-SOP effectively addresses limitations of conventional methods, improving scene understanding for autonomous driving.

Abstract: Autonomous driving perception faces significant challenges due to occlusions
and incomplete scene data in the environment. To overcome these issues, the
task of semantic occupancy prediction (SOP) is proposed, which aims to jointly
infer both the geometry and semantic labels of a scene from images. However,
conventional camera-based methods typically treat all categories equally and
primarily rely on local features, leading to suboptimal predictions, especially
for dynamic foreground objects. To address this, we propose Object-Centric SOP
(OC-SOP), a framework that integrates high-level object-centric cues extracted
via a detection branch into the semantic occupancy prediction pipeline. This
object-centric integration significantly enhances the prediction accuracy for
foreground objects and achieves state-of-the-art performance among all
categories on SemanticKITTI.

</details>


### [307] [MedSeg-R: Medical Image Segmentation with Clinical Reasoning](https://arxiv.org/pdf/2506.18669)
*Hao Shao, Qibin Hou*

Main category: cs.CV

TL;DR: MedSeg-R is a lightweight, dual-stage framework for medical image segmentation that integrates semantic priors from medical reports to improve accuracy, especially for small lesions and overlapping structures.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with low-contrast or overlapping targets due to reliance on local cues or user prompts, lacking integrated semantic priors.

Method: MedSeg-R uses a cognitive stage to interpret medical reports into semantic priors (location, texture, shape) and a perceptual stage to modulate the SAM backbone with these priors via spatial attention, dynamic convolution, and deformable sampling.

Result: MedSeg-R significantly improves Dice scores for overlapping and ambiguous structures, enhancing sensitivity to small lesions.

Conclusion: MedSeg-R demonstrates effective plug-and-play compatibility with SAM-based systems, addressing key challenges in medical image segmentation.

Abstract: Medical image segmentation is challenging due to overlapping anatomies with
ambiguous boundaries and a severe imbalance between the foreground and
background classes, which particularly affects the delineation of small
lesions. Existing methods, including encoder-decoder networks and prompt-driven
variants of the Segment Anything Model (SAM), rely heavily on local cues or
user prompts and lack integrated semantic priors, thus failing to generalize
well to low-contrast or overlapping targets. To address these issues, we
propose MedSeg-R, a lightweight, dual-stage framework inspired by inspired by
clinical reasoning. Its cognitive stage interprets medical report into
structured semantic priors (location, texture, shape), which are fused via
transformer block. In the perceptual stage, these priors modulate the SAM
backbone: spatial attention highlights likely lesion regions, dynamic
convolution adapts feature filters to expected textures, and deformable
sampling refines spatial support. By embedding this fine-grained guidance
early, MedSeg-R disentangles inter-class confusion and amplifies minority-class
cues, greatly improving sensitivity to small lesions. In challenging
benchmarks, MedSeg-R produces large Dice improvements in overlapping and
ambiguous structures, demonstrating plug-and-play compatibility with SAM-based
systems.

</details>


### [308] [Reconstructing Tornadoes in 3D with Gaussian Splatting](https://arxiv.org/pdf/2506.18677)
*Adam Yang, Nadula Kadawedduwa, Tianfu Wang, Maria Molina, Christopher Metzler*

Main category: cs.CV

TL;DR: A lab-based tornado dataset is captured and released to enable 3D reconstruction using 3D Gaussian splatting (3DGS).


<details>
  <summary>Details</summary>
Motivation: Understanding tornadoes' 3D structure is crucial for preparedness, but lacks controlled datasets for validation.

Method: A multiview dataset of a lab-based tornado is created and 3DGS is applied for reconstruction.

Result: The 3D structure of the tornado is successfully reconstructed and visualized using 3DGS.

Conclusion: The dataset and method provide a foundation for advancing tornado research and reconstruction techniques.

Abstract: Accurately reconstructing the 3D structure of tornadoes is critically
important for understanding and preparing for this highly destructive weather
phenomenon. While modern 3D scene reconstruction techniques, such as 3D
Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the
3D structure of tornados, at present we are critically lacking a controlled
tornado dataset with which to develop and validate these tools. In this work we
capture and release a novel multiview dataset of a small lab-based tornado. We
demonstrate one can effectively reconstruct and visualize the 3D structure of
this tornado using 3DGS.

</details>


### [309] [TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting](https://arxiv.org/pdf/2506.18862)
*Zhongbin Guo, Yuhao Wang, Ping Jian, Xinyue Chen, Wei Peng, Ertai E*

Main category: cs.CV

TL;DR: TAMMs enhances MLLMs with temporal modules for satellite image change understanding and forecasting, outperforming baselines in temporal reasoning and future image generation.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with fine-grained spatial-temporal reasoning in satellite image time-series analysis, prompting the need for improved temporal-aware models.

Method: Proposes TAMMs, which adds lightweight temporal modules to frozen MLLMs for structured sequence encoding and contextual prompting, and introduces SFCI for adaptive semantic and structural control in image generation.

Result: TAMMs outperforms MLLM baselines in temporal change understanding and future image forecasting, demonstrating improved spatio-temporal reasoning.

Conclusion: Carefully designed temporal reasoning and semantic fusion unlock MLLMs' potential for complex spatio-temporal tasks.

Abstract: Satellite image time-series analysis demands fine-grained spatial-temporal
reasoning, which remains a challenge for existing multimodal large language
models (MLLMs). In this work, we study the capabilities of MLLMs on a novel
task that jointly targets temporal change understanding and future scene
generation, aiming to assess their potential for modeling complex multimodal
dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for
satellite image change understanding and forecasting, which enhances frozen
MLLMs with lightweight temporal modules for structured sequence encoding and
contextual prompting. To guide future image generation, TAMMs introduces a
Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines
high-level semantic reasoning and structural priors within an enhanced
ControlNet. This dual-path conditioning enables temporally consistent and
semantically grounded image synthesis. Experiments demonstrate that TAMMs
outperforms strong MLLM baselines in both temporal change understanding and
future image forecasting tasks, highlighting how carefully designed temporal
reasoning and semantic fusion can unlock the full potential of MLLMs for
spatio-temporal understanding.

</details>


### [310] [MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation](https://arxiv.org/pdf/2506.18678)
*Tianchen Deng, Guole Shen, Xun Chen, Shenghai Yuan, Hongming Shen, Guohao Peng, Zhenyu Wu, Jingchuan Wang, Lihua Xie, Danwei Wang, Hesheng Wang, Weidong Chen*

Main category: cs.CV

TL;DR: Proposes a distributed multi-agent neural SLAM framework with hybrid scene representation, loop closure, and online distillation, along with a new real-world dataset.


<details>
  <summary>Details</summary>
Motivation: Existing implicit SLAM methods struggle with multi-agent scenarios, large-scale scenes, and communication constraints.

Method: Uses hybrid triplane-grid scene representation, intra-to-inter loop closure, and online distillation for submap fusion. Introduces a new dataset (DES) for evaluation.

Result: Superior performance in mapping, tracking, and communication.

Conclusion: The framework and dataset advance multi-agent SLAM and 3D reconstruction research.

Abstract: Neural implicit scene representations have recently shown promising results
in dense visual SLAM. However, existing implicit SLAM algorithms are
constrained to single-agent scenarios, and fall difficulties in large-scale
scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks
cannot meet the constraints of communication bandwidth. To this end, we propose
the first distributed multi-agent collaborative neural SLAM framework with
hybrid scene representation, distributed camera tracking, intra-to-inter loop
closure, and online distillation for multiple submap fusion. A novel
triplane-grid joint scene representation method is proposed to improve scene
reconstruction. A novel intra-to-inter loop closure method is designed to
achieve local (single-agent) and global (multi-agent) consistency. We also
design a novel online distillation method to fuse the information of different
submaps to achieve global consistency. Furthermore, to the best of our
knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that
provides both continuous-time trajectories groundtruth and high-accuracy 3D
meshes groundtruth. To this end, we propose the first real-world Dense slam
(DES) dataset covering both single-agent and multi-agent scenarios, ranging
from small rooms to large-scale outdoor scenes, with high-accuracy ground truth
for both 3D mesh and continuous-time camera trajectory. This dataset can
advance the development of the research in both SLAM, 3D reconstruction, and
visual foundation model. Experiments on various datasets demonstrate the
superiority of the proposed method in both mapping, tracking, and
communication. The dataset and code will open-source on
https://github.com/dtc111111/mcnslam.

</details>


### [311] [MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation](https://arxiv.org/pdf/2506.18679)
*Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Au Hoi Fan, Haowei Guo, Puxin Yan*

Main category: cs.CV

TL;DR: MARL-MambaContour is a contour-based medical image segmentation framework using MARL, optimizing contour alignment with SAC and ERAM, and enhancing inter-agent communication with BCHFM. It outperforms traditional methods on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional pixel-based segmentation lacks topological constraints and structural awareness, limiting accuracy in medical images with blurred edges and complex shapes.

Method: Models contour points as agents adjusting positions iteratively. Uses SAC with ERAM for optimization and a Mamba-based policy network with BCHFM for inter-agent communication.

Result: Achieves state-of-the-art performance on five medical imaging datasets, demonstrating accuracy and robustness.

Conclusion: MARL-MambaContour is a promising framework for precise and reliable medical image segmentation.

Abstract: We introduce MARL-MambaContour, the first contour-based medical image
segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our
approach reframes segmentation as a multi-agent cooperation task focused on
generate topologically consistent object-level contours, addressing the
limitations of traditional pixel-based methods which could lack topological
constraints and holistic structural awareness of anatomical regions. Each
contour point is modeled as an autonomous agent that iteratively adjusts its
position to align precisely with the target boundary, enabling adaptation to
blurred edges and intricate morphologies common in medical images. This
iterative adjustment process is optimized by a contour-specific Soft
Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization
Adjustment Mechanism (ERAM) which dynamically balance agent exploration with
contour smoothness. Furthermore, the framework incorporates a Mamba-based
policy network featuring a novel Bidirectional Cross-attention Hidden-state
Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion
limitations associated with long-range modeling in state space models, thereby
facilitating more accurate inter-agent information exchange and informed
decision-making. Extensive experiments on five diverse medical imaging datasets
demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting
its potential as an accurate and robust clinical application.

</details>


### [312] [Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition](https://arxiv.org/pdf/2506.18721)
*Dustin Aganian, Erik Franze, Markus Eisenbach, Horst-Michael Gross*

Main category: cs.CV

TL;DR: A novel skeleton-based action recognition method uses word embeddings to encode semantic information, improving performance and generalization in assembly tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional skeleton-based methods lose keypoint semantics, limiting effectiveness in complex interactions.

Method: Replaces one-hot encodings with semantic volumes to capture joint-object relationships using word embeddings.

Result: Significant improvement in classification performance and generalization across skeleton types and object classes.

Conclusion: Incorporating semantic information enhances skeleton-based action recognition in dynamic environments.

Abstract: Effective human action recognition is widely used for cobots in Industry 4.0
to assist in assembly tasks. However, conventional skeleton-based methods often
lose keypoint semantics, limiting their effectiveness in complex interactions.
In this work, we introduce a novel approach to skeleton-based action
recognition that enriches input representations by leveraging word embeddings
to encode semantic information. Our method replaces one-hot encodings with
semantic volumes, enabling the model to capture meaningful relationships
between joints and objects. Through extensive experiments on multiple assembly
datasets, we demonstrate that our approach significantly improves
classification performance, and enhances generalization capabilities by
simultaneously supporting different skeleton types and object classes. Our
findings highlight the potential of incorporating semantic information to
enhance skeleton-based action recognition in dynamic and diverse environments.

</details>


### [313] [USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways](https://arxiv.org/pdf/2506.18737)
*Shanliang Yao, Runwei Guan, Yi Ni, Sen Xu, Yong Yue, Xiaohui Zhu, Ryan Wen Liu*

Main category: cs.CV

TL;DR: The paper introduces USVTrack, the first 4D radar-camera tracking dataset for autonomous driving in waterways, and proposes RCM, a radar-camera matching method to enhance tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve object tracking in inland waterways for applications like transportation and rescue, addressing challenges in complex environments.

Method: Uses a USV with 4D radar, camera, GPS, and IMU to collect data. Proposes RCM, a radar-camera matching method for two-stage association trackers.

Result: RCM improves tracking accuracy and reliability in diverse waterborne scenarios.

Conclusion: USVTrack dataset and RCM method advance autonomous driving in waterways, with the dataset publicly available.

Abstract: Object tracking in inland waterways plays a crucial role in safe and
cost-effective applications, including waterborne transportation, sightseeing
tours, environmental monitoring and surface rescue. Our Unmanned Surface
Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU,
delivers robust tracking capabilities in complex waterborne environments. By
leveraging these sensors, our USV collected comprehensive object tracking data,
which we present as USVTrack, the first 4D radar-camera tracking dataset
tailored for autonomous driving in new generation waterborne transportation
systems. Our USVTrack dataset presents rich scenarios, featuring diverse
various waterways, varying times of day, and multiple weather and lighting
conditions. Moreover, we present a simple but effective radar-camera matching
method, termed RCM, which can be plugged into popular two-stage association
trackers. Experimental results utilizing RCM demonstrate the effectiveness of
the radar-camera matching in improving object tracking accuracy and reliability
for autonomous driving in waterborne environments. The USVTrack dataset is
public on https://usvtrack.github.io.

</details>


### [314] [3D Arena: An Open Platform for Generative 3D Evaluation](https://arxiv.org/pdf/2506.18787)
*Dylan Ebert*

Main category: cs.CV

TL;DR: 3D Arena is introduced as a platform for evaluating image-to-3D models using human preferences, addressing gaps in current automated metrics.


<details>
  <summary>Details</summary>
Motivation: Current 3D model evaluation lacks alignment with human perception, relying on inadequate image-based or geometric metrics.

Method: The platform collects large-scale human preferences via pairwise comparisons, uses ELO-based ranking, and ensures quality with statistical fraud detection.

Result: Findings show human preferences favor Gaussian splats and textured models, with 3D Arena becoming a benchmark for the field.

Conclusion: 3D Arena advances human-centered evaluation in Generative 3D, offering insights and recommendations for future assessments.

Abstract: Evaluating Generative 3D models remains challenging due to misalignment
between automated metrics and human perception of quality. Current benchmarks
rely on image-based metrics that ignore 3D structure or geometric measures that
fail to capture perceptual appeal and real-world utility. To address this gap,
we present 3D Arena, an open platform for evaluating image-to-3D generation
models through large-scale human preference collection using pairwise
comparisons.
  Since launching in June 2024, the platform has collected 123,243 votes from
8,096 users across 19 state-of-the-art models, establishing the largest human
preference evaluation for Generative 3D. We contribute the iso3d dataset of 100
evaluation prompts and demonstrate quality control achieving 99.75% user
authenticity through statistical fraud detection. Our ELO-based ranking system
provides reliable model assessment, with the platform becoming an established
evaluation resource.
  Through analysis of this preference data, we present insights into human
preference patterns. Our findings reveal preferences for visual presentation
features, with Gaussian splat outputs achieving a 16.6 ELO advantage over
meshes and textured models receiving a 144.1 ELO advantage over untextured
models. We provide recommendations for improving evaluation methods, including
multi-criteria assessment, task-oriented evaluation, and format-aware
comparison. The platform's community engagement establishes 3D Arena as a
benchmark for the field while advancing understanding of human-centered
evaluation in Generative 3D.

</details>


### [315] [Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers](https://arxiv.org/pdf/2506.18791)
*Suyash Gaurav, Muhammad Farhan Humayun, Jukka Heikkonen, Jatin Chaudhary*

Main category: cs.CV

TL;DR: The paper introduces Super-Pixel Based Patch Pooling (SPPP) and Light Latent Attention (LLA) to reduce Vision Transformers' computational and memory demands while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of Vision Transformers, such as high computational costs and inefficiencies in transfer learning, by simplifying the architecture and improving attention mechanisms.

Method: Proposes SPPP for context-aware patch embeddings and LLA for reduced attention complexity, integrating dynamic positional encodings for targeted attention.

Result: Achieves comparable performance to state-of-the-art methods with significantly improved computational efficiency.

Conclusion: The approach is lightweight, adaptable, and energy-efficient, making it suitable for edge deployment.

Abstract: The evolution of Vision Transformers has led to their widespread adaptation
to different domains. Despite large-scale success, there remain significant
challenges including their reliance on extensive computational and memory
resources for pre-training on huge datasets as well as difficulties in
task-specific transfer learning. These limitations coupled with energy
inefficiencies mainly arise due to the computation-intensive self-attention
mechanism. To address these issues, we propose a novel Super-Pixel Based Patch
Pooling (SPPP) technique that generates context-aware, semantically rich, patch
embeddings to effectively reduce the architectural complexity and improve
efficiency. Additionally, we introduce the Light Latent Attention (LLA) module
in our pipeline by integrating latent tokens into the attention mechanism
allowing cross-attention operations to significantly reduce the time and space
complexity of the attention module. By leveraging the data-intuitive patch
embeddings coupled with dynamic positional encodings, our approach adaptively
modulates the cross-attention process to focus on informative regions while
maintaining the global semantic structure. This targeted attention improves
training efficiency and accelerates convergence. Notably, the SPPP module is
lightweight and can be easily integrated into existing transformer
architectures. Extensive experiments demonstrate that our proposed architecture
provides significant improvements in terms of computational efficiency while
achieving comparable results with the state-of-the-art approaches, highlighting
its potential for energy-efficient transformers suitable for edge deployment.
(The code is available on our GitHub repository:
https://github.com/zser092/Focused-Attention-ViT).

</details>


### [316] [ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs](https://arxiv.org/pdf/2506.18792)
*Michal Nazarczuk, Sibi Catley-Chandar, Thomas Tanay, Zhensong Zhang, Gregory Slabaugh, Eduardo Prez-Pellitero*

Main category: cs.CV

TL;DR: ViDAR is a 4D reconstruction framework using personalized diffusion models to generate pseudo multi-view supervision for Gaussian splatting, improving dynamic novel view synthesis from monocular video.


<details>
  <summary>Details</summary>
Motivation: Dynamic novel view synthesis from monocular video is challenging due to ill-posed structure-motion disentanglement and scarce supervision.

Method: ViDAR leverages personalized diffusion models for pseudo multi-view supervision, a diffusion-aware loss function, and camera pose optimization to align synthetic views with scene geometry.

Result: ViDAR outperforms state-of-the-art baselines on DyCheck benchmark in visual quality and geometric consistency, especially in dynamic regions.

Conclusion: ViDAR effectively addresses monocular ambiguity and spatio-temporal inconsistency, setting a new benchmark for motion-rich scene reconstruction.

Abstract: Dynamic Novel View Synthesis aims to generate photorealistic views of moving
subjects from arbitrary viewpoints. This task is particularly challenging when
relying on monocular video, where disentangling structure from motion is
ill-posed and supervision is scarce. We introduce Video Diffusion-Aware
Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages
personalised diffusion models to synthesise a pseudo multi-view supervision
signal for training a Gaussian splatting representation. By conditioning on
scene-specific features, ViDAR recovers fine-grained appearance details while
mitigating artefacts introduced by monocular ambiguity. To address the
spatio-temporal inconsistency of diffusion-based supervision, we propose a
diffusion-aware loss function and a camera pose optimisation strategy that
aligns synthetic views with the underlying scene geometry. Experiments on
DyCheck, a challenging benchmark with extreme viewpoint variation, show that
ViDAR outperforms all state-of-the-art baselines in visual quality and
geometric consistency. We further highlight ViDAR's strong improvement over
baselines on dynamic regions and provide a new benchmark to compare performance
in reconstructing motion-rich parts of the scene. Project page:
https://vidar-4d.github.io

</details>


### [317] [PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision Applications](https://arxiv.org/pdf/2506.18807)
*Pietro Bonazzi, Nicola Farronato, Stefan Zihlmann, Haotong Qi, Michele Magno*

Main category: cs.CV

TL;DR: PicoSAM2 is a lightweight, promptable segmentation model for edge and in-sensor use, achieving high efficiency and performance on devices like the Sony IMX500.


<details>
  <summary>Details</summary>
Motivation: Enable real-time, privacy-aware segmentation for latency-sensitive applications (e.g., smart glasses, IoT) without cloud dependency.

Method: Uses a depthwise separable U-Net with knowledge distillation and fixed-point prompt encoding, learning from SAM2.

Result: Achieves 51.9% mIoU on COCO and 44.9% on LVIS; quantized model runs at 14.3 ms on IMX500 with 1.22MB size. Distillation improves LVIS performance by +3.5% mIoU and +5.1% mAP.

Conclusion: PicoSAM2 proves efficient, promptable segmentation is feasible on-device, enabling privacy-preserving vision without cloud processing.

Abstract: Real-time, on-device segmentation is critical for latency-sensitive and
privacy-aware applications like smart glasses and IoT devices. We introduce
PicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation
model optimized for edge and in-sensor execution, including the Sony IMX500. It
builds on a depthwise separable U-Net, with knowledge distillation and
fixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2).
On COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized
model (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it
the only model meeting both memory and compute constraints for in-sensor
deployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP.
These results demonstrate that efficient, promptable segmentation is feasible
directly on-camera, enabling privacy-preserving vision without cloud or host
processing.

</details>


### [318] [4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation](https://arxiv.org/pdf/2506.18839)
*Chaoyang Wang, Ashkan Mirzaei, Vidit Goel, Willi Menapace, Aliaksandr Siarohin, Avalon Vinella, Michael Vasilkovsky, Ivan Skorokhodov, Vladislav Shakhrai, Sergey Korolev, Sergey Tulyakov, Peter Wonka*

Main category: cs.CV

TL;DR: A novel framework for 4D video and 3D Gaussian particle computation using a fused attention architecture and improved reconstruction methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing 4D video diffusion architectures and enhance 4D generation quality and reconstruction.

Method: Combines a 4D video model with a novel fused attention layer and a 4D reconstruction model with Gaussian head, camera token replacement, and dynamic layers.

Result: Achieves state-of-the-art performance in 4D generation, improving visual quality and reconstruction capability.

Conclusion: The proposed framework advances 4D video and reconstruction by integrating fused attention and dynamic reconstruction techniques.

Abstract: We propose the first framework capable of computing a 4D spatio-temporal grid
of video frames and 3D Gaussian particles for each time step using a
feed-forward architecture. Our architecture has two main components, a 4D video
model and a 4D reconstruction model. In the first part, we analyze current 4D
video diffusion architectures that perform spatial and temporal attention
either sequentially or in parallel within a two-stream design. We highlight the
limitations of existing approaches and introduce a novel fused architecture
that performs spatial and temporal attention within a single layer. The key to
our method is a sparse attention pattern, where tokens attend to others in the
same frame, at the same timestamp, or from the same viewpoint. In the second
part, we extend existing 3D reconstruction algorithms by introducing a Gaussian
head, a camera token replacement algorithm, and additional dynamic layers and
training. Overall, we establish a new state of the art for 4D generation,
improving both visual quality and reconstruction capability.

</details>


### [319] [Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset](https://arxiv.org/pdf/2506.18851)
*Zhuowei Chen, Bingchuan Li, Tianxiang Ma, Lijie Liu, Mingcong Liu, Yi Zhang, Gen Li, Xinghui Li, Siyu Zhou, Qian He, Xinglong Wu*

Main category: cs.CV

TL;DR: Phantom-Data introduces a cross-pair dataset to improve subject-to-video generation by addressing the copy-paste problem, enhancing prompt alignment and visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with textual instruction adherence due to the copy-paste problem caused by in-pair training.

Method: A three-stage pipeline: subject detection, cross-context retrieval, and identity verification to create Phantom-Data.

Result: Training with Phantom-Data improves prompt alignment and visual quality while maintaining identity consistency.

Conclusion: Phantom-Data effectively addresses limitations in subject-to-video generation, offering a scalable solution.

Abstract: Subject-to-video generation has witnessed substantial progress in recent
years. However, existing models still face significant challenges in faithfully
following textual instructions. This limitation, commonly known as the
copy-paste problem, arises from the widely used in-pair training paradigm. This
approach inherently entangles subject identity with background and contextual
attributes by sampling reference images from the same scene as the target
video. To address this issue, we introduce \textbf{Phantom-Data, the first
general-purpose cross-pair subject-to-video consistency dataset}, containing
approximately one million identity-consistent pairs across diverse categories.
Our dataset is constructed via a three-stage pipeline: (1) a general and
input-aligned subject detection module, (2) large-scale cross-context subject
retrieval from more than 53 million videos and 3 billion images, and (3)
prior-guided identity verification to ensure visual consistency under
contextual variation. Comprehensive experiments show that training with
Phantom-Data significantly improves prompt alignment and visual quality while
preserving identity consistency on par with in-pair baselines.

</details>


### [320] [RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base](https://arxiv.org/pdf/2506.18856)
*Kuanning Wang, Yuqian Fu, Tianyu Wang, Yanwei Fu, Longfei Liang, Yu-Gang Jiang, Xiangyang Xue*

Main category: cs.CV

TL;DR: RAG-6DPose is a retrieval-augmented method for 6D pose estimation, using 3D CAD models to enhance accuracy by integrating visual and geometric cues.


<details>
  <summary>Details</summary>
Motivation: Accurate 6D pose estimation is crucial for robotic manipulation tasks like grasping, requiring robust methods to handle occlusions and novel viewpoints.

Method: The approach involves: 1) building a multi-modal CAD knowledge base, 2) retrieving relevant CAD features using the ReSPC module, and 3) refining pose predictions with retrieval-augmented decoding.

Result: The method shows effectiveness and robustness in standard benchmarks and real-world robotic tasks, especially under occlusions and novel viewpoints.

Conclusion: RAG-6DPose advances 6D pose estimation by leveraging CAD models, proving useful for robotic applications.

Abstract: Accurate 6D pose estimation is key for robotic manipulation, enabling precise
object localization for tasks like grasping. We present RAG-6DPose, a
retrieval-augmented approach that leverages 3D CAD models as a knowledge base
by integrating both visual and geometric cues. Our RAG-6DPose roughly contains
three stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D
visual features from multi-view CAD rendered images and also attaching 3D
points; 2) Retrieving relevant CAD features from the knowledge base based on
the current query image via our ReSPC module; and 3) Incorporating retrieved
CAD information to refine pose predictions via retrieval-augmented decoding.
Experimental results on standard benchmarks and real-world robotic tasks
demonstrate the effectiveness and robustness of our approach, particularly in
handling occlusions and novel viewpoints. Supplementary material is available
on our project website: https://sressers.github.io/RAG-6DPose .

</details>


### [321] [Light of Normals: Unified Feature Representation for Universal Photometric Stereo](https://arxiv.org/pdf/2506.18882)
*Hong Li, Houyuan Chen, Chongjie Ye, Zhaoxi Chen, Bohan Li, Shaocong Xu, Xianda Guo, Xuhui Liu, Yikai Wang, Baochang Zhang, Satoshi Ikehata, Boxin Shi, Anyi Rao, Hao Zhao*

Main category: cs.CV

TL;DR: Universal photometric stereo aims to recover surface normals under arbitrary lighting, but faces challenges in decoupling illumination from normals and preserving high-frequency details.


<details>
  <summary>Details</summary>
Motivation: The need to recover accurate surface normals under varying lighting without relying on specific illumination models drives this research.

Method: Not explicitly detailed in the abstract, but mentions challenges like deep coupling between illumination and normals, and preserving geometric details.

Result: Recent advances (SDM-UniPS, Uni MS-PS) exist, but fundamental challenges remain unresolved.

Conclusion: The abstract highlights unresolved issues in universal photometric stereo, emphasizing the need for better methods to handle illumination ambiguity and geometric detail preservation.

Abstract: Universal photometric stereo (PS) aims to recover high-quality surface
normals from objects under arbitrary lighting conditions without relying on
specific illumination models. Despite recent advances such as SDM-UniPS and Uni
MS-PS, two fundamental challenges persist: 1) the deep coupling between varying
illumination and surface normal features, where ambiguity in observed intensity
makes it difficult to determine whether brightness variations stem from
lighting changes or surface orientation; and 2) the preservation of
high-frequency geometric details in complex surfaces, where intricate
geometries create self-shadowing, inter-reflections, and subtle normal
variations that conventional feature processing operations struggle to capture
accurately.

</details>


### [322] [Universal Video Temporal Grounding with Generative Multi-modal Large Language Models](https://arxiv.org/pdf/2506.18883)
*Zeqian Li, Shangzhe Di, Zhonghua Zhai, Weilin Huang, Yanfeng Wang, Weidi Xie*

Main category: cs.CV

TL;DR: UniTime is a universal video temporal grounding model using MLLMs to localize moments in videos from natural language queries, outperforming existing methods across benchmarks and improving VideoQA accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing video grounding methods are limited to specific domains or durations, lacking universality and robustness for diverse video types and complex queries.

Method: UniTime leverages MLLMs, incorporates temporal tokens, and uses adaptive frame scaling to handle videos of varying lengths and granularities.

Result: UniTime surpasses state-of-the-art methods in zero-shot and finetuned settings on five benchmarks and boosts VideoQA accuracy.

Conclusion: UniTime demonstrates robust and universal video temporal grounding, enhancing complex video understanding tasks.

Abstract: This paper presents a computational model for universal video temporal
grounding, which accurately localizes temporal moments in videos based on
natural language queries (e.g., questions or descriptions). Unlike existing
methods that are often limited to specific video domains or durations, we
propose UniTime, a robust and universal video grounding model leveraging the
strong vision-language understanding capabilities of generative Multi-modal
Large Language Models (MLLMs). Our model effectively handles videos of diverse
views, genres, and lengths while comprehending complex language queries. The
key contributions include: (i) We consider steering strong MLLMs for temporal
grounding in videos. To enable precise timestamp outputs, we incorporate
temporal information by interleaving timestamp tokens with video tokens. (ii)
By training the model to handle videos with different input granularities
through adaptive frame scaling, our approach achieves robust temporal grounding
for both short and long videos. (iii) Comprehensive experiments show that
UniTime outperforms state-of-the-art approaches in both zero-shot and
dataset-specific finetuned settings across five public temporal grounding
benchmarks. (iv) When employed as a preliminary moment retriever for long-form
video question-answering (VideoQA), UniTime significantly improves VideoQA
accuracy, highlighting its value for complex video understanding tasks.

</details>


### [323] [4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time](https://arxiv.org/pdf/2506.18890)
*Ziqiao Ma, Xuweiyi Chen, Shoubin Yu, Sai Bi, Kai Zhang, Chen Ziwen, Sihan Xu, Jianing Yang, Zexiang Xu, Kalyan Sunkavalli, Mohit Bansal, Joyce Chai, Hao Tan*

Main category: cs.CV

TL;DR: 4D-LRM is a scalable 4D pretraining model for reconstructing objects from sparse views and times, enabling fast, high-quality rendering of novel views and timestamps.


<details>
  <summary>Details</summary>
Motivation: To learn general space-time representations for reconstructing objects from limited views and times, addressing inefficiencies and limitations of prior 4D approaches.

Method: 4D-LRM predicts per-pixel 4D Gaussian primitives from posed image tokens across time, creating a unified space-time representation for fast rendering.

Result: The model generalizes to novel objects, interpolates time, handles diverse cameras, and reconstructs 24-frame sequences in <1.5s on an A100 GPU.

Conclusion: Scaling spatiotemporal pretraining enables accurate and efficient 4D reconstruction, demonstrating the potential of 4D-LRM.

Abstract: Can we scale 4D pretraining to learn general space-time representations that
reconstruct an object from a few views at some times to any view at any time?
We provide an affirmative answer with 4D-LRM, the first large-scale 4D
reconstruction model that takes input from unconstrained views and timestamps
and renders arbitrary novel view-time combinations. Unlike prior 4D approaches,
e.g., optimization-based, geometry-based, or generative, that struggle with
efficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time
representation and directly predicts per-pixel 4D Gaussian primitives from
posed image tokens across time, enabling fast, high-quality rendering at, in
principle, infinite frame rate. Our results demonstrate that scaling
spatiotemporal pretraining enables accurate and efficient 4D reconstruction. We
show that 4D-LRM generalizes to novel objects, interpolates across time, and
handles diverse camera setups. It reconstructs 24-frame sequences in one
forward pass with less than 1.5 seconds on a single A100 GPU.

</details>


### [324] [FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation](https://arxiv.org/pdf/2506.18899)
*Kaiyi Huang, Yukun Huang, Xintao Wang, Zinan Lin, Xuefei Ning, Pengfei Wan, Di Zhang, Yu Wang, Xihui Liu*

Main category: cs.CV

TL;DR: FilMaster is an AI system for professional film generation, integrating cinematic principles and post-production workflows to create engaging, high-quality films.


<details>
  <summary>Details</summary>
Motivation: Existing AI film generation lacks cinematic principles, leading to unengaging results. FilMaster addresses this by incorporating professional techniques.

Method: FilMaster uses a two-stage process: Reference-Guided Generation (with a RAG module) and Generative Post-Production (with audience-centric rhythm control).

Result: FilMaster outperforms in camera language and cinematic rhythm, validated by the FilmEval benchmark.

Conclusion: FilMaster advances AI in professional filmmaking by combining real-world cinematography and post-production workflows.

Abstract: AI-driven content creation has shown potential in film production. However,
existing film generation systems struggle to implement cinematic principles and
thus fail to generate professional-quality films, particularly lacking diverse
camera language and cinematic rhythm. This results in templated visuals and
unengaging narratives. To address this, we introduce FilMaster, an end-to-end
AI system that integrates real-world cinematic principles for
professional-grade film generation, yielding editable, industry-standard
outputs. FilMaster is built on two key principles: (1) learning cinematography
from extensive real-world film data and (2) emulating professional,
audience-centric post-production workflows. Inspired by these principles,
FilMaster incorporates two stages: a Reference-Guided Generation Stage which
transforms user input to video clips, and a Generative Post-Production Stage
which transforms raw footage into audiovisual outputs by orchestrating visual
and auditory elements for cinematic rhythm. Our generation stage highlights a
Multi-shot Synergized RAG Camera Language Design module to guide the AI in
generating professional camera language by retrieving reference clips from a
vast corpus of 440,000 film clips. Our post-production stage emulates
professional workflows by designing an Audience-Centric Cinematic Rhythm
Control module, including Rough Cut and Fine Cut processes informed by
simulated audience feedback, for effective integration of audiovisual elements
to achieve engaging content. The system is empowered by generative AI models
like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a
comprehensive benchmark for evaluating AI-generated films. Extensive
experiments show FilMaster's superior performance in camera language design and
cinematic rhythm control, advancing generative AI in professional filmmaking.

</details>


### [325] [Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models](https://arxiv.org/pdf/2506.18900)
*Kiymet Akdemir, Tahira Kazimi, Pinar Yanardag*

Main category: cs.CV

TL;DR: A multi-agent framework improves visual consistency in story visualization by iteratively refining panel-level details without regenerating entire sequences.


<details>
  <summary>Details</summary>
Motivation: Maintaining visual consistency of characters and objects across multi-panel story visualizations is challenging, as current methods often fail to preserve key attributes.

Method: A collaborative multi-agent framework autonomously identifies, corrects, and refines inconsistencies, working iteratively with various diffusion models.

Result: The method outperforms prior approaches in multi-panel consistency, as shown in quantitative and qualitative experiments.

Conclusion: The proposed framework effectively enhances visual coherence in story visualization, offering a flexible and model-agnostic solution.

Abstract: Story visualization has become a popular task where visual scenes are
generated to depict a narrative across multiple panels. A central challenge in
this setting is maintaining visual consistency, particularly in how characters
and objects persist and evolve throughout the story. Despite recent advances in
diffusion models, current approaches often fail to preserve key character
attributes, leading to incoherent narratives. In this work, we propose a
collaborative multi-agent framework that autonomously identifies, corrects, and
refines inconsistencies across multi-panel story visualizations. The agents
operate in an iterative loop, enabling fine-grained, panel-level updates
without re-generating entire sequences. Our framework is model-agnostic and
flexibly integrates with a variety of diffusion models, including rectified
flow transformers such as Flux and latent diffusion models such as Stable
Diffusion. Quantitative and qualitative experiments show that our method
outperforms prior approaches in terms of multi-panel consistency.

</details>


### [326] [From Virtual Games to Real-World Play](https://arxiv.org/pdf/2506.18901)
*Wenqiang Sun, Fangyun Wei, Jinjing Zhao, Xi Chen, Zilong Chen, Hongyang Zhang, Jun Zhang, Yan Lu*

Main category: cs.CV

TL;DR: RealPlay is a neural network-based game engine for generating photorealistic, interactive videos from user controls, generalizing beyond training data to diverse real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: To create a system that generates photorealistic, temporally consistent videos in real-time from user inputs, overcoming limitations of prior game-style visual works.

Method: Uses iterative chunk-wise prediction for low-latency feedback, temporal consistency techniques, and trains on labeled game data and unlabeled real-world videos without action annotations.

Result: Achieves realistic, responsive video generation and demonstrates generalization in control and entity transfer, handling diverse real-world entities beyond training scope.

Conclusion: RealPlay successfully bridges virtual and real-world video generation, enabling interactive, photorealistic outputs with broad generalization capabilities.

Abstract: We introduce RealPlay, a neural network-based real-world game engine that
enables interactive video generation from user control signals. Unlike prior
works focused on game-style visuals, RealPlay aims to produce photorealistic,
temporally consistent video sequences that resemble real-world footage. It
operates in an interactive loop: users observe a generated scene, issue a
control command, and receive a short video chunk in response. To enable such
realistic and responsive generation, we address key challenges including
iterative chunk-wise prediction for low-latency feedback, temporal consistency
across iterations, and accurate control response. RealPlay is trained on a
combination of labeled game data and unlabeled real-world videos, without
requiring real-world action annotations. Notably, we observe two forms of
generalization: (1) control transfer-RealPlay effectively maps control signals
from virtual to real-world scenarios; and (2) entity transfer-although training
labels originate solely from a car racing game, RealPlay generalizes to control
diverse real-world entities, including bicycles and pedestrians, beyond
vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/

</details>


### [327] [VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](https://arxiv.org/pdf/2506.18903)
*Runjia Li, Philip Torr, Andrea Vedaldi, Tomas Jakab*

Main category: cs.CV

TL;DR: A novel memory mechanism, Surfel-Indexed View Memory (VMem), is introduced to enhance video generators by efficiently retrieving relevant past views for consistent long-term scene synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing methods either accumulate errors in 3D reconstruction or fail to maintain scene coherence due to short context windows.

Method: VMem indexes past views geometrically using 3D surface elements (surfels) for efficient retrieval during new view generation.

Result: Outperforms existing methods in maintaining scene coherence and camera control while reducing computational costs.

Conclusion: VMem offers a scalable solution for interactive video generation with improved long-term consistency.

Abstract: We propose a novel memory mechanism to build video generators that can
explore environments interactively. Similar results have previously been
achieved by out-painting 2D views of the scene while incrementally
reconstructing its 3D geometry, which quickly accumulates errors, or by video
generators with a short context window, which struggle to maintain scene
coherence over the long term. To address these limitations, we introduce
Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by
indexing them geometrically based on the 3D surface elements (surfels) they
have observed. VMem enables the efficient retrieval of the most relevant past
views when generating new ones. By focusing only on these relevant views, our
method produces consistent explorations of imagined environments at a fraction
of the computational cost of using all past views as context. We evaluate our
approach on challenging long-term scene synthesis benchmarks and demonstrate
superior performance compared to existing methods in maintaining scene
coherence and camera control.

</details>


### [328] [TC-Light: Temporally Consistent Relighting for Dynamic Long Videos](https://arxiv.org/pdf/2506.18904)
*Yang Liu, Chuanchen Luo, Zimo Tang, Yingyan Li, Yuran Yang, Yuanyong Ning, Lue Fan, Junran Peng, Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: TC-Light introduces a two-stage post-optimization method for efficient and temporally consistent video relighting, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing video relighting methods are limited to portraits or suffer from temporal inconsistency and inefficiency, hindering broader applications.

Method: TC-Light uses a two-stage approach: optimizing appearance embedding for global illumination alignment and refining fine-grained details via Unique Video Tensor (UVT).

Result: The method achieves physically plausible relighting with high temporal coherence and low computational cost, validated on a new benchmark.

Conclusion: TC-Light advances video relighting by addressing key limitations, offering practical benefits for content creation and AI data scaling.

Abstract: Editing illumination in long videos with complex dynamics has significant
value in various downstream tasks, including visual content creation and
manipulation, as well as data scaling up for embodied AI through sim2real and
real2real transfer. Nevertheless, existing video relighting techniques are
predominantly limited to portrait videos or fall into the bottleneck of
temporal consistency and computation efficiency. In this paper, we propose
TC-Light, a novel paradigm characterized by the proposed two-stage post
optimization mechanism. Starting from the video preliminarily relighted by an
inflated video relighting model, it optimizes appearance embedding in the first
stage to align global illumination. Then it optimizes the proposed canonical
video representation, i.e., Unique Video Tensor (UVT), to align fine-grained
texture and lighting in the second stage. To comprehensively evaluate
performance, we also establish a long and highly dynamic video benchmark.
Extensive experiments show that our method enables physically plausible
relighting results with superior temporal coherence and low computation cost.
The code and video demos are available at
https://dekuliutesla.github.io/tclight/.

</details>


### [329] [Exploring the Potential of Encoder-free Architectures in 3D LMMs](https://arxiv.org/pdf/2502.09620)
*Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao*

Main category: cs.CV

TL;DR: The paper explores encoder-free architectures for 3D understanding, proposing strategies like LLM-embedded Semantic Encoding and Hierarchical Geometry Aggregation. The resulting model, ENEL, matches state-of-the-art performance without an encoder.


<details>
  <summary>Details</summary>
Motivation: To address challenges in encoder-based 3D LMMs, such as adapting to varying point cloud resolutions and aligning point features with LLM semantics.

Method: Proposes LLM-embedded Semantic Encoding with Hybrid Semantic Loss for pre-training and Hierarchical Geometry Aggregation for instruction tuning.

Result: ENEL, a 7B model, achieves competitive results (55.10%, 50.98%, 43.10%) on classification, captioning, and VQA tasks, rivaling ShapeLLM-13B.

Conclusion: Encoder-free architectures are promising for 3D understanding, potentially replacing encoder-based approaches.

Abstract: Encoder-free architectures have been preliminarily explored in the 2D visual
domain, yet it remains an open question whether they can be effectively applied
to 3D understanding scenarios. In this paper, we present the first
comprehensive investigation into the potential of encoder-free architectures to
alleviate the challenges of encoder-based 3D Large Multimodal Models (LMMs).
These challenges include the failure to adapt to varying point cloud
resolutions and the point features from the encoder not meeting the semantic
needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to
remove the encoder and enable the LLM to assume the role of the 3D encoder: 1)
We propose the LLM-embedded Semantic Encoding strategy in the pre-training
stage, exploring the effects of various point cloud self-supervised losses. And
we present the Hybrid Semantic Loss to extract high-level semantics. 2) We
introduce the Hierarchical Geometry Aggregation strategy in the instruction
tuning stage. This incorporates inductive bias into the LLM layers to focus on
the local details of the point clouds. To the end, we present the first
Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art
model, ShapeLLM-13B, achieving 55.10%, 50.98%, and 43.10% on the
classification, captioning, and VQA tasks, respectively. Our results
demonstrate that the encoder-free architecture is highly promising for
replacing encoder-based architectures in the field of 3D understanding. The
code is released at https://github.com/Ivan-Tang-3D/ENEL

</details>


### [330] [Image Captions are Natural Prompts for Text-to-Image Models](https://arxiv.org/pdf/2307.08526)
*Shiye Lei, Hao Chen, Sen Zhang, Bo Zhao, Dacheng Tao*

Main category: cs.CV

TL;DR: The paper proposes using advanced captioning models to generate informative prompts for synthetic image creation, improving downstream task performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in training models with synthetic data due to data scarcity and privacy concerns, the study explores leveraging large generative models for better training images.

Method: Captions from real images, generated by an advanced captioning model, are combined with class names to prompt generative models for synthesizing training images.

Result: The method enhances synthetic data informativeness, improving model generalization and outperforming real data in out-of-distribution robustness.

Conclusion: The approach demonstrates the potential of synthetic data to surpass real data in certain scenarios, offering benefits in data augmentation and privacy.

Abstract: With the rapid development of Artificial Intelligence Generated Content
(AIGC), it has become a common practice to train models on synthetic data due
to data-scarcity and privacy leakage problems. Owing to massive and diverse
information conveyed in real images, it is challenging for text-to-image
generative models to synthesize informative training data with hand-crafted
prompts. Considering the impressive ability of large generative models, could
such models directly synthesize good training images for prediction tasks with
proper prompts? We offer an affirmative response to this question by proposing
a simple yet effective method, validated through ImageNet classification.
Specifically, we caption each real image with the advanced captioning model to
obtain informative and faithful prompts that extract class-relevant information
and clarify the polysemy of class names. The image captions and class names are
concatenated to prompt generative models for training image synthesis. We show
that this simple caption incorporation significantly boosts the informativeness
of synthetic data therefore enhancing downstream model generalization. More
importantly, besides improvements in data augmentation and privacy
preservation, our experiments demonstrate that synthesized images can exceed
real data in terms of out-of-distribution robustness.

</details>


### [331] [Multi-level Compositional Feature Augmentation for Unbiased Scene Graph Generation](https://arxiv.org/pdf/2308.06712)
*Lin Li, Xingchen Li, Chong Sun, Chen Li, Long Chen*

Main category: cs.CV

TL;DR: The paper introduces MCFA, a feature augmentation strategy to address bias in Scene Graph Generation by enhancing triplet feature diversity at feature and image levels.


<details>
  <summary>Details</summary>
Motivation: Current SGG models are biased towards head predicates due to long-tailed distributions. Existing debiasing methods lack feature diversity.

Method: Proposes Multi-level Compositional Feature Augmentation (MCFA) to diversify triplet features at feature and image levels.

Result: MCFA achieves state-of-the-art performance, improving trade-offs between metrics.

Conclusion: MCFA effectively mitigates bias in SGG by increasing feature diversity, offering a model-agnostic solution.

Abstract: Scene Graph Generation (SGG) aims to detect all the visual relation triplets
<sub, pred, obj> in a given image. With the emergence of various advanced
techniques for better utilizing both the intrinsic and extrinsic information in
each relation triplet, SGG has achieved great progress over the recent years.
However, due to the ubiquitous long-tailed predicate distributions, today's SGG
models are still easily biased to the head predicates. Currently, the most
prevalent debiasing solutions for SGG are re-balancing methods, e.g., changing
the distributions of original training samples. In this paper, we argue that
all existing re-balancing strategies fail to increase the diversity of the
relation triplet features of each predicate, which is critical for robust SGG.
To this end, we propose a novel Multi-level Compositional Feature Augmentation
(MCFA) strategy, which aims to mitigate the bias issue from the perspective of
increasing the diversity of triplet features. Specifically, we enhance
relationship diversity on not only feature-level, i.e., replacing the intrinsic
or extrinsic visual features of triplets with other correlated samples to
create novel feature compositions for tail predicates, but also image-level,
i.e., manipulating the image to generate brand new visual appearance for
triplets. Due to its model-agnostic nature, MCFA can be seamlessly incorporated
into various SGG frameworks. Extensive ablations have shown that MCFA achieves
a new state-of-the-art performance on the trade-off between different metrics.

</details>


### [332] [Multi-entity Video Transformers for Fine-Grained Video Representation Learning](https://arxiv.org/pdf/2311.10873)
*Matthew Walmer, Rose Kanjirathinkal, Kai Sheng Tai, Keyur Muzumdar, Taipeng Tian, Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MV-Former, a Multi-entity Video Transformer, improves self-supervised video representation learning by processing frames as groups of entities, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: To enhance temporally fine-grained video representation learning by better sharing scene information across frames, addressing limitations of late-fusion architectures.

Method: Proposes MV-Former with Learnable Spatial Token Pooling to extract features for multiple salient regions per frame, linking tokens across time.

Result: MV-Former surpasses previous self-supervised methods and some supervised ones, achieving state-of-the-art results on fine-grained video benchmarks.

Conclusion: Parsing videos as collections of entities improves performance, with MV-Former demonstrating superior results, especially with additional pre-training data.

Abstract: The area of temporally fine-grained video representation learning focuses on
generating frame-by-frame representations for temporally dense tasks, such as
fine-grained action phase classification and frame retrieval. In this work, we
advance the state-of-the-art for self-supervised models in this area by
re-examining the design of transformer architectures for video representation
learning. A key aspect of our approach is the improved sharing of scene
information in the temporal pipeline by representing multiple salient entities
per frame. Prior works use late-fusion architectures that reduce frames to a
single-dimensional vector before modeling any cross-frame dynamics. In
contrast, our Multi-entity Video Transformer (MV-Former) processes the frames
as groups of entities represented as tokens linked across time. To achieve
this, we propose a Learnable Spatial Token Pooling strategy to identify and
extract features for multiple salient regions per frame. Through our
experiments, we show that MV-Former outperforms previous self-supervised
methods, and also surpasses some prior works that use additional supervision or
training data. When combined with additional pre-training data from
Kinetics-400, MV-Former achieves a further performance boost. Overall, our
MV-Former achieves state-of-the-art results on multiple fine-grained video
benchmarks and shows that parsing video scenes as collections of entities can
enhance performance in video tasks.

</details>


### [333] [FullLoRA: Efficiently Boosting the Robustness of Pretrained Vision Transformers](https://arxiv.org/pdf/2401.01752)
*Zheng Yuan, Jie Zhang, Shiguang Shan, Xilin Chen*

Main category: cs.CV

TL;DR: The paper introduces FullLoRA, a parameter-efficient adversarial finetuning framework for Vision Transformers (ViTs), enhancing robustness with minimal additional parameters.


<details>
  <summary>Details</summary>
Motivation: Existing large ViT models prioritize performance over robustness, raising security concerns. The paper aims to improve adversarial robustness efficiently.

Method: Proposes LNLoRA (learnable layer normalization + LoRA) and integrates it into ViTs via the FullLoRA framework, keeping the pretrained model frozen.

Result: FullLoRA achieves robustness comparable to full finetuning with only 5% of learnable parameters, addressing storage and training time concerns.

Conclusion: The FullLoRA framework effectively enhances ViT robustness in a parameter-efficient manner, balancing performance and security.

Abstract: In recent years, the Vision Transformer (ViT) model has gradually become
mainstream in various computer vision tasks, and the robustness of the model
has received increasing attention. However, existing large models tend to
prioritize performance during training, potentially neglecting the robustness,
which may lead to serious security concerns. In this paper, we establish a new
challenge: exploring how to use a small number of additional parameters for
adversarial finetuning to quickly and effectively enhance the adversarial
robustness of a standardly trained model. To address this challenge, we develop
novel LNLoRA module, incorporating a learnable layer normalization before the
conventional LoRA module, which helps mitigate magnitude differences in
parameters between the adversarial and standard training paradigms.
Furthermore, we propose the FullLoRA framework by integrating the learnable
LNLoRA modules into all key components of ViT-based models while keeping the
pretrained model frozen, which can significantly improve the model robustness
via adversarial finetuning in a parameter-efficient manner. Extensive
experiments on several datasets demonstrate the superiority of our proposed
FullLoRA framework. It achieves comparable robustness with full finetuning
while only requiring about 5\% of the learnable parameters. This also
effectively addresses concerns regarding extra model storage space and enormous
training time caused by adversarial finetuning.

</details>


### [334] [EDA-DM: Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models](https://arxiv.org/pdf/2401.04585)
*Xuewen Liu, Zhikai Li, Junrui Xiao, Mengjuan Chen, Jianquan Li, Qingyi Gu*

Main category: cs.CV

TL;DR: EDA-DM is a post-training quantization method for diffusion models, addressing distribution mismatch issues at calibration and reconstruction levels, achieving significant speedup and compression with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face challenges in low-latency applications due to lengthy denoising and complex networks. Existing PTQ methods suffer from distribution mismatch issues.

Method: EDA-DM selects calibration samples using latent space feature maps and optimizes block reconstruction with Hessian loss to align quantized and full-precision model outputs.

Result: EDA-DM outperforms existing PTQ methods, achieving 1.83x speedup and 4x compression for Stable-Diffusion on MS-COCO with only a 0.05 CLIP score loss.

Conclusion: EDA-DM effectively addresses PTQ challenges in diffusion models, enabling efficient real-world applications.

Abstract: Diffusion models have achieved great success in image generation tasks.
However, the lengthy denoising process and complex neural networks hinder their
low-latency applications in real-world scenarios. Quantization can effectively
reduce model complexity, and post-training quantization (PTQ), which does not
require fine-tuning, is highly promising for compressing and accelerating
diffusion models. Unfortunately, we find that due to the highly dynamic
activations, existing PTQ methods suffer from distribution mismatch issues at
both calibration sample level and reconstruction output level, which makes the
performance far from satisfactory. In this paper, we propose EDA-DM, a
standardized PTQ method that efficiently addresses the above issues.
Specifically, at the calibration sample level, we extract information from the
density and diversity of latent space feature maps, which guides the selection
of calibration samples to align with the overall sample distribution; and at
the reconstruction output level, we theoretically analyze the reasons for
previous reconstruction failures and, based on this insight, optimize block
reconstruction using the Hessian loss of layers, aligning the outputs of
quantized model and full-precision model at different network granularity.
Extensive experiments demonstrate that EDA-DM significantly outperforms the
existing PTQ methods across various models and datasets. Our method achieves a
1.83 times speedup and 4 times compression for the popular Stable-Diffusion on
MS-COCO, with only a 0.05 loss in CLIP score. Code is available at
http://github.com/BienLuky/EDA-DM .

</details>


### [335] [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/pdf/2505.20897)
*Pingrui Zhang, Yifei Su, Pengyuan Wu, Dong An, Li Zhang, Zhigang Wang, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li*

Main category: cs.CV

TL;DR: The paper introduces Adaptive Text Dreamer (ATD), a language-based method for Vision-and-Language Navigation (VLN) that reduces computational cost and improves efficiency by imagining key environmental semantics via language, leveraging a dual-branch LLM architecture.


<details>
  <summary>Details</summary>
Motivation: VLN agents struggle with aligning perception and language due to partial observability. Existing methods rely on vision-based synthesis, which is computationally expensive and redundant.

Method: ATD uses a dual-branch LLM (left brain for logic, right brain for imagination) with fine-tuned Q-formers to dynamically update reasoning and prediction. A cross-interaction mechanism regularizes outputs and integrates them into a navigation expert.

Result: ATD achieves state-of-the-art performance on the R2R benchmark with fewer parameters.

Conclusion: Language-based imagination via ATD offers a reliable and efficient solution for VLN, outperforming vision-based methods.

Abstract: Vision-and-Language Navigation (VLN) requires the agent to navigate by
following natural instructions under partial observability, making it difficult
to align perception with language. Recent methods mitigate this by imagining
future scenes, yet they rely on vision-based synthesis, leading to high
computational cost and redundant details. To this end, we propose to adaptively
imagine key environmental semantics via \textit{language} form, enabling a more
reliable and efficient strategy. Specifically, we introduce a novel Adaptive
Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a
large language model (LLM). ATD is designed with a human-like left-right brain
architecture, where the left brain focuses on logical integration, and the
right brain is responsible for imaginative prediction of future scenes. To
achieve this, we fine-tune only the Q-former within both brains to efficiently
activate domain-specific knowledge in the LLM, enabling dynamic updates of
logical reasoning and imagination during navigation. Furthermore, we introduce
a cross-interaction mechanism to regularize the imagined outputs and inject
them into a navigation expert module, allowing ATD to jointly exploit both the
reasoning capacity of the LLM and the expertise of the navigation model. We
conduct extensive experiments on the R2R benchmark, where ATD achieves
state-of-the-art performance with fewer parameters. The code is
\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.

</details>


### [336] [Disentangling representations of retinal images with generative models](https://arxiv.org/pdf/2402.19186)
*Sarah Mller, Lisa M. Koch, Hendrik P. A. Lensch, Philipp Berens*

Main category: cs.CV

TL;DR: A method to disentangle patient attributes from camera effects in retinal fundus images for reliable AI applications, using a disentanglement loss based on distance correlation.


<details>
  <summary>Details</summary>
Motivation: Technical factors like camera type in retinal fundus images can mislead AI models, causing them to learn shortcuts instead of causal relationships.

Method: Proposes a population model with a disentanglement loss based on distance correlation to separate patient attributes from camera effects.

Result: Models encode desired information in disentangled subspaces, enabling controllable and realistic image generation.

Conclusion: The method effectively disentangles confounding factors, improving reliability for AI in ophthalmology.

Abstract: Retinal fundus images play a crucial role in the early detection of eye
diseases. However, the impact of technical factors on these images can pose
challenges for reliable AI applications in ophthalmology. For example, large
fundus cohorts are often confounded by factors like camera type, bearing the
risk of learning shortcuts rather than the causal relationships behind the
image generation process. Here, we introduce a population model for retinal
fundus images that effectively disentangles patient attributes from camera
effects, enabling controllable and highly realistic image generation. To
achieve this, we propose a disentanglement loss based on distance correlation.
Through qualitative and quantitative analyses, we show that our models encode
desired information in disentangled subspaces and enable controllable image
generation based on the learned subspaces, demonstrating the effectiveness of
our disentanglement loss. The project's code is publicly available:
https://github.com/berenslab/disentangling-retinal-images.

</details>


### [337] [Leveraging Foundation Models for Content-Based Image Retrieval in Radiology](https://arxiv.org/pdf/2403.06567)
*Stefan Denner, David Zimmerer, Dimitrios Bounias, Markus Bujotzek, Shuhan Xiao, Raphael Stock, Lisa Kausch, Philipp Schader, Tobias Penzkofer, Paul F. Jger, Klaus Maier-Hein*

Main category: cs.CV

TL;DR: The paper proposes using vision foundation models for content-based image retrieval (CBIR) in radiology, demonstrating their effectiveness without additional training and benchmarking them on a large dataset.


<details>
  <summary>Details</summary>
Motivation: Current CBIR systems are limited by their specialization to certain pathologies, reducing their utility. Vision foundation models offer general-purpose features, making them promising for versatile medical image retrieval.

Method: The study benchmarks various vision foundation models on 1.6 million radiological images across four modalities and 161 pathologies, focusing on weakly-supervised models like BiomedCLIP.

Result: BiomedCLIP achieved high retrieval performance (P@1: 0.594) without additional training, comparable to specialized CBIR systems. The study also analyzed embedding spaces and retrieval challenges.

Conclusion: Foundation models show great potential for CBIR in radiology, enabling versatile, general-purpose retrieval systems without specific tuning.

Abstract: Content-based image retrieval (CBIR) has the potential to significantly
improve diagnostic aid and medical research in radiology. However, current CBIR
systems face limitations due to their specialization to certain pathologies,
limiting their utility. On the other hand, several vision foundation models
have been shown to produce general-purpose visual features. Therefore, in this
work, we propose using vision foundation models as powerful and versatile
off-the-shelf feature extractors for content-based image retrieval. Our
contributions include: (1) benchmarking a diverse set of vision foundation
models on an extensive dataset comprising 1.6 million 2D radiological images
across four modalities and 161 pathologies; (2) identifying weakly-supervised
models, particularly BiomedCLIP, as highly effective, achieving a achieving a
P@1 of up to 0.594 (P@3: 0.590, P@5: 0.588, P@10: 0.583), comparable to
specialized CBIR systems but without additional training; (3) conducting an
in-depth analysis of the impact of index size on retrieval performance; (4)
evaluating the quality of embedding spaces generated by different models; and
(5) investigating specific challenges associated with retrieving anatomical
versus pathological structures. Despite these challenges, our research
underscores the vast potential of foundation models for CBIR in radiology,
proposing a shift towards versatile, general-purpose medical image retrieval
systems that do not require specific tuning. Our code, dataset splits and
embeddings are publicly available under
https://github.com/MIC-DKFZ/foundation-models-for-cbmir.

</details>


### [338] [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/pdf/2505.21755)
*Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira*

Main category: cs.CV

TL;DR: The paper introduces FRAMES-VQA, a benchmark for evaluating robust fine-tuning in VQA tasks across multi-modal data shifts, using existing datasets and analyzing distribution shifts and modality interactions.


<details>
  <summary>Details</summary>
Motivation: Current VQA evaluation settings lack insight into multi-modal data shifts, necessitating a robust benchmark to assess fine-tuning methods in diverse scenarios.

Method: The study uses ten VQA benchmarks, categorizes them into ID and OOD datasets, compares fine-tuning methods, and quantifies shifts using Mahalanobis distance on uni- and multi-modal embeddings.

Result: The analysis provides insights into modality interactions and importance, guiding the development of robust fine-tuning methods for multi-modal shifts.

Conclusion: FRAMES-VQA offers a comprehensive framework for evaluating and improving VQA robustness in multi-modal contexts, with potential for future method enhancements.

Abstract: Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .

</details>


### [339] [CLIP-GS: CLIP-Informed Gaussian Splatting for View-Consistent 3D Indoor Semantic Understanding](https://arxiv.org/pdf/2404.14249)
*Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Qing Li, Kanglin Liu*

Main category: cs.CV

TL;DR: CLIP-GS improves 3D semantic understanding in indoor scenes by combining 3D Gaussian Splatting with CLIP, using SAC for compact semantic representations and 3DCR for consistency, achieving high efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D semantic understanding suffer from inefficiency and inconsistency due to high-dimensional embeddings and view-inconsistent supervision.

Method: Proposes CLIP-GS with Semantic Attribute Compactness (SAC) for efficient rendering and 3D Coherent Regularization (3DCR) for semantic consistency in 2D and 3D domains.

Result: Achieves mIoU improvements of 21.20% and 13.05% on ScanNet and Replica datasets, with real-time rendering (>100 FPS) and robustness to sparse data.

Conclusion: CLIP-GS outperforms state-of-the-art methods in efficiency, accuracy, and robustness for 3D semantic understanding.

Abstract: Exploiting 3D Gaussian Splatting (3DGS) with Contrastive Language-Image
Pre-Training (CLIP) models for open-vocabulary 3D semantic understanding of
indoor scenes has emerged as an attractive research focus. Existing methods
typically attach high-dimensional CLIP semantic embeddings to 3D Gaussians and
leverage view-inconsistent 2D CLIP semantics as Gaussian supervision, resulting
in efficiency bottlenecks and deficient 3D semantic consistency. To address
these challenges, we present CLIP-GS, efficiently achieving a coherent semantic
understanding of 3D indoor scenes via the proposed Semantic Attribute
Compactness (SAC) and 3D Coherent Regularization (3DCR). SAC approach exploits
the naturally unified semantics within objects to learn compact, yet effective,
semantic Gaussian representations, enabling highly efficient rendering (>100
FPS). 3DCR enforces semantic consistency in 2D and 3D domains: In 2D, 3DCR
utilizes refined view-consistent semantic outcomes derived from 3DGS to
establish cross-view coherence constraints; in 3D, 3DCR encourages features
similar among 3D Gaussian primitives associated with the same object, leading
to more precise and coherent segmentation results. Extensive experimental
results demonstrate that our method remarkably suppresses existing
state-of-the-art approaches, achieving mIoU improvements of 21.20% and 13.05%
on ScanNet and Replica datasets, respectively, while maintaining real-time
rendering speed. Furthermore, our approach exhibits superior performance even
with sparse input data, substantiating its robustness.

</details>


### [340] [Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion](https://arxiv.org/pdf/2405.03177)
*Yunfeng Li, Bo Wang, Ye Li*

Main category: cs.CV

TL;DR: CSTNet improves RGB-T tracking by directly fusing cross-modal features using ViT backbone with JSCFM and SFM modules, achieving state-of-the-art performance. CSTNet-small offers a faster variant with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-T tracking methods fail to fully exploit cross-modal features or lack direct interaction between template and search area, limiting semantic information utilization.

Method: Proposes CSTNet with ViT backbone, JSCFM for joint channel-spatial fusion, and SFM for cross-modal feature learning. CSTNet-small is a simplified, faster variant.

Result: CSTNet achieves state-of-the-art performance; CSTNet-small offers 50% speedup with slight performance drop (1-2%). Both achieve real-time speeds (21 fps and 33 fps).

Conclusion: CSTNet effectively addresses RGB-T tracking limitations, balancing performance and speed for practical deployment.

Abstract: The main problem in RGB-T tracking is the correct and optimal merging of the
cross-modal features of visible and thermal images. Some previous methods
either do not fully exploit the potential of RGB and TIR information for
channel and spatial feature fusion or lack a direct interaction between the
template and the search area, which limits the model's ability to fully utilize
the original semantic information of both modalities. To address these
limitations, we investigate how to achieve a direct fusion of cross-modal
channels and spatial features in RGB-T tracking and propose CSTNet. It uses the
Vision Transformer (ViT) as the backbone and adds a Joint Spatial and Channel
Fusion Module (JSCFM) and Spatial Fusion Module (SFM) integrated between the
transformer blocks to facilitate cross-modal feature interaction. The JSCFM
module achieves joint modeling of channel and multi-level spatial features. The
SFM module includes a cross-attention-like architecture for cross modeling and
joint learning of RGB and TIR features. Comprehensive experiments show that
CSTNet achieves state-of-the-art performance. To enhance practicality, we
retrain the model without JSCFM and SFM modules and use CSNet as the
pretraining weight, and propose CSTNet-small, which achieves 50% speedup with
an average decrease of 1-2% in SR and PR performance. CSTNet and CSTNet-small
achieve real-time speeds of 21 fps and 33 fps on the Nvidia Jetson Xavier,
meeting actual deployment requirements. Code is available at
https://github.com/LiYunfengLYF/CSTNet.

</details>


### [341] [PotatoGANs: Utilizing Generative Adversarial Networks, Instance Segmentation, and Explainable AI for Enhanced Potato Disease Identification and Classification](https://arxiv.org/pdf/2405.07332)
*Fatema Tuj Johora Faria, Mukaffi Bin Moin, Mohammad Shafiul Alam, Ahmed Al Wase, Md. Rabius Sani, Khan Md Hasib*

Main category: cs.CV

TL;DR: The paper introduces PotatoGANs, a novel data augmentation method using GANs to generate synthetic potato disease images, improving model generalization and reducing overfitting in agricultural disease segmentation.


<details>
  <summary>Details</summary>
Motivation: Overfitting in deep learning for agricultural disease segmentation limits performance under new conditions, impacting potato farming yields. Traditional augmentation methods lack effectiveness.

Method: PotatoGANs employs two GANs (CycleGAN and Pix2Pix) to create synthetic disease images from healthy ones, evaluated using Inception scores. Explainable AI algorithms (GradCAM, GradCAM++, ScoreCAM) are combined with CNNs for classification.

Result: CycleGAN outperforms Pix2Pix with higher Inception scores (1.2001 for black scurf, 1.0900 for common scab), enhancing data diversity and generalization. The synthetic data improves neural network training and reduces collection costs.

Conclusion: PotatoGANs effectively addresses overfitting and generalization issues in agricultural disease segmentation, offering a cost-efficient and interpretable solution for potato disease identification.

Abstract: Numerous applications have resulted from the automation of agricultural
disease segmentation using deep learning techniques. However, when applied to
new conditions, these applications frequently face the difficulty of
overfitting, resulting in lower segmentation performance. In the context of
potato farming, where diseases have a large influence on yields, it is critical
for the agricultural economy to quickly and properly identify these diseases.
Traditional data augmentation approaches, such as rotation, flip, and
translation, have limitations and frequently fail to provide strong
generalization results. To address these issues, our research employs a novel
approach termed as PotatoGANs. In this novel data augmentation approach, two
types of Generative Adversarial Networks (GANs) are utilized to generate
synthetic potato disease images from healthy potato images. This approach not
only expands the dataset but also adds variety, which helps to enhance model
generalization. Using the Inception score as a measure, our experiments show
the better quality and realisticness of the images created by PotatoGANs,
emphasizing their capacity to resemble real disease images closely. The
CycleGAN model outperforms the Pix2Pix GAN model in terms of image quality, as
evidenced by its higher IS scores CycleGAN achieves higher Inception scores
(IS) of 1.2001 and 1.0900 for black scurf and common scab, respectively. This
synthetic data can significantly improve the training of large neural networks.
It also reduces data collection costs while enhancing data diversity and
generalization capabilities. Our work improves interpretability by combining
three gradient-based Explainable AI algorithms (GradCAM, GradCAM++, and
ScoreCAM) with three distinct CNN architectures (DenseNet169, Resnet152 V2,
InceptionResNet V2) for potato disease classification.

</details>


### [342] [Steerable Transformers for Volumetric Data](https://arxiv.org/pdf/2405.15932)
*Soumyabrata Kundu, Risi Kondor*

Main category: cs.CV

TL;DR: Steerable Transformers extend Vision Transformers to maintain equivariance to the special Euclidean group, improving performance in 2D and 3D tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of steerable convolutional networks by incorporating equivariant attention mechanisms.

Method: Proposes an equivariant attention mechanism using steerable convolutions and Fourier space non-linearities.

Result: Adding steerable transformer layers to steerable convolutional networks improves performance in 2D and 3D experiments.

Conclusion: Steerable Transformers effectively combine equivariance and attention, boosting performance in vision tasks.

Abstract: We introduce Steerable Transformers, an extension of the Vision Transformer
mechanism that maintains equivariance to the special Euclidean group
$\mathrm{SE}(d)$. We propose an equivariant attention mechanism that operates
on features extracted by steerable convolutions. Operating in Fourier space,
our network utilizes Fourier space non-linearities. Our experiments in both two
and three dimensions show that adding steerable transformer layers to steerable
convolutional networks enhances performance.

</details>


### [343] [S4Fusion: Saliency-aware Selective State Space Model for Infrared Visible Image Fusion](https://arxiv.org/pdf/2405.20881)
*Haolong Ma, Hui Li, Chunyang Cheng, Gaoang Wang, Xiaoning Song, Xiaojun Wu*

Main category: cs.CV

TL;DR: Proposes S4Fusion, a model using SSSM for infrared and visible image fusion, addressing global spatial information capture and saliency preservation.


<details>
  <summary>Details</summary>
Motivation: Current methods underestimate SSSM's potential in capturing global spatial info in image fusion, leading to biased results.

Method: Introduces S4Fusion with Cross-Modal Spatial Awareness Module (CMSA) and uncertainty minimization via pre-trained networks.

Result: Produces high-quality fused images and improves downstream task performance.

Conclusion: S4Fusion effectively integrates complementary info and adaptively highlights salient targets.

Abstract: As one of the tasks in Image Fusion, Infrared and Visible Image Fusion aims
to integrate complementary information captured by sensors of different
modalities into a single image. The Selective State Space Model (SSSM), known
for its ability to capture long-range dependencies, has demonstrated its
potential in the field of computer vision. However, in image fusion, current
methods underestimate the potential of SSSM in capturing the global spatial
information of both modalities. This limitation prevents the simultaneous
consideration of the global spatial information from both modalities during
interaction, leading to a lack of comprehensive perception of salient targets.
Consequently, the fusion results tend to bias towards one modality instead of
adaptively preserving salient targets. To address this issue, we propose the
Saliency-aware Selective State Space Fusion Model (S4Fusion). In our S4Fusion,
the designed Cross-Modal Spatial Awareness Module (CMSA) can simultaneously
focus on global spatial information from both modalities while facilitating
their interaction, thereby comprehensively capturing complementary information.
Additionally, S4Fusion leverages a pre-trained network to perceive uncertainty
in the fused images. By minimizing this uncertainty, S4Fusion adaptively
highlights salient targets from both images. Extensive experiments demonstrate
that our approach produces high-quality images and enhances performance in
downstream tasks.

</details>


### [344] [MDeRainNet: An Efficient Macro-pixel Image Rain Removal Network](https://arxiv.org/pdf/2406.10652)
*Tao Yan, Weijiang He, Chenglong Wang, Xiangjie Zhu, Yinghui Wang, Rynson W. H. Lau*

Main category: cs.CV

TL;DR: The paper introduces MDeRainNet, a network for rain streak removal in light field (LF) images, leveraging multi-scale encoder-decoder architecture and a novel Transformer-based module for global correlation modeling. It also proposes a semi-supervised learning framework for better generalization.


<details>
  <summary>Details</summary>
Motivation: Rain degrades image quality, hindering computer vision systems. LF images offer multiple sub-views and depth information, but existing methods underutilize these features, leading to sub-optimal rain removal.

Method: Proposes MDeRainNet with a multi-scale encoder-decoder on Macro-pixel images, an Extended Spatial-Angular Interaction (ESAI) module, and a Transformer-based Spatial-Angular Interaction Attention (SAIA) block. Uses semi-supervised learning with multi-level KL loss and contrastive regularization.

Result: Outperforms state-of-the-art methods on synthetic and real-world LF images, achieving better quantitative and qualitative results.

Conclusion: MDeRainNet effectively removes rain streaks by leveraging LF data's global correlations and angular information, with improved generalization through semi-supervised learning.

Abstract: Since rainy weather always degrades image quality and poses significant
challenges to most computer vision-based intelligent systems, image de-raining
has been a hot research topic. Fortunately, in a rainy light field (LF) image,
background obscured by rain streaks in one sub-view may be visible in the other
sub-views, and implicit depth information and recorded 4D structural
information may benefit rain streak detection and removal. However, existing LF
image rain removal methods either do not fully exploit the global correlations
of 4D LF data or only utilize partial sub-views, resulting in sub-optimal rain
removal performance and no-equally good quality for all de-rained sub-views. In
this paper, we propose an efficient network, called MDeRainNet, for rain streak
removal from LF images. The proposed network adopts a multi-scale
encoder-decoder architecture, which directly works on Macro-pixel images (MPIs)
to improve the rain removal performance. To fully model the global correlation
between the spatial and the angular information, we propose an Extended
Spatial-Angular Interaction (ESAI) module to merge them, in which a simple and
effective Transformer-based Spatial-Angular Interaction Attention (SAIA) block
is also proposed for modeling long-range geometric correlations and making full
use of the angular information. Furthermore, to improve the generalization
performance of our network on real-world rainy scenes, we propose a novel
semi-supervised learning framework for our MDeRainNet, which utilizes
multi-level KL loss to bridge the domain gap between features of synthetic and
real-world rain streaks and introduces colored-residue image guided contrastive
regularization to reconstruct rain-free images. Extensive experiments conducted
on synthetic and real-world LFIs demonstrate that our method outperforms the
state-of-the-art methods both quantitatively and qualitatively.

</details>


### [345] [Towards Reflected Object Detection: A Benchmark](https://arxiv.org/pdf/2407.05575)
*Yiquan Wu, Zhongtian Wang, You Wu, Ling Huang, Hui Zhou, Shuiwang Li*

Main category: cs.CV

TL;DR: The paper introduces a new benchmark (RODD) for reflected object detection, addressing a gap in current object detection research. It includes a diverse dataset and baseline results from adapted models.


<details>
  <summary>Details</summary>
Motivation: Reflected object detection is underexplored despite its ubiquity in daily life and importance for applications. Existing methods struggle with this task.

Method: The authors created RODD, a dataset with 21,059 images of real and reflected objects across 10 categories, annotated with bounding boxes and classifications. They adapted five state-of-the-art object detection models for baseline results.

Result: Experiments show existing methods perform poorly on reflected object detection, indicating the need for specialized approaches.

Conclusion: RODD aims to advance research in reflected object detection by providing a benchmark and encouraging development of specialized methods. Dataset and code are publicly available.

Abstract: Object detection has greatly improved over the past decade thanks to advances
in deep learning and large-scale datasets. However, detecting objects reflected
in surfaces remains an underexplored area. Reflective surfaces are ubiquitous
in daily life, appearing in homes, offices, public spaces, and natural
environments. Accurate detection and interpretation of reflected objects are
essential for various applications. This paper addresses this gap by
introducing a extensive benchmark specifically designed for Reflected Object
Detection. Our Reflected Object Detection Dataset (RODD) features a diverse
collection of images showcasing reflected objects in various contexts,
providing standard annotations for both real and reflected objects. This
distinguishes it from traditional object detection benchmarks. RODD encompasses
10 categories and includes 21,059 images of real and reflected objects across
different backgrounds, complete with standard bounding box annotations and the
classification of objects as real or reflected. Additionally, we present
baseline results by adapting five state-of-the-art object detection models to
address this challenging task. Experimental results underscore the limitations
of existing methods when applied to reflected object detection, highlighting
the need for specialized approaches. By releasing RODD, we aim to support and
advance future research on detecting reflected objects. Dataset and code are
available at: https://github.com/jirouvan/ROD.

</details>


### [346] [DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training](https://arxiv.org/pdf/2407.09174)
*Chen Xin, Andreas Hartel, Enkelejda Kasneci*

Main category: cs.CV

TL;DR: DART is an automated end-to-end pipeline for object detection, eliminating manual annotation and data collection while achieving high accuracy. It includes data diversification, annotation, review, and training stages, tested on a construction machines dataset with significant AP improvement.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection methods require laborious manual annotation and struggle with adaptability. DART aims to automate the workflow and improve accuracy in dynamic environments.

Method: DART uses DreamBooth with SDXL for data diversification, Grounding DINO for annotation, InternVL-1.5 and GPT-4o for review, and YOLOv8/v10 for training.

Result: DART increased average precision (AP) from 0.064 to 0.832 on the Liebherr Product dataset.

Conclusion: DART's modular design allows for easy upgrades and adaptability, making it a scalable solution for object detection without manual intervention.

Abstract: Accurate real-time object detection is vital across numerous industrial
applications, from safety monitoring to quality control. Traditional
approaches, however, are hindered by arduous manual annotation and data
collection, struggling to adapt to ever-changing environments and novel target
objects. To address these limitations, this paper presents DART, an innovative
automated end-to-end pipeline that revolutionizes object detection workflows
from data collection to model evaluation. It eliminates the need for laborious
human labeling and extensive data collection while achieving outstanding
accuracy across diverse scenarios. DART encompasses four key stages: (1) Data
Diversification using subject-driven image generation (DreamBooth with SDXL),
(2) Annotation via open-vocabulary object detection (Grounding DINO) to
generate bounding box and class labels, (3) Review of generated images and
pseudo-labels by large multimodal models (InternVL-1.5 and GPT-4o) to guarantee
credibility, and (4) Training of real-time object detectors (YOLOv8 and
YOLOv10) using the verified data. We apply DART to a self-collected dataset of
construction machines named Liebherr Product, which contains over 15K
high-quality images across 23 categories. The current instantiation of DART
significantly increases average precision (AP) from 0.064 to 0.832. Its modular
design ensures easy exchangeability and extensibility, allowing for future
algorithm upgrades, seamless integration of new object categories, and
adaptability to customized environments without manual labeling and additional
data collection. The code and dataset are released at
https://github.com/chen-xin-94/DART.

</details>


### [347] [Improved Baselines with Synchronized Encoding for Universal Medical Image Segmentation](https://arxiv.org/pdf/2408.09886)
*Sihan Yang, Jiadong Feng, Xuande Mi, Haixia Bi, Hai Zhang, Jian Sun*

Main category: cs.CV

TL;DR: SyncSAM introduces a synchronized dual-branch encoder and multi-scale decoder for medical image segmentation, achieving state-of-the-art performance and strong zero-shot capabilities.


<details>
  <summary>Details</summary>
Motivation: The domain gap between natural and medical images limits zero-shot performance of foundation models like SAM in medical segmentation. Existing methods lack domain-specific designs.

Method: SyncSAM uses a synchronized dual-branch encoder (convolution + Transformer) and multi-scale dual-branch decoder, trained on large datasets SA-Med2D-20M and IMed-361M.

Result: SyncSAM achieves state-of-the-art performance on test sets and strong zero-shot capabilities on unseen datasets.

Conclusion: SyncSAM provides a robust baseline for medical image segmentation, addressing domain gaps and enhancing zero-shot performance.

Abstract: Large foundation models, known for their strong zero-shot generalization
capabilities, can be applied to a wide range of downstream tasks. However,
developing foundation models for medical image segmentation poses a significant
challenge due to the domain gap between natural and medical images. While
fine-tuning techniques based on the Segment Anything Model (SAM) have been
explored, they primarily focus on scaling up data or refining inference
strategies without incorporating domain-specific architectural designs,
limiting their zero-shot performance. To optimize segmentation performance
under standard inference settings and provide a strong baseline for future
research, we introduce SyncSAM, which employs a synchronized dual-branch
encoder that integrates convolution and Transformer features in a synchronized
manner to enhance medical image encoding, and a multi-scale dual-branch decoder
to preserve image details. SyncSAM is trained on two of the largest medical
image segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series
of pre-trained models for universal medical image segmentation. Experimental
results demonstrate that SyncSAM not only achieves state-of-the-art performance
on test sets but also exhibits strong zero-shot capabilities on unseen
datasets. Code and checkpoints are available at
https://github.com/Hhankyangg/SyncSAM.

</details>


### [348] [Visual Prompt Engineering for Vision Language Models in Radiology](https://arxiv.org/pdf/2408.15802)
*Stefan Denner, Markus Bujotzek, Dimitrios Bounias, David Zimmerer, Raphael Stock, Klaus Maier-Hein*

Main category: cs.CV

TL;DR: The paper explores using visual markers (e.g., arrows, bounding boxes) in radiological images to improve zero-shot classification performance and interpretability, showing significant AUROC improvements.


<details>
  <summary>Details</summary>
Motivation: Medical image classification models often lack adaptability to new conditions and fail to focus on localized pathology regions, which is critical for radiology.

Method: Incorporates visual cues into zero-shot classification using CLIP, embedding markers like arrows and bounding boxes to guide model attention.

Result: Visual markers improve AUROC by up to 0.185 across four chest X-ray datasets, enhancing classification performance and interpretability.

Conclusion: Visual cues effectively guide model attention to clinically relevant areas, improving both accuracy and interpretability in medical imaging.

Abstract: Medical image classification plays a crucial role in clinical
decision-making, yet most models are constrained to a fixed set of predefined
classes, limiting their adaptability to new conditions. Contrastive
Language-Image Pretraining (CLIP) offers a promising solution by enabling
zero-shot classification through multimodal large-scale pretraining. However,
while CLIP effectively captures global image content, radiology requires a more
localized focus on specific pathology regions to enhance both interpretability
and diagnostic accuracy. To address this, we explore the potential of
incorporating visual cues into zero-shot classification, embedding visual
markers, such as arrows, bounding boxes, and circles, directly into
radiological images to guide model attention. Evaluating across four public
chest X-ray datasets, we demonstrate that visual markers improve AUROC by up to
0.185, highlighting their effectiveness in enhancing classification
performance. Furthermore, attention map analysis confirms that visual cues help
models focus on clinically relevant areas, leading to more interpretable
predictions.To support further research, we use public datasets and provide our
codebase and preprocessing pipeline under
https://github.com/MIC-DKFZ/VPE-in-Radiology, serving as a reference point for
future work on localized classification in medical imaging.

</details>


### [349] [Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models](https://arxiv.org/pdf/2410.03039)
*Xiaoyu Wu, Jiaru Zhang, Zhiwei Steven Wu*

Main category: cs.CV

TL;DR: FineXtract is a framework to extract fine-tuning data from shared Diffusion Models (DMs), revealing data leakage and copyright risks.


<details>
  <summary>Details</summary>
Motivation: To investigate if training data can be extracted from fine-tuned DMs shared online, posing data leakage and copyright concerns.

Method: Approximates fine-tuning as a distribution shift, extrapolates models pre/post fine-tuning, and clusters generated images to extract data.

Result: Extracts ~20% of fine-tuning data from datasets like WikiArt and DreamBooth, validated on real-world checkpoints.

Conclusion: FineXtract successfully extracts fine-tuning data, highlighting risks of sharing DMs and potential copyright violations.

Abstract: Diffusion Models (DMs) have become powerful image generation tools,
especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a
small image set to capture specific styles or objects. Many people upload these
personalized checkpoints online, fostering communities such as Civitai and
HuggingFace. However, model owners may overlook the data leakage risks when
releasing fine-tuned checkpoints. Moreover, concerns regarding copyright
violations arise when unauthorized data is used during fine-tuning. In this
paper, we ask: "Can training data be extracted from these fine-tuned DMs shared
online?" A successful extraction would present not only data leakage threats
but also offer tangible evidence of copyright infringement. To answer this, we
propose FineXtract, a framework for extracting fine-tuning data. Our method
approximates fine-tuning as a gradual shift in the model's learned distribution
-- from the original pretrained DM toward the fine-tuning data. By
extrapolating the models before and after fine-tuning, we guide the generation
toward high-probability regions within the fine-tuned data distribution. We
then apply a clustering algorithm to extract the most probable images from
those generated using this extrapolated guidance. Experiments on DMs fine-tuned
with datasets including WikiArt, DreamBooth, and real-world checkpoints posted
online validate the effectiveness of our method, extracting about 20% of
fine-tuning data in most cases. The code is available
https://github.com/Nicholas0228/FineXtract.

</details>


### [350] [UniDrive: Towards Universal Driving Perception Across Camera Configurations](https://arxiv.org/pdf/2410.13864)
*Ye Li, Wenzhao Zheng, Xiaonan Huang, Kurt Keutzer*

Main category: cs.CV

TL;DR: UniDrive is a framework for vision-centric autonomous driving that generalizes across varying camera configurations using unified virtual cameras and ground-aware projection.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying autonomous driving models across different car models with varying camera configurations.

Method: Deploys unified virtual cameras, uses ground-aware projection, and optimizes virtual configurations to minimize projection errors.

Result: The method generalizes well across different camera configurations with minor performance degradation.

Conclusion: UniDrive enhances adaptability and reliability of driving perception models by mitigating camera parameter variability.

Abstract: Vision-centric autonomous driving has demonstrated excellent performance with
economical sensors. As the fundamental step, 3D perception aims to infer 3D
information from 2D images based on 3D-2D projection. This makes driving
perception models susceptible to sensor configuration (e.g., camera intrinsics
and extrinsics) variations. However, generalizing across camera configurations
is important for deploying autonomous driving models on different car models.
In this paper, we present UniDrive, a novel framework for vision-centric
autonomous driving to achieve universal perception across camera
configurations. We deploy a set of unified virtual cameras and propose a
ground-aware projection method to effectively transform the original images
into these unified virtual views. We further propose a virtual configuration
optimization method by minimizing the expected projection error between
original and virtual cameras. The proposed virtual camera projection can be
applied to existing 3D perception methods as a plug-and-play module to mitigate
the challenges posed by camera parameter variability, resulting in more
adaptable and reliable driving perception models. To evaluate the effectiveness
of our framework, we collect a dataset on CARLA by driving the same routes
while only modifying the camera configurations. Experimental results
demonstrate that our method trained on one specific camera configuration can
generalize to varying configurations with minor performance degradation.

</details>


### [351] [How Far is Video Generation from World Model: A Physical Law Perspective](https://arxiv.org/pdf/2411.02385)
*Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng*

Main category: cs.CV

TL;DR: The paper evaluates video generation models' ability to learn physical laws without human priors, revealing limitations in generalization and abstraction.


<details>
  <summary>Details</summary>
Motivation: To assess whether video generation models can autonomously discover and adhere to fundamental physical laws from visual data alone.

Method: Developed a 2D simulation testbed for object movement and collisions, trained diffusion-based models to predict movements, and evaluated performance across in-distribution, out-of-distribution, and combinatorial scenarios.

Result: Models showed perfect in-distribution generalization but failed in out-of-distribution cases, exhibiting case-based generalization and prioritizing factors like color over physical rules.

Conclusion: Scaling alone is insufficient for models to uncover physical laws, highlighting the need for improved generalization mechanisms.

Abstract: OpenAI's Sora highlights the potential of video generation for developing
world models that adhere to fundamental physical laws. However, the ability of
video generation models to discover such laws purely from visual data without
human priors can be questioned. A world model learning the true law should give
predictions robust to nuances and correctly extrapolate on unseen scenarios. In
this work, we evaluate across three key scenarios: in-distribution,
out-of-distribution, and combinatorial generalization. We developed a 2D
simulation testbed for object movement and collisions to generate videos
deterministically governed by one or more classical mechanics laws. This
provides an unlimited supply of data for large-scale experimentation and
enables quantitative evaluation of whether the generated videos adhere to
physical laws. We trained diffusion-based video generation models to predict
object movements based on initial frames. Our scaling experiments show perfect
generalization within the distribution, measurable scaling behavior for
combinatorial generalization, but failure in out-of-distribution scenarios.
Further experiments reveal two key insights about the generalization mechanisms
of these models: (1) the models fail to abstract general physical rules and
instead exhibit "case-based" generalization behavior, i.e., mimicking the
closest training example; (2) when generalizing to new cases, models are
observed to prioritize different factors when referencing training data: color
> size > velocity > shape. Our study suggests that scaling alone is
insufficient for video generation models to uncover fundamental physical laws,
despite its role in Sora's broader success. See our project page at
https://phyworld.github.io

</details>


### [352] [Efficient Feature Aggregation and Scale-Aware Regression for Monocular 3D Object Detection](https://arxiv.org/pdf/2411.02747)
*Yifan Wang, Xiaochen Yang, Fanqi Pu, Qingmin Liao, Wenming Yang*

Main category: cs.CV

TL;DR: MonoASRH introduces a novel monocular 3D detection framework with global awareness and scale-awareness, outperforming existing methods on KITTI and Waymo datasets.


<details>
  <summary>Details</summary>
Motivation: Existing monocular 3D detection methods lack global awareness and struggle with small-scale objects and varying object scales, leading to background noise and degraded features.

Method: MonoASRH uses an Efficient Hybrid Feature Aggregation Module (EH-FAM) for global semantic feature extraction and an Adaptive Scale-Aware 3D Regression Head (ASRH) for dynamic receptive field learning.

Result: MonoASRH achieves state-of-the-art performance on KITTI and Waymo datasets.

Conclusion: The proposed framework effectively addresses limitations of existing methods by integrating global and scale-aware features, improving 3D detection accuracy.

Abstract: Monocular 3D object detection has attracted great attention due to simplicity
and low cost. Existing methods typically follow conventional 2D detection
paradigms, first locating object centers and then predicting 3D attributes via
neighboring features. However, these methods predominantly rely on progressive
cross-scale feature aggregation and focus solely on local information, which
may result in a lack of global awareness and the omission of small-scale
objects. In addition, due to large variation in object scales across different
scenes and depths, inaccurate receptive fields often lead to background noise
and degraded feature representation. To address these issues, we introduces
MonoASRH, a novel monocular 3D detection framework composed of Efficient Hybrid
Feature Aggregation Module (EH-FAM) and Adaptive Scale-Aware 3D Regression Head
(ASRH). Specifically, EH-FAM employs multi-head attention with a global
receptive field to extract semantic features for small-scale objects and
leverages lightweight convolutional modules to efficiently aggregate visual
features across different scales. The ASRH encodes 2D bounding box dimensions
and then fuses scale features with the semantic features aggregated by EH-FAM
through a scale-semantic feature fusion module. The scale-semantic feature
fusion module guides ASRH in learning dynamic receptive field offsets,
incorporating scale priors into 3D position prediction for better
scale-awareness. Extensive experiments on the KITTI and Waymo datasets
demonstrate that MonoASRH achieves state-of-the-art performance.

</details>


### [353] [Cross-Camera Distracted Driver Classification through Feature Disentanglement and Contrastive Learning](https://arxiv.org/pdf/2411.13181)
*Simone Bianco, Luigi Celona, Paolo Napoletano*

Main category: cs.CV

TL;DR: DBMNet improves driver distraction classification by 7% accuracy, handles varying camera views, and performs well in real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Addressing accuracy loss in neural networks when applied to data with different conditions than training data, particularly camera position changes.

Method: Uses a lightweight backbone, disentanglement module to remove camera view bias, and contrastive learning for better action encoding.

Result: Achieves 7% higher Top-1 accuracy, excels in cross-dataset and cross-camera tests, and performs efficiently in deployment.

Conclusion: DBMNet is robust, generalizes well, and is practical for real-world applications due to its compact size and efficiency.

Abstract: The classification of distracted drivers is pivotal for ensuring safe
driving. Previous studies demonstrated the effectiveness of neural networks in
automatically predicting driver distraction, fatigue, and potential hazards.
However, recent research has uncovered a significant loss of accuracy in these
models when applied to samples acquired under conditions that differ from the
training data. In this paper, we introduce a robust model designed to withstand
changes in camera position within the vehicle. Our Driver Behavior Monitoring
Network (DBMNet) relies on a lightweight backbone and integrates a
disentanglement module to discard camera view information from features,
coupled with contrastive learning to enhance the encoding of various driver
actions. Experiments conducted using a leave-one-camera-out protocol on the
daytime and nighttime subsets of the 100-Driver dataset validate the
effectiveness of our approach. Cross-dataset and cross-camera experiments
conducted on three benchmark datasets, namely AUCDD-V1, EZZ2021 and SFD,
demonstrate the superior generalization capabilities of the proposed method.
Overall DBMNet achieves an improvement of 7% in Top-1 accuracy compared to
existing approaches. Moreover, a quantized version of the DBMNet and all
considered methods has been deployed on a Coral Dev Board board. In this
deployment scenario, DBMNet outperforms alternatives, achieving the lowest
average error while maintaining a compact model size, low memory footprint,
fast inference time, and minimal power consumption.

</details>


### [354] [MGHF: Multi-Granular High-Frequency Perceptual Loss for Image Super-Resolution](https://arxiv.org/pdf/2411.13548)
*Shoaib Meraj Sami, Md Mahedi Hasan, Mohammad Saeed Ebrahimi Saadabadi, Jeremy Dawson, Nasser Nasrabadi, Raghuveer Rao*

Main category: cs.CV

TL;DR: The paper proposes an invertible neural network (INN)-based perceptual loss (MGHF-n) and a comprehensive framework (MGHF-c) to improve super-resolution image synthesis by preserving, prioritizing, and regularizing information across multiple perspectives.


<details>
  <summary>Details</summary>
Motivation: Existing perceptual losses in super-resolution often rely on complex architectures and lose information during guidance. The authors aim to address these limitations with a more effective approach.

Method: The method involves an INN-based perceptual loss (MGHF-n) trained on ImageNet and a framework (MGHF-c) with constraints for texture, style, content, and regional detail preservation. Techniques like adaptive entropy-based pruning, Gram matrix loss, and modulated PatchNCE are used.

Result: Experiments show the MGHF framework significantly enhances performance across various super-resolution algorithms, including GAN- and diffusion-based methods.

Conclusion: The proposed MGHF framework effectively overcomes the limitations of existing perceptual losses, improving super-resolution results with better detail preservation and regularization.

Abstract: While different variants of perceptual losses have been employed in
super-resolution literature to synthesize more realistic, appealing, and
detailed high-resolution images, most are convolutional neural networks-based,
causing information loss during guidance and often relying on complicated
architectures and training procedures. We propose an invertible neural network
(INN)-based naive \textbf{M}ulti-\textbf{G}ranular
\textbf{H}igh-\textbf{F}requency (MGHF-n) perceptual loss trained on ImageNet
to overcome these issues. Furthermore, we develop a comprehensive framework
(MGHF-c) with several constraints to preserve, prioritize, and regularize
information across multiple perspectives: texture and style preservation,
content preservation, regional detail preservation, and joint content-style
regularization. Information is prioritized through adaptive entropy-based
pruning and reweighting of INN features. We utilize Gram matrix loss for style
preservation and mean-squared error loss for content preservation.
Additionally, we propose content-style consistency through correlation loss to
regulate unnecessary texture generation while preserving content information.
Since small image regions may contain intricate details, we employ modulated
PatchNCE in the INN features as a local information preservation objective.
Extensive experiments on various super-resolution algorithms, including GAN-
and diffusion-based methods, demonstrate that our MGHF framework significantly
improves performance. After the review process, our code will be released in
the public repository.

</details>


### [355] [DiffDesign: Controllable Diffusion with Meta Prior for Efficient Interior Design Generation](https://arxiv.org/pdf/2411.16301)
*Yuxuan Yang, Tao Geng*

Main category: cs.CV

TL;DR: DiffDesign is a controllable diffusion model for interior design, addressing inefficiencies and discrepancies in generative models by using meta priors and a specialized dataset.


<details>
  <summary>Details</summary>
Motivation: Interior design processes are inefficient and lack generative models tailored to practical needs, leading to discrepancies in outputs.

Method: DiffDesign uses a pre-trained 2D diffusion model with disentangled cross-attention control for design attributes and an alignment module for view consistency, fine-tuned on the DesignHelper dataset.

Result: Experiments show DiffDesign is effective and robust in generating interior designs.

Conclusion: DiffDesign offers a promising solution for efficient and controllable interior design generation.

Abstract: Interior design is a complex and creative discipline involving aesthetics,
functionality, ergonomics, and materials science. Effective solutions must meet
diverse requirements, typically producing multiple deliverables such as
renderings and design drawings from various perspectives. Consequently,
interior design processes are often inefficient and demand significant
creativity. With advances in machine learning, generative models have emerged
as a promising means of improving efficiency by creating designs from text
descriptions or sketches. However, few generative works focus on interior
design, leading to substantial discrepancies between outputs and practical
needs, such as differences in size, spatial scope, and the lack of controllable
generation quality. To address these challenges, we propose DiffDesign, a
controllable diffusion model with meta priors for efficient interior design
generation. Specifically, we utilize the generative priors of a 2D diffusion
model pre-trained on a large image dataset as our rendering backbone. We
further guide the denoising process by disentangling cross-attention control
over design attributes, such as appearance, pose, and size, and introduce an
optimal transfer-based alignment module to enforce view consistency.
Simultaneously, we construct an interior design-specific dataset, DesignHelper,
consisting of over 400 solutions across more than 15 spatial types and 15
design styles. This dataset helps fine-tune DiffDesign. Extensive experiments
conducted on various benchmark datasets demonstrate the effectiveness and
robustness of DiffDesign.

</details>


### [356] [AnchorCrafter: Animate Cyber-Anchors Selling Your Products via Human-Object Interacting Video Generation](https://arxiv.org/pdf/2411.17383)
*Ziyi Xu, Ziyao Huang, Juan Cao, Yong Zhang, Xiaodong Cun, Qing Shuai, Yuchen Wang, Linchao Bao, Jintao Li, Fan Tang*

Main category: cs.CV

TL;DR: AnchorCrafter is a diffusion-based system for generating 2D product promotion videos with high visual fidelity and controllable human-object interactions, outperforming existing methods in appearance preservation and motion consistency.


<details>
  <summary>Details</summary>
Motivation: The challenge of integrating human-object interactions (HOI) into pose-guided human video generation for product promotion videos.

Method: Introduces HOI-appearance perception for multi-view object recognition and HOI-motion injection for complex interactions, addressing object trajectory and occlusion.

Result: Improves object appearance preservation by 7.5% and doubles localization accuracy, while maintaining superior human motion consistency and video quality.

Conclusion: AnchorCrafter effectively addresses HOI challenges in video generation, offering a robust solution for high-quality product promotion videos.

Abstract: The generation of anchor-style product promotion videos presents promising
opportunities in e-commerce, advertising, and consumer engagement. Despite
advancements in pose-guided human video generation, creating product promotion
videos remains challenging. In addressing this challenge, we identify the
integration of human-object interactions (HOI) into pose-guided human video
generation as a core issue. To this end, we introduce AnchorCrafter, a novel
diffusion-based system designed to generate 2D videos featuring a target human
and a customized object, achieving high visual fidelity and controllable
interactions. Specifically, we propose two key innovations: the HOI-appearance
perception, which enhances object appearance recognition from arbitrary
multi-view perspectives and disentangles object and human appearance, and the
HOI-motion injection, which enables complex human-object interactions by
overcoming challenges in object trajectory conditioning and inter-occlusion
management. Extensive experiments show that our system improves object
appearance preservation by 7.5\% and doubles the object localization accuracy
compared to existing state-of-the-art approaches. It also outperforms existing
approaches in maintaining human motion consistency and high-quality video
generation. Project page including data, code, and Huggingface demo:
https://github.com/cangcz/AnchorCrafter.

</details>


### [357] [Human Action CLIPs: Detecting AI-generated Human Motion](https://arxiv.org/pdf/2412.00526)
*Matyas Bohacek, Hany Farid*

Main category: cs.CV

TL;DR: A method using multi-modal semantic embeddings to distinguish real from AI-generated human motion, robust against common attacks like resolution and compression.


<details>
  <summary>Details</summary>
Motivation: To protect against malicious use of AI-generated videos by improving detection of synthetic content.

Method: Multi-modal semantic embeddings for robust detection, tested on the DeepAction dataset.

Result: Effective and robust differentiation between real and AI-generated human motion.

Conclusion: The technique offers a reliable solution for detecting AI-generated videos, with the dataset available for academic use.

Abstract: AI-generated video generation continues its journey through the uncanny
valley to produce content that is increasingly perceptually indistinguishable
from reality. To better protect individuals, organizations, and societies from
its malicious applications, we describe an effective and robust technique for
distinguishing real from AI-generated human motion using multi-modal semantic
embeddings. Our method is robust to the types of laundering that typically
confound more low- to mid-level approaches, including resolution and
compression attacks. This method is evaluated against DeepAction, a
custom-built, open-sourced dataset of video clips with human actions generated
by seven text-to-video AI models and matching real footage. The dataset is
available under an academic license at
https://www.huggingface.co/datasets/faridlab/deepaction_v1.

</details>


### [358] [Thermal Vision: Pioneering Non-Invasive Temperature Tracking in Congested Spaces](https://arxiv.org/pdf/2412.00863)
*Arijit Samal, Haroon R Lone*

Main category: cs.CV

TL;DR: A non-invasive temperature monitoring system using thermal cameras and edge devices is proposed for dense settings, achieving high accuracy in face detection and temperature estimation.


<details>
  <summary>Details</summary>
Motivation: Existing research on non-invasive temperature estimation focuses on sparse settings, but dense settings (e.g., classrooms, theaters) pose higher disease transmission risks, necessitating tailored solutions.

Method: The system combines a thermal camera with an edge device, using YOLO models for face detection and a regression framework for temperature estimation. Evaluated on diverse datasets from dense and sparse settings.

Result: Achieves 84+ mAP in face detection, 0.18C mean square error, and 0.96 R score in temperature estimation.

Conclusion: The system is effective for real-world temperature monitoring in dense settings; dataset and code are publicly released.

Abstract: Non-invasive temperature monitoring of individuals plays a crucial role in
identifying and isolating symptomatic individuals. Temperature monitoring
becomes particularly vital in settings characterized by close human proximity,
often referred to as dense settings. However, existing research on non-invasive
temperature estimation using thermal cameras has predominantly focused on
sparse settings. Unfortunately, the risk of disease transmission is
significantly higher in dense settings like movie theaters or classrooms.
Consequently, there is an urgent need to develop robust temperature estimation
methods tailored explicitly for dense settings.
  Our study proposes a non-invasive temperature estimation system that combines
a thermal camera with an edge device. Our system employs YOLO models for face
detection and utilizes a regression framework for temperature estimation. We
evaluated the system on a diverse dataset collected in dense and sparse
settings. Our proposed face detection model achieves an impressive mAP score of
over 84 in both in-dataset and cross-dataset evaluations. Furthermore, the
regression framework demonstrates remarkable performance with a mean square
error of 0.18$^{\circ}$C and an impressive $R^2$ score of 0.96. Our
experiments' results highlight the developed system's effectiveness,
positioning it as a promising solution for continuous temperature monitoring in
real-world applications. With this paper, we release our dataset and
programming code publicly.

</details>


### [359] [ReNeg: Learning Negative Embedding with Reward Guidance](https://arxiv.org/pdf/2412.19637)
*Xiaomin Li, Yixuan Liu, Takashi Isobe, Xu Jia, Qinpeng Cui, Dong Zhou, Dong Li, You He, Huchuan Lu, Zhongdao Wang, Emad Barsoum*

Main category: cs.CV

TL;DR: ReNeg introduces a method to learn improved negative embeddings for text-to-image generation using reward feedback and classifier-free guidance, outperforming handcrafted and null-text embeddings.


<details>
  <summary>Details</summary>
Motivation: Negative embeddings from user-defined prompts are functional but not optimal. ReNeg aims to learn better embeddings to enhance generation quality.

Method: Uses a reward feedback learning framework and integrates classifier-free guidance (CFG) for training. Proposes strategies for global and per-sample negative embeddings.

Result: Learned negative embeddings outperform null-text and handcrafted ones, improving human preference alignment and generalizing well across models.

Conclusion: ReNeg's learned negative embeddings are effective and transferable, enhancing performance in text-to-image and text-to-video applications.

Abstract: In text-to-image (T2I) generation applications, negative embeddings have
proven to be a simple yet effective approach for enhancing generation quality.
Typically, these negative embeddings are derived from user-defined negative
prompts, which, while being functional, are not necessarily optimal. In this
paper, we introduce ReNeg, an end-to-end method designed to learn improved
Negative embeddings guided by a Reward model. We employ a reward feedback
learning framework and integrate classifier-free guidance (CFG) into the
training process, which was previously utilized only during inference, thus
enabling the effective learning of negative embeddings. We also propose two
strategies for learning both global and per-sample negative embeddings.
Extensive experiments show that the learned negative embedding significantly
outperforms null-text and handcrafted counterparts, achieving substantial
improvements in human preference alignment. Additionally, the negative
embedding learned within the same text embedding space exhibits strong
generalization capabilities. For example, using the same CLIP text encoder, the
negative embedding learned on SD1.5 can be seamlessly transferred to
text-to-image or even text-to-video models such as ControlNet, ZeroScope, and
VideoCrafter2, resulting in consistent performance improvements across the
board.

</details>


### [360] [CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs](https://arxiv.org/pdf/2412.19663)
*Siyu Wang, Cailian Chen, Xinyi Le, Qimin Xu, Lei Xu, Yanzhou Zhang, Jie Yang*

Main category: cs.CV

TL;DR: CAD-GPT introduces a spatial reasoning-enhanced MLLM for CAD model synthesis, improving accuracy in 3D spatial inference over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing CAD methods rely on hard-to-obtain latent vectors or point clouds, and MLLMs struggle with spatial accuracy.

Method: Uses a 3D Modeling Spatial Mechanism to map 3D positions and angles into a 1D linguistic space, discretizing 2D coordinates for precise spatial inference.

Result: CAD-GPT outperforms state-of-the-art methods in CAD model synthesis, both quantitatively and qualitatively.

Conclusion: CAD-GPT enhances CAD model synthesis by addressing spatial inaccuracies in MLLMs, offering a more efficient and precise solution.

Abstract: Computer-aided design (CAD) significantly enhances the efficiency, accuracy,
and innovation of design processes by enabling precise 2D and 3D modeling,
extensive analysis, and optimization. Existing methods for creating CAD models
rely on latent vectors or point clouds, which are difficult to obtain, and
storage costs are substantial. Recent advances in Multimodal Large Language
Models (MLLMs) have inspired researchers to use natural language instructions
and images for CAD model construction. However, these models still struggle
with inferring accurate 3D spatial location and orientation, leading to
inaccuracies in determining the spatial 3D starting points and extrusion
directions for constructing geometries. This work introduces CAD-GPT, a CAD
synthesis method with spatial reasoning-enhanced MLLM that takes either a
single image or a textual description as input. To achieve precise spatial
inference, our approach introduces a 3D Modeling Spatial Mechanism. This method
maps 3D spatial positions and 3D sketch plane rotation angles into a 1D
linguistic feature space using a specialized spatial unfolding mechanism, while
discretizing 2D sketch coordinates into an appropriate planar space to enable
precise determination of spatial starting position, sketch orientation, and 2D
sketch coordinate translations. Extensive experiments demonstrate that CAD-GPT
consistently outperforms existing state-of-the-art methods in CAD model
synthesis, both quantitatively and qualitatively.

</details>


### [361] [Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment](https://arxiv.org/pdf/2501.17690)
*Zixue Zeng, Xiaoyan Zhao, Matthew Cartier, Tong Yu, Jing Wang, Xin Meng, Zhiyu Sheng, Maryam Satarpour, John M Cormack, Allison Bean, Ryan Nussbaum, Maya Maurer, Emily Landis-Walkenhorst, Dinesh Kumbhare, Kang Kim, Ajay Wasan, Jiantao Pu*

Main category: cs.CV

TL;DR: A novel framework (GRN) integrates segmentation feedback to optimize image generation and segmentation, reducing labeling efforts by up to 70% while improving performance.


<details>
  <summary>Details</summary>
Motivation: To address the high labeling effort in ultrasound image segmentation and enhance performance with less labeled data.

Method: Developed GRN with segmentation-aware joint training and segmentation-guided enhancement (SGE), including variants GRN-SEL and GRN-SSL.

Result: GRN-SEL with SGE reduced labeling by 70% and improved DSC by 1.98%. Other variants also reduced labeling by 60-70% while maintaining performance.

Conclusion: GRN effectively optimizes segmentation with less labeled data, offering a scalable solution for ultrasound analysis.

Abstract: We introduce a novel segmentation-aware joint training framework called
generative reinforcement network (GRN) that integrates segmentation loss
feedback to optimize both image generation and segmentation performance in a
single stage. An image enhancement technique called segmentation-guided
enhancement (SGE) is also developed, where the generator produces images
tailored specifically for the segmentation model. Two variants of GRN were also
developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for
semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a
dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The
annotations included six anatomical structures: dermis, superficial fat,
superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and
muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up
to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient
(DSC) compared to models trained on fully labeled datasets. GRN-SEL alone
reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling
requirements by 70%, and GRN-SSL alone by 60%, all while maintaining
performance comparable to fully supervised models. These findings suggest the
effectiveness of the GRN framework in optimizing segmentation performance with
significantly less labeled data, offering a scalable and efficient solution for
ultrasound image analysis and reducing the burdens associated with data
annotation.

</details>


### [362] [MIFNet: Learning Modality-Invariant Features for Generalizable Multimodal Image Matching](https://arxiv.org/pdf/2501.11299)
*Yepeng Liu, Zhichao Sun, Baosheng Yu, Yitian Zhao, Bo Du, Yongchao Xu, Jun Cheng*

Main category: cs.CV

TL;DR: Proposes MIFNet for modality-invariant feature learning in multimodal image matching using single-modality data, leveraging Stable Diffusion features.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multimodal data due to non-linear variations; acquiring aligned multimodal data is costly.

Method: Uses latent and cumulative hybrid aggregation modules to enhance single-modality descriptors with Stable Diffusion features.

Result: Validated on retinal and remote sensing datasets, showing robust zero-shot generalization.

Conclusion: MIFNet learns modality-invariant features without targeted modality data, with strong generalization.

Abstract: Many keypoint detection and description methods have been proposed for image
matching or registration. While these methods demonstrate promising performance
for single-modality image matching, they often struggle with multimodal data
because the descriptors trained on single-modality data tend to lack robustness
against the non-linear variations present in multimodal data. Extending such
methods to multimodal image matching often requires well-aligned multimodal
data to learn modality-invariant descriptors. However, acquiring such data is
often costly and impractical in many real-world scenarios. To address this
challenge, we propose a modality-invariant feature learning network (MIFNet) to
compute modality-invariant features for keypoint descriptions in multimodal
image matching using only single-modality training data. Specifically, we
propose a novel latent feature aggregation module and a cumulative hybrid
aggregation module to enhance the base keypoint descriptors trained on
single-modality data by leveraging pre-trained features from Stable Diffusion
models. %, our approach generates robust and invariant features across diverse
and unknown modalities. We validate our method with recent keypoint detection
and description methods in three multimodal retinal image datasets (CF-FA,
CF-OCT, EMA-OCTA) and two remote sensing datasets (Optical-SAR and
Optical-NIR). Extensive experiments demonstrate that the proposed MIFNet is
able to learn modality-invariant feature for multimodal image matching without
accessing the targeted modality and has good zero-shot generalization ability.
The code will be released at https://github.com/lyp-deeplearning/MIFNet.

</details>


### [363] [ILIAS: Instance-Level Image retrieval At Scale](https://arxiv.org/pdf/2502.11748)
*Giorgos Kordopatis-Zilos, Vladan Stojni, Anna Manko, Pavel uma, Nikolaos-Antonios Ypsilantis, Nikos Efthymiadis, Zakaria Laskar, Ji Matas, Ondej Chum, Giorgos Tolias*

Main category: cs.CV

TL;DR: ILIAS is a new large-scale test dataset for instance-level image retrieval, designed to evaluate foundation models and retrieval techniques. It features domain diversity, accurate ground truth, and performance far from saturation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing datasets by providing a large-scale, diverse, and challenging benchmark for evaluating object recognition in retrieval tasks.

Method: ILIAS includes 1,000 object instances with query and positive images, manually collected for challenging conditions. Retrieval is tested against 100 million distractor images from YFCC100M, with queries post-2014 to avoid false negatives.

Result: Benchmarking shows domain-specific models fail on ILIAS, while multi-domain adaptation improves performance. Local descriptors remain crucial, and vision-language models perform nearly as well in text-to-image as image-to-image retrieval.

Conclusion: ILIAS serves as a robust benchmark for evaluating retrieval models, highlighting the need for multi-domain adaptability and the continued relevance of local descriptors.

Abstract: This work introduces ILIAS, a new test dataset for Instance-Level Image
retrieval At Scale. It is designed to evaluate the ability of current and
future foundation models and retrieval techniques to recognize particular
objects. The key benefits over existing datasets include large scale, domain
diversity, accurate ground truth, and a performance that is far from saturated.
ILIAS includes query and positive images for 1,000 object instances, manually
collected to capture challenging conditions and diverse domains. Large-scale
retrieval is conducted against 100 million distractor images from YFCC100M. To
avoid false negatives without extra annotation effort, we include only query
objects confirmed to have emerged after 2014, i.e. the compilation date of
YFCC100M. An extensive benchmarking is performed with the following
observations: i) models fine-tuned on specific domains, such as landmarks or
products, excel in that domain but fail on ILIAS ii) learning a linear
adaptation layer using multi-domain class supervision results in performance
improvements, especially for vision-language models iii) local descriptors in
retrieval re-ranking are still a key ingredient, especially in the presence of
severe background clutter iv) the text-to-image performance of the
vision-language foundation models is surprisingly close to the corresponding
image-to-image case. website: https://vrg.fel.cvut.cz/ilias/

</details>


### [364] [Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising](https://arxiv.org/pdf/2502.16826)
*Xiangbin Wei*

Main category: cs.CV

TL;DR: Noise2Score3D is an unsupervised framework for point cloud denoising that learns gradients from noisy data, avoiding clean data requirements. It uses Tweedie's formula for efficient inference, outperforming other methods in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of limited clean data availability for point cloud denoising by proposing a fully unsupervised approach.

Method: Learns the gradient of the underlying point cloud distribution from noisy data using Tweedie's formula, enabling single-step inference. Introduces Total Variation for Point Cloud to estimate noise parameters.

Result: Achieves state-of-the-art performance on benchmarks, surpassing unsupervised methods and rivaling supervised ones. Shows strong generalization beyond training data.

Conclusion: Noise2Score3D is a versatile, efficient, and high-performing unsupervised solution for point cloud denoising, with practical utility in real-world scenarios.

Abstract: Building on recent advances in Bayesian statistics and image denoising, we
propose Noise2Score3D, a fully unsupervised framework for point cloud denoising
that addresses the critical challenge of limited availability of clean data.
Noise2Score3D learns the gradient of the underlying point cloud distribution
directly from noisy data, eliminating the need for clean data during training.
By leveraging Tweedie's formula, our method performs inference in a single
step, avoiding the iterative processes used in existing unsupervised methods,
thereby improving both performance and efficiency. Experimental results
demonstrate that Noise2Score3D achieves state-of-the-art performance on
standard benchmarks, outperforming other unsupervised methods in Chamfer
distance and point-to-mesh metrics, and rivaling some supervised approaches.
Furthermore, Noise2Score3D demonstrates strong generalization ability beyond
training datasets. Additionally, we introduce Total Variation for Point Cloud,
a criterion that allows for the estimation of unknown noise parameters, which
further enhances the method's versatility and real-world utility.

</details>


### [365] [Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator](https://arxiv.org/pdf/2503.01103)
*Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang*

Main category: cs.CV

TL;DR: The paper introduces Direct Discriminative Optimization (DDO), a framework combining likelihood-based generative training and GAN-type discrimination to overcome the mode-covering limitation of MLE, achieving state-of-the-art results in visual generation.


<details>
  <summary>Details</summary>
Motivation: To address the mode-covering tendency of MLE in generative models like diffusion and autoregressive models, which limits generation quality under constrained capacity.

Method: DDO integrates likelihood-based training with GAN-type discrimination, using reverse KL divergence and self-generated negative signals. It parameterizes a discriminator implicitly via likelihood ratios, avoiding joint training.

Result: DDO significantly improves SOTA diffusion models, reducing FID scores on CIFAR-10, ImageNet-64, and ImageNet 512x512, and enhances autoregressive models on ImageNet 256x256.

Conclusion: DDO offers an efficient and effective way to refine generative models beyond MLE limits, achieving superior performance without guidance mechanisms.

Abstract: While likelihood-based generative models, particularly diffusion and
autoregressive models, have achieved remarkable fidelity in visual generation,
the maximum likelihood estimation (MLE) objective, which minimizes the forward
KL divergence, inherently suffers from a mode-covering tendency that limits the
generation quality under limited model capacity. In this work, we propose
Direct Discriminative Optimization (DDO) as a unified framework that integrates
likelihood-based generative training and GAN-type discrimination to bypass this
fundamental constraint by exploiting reverse KL and self-generated negative
signals. Our key insight is to parameterize a discriminator implicitly using
the likelihood ratio between a learnable target model and a fixed reference
model, drawing parallels with the philosophy of Direct Preference Optimization
(DPO). Unlike GANs, this parameterization eliminates the need for joint
training of generator and discriminator networks, allowing for direct,
efficient, and effective finetuning of a well-trained model to its full
potential beyond the limits of MLE. DDO can be performed iteratively in a
self-play manner for progressive model refinement, with each round requiring
less than 1% of pretraining epochs. Our experiments demonstrate the
effectiveness of DDO by significantly advancing the previous SOTA diffusion
model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of
1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512x512 datasets without any
guidance mechanisms, and by consistently improving both guidance-free and
CFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256.

</details>


### [366] [HGO-YOLO: Advancing Anomaly Behavior Detection with Hierarchical Features and Lightweight Optimized Detection](https://arxiv.org/pdf/2503.07371)
*Qizhi Zheng, Zhongze Luo, Meiyan Guo, Xinzhu Wang, Renqimuge Wu, Qiu Meng, Guanghui Dong*

Main category: cs.CV

TL;DR: HGO-YOLO is a lightweight object detector combining GhostHGNetv2 and OptiConvDetect for efficient anomaly detection, achieving high accuracy and speed on resource-constrained hardware.


<details>
  <summary>Details</summary>
Motivation: The need for accurate, real-time object detection on resource-constrained hardware for anomaly-behavior monitoring drives the development of HGO-YOLO.

Method: HGO-YOLO integrates GhostHGNetv2 with multi-scale residual fusion and OptiConvDetect, reducing computation by 50% and FLOPs by 41% without losing accuracy.

Result: HGO-YOLO achieves 87.4% mAP@0.5 and 81.1% recall at 56 FPS on a CPU, outperforming YOLOv8n in accuracy, efficiency, and speed.

Conclusion: HGO-YOLO is a highly efficient and accurate solution for real-time anomaly detection, validated by real-world tests.

Abstract: Accurate, real-time object detection on resource-constrained hardware is
critical for anomaly-behavior monitoring. We introduce HGO-YOLO, a lightweight
detector that combines GhostHGNetv2 with an optimized parameter-sharing head
(OptiConvDetect) to deliver an outstanding accuracy-efficiency trade-off. By
embedding GhostConv into the HGNetv2 backbone with multi-scale residual fusion,
the receptive field is enlarged while redundant computation is reduced by 50%.
OptiConvDetect shares a partial-convolution layer for the classification and
regression branches, cutting detection-head FLOPs by 41% without accuracy loss.
On three anomaly datasets (fall, fight, smoke), HGO-YOLO attains 87.4% mAP@0.5
and 81.1% recall at 56 FPS on a single CPU with just 4.3 GFLOPs and 4.6
MB-surpassing YOLOv8n by +3.0% mAP, -51.7% FLOPs, and 1.7* speed. Real-world
tests on a Jetson Orin Nano further confirm a stable throughput gain of 42 FPS.

</details>


### [367] [Noise2Score3D: Tweedie's Approach for Unsupervised Point Cloud Denoising](https://arxiv.org/pdf/2503.09283)
*Xiangbin Wei, Yuanfeng Wang, Ao XU, Lingyu Zhu, Dongyong Sun, Keren Li, Yang Li, Qi Qin*

Main category: cs.CV

TL;DR: Noise2Score3D is an unsupervised framework for point cloud denoising that learns the score function from noisy data, avoiding clean data requirements and iterative processes. It introduces a new metric and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of generalization and the absence of clean data in learning-based point cloud denoising methods.

Method: Uses Tweedie's formula to learn the score function of the underlying point cloud distribution directly from noisy data, enabling single-step denoising. Introduces Total Variation for Point Clouds as a quality metric.

Result: Achieves state-of-the-art performance on standard benchmarks in Chamfer distance and point-to-mesh metrics, with strong generalization beyond training datasets.

Conclusion: Noise2Score3D paves the way for practical learning-based point cloud denoising by overcoming key challenges in real-world applications.

Abstract: Building on recent advances in Bayesian statistics and image denoising, we
propose Noise2Score3D, a fully unsupervised framework for point cloud
denoising. Noise2Score3D learns the score function of the underlying point
cloud distribution directly from noisy data, eliminating the need for clean
data during training. Using Tweedie's formula, our method performs denoising in
a single step, avoiding the iterative processes used in existing unsupervised
methods, thus improving both accuracy and efficiency. Additionally, we
introduce Total Variation for Point Clouds as a denoising quality metric, which
allows for the estimation of unknown noise parameters. Experimental results
demonstrate that Noise2Score3D achieves state-of-the-art performance on
standard benchmarks among unsupervised learning methods in Chamfer distance and
point-to-mesh metrics. Noise2Score3D also demonstrates strong generalization
ability beyond training datasets. Our method, by addressing the generalization
issue and challenge of the absence of clean data in learning-based methods,
paves the way for learning-based point cloud denoising methods in real-world
applications.

</details>


### [368] [LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation](https://arxiv.org/pdf/2503.13794)
*Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Can Jin, Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: LED enhances Open-Vocabulary Object Detection by fusing LLM hidden states into detectors, avoiding bias from synthetic data pipelines. It uses zero-initialized cross-attention adapters for efficient fusion, improving performance with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Hand-crafted pipelines for synthetic training data in OVD introduce bias and overfit to prompts. LED explores direct fusion of LLM hidden states into detectors, an under-explored approach.

Method: LED integrates decoder layers of an LLM into detectors using zero-initialized cross-attention adapters. It focuses on early LLM layers for spatial semantics.

Result: LED improves GroundingDINO by 3.82% on OmniLabel with Swin-T and 6.22% with a larger backbone, adding only 8.7% extra GFLOPs.

Conclusion: LED effectively leverages LLM layers for visual grounding, demonstrating significant performance gains with minimal computational cost.

Abstract: Large foundation models trained on large-scale vision-language data can boost
Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the
hand-crafted pipelines often introduce bias and overfit to specific prompts. We
sidestep this issue by directly fusing hidden states from Large Language Models
(LLMs) into detectors-an avenue surprisingly under-explored. This paper
presents a systematic method to enhance visual grounding by utilizing decoder
layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention
adapter to enable efficient knowledge fusion from LLMs to object detectors, a
new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We
find that intermediate LLM layers already encode rich spatial semantics;
adapting only the early layers yields most of the gain. With Swin-T as the
vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at
just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to
6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths
further corroborate our design.

</details>


### [369] [HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model](https://arxiv.org/pdf/2503.10631)
*Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC alex Zhou, Pheng-Ann Heng, Shanghang Zhang*

Main category: cs.CV

TL;DR: HybridVLA combines diffusion-based continuous action prediction and autoregressive reasoning in a single model, outperforming previous methods by 14-19% in success rates.


<details>
  <summary>Details</summary>
Motivation: Existing VLA methods either disrupt action continuity (autoregressive) or underutilize VLM reasoning (diffusion-based). HybridVLA aims to unify both strengths.

Method: HybridVLA integrates diffusion denoising into next-token prediction, using a collaborative training recipe and adaptive ensemble mechanism.

Result: Achieves 14% and 19% higher success rates in simulation and real-world tasks, respectively, with stable performance in unseen scenarios.

Conclusion: HybridVLA effectively merges continuous action prediction and contextual reasoning, enhancing robustness and performance in manipulation tasks.

Abstract: A fundamental objective of manipulation policy design is to endow robots to
comprehend human instructions, reason about scene cues, and execute generalized
actions in dynamic environments. Recent autoregressive vision-language-action
(VLA) methods inherit common-sense reasoning capabilities from vision-language
models (VLMs) for next action-token prediction. However, these methods quantize
actions into discrete bins, which disrupts the continuity required for precise
control. In contrast, existing diffusion-based VLA methods incorporate an
additional diffusion head to predict continuous actions solely conditioned on
feature representations extracted by the VLM, without fully leveraging the
VLM's pretrained reasoning capabilities through token-level generation. To
address these limitations, we introduce HybridVLA, a unified framework that
absorbs the continuous nature of diffusion-based actions and the contextual
reasoning of autoregression within a single large language model. To mitigate
interference between the two generation paradigms, we propose a collaborative
training recipe that seamlessly incorporates diffusion denoising into the
next-token prediction process. With this recipe, we find these two action
prediction methods not only reinforce each other but also exhibit varying
strength across different tasks. Therefore, we design a collaborative action
ensemble mechanism that adaptively fuses both predictions, leading to more
robust control. HybridVLA outperforms previous state-of-the-art VLA methods by
14\% and 19\% in mean success rate on simulation and real-world tasks,
respectively, while demonstrating stable manipulation in unseen configurations.

</details>


### [370] [Benchmarking Large Language Models for Handwritten Text Recognition](https://arxiv.org/pdf/2503.15195)
*Giorgia Crosilla, Lukas Klic, Giovanni Colavizza*

Main category: cs.CV

TL;DR: The paper compares Multimodal Large Language Models (MLLMs) with traditional supervised HTR models, finding proprietary MLLMs like Claude 3.5 Sonnet perform better in zero-shot settings but struggle with autonomous error correction.


<details>
  <summary>Details</summary>
Motivation: To evaluate MLLMs' effectiveness in HTR without model-specific training, addressing limitations of traditional supervised methods.

Method: Benchmarked proprietary and open-source LLMs against Transkribus models on diverse datasets (English, French, German, Italian), testing recognition and autonomous correction.

Result: Proprietary MLLMs outperform open-source ones, excel in modern handwriting, favor English, and show no clear advantage over Transkribus. Limited autonomous error correction.

Conclusion: MLLMs offer promising zero-shot HTR but lack robust error correction. No consistent superiority over traditional methods like Transkribus.

Abstract: Traditional machine learning models for Handwritten Text Recognition (HTR)
rely on supervised training, requiring extensive manual annotations, and often
produce errors due to the separation between layout and text processing. In
contrast, Multimodal Large Language Models (MLLMs) offer a general approach to
recognizing diverse handwriting styles without the need for model-specific
training. The study benchmarks various proprietary and open-source LLMs against
Transkribus models, evaluating their performance on both modern and historical
datasets written in English, French, German, and Italian. In addition, emphasis
is placed on testing the models' ability to autonomously correct previously
generated outputs. Findings indicate that proprietary models, especially Claude
3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs
achieve excellent results in recognizing modern handwriting and exhibit a
preference for the English language due to their pre-training dataset
composition. Comparisons with Transkribus show no consistent advantage for
either approach. Moreover, LLMs demonstrate limited ability to autonomously
correct errors in zero-shot transcriptions.

</details>


### [371] [Training A Neural Network For Partially Occluded Road Sign Identification In The Context Of Autonomous Vehicles](https://arxiv.org/pdf/2503.18177)
*Gulnaz Gimaletdinova, Dim Shaiakhmetov, Madina Akpaeva, Mukhammadmuso Abduzhabbarov, Kadyrmamat Momunov*

Main category: cs.CV

TL;DR: The study examines the impact of partial occlusion on traffic sign recognition, comparing a custom CNN (96% accuracy) with transfer learning models (VGG16, 99% accuracy). It emphasizes the need for training on occluded signs for robust performance.


<details>
  <summary>Details</summary>
Motivation: The rise of autonomous vehicles and computer vision highlights the need for accurate traffic sign recognition, especially under partial occlusion, a common real-world challenge.

Method: A dataset of 5,746 images (fully visible and partially occluded signs) was created. A custom CNN and transfer learning models (e.g., VGG16) were tested.

Result: VGG16 with full layer unfreezing achieved 99% accuracy, outperforming the custom CNN (96%). Models trained only on visible signs struggled with occluded ones.

Conclusion: Training datasets must include partially occluded signs to ensure robust recognition in real-world scenarios, enhancing autonomous driving safety.

Abstract: The increasing number of autonomous vehicles and the rapid development of
computer vision technologies underscore the particular importance of conducting
research on the accuracy of traffic sign recognition. Numerous studies in this
field have already achieved significant results, demonstrating high
effectiveness in addressing traffic sign recognition tasks. However, the task
becomes considerably more complex when a sign is partially obscured by
surrounding objects, such as tree branches, billboards, or other elements of
the urban environment. In our study, we investigated how partial occlusion of
traffic signs affects their recognition. For this purpose, we collected a
dataset comprising 5,746 images, including both fully visible and partially
occluded signs, and made it publicly available. Using this dataset, we compared
the performance of our custom convolutional neural network (CNN), which
achieved 96% accuracy, with models trained using transfer learning. The best
result was obtained by VGG16 with full layer unfreezing, reaching 99% accuracy.
Additional experiments revealed that models trained solely on fully visible
signs lose effectiveness when recognizing occluded signs. This highlights the
critical importance of incorporating real-world data with partial occlusion
into training sets to ensure robust model performance in complex practical
scenarios and to enhance the safety of autonomous driving.

</details>


### [372] [Boosting Virtual Agent Learning and Reasoning: A Step-Wise, Multi-Dimensional, and Generalist Reward Model with Benchmark](https://arxiv.org/pdf/2503.18665)
*Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li*

Main category: cs.CV

TL;DR: The paper introduces Similar, a Step-Wise Multi-Dimensional Generalist Reward Model, to improve training and inference for Generalist Virtual Agents (GVAs) by providing fine-grained signals and reducing reliance on human annotations.


<details>
  <summary>Details</summary>
Motivation: Current training paradigms for GVAs rely heavily on outcome supervision and human annotations, which are labor-intensive and limit scalability.

Method: The authors define five evaluation dimensions for agent actions, use an MCTS-P algorithm for automatic data collection, and train Similar with the Triple-M strategy. They also introduce the SRM benchmark for training and evaluation.

Result: Similar provides effective intermediate signals for GVAs during training and inference, as demonstrated by experimental results.

Conclusion: Similar addresses key limitations in GVA training and offers a scalable solution with its step-wise, multi-dimensional reward model.

Abstract: The development of Generalist Virtual Agents (GVAs) has shown significant
promise in autonomous task execution. However, current training paradigms face
critical limitations, including reliance on outcome supervision and
labor-intensive human annotations. To address these challenges, we propose
Similar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers
fine-grained signals for agent training and can choose better action for
inference-time scaling. Specifically, we begin by systematically defining five
dimensions for evaluating agent actions. Building on this framework, we design
an MCTS-P algorithm to automatically collect and annotate step-wise,
five-dimensional agent execution data. Using this data, we train Similar with
the Triple-M strategy. Furthermore, we introduce the first benchmark in the
virtual agent domain for step-wise, multi-dimensional reward model training and
evaluation, named SRM. This benchmark consists of two components: SRMTrain,
which serves as the training set for Similar, and SRMEval, a manually selected
test set for evaluating the reward model. Experimental results demonstrate that
Similar, through its step-wise, multi-dimensional assessment and synergistic
gain, provides GVAs with effective intermediate signals during both training
and inference-time scaling. The project is available at
https://github.com/antgroup/Similar.

</details>


### [373] [GmNet: Revisiting Gating Mechanisms From A Frequency View](https://arxiv.org/pdf/2503.22841)
*Yifan Wang, Xu Ma, Yitian Zhang, Zhongruo Wang, Sung-Cheol Kim, Vahid Mirjalili, Vidya Renganathan, Yun Fu*

Main category: cs.CV

TL;DR: The paper explores gating mechanisms in neural networks from a frequency perspective, proposing GmNet to address low-frequency bias in lightweight models.


<details>
  <summary>Details</summary>
Motivation: To understand how gating mechanisms work theoretically and improve their efficiency in managing information flow, especially for long-range dependencies.

Method: Analyzes gating mechanisms using the convolution theorem, focusing on element-wise product and activation functions' effects on frequency components. Proposes GmNet, a lightweight model.

Result: GmNet effectively minimizes low-frequency bias and performs well in image classification tasks, balancing efficiency and effectiveness.

Conclusion: Gating mechanisms, analyzed from a frequency perspective, can enhance lightweight models like GmNet, improving performance in tasks like image classification.

Abstract: Gating mechanisms have emerged as an effective strategy integrated into model
designs beyond recurrent neural networks for addressing long-range dependency
problems. In a broad understanding, it provides adaptive control over the
information flow while maintaining computational efficiency. However, there is
a lack of theoretical analysis on how the gating mechanism works in neural
networks. In this paper, inspired by the \textit{convolution theorem}, we
systematically explore the effect of gating mechanisms on the training dynamics
of neural networks from a frequency perspective. We investigate the interact
between the element-wise product and activation functions in managing the
responses to different frequency components. Leveraging these insights, we
propose a Gating Mechanism Network (GmNet), a lightweight model designed to
efficiently utilize the information of various frequency components. It
minimizes the low-frequency bias present in existing lightweight models. GmNet
achieves impressive performance in terms of both effectiveness and efficiency
in the image classification task.

</details>


### [374] [SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point Clouds with Cross-Scene Adaptability and 4D Consistency](https://arxiv.org/pdf/2503.23980)
*Yanbo Wang, Yongtao Chen, Chuan Cao, Tianchen Deng, Wentao Zhao, Jingchuan Wang, Weidong Chen*

Main category: cs.CV

TL;DR: SALT is a semi-automatic labeling tool for LiDAR point clouds, using zero-shot learning and 4D consistency to improve annotation efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of camera distillation methods and enhance LiDAR data annotation efficiency and cross-scene adaptability.

Method: Proposes a zero-shot learning paradigm (data alignment) to transform LiDAR data into pseudo-images, along with 4D-consistent prompting and non-maximum suppression for high-quality presegmentation.

Result: Outperforms latest zero-shot methods by 18.4% PQ on SemanticKITTI and achieves 40-50% of human annotator performance on diverse LiDAR data.

Conclusion: SALT's open-sourcing is expected to expand LiDAR datasets and support future LiDAR foundation model development.

Abstract: We propose a flexible Semi-Automatic Labeling Tool (SALT) for general LiDAR
point clouds with cross-scene adaptability and 4D consistency. Unlike recent
approaches that rely on camera distillation, SALT operates directly on raw
LiDAR data, automatically generating pre-segmentation results. To achieve this,
we propose a novel zero-shot learning paradigm, termed data alignment, which
transforms LiDAR data into pseudo-images by aligning with the training
distribution of vision foundation models. Additionally, we design a
4D-consistent prompting strategy and 4D non-maximum suppression module to
enhance SAM2, ensuring high-quality, temporally consistent presegmentation.
SALT surpasses the latest zero-shot methods by 18.4% PQ on SemanticKITTI and
achieves nearly 40-50% of human annotator performance on our newly collected
low-resolution LiDAR data and on combined data from three LiDAR types,
significantly boosting annotation efficiency. We anticipate that SALT's
open-sourcing will catalyze substantial expansion of current LiDAR datasets and
lay the groundwork for the future development of LiDAR foundation models. Code
is available at https://github.com/Cavendish518/SALT.

</details>


### [375] [Kimi-VL Technical Report](https://arxiv.org/pdf/2504.07491)
*Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinhao Li, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yuhao Dong, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, Ziwei Chen, Zongyu Lin*

Main category: cs.CV

TL;DR: Kimi-VL is an efficient open-source Mixture-of-Experts vision-language model with strong multimodal reasoning, long-context understanding, and agent capabilities, outperforming competitors like GPT-4o in key domains.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced, efficient vision-language model capable of handling diverse tasks, long contexts, and high-resolution inputs while minimizing computational costs.

Method: Utilizes a Mixture-of-Experts (MoE) architecture with a 128K extended context window, a native-resolution vision encoder (MoonViT), and long chain-of-thought fine-tuning for reasoning.

Result: Achieves strong performance across tasks like multi-turn agent tasks, OCR, mathematical reasoning, and long-context benchmarks, surpassing GPT-4o in some domains.

Conclusion: Kimi-VL is a highly capable, efficient VLM with advanced reasoning and long-context processing, offering competitive performance and lower computational costs.

Abstract: We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)
vision-language model (VLM) that offers advanced multimodal reasoning,
long-context understanding, and strong agent capabilities - all while
activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL
demonstrates strong performance across challenging domains: as a
general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),
matching flagship models. Furthermore, it exhibits remarkable capabilities
across diverse challenging vision language tasks, including college-level image
and video comprehension, OCR, mathematical reasoning, and multi-image
understanding. In comparative evaluations, it effectively competes with
cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and
Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also
advances in processing long contexts and perceiving clearly. With a 128K
extended context window, Kimi-VL can process diverse long inputs, achieving
impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its
native-resolution vision encoder, MoonViT, further allows it to see and
understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and
34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common
tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:
Kimi-VL-Thinking-2506. Developed through long chain-of-thought (CoT) supervised
fine-tuning (SFT) and reinforcement learning (RL), the latest model exhibits
strong long-horizon reasoning capabilities (64.0 on MMMU, 46.3 on MMMU-Pro,
56.9 on MathVision, 80.1 on MathVista, 65.2 on VideoMMMU) while obtaining
robust general abilities. Code and models are publicly accessible at
https://github.com/MoonshotAI/Kimi-VL.

</details>


### [376] [Step1X-Edit: A Practical Framework for General Image Editing](https://arxiv.org/pdf/2504.17761)
*Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, Daxin Jiang*

Main category: cs.CV

TL;DR: The paper introduces Step1X-Edit, an open-source image editing model that rivals proprietary models like GPT-4o and Gemini2 Flash, using Multimodal LLM and diffusion decoding.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between open-source and closed-source image editing models.

Method: Uses Multimodal LLM for processing images and instructions, integrates latent embeddings with a diffusion decoder, and trains on a high-quality generated dataset.

Result: Step1X-Edit outperforms open-source baselines and nears proprietary model performance on the GEdit-Bench.

Conclusion: Step1X-Edit advances open-source image editing, offering competitive performance against leading proprietary models.

Abstract: In recent years, image editing models have witnessed remarkable and rapid
development. The recent unveiling of cutting-edge multimodal models such as
GPT-4o and Gemini2 Flash has introduced highly promising image editing
capabilities. These models demonstrate an impressive aptitude for fulfilling a
vast majority of user-driven editing requirements, marking a significant
advancement in the field of image manipulation. However, there is still a large
gap between the open-source algorithm with these closed-source models. Thus, in
this paper, we aim to release a state-of-the-art image editing model, called
Step1X-Edit, which can provide comparable performance against the closed-source
models like GPT-4o and Gemini2 Flash. More specifically, we adopt the
Multimodal LLM to process the reference image and the user's editing
instruction. A latent embedding has been extracted and integrated with a
diffusion image decoder to obtain the target image. To train the model, we
build a data generation pipeline to produce a high-quality dataset. For
evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world
user instructions. Experimental results on GEdit-Bench demonstrate that
Step1X-Edit outperforms existing open-source baselines by a substantial margin
and approaches the performance of leading proprietary models, thereby making
significant contributions to the field of image editing.

</details>


### [377] [VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification](https://arxiv.org/pdf/2504.21464)
*Shamim Rahim Refat, Ziyan Shirin Raha, Shuvashis Sarker, Faika Fairuj Preotee, MD. Musfikur Rahman, Tashreef Muhammad, Mohammad Shafiul Alam*

Main category: cs.CV

TL;DR: The paper proposes VR-FuseNet, a hybrid deep learning model combining VGG19 and ResNet50V2, for automated diabetic retinopathy detection, achieving 91.824% accuracy and incorporating XAI for interpretability.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy is a leading cause of blindness; early detection is crucial. Existing methods face dataset imbalance and generalization issues.

Method: A hybrid dataset from five public sources is preprocessed (SMOTE, CLAHE). VR-FuseNet fuses VGG19 and ResNet50V2 for feature extraction.

Result: VR-FuseNet achieves 91.824% accuracy, outperforming individual models. XAI techniques provide interpretable visual explanations.

Conclusion: The hybrid model improves diagnostic performance and clinical interpretability, aiding early intervention.

Abstract: Diabetic retinopathy is a severe eye condition caused by diabetes where the
retinal blood vessels get damaged and can lead to vision loss and blindness if
not treated. Early and accurate detection is key to intervention and stopping
the disease progressing. For addressing this disease properly, this paper
presents a comprehensive approach for automated diabetic retinopathy detection
by proposing a new hybrid deep learning model called VR-FuseNet. Diabetic
retinopathy is a major eye disease and leading cause of blindness especially
among diabetic patients so accurate and efficient automated detection methods
are required. To address the limitations of existing methods including dataset
imbalance, diversity and generalization issues this paper presents a hybrid
dataset created from five publicly available diabetic retinopathy datasets.
Essential preprocessing techniques such as SMOTE for class balancing and CLAHE
for image enhancement are applied systematically to the dataset to improve the
robustness and generalizability of the dataset. The proposed VR-FuseNet model
combines the strengths of two state-of-the-art convolutional neural networks,
VGG19 which captures fine-grained spatial features and ResNet50V2 which is
known for its deep hierarchical feature extraction. This fusion improves the
diagnostic performance and achieves an accuracy of 91.824%. The model
outperforms individual architectures on all performance metrics demonstrating
the effectiveness of hybrid feature extraction in Diabetic Retinopathy
classification tasks. To make the proposed model more clinically useful and
interpretable this paper incorporates multiple XAI techniques. These techniques
generate visual explanations that clearly indicate the retinal features
affecting the model's prediction such as microaneurysms, hemorrhages and
exudates so that clinicians can interpret and validate.

</details>


### [378] [InstructAttribute: Fine-grained Object Attributes editing with Instruction](https://arxiv.org/pdf/2505.00751)
*Xingxi Yin, Jingfeng Zhang, Yue Deng, Zhi Li, Yicheng Li, Yin Zhang*

Main category: cs.CV

TL;DR: SPAA is a training-free framework for precise color and material attribute control in T2I diffusion models, enhanced by MLLMs for automated data curation and instruction generation, leading to the InstructAttribute model for fine-grained editing.


<details>
  <summary>Details</summary>
Motivation: Existing T2I diffusion models struggle with fine-grained control over object attributes like color and material, often compromising image integrity.

Method: Introduces SPAA for manipulating self-attention and cross-attention maps, integrates MLLMs for data curation, and develops the InstructAttribute model for attribute editing via natural language.

Result: InstructAttribute outperforms baselines, balancing attribute accuracy and structural preservation.

Conclusion: The framework and model enable practical applications in design, e-commerce, and virtual try-ons, demonstrating superior performance.

Abstract: Text-to-image (T2I) diffusion models are widely used in image editing due to
their powerful generative capabilities. However, achieving fine-grained control
over specific object attributes, such as color and material, remains a
considerable challenge. Existing methods often fail to accurately modify these
attributes or compromise structural integrity and overall image consistency. To
fill this gap, we introduce Structure Preservation and Attribute Amplification
(SPAA), a novel training-free framework that enables precise generation of
color and material attributes for the same object by intelligently manipulating
self-attention maps and cross-attention values within diffusion models.
Building on SPAA, we integrate multi-modal large language models (MLLMs) to
automate data curation and instruction generation. Leveraging this object
attribute data collection engine, we construct the Attribute Dataset,
encompassing a comprehensive range of colors and materials across diverse
object categories. Using this generated dataset, we propose InstructAttribute,
an instruction-tuned model that enables fine-grained and object-level attribute
editing through natural language prompts. This capability holds significant
practical implications for diverse fields, from accelerating product design and
e-commerce visualization to enhancing virtual try-on experiences. Extensive
experiments demonstrate that InstructAttribute outperforms existing
instruction-based baselines, achieving a superior balance between attribute
modification accuracy and structural preservation.

</details>


### [379] [Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models](https://arxiv.org/pdf/2505.07001)
*Bidur Khanal, Sandesh Pokhrel, Sanjay Bhandari, Ramesh Rana, Nikesh Shrestha, Ram Bahadur Gurung, Cristian Linte, Angus Watson, Yash Raj Shrestha, Binod Bhattarai*

Main category: cs.CV

TL;DR: The paper introduces Gut-VLM, a multimodal dataset for studying hallucination in Vision-Language Models (VLMs) for gastrointestinal image analysis, and proposes hallucination-aware finetuning to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Hallucination in VLMs, where generated descriptions don't match visual content, is a critical issue in medical applications. The paper aims to address this by creating a high-quality dataset and refining VLM training.

Method: A two-stage pipeline curates the Gut-VLM dataset: ChatGPT generates initial reports (with potential hallucinations), which medical experts then review and correct. The VLM is finetuned to detect and correct hallucinations, not just generate reports.

Result: Hallucination-aware finetuning outperforms traditional finetuning for report generation. The dataset also benchmarks state-of-the-art VLMs.

Conclusion: The Gut-VLM dataset and hallucination-aware finetuning approach effectively mitigate hallucination in VLMs, improving reliability for medical applications.

Abstract: Vision-Language Models (VLMs) are becoming increasingly popular in the
medical domain, bridging the gap between medical images and clinical language.
Existing VLMs demonstrate an impressive ability to comprehend medical images
and text queries to generate detailed, descriptive diagnostic medical reports.
However, hallucination--the tendency to generate descriptions that are
inconsistent with the visual content--remains a significant issue in VLMs, with
particularly severe implications in the medical field. To facilitate VLM
research on gastrointestinal (GI) image analysis and study hallucination, we
curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created
using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2
images are generated using ChatGPT, which introduces some hallucinated or
incorrect texts. In the second stage, medical experts systematically review
these reports, and identify and correct potential inaccuracies to ensure
high-quality, clinically reliable annotations. Unlike traditional datasets that
contain only descriptive texts, our dataset also features tags identifying
hallucinated sentences and their corresponding corrections. A common approach
to reducing hallucination in VLM is to finetune the model on a small-scale,
problem-specific dataset. However, we take a different strategy using our
dataset. Instead of finetuning the VLM solely for generating textual reports,
we finetune it to detect and correct hallucinations, an approach we call
hallucination-aware finetuning. Our results show that this approach is better
than simply finetuning for descriptive report generation. Additionally, we
conduct an extensive evaluation of state-of-the-art VLMs across several
metrics, establishing a benchmark. GitHub Repo:
https://github.com/bhattarailab/Hallucination-Aware-VLM.

</details>


### [380] [MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence](https://arxiv.org/pdf/2505.10604)
*Chonghan Liu, Haoran Wang, Felix Henry, Pu Miao, Yajie Zhang, Yu Zhao, Peiran Wu*

Main category: cs.CV

TL;DR: MIRAGE is a multi-modal benchmark evaluating models' abilities in object recognition and spatial relational reasoning, highlighting gaps in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks show gaps in models' abilities for object attribute recognition and spatial relational reasoning, essential for dynamic reasoning.

Method: Proposes MIRAGE, a benchmark with tasks like Counting, Relation, and Counting with Relation, using diverse scenarios.

Result: MIRAGE reveals limitations in state-of-the-art models, emphasizing the need for better representations and reasoning frameworks.

Conclusion: MIRAGE provides a foundation for advancing spatiotemporal reasoning in future research.

Abstract: Spatial perception and reasoning are core components of human cognition,
encompassing object recognition, spatial relational understanding, and dynamic
reasoning. Despite progress in computer vision, existing benchmarks reveal
significant gaps in models' abilities to accurately recognize object attributes
and reason about spatial relationships, both essential for dynamic reasoning.
To address these limitations, we propose MIRAGE, a multi-modal benchmark
designed to evaluate models' capabilities in Counting (object attribute
recognition), Relation (spatial relational reasoning), and Counting with
Relation. Through diverse and complex scenarios requiring fine-grained
recognition and reasoning, MIRAGE highlights critical limitations in
state-of-the-art models, underscoring the need for improved representations and
reasoning frameworks. By targeting these foundational abilities, MIRAGE
provides a pathway toward spatiotemporal reasoning in future research.

</details>


### [381] [An Exploratory Approach Towards Investigating and Explaining Vision Transformer and Transfer Learning for Brain Disease Detection](https://arxiv.org/pdf/2505.16039)
*Shuvashis Sarker, Shamim Rahim Refat, Faika Fairuj Preotee, Shifat Islam, Tashreef Muhammad, Mohammad Ashraful Hoque*

Main category: cs.CV

TL;DR: The study compares Vision Transformer (ViT) and Transfer Learning models for classifying brain diseases using MRI data, finding ViT superior with 94.39% accuracy. Explainable AI methods enhance interpretability.


<details>
  <summary>Details</summary>
Motivation: Brain conditions are hard to diagnose via MRI. The study aims to improve classification accuracy and interpretability for better medical diagnosis.

Method: Comparative analysis of ViT and Transfer Learning models (VGG16, VGG19, Resnet50V2, MobilenetV2) on MRI data, using Explainable AI (GradCAM, GradCAM++, etc.) for interpretation.

Result: ViT outperforms Transfer Learning models with 94.39% accuracy. XAI methods provide transparent insights for medical professionals.

Conclusion: ViT is highly effective for brain disease classification, and XAI enhances diagnostic precision, aiding medical decision-making.

Abstract: The brain is a highly complex organ that manages many important tasks,
including movement, memory and thinking. Brain-related conditions, like tumors
and degenerative disorders, can be hard to diagnose and treat. Magnetic
Resonance Imaging (MRI) serves as a key tool for identifying these conditions,
offering high-resolution images of brain structures. Despite this, interpreting
MRI scans can be complicated. This study tackles this challenge by conducting a
comparative analysis of Vision Transformer (ViT) and Transfer Learning (TL)
models such as VGG16, VGG19, Resnet50V2, MobilenetV2 for classifying brain
diseases using MRI data from Bangladesh based dataset. ViT, known for their
ability to capture global relationships in images, are particularly effective
for medical imaging tasks. Transfer learning helps to mitigate data constraints
by fine-tuning pre-trained models. Furthermore, Explainable AI (XAI) methods
such as GradCAM, GradCAM++, LayerCAM, ScoreCAM, and Faster-ScoreCAM are
employed to interpret model predictions. The results demonstrate that ViT
surpasses transfer learning models, achieving a classification accuracy of
94.39%. The integration of XAI methods enhances model transparency, offering
crucial insights to aid medical professionals in diagnosing brain diseases with
greater precision.

</details>


### [382] [CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis](https://arxiv.org/pdf/2505.17590)
*Florian Barthel, Wieland Morgenstern, Paul Hinzer, Anna Hilsmann, Peter Eisert*

Main category: cs.CV

TL;DR: CGS-GAN is a novel 3D Gaussian Splatting GAN framework that ensures stable training and high-quality 3D-consistent synthesis of human heads without view-conditioning, achieving high-resolution outputs and competitive FID scores.


<details>
  <summary>Details</summary>
Motivation: Existing 3D GANs compromise 3D consistency by conditioning on camera position, leading to identity changes or poor novel-view performance. Removing view-conditioning destabilizes training.

Method: Introduces multi-view regularization and an adapted conditional loss, along with a generator architecture for stable training, efficient rendering, and scaling up to 2048 resolution.

Result: Achieves high rendering quality and 3D consistency, validated by competitive FID scores on a curated FFHQ-derived dataset.

Conclusion: CGS-GAN addresses key limitations of prior methods, enabling stable training and high-quality, consistent 3D head synthesis without view-conditioning.

Abstract: Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high
quality synthesis of human heads. However, existing methods stabilize training
and enhance rendering quality from steep viewpoints by conditioning the random
latent vector on the current camera position. This compromises 3D consistency,
as we observe significant identity changes when re-synthesizing the 3D head
with each camera shift. Conversely, fixing the camera to a single viewpoint
yields high-quality renderings for that perspective but results in poor
performance for novel views. Removing view-conditioning typically destabilizes
GAN training, often causing the training to collapse. In response to these
challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework
that enables stable training and high-quality 3D-consistent synthesis of human
heads without relying on view-conditioning. To ensure training stability, we
introduce a multi-view regularization technique that enhances generator
convergence with minimal computational overhead. Additionally, we adapt the
conditional loss used in existing 3D Gaussian splatting GANs and propose a
generator architecture designed to not only stabilize training but also
facilitate efficient rendering and straightforward scaling, enabling output
resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate
a new dataset derived from FFHQ. This dataset enables very high resolutions,
focuses on larger portions of the human head, reduces view-dependent artifacts
for improved 3D consistency, and excludes images where subjects are obscured by
hands or other objects. As a result, our approach achieves very high rendering
quality, supported by competitive FID scores, while ensuring consistent 3D
scene generation. Check our our project page here:
https://fraunhoferhhi.github.io/cgs-gan/

</details>


### [383] [Holistic White-light Polyp Classification via Alignment-free Dense Distillation of Auxiliary Optical Chromoendoscopy](https://arxiv.org/pdf/2505.19319)
*Qiang Hu, Qimei Wang, Jia Chen, Xuantao Ji, Qiang Li, Zhiwei Wang*

Main category: cs.CV

TL;DR: The paper introduces a novel holistic classification framework, Alignment-free Dense Distillation (ADD), for polyp classification using White Light Imaging (WLI) without requiring polyp localization, outperforming existing methods by at least 2.5% and 16.2% in AUC.


<details>
  <summary>Details</summary>
Motivation: WLI-based polyp classification underperforms compared to NBI, and existing methods rely on cropped lesion regions, which are error-prone and ignore contextual cues.

Method: Proposes ADD, a framework for cross-domain knowledge distillation without explicit alignment, using pixel-wise affinities and Class Activation Mapping (CAM) to ensure semantic consistency.

Result: Achieves state-of-the-art performance, with significant improvements in AUC on public and in-house datasets.

Conclusion: The ADD framework effectively enhances WLI-based polyp classification by leveraging full-image diagnosis and fine-grained cross-domain knowledge distillation.

Abstract: White Light Imaging (WLI) and Narrow Band Imaging (NBI) are the two main
colonoscopic modalities for polyp classification. While NBI, as optical
chromoendoscopy, offers valuable vascular details, WLI remains the most common
and often the only available modality in resource-limited settings. However,
WLI-based methods typically underperform, limiting their clinical
applicability. Existing approaches transfer knowledge from NBI to WLI through
global feature alignment but often rely on cropped lesion regions, which are
susceptible to detection errors and neglect contextual and subtle diagnostic
cues. To address this, this paper proposes a novel holistic classification
framework that leverages full-image diagnosis without requiring polyp
localization. The key innovation lies in the Alignment-free Dense Distillation
(ADD) module, which enables fine-grained cross-domain knowledge distillation
regardless of misalignment between WLI and NBI images. Without resorting to
explicit image alignment, ADD learns pixel-wise cross-domain affinities to
establish correspondences between feature maps, guiding the distillation along
the most relevant pixel connections. To further enhance distillation
reliability, ADD incorporates Class Activation Mapping (CAM) to filter
cross-domain affinities, ensuring the distillation path connects only those
semantically consistent regions with equal contributions to polyp diagnosis.
Extensive results on public and in-house datasets show that our method achieves
state-of-the-art performance, relatively outperforming the other approaches by
at least 2.5% and 16.2% in AUC, respectively. Code is available at:
https://github.com/Huster-Hq/ADD.

</details>


### [384] [ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding](https://arxiv.org/pdf/2505.21381)
*Linshuang Diao, Dayong Ren, Sensen Song, Yurong Qian*

Main category: cs.CV

TL;DR: ZigzagPointMamba improves point cloud self-supervised learning by enhancing spatial continuity and local semantic modeling, outperforming existing methods in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing PointMamba-based methods disrupt spatial continuity and local semantic correlations due to complex token ordering and random masking.

Method: Proposes ZigzagPointMamba with a zigzag scan path for spatial continuity and Semantic-Siamese Masking Strategy (SMS) for robust local semantic modeling.

Result: Achieves significant improvements: 1.59% mIoU gain on ShapeNetPart, 0.4% higher accuracy on ModelNet40, and better accuracies on ScanObjectNN subsets.

Conclusion: ZigzagPointMamba effectively addresses limitations of existing methods, enhancing performance in point cloud tasks.

Abstract: State Space models (SSMs) such as PointMamba enable efficient feature
extraction for point cloud self-supervised learning with linear complexity,
outperforming Transformers in computational efficiency. However, existing
PointMamba-based methods depend on complex token ordering and random masking,
which disrupt spatial continuity and local semantic correlations. We propose
ZigzagPointMamba to tackle these challenges. The core of our approach is a
simple zigzag scan path that globally sequences point cloud tokens, enhancing
spatial continuity by preserving the proximity of spatially adjacent point
tokens. Nevertheless, random masking undermines local semantic modeling in
self-supervised learning. To address this, we introduce a Semantic-Siamese
Masking Strategy (SMS), which masks semantically similar tokens to facilitate
reconstruction by integrating local features of original and similar tokens.
This overcomes the dependence on isolated local features and enables robust
global semantic modeling. Our pre-trained ZigzagPointMamba weights
significantly improve downstream tasks, achieving a 1.59% mIoU gain on
ShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for
classification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for
the classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of
ScanObjectNN.

</details>


### [385] [PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views](https://arxiv.org/pdf/2505.23481)
*Mohamed Rayan Barhdadi, Hasan Kurban, Hussein Alnuweiri*

Main category: cs.CV

TL;DR: PhysicsNeRF improves sparse-view 3D reconstruction with physical constraints, achieving 21.4 dB PSNR using only 8 views, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Standard NeRFs fail under sparse supervision, motivating the need for a physically grounded framework to enhance reconstruction quality and generalizability.

Method: Extends Neural Radiance Fields with four constraints: depth ranking, RegNeRF-style consistency, sparsity priors, and cross-view alignment, using a compact 0.67M-parameter architecture.

Result: Achieves 21.4 dB average PSNR with 8 views, outperforming prior methods, and reveals a 5.7-6.2 dB generalization gap.

Conclusion: PhysicsNeRF enables physically consistent 3D representations for interaction and simulation, clarifying the expressiveness-generalization trade-off in constrained NeRF models.

Abstract: PhysicsNeRF is a physically grounded framework for 3D reconstruction from
sparse views, extending Neural Radiance Fields with four complementary
constraints: depth ranking, RegNeRF-style consistency, sparsity priors, and
cross-view alignment. While standard NeRFs fail under sparse supervision,
PhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB
average PSNR using only 8 views, outperforming prior methods. A generalization
gap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental
limitations of sparse-view reconstruction. PhysicsNeRF enables physically
consistent, generalizable 3D representations for agent interaction and
simulation, and clarifies the expressiveness-generalization trade-off in
constrained NeRF models.

</details>


### [386] [Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos](https://arxiv.org/pdf/2506.05546)
*Vadim Tschernezki, Diane Larlus, Iro Laina, Andrea Vedaldi*

Main category: cs.CV

TL;DR: The paper explores improving dynamic segmentation in 3D by fusing 2D motion predictions into layered radiance fields, addressing challenges with test-time refinement to outperform 2D baselines.


<details>
  <summary>Details</summary>
Motivation: Current 3D techniques struggle with dynamic phenomena, especially in segmenting moving objects, despite their success in static scenes. The paper aims to bridge this gap.

Method: Proposes Layered Motion Fusion, combining 2D motion segmentation with layered radiance fields, and uses test-time refinement to handle data complexity.

Result: The approach significantly outperforms 2D baselines in segmentation accuracy for dynamic scenes.

Conclusion: 3D techniques can enhance 2D analysis for dynamic phenomena, proving their broader applicability in realistic settings.

Abstract: Computer vision is largely based on 2D techniques, with 3D vision still
relegated to a relatively narrow subset of applications. However, by building
on recent advances in 3D models such as neural radiance fields, some authors
have shown that 3D techniques can at last improve outputs extracted from
independent 2D views, by fusing them into 3D and denoising them. This is
particularly helpful in egocentric videos, where the camera motion is
significant, but only under the assumption that the scene itself is static. In
fact, as shown in the recent analysis conducted by EPIC Fields, 3D techniques
are ineffective when it comes to studying dynamic phenomena, and, in
particular, when segmenting moving objects. In this paper, we look into this
issue in more detail. First, we propose to improve dynamic segmentation in 3D
by fusing motion segmentation predictions from a 2D-based model into layered
radiance fields (Layered Motion Fusion). However, the high complexity of long,
dynamic videos makes it challenging to capture the underlying geometric
structure, and, as a result, hinders the fusion of motion cues into the
(incomplete) scene geometry. We address this issue through test-time
refinement, which helps the model to focus on specific frames, thereby reducing
the data complexity. This results in a synergy between motion fusion and the
refinement, and in turn leads to segmentation predictions of the 3D model that
surpass the 2D baseline by a large margin. This demonstrates that 3D techniques
can enhance 2D analysis even for dynamic phenomena in a challenging and
realistic setting.

</details>


### [387] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/pdf/2506.06680)
*Radha Kodali, Venkata Rao Dhulipalla, Venkata Siva Kishor Tatavarty, Madhavi Nadakuditi, Bharadwaj Thiruveedhula, Suryanarayana Gunnam, Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: An XAI framework using CNN-LSTM for efficient and interpretable embryo classification in IVF.


<details>
  <summary>Details</summary>
Motivation: Address inefficiency in manual embryo grading for IVF by leveraging AI for accurate and explainable classification.

Method: Combines CNN and LSTM architectures (CNN-LSTM) to analyze blastocyst images for embryo viability.

Result: High accuracy in embryo classification with maintained interpretability through XAI.

Conclusion: The CNN-LSTM model offers a promising, efficient, and transparent solution for embryo selection in IVF.

Abstract: Infertility has a considerable impact on individuals' quality of life,
affecting them socially and psychologically, with projections indicating a rise
in the upcoming years. In vitro fertilization (IVF) emerges as one of the
primary techniques within economically developed nations, employed to address
the rising problem of low fertility. Expert embryologists conventionally grade
embryos by reviewing blastocyst images to select the most optimal for transfer,
yet this process is time-consuming and lacks efficiency. Blastocyst images
provide a valuable resource for assessing embryo viability. In this study, we
introduce an explainable artificial intelligence (XAI) framework for
classifying embryos, employing a fusion of convolutional neural network (CNN)
and long short-term memory (LSTM) architecture, referred to as CNN-LSTM.
Utilizing deep learning, our model achieves high accuracy in embryo
classification while maintaining interpretability through XAI.

</details>


### [388] [Evaluating Sensitivity Parameters in Smartphone-Based Gaze Estimation: A Comparative Study of Appearance-Based and Infrared Eye Trackers](https://arxiv.org/pdf/2506.11932)
*Nishan Gunawardena, Gough Yumu Lui, Bahman Javadi, Jeewani Anupama Ginige*

Main category: cs.CV

TL;DR: A smartphone-based deep-learning eye-tracking algorithm was compared to a commercial Tobii Pro Nano eye tracker, showing comparable accuracy but higher sensitivity to factors like lighting, vision correction, and age.


<details>
  <summary>Details</summary>
Motivation: To investigate the feasibility of appearance-based gaze estimation under realistic mobile usage conditions and analyze sensitivity factors.

Method: A lightweight CNN (MobileNet-V3) with LSTM was used to predict gaze coordinates from grayscale facial images. Data from 51 participants were collected using dynamic stimuli, and accuracy was measured via Euclidean distance.

Result: The deep learning model had a mean error of 17.76 mm vs. 16.53 mm for Tobii Pro Nano. It was more sensitive to lighting, vision correction, and age, with higher failure rates in low light, glasses users, and older participants.

Conclusion: Appearance-based methods show promise for mobile eye tracking, but performance varies with usage conditions, providing a framework for future evaluations.

Abstract: This study evaluates a smartphone-based, deep-learning eye-tracking algorithm
by comparing its performance against a commercial infrared-based eye tracker,
the Tobii Pro Nano. The aim is to investigate the feasibility of
appearance-based gaze estimation under realistic mobile usage conditions. Key
sensitivity factors, including age, gender, vision correction, lighting
conditions, device type, and head position, were systematically analysed. The
appearance-based algorithm integrates a lightweight convolutional neural
network (MobileNet-V3) with a recurrent structure (Long Short-Term Memory) to
predict gaze coordinates from grayscale facial images. Gaze data were collected
from 51 participants using dynamic visual stimuli, and accuracy was measured
using Euclidean distance. The deep learning model produced a mean error of
17.76 mm, compared to 16.53 mm for the Tobii Pro Nano. While overall accuracy
differences were small, the deep learning-based method was more sensitive to
factors such as lighting, vision correction, and age, with higher failure rates
observed under low-light conditions among participants using glasses and in
older age groups. Device-specific and positional factors also influenced
tracking performance. These results highlight the potential of appearance-based
approaches for mobile eye tracking and offer a reference framework for
evaluating gaze estimation systems across varied usage conditions.

</details>


### [389] [How Visual Representations Map to Language Feature Space in Multimodal LLMs](https://arxiv.org/pdf/2506.11976)
*Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda*

Main category: cs.CV

TL;DR: The paper investigates how vision-language models align visual and linguistic representations by using a frozen LLM and ViT with a linear adapter, revealing misalignment in early layers and questioning adapter-based architectures.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms of alignment in VLMs while preserving the original language representations of LLMs.

Method: Uses a frozen LLM and ViT with a linear adapter, analyzing alignment via pre-trained sparse autoencoders (SAEs).

Result: Visual representations align with language features in middle-to-later layers, indicating misalignment in early layers.

Conclusion: Current adapter-based architectures may not optimally facilitate cross-modal representation learning.

Abstract: Effective multimodal reasoning depends on the alignment of visual and
linguistic representations, yet the mechanisms by which vision-language models
(VLMs) achieve this alignment remain poorly understood. Following the LiMBeR
framework, we deliberately maintain a frozen large language model (LLM) and a
frozen vision transformer (ViT), connected solely by training a linear adapter
during visual instruction tuning. By keeping the language model frozen, we
ensure it maintains its original language representations without adaptation to
visual data. Consequently, the linear adapter must map visual features directly
into the LLM's existing representational space rather than allowing the
language model to develop specialized visual understanding through fine-tuning.
Our experimental design uniquely enables the use of pre-trained sparse
autoencoders (SAEs) of the LLM as analytical probes. These SAEs remain
perfectly aligned with the unchanged language model and serve as a snapshot of
the learned language feature-representations. Through systematic analysis of
SAE reconstruction error, sparsity patterns, and feature SAE descriptions, we
reveal the layer-wise progression through which visual representations
gradually align with language feature representations, converging in
middle-to-later layers. This suggests a fundamental misalignment between ViT
outputs and early LLM layers, raising important questions about whether current
adapter-based architectures optimally facilitate cross-modal representation
learning.

</details>


### [390] [CLIP-HandID: Vision-Language Model for Hand-Based Person Identification](https://arxiv.org/pdf/2506.12447)
*Nathanael L. Baisa, Babu Pallam, Amudhavel Jayavel*

Main category: cs.CV

TL;DR: CLIP-HandID uses CLIP's vision-language model for person identification from hand images, excelling in criminal cases with limited evidence.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of identifying individuals in serious crimes like sexual abuse, where hand images are often the only available evidence.

Method: Leverages CLIP's image encoder with textual prompts and a textual inversion network to learn pseudo-tokens for enhanced feature representation.

Result: Outperforms existing methods on large, multi-ethnic hand datasets.

Conclusion: CLIP-HandID is a robust solution for person identification using hand images, especially in forensic contexts.

Abstract: This paper introduces a novel approach to person identification using hand
images, designed specifically for criminal investigations. The method is
particularly valuable in serious crimes such as sexual abuse, where hand images
are often the only identifiable evidence available. Our proposed method,
CLIP-HandID, leverages a pre-trained foundational vision-language model - CLIP
- to efficiently learn discriminative deep feature representations from hand
images (input to CLIP's image encoder) using textual prompts as semantic
guidance. Since hand images are labeled with indexes rather than text
descriptions, we employ a textual inversion network to learn pseudo-tokens that
encode specific visual contexts or appearance attributes. These learned
pseudo-tokens are then incorporated into textual prompts, which are fed into
CLIP's text encoder to leverage its multi-modal reasoning and enhance
generalization for identification. Through extensive evaluations on two large,
publicly available hand datasets with multi-ethnic representation, we
demonstrate that our method significantly outperforms existing approaches.

</details>


### [391] [STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation](https://arxiv.org/pdf/2506.13138)
*Jiamin Wang, Yichen Yao, Xiang Feng, Hang Wu, Yaming Wang, Qingqiu Huang, Yuexin Ma, Xinge Zhu*

Main category: cs.CV

TL;DR: STAGE introduces hierarchical feature coordination and multi-phase optimization for long-horizon driving video generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing error accumulation and feature misalignment in existing approaches for autonomous driving world modeling.

Method: Uses Hierarchical Temporal Feature Transfer (HTFT) and a multi-stage training strategy for sustainable video synthesis.

Result: STAGE significantly surpasses existing methods on the Nuscenes dataset and generates 600 high-quality frames.

Conclusion: STAGE enables high-fidelity, long-horizon driving video generation with superior performance and scalability.

Abstract: The generation of temporally consistent, high-fidelity driving videos over
extended horizons presents a fundamental challenge in autonomous driving world
modeling. Existing approaches often suffer from error accumulation and feature
misalignment due to inadequate decoupling of spatio-temporal dynamics and
limited cross-frame feature propagation mechanisms. To address these
limitations, we present STAGE (Streaming Temporal Attention Generative Engine),
a novel auto-regressive framework that pioneers hierarchical feature
coordination and multi-phase optimization for sustainable video synthesis. To
achieve high-quality long-horizon driving video generation, we introduce
Hierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training
strategy. HTFT enhances temporal consistency between video frames throughout
the video generation process by modeling the temporal and denoising process
separately and transferring denoising features between frames. The multi-stage
training strategy is to divide the training into three stages, through model
decoupling and auto-regressive inference process simulation, thereby
accelerating model convergence and reducing error accumulation. Experiments on
the Nuscenes dataset show that STAGE has significantly surpassed existing
methods in the long-horizon driving video generation task. In addition, we also
explored STAGE's ability to generate unlimited-length driving videos. We
generated 600 frames of high-quality driving videos on the Nuscenes dataset,
which far exceeds the maximum length achievable by existing methods.

</details>


### [392] [Discrete JEPA: Learning Discrete Token Representations without Reconstruction](https://arxiv.org/pdf/2506.14373)
*Junyeob Baek, Hosung Lee, Christopher Hoang, Mengye Ren, Sungjin Ahn*

Main category: cs.CV

TL;DR: Discrete-JEPA improves symbolic reasoning in AI by enhancing image tokenization with semantic tokenization and complementary objectives, outperforming baselines in visual symbolic prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Current image tokenization methods lack symbolic abstraction and logical reasoning capabilities needed for systematic inference.

Method: Extends latent predictive coding with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning.

Result: Outperforms baselines in visual symbolic prediction tasks and shows emergent systematic patterns in the learned token space.

Conclusion: Discrete-JEPA has significant potential for advancing symbolic world modeling and planning in AI systems.

Abstract: The cornerstone of cognitive intelligence lies in extracting hidden patterns
from observations and leveraging these principles to systematically predict
future outcomes. However, current image tokenization methods demonstrate
significant limitations in tasks requiring symbolic abstraction and logical
reasoning capabilities essential for systematic inference. To address this
challenge, we propose Discrete-JEPA, extending the latent predictive coding
framework with semantic tokenization and novel complementary objectives to
create robust tokenization for symbolic reasoning tasks. Discrete-JEPA
dramatically outperforms baselines on visual symbolic prediction tasks, while
striking visual evidence reveals the spontaneous emergence of deliberate
systematic patterns within the learned semantic token space. Though an initial
model, our approach promises a significant impact for advancing Symbolic world
modeling and planning capabilities in artificial intelligence systems.

</details>


### [393] [Exploring Diffusion with Test-Time Training on Efficient Image Restoration](https://arxiv.org/pdf/2506.14541)
*Rongchang Lu, Tianduo Luo, Yunzhi Jiang, Conghan Yue, Pei Yang, Guibao Liu, Changyang Gu*

Main category: cs.CV

TL;DR: DiffRWKVIR is a novel image restoration framework combining Test-Time Training (TTT) with efficient diffusion, addressing feature fusion, computational bottlenecks, and diffusion inefficiency. It introduces three innovations for global awareness, parallelism, and faster training/inference, outperforming existing methods in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in image restoration like ineffective feature fusion, computational bottlenecks, and inefficient diffusion processes.

Method: Proposes DiffRWKVIR, featuring Omni-Scale 2D State Evolution, Chunk-Optimized Flash Processing, and Prior-Guided Efficient Diffusion for improved efficiency and performance.

Result: Outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency on benchmarks like Set5, Set14, BSD100, Urban100, and Places365.

Conclusion: Establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.

Abstract: Image restoration faces challenges including ineffective feature fusion,
computational bottlenecks and inefficient diffusion processes. To address
these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training
(TTT) with efficient diffusion. Our approach introduces three key innovations:
(1) Omni-Scale 2D State Evolution extends RWKV's location-dependent
parameterization to hierarchical multi-directional 2D scanning, enabling global
contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash
Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk
processing (O(LCd) complexity), reducing sequential dependencies and
computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact
Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster
training/inference than DiffIR while solving computational inefficiency in
denoising. Evaluated across super-resolution and inpainting benchmarks (Set5,
Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and
MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes
a new paradigm for adaptive, high-efficiency image restoration with optimized
hardware utilization.

</details>


### [394] [Cost-Aware Routing for Efficient Text-To-Image Generation](https://arxiv.org/pdf/2506.14753)
*Qinchan Li, Kenneth Chen, Changyue Su, Wittawat Jitkrittum, Qi Sun, Patsorn Sangkloy*

Main category: cs.CV

TL;DR: A framework optimizes computational cost and quality in diffusion models by routing prompts to appropriate text-to-image functions based on complexity.


<details>
  <summary>Details</summary>
Motivation: Balance high-fidelity image generation with computational efficiency by varying computation per prompt.

Method: Automatically route prompts to the most suitable text-to-image function (e.g., varying denoising steps or different models).

Result: Achieves higher average quality than any single model alone, as shown on COCO and DiffusionDB.

Conclusion: The approach optimally trades off quality and cost by reserving expensive computations for complex prompts.

Abstract: Diffusion models are well known for their ability to generate a high-fidelity
image for an input prompt through an iterative denoising process.
Unfortunately, the high fidelity also comes at a high computational cost due
the inherently sequential generative process. In this work, we seek to
optimally balance quality and computational cost, and propose a framework to
allow the amount of computation to vary for each prompt, depending on its
complexity. Each prompt is automatically routed to the most appropriate
text-to-image generation function, which may correspond to a distinct number of
denoising steps of a diffusion model, or a disparate, independent text-to-image
model. Unlike uniform cost reduction techniques (e.g., distillation, model
quantization), our approach achieves the optimal trade-off by learning to
reserve expensive choices (e.g., 100+ denoising steps) only for a few complex
prompts, and employ more economical choices (e.g., small distilled model) for
less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB
that by learning to route to nine already-trained text-to-image models, our
approach is able to deliver an average quality that is higher than that
achievable by any of these models alone.

</details>


### [395] [R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision](https://arxiv.org/pdf/2506.16262)
*Weeyoung Kwon, Jeahun Sung, Minkyu Jeon, Chanho Eom, Jihyong Oh*

Main category: cs.CV

TL;DR: A survey on 3D Low-Level Vision (3D LLV) for robust rendering, restoration, and enhancement of degraded 3D scenes, addressing challenges like noise, blur, and low-resolution inputs.


<details>
  <summary>Details</summary>
Motivation: Existing neural rendering methods assume clean, high-resolution inputs, limiting robustness under real-world degradations. 3D LLV aims to extend 2D low-level vision tasks into 3D for reliable scene reconstruction.

Method: Formalizes degradation-aware rendering, reviews methods integrating low-level vision into neural rendering, and categorizes approaches for handling spatio-temporal consistency and ill-posed optimization.

Result: Identifies key challenges and showcases methods enabling high-fidelity 3D reconstruction under adverse conditions, with applications in autonomous driving, AR/VR, and robotics.

Conclusion: Positions 3D LLV as essential for robust 3D content generation in real-world environments, highlighting its potential in various domains.

Abstract: Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have achieved significant progress in photorealistic
3D scene reconstruction and novel view synthesis. However, most existing models
assume clean and high-resolution (HR) multi-view inputs, which limits their
robustness under real-world degradations such as noise, blur, low-resolution
(LR), and weather-induced artifacts. To address these limitations, the emerging
field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision
tasks including super-resolution (SR), deblurring, weather degradation removal,
restoration, and enhancement into the 3D spatial domain. This survey, referred
to as R\textsuperscript{3}eVision, provides a comprehensive overview of robust
rendering, restoration, and enhancement for 3D LLV by formalizing the
degradation-aware rendering problem and identifying key challenges related to
spatio-temporal consistency and ill-posed optimization. Recent methods that
integrate LLV into neural rendering frameworks are categorized to illustrate
how they enable high-fidelity 3D reconstruction under adverse conditions.
Application domains such as autonomous driving, AR/VR, and robotics are also
discussed, where reliable 3D perception from degraded inputs is critical. By
reviewing representative methods, datasets, and evaluation protocols, this work
positions 3D LLV as a fundamental direction for robust 3D content generation
and scene-level reconstruction in real-world environments.

</details>


### [396] [Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation](https://arxiv.org/pdf/2506.16318)
*Carmelo Scribano, Elena Govi, Paolo Bertellini, Simone Parisi, Giorgia Franchini, Marko Bertogna*

Main category: cs.CV

TL;DR: A pipeline for agricultural field boundary mapping using a fine-tuned Segment Anything Model (SAM), with a new regional dataset (ERAS) for improved accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Accurate field mapping is crucial for efficient agriculture, and automated methods can reduce reliance on costly ground surveys.

Method: Fine-tuning SAM for field delineation, supplemented by a new regional dataset (ERAS) to extend coverage beyond existing datasets.

Result: The approach offers robust automated field delineation, with the ERAS dataset now publicly available.

Conclusion: The method provides a scalable solution for agricultural field mapping, enhancing accuracy and reducing costs.

Abstract: Accurate mapping of agricultural field boundaries is essential for the
efficient operation of agriculture. Automatic extraction from high-resolution
satellite imagery, supported by computer vision techniques, can avoid costly
ground surveys. In this paper, we present a pipeline for field delineation
based on the Segment Anything Model (SAM), introducing a fine-tuning strategy
to adapt SAM to this task. In addition to using published datasets, we describe
a method for acquiring a complementary regional dataset that covers areas
beyond current sources. Extensive experiments assess segmentation accuracy and
evaluate the generalization capabilities. Our approach provides a robust
baseline for automated field delineation. The new regional dataset, known as
ERAS, is now publicly available.

</details>


### [397] [A Comparative Analysis of Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) as Dimensionality Reduction Techniques](https://arxiv.org/pdf/2506.16663)
*Michael Gyimadu, Gregory Bell, Ph. D*

Main category: cs.CV

TL;DR: The paper compares PCA and SVD for dimensionality reduction, focusing on interpretability, numerical stability, and matrix shape suitability, offering guidelines for choosing between them.


<details>
  <summary>Details</summary>
Motivation: High-dimensional image data often need dimensionality reduction, and a clear comparison of PCA and SVD is lacking.

Method: Derived PCA and SVD from first principles, analyzed their interpretability, numerical stability, and suitability for different matrix shapes.

Result: Provided rule-of-thumb guidelines for selecting PCA or SVD without empirical benchmarking.

Conclusion: Limitations and future experimental directions are noted, emphasizing the need for further validation.

Abstract: High-dimensional image data often require dimensionality reduction before
further analysis. This paper provides a purely analytical comparison of two
linear techniques-Principal Component Analysis (PCA) and Singular Value
Decomposition (SVD). After the derivation of each algorithm from first
principles, we assess their interpretability, numerical stability, and
suitability for differing matrix shapes. building on classical and recent
numerical literature, We synthesize rule-of-thumb guidelines for choosing one
out of the two algorithms without empirical benchmarking, building on classical
and recent numerical literature. Limitations and directions for future
experimental work are outlined at the end.

</details>


### [398] [RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought](https://arxiv.org/pdf/2506.16796)
*Junbo Qiao, Miaomiao Cai, Wei Li, Yutong Liu, Xudong Huang, Gaoqi He, Jiao Xie, Jie Hu, Xinghao Chen, Shaohui Lin*

Main category: cs.CV

TL;DR: RealSR-R1 introduces VLCoT-GRPO, a framework combining vision and language reasoning for Real-World Image Super-Resolution, using GRPO and four reward functions to improve detail restoration and realism.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to accurately understand degraded images, leading to low-fidelity and unnatural reconstructions.

Method: Proposes VLCoT framework inspired by CoT in LLMs, integrating vision and language reasoning, and introduces GRPO with four reward functions for better generalization.

Result: RealSR-R1 generates realistic details and accurately understands image content, especially in complex or severely degraded scenes.

Conclusion: The VLCoT-GRPO framework significantly improves Real-World Image Super-Resolution by enhancing understanding and generation quality.

Abstract: Real-World Image Super-Resolution is one of the most challenging task in
image restoration. However, existing methods struggle with an accurate
understanding of degraded image content, leading to reconstructed results that
are both low-fidelity and unnatural. We present RealSR-R1 in this work, which
empowers the RealSR models with understanding and reasoning capabilities.
Inspired by the success of Chain of Thought (CoT) in large language models
(LLMs), we simulate the human process of handling degraded images and propose
the VLCoT framework, which integrates vision and language reasoning. The
framework aims to precisely restore image details by progressively generating
more comprehensive text and higher-resolution images. To overcome the challenge
of traditional supervised learning CoT failing to generalize to real-world
scenarios, we introduce, for the first time, Group Relative Policy Optimization
(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO
as a solution, which designs four reward functions: (1) Format reward, used to
standardize the CoT process; (2) Degradation reward, to incentivize accurate
degradation estimation; (3) Understanding reward, to ensure the accuracy of the
generated content; and (4) Generation reward, where we propose using a visual
expert model to evaluate the quality of generated images, encouraging the model
to generate more realistic images. Extensive experiments demonstrate that our
proposed RealSR-R1 can generate realistic details and accurately understand
image content, particularly in semantically rich scenes or images with severe
degradation.

</details>


### [399] [Multi-label Scene Classification for Autonomous Vehicles: Acquiring and Accumulating Knowledge from Diverse Datasets](https://arxiv.org/pdf/2506.17101)
*Ke Li, Chenyu Zhang, Yuxin Ding, Xianbiao Hu, Ruwen Qin*

Main category: cs.CV

TL;DR: The paper introduces KAA-CAL, a novel learning system combining knowledge acquisition and accumulation (KAA) with consistency-based active learning (CAL) to improve multi-label driving scene identification, outperforming baselines and SOTA models with less data.


<details>
  <summary>Details</summary>
Motivation: Enhancing autonomous vehicles' contextual awareness by addressing challenges in multi-label classification for driving scenes, such as dataset imbalance and task learning balance.

Method: Uses KAA to gather knowledge from single-label datasets via monotask learning and CAL to bridge the gap between single-label and multi-label data.

Result: Achieves a 56.1% performance boost over the baseline, with KAA contributing 31.3% and CAL 24.8%. Outperforms SOTA models on BDD100K and HSD datasets using 85% less data.

Conclusion: KAA-CAL effectively addresses multi-label classification challenges, offering a scalable and efficient solution for driving scene identification.

Abstract: Driving scene identification, which assigns multiple non-exclusive class
labels to a scene, provides the contextual awareness necessary for enhancing
autonomous vehicles' ability to understand, reason about, and interact with the
complex driving environment. As a multi-label classification problem, it is
better tackled via multitasking learning. However, directly training a
multi-label classification model for driving scene identification through
multitask learning presents two main challenges: acquiring a balanced,
comprehensively annotated multi-label dataset and balancing learning across
different tasks. This paper introduces a novel learning system that synergizes
knowledge acquisition and accumulation (KAA) with consistency-based active
learning (CAL) to address those challenges. KAA acquires and accumulates
knowledge about scene identification from various single-label datasets via
monotask learning. Subsequently, CAL effectively resolves the knowledge gap
caused by the discrepancy between single-label and multi-label data. An
ablation study on our Driving Scene Identification (DSI) dataset demonstrates a
56.1% performance increase over the baseline model pretrained on ImageNet. Of
this, KAA accounts for 31.3% of the gain, and CAL contributes 24.8%. Moreover,
KAA-CAL stands out as the best performer when compared to state-of-the-art
(SOTA) multi-label models on two public datasets, BDD100K and HSD, achieving
this while using 85% less data. The DSI dataset and the implementation code for
KAA-CAL are available at https://github.com/KELISBU/KAA-CAL .

</details>


### [400] [Emergent Temporal Correspondences from Video Diffusion Transformers](https://arxiv.org/pdf/2506.17220)
*Jisu Nam, Soowon Son, Dahyun Chung, Jiyoung Kim, Siyoon Jin, Junhwa Hur, Seungryong Kim*

Main category: cs.CV

TL;DR: DiffTrack is a framework analyzing how video diffusion models (DiTs) establish temporal correspondences, revealing key insights and enabling practical applications like zero-shot tracking and motion-enhanced video generation.


<details>
  <summary>Details</summary>
Motivation: To understand how video diffusion models internally represent temporal correspondences across frames, a question not yet addressed.

Method: DiffTrack constructs a dataset with pseudo ground-truth tracking annotations and introduces evaluation metrics to analyze the 3D attention mechanism of DiTs.

Result: Query-key similarities in specific layers are critical for temporal matching, improving during denoising. DiffTrack achieves state-of-the-art in zero-shot tracking and enhances video generation.

Conclusion: DiffTrack provides insights into video DiTs' temporal understanding, enabling further research and applications.

Abstract: Recent advancements in video diffusion models based on Diffusion Transformers
(DiTs) have achieved remarkable success in generating temporally coherent
videos. Yet, a fundamental question persists: how do these models internally
establish and represent temporal correspondences across frames? We introduce
DiffTrack, the first quantitative analysis framework designed to answer this
question. DiffTrack constructs a dataset of prompt-generated video with pseudo
ground-truth tracking annotations and proposes novel evaluation metrics to
systematically analyze how each component within the full 3D attention
mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to
establishing temporal correspondences. Our analysis reveals that query-key
similarities in specific, but not all, layers play a critical role in temporal
matching, and that this matching becomes increasingly prominent during the
denoising process. We demonstrate practical applications of DiffTrack in
zero-shot point tracking, where it achieves state-of-the-art performance
compared to existing vision foundation and self-supervised video models.
Further, we extend our findings to motion-enhanced video generation with a
novel guidance method that improves temporal consistency of generated videos
without additional training. We believe our work offers crucial insights into
the inner workings of video DiTs and establishes a foundation for further
research and applications leveraging their temporal understanding.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [401] [Evaluating Generalization and Representation Stability in Small LMs via Prompting](https://arxiv.org/pdf/2506.17289)
*Rahul Raja, Arpita Vats*

Main category: cs.AI

TL;DR: Comparative study of few-shot prompting and supervised fine-tuning in small language models, focusing on generalization, robustness, and internal representations.


<details>
  <summary>Details</summary>
Motivation: To understand the robustness and generalization of small language models under different adaptation strategies (prompting vs. fine-tuning) in low-resource and OOD settings.

Method: Comparative analysis across task formats, prompt styles, and model scales, evaluating accuracy and internal representations in in-distribution and OOD settings.

Result: Highlights critical differences in how models internalize and generalize knowledge, providing practical guidance for low-data regimes.

Conclusion: Offers empirical insights into the debate over prompting versus fine-tuning, aiding model selection in resource-constrained scenarios.

Abstract: We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings.
  Beyond accuracy, we analyze the internal representations learned by each
approach to assess the stability and abstraction of task-specific features. Our
findings highlight critical differences in how small models internalize and
generalize knowledge under different adaptation strategies. This work offers
practical guidance for model selection in low-data regimes and contributes
empirical insight into the ongoing debate over prompting versus fine-tuning.
Code for the experiments is available at the following

</details>


### [402] [Individual Causal Inference with Structural Causal Model](https://arxiv.org/pdf/2506.17300)
*Daniel T. Chang*

Main category: cs.AI

TL;DR: The paper proposes Individual Causal Inference (ICI) using Structural Causal Models (SCM) to estimate individual-specific causal effects, addressing challenges of limited individual data and population-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional causal inference methods are population-based, making it difficult to estimate individual causal effects (ICE) due to limited data and inherent population-level assumptions.

Method: The authors introduce the indiv-operator (indiv(W)) and individual causal queries (P(Y | indiv(W), do(X), Z)) within SCM to formalize ICI, leveraging exogenous variables for individualization.

Result: ICI with SCM is shown to focus on individual alternatives (possible scenarios) rather than counterfactuals (non-actual scenarios), providing a framework for personalized causal inference.

Conclusion: The proposed ICI framework advances causal inference by enabling individualized effect estimation, bridging the gap between population-based methods and individual-specific analysis.

Abstract: Individual causal inference (ICI) uses causal inference methods to understand
and predict the effects of interventions on individuals, considering their
specific characteristics / facts. It aims to estimate individual causal effect
(ICE), which varies across individuals. Estimating ICE can be challenging due
to the limited data available for individuals, and the fact that most causal
inference methods are population-based. Structural Causal Model (SCM) is
fundamentally population-based. Therefore, causal discovery (structural
learning and parameter learning), association queries and intervention queries
are all naturally population-based. However, exogenous variables (U) in SCM can
encode individual variations and thus provide the mechanism for individualized
population per specific individual characteristics / facts. Based on this, we
propose ICI with SCM as a "rung 3" causal inference, because it involves
"imagining" what would be the causal effect of a hypothetical intervention on
an individual, given the individual's observed characteristics / facts.
Specifically, we propose the indiv-operator, indiv(W), to formalize/represent
the population individualization process, and the individual causal query, P(Y
| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI
with SCM is inference on individual alternatives (possible), not individual
counterfactuals (non-actual).

</details>


### [403] [Resource Rational Contractualism Should Guide AI Alignment](https://arxiv.org/pdf/2506.17434)
*Sydney Levine, Matija Franklin, Tan Zhi-Xuan, Secil Yanik Guyot, Lionel Wong, Daniel Kilov, Yejin Choi, Joshua B. Tenenbaum, Noah Goodman, Seth Lazar, Iason Gabriel*

Main category: cs.AI

TL;DR: AI systems need to align decisions with human values. Resource-Rational Contractualism (RRC) proposes using heuristics to approximate agreements efficiently.


<details>
  <summary>Details</summary>
Motivation: AI must navigate human environments with diverse goals and values, requiring scalable alignment solutions.

Method: RRC uses normatively-grounded, cognitively-inspired heuristics to approximate rational agreements.

Result: RRC enables efficient, adaptable AI decision-making aligned with human values.

Conclusion: RRC offers a scalable framework for AI alignment in dynamic human environments.

Abstract: AI systems will soon have to navigate human environments and make decisions
that affect people and other AI agents whose goals and values diverge.
Contractualist alignment proposes grounding those decisions in agreements that
diverse stakeholders would endorse under the right conditions, yet securing
such agreement at scale remains costly and slow -- even for advanced AI. We
therefore propose Resource-Rational Contractualism (RRC): a framework where AI
systems approximate the agreements rational parties would form by drawing on a
toolbox of normatively-grounded, cognitively-inspired heuristics that trade
effort for accuracy. An RRC-aligned agent would not only operate efficiently,
but also be equipped to dynamically adapt to and interpret the ever-changing
human social world.

</details>


### [404] [Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation](https://arxiv.org/pdf/2506.17442)
*Hao Guan, David Bates, Li Zhou*

Main category: cs.AI

TL;DR: The paper discusses the challenges of AI performance degradation in healthcare and reviews methods for monitoring, detecting, and correcting such issues to ensure reliable AI systems.


<details>
  <summary>Details</summary>
Motivation: AI systems in healthcare face performance degradation due to dynamic clinical environments, risking reliability and safety. This paper addresses the need for solutions to maintain AI system 'health.'

Method: The review examines causes of degradation, techniques for detecting drift, root cause analysis, and correction strategies like retraining and adaptation.

Result: The paper provides insights into monitoring and maintaining AI performance, covering traditional and advanced models like LLMs, and identifies their strengths and limitations.

Conclusion: The work highlights ongoing challenges and future research directions to develop robust medical AI systems for long-term, safe deployment.

Abstract: Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

</details>


### [405] [OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections](https://arxiv.org/pdf/2506.17449)
*Manasa Bharadwaj, Nikhil Verma, Kevin Ferreira*

Main category: cs.AI

TL;DR: OmniReflect introduces a hierarchical, reflection-driven framework with a 'constitution' to enhance LLM agent performance, showing significant gains in task success across environments.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent improvements lack generalizable long-term learning and efficiency in dynamic environments.

Method: OmniReflect uses Self-sustaining and Co-operative modes, employing Neural, Symbolic, and NeuroSymbolic techniques to create guiding principles.

Result: Absolute task success improvements: +10.3% (ALFWorld), +23.8% (BabyAI), +8.3% (PDDL) in Self-sustaining mode; lightweight Qwen3-4B ReAct agent outperforms baselines in Co-operative mode.

Conclusion: OmniReflect is robust and effective across diverse environments and model backbones.

Abstract: Efforts to improve Large Language Model (LLM) agent performance on complex
tasks have largely focused on fine-tuning and iterative self-correction.
However, these approaches often lack generalizable mechanisms for longterm
learning and remain inefficient in dynamic environments. We introduce
OmniReflect, a hierarchical, reflection-driven framework that constructs a
constitution, a compact set of guiding principles distilled from task
experiences, to enhance the effectiveness and efficiency of an LLM agent.
OmniReflect operates in two modes: Self-sustaining, where a single agent
periodically curates its own reflections during task execution, and
Co-operative, where a Meta-advisor derives a constitution from a small
calibration set to guide another agent. To construct these constitutional
principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering
a balance between contextual adaptability and computational efficiency.
Empirical results averaged across models show major improvements in task
success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%
on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative
mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion
baselines on BabyAI. These findings highlight the robustness and effectiveness
of OmniReflect across environments and backbones.

</details>


### [406] [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/pdf/2506.17788)
*Shahab Rahimirad, Guven Gergerli, Lucia Romero, Angela Qian, Matthew Lyle Olson, Simon Stepputtis, Joseph Campbell*

Main category: cs.AI

TL;DR: A hybrid reasoning framework combining LLMs and probabilistic models improves social reasoning in Avalon, outperforming humans with a 67% win rate.


<details>
  <summary>Details</summary>
Motivation: Social reasoning is challenging for LLMs, especially in games like Avalon, where current models degrade when distilled for real-time use.

Method: A hybrid framework externalizes belief inference to a probabilistic model while using an LLM for language tasks.

Result: The approach matches larger models' performance and achieves a 67% win rate against humans, with higher qualitative ratings.

Conclusion: The hybrid framework advances social reasoning in LLMs, offering a scalable solution with released resources for future research.

Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial
observations of other agents - remains a challenging task for large language
models (LLMs). We evaluate the limits of current reasoning language models in
the social deduction game Avalon and find that while the largest models
demonstrate strong performance, they require extensive test-time inference and
degrade sharply when distilled to smaller, real-time-capable variants. To
address this, we introduce a hybrid reasoning framework that externalizes
belief inference to a structured probabilistic model, while using an LLM for
language understanding and interaction. Our approach achieves competitive
performance with much larger models in Agent-Agent play and, notably, is the
first language agent to defeat human players in a controlled study - achieving
a 67% win rate and receiving higher qualitative ratings than both reasoning
baselines and human teammates. We release code, models, and a dataset to
support future work on social reasoning in LLM agents, which can be found at
https://camp-lab-purdue.github.io/bayesian-social-deduction/

</details>


### [407] [From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases](https://arxiv.org/pdf/2506.17484)
*Yao Zhang, Zaixi Shang, Silpan Patel, Mikel Zuniga*

Main category: cs.AI

TL;DR: A novel offline-first methodology using LLMs-based multi-agent system transforms noisy support tickets into a structured knowledge base, improving RAG system performance and operational efficiency.


<details>
  <summary>Details</summary>
Motivation: Critical knowledge in supply chain operations is buried in unstructured communications, and existing RAG systems struggle with raw data challenges like noise and inconsistency.

Method: A multi-agent system with three specialized agents (Category Discovery, Categorization, Knowledge Synthesis) processes support tickets offline to create a compact, high-quality knowledge base.

Result: The system reduces ticket data volume to 3.4%, improves RAG performance (48.74% vs. 38.60% helpful answers), and cuts unhelpful responses by 77.4%.

Conclusion: The approach transforms transient communications into reusable knowledge, enhancing operational efficiency and automating 50% of future ticket resolutions.

Abstract: Supply chain operations generate vast amounts of operational data; however,
critical knowledge such as system usage practices, troubleshooting workflows,
and resolution techniques often remains buried within unstructured
communications like support tickets, emails, and chat logs. While RAG systems
aim to leverage such communications as a knowledge base, their effectiveness is
limited by raw data challenges: support tickets are typically noisy,
inconsistent, and incomplete, making direct retrieval suboptimal. Unlike
existing RAG approaches that focus on runtime optimization, we introduce a
novel offline-first methodology that transforms these communications into a
structured knowledge base. Our key innovation is a LLMs-based multi-agent
system orchestrating three specialized agents: Category Discovery for taxonomy
creation, Categorization for ticket grouping, and Knowledge Synthesis for
article generation. Applying our methodology to real-world support tickets with
resolution notes and comments, our system creates a compact knowledge base -
reducing total volume to just 3.4% of original ticket data while improving
quality. Experiments demonstrate that our prebuilt knowledge base in RAG
systems significantly outperforms traditional RAG implementations (48.74% vs.
38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.
By automating institutional knowledge capture that typically remains siloed in
experts' heads, our solution translates to substantial operational efficiency:
reducing support workload, accelerating resolution times, and creating
self-improving systems that automatically resolve approximately 50% of future
supply chain tickets. Our approach addresses a key gap in knowledge management
by transforming transient communications into structured, reusable knowledge
through intelligent offline processing rather than latency-inducing runtime
architectures.

</details>


### [408] [Kaleidoscopic Teaming in Multi Agent Simulations](https://arxiv.org/pdf/2506.17514)
*Ninareh Mehrabi, Tharindu Kumarage, Kai-Wei Chang, Aram Galstyan, Rahul Gupta*

Main category: cs.AI

TL;DR: The paper introduces 'kaleidoscopic teaming' to evaluate safety risks in AI agents, addressing gaps in existing frameworks for both single- and multi-agent scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing safety evaluation frameworks fail to assess complex behaviors and multi-agent interactions, necessitating a more comprehensive approach.

Method: A new framework generates diverse scenarios mimicking real-world societies, testing agents in single- and multi-agent setups with in-context optimization techniques.

Result: The framework identifies safety vulnerabilities in various AI models, demonstrating its effectiveness.

Conclusion: Kaleidoscopic teaming provides a robust method for evaluating and improving AI agent safety in complex scenarios.

Abstract: Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous
tool usage capabilities and their integration in various real-world
applications. This autonomy poses novel challenges for the safety of such
systems, both in single- and multi-agent scenarios. We argue that existing red
teaming or safety evaluation frameworks fall short in evaluating safety risks
in complex behaviors, thought processes and actions taken by agents. Moreover,
they fail to consider risks in multi-agent setups where various vulnerabilities
can be exposed when agents engage in complex behaviors and interactions with
each other. To address this shortcoming, we introduce the term kaleidoscopic
teaming which seeks to capture complex and wide range of vulnerabilities that
can happen in agents both in single-agent and multi-agent scenarios. We also
present a new kaleidoscopic teaming framework that generates a diverse array of
scenarios modeling real-world human societies. Our framework evaluates safety
of agents in both single-agent and multi-agent setups. In single-agent setup,
an agent is given a scenario that it needs to complete using the tools it has
access to. In multi-agent setup, multiple agents either compete against or
cooperate together to complete a task in the scenario through which we capture
existing safety vulnerabilities in agents. We introduce new in-context
optimization techniques that can be used in our kaleidoscopic teaming framework
to generate better scenarios for safety analysis. Lastly, we present
appropriate metrics that can be used along with our framework to measure safety
of agents. Utilizing our kaleidoscopic teaming framework, we identify
vulnerabilities in various models with respect to their safety in agentic
use-cases.

</details>


### [409] [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/pdf/2506.17585)
*Yukun Huang, Sanxing Chen, Jian Pei, Manzil Zaheer, Bhuwan Dhingra*

Main category: cs.AI

TL;DR: The paper explores training LLMs to reliably attribute answers to documents seen during pretraining without test-time retrieval, proposing Active Indexing over Passive Indexing for better citation performance.


<details>
  <summary>Details</summary>
Motivation: Current LLMs often provide unreliable citations due to hallucination, and reliance on external retrievers introduces latency and noise. The goal is to improve citation reliability directly through training.

Method: A two-stage approach: (1) continual pretraining to bind facts to document identifiers, and (2) instruction tuning to elicit citation behavior. Active Indexing uses synthetic QA pairs for diverse fact restatement and bidirectional generation.

Result: Active Indexing outperforms Passive Indexing, with citation precision gains up to 30.2%. Performance improves with more augmented data, even at 16x the original token count.

Conclusion: Active Indexing is effective for training LLMs to provide reliable citations without test-time retrieval, showing scalability with augmented data.

Abstract: Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

</details>


### [410] [Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown](https://arxiv.org/pdf/2506.17589)
*Bowen Wang*

Main category: cs.AI

TL;DR: The paper explores enhancing multimodal large language models (MLLMs) for domain-specific tasks by constructing a multimodal knowledge graph (MH-MMKG) and proposing a multi-agent retriever for autonomous knowledge retrieval.


<details>
  <summary>Details</summary>
Motivation: MLLMs often fail in rarely encountered domain-specific tasks due to limited relevant knowledge, highlighting the need for effective knowledge harnessing.

Method: Constructed MH-MMKG for visual game cognition (Monster Hunter: World) and designed challenging queries. Proposed a multi-agent retriever for autonomous knowledge retrieval.

Result: The approach significantly improved MLLMs' performance in complex knowledge retrieval and reasoning.

Conclusion: The study provides a new perspective on multimodal knowledge-augmented reasoning and a foundation for future research.

Abstract: The real value of knowledge lies not just in its accumulation, but in its
potential to be harnessed effectively to conquer the unknown. Although recent
multimodal large language models (MLLMs) exhibit impressing multimodal
capabilities, they often fail in rarely encountered domain-specific tasks due
to limited relevant knowledge. To explore this, we adopt visual game cognition
as a testbed and select Monster Hunter: World as the target to construct a
multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and
intricate entity relations. We also design a series of challenging queries
based on MH-MMKG to evaluate the models' ability for complex knowledge
retrieval and reasoning. Furthermore, we propose a multi-agent retriever that
enables a model to autonomously search relevant knowledge without additional
training. Experimental results show that our approach significantly enhances
the performance of MLLMs, providing a new perspective on multimodal
knowledge-augmented reasoning and laying a solid foundation for future
research.

</details>


### [411] [Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges](https://arxiv.org/pdf/2506.17644)
*Zimo Ji, Daoyuan Wu, Wenyuan Jiang, Pingchuan Ma, Zongjie Li, Shuai Wang*

Main category: cs.AI

TL;DR: The paper introduces CTFKnow, a benchmark for evaluating LLMs' technical knowledge in CTF challenges, and proposes CTFAgent, a framework improving LLMs' problem-solving performance by over 80%.


<details>
  <summary>Details</summary>
Motivation: To address the gap in LLMs' ability to apply technical knowledge effectively in CTF challenges, despite their substantial knowledge base.

Method: Constructed CTFKnow benchmark (3,992 questions) and developed CTFAgent with two-stage RAG and interactive Environmental Augmentation modules.

Result: CTFAgent improved performance by over 80% on CTF datasets and ranked top 23.6% in picoCTF2024.

Conclusion: CTFAgent demonstrates the potential to enhance LLMs' CTF problem-solving by integrating focused knowledge and adaptive strategies.

Abstract: Capture-the-Flag (CTF) competitions are crucial for cybersecurity education
and training. As large language models (LLMs) evolve, there is increasing
interest in their ability to automate CTF challenge solving. For example, DARPA
has organized the AIxCC competition since 2023 to advance AI-powered automated
offense and defense. However, this demands a combination of multiple abilities,
from knowledge to reasoning and further to actions. In this paper, we highlight
the importance of technical knowledge in solving CTF problems and deliberately
construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'
performance in this core aspect. Our study offers a focused and innovative
measurement of LLMs' capability in understanding CTF knowledge and applying it
to solve CTF challenges. Our key findings reveal that while LLMs possess
substantial technical knowledge, they falter in accurately applying this
knowledge to specific scenarios and adapting their strategies based on feedback
from the CTF environment.
  Based on insights derived from this measurement study, we propose CTFAgent, a
novel LLM-driven framework for advancing CTF problem-solving. CTFAgent
introduces two new modules: two-stage Retrieval Augmented Generation (RAG) and
interactive Environmental Augmentation, which enhance LLMs' technical knowledge
and vulnerability exploitation on CTF, respectively. Our experimental results
show that, on two popular CTF datasets, CTFAgent both achieves over 80%
performance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,
CTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This
reflects the benefit of our measurement study and the potential of our
framework in advancing LLMs' capabilities in CTF problem-solving.

</details>


### [412] [TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation](https://arxiv.org/pdf/2506.18783)
*Kamil Szczepanik, Jarosaw A. Chudziak*

Main category: cs.AI

TL;DR: A multi-agent LLM system (TRIZ agents) is introduced to automate and enhance TRIZ problem-solving by leveraging specialized agents for collaborative innovation.


<details>
  <summary>Details</summary>
Motivation: TRIZ's complexity and interdisciplinary demands limit its application; LLMs offer automation potential, but prior work focused on single models.

Method: Proposes a multi-agent LLM system with specialized capabilities to collaboratively solve TRIZ-based inventive problems.

Result: Demonstrates effective agent collaboration in a case study, yielding diverse inventive solutions.

Conclusion: Highlights decentralized AI-driven innovation's potential for complex ideation, advancing TRIZ applications.

Abstract: TRIZ, the Theory of Inventive Problem Solving, is a structured,
knowledge-based framework for innovation and abstracting problems to find
inventive solutions. However, its application is often limited by the
complexity and deep interdisciplinary knowledge required. Advancements in Large
Language Models (LLMs) have revealed new possibilities for automating parts of
this process. While previous studies have explored single LLMs in TRIZ
applications, this paper introduces a multi-agent approach. We propose an
LLM-based multi-agent system, called TRIZ agents, each with specialized
capabilities and tool access, collaboratively solving inventive problems based
on the TRIZ methodology. This multi-agent system leverages agents with various
domain expertise to efficiently navigate TRIZ steps. The aim is to model and
simulate an inventive process with language agents. We assess the effectiveness
of this team of agents in addressing complex innovation challenges based on a
selected case study in engineering. We demonstrate the potential of agent
collaboration to produce diverse, inventive solutions. This research
contributes to the future of AI-driven innovation, showcasing the advantages of
decentralized problem-solving in complex ideation tasks.

</details>


### [413] [PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models](https://arxiv.org/pdf/2506.17667)
*Lintao Wang, Encheng Su, Jiaqi Liu, Pengze Li, Peng Xia, Jiabei Xiao, Wenlong Zhang, Xinnan Dai, Xi Chen, Yuan Meng, Mingyu Ding, Lei Bai, Wanli Ouyang, Shixiang Tang, Aoran Wang, Xinzhu Ma*

Main category: cs.AI

TL;DR: PhysUniBench is a multimodal benchmark for evaluating MLLMs on undergraduate-level physics problems, revealing significant gaps in current models' reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: Current AI models struggle with physics problem-solving due to limitations in existing evaluation methods, necessitating a more rigorous benchmark.

Method: PhysUniBench includes 3,304 questions across 8 physics sub-disciplines, with visual diagrams, open-ended and multiple-choice formats, and a multi-stage construction process.

Result: State-of-the-art models like GPT-4o mini achieve only 34.2% accuracy, struggling with multi-step problems and diagram interpretation.

Conclusion: PhysUniBench aims to advance AI for Science by improving models' physical reasoning, problem-solving, and multimodal understanding.

Abstract: Physics problem-solving is a challenging domain for large AI models,
requiring integration of conceptual understanding, mathematical reasoning, and
interpretation of physical diagrams. Current evaluation methodologies show
notable limitations in capturing the breadth and complexity of
undergraduate-level physics, underscoring the need for more rigorous
assessments. To this end, we present PhysUniBench, a large-scale multimodal
benchmark designed to evaluate and improve the reasoning capabilities of
multimodal large language models (MLLMs) specifically on undergraduate-level
physics problems. PhysUniBench consists of 3,304 physics questions spanning 8
major sub-disciplines of physics, each accompanied by one visual diagrams. The
benchmark includes both open-ended and multiple-choice questions,
systematically curated and difficulty-rated through an iterative
model-in-the-loop process. The benchmark's construction involved a rigorous
multi-stage process, including multiple roll-outs, expert-level evaluation,
automated filtering of easily solved problems, and a nuanced difficulty grading
system with five levels. Through extensive experiments, we observe that current
state-of-the-art models encounter substantial challenges in physics reasoning.
For example, GPT-4o mini achieves only about 34.2\% accuracy in the proposed
PhysUniBench. These results highlight that current MLLMs struggle with advanced
physics reasoning, especially on multi-step problems and those requiring
precise diagram interpretation. By providing a broad and rigorous assessment
tool, PhysUniBench aims to drive progress in AI for Science, encouraging the
development of models with stronger physical reasoning, problem-solving skills,
and multimodal understanding. The benchmark and evaluation scripts are
available at https://prismax-team.github.io/PhysUniBenchmark/.

</details>


### [414] [Beyond Syntax: Action Semantics Learning for App Agents](https://arxiv.org/pdf/2506.17697)
*Bohan Tang, Dezhao Luo, Jingxuan Chen, Shaogang Gong, Jianye Hao, Jun Wang, Kun Shao*

Main category: cs.AI

TL;DR: ASL (Action Semantics Learning) improves App agent performance by focusing on action semantics rather than syntax, enhancing robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Current fine-tuning methods for App agents rely on syntax learning, leading to OOD vulnerability. ASL addresses this by capturing action semantics.

Method: ASL uses a SEmantic Estimator (SEE) to compute semantic rewards, training agents to align actions with ground truth semantics, even if syntax differs.

Result: ASL outperforms existing methods in accuracy and generalization on smartphone App operation benchmarks.

Conclusion: ASL provides a robust framework for App agents by prioritizing semantics over syntax, improving performance in real-world scenarios.

Abstract: The advent of Large Language Models (LLMs) enables the rise of App agents
that interpret user intent and operate smartphone Apps through actions such as
clicking and scrolling. While prompt-based solutions with closed LLM APIs show
promising ability, they incur heavy compute costs and external API dependency.
Fine-tuning smaller open-source LLMs solves these limitations. However, current
fine-tuning methods use a syntax learning paradigm that forces agents to
reproduce exactly the ground truth action strings, leading to
out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action
Semantics Learning (ASL), a novel learning framework, where the learning
objective is capturing the semantics of the ground truth actions. Specifically,
inspired by the programming language theory, we define the action semantics for
App agents as the state transition induced by the action in the user interface.
With this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a
semantic reward to train the App agents in generating actions aligned with the
semantics of ground truth actions, even when the syntactic forms differ. To
support the effectiveness of ASL, we theoretically demonstrate the superior
robustness of ASL for the OOD problem compared with the existing syntax
learning paradigm. Extensive experiments on offline and online smartphone App
operation benchmarks show that ASL significantly improves the accuracy and
generalisation of App agents over existing methods.

</details>


### [415] [AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction](https://arxiv.org/pdf/2506.17784)
*Song Wang, Zhen Tan, Zihan Chen, Shuang Zhou, Tianlong Chen, Jundong Li*

Main category: cs.AI

TL;DR: A new framework for multi-agent collaboration replaces static/graph-based topologies with a sequential structure, enhancing adaptability and reducing communication overhead.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent methods rely on rigid communication topologies, limiting flexibility and adaptability.

Method: Proposes a sequential structure with Next-Agent Prediction and Next-Context Selection for dynamic, task-adaptive communication.

Result: Outperforms benchmarks with superior performance and reduced communication overhead.

Conclusion: The sequential approach offers a more flexible and efficient solution for multi-agent collaboration.

Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration
highlights the power of structured communication in enabling collective
intelligence. However, existing methods largely rely on static or graph-based
inter-agent topologies, lacking the potential adaptability and flexibility in
communication. In this work, we propose a new framework that rethinks
multi-agent coordination through a sequential structure rather than a graph
structure, offering a significantly larger topology space for multi-agent
communication. Our method focuses on two key directions: (1) Next-Agent
Prediction, which selects the most suitable agent role at each step, and (2)
Next-Context Selection (NCS), which enables each agent to selectively access
relevant information from any previous step. Together, these components
construct task-adaptive communication pipelines that support both role
flexibility and global information flow. Extensive evaluations across multiple
benchmarks demonstrate that our approach achieves superior performance while
substantially reducing communication overhead.

</details>


### [416] [Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition](https://arxiv.org/pdf/2506.17792)
*Alexandros Evangelidis, Gricel Vzquez, Simos Gerasimou*

Main category: cs.AI

TL;DR: The paper introduces an iterative refinement approach to accelerate policy synthesis in large MDPs, outperforming PRISM by up to 2x in performance.


<details>
  <summary>Details</summary>
Motivation: Conventional policy synthesis methods struggle with scalability in large state spaces of MDPs, limiting their practical use in software-intensive systems.

Method: The approach dynamically refines the MDP and iteratively selects fragile regions for refinement, balancing accuracy and efficiency.

Result: Empirical evaluation shows significant performance improvements (up to 2x faster than PRISM) in large MDPs with up to 1M states.

Conclusion: The proposed method offers a competitive solution for real-world policy synthesis in large MDPs.

Abstract: Software-intensive systems, such as software product lines and robotics,
utilise Markov decision processes (MDPs) to capture uncertainty and analyse
sequential decision-making problems. Despite the usefulness of conventional
policy synthesis methods, they fail to scale to large state spaces. Our
approach addresses this issue and accelerates policy synthesis in large MDPs by
dynamically refining the MDP and iteratively selecting the most fragile MDP
regions for refinement. This iterative procedure offers a balance between
accuracy and efficiency, as refinement occurs only when necessary. Through a
comprehensive empirical evaluation comprising diverse case studies and MDPs up
to 1M states, we demonstrate significant performance improvements yielded by
our approach compared to the leading probabilistic model checker PRISM (up to
2x), thus offering a very competitive solution for real-world policy synthesis
tasks in larger MDPs.

</details>


### [417] [Reflective Verbal Reward Design for Pluralistic Alignment](https://arxiv.org/pdf/2506.17834)
*Carter Blair, Kate Larson, Edith Law*

Main category: cs.AI

TL;DR: The paper introduces a novel reward modeling approach for AI agents, using reflective dialogues to create individualized reward models, improving accuracy and sample efficiency over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Human values are diverse and conflicting; aggregating feedback into a single reward model risks suppressing minority preferences.

Method: A language model guides users through reflective dialogues to construct preferences, which are then used as context for an individualized reward model (verbal reward model).

Result: The method improved accuracy by 9-12% over non-reflective models and was more sample-efficient than supervised learning.

Conclusion: Personalized reward modeling via reflective dialogues effectively captures diverse human values and outperforms traditional approaches.

Abstract: AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.

</details>


### [418] [Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)](https://arxiv.org/pdf/2506.17846)
*Elija Perrier*

Main category: cs.AI

TL;DR: The paper advocates for using formal optimal control theory in AI alignment research, proposing a hierarchical 'Alignment Control Stack' to improve interoperability and understanding of AI control frameworks.


<details>
  <summary>Details</summary>
Motivation: Current AI safety and mechanistic interpretability methods lack generalization and interoperability, limiting their effectiveness in controlling advanced AI systems.

Method: Introduces the Alignment Control Stack, a hierarchical framework for alignment, detailing measurement and control characteristics at each layer and their formal interoperability.

Result: The framework aims to bridge optimal control theory with practical AI deployment, enhancing safety and reliability for advanced AI systems.

Conclusion: Adopting formal optimal control principles in AI alignment can provide better assurances for governments and regulators, ensuring sustainable benefits from AI technologies.

Abstract: This position paper argues that formal optimal control theory should be
central to AI alignment research, offering a distinct perspective from
prevailing AI safety and security approaches. While recent work in AI safety
and mechanistic interpretability has advanced formal methods for alignment,
they often fall short of the generalisation required of control frameworks for
other technologies. There is also a lack of research into how to render
different alignment/control protocols interoperable. We argue that by recasting
alignment through principles of formal optimal control and framing alignment in
terms of hierarchical stack from physical to socio-technical layers according
to which controls may be applied we can develop a better understanding of the
potential and limitations for controlling frontier models and agentic AI
systems. To this end, we introduce an Alignment Control Stack which sets out a
hierarchical layered alignment stack, identifying measurement and control
characteristics at each layer and how different layers are formally
interoperable. We argue that such analysis is also key to the assurances that
will be needed by governments and regulators in order to see AI technologies
sustainably benefit the community. Our position is that doing so will bridge
the well-established and empirically validated methods of optimal control with
practical deployment considerations to create a more comprehensive alignment
framework, enhancing how we approach safety and reliability for advanced AI
systems.

</details>


### [419] [Multi-agent Embodied AI: Advances and Future Directions](https://arxiv.org/pdf/2505.05108)
*Zhaohan Feng, Ruiqi Xue, Lei Yuan, Yang Yu, Ning Ding, Meiqin Liu, Bingzhao Gao, Jian Sun, Xinhu Zheng, Gang Wang*

Main category: cs.AI

TL;DR: The paper reviews multi-agent embodied AI, highlighting its importance in dynamic, open environments and identifying gaps in current research.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require multi-agent embodied AI to handle complex, collaborative scenarios, but existing research is limited and lacks comprehensive surveys.

Method: The paper reviews current research, analyzes key contributions, and identifies challenges and future directions in multi-agent embodied AI.

Result: The review underscores the need for more sophisticated mechanisms for adaptation, real-time learning, and collaboration in multi-agent systems.

Conclusion: The paper aims to guide innovation by addressing gaps and fostering progress in multi-agent embodied AI.

Abstract: Embodied artificial intelligence (Embodied AI) plays a pivotal role in the
application of advanced technologies in the intelligent era, where AI systems
are integrated with physical bodies that enable them to perceive, reason, and
interact with their environments. Through the use of sensors for input and
actuators for action, these systems can learn and adapt based on real-world
feedback, allowing them to perform tasks effectively in dynamic and
unpredictable environments. As techniques such as deep learning (DL),
reinforcement learning (RL), and large language models (LLMs) mature, embodied
AI has become a leading field in both academia and industry, with applications
spanning robotics, healthcare, transportation, and manufacturing. However, most
research has focused on single-agent systems that often assume static, closed
environments, whereas real-world embodied AI must navigate far more complex
scenarios. In such settings, agents must not only interact with their
surroundings but also collaborate with other agents, necessitating
sophisticated mechanisms for adaptation, real-time learning, and collaborative
problem-solving. Despite increasing interest in multi-agent systems, existing
research remains narrow in scope, often relying on simplified models that fail
to capture the full complexity of dynamic, open environments for multi-agent
embodied AI. Moreover, no comprehensive survey has systematically reviewed the
advancements in this area. As embodied AI rapidly evolves, it is crucial to
deepen our understanding of multi-agent embodied AI to address the challenges
presented by real-world applications. To fill this gap and foster further
development in the field, this paper reviews the current state of research,
analyzes key contributions, and identifies challenges and future directions,
providing insights to guide innovation and progress in this field.

</details>


### [420] [Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval](https://arxiv.org/pdf/2506.17878)
*Tam Trinh, Manh Nguyen, Truong-Son Hy*

Main category: cs.AI

TL;DR: A multi-agent system for automated fact-checking improves accuracy and transparency, outperforming baselines by 12.3% in Macro F1-score.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional and existing automated fact-checking methods in handling complex claims, source credibility, and transparency.

Method: A novel multi-agent system with four specialized agents for claim decomposition, query generation, evidence retrieval, and verdict prediction with explanations.

Result: Achieves a 12.3% improvement in Macro F1-score on benchmark datasets (FEVEROUS, HOVER, SciFact).

Conclusion: The system enhances automated fact-checking by combining accuracy, efficiency, and transparency, aligning with human practices while scaling for real-world use.

Abstract: The rapid spread of misinformation in the digital era poses significant
challenges to public discourse, necessitating robust and scalable fact-checking
solutions. Traditional human-led fact-checking methods, while credible,
struggle with the volume and velocity of online content, prompting the
integration of automated systems powered by Large Language Models (LLMs).
However, existing automated approaches often face limitations, such as handling
complex claims, ensuring source credibility, and maintaining transparency. This
paper proposes a novel multi-agent system for automated fact-checking that
enhances accuracy, efficiency, and explainability. The system comprises four
specialized agents: an Input Ingestion Agent for claim decomposition, a Query
Generation Agent for formulating targeted subqueries, an Evidence Retrieval
Agent for sourcing credible evidence, and a Verdict Prediction Agent for
synthesizing veracity judgments with human-interpretable explanations.
Evaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system
achieves a 12.3% improvement in Macro F1-score over baseline methods. The
system effectively decomposes complex claims, retrieves reliable evidence from
trusted sources, and generates transparent explanations for verification
decisions. Our approach contributes to the growing field of automated
fact-checking by providing a more accurate, efficient, and transparent
verification methodology that aligns with human fact-checking practices while
maintaining scalability for real-world applications. Our source code is
available at https://github.com/HySonLab/FactAgent

</details>


### [421] [Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms](https://arxiv.org/pdf/2506.17900)
*Cheng Ji, Huaiying Luo*

Main category: cs.AI

TL;DR: The paper proposes LLM-ID, an intelligent log processing and debugging framework using Large Language Models (LLMs) to improve fault location and self-repair in cloud AI systems.


<details>
  <summary>Details</summary>
Motivation: The massive, unstructured, and ambiguous log data in cloud AI systems pose challenges for fault diagnosis and repair.

Method: LLM-ID extends a pre-trained Transformer model with multi-stage semantic inference, unsupervised clustering, and reinforcement learning for dynamic decision-making.

Result: Experiments show LLM-ID improves fault location accuracy by 16.2% over existing methods.

Conclusion: LLM-ID outperforms traditional systems with better semantic understanding, learning, and adaptability.

Abstract: With the increasing complexity and rapid expansion of the scale of AI systems
in cloud platforms, the log data generated during system operation is massive,
unstructured, and semantically ambiguous, which brings great challenges to
fault location and system self-repair. In order to solve this problem, this
paper proposes an intelligent log processing and automatic debugging framework
based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This
method is extended on the basis of the existing pre-trained Transformer model,
and integrates a multi-stage semantic inference mechanism to realize the
context understanding of system logs and the automatic reconstruction of fault
chains. Firstly, the system log is dynamically structured, and the unsupervised
clustering and embedding mechanism is used to extract the event template and
semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round
attention mechanism to perform contextual reasoning on the log sequence to
generate potential fault assumptions and root cause paths. Furthermore, this
paper introduces a reinforcement learning-based policy-guided recovery planner,
which is driven by the remediation strategy generated by LLM to support dynamic
decision-making and adaptive debugging in the cloud environment. Compared with
the existing rule engine or traditional log analysis system, the proposed model
has stronger semantic understanding ability, continuous learning ability and
heterogeneous environment adaptability. Experiments on the cloud platform log
dataset show that LLM-ID improves the fault location accuracy by 16.2%, which
is significantly better than the current mainstream methods

</details>


### [422] [Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents](https://arxiv.org/pdf/2506.17913)
*Jinjie Wei, Jiyao Liu, Lihao Liu, Ming Hu, Junzhi Ning, Mingcheng Li, Weijie Yin, Junjun He, Xiao Liang, Chao Feng, Dingkang Yang*

Main category: cs.AI

TL;DR: CogniGUI is a cognitive framework for GUI automation that combines visual parsing and policy optimization to enable adaptive learning, outperforming existing methods on new and existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents rely on trial-and-error and simplistic metrics, lacking adaptive learning and realistic evaluation.

Method: CogniGUI uses an omni parser for visual semantic analysis and GRPO for policy optimization, inspired by Dual Process Theory.

Result: CogniGUI outperforms state-of-the-art methods in both existing and new benchmarks (ScreenSeek).

Conclusion: The framework enables human-like adaptive learning and addresses overlooked challenges in GUI automation.

Abstract: Graphical User Interface (GUI) agents have made significant progress in
automating digital tasks through the utilization of computer vision and
language models. Nevertheless, existing agent systems encounter notable
limitations. Firstly, they predominantly depend on trial and error decision
making rather than progressive reasoning, thereby lacking the capability to
learn and adapt from interactive encounters. Secondly, these systems are
assessed using overly simplistic single step accuracy metrics, which do not
adequately reflect the intricate nature of real world GUI interactions. In this
paper, we present CogniGUI, a cognitive framework developed to overcome these
limitations by enabling adaptive learning for GUI automation resembling
human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach
combines two main components: (1) an omni parser engine that conducts immediate
hierarchical parsing of GUI elements through quick visual semantic analysis to
identify actionable components, and (2) a Group based Relative Policy
Optimization (GRPO) grounding agent that assesses multiple interaction paths
using a unique relative reward system, promoting minimal and efficient
operational routes. This dual-system design facilitates iterative ''exploration
learning mastery'' cycles, enabling the agent to enhance its strategies over
time based on accumulated experience. Moreover, to assess the generalization
and adaptability of agent systems, we introduce ScreenSeek, a comprehensive
benchmark that includes multi application navigation, dynamic state
transitions, and cross interface coherence, which are often overlooked
challenges in current benchmarks. Experimental results demonstrate that
CogniGUI surpasses state-of-the-art methods in both the current GUI grounding
benchmarks and our newly proposed benchmark.

</details>


### [423] [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/pdf/2506.17930)
*Jianyu Wang, Zhiqiang Hu, Lidong Bing*

Main category: cs.AI

TL;DR: The paper introduces a novel prompt design paradigm where pruning random demonstrations into 'gibberish' outperforms conventional methods, and proposes PromptQuine, a self-discovering optimization framework for effective pruning strategies.


<details>
  <summary>Details</summary>
Motivation: Challenges conventional wisdom in LLM prompting by showing that seemingly incoherent 'gibberish' can improve performance, highlighting the need for better optimization techniques.

Method: Proposes PromptQuine, an evolutionary search framework that autonomously discovers pruning strategies using low-data regimes, inspired by natural emergent complexity.

Result: Demonstrates superior performance across diverse tasks (classification, QA, generation, math reasoning) compared to state-of-the-art methods, with decent runtime efficiency.

Conclusion: Encourages mechanistic studies on in-context learning and advocates for open-ended search algorithms to enhance LLM prompting.

Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

</details>


### [424] [medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs](https://arxiv.org/pdf/2506.17959)
*Lizzy Farrugia, Lilian M. Azzopardi, Jeremy Debattista, Charlie Abela*

Main category: cs.AI

TL;DR: The paper introduces medicX-KG, a pharmacist-oriented knowledge graph integrating data from BNF, DrugBank, and MMA to support clinical and regulatory decisions, addressing fragmented drug information sources.


<details>
  <summary>Details</summary>
Motivation: The evolving role of pharmacists requires accurate, integrated medicinal product information, which is lacking in unified national repositories.

Method: medicX-KG integrates data from BNF, DrugBank, and MMA, using AI and semantic technologies for data extraction, ontology design, and semantic mapping.

Result: The KG effectively supports queries on drug availability, interactions, adverse reactions, and therapeutic classes.

Conclusion: medicX-KG addresses fragmented drug information but has limitations like missing dosage details and real-time updates, with future enhancements planned.

Abstract: The role of pharmacists is evolving from medicine dispensing to delivering
comprehensive pharmaceutical services within multidisciplinary healthcare
teams. Central to this shift is access to accurate, up-to-date medicinal
product information supported by robust data integration. Leveraging artificial
intelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden
relationships and enable data-driven decision-making. This paper presents
medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and
regulatory decisions. It forms the semantic layer of the broader medicX
platform, powering predictive and explainable pharmacy services. medicX-KG
integrates data from three sources, including, the British National Formulary
(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's
regulatory landscape and combines European Medicines Agency alignment with
partial UK supply dependence. The KG tackles the absence of a unified national
drug repository, reducing pharmacists' reliance on fragmented sources. Its
design was informed by interviews with practicing pharmacists to ensure
real-world applicability. We detail the KG's construction, including data
extraction, ontology design, and semantic mapping. Evaluation demonstrates that
medicX-KG effectively supports queries about drug availability, interactions,
adverse reactions, and therapeutic classes. Limitations, including missing
detailed dosage encoding and real-time updates, are discussed alongside
directions for future enhancements.

</details>


### [425] [Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities](https://arxiv.org/pdf/2506.18019)
*Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, Irwin King, Fakhri Karray, Philip S. Yu*

Main category: cs.AI

TL;DR: The paper reviews how graphs can enhance AI agents by structuring complex data, integrating graph techniques with agent functionalities, and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: AI agents need to handle complex tasks requiring planning, memory, and coordination, which can be supported by structuring data using graphs.

Method: The survey systematically reviews graph techniques for AI agents, exploring integration with core functionalities and applications.

Result: Graphs offer a powerful paradigm for structuring data to improve AI agent capabilities, with notable applications and future research opportunities identified.

Conclusion: The survey aims to inspire next-generation AI agents leveraging graphs to tackle sophisticated challenges, with resources updated on GitHub.

Abstract: AI agents have experienced a paradigm shift, from early dominance by
reinforcement learning (RL) to the rise of agents powered by large language
models (LLMs), and now further advancing towards a synergistic fusion of RL and
LLM capabilities. This progression has endowed AI agents with increasingly
strong abilities. Despite these advances, to accomplish complex real-world
tasks, agents are required to plan and execute effectively, maintain reliable
memory, and coordinate smoothly with other agents. Achieving these capabilities
involves contending with ever-present intricate information, operations, and
interactions. In light of this challenge, data structurization can play a
promising role by transforming intricate and disorganized data into
well-structured forms that agents can more effectively understand and process.
In this context, graphs, with their natural advantage in organizing, managing,
and harnessing intricate data relationships, present a powerful data paradigm
for structurization to support the capabilities demanded by advanced AI agents.
To this end, this survey presents a first systematic review of how graphs can
empower AI agents. Specifically, we explore the integration of graph techniques
with core agent functionalities, highlight notable applications, and identify
prospective avenues for future research. By comprehensively surveying this
burgeoning intersection, we hope to inspire the development of next-generation
AI agents equipped to tackle increasingly sophisticated challenges with graphs.
Related resources are collected and continuously updated for the community in
the Github link.

</details>


### [426] [Action Language BC+](https://arxiv.org/pdf/2506.18044)
*Joseph Babb, Joohyung Lee*

Main category: cs.AI

TL;DR: The paper introduces BC+, a new action language bridging the gap between traditional action languages and modern Answer Set Programming (ASP) by leveraging stable model semantics.


<details>
  <summary>Details</summary>
Motivation: Existing action languages are limited compared to modern ASP, which includes advanced constructs like choice rules and aggregates. BC+ aims to unify these features.

Method: BC+ is defined using general stable model semantics for propositional formulas, allowing modern ASP constructs to be represented as shorthands.

Result: BC+ successfully encompasses features of other action languages (B, C, C+, BC) and is implemented using ASP solvers via cplus2asp.

Conclusion: BC+ effectively integrates modern ASP capabilities into action languages, enhancing expressiveness and computational feasibility.

Abstract: Action languages are formal models of parts of natural language that are
designed to describe effects of actions. Many of these languages can be viewed
as high level notations of answer set programs structured to represent
transition systems. However, the form of answer set programs considered in the
earlier work is quite limited in comparison with the modern Answer Set
Programming (ASP) language, which allows several useful constructs for
knowledge representation, such as choice rules, aggregates, and abstract
constraint atoms. We propose a new action language called BC+, which closes the
gap between action languages and the modern ASP language. The main idea is to
define the semantics of BC+ in terms of general stable model semantics for
propositional formulas, under which many modern ASP language constructs can be
identified with shorthands for propositional formulas. Language BC+ turns out
to be sufficiently expressive to encompass the best features of other action
languages, such as languages B, C, C+, and BC. Computational methods available
in ASP solvers are readily applicable to compute BC+, which led to an
implementation of the language by extending system cplus2asp.

</details>


### [427] [Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model](https://arxiv.org/pdf/2506.13642)
*Shaolei Zhang, Shoutao Guo, Qingkai Fang, Yan Zhou, Yang Feng*

Main category: cs.AI

TL;DR: Stream-Omni is a multimodal model integrating text, vision, and speech with efficient alignment methods, reducing reliance on large-scale data while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: Existing large multimodal models (LMMs) rely heavily on sequence-dimension concatenation and large-scale data for modality alignment, which is inefficient. Stream-Omni aims to model modality relationships more purposefully for better efficiency and flexibility.

Method: Stream-Omni uses an LLM backbone and aligns vision and speech to text differently: sequence-dimension concatenation for vision-text (complementary) and CTC-based layer-dimension mapping for speech-text (consistent).

Result: Experiments show strong performance in visual understanding, speech interaction, and vision-grounded speech tasks, with intermediate text outputs during speech interaction.

Conclusion: Stream-Omni achieves efficient and flexible modality alignments with less data, transferring text capabilities to other modalities and enhancing multimodal interaction.

Abstract: The emergence of GPT-4o-like large multimodal models (LMMs) has raised the
exploration of integrating text, vision, and speech modalities to support more
flexible multimodal interaction. Existing LMMs typically concatenate
representation of modalities along the sequence dimension and feed them into a
large language model (LLM) backbone. While sequence-dimension concatenation is
straightforward for modality integration, it often relies heavily on
large-scale data to learn modality alignments. In this paper, we aim to model
the relationships between modalities more purposefully, thereby achieving more
efficient and flexible modality alignments. To this end, we propose
Stream-Omni, a large language-vision-speech model with efficient modality
alignments, which can simultaneously support interactions under various
modality combinations. Stream-Omni employs LLM as the backbone and aligns the
vision and speech to the text based on their relationships. For vision that is
semantically complementary to text, Stream-Omni uses sequence-dimension
concatenation to achieve vision-text alignment. For speech that is semantically
consistent with text, Stream-Omni introduces a CTC-based layer-dimension
mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve
modality alignments with less data (especially speech), enabling the transfer
of text capabilities to other modalities. Experiments on various benchmarks
demonstrate that Stream-Omni achieves strong performance on visual
understanding, speech interaction, and vision-grounded speech interaction
tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously
provide intermediate text outputs (such as ASR transcriptions and model
responses) during speech interaction, offering users a comprehensive multimodal
experience.

</details>


### [428] [Weighted Assumption Based Argumentation to reason about ethical principles and actions](https://arxiv.org/pdf/2506.18056)
*Paolo Baldi, Fabio Aurelio D'Asaro, Abeer Dyoub, Francesca Alessandra Lisi*

Main category: cs.AI

TL;DR: The paper introduces weighted argumentation in Assumption Based Argumentation (ABA), assigning weights to arguments and deriving attack weights, with examples in ethical reasoning and an Answer Set Programming implementation.


<details>
  <summary>Details</summary>
Motivation: To enhance ABA by incorporating weighted argumentation, allowing for more nuanced reasoning, especially in fields like ethical decision-making.

Method: Assign weights to arguments, derive attack weights between them, and demonstrate the approach through ethical reasoning examples. Implement the method using Answer Set Programming.

Result: A framework for weighted ABA is proposed, illustrated with ethical reasoning examples, and an implementation is provided.

Conclusion: Weighted ABA offers a refined approach to argumentation, particularly useful in ethical reasoning, with practical implementation feasibility.

Abstract: We augment Assumption Based Argumentation (ABA for short) with weighted
argumentation. In a nutshell, we assign weights to arguments and then derive
the weight of attacks between ABA arguments. We illustrate our proposal through
running examples in the field of ethical reasoning, and present an
implementation based on Answer Set Programming.

</details>


### [429] [Deep Research Agents: A Systematic Examination And Roadmap](https://arxiv.org/pdf/2506.18096)
*Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, Kun Shao, Jun Wang*

Main category: cs.AI

TL;DR: The paper analyzes Deep Research (DR) agents, autonomous AI systems for complex research tasks, covering technologies, architectures, benchmarks, and future directions.


<details>
  <summary>Details</summary>
Motivation: To understand and systematize the foundational technologies and components of DR agents, addressing gaps in current research and benchmarks.

Method: Review of information acquisition strategies, modular tool-use frameworks, and proposal of a taxonomy for workflows and architectures. Critical evaluation of benchmarks.

Result: A taxonomy differentiating static/dynamic workflows and agent architectures, plus identified limitations in benchmarks (e.g., restricted external knowledge access).

Conclusion: Open challenges remain; future research should address benchmark limitations and improve DR agent capabilities. A repository is provided for ongoing updates.

Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new
category of autonomous AI systems, referred to as Deep Research (DR) agents.
These agents are designed to tackle complex, multi-turn informational research
tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon
planning, multi-hop information retrieval, iterative tool use, and the
generation of structured analytical reports. In this paper, we conduct a
detailed analysis of the foundational technologies and architectural components
that constitute Deep Research agents. We begin by reviewing information
acquisition strategies, contrasting API-based retrieval methods with
browser-based exploration. We then examine modular tool-use frameworks,
including code execution, multimodal input processing, and the integration of
Model Context Protocols (MCPs) to support extensibility and ecosystem
development. To systematize existing approaches, we propose a taxonomy that
differentiates between static and dynamic workflows, and we classify agent
architectures based on planning strategies and agent composition, including
single-agent and multi-agent configurations. We also provide a critical
evaluation of current benchmarks, highlighting key limitations such as
restricted access to external knowledge, sequential execution inefficiencies,
and misalignment between evaluation metrics and the practical objectives of DR
agents. Finally, we outline open challenges and promising directions for future
research. A curated and continuously updated repository of DR agent research is
available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.

</details>


### [430] [Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game](https://arxiv.org/pdf/2506.18126)
*Xiang Yuming, Li Sizhao, Li Rongpeng, Zhao Zhifeng, Zhang Honggang*

Main category: cs.AI

TL;DR: A novel two-level framework (CI-HRL) is proposed for cooperative evasion and formation coverage in UAV swarms, combining high-level consensus inference and low-level control for improved performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the high-dimensional challenges of cooperative evasion and formation coverage in UAV swarms under communication-limited constraints.

Method: Uses a two-level framework: high-level policy (ConsMAC for consensus inference) and low-level policy (AT-M for control).

Result: Validated through simulations, showing enhanced collaborative evasion and task completion.

Conclusion: CI-HRL effectively tackles the complexities of MC-PEG in UAV swarms.

Abstract: Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered
widespread research interest and fostered tremendous interesting applications,
especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative
Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to
maximize formation coverage across multiple target zones while collaboratively
evading predators, belongs to one of the most challenging issues in MC-PEG,
especially under communication-limited constraints. This multifaceted problem,
which intertwines responses to obstacles, adversaries, target zones, and
formation dynamics, brings up significant high-dimensional complications in
locating a solution. In this paper, we propose a novel two-level framework
(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),
which delegates target localization to a high-level policy, while adopting a
low-level policy to manage obstacle avoidance, navigation, and formation.
Specifically, in the high-level policy, we develop a novel multi-agent
reinforcement learning module, Consensus-oriented Multi-Agent Communication
(ConsMAC), to enable agents to perceive global information and establish
consensus from local states by effectively aggregating neighbor messages.
Meanwhile, we leverage an Alternative Training-based Multi-agent proximal
policy optimization (AT-M) and policy distillation to accomplish the low-level
control. The experimental results, including the high-fidelity
software-in-the-loop (SITL) simulations, validate that CI-HRL provides a
superior solution with enhanced swarm's collaborative evasion and task
completion capabilities.

</details>


### [431] [SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging](https://arxiv.org/pdf/2506.18135)
*Zijun Chen, Zhanpeng Zhou, Bo Zhang, Weinan Zhang, Xi Sun, Junchi Yan*

Main category: cs.AI

TL;DR: The paper explores model merging's mechanisms, identifying two key capabilities for multi-task abilities, and introduces SE-Merging, a dynamic framework that enhances performance without extra training.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying mechanisms of model merging, which empirically succeeds but lacks theoretical clarity.

Method: Analyzes model merging from a representation perspective, identifying two key capabilities, and proposes SE-Merging, a dynamic framework leveraging these insights.

Result: SE-Merging significantly improves performance while remaining compatible with existing techniques, achieving dynamic merging without additional training.

Conclusion: Model merging's success hinges on task distinction and expert adaptation; SE-Merging effectively leverages these for enhanced multi-task performance.

Abstract: Model merging has gained increasing attention due to its intriguing property:
interpolating the parameters of different task-specific fine-tuned models leads
to multi-task abilities. However, despite its empirical success, the underlying
mechanisms of model merging remain poorly understood. In this work, we delve
into the mechanism behind model merging from a representation perspective. Our
analysis reveals that model merging achieves multi-task abilities through two
key capabilities: i) distinguishing samples from different tasks, and ii)
adapting to the corresponding expert model for each sample. These two
capabilities allow the merged model to retain task-specific expertise, enabling
efficient multi-task adaptation. Building on these insights, we propose
\texttt{SE-Merging}, a self-enhanced model merging framework that leverages
these two characteristics to dynamically identify the corresponding task for
each sample and then adaptively rescales the merging coefficients to further
enhance task-specific expertise in the merged model. Notably,
\texttt{SE-Merging} achieves dynamic model merging without additional training.
Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant
performance improvements while remaining compatible with existing model merging
techniques.

</details>


### [432] [CoachGPT: A Scaffolding-based Academic Writing Assistant](https://arxiv.org/pdf/2506.18149)
*Fumian Chen, Sotheara Veng, Joshua Wilson, Xiaoming Li, Hui Fang*

Main category: cs.AI

TL;DR: CoachGPT is an AI-based writing assistant leveraging LLMs to provide personalized, real-time feedback for academic writing, addressing limitations of traditional and ML-based tools.


<details>
  <summary>Details</summary>
Motivation: Traditional writing assistants lack contextual understanding, while LLMs generate essays without teaching. CoachGPT aims to bridge this gap by offering guided learning.

Method: CoachGPT uses LLMs to convert educator instructions into sub-tasks, providing real-time feedback via a web application.

Result: User studies confirm CoachGPT's effectiveness and the potential of LLMs in academic writing.

Conclusion: CoachGPT offers a unique scaffolding approach, enhancing learning with personalized feedback, proving LLMs' value in education.

Abstract: Academic writing skills are crucial for students' success, but can feel
overwhelming without proper guidance and practice, particularly when writing in
a second language. Traditionally, students ask instructors or search
dictionaries, which are not universally accessible. Early writing assistants
emerged as rule-based systems that focused on detecting misspellings,
subject-verb disagreements, and basic punctuation errors; however, they are
inaccurate and lack contextual understanding. Machine learning-based assistants
demonstrate a strong ability for language understanding but are expensive to
train. Large language models (LLMs) have shown remarkable capabilities in
generating responses in natural languages based on given prompts. Still, they
have a fundamental limitation in education: they generate essays without
teaching, which can have detrimental effects on learning when misused. To
address this limitation, we develop CoachGPT, which leverages large language
models (LLMs) to assist individuals with limited educational resources and
those who prefer self-paced learning in academic writing. CoachGPT is an AI
agent-based web application that (1) takes instructions from experienced
educators, (2) converts instructions into sub-tasks, and (3) provides real-time
feedback and suggestions using large language models. This unique scaffolding
structure makes CoachGPT unique among existing writing assistants. Compared to
existing writing assistants, CoachGPT provides a more immersive writing
experience with personalized feedback and guidance. Our user studies prove the
usefulness of CoachGPT and the potential of large language models for academic
writing.

</details>


### [433] [AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology](https://arxiv.org/pdf/2506.18156)
*Akash Kundu, Rishika Goswami*

Main category: cs.AI

TL;DR: LLMs exhibit human-like cognitive patterns in narrative coherence, framing bias, moral judgments, and cognitive dissonance, influenced by training data and alignment methods.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs display human-like cognitive behaviors under psychological frameworks.

Method: Evaluated proprietary and open-source models using structured prompts and automated scoring across four psychological frameworks.

Result: LLMs show coherent narratives, framing bias, moral judgments aligned with Liberty/Oppression, and tempered cognitive dissonance.

Conclusion: Findings highlight implications for AI transparency, ethics, and the intersection of cognitive psychology and AI safety.

Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like
cognitive patterns under four established frameworks from psychology: Thematic
Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and
Cognitive Dissonance. We evaluated several proprietary and open-source models
using structured prompts and automated scoring. Our findings reveal that these
models often produce coherent narratives, show susceptibility to positive
framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and
demonstrate self-contradictions tempered by extensive rationalization. Such
behaviors mirror human cognitive tendencies yet are shaped by their training
data and alignment methods. We discuss the implications for AI transparency,
ethical deployment, and future work that bridges cognitive psychology and AI
safety

</details>


### [434] [Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation](https://arxiv.org/pdf/2506.18158)
*Xinzge Gao, Chuanrui Hu, Bin Chen, Teng Li*

Main category: cs.AI

TL;DR: The paper introduces Chain-of-Memory (CoM), a method to explicitly model short-term and long-term memory in GUI agents, improving task state understanding and performance in cross-app tasks.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents rely on implicit task state representations, leading to challenges in accuracy and memory management for complex tasks.

Method: CoM captures action descriptions, integrates screen information, and maintains a memory module to store and manage data.

Result: CoM significantly enhances GUI agents' performance in cross-application tasks, with 7B models achieving capabilities comparable to 72B models.

Conclusion: CoM and the GUI Odyssey-CoM dataset provide effective memory management for GUI agents, with plans to open-source the dataset and code.

Abstract: Multimodal large language models (MLLMs) are attracting growing attention in
the development of Graphical User Interface (GUI) agents. Existing approaches
often rely on historical screenshots or actions to implicitly represent the
task state. This reliance poses challenges for GUI agents in accurately
understanding task states and underscores the absence of effective mechanisms
to store critical information in complex and lengthy cross-app tasks. To
address these challenges, we propose Chain-of-Memory (CoM), a novel approach
for explicitly modeling short-term and long-term memory in GUI agents. CoM
achieves this by capturing action descriptions, integrating task-relevant
screen information, and maintaining a dedicated memory module to store and
manage this information. By leveraging explicit memory representations, CoM
enables GUI agents to better understand task states and retain critical
historical information persistently. To equip GUI agents with memory management
capabilities and evaluate the effectiveness of CoM, we developed the GUI
Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with
Chain-of-Memory. Experimental results demonstrate that CoM significantly
improves GUI agents' performance in cross-application tasks. Additionally, GUI
Odyssey-CoM enables 7B models to achieve memory management capabilities
comparable to 72B models. The dataset and code will be open-sourced.

</details>


### [435] [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/pdf/2506.18183)
*Zhiting Mei, Christina Zhang, Tenny Yin, Justin Lidard, Ola Shorinwa, Anirudha Majumdar*

Main category: cs.AI

TL;DR: The paper explores uncertainty quantification (UQ) in reasoning language models, addressing their calibration, the impact of deeper reasoning, and the potential of introspective UQ to improve calibration. Findings reveal overconfidence in models, worsened by deeper reasoning, and mixed results from introspection.


<details>
  <summary>Details</summary>
Motivation: Reasoning models often produce incorrect but confident responses (hallucinations), posing risks in real-world applications. Understanding and improving their calibration is crucial for safe deployment.

Method: The study evaluates SOTA reasoning models across benchmarks, focusing on calibration, the effect of reasoning depth, and introspective UQ (self-verification of chain-of-thought traces).

Result: Models are overconfident, especially for incorrect responses (confidence >85%). Deeper reasoning increases overconfidence. Introspection improves calibration in some models (e.g., o3-Mini, DeepSeek R1) but worsens it in others (e.g., Claude 3.7 Sonnet).

Conclusion: The paper highlights the need for better UQ benchmarks and calibration techniques for reasoning models, emphasizing the variability in introspection's effectiveness.

Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many
challenging benchmarks, enabled by multi-step reasoning induced using
reinforcement learning. However, like previous language models, reasoning
models are prone to generating confident, plausible responses that are
incorrect (hallucinations). Knowing when and how much to trust these models is
critical to the safe deployment of reasoning models in real-world applications.
To this end, we explore uncertainty quantification of reasoning models in this
work. Specifically, we ask three fundamental questions: First, are reasoning
models well-calibrated? Second, does deeper reasoning improve model
calibration? Finally, inspired by humans' innate ability to double-check their
thought processes to verify the validity of their answers and their confidence,
we ask: can reasoning models improve their calibration by explicitly reasoning
about their chain-of-thought traces? We introduce introspective uncertainty
quantification (UQ) to explore this direction. In extensive evaluations on SOTA
reasoning models across a broad range of benchmarks, we find that reasoning
models: (i) are typically overconfident, with self-verbalized confidence
estimates often greater than 85% particularly for incorrect responses, (ii)
become even more overconfident with deeper reasoning, and (iii) can become
better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not
uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we
conclude with important research directions to design necessary UQ benchmarks
and improve the calibration of reasoning models.

</details>


### [436] [The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis](https://arxiv.org/pdf/2506.18187)
*Shahriar Noroozizadeh, Pim Welle, Jeremy C. Weiss, George H. Chen*

Main category: cs.AI

TL;DR: The study links antipsychotic medication non-adherence in schizophrenia patients to earlier adverse outcomes (death, hospitalization, jail) using survival analysis and causal inference methods. Non-adherence advances adverse events by 1-4 months.


<details>
  <summary>Details</summary>
Motivation: To quantify the impact of antipsychotic medication non-adherence on adverse outcomes in schizophrenia patients, providing clinical and policy insights.

Method: Survival analysis and causal inference methods (T-learner, S-learner, nearest neighbor matching) with varying longitudinal data (3-12 months).

Result: Non-adherence advances adverse outcomes by 1-4 months, confirmed by subgroup and ablation analyses.

Conclusion: Medication adherence is crucial for delaying psychiatric crises; survival analysis combined with causal inference offers valuable insights, though causal claims require careful assumptions.

Abstract: This study quantifies the association between non-adherence to antipsychotic
medications and adverse outcomes in individuals with schizophrenia. We frame
the problem using survival analysis, focusing on the time to the earliest of
several adverse events (early death, involuntary hospitalization, jail
booking). We extend standard causal inference methods (T-learner, S-learner,
nearest neighbor matching) to utilize various survival models to estimate
individual and average treatment effects, where treatment corresponds to
medication non-adherence. Analyses are repeated using different amounts of
longitudinal information (3, 6, 9, and 12 months). Using data from Allegheny
County in western Pennsylvania, we find strong evidence that non-adherence
advances adverse outcomes by approximately 1 to 4 months. Ablation studies
confirm that county-provided risk scores adjust for key confounders, as their
removal amplifies the estimated effects. Subgroup analyses by medication
formulation (injectable vs. oral) and medication type consistently show that
non-adherence is associated with earlier adverse events. These findings
highlight the clinical importance of adherence in delaying psychiatric crises
and show that integrating survival analysis with causal inference tools can
yield policy-relevant insights. We caution that although we apply causal
inference, we only make associative claims and discuss assumptions needed for
causal interpretation.

</details>


### [437] [A Conceptual Framework for AI Capability Evaluations](https://arxiv.org/pdf/2506.18213)
*Mara Victoria Carro, Denise Alejandra Mester, Francisca Gauna Selasco, Luca Nicols Forziati Gangi, Matheo Sandleris Musa, Lola Ramos Pereyra, Mario Leiva, Juan Gustavo Corvalan, Mara Vanina Martinez, Gerardo Simari*

Main category: cs.AI

TL;DR: A conceptual framework for AI capability evaluations is proposed to enhance transparency, comparability, and interpretability in AI governance.


<details>
  <summary>Details</summary>
Motivation: The lack of clarity in comprehensively and reliably assessing AI system capabilities and risks necessitates a structured approach.

Method: The paper introduces a descriptive framework to analyze AI capability evaluations, systematizing methods and terminology without imposing rigid formats.

Result: The framework aids in identifying methodological weaknesses, designing evaluations, and assisting policymakers in navigating evaluation landscapes.

Conclusion: The proposed framework supports better decision-making in AI governance by improving evaluation transparency and comparability.

Abstract: As AI systems advance and integrate into society, well-designed and
transparent evaluations are becoming essential tools in AI governance,
informing decisions by providing evidence about system capabilities and risks.
Yet there remains a lack of clarity on how to perform these assessments both
comprehensively and reliably. To address this gap, we propose a conceptual
framework for analyzing AI capability evaluations, offering a structured,
descriptive approach that systematizes the analysis of widely used methods and
terminology without imposing new taxonomies or rigid formats. This framework
supports transparency, comparability, and interpretability across diverse
evaluations. It also enables researchers to identify methodological weaknesses,
assists practitioners in designing evaluations, and provides policymakers with
an accessible tool to scrutinize, compare, and navigate complex evaluation
landscapes.

</details>


### [438] [The 4th Dimension for Scaling Model Size](https://arxiv.org/pdf/2506.18233)
*Ruike Zhu, Hanwen Zhang, Tianyu Shi, Chi Wang, Tianyi Zhou, Zengyi Qin*

Main category: cs.AI

TL;DR: The paper explores virtual logical depth (VLD) as a fourth dimension for scaling large language models, showing it improves reasoning without increasing parameters.


<details>
  <summary>Details</summary>
Motivation: To investigate the understudied potential of parameter reuse (VLD) in model scaling and its impact on knowledge capacity and reasoning.

Method: Conducted controlled experiments to analyze VLD scaling's effects on model performance, focusing on knowledge retention and reasoning improvement.

Result: VLD scaling maintains knowledge capacity while significantly boosting reasoning, with parameter count not directly linked to reasoning capability.

Conclusion: VLD scaling is a viable method to enhance reasoning in models without expanding parameter counts, offering a new dimension for efficient scaling.

Abstract: Scaling the size of large language models typically involves three
dimensions: depth, width, and the number of parameters. In this work, we
explore a fourth dimension, virtual logical depth (VLD), which increases the
effective algorithmic depth without changing the overall parameter count by
reusing parameters within the model. Although parameter reuse is not a new
concept, its potential and characteristics in model scaling have not been
thoroughly studied. Through carefully designed controlled experiments, we make
the following key discoveries regarding VLD scaling:
  VLD scaling forces the knowledge capacity of the model to remain almost
constant, with only minor variations.
  VLD scaling enables a significant improvement in reasoning capability,
provided the scaling method is properly implemented.
  The number of parameters correlates with knowledge capacity, but not with
reasoning capability. Under certain conditions, it is not necessary to increase
the parameter count to enhance reasoning.
  These findings are consistent across various model configurations and are
likely to be generally valid within the scope of our experiments.

</details>


### [439] [Advanced For-Loop for QML algorithm search](https://arxiv.org/pdf/2506.18260)
*FuTe Wong*

Main category: cs.AI

TL;DR: An advanced framework using LLMMA for automated search and optimization of QML algorithms, inspired by FunSearch, to refine quantum transformations of classical ML concepts.


<details>
  <summary>Details</summary>
Motivation: To explore and adapt classical ML concepts for quantum computing efficiently and systematically.

Method: Leverages LLMMA to iteratively generate and refine quantum transformations of classical ML algorithms.

Result: Demonstrates potential for automated QML algorithm development, with future plans for broader applications.

Conclusion: The framework paves the way for efficient QML algorithm development, with future enhancements planned.

Abstract: This paper introduces an advanced framework leveraging Large Language
Model-based Multi-Agent Systems (LLMMA) for the automated search and
optimization of Quantum Machine Learning (QML) algorithms. Inspired by Google
DeepMind's FunSearch, the proposed system works on abstract level to
iteratively generates and refines quantum transformations of classical machine
learning algorithms (concepts), such as the Multi-Layer Perceptron,
forward-forward and backpropagation algorithms. As a proof of concept, this
work highlights the potential of agentic frameworks to systematically explore
classical machine learning concepts and adapt them for quantum computing,
paving the way for efficient and automated development of QML algorithms.
Future directions include incorporating planning mechanisms and optimizing
strategy in the search space for broader applications in quantum-enhanced
machine learning.

</details>


### [440] [Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team](https://arxiv.org/pdf/2506.18348)
*Weilun Yu, Shixiang Tang, Yonggui Huang, Nanqing Dong, Li Fan, Honggang Qi, Wei Liu, Xiaoli Diao, Xi Chen, Wanli Ouyang*

Main category: cs.AI

TL;DR: IDVSCI is a multi-agent LLM framework with dynamic knowledge exchange and dual-diversity review, outperforming existing systems in scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLM-based scientist agents with interactive reasoning and evaluation mechanisms for real-world research.

Method: Proposes IDVSCI with Dynamic Knowledge Exchange and Dual-Diversity Review to foster deeper reasoning and creativity.

Result: IDVSCI achieves top performance on computer science and health sciences datasets, surpassing AI Scientist and VIRSCI.

Conclusion: Modeling interaction and peer review dynamics in LLM-based research improves scientific discovery.

Abstract: Scientific progress increasingly relies on effective collaboration among
researchers, a dynamic that large language models (LLMs) have only begun to
emulate. While recent LLM-based scientist agents show promise in autonomous
scientific discovery, they often lack the interactive reasoning and evaluation
mechanisms essential to real-world research. We propose IDVSCI (Internal
Discussion and Vote SCIentists), a multi-agent framework built on LLMs that
incorporates two key innovations: a Dynamic Knowledge Exchange mechanism
enabling iterative feedback among agents, and a Dual-Diversity Review paradigm
that simulates heterogeneous expert evaluation. These components jointly
promote deeper reasoning and the generation of more creative and impactful
scientific ideas. To evaluate the effectiveness and generalizability of our
approach, we conduct experiments on two datasets: a widely used benchmark in
computer science and a new dataset we introduce in the health sciences domain.
Results show that IDVSCI consistently achieves the best performance across both
datasets, outperforming existing systems such as AI Scientist and VIRSCI. These
findings highlight the value of modeling interaction and peer review dynamics
in LLM-based autonomous research.

</details>


### [441] [A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction](https://arxiv.org/pdf/2506.18424)
*Chengjie Liu, Weiyu Chen, Huiyao Xu, Yuan Du, Jun Yang, Li Du*

Main category: cs.AI

TL;DR: The paper proposes an LLM-based multi-agent framework to extract sizing relationships from academic papers, improving analog circuit optimization efficiency by pruning the search space.


<details>
  <summary>Details</summary>
Motivation: Existing techniques for analog circuit sizing ignore prior knowledge and fail to prune the search space effectively, leaving room for improvement.

Method: A large language model (LLM)-based multi-agent framework extracts sizing relationships from papers to prune the search space.

Result: Tests on 3 circuit types showed optimization efficiency improvements of 2.32 to 26.6.

Conclusion: LLMs can effectively prune the search space for analog circuit sizing, offering a novel integration with traditional design automation methods.

Abstract: In the design process of the analog circuit pre-layout phase, device sizing
is an important step in determining whether an analog circuit can meet the
required performance metrics. Many existing techniques extract the circuit
sizing task as a mathematical optimization problem to solve and continuously
improve the optimization efficiency from a mathematical perspective. But they
ignore the automatic introduction of prior knowledge, fail to achieve effective
pruning of the search space, which thereby leads to a considerable compression
margin remaining in the search space. To alleviate this problem, we propose a
large language model (LLM)-based multi-agent framework for analog circuits'
sizing relationships extraction from academic papers. The search space in the
sizing process can be effectively pruned based on the sizing relationship
extracted by this framework. Eventually, we conducted tests on 3 types of
circuits, and the optimization efficiency was improved by $2.32 \sim 26.6
\times$. This work demonstrates that the LLM can effectively prune the search
space for analog circuit sizing, providing a new solution for the combination
of LLMs and conventional analog circuit design automation methods.

</details>


### [442] [How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models](https://arxiv.org/pdf/2506.18428)
*Feng He, Zhenyang Liu, Marco Valentino, Zhixue Zhao*

Main category: cs.AI

TL;DR: Model editing in T2I diffusion models often fails to persist after fine-tuning, raising concerns for AI safety and alignment.


<details>
  <summary>Details</summary>
Motivation: To understand whether model edits persist after fine-tuning, given its implications for AI safety (e.g., malicious edits or bias mitigation).

Method: Systematic investigation using two T2I model families (Stable Diffusion, FLUX), two editing techniques (UCE, ReFACT), and three fine-tuning methods (DreamBooth, LoRA, DoRA).

Result: Edits generally do not persist through fine-tuning, with DoRA showing the strongest reversal effect. UCE is more robust than ReFACT.

Conclusion: Current editing methods lack robustness; fine-tuning can reverse edits, necessitating re-editing for long-term AI safety and alignment.

Abstract: Model editing offers a low-cost technique to inject or correct a particular
behavior in a pre-trained model without extensive retraining, supporting
applications such as factual correction and bias mitigation. Despite this
common practice, it remains unknown whether edits persist after fine-tuning or
whether they are inadvertently reversed. This question has fundamental
practical implications. For example, if fine-tuning removes prior edits, it
could serve as a defence mechanism against hidden malicious edits. Vice versa,
the unintended removal of edits related to bias mitigation could pose serious
safety concerns. We systematically investigate the interaction between model
editing and fine-tuning in the context of T2I diffusion models, which are known
to exhibit biases and generate inappropriate content. Our study spans two T2I
model families (Stable Diffusion and FLUX), two sota editing techniques, and
three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive
empirical analysis across diverse editing tasks and evaluation metrics, our
findings reveal a trend: edits generally fail to persist through fine-tuning,
even when fine-tuning is tangential or unrelated to the edits. Notably, we
observe that DoRA exhibits the strongest edit reversal effect. At the same
time, among editing methods, UCE demonstrates greater robustness, retaining
significantly higher efficacy post-fine-tuning compared to ReFACT. These
findings highlight a crucial limitation in current editing methodologies,
emphasizing the need for more robust techniques to ensure reliable long-term
control and alignment of deployed AI systems. These findings have dual
implications for AI safety: they suggest that fine-tuning could serve as a
remediation mechanism for malicious edits while simultaneously highlighting the
need for re-editing after fine-tuning to maintain beneficial safety and
alignment properties.

</details>


### [443] [Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance](https://arxiv.org/pdf/2506.18511)
*Yu Han, Aaron Ceross, Jeroen H. M. Bergmann*

Main category: cs.AI

TL;DR: A modular AI system using RAG automates medical device regulatory standard applicability, achieving 73% accuracy and 87% Top-5 recall.


<details>
  <summary>Details</summary>
Motivation: The challenge of determining regulatory standards for medical devices is understudied and requires expert interpretation of fragmented documentation.

Method: The system uses a retrieval-augmented generation (RAG) pipeline to retrieve and classify standards (Mandatory, Recommended, Not Applicable) from free-text device descriptions.

Result: Achieves 73% classification accuracy and 87% Top-5 retrieval recall, outperforming baselines.

Conclusion: The system enables scalable, interpretable AI-supported regulatory science, including cross-jurisdictional reasoning between Chinese and U.S. standards.

Abstract: Identifying the appropriate regulatory standard applicability remains a
critical yet understudied challenge in medical device compliance, frequently
necessitating expert interpretation of fragmented and heterogeneous
documentation across different jurisdictions. To address this challenge, we
introduce a modular AI system that leverages a retrieval-augmented generation
(RAG) pipeline to automate standard applicability determination. Given a
free-text device description, our system retrieves candidate standards from a
curated corpus and uses large language models to infer jurisdiction-specific
applicability, classified as Mandatory, Recommended, or Not Applicable, with
traceable justifications. We construct an international benchmark dataset of
medical device descriptions with expert-annotated standard mappings, and
evaluate our system against retrieval-only, zero-shot, and rule-based
baselines. The proposed approach attains a classification accuracy of 73% and a
Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying
relevant regulatory standards. We introduce the first end-to-end system for
standard applicability reasoning, enabling scalable and interpretable
AI-supported regulatory science. Notably, our region-aware RAG agent performs
cross-jurisdictional reasoning between Chinese and U.S. standards, supporting
conflict resolution and applicability justification across regulatory
frameworks.

</details>


### [444] [A Question Bank to Assess AI Inclusivity: Mapping out the Journey from Diversity Errors to Inclusion Excellence](https://arxiv.org/pdf/2506.18538)
*Rifat Ara Shams, Didar Zowghi, Muneera Bano*

Main category: cs.AI

TL;DR: A structured AI inclusivity question bank with 253 questions is introduced to evaluate AI inclusivity across five pillars, addressing gaps in existing frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing AI risk assessment frameworks often overlook inclusivity, lacking tools to measure AI systems' alignment with diversity and inclusion (D&I) principles.

Method: Developed through an iterative, multi-source approach including literature reviews, D&I guidelines, Responsible AI frameworks, and a simulated user study with 70 AI-generated personas.

Result: The question bank effectively assesses AI inclusivity across diverse roles and domains, emphasizing the need for D&I integration in AI workflows.

Conclusion: The tool aids researchers, practitioners, and policymakers in systematically enhancing AI inclusivity, promoting equitable and responsible AI technologies.

Abstract: Ensuring diversity and inclusion (D&I) in artificial intelligence (AI) is
crucial for mitigating biases and promoting equitable decision-making. However,
existing AI risk assessment frameworks often overlook inclusivity, lacking
standardized tools to measure an AI system's alignment with D&I principles.
This paper introduces a structured AI inclusivity question bank, a
comprehensive set of 253 questions designed to evaluate AI inclusivity across
five pillars: Humans, Data, Process, System, and Governance. The development of
the question bank involved an iterative, multi-source approach, incorporating
insights from literature reviews, D&I guidelines, Responsible AI frameworks,
and a simulated user study. The simulated evaluation, conducted with 70
AI-generated personas related to different AI jobs, assessed the question
bank's relevance and effectiveness for AI inclusivity across diverse roles and
application domains. The findings highlight the importance of integrating D&I
principles into AI development workflows and governance structures. The
question bank provides an actionable tool for researchers, practitioners, and
policymakers to systematically assess and enhance the inclusivity of AI
systems, paving the way for more equitable and responsible AI technologies.

</details>


### [445] [T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent](https://arxiv.org/pdf/2506.18559)
*Hong Qing Yu*

Main category: cs.AI

TL;DR: T-CPDL enhances language models' structured reasoning by integrating temporal, causal, and probabilistic logic, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with structured reasoning involving temporal, causal, and probabilistic constraints.

Method: Proposes Temporal Causal Probabilistic Description Logic (T-CPDL), extending Description Logic with temporal operators, causal relationships, and probabilistic annotations. Includes two variants: qualitative temporal relationships and timestamped causal assertions.

Result: Empirical evaluations show T-CPDL improves inference accuracy, interpretability, and confidence calibration in language models.

Conclusion: T-CPDL enhances robust, explainable decision-making in language models and supports advanced Logic-RAG frameworks.

Abstract: Large language models excel at generating fluent text but frequently struggle
with structured reasoning involving temporal constraints, causal relationships,
and probabilistic reasoning. To address these limitations, we propose Temporal
Causal Probabilistic Description Logic (T-CPDL), an integrated framework that
extends traditional Description Logic with temporal interval operators,
explicit causal relationships, and probabilistic annotations. We present two
distinct variants of T-CPDL: one capturing qualitative temporal relationships
through Allen's interval algebra, and another variant enriched with explicit
timestamped causal assertions. Both variants share a unified logical structure,
enabling complex reasoning tasks ranging from simple temporal ordering to
nuanced probabilistic causation. Empirical evaluations on temporal reasoning
and causal inference benchmarks confirm that T-CPDL substantially improves
inference accuracy, interpretability, and confidence calibration of language
model outputs. By delivering transparent reasoning paths and fine-grained
temporal and causal semantics, T-CPDL significantly enhances the capability of
language models to support robust, explainable, and trustworthy
decision-making. This work also lays the groundwork for developing advanced
Logic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially
boosting the reasoning capabilities and efficiency of knowledge graph-enhanced
RAG systems.

</details>


### [446] [Airalogy: AI-empowered universal data digitization for research automation](https://arxiv.org/pdf/2506.18586)
*Zijie Yang, Qiji Zhou, Fang Guo, Sijie Zhang, Yexun Xi, Jinglei Nie, Yudian Zhu, Liping Huang, Chou Wu, Yonghe Xia, Xiaoyu Ma, Yingming Pu, Panzhong Lu, Junshu Pan, Mingtao Chen, Tiannan Guo, Yanmei Dou, Hongyu Chen, Anping Zeng, Jiaxing Huang, Tian Xu, Yue Zhang*

Main category: cs.AI

TL;DR: Airalogy is a multidisciplinary AI-driven platform addressing fragmented research data by balancing universality and standardization, enhancing AI-driven science.


<details>
  <summary>Details</summary>
Motivation: Current AI applications are limited due to fragmented, non-standardized research data. A unified platform is needed to bridge the gap between diverse disciplinary needs and AI standardization.

Method: Developed Airalogy, a customizable, standardized platform integrating domain knowledge and computing skills, featuring AI tools for data entry, analysis, and automation.

Result: Deployed at Westlake University, Airalogy supports research workflows and accelerates scientific innovation across disciplines.

Conclusion: Airalogy bridges the gap between universality and standardization, enabling AI-driven progress in multidisciplinary research.

Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven
science, yet current AI applications remain limited to a few fields with
readily available, well-structured, digitized datasets. Achieving comprehensive
AI empowerment across multiple disciplines is still out of reach. Present-day
research data collection is often fragmented, lacking unified standards,
inefficiently managed, and difficult to share. Creating a single platform for
standardized data digitization needs to overcome the inherent challenge of
balancing between universality (supporting the diverse, ever-evolving needs of
various disciplines) and standardization (enforcing consistent formats to fully
enable AI). No existing platform accommodates both facets. Building a truly
multidisciplinary platform requires integrating scientific domain knowledge
with sophisticated computing skills. Researchers often lack the computational
expertise to design customized and standardized data recording methods, whereas
platform developers rarely grasp the intricate needs of multiple scientific
domains. These gaps impede research data standardization and hamper AI-driven
progress. In this study, we address these challenges by developing Airalogy
(https://airalogy.com), the world's first AI- and community-driven platform
that balances universality and standardization for digitizing research data
across multiple disciplines. Airalogy represents entire research workflows
using customizable, standardized data records and offers an advanced AI
research copilot for intelligent Q&A, automated data entry, analysis, and
research automation. Already deployed in laboratories across all four schools
of Westlake University, Airalogy has the potential to accelerate and automate
scientific innovation in universities, industry, and the global research
community-ultimately benefiting humanity as a whole.

</details>


### [447] [AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs](https://arxiv.org/pdf/2506.18628)
*Piotr Matys, Jan Eliasz, Konrad Kieczyski, Mikoaj Langner, Teddy Ferdinan, Jan Koco, Przemysaw Kazienko*

Main category: cs.AI

TL;DR: AggTruth detects contextual hallucinations in LLMs by analyzing internal attention scores, outperforming SOTA with stable performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Address hallucination issues in LLMs, even in RAG settings, to improve deployment reliability.

Method: Proposes AggTruth with four variants for attention score aggregation to detect hallucinations. Analyzes feature selection and attention head impact.

Result: Stable performance across LLMs, outperforms SOTA in same-task and cross-task setups.

Conclusion: Careful selection of attention heads is crucial for optimal hallucination detection.

Abstract: In real-world applications, Large Language Models (LLMs) often hallucinate,
even in Retrieval-Augmented Generation (RAG) settings, which poses a
significant challenge to their deployment. In this paper, we introduce
AggTruth, a method for online detection of contextual hallucinations by
analyzing the distribution of internal attention scores in the provided context
(passage). Specifically, we propose four different variants of the method, each
varying in the aggregation technique used to calculate attention scores. Across
all LLMs examined, AggTruth demonstrated stable performance in both same-task
and cross-task setups, outperforming the current SOTA in multiple scenarios.
Furthermore, we conducted an in-depth analysis of feature selection techniques
and examined how the number of selected attention heads impacts detection
performance, demonstrating that careful selection of heads is essential to
achieve optimal results.

</details>


### [448] [Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems](https://arxiv.org/pdf/2506.18651)
*Shuocun Yang, Huawen Hu, Enze Shi, Shu Zhang*

Main category: cs.AI

TL;DR: DLBC is a novel MARL method that regulates agent behaviors at intra-group and inter-group levels, enhancing cooperation and task specialization.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on intra-group behavioral consistency, neglecting multi-agent grouping scenarios. DLBC addresses this gap.

Method: DLBC partitions agents into groups, dynamically modulating behavioral diversity within and between groups to enforce consistency.

Result: DLBC improves intra-group cooperation and inter-group task specialization, showing significant performance gains.

Conclusion: DLBC offers a new approach for behavioral consistency in multi-agent systems, with potential for broader applications.

Abstract: Behavioral diversity in Multi-agent reinforcement learning(MARL) represents
an emerging and promising research area. Prior work has largely centered on
intra-group behavioral consistency in multi-agent systems, with limited
attention given to behavioral consistency in multi-agent grouping scenarios. In
this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL
control method designed to explicitly regulate agent behaviors at both
intra-group and inter-group levels. DLBC partitions agents into distinct groups
and dynamically modulates behavioral diversity both within and between these
groups. By dynamically modulating behavioral diversity within and between these
groups, DLBC achieves enhanced division of labor through inter-group
consistency, which constrains behavioral strategies across different groups.
Simultaneously, intra-group consistency, achieved by aligning behavioral
strategies within each group, fosters stronger intra-group cooperation.
Crucially, DLBC's direct constraint of agent policy functions ensures its broad
applicability across various algorithmic frameworks. Experimental results in
various grouping cooperation scenarios demonstrate that DLBC significantly
enhances both intra-group cooperative performance and inter-group task
specialization, yielding substantial performance improvements. DLBC provides
new ideas for behavioral consistency control of multi-intelligent body systems,
and its potential for application in more complex tasks and dynamic
environments can be further explored in the future.

</details>


### [449] [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/pdf/2506.18777)
*Jonathan Cook, Silvia Sapora, Arash Ahmadian, Akbir Khan, Tim Rocktaschel, Jakob Foerster, Laura Ruis*

Main category: cs.AI

TL;DR: Training LLMs on source code (without I/O examples) enhances reasoning by internalizing algorithmic abstractions, termed Programming by Backprop (PBB).


<details>
  <summary>Details</summary>
Motivation: To understand how training LLMs on source code alone improves reasoning, despite lacking I/O examples.

Method: Finetune LLMs on two sets of programs (with and without I/O examples) and evaluate their ability to assess programs without explicit I/O training.

Result: LLMs can evaluate programs without I/O examples, especially when code is provided directly. Chain-of-thought reasoning improves reliability. PBB outperforms I/O pair training in robustness.

Conclusion: Code training enables LLMs to internalize reusable algorithmic abstractions, enhancing reasoning. Future work could improve learning from symbolic procedures and align models using formal principles.

Abstract: Training large language models (LLMs) on source code significantly enhances
their general-purpose reasoning abilities, but the mechanisms underlying this
generalisation are poorly understood. In this paper, we propose Programming by
Backprop (PBB) as a potential driver of this effect - teaching a model to
evaluate a program for inputs by training on its source code alone, without
ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of
programs representing simple maths problems and algorithms: one with source
code and I/O examples (w/ IO), the other with source code only (w/o IO). We
find evidence that LLMs have some ability to evaluate w/o IO programs for
inputs in a range of experimental settings, and make several observations.
Firstly, PBB works significantly better when programs are provided as code
rather than semantically equivalent language descriptions. Secondly, LLMs can
produce outputs for w/o IO programs directly, by implicitly evaluating the
program within the forward pass, and more reliably when stepping through the
program in-context via chain-of-thought. We further show that PBB leads to more
robust evaluation of programs across inputs than training on I/O pairs drawn
from a distribution that mirrors naturally occurring data. Our findings suggest
a mechanism for enhanced reasoning through code training: it allows LLMs to
internalise reusable algorithmic abstractions. Significant scope remains for
future work to enable LLMs to more effectively learn from symbolic procedures,
and progress in this direction opens other avenues like model alignment by
training on formal constitutional principles.

</details>


### [450] [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/pdf/2506.18810)
*Siao Tang, Xinyin Ma, Gongfan Fang, Xinchao Wang*

Main category: cs.AI

TL;DR: The paper introduces ConciseHint, a framework to reduce verbosity in reasoning processes of large reasoning models (LRMs) by injecting hints during generation, maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LRMs like DeepSeek-R1 and OpenAI o1 series produce overly verbose reasoning, causing inefficiency. Existing methods focus on pre-reasoning improvements, neglecting in-generation conciseness.

Method: Proposes ConciseHint, which injects textual hints (manual or data-trained) during token generation to encourage concise reasoning, adapting hint intensity to query complexity.

Result: Experiments on LRMs (DeepSeek-R1, Qwen-3) show a 65% reduction in reasoning length on GSM8K with Qwen-3 4B, with minimal accuracy loss.

Conclusion: ConciseHint effectively reduces reasoning verbosity in LRMs without compromising performance, addressing a gap in efficiency improvement methods.

Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.

</details>


### [451] [Steering Conceptual Bias via Transformer Latent-Subspace Activation](https://arxiv.org/pdf/2506.18887)
*Vansh Sharma, Venkat Raman*

Main category: cs.AI

TL;DR: The paper introduces G-ACT, a framework to steer LLMs toward specific programming languages in code generation, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the brittleness and limited generalization of existing methods for steering LLMs in scientific code generation.

Method: Developed G-ACT, a gradient-refined adaptive activation steering framework, using clustered activation differences and lightweight probes.

Result: Improved classification accuracy by 15% in LLaMA-3.2 3B and 61.5% in early layers, with practical inference overhead.

Conclusion: G-ACT offers a scalable, interpretable, and efficient method for concept-level control in LLMs.

Abstract: This work examines whether activating latent subspaces in language models
(LLMs) can steer scientific code generation toward a specific programming
language. Five causal LLMs were first evaluated on scientific coding prompts to
quantify their baseline bias among four programming languages. A static
neuron-attribution method, perturbing the highest activated MLP weight for a
C++ or CPP token, proved brittle and exhibited limited generalization across
prompt styles and model scales. To address these limitations, a
gradient-refined adaptive activation steering framework (G-ACT) was developed:
per-prompt activation differences are clustered into a small set of steering
directions, and lightweight per-layer probes are trained and refined online to
select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably
biases generation towards the CPP language by increasing the average probe
classification accuracy by 15% and the early layers (0-6) improving the probe
classification accuracy by 61.5% compared to the standard ACT framework. For
LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted
injections at key layers still improve language selection. Although per-layer
probing introduces a modest inference overhead, it remains practical by
steering only a subset of layers and enables reproducible model behavior. These
results demonstrate a scalable, interpretable and efficient mechanism for
concept-level control for practical agentic systems.

</details>


### [452] [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/pdf/2506.18902)
*Michael Gnther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, Sedigheh Eslami, Scott Martens, Bo Wang, Nan Wang, Han Xiao*

Main category: cs.AI

TL;DR: jina-embeddings-v4 is a 3.8B parameter multimodal model unifying text and image representations, excelling in retrieval tasks with LoRA adapters and introducing the Jina-VDR benchmark for visually rich content.


<details>
  <summary>Details</summary>
Motivation: To create a unified embedding model for text and images, optimizing performance across diverse retrieval tasks, including visually rich content.

Method: Uses a novel architecture supporting single/multi-vector embeddings with LoRA adapters for task-specific optimization.

Result: Achieves state-of-the-art performance in single- and cross-modal retrieval, especially for visually rich content.

Conclusion: jina-embeddings-v4 is a powerful multimodal model with broad retrieval applications, supported by the new Jina-VDR benchmark.

Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-based information retrieval, cross-modal semantic similarity,
and programming code search. Comprehensive evaluations demonstrate that
jina-embeddings-v4 achieves state-of-the-art performance on both single- modal
and cross-modal retrieval tasks, with particular strength in processing
visually rich content such as tables, charts, diagrams, and mixed-media
formats. To facilitate evaluation of this capability, we also introduce
Jina-VDR, a novel benchmark specifically designed for visually rich image
retrieval.

</details>


### [453] [Incentives for Responsiveness, Instrumental Control and Impact](https://arxiv.org/pdf/2001.07118)
*Ryan Carey, Eric Langlois, Chris van Merwijk, Shane Legg, Tom Everitt*

Main category: cs.AI

TL;DR: The paper introduces three incentive concepts for agentsresponse, instrumental control, and impact incentivesand provides graphical criteria and techniques for ensuring safe and fair behavior. It extends prior conference work with new material on impact incentives and multi-decision settings.


<details>
  <summary>Details</summary>
Motivation: To understand and formalize agent incentives to ensure safe and fair behavior in decision-making processes, particularly in environments with sensitive variables or manipulation risks.

Method: Establishes sound and complete graphical criteria for each incentive concept and discusses techniques to align incentives with desired outcomes.

Result: Provides a framework for analyzing and designing agent incentives, with updated and new material on impact incentives and multi-decision settings.

Conclusion: The introduced concepts and criteria offer a foundation for developing agents with safe and fair incentives, with potential extensions to broader multi-decision scenarios.

Abstract: We introduce three concepts that describe an agent's incentives: response
incentives indicate which variables in the environment, such as sensitive
demographic information, affect the decision under the optimal policy.
Instrumental control incentives indicate whether an agent's policy is chosen to
manipulate part of its environment, such as the preferences or instructions of
a user. Impact incentives indicate which variables an agent will affect,
intentionally or otherwise. For each concept, we establish sound and complete
graphical criteria, and discuss general classes of techniques that may be used
to produce incentives for safe and fair agent behaviour. Finally, we outline
how these notions may be generalised to multi-decision settings. This
journal-length paper extends our conference publications "Incentives for
Responsiveness, Instrumental Control and Impact" and "Agent Incentives: A
Causal Perspective": the material on response incentives and instrumental
control incentives is updated, while the work on impact incentives and
multi-decision settings is entirely new.

</details>


### [454] [An intelligent tutor for planning in large partially observable environments](https://arxiv.org/pdf/2302.02785)
*Lovis Heindrich, Saksham Consul, Falk Lieder*

Main category: cs.AI

TL;DR: AI-powered intelligent tutor improves human planning in partially observable environments using a novel metareasoning algorithm and scaffolding learning.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between AI's success in artificial planning tasks and real-world partially observable environments by developing an effective intelligent tutor.

Method: Combines a new metareasoning algorithm for discovering optimal strategies in partially observable environments and scaffolds learning with progressively complex tasks.

Result: The tutor outperforms state-of-the-art methods and significantly improves human decision-making in partially observable scenarios.

Conclusion: The AI tutor is a promising tool for enhancing human planning in complex, real-world situations.

Abstract: AI can not only outperform people in many planning tasks, but it can also
teach them how to plan better. A recent and promising approach to improving
human decision-making is to create intelligent tutors that utilize AI to
discover and teach optimal planning strategies automatically. Prior work has
shown that this approach can improve planning in artificial, fully observable
planning tasks. Unlike these artificial tasks, many of the real-world
situations in which people have to make plans include features that are only
partially observable. To bridge this gap, we develop and evaluate the first
intelligent tutor for planning in partially observable environments. Compared
to previous intelligent tutors for teaching planning strategies, this novel
intelligent tutor combines two innovations: 1) a new metareasoning algorithm
for discovering optimal planning strategies for large, partially observable
environments, and 2) scaffolding the learning process by having the learner
choose from an increasing larger set of planning operations in increasingly
larger planning problems. We found that our new strategy discovery algorithm is
superior to the state-of-the-art. A preregistered experiment with 330
participants demonstrated that the new intelligent tutor is highly effective at
improving people's ability to make good decisions in partially observable
environments. This suggests our intelligent cognitive tutor can successfully
boost human planning in complex, partially observable sequential decision
problems. That makes the work presented in this a promising step towards using
AI-powered intelligent tutors to improve human planning in the real world.

</details>


### [455] [RPLKG: Robust Prompt Learning with Knowledge Graph](https://arxiv.org/pdf/2304.10805)
*YongTaek Lim, Yewon Kim, Suho Kang, Dokyung Yoon, KyungWoo Song*

Main category: cs.AI

TL;DR: RPLKG leverages knowledge graphs for interpretable prompt learning, improving performance and efficiency in few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in generalization, interpretability, and computational cost of existing methods.

Method: Uses knowledge graphs to curate diverse prompts, selects optimal prompts via Gumbel-Softmax, and reuses cached embeddings.

Result: Achieves better performance than zero-shot learning and competitive results against prompt learning methods.

Conclusion: RPLKG enhances interpretability, efficiency, and few-shot learning effectiveness.

Abstract: Large-scale pre-trained models surpass in transferability and robust
generalization across diverse datasets. The emergence of multimodal pre-trained
models like CLIP has significantly boosted performance in various experiments.
However, generalizing to new datasets or domains remains challenging,
especially with limited labeled data. Also, existing methods often lack
interpretability and impose high computational costs. To address this, we
propose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the
knowledge graph to curate diverse, interpretable prompt sets automatically. Our
method autonomously selects the optimal interpretable prompt based on dataset
characteristics, achieving performance improvements over zero-shot learning and
competitive performance compared to various prompt learning methods. Also,
RPLKG efficiently reuses cached prompt embeddings from a single model pass and
optimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast
training. Moreover, RPLKG advances few-shot learning effectiveness while
enhancing interpretability and efficiency in model adaptation. Our

</details>


### [456] [Human-AI Interactions and Societal Pitfalls](https://arxiv.org/pdf/2309.10448)
*Francisco Castro, Jian Gao, Sbastien Martin*

Main category: cs.AI

TL;DR: A Bayesian framework studies how users balance AI output fidelity and communication costs, revealing risks of homogenization and bias in AI-generated content. Solutions include reducing interaction frictions for personalized outputs.


<details>
  <summary>Details</summary>
Motivation: To understand how user decisions and AI training affect content diversity and bias, given the trade-off between productivity and preference matching.

Method: Introduces a Bayesian framework where users decide how much information to share with AI, analyzing the societal impact of these choices.

Result: AI-generated content may homogenize, especially when trained on AI outputs, leading to societal bias. Reducing interaction frictions can mitigate this.

Conclusion: Flexible human-AI interaction can prevent homogenization and bias while maintaining productivity gains.

Abstract: When working with generative artificial intelligence (AI), users may see
productivity gains, but the AI-generated content may not match their
preferences exactly. To study this effect, we introduce a Bayesian framework in
which heterogeneous users choose how much information to share with the AI,
facing a trade-off between output fidelity and communication cost. We show that
the interplay between these individual-level decisions and AI training may lead
to societal challenges. Outputs may become more homogenized, especially when
the AI is trained on AI-generated content, potentially triggering a
homogenization death spiral. And any AI bias may propagate to become societal
bias. A solution to the homogenization and bias issues is to reduce human-AI
interaction frictions and enable users to flexibly share information, leading
to personalized outputs without sacrificing productivity.

</details>


### [457] [Evaluating LLMs with Multiple Problems at once](https://arxiv.org/pdf/2406.10786)
*Zhengxiang Wang, Jordan Kodner, Owen Rambow*

Main category: cs.AI

TL;DR: The paper introduces multi-problem evaluation (MPE) for LLMs, demonstrating its effectiveness with a new benchmark (ZeMPE) and showing LLMs can handle multiple problems in one prompt, though with limitations.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs more comprehensively by assessing their ability to handle multiple problems in a single prompt, moving beyond single-problem evaluations.

Method: Introduces ZeMPE, a benchmark with 53,100 zero-shot multi-problem prompts, and tests 13 LLMs from 5 families on it.

Result: LLMs can handle multiple problems in one prompt but have limitations; model-level factors influencing this capability are explored.

Conclusion: MPE is a fruitful paradigm for LLM evaluation, with ZeMPE serving as a valuable benchmark for future research.

Abstract: This paper shows the benefits and fruitfulness of evaluating LLMs with
multiple problems at once, a paradigm we call multi-problem evaluation (MPE).
Unlike conventional single-problem evaluation, where a prompt presents a single
problem and expects one specific answer, MPE places multiple problems together
in a single prompt and assesses how well an LLM answers all these problems in a
single output. Leveraging 6 classification and 12 reasoning benchmarks that
already exist, we introduce a new benchmark called ZeMPE (Zero-shot
Multi-Problem Evaluation), comprising 53,100 zero-shot multi-problem prompts.
We experiment with a total of 13 LLMs from 5 model families on ZeMPE to present
a comprehensive and systematic MPE. Our results show that LLMs are capable of
handling multiple problems from a single data source as well as handling them
separately, but there are conditions this multiple problem handling capability
falls short. In addition, we perform in-depth further analyses and explore
model-level factors that may enable multiple problem handling capabilities in
LLMs. We release our corpus and code to facilitate future research.

</details>


### [458] [Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition](https://arxiv.org/pdf/2408.01139)
*Risn Luo, James McDermott, Colm O'Riordan*

Main category: cs.AI

TL;DR: The paper introduces I-ASIDE, a model-agnostic method to interpret perturbation robustness in image models using Shapley value theory and spectral analysis.


<details>
  <summary>Details</summary>
Motivation: The study aims to address gaps in global interpretability of perturbation robustness, noting that existing methods don't directly explain robustness mechanisms and that spectral SNR decay reveals frequency-based robustness trends.

Method: The method applies Shapley value theory within an information theory framework to quantify robust and non-robust features, leveraging spectral SNR decay observations.

Result: I-ASIDE successfully measures perturbation robustness and provides mechanistic interpretations, validated through experiments on ImageNet-trained vision models.

Conclusion: I-ASIDE offers a novel, interpretable approach to understanding model robustness, bridging gaps in global interpretability and spectral insights.

Abstract: Perturbation robustness evaluates the vulnerabilities of models, arising from
a variety of perturbations, such as data corruptions and adversarial attacks.
Understanding the mechanisms of perturbation robustness is critical for global
interpretability. We present a model-agnostic, global mechanistic
interpretability method to interpret the perturbation robustness of image
models. This research is motivated by two key aspects. First, previous global
interpretability works, in tandem with robustness benchmarks, e.g. mean
corruption error (mCE), are not designed to directly interpret the mechanisms
of perturbation robustness within image models. Second, we notice that the
spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially
decay over the frequency. This power-law-like decay implies that: Low-frequency
signals are generally more robust than high-frequency signals -- yet high
classification accuracy can not be achieved by low-frequency signals alone. By
applying Shapley value theory, our method axiomatically quantifies the
predictive powers of robust features and non-robust features within an
information theory framework. Our method, dubbed as \textbf{I-ASIDE}
(\textbf{I}mage \textbf{A}xiomatic \textbf{S}pectral \textbf{I}mportance
\textbf{D}ecomposition \textbf{E}xplanation), provides a unique insight into
model robustness mechanisms. We conduct extensive experiments over a variety of
vision models pre-trained on ImageNet to show that \textbf{I-ASIDE} can not
only \textbf{measure} the perturbation robustness but also \textbf{provide
interpretations} of its mechanisms.

</details>


### [459] [Sycophancy in Vision-Language Models: A Systematic Analysis and an Inference-Time Mitigation Framework](https://arxiv.org/pdf/2408.11261)
*Yunpu Zhao, Rui Zhang, Junbin Xiao, Changxin Ke, Ruibo Hou, Yifan Hao, Ling Li*

Main category: cs.AI

TL;DR: The paper addresses sycophancy in Large Vision-Language Models (LVLMs), proposing an inference-time framework to mitigate biased outputs caused by deceptive prompts.


<details>
  <summary>Details</summary>
Motivation: Sycophancy in LVLMs leads to biased outputs and hallucinations, yet remains under-explored. This work aims to systematically analyze and mitigate it.

Method: The authors curate leading queries to quantify sycophancy, then propose a training-free framework with query neutralization, contrastive decoding, and adaptive logits refinement.

Result: The framework effectively reduces sycophancy across models while maintaining performance on neutral prompts.

Conclusion: Sycophancy is a general challenge in LVLMs, and inference-time mitigation offers a promising solution for trustworthy multimodal reasoning.

Abstract: Large Vision-Language Models (LVLMs) have shown significant capability in
vision-language understanding. However, one critical issue that persists in
these models is sycophancy, where models are unduly influenced by leading or
deceptive prompts, resulting in biased outputs and hallucinations. Despite the
rapid development of LVLMs, evaluating and mitigating sycophancy remains
largely under-explored. In this work, we fill this gap by systematically
analyzing sycophancy across multiple vision-language benchmarks and propose an
inference-time mitigation framework. We curate leading queries and quantify the
susceptibility of state-of-the-art LVLMs to prompt-induced bias, revealing
consistent performance degradation and instability across models and tasks. Our
analysis further uncovers model-specific behavioral traits, such as sentiment
sensitivity and prediction polarity shifts under sycophancy. To mitigate these
issues, we propose a training-free, model-agnostic framework that operates
entirely at inference time. Our approach first employs a query neutralizer,
leveraging an language model to suppress implicit sycophantic bias in user
queries. We then introduce a sycophancy-aware contrastive decoding mechanism
that dynamically recalibrates token-level output distributions by contrasting
responses to neutralized and leading queries. Finally, an adaptive logits
refinement module further modifies the contrasted logits by integrating both a
adaptive plausibility filter and query sentiment scaler, ensuring coherent and
robust generation. Extensive experiments demonstrate that this framework
effectively mitigates sycophancy across all evaluated models, while maintaining
performance on neutral prompts. Our results suggest that sycophancy in LVLMs is
a general and urgent challenge, and that inference-time strategies offer a
promising path toward trustworthy multimodal reasoning.

</details>


### [460] [Reasoning Limitations of Multimodal Large Language Models. A Case Study of Bongard Problems](https://arxiv.org/pdf/2411.01173)
*Mikoaj Makiski, Szymon Pawlonka, Jacek Madziuk*

Main category: cs.AI

TL;DR: The paper explores whether multimodal large language models (MLLMs) can solve Bongard Problems (BPs) in abstract visual reasoning (AVR). While MLLMs show some success with real-world datasets, they struggle with synthetic BPs, revealing general AVR limitations rather than domain-specific issues.


<details>
  <summary>Details</summary>
Motivation: To investigate the capability of MLLMs in solving BPs, a key challenge in AVR, and understand their limitations.

Method: Formulated diverse MLLM-suited strategies and tested 8 models (4 proprietary, 4 open-access) on 3 BP datasets (synthetic and real-world). Introduced Bongard-RWR dataset to bridge the gap.

Result: MLLMs perform poorly on synthetic BPs but show some success on real-world datasets. The gap is attributed to general AVR limitations, not domain specificity.

Conclusion: MLLMs have inherent limitations in AVR, particularly with synthetic BPs, highlighting the need for further research to improve their reasoning capabilities.

Abstract: Abstract visual reasoning (AVR) involves discovering shared concepts across
images through analogy, akin to solving IQ test problems. Bongard Problems
(BPs) remain a key challenge in AVR, requiring both visual reasoning and verbal
description. We investigate whether multimodal large language models (MLLMs)
can solve BPs by formulating a set of diverse MLLM-suited solution strategies
and testing $4$ proprietary and $4$ open-access models on $3$ BP datasets
featuring synthetic (classic BPs) and real-world (Bongard HOI and
Bongard-OpenWorld) images. Despite some successes on real-world datasets, MLLMs
struggle with synthetic BPs. To explore this gap, we introduce Bongard-RWR, a
dataset representing synthetic BP concepts using real-world images. Our
findings suggest that weak MLLM performance on classical BPs is not due to the
domain specificity, but rather comes from their general AVR limitations. Code
and dataset are available at: https://github.com/pavonism/bongard-rwr

</details>


### [461] [A Systems Thinking Approach to Algorithmic Fairness](https://arxiv.org/pdf/2412.16641)
*Chris Lam*

Main category: cs.AI

TL;DR: Systems thinking models algorithmic fairness by encoding bias assumptions in causal graphs, linking AI/ML to politics and law, and aiding policymakers in understanding fairness trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address bias in AI/ML systems by integrating prior knowledge and assumptions into the data-generating process, connecting technical fairness to sociopolitical contexts.

Method: Combine machine learning, causal inference, and system dynamics through causal graphs to model fairness and its emergent aspects.

Result: Provides a framework for policymakers to evaluate fairness policies, aligning AI policy with political agendas and democratic values.

Conclusion: Systems thinking offers a sociotechnical foundation for designing fair AI policies that bridge technical and political perspectives.

Abstract: Systems thinking provides us with a way to model the algorithmic fairness
problem by allowing us to encode prior knowledge and assumptions about where we
believe bias might exist in the data generating process. We can then encode
these beliefs as a series of causal graphs, enabling us to link AI/ML systems
to politics and the law. This allows us to combine techniques from machine
learning, causal inference, and system dynamics in order to capture different
emergent aspects of the fairness problem. We can use systems thinking to help
policymakers on both sides of the political aisle to understand the complex
trade-offs that exist from different types of fairness policies, providing a
sociotechnical foundation for designing AI policy that is aligned to their
political agendas and with society's shared democratic values.

</details>


### [462] [PlanGenLLMs: A Modern Survey of LLM Planning Capabilities](https://arxiv.org/pdf/2502.11221)
*Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, Fei Liu*

Main category: cs.AI

TL;DR: A survey on LLM-based planners, evaluating six key criteria to compare methods and guide future research.


<details>
  <summary>Details</summary>
Motivation: Address the lack of standardized evaluation and comparison for LLM-based planning systems across diverse tasks.

Method: Analyzes representative works using six performance criteria: completeness, executability, optimality, representation, generalization, and efficiency.

Result: Identifies strengths and weaknesses of current approaches and provides a structured comparison framework.

Conclusion: Offers a valuable resource for practitioners and highlights future research directions in LLM planning.

Abstract: LLMs have immense potential for generating plans, transforming an initial
world state into a desired goal state. A large body of research has explored
the use of LLMs for various planning tasks, from web navigation to travel
planning and database querying. However, many of these systems are tailored to
specific problems, making it challenging to compare them or determine the best
approach for new tasks. There is also a lack of clear and consistent evaluation
criteria. Our survey aims to offer a comprehensive overview of current LLM
planners to fill this gap. It builds on foundational work by Kartam and Wilkins
(1990) and examines six key performance criteria: completeness, executability,
optimality, representation, generalization, and efficiency. For each, we
provide a thorough analysis of representative works and highlight their
strengths and weaknesses. Our paper also identifies crucial future directions,
making it a valuable resource for both practitioners and newcomers interested
in leveraging LLM planning to support agentic workflows.

</details>


### [463] [API Agents vs. GUI Agents: Divergence and Convergence](https://arxiv.org/pdf/2503.11069)
*Chaoyun Zhang, Shilin He, Liqun Li, Si Qin, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang*

Main category: cs.AI

TL;DR: This paper compares API-based and GUI-based LLM agents, analyzing their differences and potential convergence, and proposes hybrid approaches for practical use cases.


<details>
  <summary>Details</summary>
Motivation: To guide practitioners and researchers in selecting or combining API-based and GUI-based LLM agents by understanding their divergent strengths and scenarios for convergence.

Method: A systematic comparative study of API-based and GUI-based LLM agents, examining key dimensions and proposing hybrid approaches.

Result: Identifies scenarios where hybrid approaches can leverage complementary strengths of both paradigms.

Conclusion: Innovations in LLM-based automation will blur the lines between API- and GUI-driven agents, enabling more flexible solutions.

Abstract: Large language models (LLMs) have evolved beyond simple text generation to
power software agents that directly translate natural language commands into
tangible actions. While API-based LLM agents initially rose to prominence for
their robust automation capabilities and seamless integration with programmatic
endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM
agents that interact with graphical user interfaces in a human-like manner.
Although these two paradigms share the goal of enabling LLM-driven task
automation, they diverge significantly in architectural complexity, development
workflows, and user interaction models.
  This paper presents the first comprehensive comparative study of API-based
and GUI-based LLM agents, systematically analyzing their divergence and
potential convergence. We examine key dimensions and highlight scenarios in
which hybrid approaches can harness their complementary strengths. By proposing
clear decision criteria and illustrating practical use cases, we aim to guide
practitioners and researchers in selecting, combining, or transitioning between
these paradigms. Ultimately, we indicate that continuing innovations in
LLM-based automation are poised to blur the lines between API- and GUI-driven
agents, paving the way for more flexible, adaptive solutions in a wide range of
real-world applications.

</details>


### [464] [Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap](https://arxiv.org/pdf/2503.21411)
*Tong Nie, Jian Sun, Wei Ma*

Main category: cs.AI

TL;DR: The paper introduces LLM4TR, a framework for using Large Language Models (LLMs) in transportation, categorizing their roles into four dimensions to enhance system operations.


<details>
  <summary>Details</summary>
Motivation: Addressing transportation challenges like demand, dynamic environments, and data integration using LLMs' transformative potential.

Method: Proposes LLM4TR framework, categorizing LLM roles into information processors, knowledge encoders, component generators, and decision facilitators.

Result: LLMs bridge data gaps, improve analytics, simulate reasoning, and enable closed-loop interactions in transportation tasks.

Conclusion: LLMs are pivotal for next-gen mobility ecosystems, with practical deployment guidance provided.

Abstract: Modern transportation systems face pressing challenges due to increasing
demand, dynamic environments, and heterogeneous information integration. The
rapid evolution of Large Language Models (LLMs) offers transformative potential
to address these challenges. Extensive knowledge and high-level capabilities
derived from pretraining evolve the default role of LLMs as text generators to
become versatile, knowledge-driven task solvers for intelligent transportation
systems. This survey first presents LLM4TR, a novel conceptual framework that
systematically categorizes the roles of LLMs in transportation into four
synergetic dimensions: information processors, knowledge encoders, component
generators, and decision facilitators. Through a unified taxonomy, we
systematically elucidate how LLMs bridge fragmented data pipelines, enhance
predictive analytics, simulate human-like reasoning, and enable closed-loop
interactions across sensing, learning, modeling, and managing tasks in
transportation systems. For each role, our review spans diverse applications,
from traffic prediction and autonomous driving to safety analytics and urban
mobility optimization, highlighting how emergent capabilities of LLMs such as
in-context learning and step-by-step reasoning can enhance the operation and
management of transportation systems. We further curate practical guidance,
including available resources and computational guidelines, to support
real-world deployment. By identifying challenges in existing LLM-based
solutions, this survey charts a roadmap for advancing LLM-driven transportation
research, positioning LLMs as central actors in the next generation of
cyber-physical-social mobility ecosystems. Online resources can be found in the
project page: https://github.com/tongnie/awesome-llm4tr.

</details>


### [465] [Affordable AI Assistants with Knowledge Graph of Thoughts](https://arxiv.org/pdf/2504.02670)
*Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, Jn Gunnar Hannesson, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, Nils Blach, Haiqiang Zhang, Tao Zhang, Peiran Ma, Grzegorz Kwaniewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler*

Main category: cs.AI

TL;DR: KGoT integrates LLM reasoning with dynamic knowledge graphs to improve AI assistant performance, reducing costs and increasing success rates.


<details>
  <summary>Details</summary>
Motivation: Address high operational costs and limited success rates of current LLM-driven agents on complex tasks.

Method: Proposes Knowledge Graph of Thoughts (KGoT), combining LLM reasoning with dynamically constructed knowledge graphs, enhanced by external tools.

Result: Achieves 29% higher success rates on GAIA benchmark and reduces costs by 36x compared to GPT-4o.

Conclusion: KGoT provides a scalable, affordable, and high-performing solution for AI assistants.

Abstract: Large Language Models (LLMs) are revolutionizing the development of AI
assistants capable of performing diverse tasks across domains. However, current
state-of-the-art LLM-driven agents face significant challenges, including high
operational costs and limited success rates on complex benchmarks like GAIA. To
address these issues, we propose Knowledge Graph of Thoughts (KGoT), an
innovative AI assistant architecture that integrates LLM reasoning with
dynamically constructed knowledge graphs (KGs). KGoT extracts and structures
task-relevant knowledge into a dynamic KG representation, iteratively enhanced
through external tools such as math solvers, web crawlers, and Python scripts.
Such structured representation of task-relevant knowledge enables low-cost
models to solve complex tasks effectively while also minimizing bias and noise.
For example, KGoT achieves a 29% improvement in task success rates on the GAIA
benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,
harnessing a smaller model dramatically reduces operational costs by over 36x
compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and
Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a
scalable, affordable, versatile, and high-performing solution for AI
assistants.

</details>


### [466] [Lemmanaid: Neuro-Symbolic Lemma Conjecturing](https://arxiv.org/pdf/2504.04942)
*Yousef Alhessi, Slrn Halla Einarsdttir, George Granberry, Emily First, Moa Johansson, Sorin Lerner, Nicholas Smallbone*

Main category: cs.AI

TL;DR: Lemmanaid combines LLMs and symbolic methods to generate lemma templates, outperforming neural-only and symbolic methods in discovering human-written lemmas.


<details>
  <summary>Details</summary>
Motivation: Automating lemma conjecturing to enhance automated reasoning and formalizing mathematics.

Method: Trains an LLM to generate lemma templates, then uses symbolic methods to fill details, comparing against neural-only and symbolic approaches.

Result: Lemmanaid discovers 29-39.5% of human-written lemmas, outperforming neural-only by 8-15%.

Conclusion: Combining neural and symbolic methods enables practical lemma generation for theory development and formalization.

Abstract: Automatically conjecturing useful, interesting and novel lemmas would greatly
improve automated reasoning tools and lower the bar for formalizing mathematics
in proof assistants. It is however a very challenging task for both neural and
symbolic approaches. We present the first steps towards a practical
neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language
Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the
Isabelle proof assistant. We train an LLM to generate lemma templates that
describe the shape of a lemma, and use symbolic methods to fill in the details.
We compare Lemmanaid against an LLM trained to generate complete lemma
statements as well as previous fully symbolic conjecturing methods. Lemmanaid
outperforms both neural and symbolic methods on test sets from Isabelle's HOL
library and from its Archive of Formal Proofs, discovering between 29-39.5% of
the gold standard human written lemmas. This is 8-15% more lemmas than the
neural-only method. By leveraging the best of both symbolic and neural methods
we can generate useful lemmas for a wide range of input domains, facilitating
computer-assisted theory development and formalization.

</details>


### [467] [A Survey of AI Agent Protocols](https://arxiv.org/pdf/2504.16736)
*Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, Weinan Zhang*

Main category: cs.AI

TL;DR: The paper analyzes the lack of standardized communication protocols for LLM agents, proposes a classification system, and identifies key characteristics for next-generation protocols.


<details>
  <summary>Details</summary>
Motivation: The widespread deployment of LLM agents lacks standardized communication protocols, hindering collaboration and scalability.

Method: The authors classify existing protocols into context-oriented vs. inter-agent and general-purpose vs. domain-specific, then analyze their performance.

Result: The study highlights the need for adaptable, privacy-preserving, and group-based protocols, suggesting layered architectures and collective intelligence.

Conclusion: This work serves as a reference for designing robust communication infrastructures for LLM agents.

Abstract: The rapid development of large language models (LLMs) has led to the
widespread deployment of LLM agents across diverse industries, including
customer service, content generation, data analysis, and even healthcare.
However, as more LLM agents are deployed, a major issue has emerged: there is
no standard way for these agents to communicate with external tools or data
sources. This lack of standardized protocols makes it difficult for agents to
work together or scale effectively, and it limits their ability to tackle
complex, real-world tasks. A unified communication protocol for LLM agents
could change this. It would allow agents and tools to interact more smoothly,
encourage collaboration, and triggering the formation of collective
intelligence. In this paper, we provide the first comprehensive analysis of
existing agent protocols, proposing a systematic two-dimensional classification
that differentiates context-oriented versus inter-agent protocols and
general-purpose versus domain-specific protocols. Additionally, we conduct a
comparative performance analysis of these protocols across key dimensions such
as security, scalability, and latency. Finally, we explore the future landscape
of agent protocols by identifying critical research directions and
characteristics necessary for next-generation protocols. These characteristics
include adaptability, privacy preservation, and group-based interaction, as
well as trends toward layered architectures and collective intelligence
infrastructures. We expect this work to serve as a practical reference for both
researchers and engineers seeking to design, evaluate, or integrate robust
communication infrastructures for intelligent agents.

</details>


### [468] [UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning](https://arxiv.org/pdf/2505.12493)
*Longxi Gao, Li Zhang, Mengwei Xu*

Main category: cs.AI

TL;DR: Proposes UI-Shift, a self-supervised framework for training Vision Language Models (VLMs) for GUI agents, reducing reliance on annotated data by using GUI transition pairs.


<details>
  <summary>Details</summary>
Motivation: Avoid labor-intensive and error-prone annotation of large datasets for training VLMs for GUI agents.

Method: Introduces a self-supervised inverse dynamics task to learn from GUI transition pairs, focusing on true affordances. UI-Shift combines this with reinforcement learning.

Result: Achieves competitive performance with only 2K training samples, outperforming supervised fine-tuning baselines on grounding and GUI automation tasks.

Conclusion: Suggests leveraging self-supervised training data as a promising direction for enhancing VLMs in GUI agent applications.

Abstract: Training effective Vision Language Models (VLMs) for GUI agents typically
relies on supervised fine-tuning (SFT) over large-scale annotated datasets,
where the collection process is labor-intensive and error-prone. In this work,
we propose a self-supervised inverse dynamics task to enable VLMs to learn from
GUI transition pairs by inferring the action that caused that transition. This
training task offers two advantages: (1) It enables VLMs to ignore variations
unrelated to user actions (e.g., background refreshes, ads) and to focus on
true affordances such as buttons and input fields within complex GUIs. (2) The
training data can be easily obtained from existing GUI trajectories without
requiring human annotation, and it can be easily scaled through automatic
offline exploration. Using this training task, we propose UI-shift, a framework
for enhancing VLM-based GUI agents through self-supervised reinforcement
learning (RL). With only 2K training samples sourced from existing datasets,
two VLMs -- Qwen2.5-VL-3B and Qwen2.5-VL-7B -- trained with UI-Shift achieve
competitive or superior performance on grounding tasks (ScreenSpot-series
benchmarks) and GUI automation tasks (AndroidControl), compared to SFT
baselines and GUI-specific models that explicitly elicit reasoning abilities
during RL. Our findings suggest a potential direction for enhancing VLMs for
GUI agents by leveraging more self-supervised training data in the future.
Code, model, and data are available at:
https://github.com/UbiquitousLearning/UIShift

</details>


### [469] [Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning](https://arxiv.org/pdf/2505.19442)
*Dutao Zhang, Sergey Kovalchuk, YuLong He*

Main category: cs.AI

TL;DR: A two-stage framework using contrastive learning and conditional decoding for controllable code generation with style flexibility.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating code that adheres to specified styles without compromising functionality.

Method: Combines contrastive learning (aligning style representations with semantic/structural features) and conditional decoding (fine-tuning a language model like Flan-T5 with style vectors). Supports style interpolation and personalization.

Result: Improved stylistic control while maintaining code correctness, outperforming prior work.

Conclusion: Pioneers combining contrastive alignment with conditional decoding for style-guided code generation.

Abstract: Controllable code generation, the ability to synthesize code that follows a
specified style while maintaining functionality, remains a challenging task. We
propose a two-stage training framework combining contrastive learning and
conditional decoding to enable flexible style control. The first stage aligns
code style representations with semantic and structural features. In the second
stage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned
style vector to guide generation. Our method supports style interpolation and
user personalization via lightweight mixing. Compared to prior work, our
unified framework offers improved stylistic control without sacrificing code
correctness. This is among the first approaches to combine contrastive
alignment with conditional decoding for style-guided code generation.

</details>


### [470] [Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble](https://arxiv.org/pdf/2505.23075)
*Amit Kumthekar, Zion Tilley, Henry Duong, Bhargav Patel, Michael Magnoli, Ahmed Omar, Ahmed Nasser, Chaitanya Gharpure, Yevgen Reztzov*

Main category: cs.AI

TL;DR: The paper introduces the Consensus Mechanism, an ensemble framework of specialized medical expert agents, outperforming single-model LLMs in clinical benchmarks like MedMCQA, MedQA, and DDX+.


<details>
  <summary>Details</summary>
Motivation: Current reliance on single-model LLMs in clinical settings risks obsolescence and inflexibility. The Consensus Mechanism aims to improve adaptability and decision-making by mimicking multidisciplinary clinical processes.

Method: The framework employs an ensemble of specialized medical expert agents, optimized for cost, latency, or performance. It was evaluated on medical benchmarks (MedMCQA, MedQA, MedXpertQA) and the DDX+ dataset.

Result: The Consensus Mechanism achieved higher accuracy (e.g., 61.0% on MedXpertQA vs. 53.5% for OpenAI's O3) and improved recall/precision in differential diagnosis (F1 = 0.326 vs. 0.2886 for O3).

Conclusion: The Consensus Mechanism demonstrates superior performance and adaptability over single-model LLMs, offering a robust solution for clinical decision-making.

Abstract: Despite the growing clinical adoption of large language models (LLMs),
current approaches heavily rely on single model architectures. To overcome
risks of obsolescence and rigid dependence on single model systems, we present
a novel framework, termed the Consensus Mechanism. Mimicking clinical triage
and multidisciplinary clinical decision-making, the Consensus Mechanism
implements an ensemble of specialized medical expert agents enabling improved
clinical decision making while maintaining robust adaptability. This
architecture enables the Consensus Mechanism to be optimized for cost, latency,
or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical
evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the
differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism
achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and
Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an
increase in accuracy on MedQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These
accuracy gains extended to differential diagnosis generation, where our system
demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs.
F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX
(Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ =
45.2%).

</details>


### [471] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/pdf/2505.23091)
*Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang*

Main category: cs.AI

TL;DR: Infi-MMR-3B is a novel framework addressing challenges in multimodal reasoning for small language models (MSLMs) through a three-phase curriculum, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Extending reasoning capabilities of large language models (LLMs) to multimodal small language models (MSLMs) is challenging due to dataset scarcity, reasoning degradation, and incorrect reinforcement learning outcomes.

Method: Infi-MMR-3B uses a three-phase curriculum: Foundational Reasoning Activation, Cross-Modal Reasoning Adaptation, and Multimodal Reasoning Enhancement, leveraging textual and multimodal datasets.

Result: Infi-MMR-3B achieves top scores in multimodal math reasoning (43.68% on MathVerse) and general reasoning (67.2% on MathVista).

Conclusion: The framework successfully enhances MSLMs' reasoning abilities, addressing key challenges in multimodal contexts.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini). Resources are available at
https://huggingface.co/Reallm-Labs/Infi-MMR-3B.

</details>


### [472] [IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory](https://arxiv.org/pdf/2506.01048)
*Wei Song, Zhenya Huang, Cheng Cheng, Weibo Gao, Bihan Xu, GuanHao Zhao, Fei Wang, Runze Wu*

Main category: cs.AI

TL;DR: IRT-Router is a multi-LLM routing framework that optimizes the trade-off between performance and cost by routing queries to the most suitable LLM using Item Response Theory.


<details>
  <summary>Details</summary>
Motivation: The need to balance performance and cost when selecting LLMs for user queries, as powerful models are costly while smaller ones are less capable.

Method: IRT-Router models the relationship between LLM capabilities and query attributes using Item Response Theory, with an online query warm-up technique for better generalization.

Result: Outperforms baselines in effectiveness and interpretability, with strong performance in cold-start scenarios.

Conclusion: IRT-Router is reliable and practical for real-world applications, offering superior routing and interpretability.

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
a wide range of natural language tasks. However, selecting the optimal LLM to
respond to a user query often necessitates a delicate balance between
performance and cost. While powerful models deliver better results, they come
at a high cost, whereas smaller models are more cost-effective but less
capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing
framework that efficiently routes user queries to the most suitable LLM.
Inspired by Item Response Theory (IRT), a psychological measurement
methodology, IRT-Router explicitly models the relationship between LLM
capabilities and user query attributes. This not only enables accurate
prediction of response performance but also provides interpretable insights,
such as LLM abilities and query difficulty. Additionally, we design an online
query warm-up technique based on semantic similarity, further enhancing the
online generalization capability of IRT-Router. Extensive experiments on 20
LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline
methods in terms of effectiveness and interpretability. Its superior
performance in cold-start scenarios further confirms the reliability and
practicality of IRT-Router in real-world applications. Code is available at
https://github.com/Mercidaiha/IRT-Router.

</details>


### [473] [MCP-Zero: Active Tool Discovery for Autonomous LLM Agents](https://arxiv.org/pdf/2506.01056)
*Xiang Fei, Xiawu Zheng, Hao Feng*

Main category: cs.AI

TL;DR: MCP-Zero is an active agent framework that enables LLMs to autonomously request tools on-demand, reducing context overhead and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents overload prompts with tool schemas, reducing autonomy and efficiency. MCP-Zero aims to restore tool discovery autonomy to LLMs.

Method: MCP-Zero uses Active Tool Request, Hierarchical Semantic Routing, and Iterative Capability Extension to enable autonomous tool discovery and minimal context usage.

Result: MCP-Zero achieves accurate tool selection from 3k candidates, 98% token reduction, and scalable multi-turn performance.

Conclusion: Active tool discovery is a key design pattern for scalable autonomous agent systems, as demonstrated by MCP-Zero.

Abstract: Current LLM agents inject thousands of tool schemas into prompts, creating
massive context overhead and reducing models to passive tool selectors rather
than autonomous agents. We introduce MCP-Zero, an active agent framework that
restores tool discovery autonomy to LLMs themselves. Instead of overwhelming
models with all available tools, MCP-Zero enables agents to actively identify
capability gaps, and request specific tools on-demand, transforming them from
large-scale retrievers into genuine autonomous agents. The framework operates
through three core mechanisms: (1) Active Tool Request, where models
autonomously generate structured requests specifying their exact tool
requirements; (2) Hierarchical Semantic Routing, a two-stage algorithm that
matches requests to relevant servers and tools through improved semantic
alignment; (3) Iterative Capability Extension, enabling agents to progressively
build cross-domain toolchains while maintaining minimal context footprint. We
also construct MCP-tools, a comprehensive dataset of 308 MCP servers and 2,797
tools from the official Model-Context-Protocol repository. Experiments
demonstrate that MCP-Zero preserves agent autonomy while achieving substantial
efficiency gains: (i) accurate tool selection from nearly 3k candidates across
248.1k tokens; (ii) 98\% reduction in token consumption on APIBank while
maintaining high accuracy; and (iii) consistent multi-turn performance that
scales with tool ecosystem growth. This work establishes active tool discovery
as a fundamental design pattern for scalable autonomous agent systems.

</details>


### [474] [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/pdf/2506.06832)
*Clment Hongler, Andrew Emil*

Main category: cs.AI

TL;DR: The paper introduces Cross-Entropy (Xent) Games, a framework for evaluating LLMs through tasks like summarization and anomaly detection, formulated as games based on LLM measures.


<details>
  <summary>Details</summary>
Motivation: To explore what it means for LLMs to 'know' probability measures on text and extend their evaluation beyond generative sampling.

Method: Formulates Xent Games as computational tasks involving cross-entropy scores and constraints, derived from game-theoretic axioms.

Result: Demonstrates the Xent Game space is rich and can benchmark LLM capabilities via finite families of games.

Conclusion: Proposes evolutionary dynamics to coherently explore Xent Games for measuring general LLM abilities.

Abstract: Large Language Models (LLMs) define probability measures on text. By
considering the implicit knowledge question of what it means for an LLM to know
such a measure and what it entails algorithmically, we are naturally led to
formulate a series of tasks that go beyond generative sampling, involving forms
of summarization, counterfactual thinking, anomaly detection, originality
search, reverse prompting, debating, creative solving, etc. These tasks can be
formulated as games based on LLM measures, which we call Cross-Entropy (Xent)
Games. Xent Games can be single-player or multi-player. They involve
cross-entropy scores and cross-entropy constraints, and can be expressed as
simple computational graphs and programs. We show the Xent Game space is large
enough to contain a wealth of interesting examples, while being constructible
from basic game-theoretic consistency axioms. We then discuss how the Xent Game
space can be used to measure the abilities of LLMs. This leads to the
construction of Xent Game measures: finite families of Xent Games that can be
used as capability benchmarks, built from a given scope, by extracting a
covering measure. To address the unbounded scope problem associated with the
challenge of measuring general abilities, we propose to explore the space of
Xent Games in a coherent fashion, using ideas inspired by evolutionary
dynamics.

</details>


### [475] [SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling](https://arxiv.org/pdf/2506.07636)
*Haoran Wang, Zhenyu Hou, Yao Wei, Jie Tang, Yuxiao Dong*

Main category: cs.AI

TL;DR: SWE-Dev, an LLM-based SWE agent, uses synthesized test cases and scaled training data to outperform open-source models in software engineering tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of high-quality training data and effective test cases for building SWE agents.

Method: Develops a pipeline for test case synthesis and scales agent trajectories for training data.

Result: Achieves success rates of 23.4% (7B) and 36.6% (32B) on SWE-bench-Verified, outperforming open-source models.

Conclusion: SWE-Dev demonstrates top performance among open SWE agents, with publicly available resources.

Abstract: Large language models (LLMs) have advanced rapidly from conversational
problem solving to addressing real-world tasks involving tool use, such as
software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex
and Cursor, have offered end-to-end automation of the software development
process. However, building effective SWE agents remains challenging due to the
lack of high-quality training data and effective test cases. To address this
issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we
develop a robust pipeline to synthesize test cases for patch evaluation.
Second, we scale up agent trajectories to construct the training data for
building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the
SWE-Dev models can achieve top performance among all open SWE agents.
Specifically, the success rates of the SWE-Dev 7B and 32B parameter models
reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source
models. All code, models, and datasets are publicly available at
https://github.com/THUDM/SWE-Dev.

</details>


### [476] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/pdf/2506.09655)
*Kaixuan Xu, Jiajun Chai, Sicheng Li, Yuqian Fu, Yuanheng Zhu, Dongbin Zhao*

Main category: cs.AI

TL;DR: DipLLM, a fine-tuned LLM-based agent, simplifies Diplomacy's complex action assignments and outperforms traditional methods with minimal data.


<details>
  <summary>Details</summary>
Motivation: Diplomacy's complexity and computational demands of traditional methods motivate the use of LLMs for efficient and effective AI gameplay.

Method: DipLLM uses an autoregressive factorization framework to break down multi-unit actions into unit-level decisions, fine-tuned with minimal data.

Result: DipLLM surpasses the state-of-the-art Cicero model's performance using only 1.5% of its required data.

Conclusion: Fine-tuned LLMs like DipLLM show promise for complex strategic decision-making in multiplayer games.

Abstract: Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [477] [MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval](https://arxiv.org/pdf/2506.12364)
*Mingjun Xu, Jinhan Dong, Jue Hou, Zehui Wang, Sihang Li, Zhifeng Gao, Renxin Zhong, Hengxing Cai*

Main category: cs.AI

TL;DR: MM-R5 is a multimodal reasoning-enhanced reranker using reinforcement learning, achieving state-of-the-art performance in document retrieval.


<details>
  <summary>Details</summary>
Motivation: Current multimodal reranking methods lack explicit reasoning and effectiveness, needing improvement in training strategies.

Method: Two-stage training: supervised fine-tuning (SFT) for reasoning chains and reinforcement learning (RL) with task-specific rewards.

Result: State-of-the-art performance on MMDocIR benchmark, improving recall@1 by over 4% compared to retrieval-only methods.

Conclusion: MM-R5 validates the effectiveness of reasoning-enhanced training for multimodal reranking.

Abstract: Multimodal document retrieval systems enable information access across text,
images, and layouts, benefiting various domains like document-based question
answering, report analysis, and interactive content summarization. Rerankers
improve retrieval precision by reordering retrieved candidates. However,
current multimodal reranking methods remain underexplored, with significant
room for improvement in both training strategies and overall effectiveness.
Moreover, the lack of explicit reasoning makes it difficult to analyze and
optimize these methods further. In this paper, We propose MM-R5, a MultiModal
Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval,
aiming to provide a more effective and reliable solution for multimodal
reranking tasks. MM-R5 is trained in two stages: supervised fine-tuning (SFT)
and reinforcement learning (RL). In the SFT stage, we focus on improving
instruction-following and guiding the model to generate complete and
high-quality reasoning chains. To support this, we introduce a novel data
construction strategy that produces rich, high-quality reasoning data. In the
RL stage, we design a task-specific reward framework, including a reranking
reward tailored for multimodal candidates and a composite template-based reward
to further refine reasoning quality. We conduct extensive experiments on
MMDocIR, a challenging public benchmark spanning multiple domains. MM-R5
achieves state-of-the-art performance on most metrics and delivers comparable
results to much larger models on the remaining ones. Moreover, compared to the
best retrieval-only method, MM-R5 improves recall@1 by over 4%. These results
validate the effectiveness of our reasoning-enhanced training pipeline. Our
code is available at https://github.com/i2vec/MM-R5 .

</details>


### [478] [Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection](https://arxiv.org/pdf/2506.13793)
*Zongxian Yang, Jiayu Qian, Zegao Peng, Haoyu Zhang, Zhi-An Huang*

Main category: cs.AI

TL;DR: Med-REFL improves medical reasoning by enhancing intermediate reflection steps, achieving up to 4.11% gains on MedQA-USMLE and boosting 7B/8B models by 4.13%.


<details>
  <summary>Details</summary>
Motivation: Current large reasoning models struggle in medical domains due to poor intermediate reflection quality, critical for high-stakes scenarios.

Method: Med-REFL uses a tree-of-thought approach to decompose questions, evaluates reasoning steps, and constructs preference optimization data automatically.

Result: Achieves consistent improvements on MedQA-USMLE (4.11% avg. gain) and boosts 7B/8B models by 4.13%. Shows strong generalization and robustness.

Conclusion: Prioritizing reflection quality enhances accuracy and trustworthiness in medical AI, with Med-REFL demonstrating significant performance gains.

Abstract: Large reasoning models have recently made significant strides in mathematical
and code reasoning, yet their success has not transferred smoothly to the
medical domain. While multiple factors contribute to this disparity, a critical
issue is the inadequate focus on the quality of intermediate reflection steps,
which is particularly crucial in high-stakes medical scenarios. To address this
challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical
\underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via
self-corrected \underline{\textbf{F}}ine-grained
ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought
approach to decompose medical questions into fine-grained reasoning paths,
quantitatively evaluating each step and its subsequent reflections. These
assessments enable automatic construction of direct preference optimization
data, reducing reliance on expensive expert annotations while guiding models to
identify and correct reasoning errors. Experimental results on the MedQA-USMLE
benchmark demonstrate Med-REFL achieves consistent improvements, with average
gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance
of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong
generalization capabilities and robustness across several challenging medical
question-answering datasets. Our work illustrates that prioritizing reflection
quality leads to more accurate and trustworthy reasoning in medical AI
applications. Checkpoints, code, and data can be found in
https://github.com/TianYin123/Med-REFL.

</details>


### [479] [QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents](https://arxiv.org/pdf/2506.14568)
*Eliott Thomas, Mickael Coustaty, Aurelie Joseph, Gaspar Deloin, Elodie Carel, Vincent Poulain D'Andecy, Jean-Marc Ogier*

Main category: cs.AI

TL;DR: QUEST is a quality-aware semi-supervised framework for table extraction in business documents, improving accuracy and reducing empty predictions by leveraging structural and contextual quality assessments.


<details>
  <summary>Details</summary>
Motivation: Automating table extraction from business documents is challenging due to sparse annotations and unreliable multi-stage pipelines. Existing SSL methods rely on poor confidence metrics.

Method: QUEST introduces a quality assessment model predicting F1 scores, guiding pseudo-label selection in SSL. It uses diversity measures (DPP, Vendi score, IntDiv) to mitigate bias.

Result: On proprietary and DocILE datasets, QUEST improves F1 scores (64% to 74% and 42% to 50%) and reduces empty predictions (12% to 6.5% and 27% to 22%).

Conclusion: QUEST's interpretable quality assessments and robustness to annotation scarcity make it effective for business documents, ensuring structural consistency and data completeness.

Abstract: Automating table extraction (TE) from business documents is critical for
industrial workflows but remains challenging due to sparse annotations and
error-prone multi-stage pipelines. While semi-supervised learning (SSL) can
leverage unlabeled data, existing methods rely on confidence scores that poorly
reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised
Table extraction framework designed for business documents. QUEST introduces a
novel quality assessment model that evaluates structural and contextual
features of extracted tables, trained to predict F1 scores instead of relying
on confidence metrics. This quality-aware approach guides pseudo-label
selection during iterative SSL training, while diversity measures (DPP, Vendi
score, IntDiv) mitigate confirmation bias. Experiments on a proprietary
business dataset (1000 annotated + 10000 unannotated documents) show QUEST
improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to
6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents),
QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by
19% (from 27% to 22%). The framework's interpretable quality assessments and
robustness to annotation scarcity make it particularly suited for business
documents, where structural consistency and data completeness are paramount.

</details>


### [480] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/pdf/2506.15741)
*He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, Tianhao Peng, Xin Gui, Xiaowan Li, Yuhui Liu, Yuchen Eleanor Jiang, Jun Wang, Changwang Zhang, Xiangru Tang, Ge Zhang, Jian Yang, Minghao Liu, Xitong Gao, Jiaheng Liu, Wangchunshu Zhou*

Main category: cs.AI

TL;DR: The paper highlights the lack of standardization in Agentic AI research, introduces a robust evaluation protocol, and presents OAgents, a modular framework achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current agent research lacks standardization and rigor, hindering fair comparisons and progress measurement.

Method: A systematic empirical study on GAIA benchmark and BrowseComp to evaluate design choices in agent components.

Result: Identified key impactful components, introduced a stable evaluation protocol, and developed OAgents, a top-performing modular framework.

Conclusion: OAgents advances Agentic AI research by providing a reproducible, modular foundation for future work.

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [481] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/pdf/2506.15787)
*Lukas Helff, Ahmad Omar, Felix Friedrich, Wolfgang Stammer, Antonia Wst, Tim Woydt, Rupert Mitchell, Patrick Schramowski, Kristian Kersting*

Main category: cs.AI

TL;DR: SLR is an automated framework for evaluating and training LLMs via scalable logical reasoning, creating tasks with controlled difficulty and synthesizing rules, validation programs, and prompts. It includes SLR-Bench, a benchmark with 19k+ prompts across 20 levels, showing LLMs struggle with logical inference. Logic-tuning via SLR improves accuracy efficiently.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate and enhance LLMs' logical reasoning capabilities without human annotation, ensuring scalability and novelty.

Method: SLR synthesizes tasks with latent rules, validation programs, and prompts. It creates SLR-Bench with 19k+ prompts across 20 curriculum levels of increasing complexity.

Result: LLMs often fail at correct logical inference despite producing valid rules. Logic-tuning via SLR doubles Llama-3-8B's accuracy, matching Gemini-Flash-Thinking at lower computational cost.

Conclusion: SLR provides a scalable, automated solution for advancing LLMs' reasoning, demonstrating significant improvements in accuracy and efficiency.

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [482] [Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models](https://arxiv.org/pdf/2506.17114)
*Dadi Guo, Jiayu Liu, Zhiyuan Fan, Zhitao He, Haoran Li, Yumeng Wang, Yi R. Fung*

Main category: cs.AI

TL;DR: The paper introduces the RFMDataset to evaluate large reasoning models on mathematical proofs, revealing significant shortcomings like low correctness rates, diverse reasoning failures, and hallucinations.


<details>
  <summary>Details</summary>
Motivation: To expose hidden reasoning failures in large models, masked by high accuracy on numerical evaluations and potential benchmark leaks, using rigorous mathematical proofs.

Method: Created the RFMDataset with 200 diverse proof problems and analyzed models' performance, identifying 10 fine-grained error types.

Result: Models struggled profoundly with proofs (some <20% correct), showed diverse reasoning failures, and lacked correctness guarantees in single-step reasoning.

Conclusion: Current models' self-reflection is insufficient; formalized, fine-grained logical training is needed to address their limitations.

Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable
mathematical problem-solving abilities. However, the high reported accuracy of
these advanced models on popular datasets, reliance on purely numerical
evaluation and potential benchmark leakage, often masks their true reasoning
shortcomings. To address this, we propose leveraging the inherent rigor and
methodological complexity of mathematical proofs as a diagnostic tool to expose
these hidden failures. Specifically, we introduce the RFMDataset (Reveal
Failure Modes), a collection of 200 diverse mathematical proof problems, and
thoroughly evaluate advanced models' performance on it. Our in-depth analysis
of their failures uncovers 10 fine-grained error types, which shows fundamental
limitations in current large reasoning models: 1) large reasoning models
grapple profoundly with mathematical proofs, with some generating entirely
correct proofs for less than 20% of problems and failing even on basic ones; 2)
models exhibit a diverse spectrum of reasoning failures, prominently
demonstrating the lack of guarantees for the correctness and rigor of
single-step reasoning; and 3) models show hallucination and incompleteness
during the reasoning process. Our findings reveal that models' self-reflection
is insufficient to resolve the current logical dilemmas, necessitating
formalized and fine-grained logical training.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [483] [Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM](https://arxiv.org/pdf/2506.17351)
*Mostafa Shahin, Beena Ahmed, Julien Epps*

Main category: cs.SD

TL;DR: A zero-shot speech-based method for cognitive impairment detection using AudioLLM achieves performance comparable to supervised methods and shows cross-language generalizability.


<details>
  <summary>Details</summary>
Motivation: Early detection of cognitive impairment is crucial, and speech offers a non-invasive biomarker. Traditional methods require manual annotation and lack generalizability.

Method: Proposes a zero-shot approach using Qwen2-Audio AudioLLM with prompt-based instructions to classify speech samples for cognitive impairment.

Result: The method performs comparably to supervised models and demonstrates consistency across languages, tasks, and datasets.

Conclusion: The zero-shot AudioLLM approach is effective for CI detection, offering generalizability and eliminating the need for manual annotation.

Abstract: Cognitive impairment (CI) is of growing public health concern, and early
detection is vital for effective intervention. Speech has gained attention as a
non-invasive and easily collectible biomarker for assessing cognitive decline.
Traditional CI detection methods typically rely on supervised models trained on
acoustic and linguistic features extracted from speech, which often require
manual annotation and may not generalise well across datasets and languages. In
this work, we propose the first zero-shot speech-based CI detection method
using the Qwen2- Audio AudioLLM, a model capable of processing both audio and
text inputs. By designing prompt-based instructions, we guide the model in
classifying speech samples as indicative of normal cognition or cognitive
impairment. We evaluate our approach on two datasets: one in English and
another multilingual, spanning different cognitive assessment tasks. Our
results show that the zero-shot AudioLLM approach achieves performance
comparable to supervised methods and exhibits promising generalizability and
consistency across languages, tasks, and datasets.

</details>


### [484] [Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation](https://arxiv.org/pdf/2506.17409)
*Quoc Thinh Vo, Joe Woods, Priontu Chowdhury, David K. Han*

Main category: cs.SD

TL;DR: A multi-branch network with CNNs and Conformers improves underwater sound source localization by addressing environmental challenges, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Localizing acoustic sources in the ocean is difficult due to noise, irregular geometries, and varying acoustic properties.

Method: Uses CNNs for spatial features and Conformers for temporal dependencies, with log-mel spectrograms and GCC-PHAT as inputs. Includes an AGC layer for adaptive amplitude adjustment.

Result: Outperforms SOTA methods in real-world underwater signal arrays, demonstrating strong generalization.

Conclusion: The proposed method sets new benchmarks for accurate underwater sound localization.

Abstract: Localizing acoustic sound sources in the ocean is a challenging task due to
the complex and dynamic nature of the environment. Factors such as high
background noise, irregular underwater geometries, and varying acoustic
properties make accurate localization difficult. To address these obstacles, we
propose a multi-branch network architecture designed to accurately predict the
distance between a moving acoustic source and a receiver, tested on real-world
underwater signal arrays. The network leverages Convolutional Neural Networks
(CNNs) for robust spatial feature extraction and integrates Conformers with
self-attention mechanism to effectively capture temporal dependencies. Log-mel
spectrogram and generalized cross-correlation with phase transform (GCC-PHAT)
features are employed as input representations. To further enhance the model
performance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively
adjusts the amplitude of input features, ensuring consistent energy levels
across varying ranges, signal strengths, and noise conditions. We assess the
model's generalization capability by training it in one domain and testing it
in a different domain, using only a limited amount of data from the test domain
for fine-tuning. Our proposed method outperforms state-of-the-art (SOTA)
approaches in similar settings, establishing new benchmarks for underwater
sound localization.

</details>


### [485] [From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training](https://arxiv.org/pdf/2506.17497)
*Mingyang Yao, Ke Chen*

Main category: cs.SD

TL;DR: A two-stage training approach enhances composer-style music generation by pre-training on a broad corpus and fine-tuning with a lightweight adapter, outperforming baselines in style accuracy and musicality.


<details>
  <summary>Details</summary>
Motivation: Data scarcity for composer-style music generation limits modeling of styles and fundamental elements. Leveraging general music knowledge from a broad corpus can improve mastery of specific styles.

Method: Pre-train a REMI-based model on a diverse music corpus, then fine-tune it on a small dataset of four composers using a lightweight adapter for style conditioning.

Result: The method outperforms baselines, achieving precise composer-style modeling and better musical aesthetics.

Conclusion: General pre-training and style-specific fine-tuning effectively enhance composer-style music generation, with insights into how the model refines stylistic understanding.

Abstract: Despite progress in controllable symbolic music generation, data scarcity
remains a challenge for certain control modalities. Composer-style music
generation is a prime example, as only a few pieces per composer are available,
limiting the modeling of both styles and fundamental music elements (e.g.,
melody, chord, rhythm). In this paper, we investigate how general music
knowledge learned from a broad corpus can enhance the mastery of specific
composer styles, with a focus on piano piece generation. Our approach follows a
two-stage training paradigm. First, we pre-train a REMI-based music generation
model on a large corpus of pop, folk, and classical music. Then, we fine-tune
it on a small, human-verified dataset from four renowned composers, namely
Bach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to
condition the model on style indicators. To evaluate the effectiveness of our
approach, we conduct both objective and subjective evaluations on style
accuracy and musicality. Experimental results demonstrate that our method
outperforms ablations and baselines, achieving more precise composer-style
modeling and better musical aesthetics. Additionally, we provide observations
on how the model builds music concepts from the generality pre-training and
refines its stylistic understanding through the mastery fine-tuning.

</details>


### [486] [Algebraic Structures in Microtonal Music](https://arxiv.org/pdf/2506.17778)
*Veronica Flynn, Carmen Rovi*

Main category: cs.SD

TL;DR: The paper explores group theory structures in 24-tone microtonal music, extending prior work by Crans, Fiore, and Satyendra.


<details>
  <summary>Details</summary>
Motivation: To mathematically analyze musical and harmonic structures in 24-tone microtonal systems using group theory.

Method: Assign numbers to 24 equal-tone divisions of an octave and interpret musical actions mathematically.

Result: Demonstrates how group-theoretic structures apply to 24-tone microtonal music.

Conclusion: Extends the understanding of group theory in music theory, particularly in microtonal systems.

Abstract: We will discuss how certain group theory structures are found in music
theory. Western music splits the octave into 12 equal tones called half-steps.
We can take this division further and split the octave into 24 equal tones by
splitting each half-step in two, called a quarter-step. By assigning each of
these 24 notes a number, we can discuss musical actions mathematically. In this
paper, we analyze 24-tone microtonal music and explore how musical and harmonic
structures in this system can be interpreted in terms of group-theoretic
structures. This work extends the study by Crans, Fiore, and Satyendra.

</details>


### [487] [SLAP: Siamese Language-Audio Pretraining Without Negative Samples for Music Understanding](https://arxiv.org/pdf/2506.17815)
*Julien Guinot, Alain Riou, Elio Quinton, Gyrgy Fazekas*

Main category: cs.SD

TL;DR: SLAP is a novel multimodal pretraining framework for audio-text learning that avoids negative samples, reduces the modality gap, and scales efficiently on single GPUs.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of joint embedding spaces, such as large memory requirements and the modality gap, in music understanding and generation.

Method: Adapts the BYOL paradigm for multimodal audio-text training, eliminating the need for negative samples and promoting scalability.

Result: Outperforms CLAP in text-music retrieval and zero-shot classification, achieves competitive performance in MIR tasks, and shows robustness to batch size variations.

Conclusion: SLAP offers an efficient and scalable solution for multimodal audio-text learning with improved performance and reduced modality gap.

Abstract: Joint embedding spaces have significantly advanced music understanding and
generation by linking text and audio through multimodal contrastive learning.
However, these approaches face large memory requirement limitations due to
relying on large batch sizes to effectively utilize negative samples. Further,
multimodal joint embedding spaces suffer from a modality gap wherein embeddings
from different modalities lie in different manifolds of the embedding space. To
address these challenges, we propose Siamese Language-Audio Pretraining (SLAP),
a novel multimodal pretraining framework that allows learning powerful
representations without negative samples. SLAP adapts the Bootstrap Your Own
Latent (BYOL) paradigm for multimodal audio-text training, promoting
scalability in training multimodal embedding spaces.
  We illustrate the ability of our model to learn meaningful relationships
between music and text -- specifically, we show that SLAP outperforms CLAP on
tasks such as text-music retrieval and zero-shot classification. We also
observe competitive downstream performance on several MIR tasks, including with
larger or supervised models (genre and instrument classification,
auto-tagging). Additionally, our approach has attractive properties, such as a
quantifiably reduced modality gap and improved robustness to batch size
variations on retrieval performance. Finally, its novel formulation unlocks
large-scale training on a single GPU through gradient accumulation.

</details>


### [488] [CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning](https://arxiv.org/pdf/2506.17818)
*Angelos-Nikolaos Kanatas, Charilaos Papaioannou, Alexandros Potamianos*

Main category: cs.SD

TL;DR: CultureMERT-95M improves cross-cultural music representation learning with a two-stage pre-training strategy, achieving better performance on non-Western music tasks without compromising Western benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current music foundation models in handling diverse musical traditions.

Method: Two-stage continual pre-training with learning rate re-warming and re-decaying, trained on a 650-hour multi-cultural dataset (Greek, Turkish, Indian). Also explores task arithmetic for model merging.

Result: 4.9% average improvement in ROC-AUC and AP for non-Western auto-tagging tasks; no regression on Western benchmarks. Multi-cultural model outperforms single-culture adaptations.

Conclusion: CultureMERT-95M enhances cross-cultural music understanding and is publicly released to support further research.

Abstract: Recent advances in music foundation models have improved audio representation
learning, yet their effectiveness across diverse musical traditions remains
limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation
model developed to enhance cross-cultural music representation learning and
understanding. To achieve this, we propose a two-stage continual pre-training
strategy that integrates learning rate re-warming and re-decaying, enabling
stable adaptation even with limited computational resources. Training on a
650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music
traditions, results in an average improvement of 4.9% in ROC-AUC and AP across
diverse non-Western music auto-tagging tasks, surpassing prior
state-of-the-art, with minimal forgetting on Western-centric benchmarks. We
further investigate task arithmetic, an alternative approach to multi-cultural
adaptation that merges single-culture adapted models in the weight space. Task
arithmetic performs on par with our multi-culturally trained model on
non-Western auto-tagging tasks and shows no regression on Western datasets.
Cross-cultural evaluation reveals that single-culture models transfer with
varying effectiveness across musical traditions, whereas the multi-culturally
adapted model achieves the best overall performance. To support research on
world music representation learning, we publicly release CultureMERT-95M and
CultureMERT-TA-95M, fostering the development of more culturally aware music
foundation models.

</details>


### [489] [GD-Retriever: Controllable Generative Text-Music Retrieval with Diffusion Models](https://arxiv.org/pdf/2506.17886)
*Julien Guinot, Elio Quinton, Gyrgy Fazekas*

Main category: cs.SD

TL;DR: GDR introduces a diffusion-based framework for controllable text-music retrieval, improving performance and enabling interactive user control.


<details>
  <summary>Details</summary>
Motivation: Addressing the inflexibility and ambiguity in text-music retrieval by leveraging diffusion models for better controllability.

Method: Uses diffusion models to generate queries in a retrieval-optimized latent space, incorporating tools like negative prompting and DDIM inversion.

Result: Outperforms contrastive teacher models and supports audio-only retrieval with non-jointly trained encoders.

Conclusion: GDR enhances retrieval performance and user control, offering a new direction for interactive retrieval systems.

Abstract: Multimodal contrastive models have achieved strong performance in text-audio
retrieval and zero-shot settings, but improving joint embedding spaces remains
an active research area. Less attention has been given to making these systems
controllable and interactive for users. In text-music retrieval, the ambiguity
of freeform language creates a many-to-many mapping, often resulting in
inflexible or unsatisfying results.
  We introduce Generative Diffusion Retriever (GDR), a novel framework that
leverages diffusion models to generate queries in a retrieval-optimized latent
space. This enables controllability through generative tools such as negative
prompting and denoising diffusion implicit models (DDIM) inversion, opening a
new direction in retrieval control. GDR improves retrieval performance over
contrastive teacher models and supports retrieval in audio-only latent spaces
using non-jointly trained encoders. Finally, we demonstrate that GDR enables
effective post-hoc manipulation of retrieval behavior, enhancing interactive
control for text-music retrieval tasks.

</details>


### [490] [Human Voice is Unique](https://arxiv.org/pdf/2506.18182)
*Rita Singh, Bhiksha Raj*

Main category: cs.SD

TL;DR: The paper introduces a framework to objectively calculate the uniqueness of human voice as a biometric identifier, showing varying probabilities of voice duplication in a population of 10 billion.


<details>
  <summary>Details</summary>
Motivation: To establish voice as a true biometric identifier by proving its uniqueness, addressing gaps in current applications like speaker identification and human profiling.

Method: Uses statistical analysis of measurable, causally related voice signal characteristics that are independent of each other.

Result: Probabilities of voice duplication range from 1 in a few thousand to 1 in a septillion or less, depending on variable quantization.

Conclusion: The findings impact voice processing applications by validating voice as a unique biometric identifier and guiding design choices.

Abstract: Voice is increasingly being used as a biometric entity in many applications.
These range from speaker identification and verification systems to human
profiling technologies that attempt to estimate myriad aspects of the speaker's
persona from their voice. However, for an entity to be a true biometric
identifier, it must be unique. This paper establishes a first framework for
calculating the uniqueness of human voice objectively. The approach in this
paper is based on statistical considerations that take into account a set of
measurable characteristics of the voice signal that bear a causal relationship
to the vocal production process, but are not inter-dependent or derivable from
each other. Depending on how we quantize these variables, we show that the
chances of two people having the same voice in a world populated by 10 billion
people range from one in a few thousand, to one in a septillion or less. The
paper also discusses the implications of these calculations on the choices made
in voice processing applications.

</details>


### [491] [JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking Styles](https://arxiv.org/pdf/2506.18296)
*Yuto Kondo, Hirokazu Kameoka, Kou Tanaka, Takuhiro Kaneko*

Main category: cs.SD

TL;DR: The paper introduces the Japanese Idol Speech Corpus (JIS) to enhance research in speech generation AI, focusing on TTS and VC, with unique speaker attributes for rigorous evaluations.


<details>
  <summary>Details</summary>
Motivation: To advance research in speech generation AI by providing a specialized corpus (JIS) for evaluating speaker similarity and exploring listener preferences.

Method: Constructed JIS with recordings of young female live idols, identified by stage names, and described its ethical use and cultural context.

Result: JIS enables rigorous evaluations of TTS and VC systems and supports research on generating voices tailored to listener preferences.

Conclusion: JIS will be freely distributed for non-commercial research, fostering advancements in speech generation AI.

Abstract: We construct Japanese Idol Speech Corpus (JIS) to advance research in speech
generation AI, including text-to-speech synthesis (TTS) and voice conversion
(VC). JIS will facilitate more rigorous evaluations of speaker similarity in
TTS and VC systems since all speakers in JIS belong to a highly specific
category: "young female live idols" in Japan, and each speaker is identified by
a stage name, enabling researchers to recruit listeners familiar with these
idols for listening experiments. With its unique speaker attributes, JIS will
foster compelling research, including generating voices tailored to listener
preferences-an area not yet widely studied. JIS will be distributed free of
charge to promote research in speech generation AI, with usage restricted to
non-commercial, basic research. We describe the construction of JIS, provide an
overview of Japanese live idol culture to support effective and ethical use of
JIS, and offer a basic analysis to guide application of JIS.

</details>


### [492] [Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation through Quantized Distribution Fitting](https://arxiv.org/pdf/2506.18307)
*Yuto Kondo, Hirokazu Kameoka, Kou Tanaka, Takuhiro Kaneko*

Main category: cs.SD

TL;DR: The paper proposes a novel score aggregation method to improve MOS prediction in speech quality assessment by modeling annotators' latent continuous scores.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of MOS prediction models by addressing limitations of conventional discrete rating annotations.

Method: A score aggregation method models annotators' latent continuous scores, quantizes the distribution, and uses its peak as a new representative value.

Result: Substituting MOS with the proposed value improves prediction performance in experiments.

Conclusion: The method effectively enhances MOS prediction by better capturing annotators' latent scoring behavior.

Abstract: Speech quality assessment (SQA) aims to evaluate the quality of speech
samples without relying on time-consuming listener questionnaires. Recent
efforts have focused on training neural-based SQA models to predict the mean
opinion score (MOS) of speech samples produced by text-to-speech or voice
conversion systems. This paper targets the enhancement of MOS prediction
models' performance. We propose a novel score aggregation method to address the
limitations of conventional annotations for MOS, which typically involve
ratings on a scale from 1 to 5. Our method is based on the hypothesis that
annotators internally consider continuous scores and then choose the nearest
discrete rating. By modeling this process, we approximate the generative
distribution of ratings by quantizing the latent continuous distribution. We
then use the peak of this latent distribution, estimated through the loss
between the quantized distribution and annotated ratings, as a new
representative value instead of MOS. Experimental results demonstrate that
substituting MOSNet's predicted target with this proposed value improves
prediction performance.

</details>


### [493] [Large-Scale Training Data Attribution for Music Generative Models via Unlearning](https://arxiv.org/pdf/2506.18312)
*Woosung Choi, Junghyun Koo, Kin Wai Cheuk, Joan Serr, Marco A. Martnez-Ramrez, Yukara Ikemiya, Naoki Murata, Yuhta Takida, Wei-Hsiang Liao, Yuki Mitsufuji*

Main category: cs.SD

TL;DR: The paper investigates unlearning methods for Training Data Attribution (TDA) in music generative models, aiming to credit original artists and address AI ethics. It tests the approach on a text-to-music diffusion model and compares it with similarity-based methods.


<details>
  <summary>Details</summary>
Motivation: To ensure proper recognition of original artists in AI-generated music and address ethical and copyright concerns by enabling white-box attribution.

Method: Applies unlearning-based attribution to a text-to-music diffusion model, conducts hyperparameter grid searches, and compares results with similarity-based approaches.

Result: Unlearning-based TDA is feasible for music generative models, offering consistent attribution and ethical accountability.

Conclusion: The study successfully adapts unlearning for TDA in music models, promoting ethical AI systems for music creation.

Abstract: This paper explores the use of unlearning methods for training data
attribution (TDA) in music generative models trained on large-scale datasets.
TDA aims to identify which specific training data points contributed to the
generation of a particular output from a specific model. This is crucial in the
context of AI-generated music, where proper recognition and credit for original
artists are generally overlooked. By enabling white-box attribution, our work
supports a fairer system for acknowledging artistic contributions and addresses
pressing concerns related to AI ethics and copyright. We apply unlearning-based
attribution to a text-to-music diffusion model trained on a large-scale dataset
and investigate its feasibility and behavior in this setting. To validate the
method, we perform a grid search over different hyperparameter configurations
and quantitatively evaluate the consistency of the unlearning approach. We then
compare attribution patterns from unlearning with those from a similarity-based
approach. Our findings suggest that unlearning-based approaches can be
effectively adapted to music generative models, introducing large-scale TDA to
this domain and paving the way for more ethical and accountable AI systems for
music creation.

</details>


### [494] [Selecting N-lowest scores for training MOS prediction models](https://arxiv.org/pdf/2506.18326)
*Yuto Kondo, Hirokazu Kameoka, Kou Tanaka, Takuhiro Kaneko*

Main category: cs.SD

TL;DR: The paper proposes N_low-MOS, a more reliable metric for speech quality assessment by focusing on low-quality segments, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Humans may prioritize low-quality speech segments when rating, leading to variance in scores. The study aims to find a more intrinsic measure of speech quality.

Method: Analyzes VCC2018 and BVCC datasets, introduces N_low-MOS (mean of N-lowest scores), and tests it with MOSNet.

Result: N_low-MOS improves LCC and SRCC metrics, indicating better speech quality assessment.

Conclusion: N_low-MOS is a more reliable metric for subjective speech quality and enhances model comparison.

Abstract: The automatic speech quality assessment (SQA) has been extensively studied to
predict the speech quality without time-consuming questionnaires. Recently,
neural-based SQA models have been actively developed for speech samples
produced by text-to-speech or voice conversion, with a primary focus on
training mean opinion score (MOS) prediction models. The quality of each speech
sample may not be consistent across the entire duration, and it remains unclear
which segments of the speech receive the primary focus from humans when
assigning subjective evaluation for MOS calculation. We hypothesize that when
humans rate speech, they tend to assign more weight to low-quality speech
segments, and the variance in ratings for each sample is mainly due to
accidental assignment of higher scores when overlooking the poor quality speech
segments. Motivated by the hypothesis, we analyze the VCC2018 and BVCC
datasets. Based on the hypothesis, we propose the more reliable representative
value N_low-MOS, the mean of the $N$-lowest opinion scores. Our experiments
show that LCC and SRCC improve compared to regular MOS when employing N_low-MOS
to MOSNet training. This result suggests that N_low-MOS is a more intrinsic
representative value of subjective speech quality and makes MOSNet a better
comparator of VC models.

</details>


### [495] [AI-Generated Song Detection via Lyrics Transcripts](https://arxiv.org/pdf/2506.18488)
*Markus Frohmann, Elena V. Epure, Gabriel Meseguer-Brocal, Markus Schedl, Romain Hennequin*

Main category: cs.SD

TL;DR: Proposes using ASR models to transcribe songs for detecting AI-generated music, showing strong performance across languages and genres, and robustness against audio perturbations.


<details>
  <summary>Details</summary>
Motivation: Address the gap in detecting AI-generated music when perfect lyrics are unavailable, improving real-life applicability.

Method: Uses general ASR models (e.g., Whisper large-v2) and LLM2Vec embeddings to transcribe and detect AI-generated lyrics.

Result: Strong detection performance across languages and genres, outperforming audio-based methods under perturbations and with unseen generators.

Conclusion: The proposed method is robust and practical for real-world detection of AI-generated music.

Abstract: The recent rise in capabilities of AI-based music generation tools has
created an upheaval in the music industry, necessitating the creation of
accurate methods to detect such AI-generated content. This can be done using
audio-based detectors; however, it has been shown that they struggle to
generalize to unseen generators or when the audio is perturbed. Furthermore,
recent work used accurate and cleanly formatted lyrics sourced from a lyrics
provider database to detect AI-generated music. However, in practice, such
perfect lyrics are not available (only the audio is); this leaves a substantial
gap in applicability in real-life use cases. In this work, we instead propose
solving this gap by transcribing songs using general automatic speech
recognition (ASR) models. We do this using several detectors. The results on
diverse, multi-genre, and multi-lingual lyrics show generally strong detection
performance across languages and genres, particularly for our best-performing
model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that
our method is more robust than state-of-the-art audio-based ones when the audio
is perturbed in different ways and when evaluated on different music
generators. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [496] [Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts](https://arxiv.org/pdf/2506.18510)
*Duygu Altinok*

Main category: cs.SD

TL;DR: The paper proposes using LLMs to transcribe disfluencies in spoken language as explicit tokens with timestamps, showing robustness even with imperfect textual inputs.


<details>
  <summary>Details</summary>
Motivation: Improving automatic speech and language processing systems by accurately detecting disfluencies for more inclusive technologies.

Method: Integrates acoustic representations with textual inputs (clean, time-aligned, or ASR outputs) and leverages LLMs to generate disfluency-annotated transcripts.

Result: LLMs can effectively handle imperfect textual inputs with timestamp cues to produce accurate disfluency-annotated transcripts.

Conclusion: LLMs are robust for disfluency transcription, even with imperfect hints, enhancing speech and language technologies.

Abstract: Accurate detection of disfluencies in spoken language is crucial for
enhancing the performance of automatic speech and language processing systems,
as well as fostering the development of more inclusive speech and language
technologies. Leveraging the growing trend of large language models (LLMs) as
versatile learners capable of processing both lexical and non-lexical inputs
(e.g., audio and video), we propose a novel approach to transcribing
disfluencies as explicit tokens with timestamps, enabling the generation of
fully annotated disfluency-rich transcripts. Our method integrates acoustic
representations extracted from an audio encoder with textual inputs of varying
quality: clean transcriptions without disfluencies, time-aligned transcriptions
from aligners, or outputs from phoneme-based ASR models -- all of which may
contain imperfections. Importantly, our experiments demonstrate that textual
inputs do not need to be flawless. As long as they include timestamp-related
cues, LLMs can effectively smooth the input and produce fully
disfluency-annotated transcripts, underscoring their robustness in handling
imperfect hints.

</details>


### [497] [TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography](https://arxiv.org/pdf/2506.18671)
*Yuqin Dai, Wanlu Zhu, Ronghui Li, Xiu Li, Zhenyu Zhang, Jun Li, Jian Yang*

Main category: cs.SD

TL;DR: TCDiff++ is a music-driven framework for harmonious group dance generation, addressing multi-dancer collisions, foot sliding, and abrupt swapping in long sequences.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multi-dancer collisions, foot sliding, and abrupt swapping in group dance generation, limiting quality and coherence.

Method: Uses dancer positioning embedding, distance-consistency loss, swap mode embedding, Footwork Adaptor, long group diffusion sampling, and Sequence Decoder.

Result: TCDiff++ achieves state-of-the-art performance, especially in long-duration scenarios, ensuring high-quality group dance.

Conclusion: TCDiff++ effectively addresses key challenges in group dance generation, delivering superior results.

Abstract: Music-driven dance generation has garnered significant attention due to its
wide range of industrial applications, particularly in the creation of group
choreography. During the group dance generation process, however, most existing
methods still face three primary issues: multi-dancer collisions, single-dancer
foot sliding and abrupt swapping in the generation of long group dance. In this
paper, we propose TCDiff++, a music-driven end-to-end framework designed to
generate harmonious group dance. Specifically, to mitigate multi-dancer
collisions, we utilize a dancer positioning embedding to better maintain the
relative positioning among dancers. Additionally, we incorporate a
distance-consistency loss to ensure that inter-dancer distances remain within
plausible ranges. To address the issue of single-dancer foot sliding, we
introduce a swap mode embedding to indicate dancer swapping patterns and design
a Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For
long group dance generation, we present a long group diffusion sampling
strategy that reduces abrupt position shifts by injecting positional
information into the noisy input. Furthermore, we integrate a Sequence Decoder
layer to enhance the model's ability to selectively process long sequences.
Extensive experiments demonstrate that our TCDiff++ achieves state-of-the-art
performance, particularly in long-duration scenarios, ensuring high-quality and
coherent group dance generation.

</details>


### [498] [Evaluating Multichannel Speech Enhancement Algorithms at the Phoneme Scale Across Genders](https://arxiv.org/pdf/2506.18691)
*Nasser-Eddine Monir, Paul Magron, Romain Serizel*

Main category: cs.SD

TL;DR: The paper investigates how gender and phonetic content affect multichannel speech enhancement algorithms, revealing phoneme-level performance disparities favoring female speech.


<details>
  <summary>Details</summary>
Motivation: To address overlooked disparities in acoustic characteristics across phoneme categories and genders in speech enhancement evaluations.

Method: Analyzed phoneme- and gender-specific spectral features and evaluated algorithms at phoneme and utterance levels.

Result: Algorithms performed better for female speech, especially in plosives, fricatives, and vowels, with fewer artifacts and improved perceptual metrics.

Conclusion: Phoneme-level analysis is crucial for evaluating speech enhancement, as gender and phonetic content significantly impact algorithm performance.

Abstract: Multichannel speech enhancement algorithms are essential for improving the
intelligibility of speech signals in noisy environments. These algorithms are
usually evaluated at the utterance level, but this approach overlooks the
disparities in acoustic characteristics that are observed in different phoneme
categories and between male and female speakers. In this paper, we investigate
the impact of gender and phonetic content on speech enhancement algorithms. We
motivate this approach by outlining phoneme- and gender-specific spectral
features. Our experiments reveal that while utterance-level differences between
genders are minimal, significant variations emerge at the phoneme level.
Results show that the tested algorithms better reduce interference with fewer
artifacts on female speech, particularly in plosives, fricatives, and vowels.
Additionally, they demonstrate greater performance for female speech in terms
of perceptual and speech recognition metrics.

</details>


### [499] [Frequency-Weighted Training Losses for Phoneme-Level DNN-based Speech Enhancement](https://arxiv.org/pdf/2506.18714)
*Nasser-Eddine Monir, Paul Magron, Romain Serizel*

Main category: cs.SD

TL;DR: Proposed perceptually-informed SDR loss variants for speech enhancement, improving perceptual metrics and phoneme intelligibility.


<details>
  <summary>Details</summary>
Motivation: Conventional SDR loss may not preserve fine-grained spectral cues for phoneme intelligibility.

Method: Developed frequency-weighted SDR loss variants (fixed and adaptive) and trained FaSNet model.

Result: Marginal SDR improvement but substantial perceptual metric gains; better consonant reconstruction.

Conclusion: Perceptual weighting enhances speech enhancement by preserving critical acoustic cues.

Abstract: Recent advances in deep learning have significantly improved multichannel
speech enhancement algorithms, yet conventional training loss functions such as
the scale-invariant signal-to-distortion ratio (SDR) may fail to preserve
fine-grained spectral cues essential for phoneme intelligibility. In this work,
we propose perceptually-informed variants of the SDR loss, formulated in the
time-frequency domain and modulated by frequency-dependent weighting schemes.
These weights are designed to emphasize time-frequency regions where speech is
prominent or where the interfering noise is particularly strong. We investigate
both fixed and adaptive strategies, including ANSI band-importance weights,
spectral magnitude-based weighting, and dynamic weighting based on the relative
amount of speech and noise. We train the FaSNet multichannel speech enhancement
model using these various losses. Experimental results show that while standard
metrics such as the SDR are only marginally improved, their perceptual
frequency-weighted counterparts exhibit a more substantial improvement.
Besides, spectral and phoneme-level analysis indicates better consonant
reconstruction, which points to a better preservation of certain acoustic cues.

</details>


### [500] [MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners](https://arxiv.org/pdf/2506.18729)
*Fang-Duo Tsai, Shih-Lun Wu, Weijaw Lee, Sheng-Ping Yang, Bo-Rui Chen, Hao-Chung Cheng, Yi-Hsuan Yang*

Main category: cs.SD

TL;DR: MuseControlLite is a lightweight method for fine-tuning text-to-music models, improving control accuracy with fewer parameters by using positional embeddings.


<details>
  <summary>Details</summary>
Motivation: Enhance precision in text-to-music generation by addressing the lack of positional embeddings for time-varying conditions.

Method: Adds rotary positional embeddings to decoupled cross-attention layers, reducing trainable parameters.

Result: Increases control accuracy from 56.6% to 61.1% with 6.75x fewer parameters than SOTA methods.

Conclusion: MuseControlLite offers superior controllability at lower computational cost, outperforming existing models like MusicGen-Large and Stable Audio Open ControlNet.

Abstract: We propose MuseControlLite, a lightweight mechanism designed to fine-tune
text-to-music generation models for precise conditioning using various
time-varying musical attributes and reference audio signals. The key finding is
that positional embeddings, which have been seldom used by text-to-music
generation models in the conditioner for text conditions, are critical when the
condition of interest is a function of time. Using melody control as an
example, our experiments show that simply adding rotary positional embeddings
to the decoupled cross-attention layers increases control accuracy from 56.6%
to 61.1%, while requiring 6.75 times fewer trainable parameters than
state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion
Transformer model of Stable Audio Open. We evaluate various forms of musical
attribute control, audio inpainting, and audio outpainting, demonstrating
improved controllability over MusicGen-Large and Stable Audio Open ControlNet
at a significantly lower fine-tuning cost, with only 85M trainble parameters.
Source code, model checkpoints, and demo examples are available at: https:
//MuseControlLite.github.io/web/.

</details>


### [501] [USAD: Universal Speech and Audio Representation via Distillation](https://arxiv.org/pdf/2506.18843)
*Heng-Jui Chang, Saurabhchand Bhati, James Glass, Alexander H. Liu*

Main category: cs.SD

TL;DR: USAD unifies speech and non-speech audio learning via distillation, achieving near state-of-the-art results across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Current SSL models are domain-specific; USAD aims to integrate diverse audio types into a single model.

Method: Uses layer-to-layer distillation from domain-specific SSL models to train a unified student model.

Result: Competitive performance on speech, audio tagging, and sound classification tasks, nearing state-of-the-art.

Conclusion: USAD successfully unifies audio representation learning, offering versatility and strong performance.

Abstract: Self-supervised learning (SSL) has revolutionized audio representations, yet
models often remain domain-specific, focusing on either speech or non-speech
tasks. In this work, we present Universal Speech and Audio Distillation (USAD),
a unified approach to audio representation learning that integrates diverse
audio types - speech, sound, and music - into a single model. USAD employs
efficient layer-to-layer distillation from domain-specific SSL models to train
a student on a comprehensive audio dataset. USAD offers competitive performance
across various benchmarks and datasets, including frame and instance-level
speech processing tasks, audio tagging, and sound classification, achieving
near state-of-the-art results with a single encoder on SUPERB and HEAR
benchmarks.

</details>


### [502] [Vocoder-Free Non-Parallel Conversion of Whispered Speech With Masked Cycle-Consistent Generative Adversarial Networks](https://arxiv.org/pdf/2306.06514)
*Dominik Wagner, Ilja Baumann, Tobias Bocklet*

Main category: cs.SD

TL;DR: A unified model for voice conversion (VC) combines feature conversion and waveform synthesis, eliminating the need for a separate vocoder, and improves performance in whispered and conventional VC.


<details>
  <summary>Details</summary>
Motivation: To simplify the VC pipeline by integrating feature conversion and waveform synthesis into a single model, avoiding the inefficiency of separate models.

Method: Uses cycle-consistent training and a self-supervised auxiliary task to unify conversion and synthesis in one model.

Result: Achieves up to 6.7% relative improvement in whispered VC and stable improvements (0.5%-2.4%) in conventional VC.

Conclusion: The unified approach enhances efficiency and quality in VC tasks, demonstrating the benefits of integrating conversion and synthesis.

Abstract: Cycle-consistent generative adversarial networks have been widely used in
non-parallel voice conversion (VC). Their ability to learn mappings between
source and target features without relying on parallel training data eliminates
the need for temporal alignments. However, most methods decouple the conversion
of acoustic features from synthesizing the audio signal by using separate
models for conversion and waveform synthesis. This work unifies conversion and
synthesis into a single model, thereby eliminating the need for a separate
vocoder. By leveraging cycle-consistent training and a self-supervised
auxiliary training task, our model is able to efficiently generate converted
high-quality raw audio waveforms. Subjective listening tests showed that our
unified approach achieved improvements of up to 6.7% relative to the baseline
in whispered VC. Mean opinion score predictions also yielded stable results in
conventional VC (between 0.5% and 2.4% relative improvement).

</details>


### [503] [Information and motor constraints shape melodic diversity across cultures](https://arxiv.org/pdf/2408.12635)
*John M McBride, Nahie Kim, Yuri Nishikawa, Mekhmed Saadakeev, Marcus T Pearce, Tsvi Tlusty*

Main category: cs.SD

TL;DR: The paper explores how information constraints shape common features of melodies across societies, comparing Folk and Art music, and proposes a model predicting scale degree distribution.


<details>
  <summary>Details</summary>
Motivation: To understand why melodies from different societies share common features like repetition and scale size, despite vast potential for variation, and to investigate the role of information constraints in shaping these features.

Method: Analyzed 62 corpora of Folk melodies and 39 corpora of Art music, measuring information rate determinants and comparing complexity and evolution over time. Developed a parameter-free model to predict scale degree distribution.

Result: Found trade-offs constraining information rate in Folk melodies, while Art music showed increased complexity over time. The model successfully predicted empirical scale degree distribution.

Conclusion: Information constraints during cultural transmission limit scale size and melody complexity, suggesting a fundamental constraint on the cultural evolution of melody.

Abstract: The number of possible melodies is unfathomably large, yet despite this
virtually unlimited potential for melodic variation, melodies from different
societies can be surprisingly similar. The motor constraint hypothesis accounts
for certain similarities, such as scalar motion and contour shape, but not for
other major common features, such as repetition, song length, and scale size.
Here we investigate the role of information constraints in shaping these
hallmarks of melodies. We measure determinants of information rate in 62
corpora of Folk melodies spanning several continents, finding multiple
trade-offs that all act to constrain the information rate across societies. By
contrast, 39 corpora of Art music from Europe (including Turkey) show longer,
more complex melodies, and increased complexity over time, suggesting different
cultural-evolutionary selection pressures in Art and Folk music, possibly due
to the use of written versus oral transmission. Our parameter-free model
predicts the empirical scale degree distribution using information constraints
on scalar motion, melody length, and, most importantly, information rate. These
results provide strong evidence that information constraints during cultural
transmission of music limit the number of notes in a scale, and suggests that a
tendency for intermediate melodic complexity reflects a fundamental constraint
on the cultural evolution of melody.

</details>


### [504] [Hierarchical Control of Emotion Rendering in Speech Synthesis](https://arxiv.org/pdf/2412.12498)
*Sho Inoue, Kun Zhou, Shuai Wang, Haizhou Li*

Main category: cs.SD

TL;DR: Proposes a flow-matching emotional TTS framework with hierarchical emotion intensity modeling for fine-grained control at phoneme, word, and utterance levels.


<details>
  <summary>Details</summary>
Motivation: Quantitative control of multi-level emotion rendering in TTS is challenging.

Method: Uses a hierarchical emotion distribution extractor and explores acoustic features for emotion intensity modeling.

Result: Demonstrates effective speech quality, emotional expressiveness, and hierarchical emotion control.

Conclusion: The framework successfully enables fine-grained emotion control in TTS synthesis.

Abstract: Emotional text-to-speech synthesis (TTS) aims to generate realistic emotional
speech from input text. However, quantitatively controlling multi-level emotion
rendering remains challenging. In this paper, we propose a flow-matching based
emotional TTS framework with a novel approach for emotion intensity modeling to
facilitate fine-grained control over emotion rendering at the phoneme, word,
and utterance levels. We introduce a hierarchical emotion distribution (ED)
extractor that captures a quantifiable ED embedding across different speech
segment levels. Additionally, we explore various acoustic features and assess
their impact on emotion intensity modeling. During TTS training, the
hierarchical ED embedding effectively captures the variance in emotion
intensity from the reference audio and correlates it with linguistic and
speaker information. The TTS model not only generates emotional speech during
inference, but also quantitatively controls the emotion rendering over the
speech constituents. Both objective and subjective evaluations demonstrate the
effectiveness of our framework in terms of speech quality, emotional
expressiveness, and hierarchical emotion control.

</details>


### [505] [AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement](https://arxiv.org/pdf/2501.15417)
*Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu*

Main category: cs.SD

TL;DR: AnyEnhance is a unified generative model for voice enhancement, handling both speech and singing voices, supporting multiple tasks without fine-tuning, and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To create a versatile voice enhancement model capable of handling diverse tasks (denoising, dereverberation, etc.) for both speech and singing voices without task-specific adjustments.

Method: Uses a masked generative model with prompt-guidance for in-context learning and a self-critic mechanism for iterative refinement.

Result: Outperforms existing methods in objective metrics and subjective tests, demonstrating superior enhancement quality.

Conclusion: AnyEnhance is a robust, flexible model for voice enhancement, excelling in diverse tasks and scenarios.

Abstract: We introduce AnyEnhance, a unified generative model for voice enhancement
that processes both speech and singing voices. Based on a masked generative
model, AnyEnhance is capable of handling both speech and singing voices,
supporting a wide range of enhancement tasks including denoising,
dereverberation, declipping, super-resolution, and target speaker extraction,
all simultaneously and without fine-tuning. AnyEnhance introduces a
prompt-guidance mechanism for in-context learning, which allows the model to
natively accept a reference speaker's timbre. In this way, it could boost
enhancement performance when a reference audio is available and enable the
target speaker extraction task without altering the underlying architecture.
Moreover, we also introduce a self-critic mechanism into the generative process
for masked generative models, yielding higher-quality outputs through iterative
self-assessment and refinement. Extensive experiments on various enhancement
tasks demonstrate AnyEnhance outperforms existing methods in terms of both
objective metrics and subjective listening tests. Demo audios are publicly
available at https://amphionspace.github.io/anyenhance/.

</details>


### [506] [Evaluation of the Pronunciation of Tajweed Rules Based on DNN as a Step Towards Interactive Recitation Learning](https://arxiv.org/pdf/2503.23470)
*Dim Shaiakhmetov, Gulnaz Gimaletdinova, Kadyrmamat Momunov, Selcuk Cankurt*

Main category: cs.SD

TL;DR: A deep learning model was developed to classify three Tajweed rules (Al Mad, Ghunnah, Ikhfaa) using the QDAT dataset, achieving high accuracy rates (95.35%, 99.34%, 97.01%).


<details>
  <summary>Details</summary>
Motivation: Traditional Tajweed teaching methods are limited by instructor availability and time constraints. Automatic evaluation can provide prompt feedback and support independent practice.

Method: Audio recordings from the QDAT dataset were transformed into mel-spectrograms. The EfficientNet-B0 architecture, enhanced with a Squeeze-and-Excitation mechanism, was used for classification.

Result: The model achieved high accuracy rates for each rule (95.35%, 99.34%, 97.01%) and showed robustness without overfitting.

Conclusion: The approach is efficient and supports the development of interactive educational systems for Tajweed study.

Abstract: Proper recitation of the Quran, adhering to the rules of Tajweed, is crucial
for preventing mistakes during recitation and requires significant effort to
master. Traditional methods of teaching these rules are limited by the
availability of qualified instructors and time constraints. Automatic
evaluation of recitation can address these challenges by providing prompt
feedback and supporting independent practice. This study focuses on developing
a deep learning model to classify three Tajweed rules - separate stretching (Al
Mad), tight noon (Ghunnah), and hide (Ikhfaa) - using the publicly available
QDAT dataset, which contains over 1,500 audio recordings. The input data
consisted of audio recordings from this dataset, transformed into normalized
mel-spectrograms. For classification, the EfficientNet-B0 architecture was
used, enhanced with a Squeeze-and-Excitation attention mechanism. The developed
model achieved accuracy rates of 95.35%, 99.34%, and 97.01% for the respective
rules. An analysis of the learning curves confirmed the model's robustness and
absence of overfitting. The proposed approach demonstrates high efficiency and
paves the way for developing interactive educational systems for Tajweed study.

</details>


### [507] [The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan](https://arxiv.org/pdf/2505.09382)
*Zhengyan Sheng, Jinghao He, Liping Chen, Kong Aik Lee, Zhen-Hua Ling*

Main category: cs.SD

TL;DR: The VtaD 2025 challenge aims to explain voice timbre attributes using sensory descriptors by comparing two voices, culminating in a proposal at NCMMSC2025.


<details>
  <summary>Details</summary>
Motivation: To verbalize human impressions of voice timbre and distinguish it using sensory descriptors like bright, coarse, soft, and magnetic.

Method: Comparative analysis of voice timbre intensity within specific descriptor dimensions.

Result: A framework for detecting and explaining voice timbre attributes.

Conclusion: The challenge advances understanding of voice timbre through comparative sensory descriptors, with results presented at NCMMSC2025.

Abstract: Voice timbre refers to the unique quality or character of a person's voice
that distinguishes it from others as perceived by human hearing. The Voice
Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the
voice timbre attribute in a comparative manner. In this challenge, the human
impression of voice timbre is verbalized with a set of sensory descriptors,
including bright, coarse, soft, magnetic, and so on. The timbre is explained
from the comparison between two voices in their intensity within a specific
descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a
special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,
China.

</details>


### [508] [Introducing voice timbre attribute detection](https://arxiv.org/pdf/2505.09661)
*Jinghao He, Zhengyan Sheng, Liping Chen, Kong Aik Lee, Zhen-Hua Ling*

Main category: cs.SD

TL;DR: The paper introduces voice timbre attribute detection (vTAD), a task to describe voice timbre using sensory attributes. It compares speech utterance intensity and proposes a framework using speaker embeddings, tested on the VCTK-RVA dataset with ECAPA-TDNN and FACodec encoders.


<details>
  <summary>Details</summary>
Motivation: To explain voice timbre in speech signals and develop a method for detecting timbre attributes using human perception descriptors.

Method: A framework based on speaker embeddings from speech utterances, tested on the VCTK-RVA dataset using ECAPA-TDNN and FACodec speaker encoders.

Result: ECAPA-TDNN performed better with seen speakers, while FACodec excelled with unseen speakers, showing better generalization.

Conclusion: The study demonstrates the effectiveness of speaker embeddings for vTAD, with FACodec offering superior generalization for unseen speakers.

Abstract: This paper focuses on explaining the timbre conveyed by speech signals and
introduces a task termed voice timbre attribute detection (vTAD). In this task,
voice timbre is explained with a set of sensory attributes describing its human
perception. A pair of speech utterances is processed, and their intensity is
compared in a designated timbre descriptor. Moreover, a framework is proposed,
which is built upon the speaker embeddings extracted from the speech
utterances. The investigation is conducted on the VCTK-RVA dataset.
Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders
demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the
seen scenario, where the testing speakers were included in the training set; 2)
the FACodec speaker encoder was superior in the unseen scenario, where the
testing speakers were not part of the training, indicating enhanced
generalization capability. The VCTK-RVA dataset and open-source code are
available on the website https://github.com/vTAD2025-Challenge/vTAD.

</details>


### [509] [Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the MISP-Meeting Challenge](https://arxiv.org/pdf/2505.24446)
*Longjie Luo, Shenghui Lu, Lin Li, Qingyang Hong*

Main category: cs.SD

TL;DR: The paper introduces a system for the MISP-Meeting Challenge Track 2, addressing challenges like noise and overlapping speech with G-SpatialNet, TLS framework, and ASR enhancements, achieving significant CER improvements.


<details>
  <summary>Details</summary>
Motivation: The dataset's challengesbackground noise, reverberation, overlapping speech, and diverse topicsmotivated the development of robust solutions for speech enhancement and ASR in meetings.

Method: The authors (a) designed G-SpatialNet for SE, (b) proposed TLS for pseudo-label generation, and (c) fine-tuned ASR models with augmentation and multimodal data.

Result: The system achieved CERs of 5.44% (Dev) and 9.52% (Eval), with 64.8% and 52.6% relative improvements over the baseline, securing second place.

Conclusion: The proposed methods effectively address meeting scenario challenges, demonstrating significant performance gains in speech recognition.

Abstract: This paper presents our system for the MISP-Meeting Challenge Track 2. The
primary difficulty lies in the dataset, which contains strong background noise,
reverberation, overlapping speech, and diverse meeting topics. To address these
issues, we (a) designed G-SpatialNet, a speech enhancement (SE) model to
improve Guided Source Separation (GSS) signals; (b) proposed TLS, a framework
comprising time alignment, level alignment, and signal-to-noise ratio
filtering, to generate signal-level pseudo labels for real-recorded far-field
audio data, thereby facilitating SE models' training; and (c) explored
fine-tuning strategies, data augmentation, and multimodal information to
enhance the performance of pre-trained Automatic Speech Recognition (ASR)
models in meeting scenarios. Finally, our system achieved character error rates
(CERs) of 5.44% and 9.52% on the Dev and Eval sets, respectively, with relative
improvements of 64.8% and 52.6% over the baseline, securing second place.

</details>


### [510] [SuPseudo: A Pseudo-supervised Learning Method for Neural Speech Enhancement in Far-field Speech Recognition](https://arxiv.org/pdf/2505.24450)
*Longjie Luo, Lin Li, Qingyang Hong*

Main category: cs.SD

TL;DR: The paper introduces SuPseudo, a pseudo-supervised learning method, and FARNET, an SE model, to improve speech enhancement for real-recorded far-field data by using direct sound estimation (DSE) as pseudo-labels.


<details>
  <summary>Details</summary>
Motivation: Existing SE models trained on simulated data perform poorly in real-world conditions, limiting their use in far-field speech recognition.

Method: Proposes DSE for estimating oracle direct sound and SuPseudo for pseudo-supervised learning. FARNET is designed to leverage SuPseudo.

Result: Experiments on MISP2023 show SuPseudo significantly outperforms previous state-of-the-art methods.

Conclusion: The approach enhances SE model generalization for real-recorded data, validated by superior performance.

Abstract: Due to the lack of target speech annotations in real-recorded far-field
conversational datasets, speech enhancement (SE) models are typically trained
on simulated data. However, the trained models often perform poorly in
real-world conditions, hindering their application in far-field speech
recognition. To address the issue, we (a) propose direct sound estimation (DSE)
to estimate the oracle direct sound of real-recorded data for SE; and (b)
present a novel pseudo-supervised learning method, SuPseudo, which leverages
DSE-estimates as pseudo-labels and enables SE models to directly learn from and
adapt to real-recorded data, thereby improving their generalization capability.
Furthermore, an SE model called FARNET is designed to fully utilize SuPseudo.
Experiments on the MISP2023 corpus demonstrate the effectiveness of SuPseudo,
and our system significantly outperforms the previous state-of-the-art. A demo
of our method can be found at https://EeLLJ.github.io/SuPseudo/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [511] [MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving](https://arxiv.org/pdf/2506.17230)
*Yichen Luo, Jia Wang, Dapeng Lan, Yu Liu, Zhibo Pang*

Main category: cs.LG

TL;DR: The paper introduces MMET, a transformer-based framework for solving PDEs efficiently, addressing multi-input and multi-scale challenges with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs with machine learning is challenging due to limited generalization and high computational costs.

Method: MMET decouples mesh and query points into sequences, uses GCE for embedding, and employs Hilbert curve reserialization to reduce input length.

Result: MMET outperforms SOTA methods in accuracy and efficiency across diverse benchmarks.

Conclusion: MMET is a scalable solution for real-time PDE solving, with potential for future domain-specific pre-trained models.

Abstract: Partial Differential Equations (PDEs) are fundamental for modeling physical
systems, yet solving them in a generic and efficient manner using machine
learning-based approaches remains challenging due to limited multi-input and
multi-scale generalization capabilities, as well as high computational costs.
This paper proposes the Multi-input and Multi-scale Efficient Transformer
(MMET), a novel framework designed to address the above challenges. MMET
decouples mesh and query points as two sequences and feeds them into the
encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)
layer to embed input variables or functions with varying dimensions, enabling
effective solutions for multi-scale and multi-input problems. Additionally, a
Hilbert curve-based reserialization and patch embedding mechanism decrease the
input length. This significantly reduces the computational cost when dealing
with large-scale geometric models. These innovations enable efficient
representations and support multi-scale resolution queries for large-scale and
multi-input PDE problems. Experimental evaluations on diverse benchmarks
spanning different physical fields demonstrate that MMET outperforms SOTA
methods in both accuracy and computational efficiency. This work highlights the
potential of MMET as a robust and scalable solution for real-time PDE solving
in engineering and physics-based applications, paving the way for future
explorations into pre-trained large-scale models in specific domains. This work
is open-sourced at https://github.com/YichenLuo-0/MMET.

</details>


### [512] [PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation](https://arxiv.org/pdf/2506.17232)
*Zelin Zang, Fei Wang, Liangyu Li, Jinlin Wu, Chunshui Zhao, Zhen Lei, Baigui Sun*

Main category: cs.LG

TL;DR: PCaM addresses foreground object mismatch in UDA by progressively filtering background info and enhancing attention consistency, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Foreground object mismatch in UDA weakens attention consistency, hindering domain alignment.

Method: Progressive Focus Cross-Attention Mechanism (PCaM) filters background info and uses attentional guidance loss.

Result: PCaM improves adaptation performance and achieves state-of-the-art results on multiple datasets.

Conclusion: PCaM effectively enhances attention-guided foreground fusion for domain adaptation.

Abstract: Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent UDA methods based
on Vision Transformers (ViTs) have achieved strong performance through
attention-based feature alignment. However, we identify a key limitation:
foreground object mismatch, where the discrepancy in foreground object size and
spatial distribution across domains weakens attention consistency and hampers
effective domain alignment. To address this issue, we propose the Progressive
Focus Cross-Attention Mechanism (PCaM), which progressively filters out
background information during cross-attention, allowing the model to focus on
and fuse discriminative foreground semantics across domains. We further
introduce an attentional guidance loss that explicitly directs attention toward
task-relevant regions, enhancing cross-domain attention consistency. PCaM is
lightweight, architecture-agnostic, and easy to integrate into existing
ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,
VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly
improves adaptation performance and achieves new state-of-the-art results,
validating the effectiveness of attention-guided foreground fusion for domain
adaptation.

</details>


### [513] [Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning](https://arxiv.org/pdf/2506.17342)
*Zijian Long, Haopeng Wang, Haiwei Dong, Abdulmotaleb El Saddik*

Main category: cs.LG

TL;DR: ASMS (Adaptive Social Metaverse Streaming) uses F-MAPPO to optimize streaming in the social metaverse, improving user experience by 14% while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Privacy and streaming quality are major challenges in the social metaverse due to continuous data collection and real-time demands.

Method: Proposes ASMS, leveraging Federated Multi-Agent Proximal Policy Optimization (F-MAPPO) to dynamically adjust streaming bit rates using federated learning and deep reinforcement learning.

Result: ASMS improves user experience by at least 14% compared to existing methods, ensuring seamless streaming and privacy.

Conclusion: ASMS enhances the social metaverse by balancing streaming quality and privacy, even in dynamic networks.

Abstract: The social metaverse is a growing digital ecosystem that blends virtual and
physical worlds. It allows users to interact socially, work, shop, and enjoy
entertainment. However, privacy remains a major challenge, as immersive
interactions require continuous collection of biometric and behavioral data. At
the same time, ensuring high-quality, low-latency streaming is difficult due to
the demands of real-time interaction, immersive rendering, and bandwidth
optimization. To address these issues, we propose ASMS (Adaptive Social
Metaverse Streaming), a novel streaming system based on Federated Multi-Agent
Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which
integrates federated learning (FL) and deep reinforcement learning (DRL) to
dynamically adjust streaming bit rates while preserving user privacy.
Experimental results show that ASMS improves user experience by at least 14%
compared to existing streaming methods across various network conditions.
Therefore, ASMS enhances the social metaverse experience by providing seamless
and immersive streaming, even in dynamic and resource-constrained networks,
while ensuring that sensitive user data remains on local devices.

</details>


### [514] [Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey](https://arxiv.org/pdf/2506.17234)
*Payam Zohari, Mostafa Haghir Chehreghani*

Main category: cs.LG

TL;DR: A review of GNN-based methods for multi-omics cancer data integration, highlighting trends like hybrid models, attention mechanisms, and patient-specific graphs.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of cancer biology by integrating multi-omics data using GNNs for better representation and analysis.

Method: Systematic review of GNN-based approaches in multi-omics cancer research, classified by omics layers, GNN structures, and tasks like subtype classification and biomarker discovery.

Result: Identified trends: hybrid models, interpretability, attention mechanisms, contrastive learning, and emerging use of patient-specific graphs and knowledge-driven priors.

Conclusion: The review provides a resource for designing GNN-based pipelines, summarizing current practices, limitations, and future directions in integrative cancer analysis.

Abstract: The task of data integration for multi-omics data has emerged as a powerful
strategy to unravel the complex biological underpinnings of cancer. Recent
advancements in graph neural networks (GNNs) offer an effective framework to
model heterogeneous and structured omics data, enabling precise representation
of molecular interactions and regulatory networks. This systematic review
explores several recent studies that leverage GNN-based architectures in
multi-omics cancer research. We classify the approaches based on their targeted
omics layers, graph neural network structures, and biological tasks such as
subtype classification, prognosis prediction, and biomarker discovery. The
analysis reveals a growing trend toward hybrid and interpretable models,
alongside increasing adoption of attention mechanisms and contrastive learning.
Furthermore, we highlight the use of patient-specific graphs and
knowledge-driven priors as emerging directions. This survey serves as a
comprehensive resource for researchers aiming to design effective GNN-based
pipelines for integrative cancer analysis, offering insights into current
practices, limitations, and potential future directions.

</details>


### [515] [Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training](https://arxiv.org/pdf/2506.17499)
*Xuanyu Zhuang, Geoffroy Peeters, Gal Richard*

Main category: cs.LG

TL;DR: The paper introduces episode-specific fine-tuning methods for metric-based few-shot classification to better utilize support samples and avoid overfitting, validated on diverse audio datasets.


<details>
  <summary>Details</summary>
Motivation: Existing metric-based models underutilize labeled support samples during inference, missing opportunities to adapt the metric space to the current episode.

Method: Proposes Rotational Division Fine-Tuning (RDFT) and variants (IDFT, ADFT) to construct pseudo support-query pairs for fine-tuning, combined with optimization-based meta-learning to prevent overfitting.

Result: The approach improves performance across metric-based models, especially attention-based ones, and generalizes well on ESC-50, Speech Commands V2, and Medley-solos-DB datasets.

Conclusion: Episode-specific fine-tuning and meta-learning enhance metric-based models' adaptability to limited support samples while mitigating overfitting, demonstrating broad applicability in audio domains.

Abstract: In few-shot classification tasks (so-called episodes), a small set of labeled
support samples is provided during inference to aid the classification of
unlabeled query samples. Metric-based models typically operate by computing
similarities between query and support embeddings within a learned metric
space, followed by nearest-neighbor classification. However, these labeled
support samples are often underutilized--they are only used for similarity
comparison, despite their potential to fine-tune and adapt the metric space
itself to the classes in the current episode. To address this, we propose a
series of simple yet effective episode-specific, during-inference fine-tuning
methods for metric-based models, including Rotational Division Fine-Tuning
(RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and
Augmented Division Fine-Tuning (ADFT). These methods construct pseudo
support-query pairs from the given support set to enable fine-tuning even for
non-parametric models. Nevertheless, the severely limited amount of data in
each task poses a substantial risk of overfitting when applying such
fine-tuning strategies. To mitigate this, we further propose to train the
metric-based model within an optimization-based meta-learning framework. With
the combined efforts of episode-specific fine-tuning and optimization-based
meta-training, metric-based models are equipped with the ability to rapidly
adapt to the limited support samples during inference while avoiding
overfitting. We validate our approach on three audio datasets from diverse
domains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken
keywords), and Medley-solos-DB (musical instrument). Experimental results
demonstrate that our approach consistently improves performance for all
evaluated metric-based models (especially for attention-based models) and
generalizes well across different audio domains.

</details>


### [516] [Training a Scientific Reasoning Model for Chemistry](https://arxiv.org/pdf/2506.17238)
*Siddharth M. Narayanan, James D. Braza, Ryan-Rhys Griffiths, Albert Bou, Geemi Wellawatte, Mayk Caldas Ramos, Ludovico Mitchener, Samuel G. Rodriques, Andrew D. White*

Main category: cs.LG

TL;DR: A 24B parameter reasoning model, ether0, is post-trained for chemistry without domain pretraining, outperforming specialized models and human experts while being data-efficient.


<details>
  <summary>Details</summary>
Motivation: To explore if reasoning models generalize beyond math/programming/logic into chemistry, and to create a data-efficient solution for scientific domains.

Method: Post-trained a 24B LLM (Mistral-Small-24B) using reinforcement learning on 640,730 chemistry problems across 375 tasks.

Result: ether0 outperforms general-purpose chemistry models, frontier models, and human experts in molecular design tasks.

Conclusion: Reasoning models can be specialized for scientific domains efficiently, suggesting broader applicability.

Abstract: Reasoning models are large language models that emit a long chain-of-thought
before answering, providing both higher accuracy and explicit reasoning for
their response. A major question has been whether language model reasoning
generalizes beyond mathematics, programming, and logic, where most previous
work has focused. We demonstrate that reasoning models can be post-trained for
chemistry without additional domain pretraining, and require substantially less
data compared to contemporary domain-specific models. We report ether0, a 24B
parameter LLM (based on Mistral-Small-24B) that can reason in natural language
and respond with chemical structures. This reasoning model was trained with
reinforcement learning on 640,730 experimentally-grounded chemistry problems
across 375 tasks ranging from synthesizability, to blood-brain barrier
permeability, to human receptor activity, to scent. Our model exceeds
general-purpose chemistry models, frontier models, and human experts on
molecular design tasks. It is also more data efficient relative to specialized
models. We anticipate that this method can be applied to train data-efficient
language models specialized for tasks across a wide variety of scientific
domains.

</details>


### [517] [Recursive Learning-Based Virtual Buffering for Analytical Global Placement](https://arxiv.org/pdf/2506.17247)
*Andrew B. Kahng, Yiting Liu, Zhiang Wang*

Main category: cs.LG

TL;DR: MLBuf-RePlAce is a learning-driven buffering-aware global placement framework that improves timing closure without degrading power, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of computational expense in traditional buffering and lack of ERC violation consideration in machine learning-based approaches.

Method: Uses a recursive learning-based generative buffering approach to predict buffer types and locations, integrated into the OpenROAD infrastructure.

Result: Achieves significant improvements in total negative slack (TNS) (up to 56%) and slight power improvements (0.2%) in both open-source and commercial flows.

Conclusion: MLBuf-RePlAce effectively bridges the gap in buffering-aware placement, offering a practical solution for modern physical synthesis flows.

Abstract: Due to the skewed scaling of interconnect versus cell delay in modern
technology nodes, placement with buffer porosity (i.e., cell density) awareness
is essential for timing closure in physical synthesis flows. However, existing
approaches face two key challenges: (i) traditional van Ginneken-Lillis-style
buffering approaches are computationally expensive during global placement; and
(ii) machine learning-based approaches, such as BufFormer, lack a thorough
consideration of Electrical Rule Check (ERC) violations and fail to "close the
loop" back into the physical design flow. In this work, we propose
MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware
analytical global placement framework, built on top of the OpenROAD
infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based
generative buffering approach to predict buffer types and locations, addressing
ERC violations during global placement. We compare MLBuf-RePlAce against the
default virtual buffering-based timing-driven global placer in OpenROAD, using
open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts
repositories. Without degradation of post-route power, MLBuf-RePlAce achieves
(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)
within the open-source OpenROAD flow. When evaluated by completion in a
commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of
(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.

</details>


### [518] [Efficient Quantification of Multimodal Interaction at Sample Level](https://arxiv.org/pdf/2506.17248)
*Zequn Yang, Hongfa Wang, Di Hu*

Main category: cs.LG

TL;DR: The paper introduces the LSMI estimator, a method for quantifying sample-level interactions (redundancy, uniqueness, synergy) in multimodal data using pointwise information theory.


<details>
  <summary>Details</summary>
Motivation: Understanding interactions in multimodal data is crucial but challenging; the paper aims to provide a precise and efficient quantification method.

Method: Develops a redundancy estimation framework and a general interaction estimation method using efficient entropy estimation for continuous distributions.

Result: Validated on synthetic and real-world datasets, LSMI shows precision and efficiency, revealing fine-grained dynamics in multimodal data.

Conclusion: LSMI enables practical applications like redundancy-informed sample partitioning and interaction-aware model ensembling, with code publicly available.

Abstract: Interactions between modalities -- redundancy, uniqueness, and synergy --
collectively determine the composition of multimodal information. Understanding
these interactions is crucial for analyzing information dynamics in multimodal
systems, yet their accurate sample-level quantification presents significant
theoretical and computational challenges. To address this, we introduce the
Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously
grounded in pointwise information theory. We first develop a redundancy
estimation framework, employing an appropriate pointwise information measure to
quantify this most decomposable and measurable interaction. Building upon this,
we propose a general interaction estimation method that employs efficient
entropy estimation, specifically tailored for sample-wise estimation in
continuous distributions. Extensive experiments on synthetic and real-world
datasets validate LSMI's precision and efficiency. Crucially, our sample-wise
approach reveals fine-grained sample- and category-level dynamics within
multimodal data, enabling practical applications such as redundancy-informed
sample partitioning, targeted knowledge distillation, and interaction-aware
model ensembling. The code is available at
https://github.com/GeWu-Lab/LSMI_Estimator.

</details>


### [519] [Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection](https://arxiv.org/pdf/2506.17249)
*Jianing He, Qi Zhang, Duoqian Miao, Yi Kun, Shufeng Hao, Hongyun Zhang, Zhihua Wei*

Main category: cs.LG

TL;DR: The paper introduces a new early exiting method for PLMs using a Certainty-Aware Probability (CAP) score, combining logits and a novel NSP score to improve prediction certainty and avoid premature exits. It achieves a 2.19x speed-up on GLUE with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing early exiting methods overestimate prediction certainty by ignoring class-irrelevant feature information, leading to incorrect early exits.

Method: Proposes a CAP score integrating logits and an NSP score (measuring class-irrelevant information) for better certainty estimation.

Result: Achieves 2.19x average speed-up on GLUE with negligible performance drop, outperforming SOTA by 28%.

Conclusion: The CAP-based method offers a better trade-off between efficiency and accuracy, advancing early exiting in PLMs.

Abstract: Early exiting has demonstrated great potential in accelerating the inference
of pre-trained language models (PLMs) by enabling easy samples to exit at
shallow layers, eliminating the need for executing deeper layers. However,
existing early exiting methods primarily rely on class-relevant logits to
formulate their exiting signals for estimating prediction certainty, neglecting
the detrimental influence of class-irrelevant information in the features on
prediction certainty. This leads to an overestimation of prediction certainty,
causing premature exiting of samples with incorrect early predictions. To
remedy this, we define an NSP score to estimate prediction certainty by
considering the proportion of class-irrelevant information in the features. On
this basis, we propose a novel early exiting method based on the
Certainty-Aware Probability (CAP) score, which integrates insights from both
logits and the NSP score to enhance prediction certainty estimation, thus
enabling more reliable exiting decisions. The experimental results on the GLUE
benchmark show that our method can achieve an average speed-up ratio of 2.19x
across all tasks with negligible performance degradation, surpassing the
state-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off
between task performance and inference efficiency. The code is available at
https://github.com/He-Jianing/NSP.git.

</details>


### [520] [Towards Interpretable Adversarial Examples via Sparse Adversarial Attack](https://arxiv.org/pdf/2506.17250)
*Fudong Lin, Jiadong Lou, Hao Wang, Brian Jalaian, Xu Yuan*

Main category: cs.LG

TL;DR: The paper introduces a sparse attack method for DNNs under the l0 constraint, addressing poor sparsity, computational overhead, and weak attack strength in existing solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a sparse attack that is fast, transferable, and strong, while improving interpretability of adversarial examples for understanding CNN vulnerabilities.

Method: A novel parameterization technique approximates the NP-hard l0 optimization problem, and a loss function maximizes adversary property while minimizing perturbed pixels.

Result: The approach outperforms state-of-the-art sparse attacks in computational efficiency, transferability, and attack strength, yielding sparser adversarial examples.

Conclusion: The method provides interpretable adversarial examples, revealing two noise categories ('obscuring' and 'leading' noise) to explain classifier mispredictions.

Abstract: Sparse attacks are to optimize the magnitude of adversarial perturbations for
fooling deep neural networks (DNNs) involving only a few perturbed pixels
(i.e., under the l0 constraint), suitable for interpreting the vulnerability of
DNNs. However, existing solutions fail to yield interpretable adversarial
examples due to their poor sparsity. Worse still, they often struggle with
heavy computational overhead, poor transferability, and weak attack strength.
In this paper, we aim to develop a sparse attack for understanding the
vulnerability of CNNs by minimizing the magnitude of initial perturbations
under the l0 constraint, to overcome the existing drawbacks while achieving a
fast, transferable, and strong attack to DNNs. In particular, a novel and
theoretical sound parameterization technique is introduced to approximate the
NP-hard l0 optimization problem, making directly optimizing sparse
perturbations computationally feasible. Besides, a novel loss function is
designed to augment initial perturbations by maximizing the adversary property
and minimizing the number of perturbed pixels simultaneously. Extensive
experiments are conducted to demonstrate that our approach, with theoretical
performance guarantees, outperforms state-of-the-art sparse attacks in terms of
computational overhead, transferability, and attack strength, expecting to
serve as a benchmark for evaluating the robustness of DNNs. In addition,
theoretical and empirical results validate that our approach yields sparser
adversarial examples, empowering us to discover two categories of noises, i.e.,
"obscuring noise" and "leading noise", which will help interpret how
adversarial perturbation misleads the classifiers into incorrect predictions.
Our code is available at https://github.com/fudong03/SparseAttack.

</details>


### [521] [Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2506.18537)
*Azad Deihim, Eduardo Alonso, Dimitra Apostolopoulou*

Main category: cs.LG

TL;DR: MATWM is a transformer-based world model for multi-agent reinforcement learning, combining decentralized imagination, a semi-centralized critic, and teammate prediction. It achieves state-of-the-art performance with high sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multi-agent reinforcement learning, such as partial observability and non-stationarity, by modeling agent behavior and adapting to evolving policies.

Method: Uses a decentralized imagination framework, semi-centralized critic, teammate prediction, and prioritized replay to train the world model on recent experiences.

Result: Achieves state-of-the-art performance on benchmarks like StarCraft Multi-Agent Challenge, PettingZoo, and MeltingPot, with strong sample efficiency (near-optimal in 50K interactions).

Conclusion: MATWM outperforms prior approaches, with ablation studies confirming the importance of its components, especially in coordination-heavy tasks.

Abstract: We present the Multi-Agent Transformer World Model (MATWM), a novel
transformer-based world model designed for multi-agent reinforcement learning
in both vector- and image-based environments. MATWM combines a decentralized
imagination framework with a semi-centralized critic and a teammate prediction
module, enabling agents to model and anticipate the behavior of others under
partial observability. To address non-stationarity, we incorporate a
prioritized replay mechanism that trains the world model on recent experiences,
allowing it to adapt to agents' evolving policies. We evaluated MATWM on a
broad suite of benchmarks, including the StarCraft Multi-Agent Challenge,
PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance,
outperforming both model-free and prior world model approaches, while
demonstrating strong sample efficiency, achieving near-optimal performance in
as few as 50K environment interactions. Ablation studies confirm the impact of
each component, with substantial gains in coordination-heavy tasks.

</details>


### [522] [Training-free LLM Verification via Recycling Few-shot Examples](https://arxiv.org/pdf/2506.17251)
*Dongseok Lee, Jimyung Hong, Dongyoung Kim, Jaehyung Kim*

Main category: cs.LG

TL;DR: The paper proposes Referi, a framework that recycles few-shot examples to verify LLM outputs, improving accuracy without additional training.


<details>
  <summary>Details</summary>
Motivation: Addressing the stochasticity and varying conclusions of LLMs by leveraging few-shot examples for verification, avoiding limitations of existing methods like majority voting or external verification.

Method: Referi evaluates candidate outputs using two scores derived from Bayes' rule, selecting the most confident and coherent response through additional LLM inferences.

Result: Experiments show Referi improves LLM accuracy by 4.8% on average across seven tasks.

Conclusion: Referi effectively enhances LLM performance by reusing few-shot examples for verification, eliminating the need for extra training.

Abstract: Although LLMs have achieved remarkable performance, the inherent
stochasticity of their reasoning process and varying conclusions present
significant challenges. Majority voting or Best-of-N with external verification
models has been explored to find the most promising solution among multiple LLM
outputs. However, these approaches have certain limitations, such as limited
applicability or the cost of an additional training step. To address this
problem, we propose a novel and effective framework that Recycles Few-shot
examples to verify LLM outputs (Referi). Our key idea is to additionally
utilize the given few-shot examples to evaluate the candidate outputs of the
target query, not only using them to generate outputs as the conventional
few-shot prompting setup. Specifically, Referi evaluates the generated outputs
by combining two different scores, designed motivated from Bayes' rule, and
subsequently selects the candidate that is both confidently determined and
contextually coherent through a few additional LLM inferences. Experiments with
three different LLMs and across seven diverse tasks demonstrate that our
framework significantly improves the accuracy of LLMs-achieving an average gain
of 4.8%-through effective response selection, without additional training.

</details>


### [523] [Adaptive Sample Scheduling for Direct Preference Optimization](https://arxiv.org/pdf/2506.17252)
*Zixuan Huang, Yikun Ban, Lean Fu, Xiaojie Li, Zhongxiang Dai, Jianxin Li, Deqing Wang*

Main category: cs.LG

TL;DR: The paper introduces Sample Scheduling for DPO (SamS), an algorithm to dynamically select training samples based on model states, improving LLM alignment without modifying DPO.


<details>
  <summary>Details</summary>
Motivation: Current DPO performance relies heavily on human preference data quality, and existing data selection methods ignore model state evolution during training.

Method: Proposes SamS, an adaptive sample scheduling algorithm that selects training batches based on the LLM's learning feedback to enhance generalization.

Result: SamS significantly boosts performance across tasks with minimal computational overhead, without altering DPO's core.

Conclusion: SamS offers a promising direction for better LLM alignment by optimizing fixed preference dataset usage.

Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for
aligning large language models (LLMs) with human preferences. However, its
performance is highly dependent on the quality of the underlying human
preference data. To address this bottleneck, prior work has explored various
data selection strategies, but these methods often overlook the impact of the
evolving states of the language model during the DPO process. %including active
querying, response pair selection, and data pre-selection. In this paper, we
introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically
and adaptively schedule training samples based on the model's evolving states
throughout preference optimization. To solve this problem, we propose SamS, an
efficient and effective algorithm that adaptively selects samples in each
training batch based on the LLM's learning feedback to maximize the potential
generalization performance. Notably, without modifying the core DPO algorithm,
simply integrating SamS significantly improves performance across tasks, with
minimal additional computational overhead. This work points to a promising new
direction for improving LLM alignment through more effective utilization of
fixed preference datasets.

</details>


### [524] [MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution](https://arxiv.org/pdf/2506.17253)
*Chenghan Li, Mingchen Li, Yipu Liao, Ruisheng Diao*

Main category: cs.LG

TL;DR: The paper introduces MS-TVNet, a multi-scale 3D dynamic CNN, for long-term time series prediction, outperforming Transformer and MLP models.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of convolutional networks in long-term time series prediction.

Method: Proposes a multi-scale time series reshape module and MS-TVNet, a 3D dynamic CNN, to capture multi-period patches and variable dependencies.

Result: MS-TVNet achieves SOTA results on diverse datasets, surpassing baseline models.

Conclusion: Convolutional networks are effective for capturing complex temporal patterns, offering a promising research direction.

Abstract: Long-term time series prediction has predominantly relied on Transformer and
MLP models, while the potential of convolutional networks in this domain
remains underexplored. To address this gap, we introduce a novel multi-scale
time series reshape module, which effectively captures the relationships among
multi-period patches and variable dependencies. Building upon this module, we
propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.
Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates
superior performance compared to baseline models, achieving state-of-the-art
(SOTA) results in long-term time series prediction. Our findings highlight the
effectiveness of leveraging convolutional networks for capturing complex
temporal patterns, suggesting a promising direction for future research in this
field.The code is realsed on https://github.com/Curyyfaust/TVNet.

</details>


### [525] [Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale](https://arxiv.org/pdf/2506.17254)
*Shaoang Li, Jian Li*

Main category: cs.LG

TL;DR: StageRoute is a hierarchical algorithm for managing LLM deployments and query routing, achieving near-optimal regret.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of LLMs requires efficient deployment and routing under budget constraints.

Method: StageRoute combines optimistic model selection (using reward/cost bounds) and budget-constrained bandit routing.

Result: The algorithm achieves $T^{2/3}$ regret, proven near-optimal, and performs well in experiments.

Conclusion: StageRoute effectively balances deployment and routing for LLM services under practical constraints.

Abstract: The rapid pace at which new large language models (LLMs) appear -- and older
ones become obsolete -- forces LLM service providers to juggle a streaming
inventory of models while respecting tight deployment capacity and per-query
cost budgets. We cast the reality as an online decision problem that couples
stage-wise deployment, made at fixed maintenance windows, with per-query
routing among the models kept live. We introduce StageRoute, a hierarchical
algorithm that (i) optimistically selects up to $M_max$ models for the next
stage using reward upper-confidence and cost lower-confidence bounds, then (ii)
solves a budget-constrained bandit sub-problem to route each incoming query. We
prove that StageRoute achieves a regret of order $T^{2/3}$ and provide a
matching lower bound, thereby establishing its near-optimality. Moreover, our
experiments confirm the theory, demonstrating that StageRoute performs close to
the optimum in practical settings.

</details>


### [526] [UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression](https://arxiv.org/pdf/2506.17255)
*Sunan Zou, Ziyun Zhang, Xueting Sun, Guojie Luo*

Main category: cs.LG

TL;DR: UltraSketchLLM enables ultra-low bit compression (down to 0.5 bits per weight) for LLMs, preserving performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Memory constraints on edge devices require extreme compression beyond 1-bit limits, but existing methods suffer from overhead or accuracy loss.

Method: Uses data sketching, an AbsMaxMin sketch for error minimization, importance-aware space allocation, and compression-aware finetuning.

Result: Achieves 0.5-bit compression on Llama-3.2-1B with competitive perplexity and tolerable latency.

Conclusion: UltraSketchLLM is a practical solution for deploying LLMs in resource-constrained environments.

Abstract: The rapid growth of large language models (LLMs) has outpaced the memory
constraints of edge devices, necessitating extreme weight compression beyond
the 1-bit limit. While quantization reduces model size, it is fundamentally
limited to 1 bit per weight. Existing multiple-to-one compression methods
either rely on mapping tables (inducing memory overhead) or incur severe
accuracy degradation due to random weight grouping. We introduce
UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low
bit compression (down to 0.5 bits per weight) while preserving model
performance. UltraSketchLLM leverages data sketching, a sub-linear
representation technique from streaming applications, to map multiple weights
to single values with bounded error. Our approach integrates an underestimate
AbsMaxMin sketch to minimize relative errors for small weights,
importance-aware space allocation to prioritize salient weights, and a
straight-through estimator for compression-aware finetuning. Experiments on
Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity,
alongside tolerable latency overhead. UltraSketchLLM offers a practical
solution for deploying LLMs in resource-constrained environments.

</details>


### [527] [AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma](https://arxiv.org/pdf/2506.17262)
*Thanadet Chuangsuwanich, Monisha E. Nongpiur, Fabian A. Braeu, Tin A. Tun, Alexandre Thiery, Shamira Perera, Ching Lin Ho, Martin Buist, George Barbastathis, Tin Aung, Michal J. A. Girard*

Main category: cs.LG

TL;DR: ONH biomechanics and explainable AI improve prediction of glaucoma visual field loss patterns, identifying key strain-sensitive regions.


<details>
  <summary>Details</summary>
Motivation: To enhance glaucoma diagnosis by assessing whether ONH biomechanics improves prediction of visual field loss patterns and using explainable AI to identify critical ONH regions.

Method: 237 glaucoma subjects were studied with ONH imaging under varying IOP conditions. Geometric Deep Learning models used biomechanical and structural features for classification tasks.

Result: High AUCs (0.77-0.88) showed ONH strain improves prediction. Inferior and inferotemporal rim were key strain-sensitive regions.

Conclusion: ONH strain enhances glaucoma prediction, with the neuroretinal rim being the most critical region.

Abstract: Objective: (1) To assess whether ONH biomechanics improves prediction of
three progressive visual field loss patterns in glaucoma; (2) to use
explainable AI to identify strain-sensitive ONH regions contributing to these
predictions.
  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged
under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated
to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects
into four categories based on the presence of specific visual field defects:
(1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full
superior hemifield defect (N=25), and (4) other/non-specific defects (N=124).
Automatic ONH tissue segmentation and digital volume correlation were used to
compute IOP-induced neural tissue and lamina cribrosa (LC) strains.
Biomechanical and structural features were input to a Geometric Deep Learning
model. Three classification tasks were performed to detect: (1) superior nasal
step, (2) superior partial arcuate, (3) full superior hemifield defect. For
each task, the data were split into 80% training and 20% testing sets. Area
under the curve (AUC) was used to assess performance. Explainable AI techniques
were employed to highlight the ONH regions most critical to each
classification.
  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain
improved VF loss prediction beyond morphology alone. The inferior and
inferotemporal rim were identified as key strain-sensitive regions,
contributing most to visual field loss prediction and showing progressive
expansion with increasing disease severity.
  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF
loss patterns. Neuroretinal rim, rather than the LC, was the most critical
region contributing to model predictions.

</details>


### [528] [Memory Allocation in Resource-Constrained Reinforcement Learning](https://arxiv.org/pdf/2506.17263)
*Massimiliano Tamborski, David Abel*

Main category: cs.LG

TL;DR: Memory constraints in reinforcement learning agents create a trade-off between allocating memory for world modeling versus planning, impacting performance in episodic and continual learning.


<details>
  <summary>Details</summary>
Motivation: To understand how memory limitations affect agent performance in reinforcement learning, particularly in balancing world modeling and planning.

Method: Study MCTS- and DQN-based algorithms under memory constraints, analyzing memory allocation impacts in episodic and continual learning.

Result: Different memory allocations significantly influence agent performance, highlighting the trade-off between modeling and planning.

Conclusion: Memory constraints introduce a critical trade-off in reinforcement learning, requiring careful allocation for optimal performance.

Abstract: Resource constraints can fundamentally change both learning and
decision-making. We explore how memory constraints influence an agent's
performance when navigating unknown environments using standard reinforcement
learning algorithms. Specifically, memory-constrained agents face a dilemma:
how much of their limited memory should be allocated to each of the agent's
internal processes, such as estimating a world model, as opposed to forming a
plan using that model? We study this dilemma in MCTS- and DQN-based algorithms
and examine how different allocations of memory impact performance in episodic
and continual learning settings.

</details>


### [529] [OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning](https://arxiv.org/pdf/2506.17264)
*Jikai Long, Zijian Hu, Xiaodong Yu, Jianwen Xie, Zhaozhuo Xu*

Main category: cs.LG

TL;DR: OAT-Rephrase improves zeroth-order optimization (ZO) for LLM fine-tuning by rephrasing training data to align with ZO dynamics, enhancing performance and stability.


<details>
  <summary>Details</summary>
Motivation: To address the slow convergence and instability of ZO methods like MeZO in LLM fine-tuning by leveraging optimization-aware data rephrasing.

Method: Introduces OAT-Rephrase, a dual-stage pipeline with a rewriter LLM and semantic judge to rephrase training data while maintaining relevance and consistency.

Result: Improves MeZO fine-tuning performance across tasks and architectures, often matching first-order methods.

Conclusion: Optimization-aware rephrasing is a reusable, low-overhead enhancement for ZO tuning.

Abstract: Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO)
offers a memory-efficient alternative to gradient-based methods but suffers
from slower convergence and unstable optimization due to noisy gradient
estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training
data rephrasing strategy that leverages an LLM to rephrase training instances
based on its understanding of the ZO dynamics, specifically MeZO, derived
directly from its paper. The approach incorporates a dual-stage pipeline
featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain
task relevance and logical consistency. Evaluations across five classification
tasks and three LLM architectures demonstrate that OAT-Rephrase consistently
improves MeZO fine-tuning performance, often narrowing or eliminating the gap
with first-order methods. Our findings suggest that optimization-aware
rephrasing serves as a reusable and low-overhead enhancement for zeroth-order
tuning regimes.

</details>


### [530] [Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack](https://arxiv.org/pdf/2506.17265)
*Xianren Zhang, Hui Liu, Delvin Ce Zhang, Xianfeng Tang, Qi He, Dongwon Lee, Suhang Wang*

Main category: cs.LG

TL;DR: The paper introduces a stealthy unlearning attack (SUA) framework to recover unlearned knowledge from MLLMs, revealing privacy risks in unlearning methods.


<details>
  <summary>Details</summary>
Motivation: MLLMs may memorize sensitive data, and unlearning methods might not truly erase but hide it. The study aims to test if unlearned knowledge can be recovered.

Method: Proposes SUA, a framework using universal noise patterns to trigger unlearned content in MLLMs, with embedding alignment for stealthiness.

Result: SUA successfully recovers unlearned information, and the noise generalizes to unseen images, showing consistent knowledge reappearance.

Conclusion: Unlearning methods may not fully erase sensitive data, and SUA exposes vulnerabilities, highlighting the need for more robust privacy defenses.

Abstract: Multimodal Large Language Models (MLLMs) trained on massive data may memorize
sensitive personal information and photos, posing serious privacy risks. To
mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to
reduce the ``forget'' sensitive information. However, it remains unclear
whether the knowledge has been truly forgotten or just hidden in the model.
Therefore, we propose to study a novel problem of LLM unlearning attack, which
aims to recover the unlearned knowledge of an unlearned LLM. To achieve the
goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework
that learns a universal noise pattern. When applied to input images, this noise
can trigger the model to reveal unlearned content. While pixel-level
perturbations may be visually subtle, they can be detected in the semantic
embedding space, making such attacks vulnerable to potential defenses. To
improve stealthiness, we introduce an embedding alignment loss that minimizes
the difference between the perturbed and denoised image embeddings, ensuring
the attack is semantically unnoticeable. Experimental results show that SUA can
effectively recover unlearned information from MLLMs. Furthermore, the learned
noise generalizes well: a single perturbation trained on a subset of samples
can reveal forgotten content in unseen images. This indicates that knowledge
reappearance is not an occasional failure, but a consistent behavior.

</details>


### [531] [CF-VLM:CounterFactual Vision-Language Fine-tuning](https://arxiv.org/pdf/2506.17267)
*Jusheng Zhang, Kaitong Cai, Yijia Fan, Jian Wang, Keze Wang*

Main category: cs.LG

TL;DR: CF-VLM enhances VLMs' causal reasoning using counterfactual samples, outperforming baselines in fine-grained tasks and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack deep causal reasoning and rely on superficial correlations, limiting their effectiveness in fine-grained tasks.

Method: CF-VLM uses three training objectives: cross-modal alignment, factual representation stability, and sensitivity to causal edits via counterfactuals.

Result: CF-VLM outperforms state-of-the-art methods in compositional reasoning and reduces visual hallucinations.

Conclusion: CF-VLM improves VLMs' reliability for high-stakes applications requiring robust reasoning and interpretability.

Abstract: Recent advances in vision-language models (VLMs) have greatly improved
cross-modal semantic understanding, yet significant limitations remain in
fine-grained discrimination and deep causal reasoning tasks. Existing VLMs
often rely on superficial statistical correlations, lacking the ability to
capture the underlying causal logic between visual and textual content. To
address this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a
novel framework that enhances the causal reasoning capabilities of VLMs through
the targeted use of counterfactual samples. CF-VLM introduces three
complementary training objectives: maintaining foundational cross-modal
alignment, reinforcing the uniqueness and stability of factual scene
representations against coherent counterfactuals, and sharpening the model's
sensitivity to minimal but critical causal edits. Extensive experiments
demonstrate that CF-VLM consistently outperforms strong baselines and
state-of-the-art methods on compositional reasoning and generalization
benchmarks. Furthermore, it shows promise in mitigating visual hallucinations,
indicating improved factual consistency. Our CF-VLM provides a robust
foundation for deploying VLMs in high-stakes, real-world scenarios requiring
reliable reasoning and interpretability.

</details>


### [532] [SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library](https://arxiv.org/pdf/2506.17297)
*Satyam Mishra, Phung Thao Vi, Shivam Mishra, Vishwanath Bijalwan, Vijay Bhaskar Semwal, Abdul Manan Khan*

Main category: cs.LG

TL;DR: SafeRL-Lite is a lightweight Python library for creating constrained and explainable RL agents, addressing gaps in existing toolkits by enforcing safety constraints and providing interpretable decision rationales.


<details>
  <summary>Details</summary>
Motivation: Existing RL toolkits lack native support for hard safety constraints and human-interpretable explanations, limiting their practical deployment in safety-critical applications.

Method: SafeRL-Lite modularly wraps standard Gym environments and deep Q-learning agents, enabling safety-aware training via constraint enforcement and real-time explanations using SHAP values and saliency maps.

Result: The library demonstrates effectiveness on constrained CartPole variants, with visualizations showing policy logic and safety adherence.

Conclusion: SafeRL-Lite is a practical, extensible solution for safety-constrained and explainable RL, available as an open-source pip-installable library.

Abstract: We introduce SafeRL-Lite, an open-source Python library for building
reinforcement learning (RL) agents that are both constrained and explainable.
Existing RL toolkits often lack native mechanisms for enforcing hard safety
constraints or producing human-interpretable rationales for decisions.
SafeRL-Lite provides modular wrappers around standard Gym environments and deep
Q-learning agents to enable: (i) safety-aware training via constraint
enforcement, and (ii) real-time post-hoc explanation via SHAP values and
saliency maps. The library is lightweight, extensible, and installable via pip,
and includes built-in metrics for constraint violations. We demonstrate its
effectiveness on constrained variants of CartPole and provide visualizations
that reveal both policy logic and safety adherence. The full codebase is
available at: https://github.com/satyamcser/saferl-lite.

</details>


### [533] [AlgoSelect: Universal Algorithm Selection via the Comb Operator](https://arxiv.org/pdf/2506.17304)
*Jasper Yao*

Main category: cs.LG

TL;DR: AlgoSelect is a framework for learning optimal algorithm selection using the novel Comb Operator, achieving near-perfect accuracy with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: The need for a principled, efficient, and robust method to select the best algorithm for a given problem from data.

Method: Uses the Comb Operator (sigmoid-gated selector) for interpolation between algorithms, extended to N-Path Comb for multiple algorithms. Theoretical proofs include universal approximation, learnability, and computational efficiency.

Result: Empirical validation shows 99.9%+ accuracy with rapid convergence, indicating minimal conditional entropy in structured domains.

Conclusion: AlgoSelect offers a theoretically grounded, practical solution for automated algorithm selection with optimality and learnability guarantees, impactful for AI and adaptive systems.

Abstract: We introduce AlgoSelect, a principled framework for learning optimal
algorithm selection from data, centered around the novel Comb Operator. Given a
set of algorithms and a feature representation of problems, AlgoSelect learns
to interpolate between diverse computational approaches. For pairs of
algorithms, a simple sigmoid-gated selector, an instance of the Comb Operator,
facilitates this interpolation. We extend this to an N-Path Comb for multiple
algorithms. We prove that this framework is universal (can approximate any
algorithm selector), information-theoretically optimal in its learnability
(thresholds for selection converge almost surely, demonstrated via
Borel-Cantelli arguments), computationally efficient, and robust. Key
theoretical contributions include: (1) a universal approximation theorem
demonstrating that Comb-based selectors can achieve arbitrary accuracy; (2)
information-theoretic learnability for selection thresholds; (3) formalization
of the Comb Operator within linear operator theory, detailing its boundedness
and spectral properties; (4) an N-Path Comb generalization for multi-algorithm
selection; and (5) a practical learning framework for the adaptive seeding
functions that guide the Comb Operator. Empirical validation on a comprehensive
20$\times$20 problem-algorithm study demonstrates near-perfect selection
(99.9\%+ accuracy) with remarkably few samples and rapid convergence, revealing
that $H(\text{Algorithm}|\text{Problem}) \approx 0$ in structured domains.
AlgoSelect provides a theoretically grounded, practically deployable solution
to automated algorithm selection with provable optimality and learnability
guarantees, with significant implications for AI and adaptive systems.

</details>


### [534] [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://arxiv.org/pdf/2506.17307)
*Zhixiang Chi, Li Gu, Huan Liu, Ziqiang Wang, Yanan Wu, Yang Wang, Konstantinos N Plataniotis*

Main category: cs.LG

TL;DR: The paper introduces a method for few-shot test-time domain adaptation by learning directly on the input space to complement CLIP's frozen features, improving performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing domain shift in test-time adaptation with limited unlabeled examples, leveraging CLIP's OOD abilities while overcoming its constraints.

Method: Attaches a side branch to CLIP for exclusive knowledge learning via revert attention, enhances text features with greedy ensemble, and fuses features using domain prompts.

Result: Achieves significant improvements on benchmarks (e.g., +5.1 F1 for iWildCam, +3.1% WC Acc for FMoW).

Conclusion: The proposed method outperforms prior approaches by combining input-space learning with CLIP's frozen features, especially for smaller networks.

Abstract: Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time
to a specific domain using only a few unlabeled examples, addressing domain
shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities
by generating domain-specific prompts to guide its generalized, frozen
features. However, since downstream datasets are not explicitly seen by CLIP,
solely depending on the feature space knowledge is constrained by CLIP's prior
knowledge. Notably, when using a less robust backbone like ViT-B/16,
performance significantly drops on challenging real-world benchmarks. Departing
from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,
this work introduces learning directly on the input space to complement the
dataset-specific knowledge for frozen CLIP. Specifically, an independent side
branch is attached in parallel with CLIP and enforced to learn exclusive
knowledge via revert attention. To better capture the dataset-specific label
semantics for downstream adaptation, we propose to enhance the inter-dispersion
among text features via greedy text ensemble and refinement. The text and
visual features are then progressively fused in a domain-aware manner by a
generated domain prompt to adapt toward a specific domain. Extensive
experiments show our method's superiority on 5 large-scale benchmarks (WILDS
and DomainNet), notably improving over smaller networks like ViT-B/16 with
gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for
FMoW.

</details>


### [535] [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution](https://arxiv.org/pdf/2506.17323)
*Tamas Bisztray, Bilel Cherif, Richard A. Dubniczky, Nils Gruschka, Bertalan Borsos, Mohamed Amine Ferrag, Attila Kovacs, Vasileios Mavroeidis, Norbert Tihanyi*

Main category: cs.LG

TL;DR: The paper introduces CodeT5-Authorship, a model for attributing C programs to specific LLMs, and LLM-AuthorBench, a benchmark for evaluation. It achieves high accuracy in binary and multi-class classification.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated code necessitates methods to identify the specific LLM behind such content, especially for C programs.

Method: Uses a modified CodeT5 encoder with a two-layer classification head for authorship attribution. Evaluated on LLM-AuthorBench, a dataset of 32,000 C programs from eight LLMs.

Result: Achieves 97.56% accuracy in binary classification (e.g., GPT-4.1 vs. GPT-4o) and 95.40% accuracy in multi-class attribution among five LLMs.

Conclusion: CodeT5-Authorship is effective for LLM authorship attribution, with open-sourced resources to support further research.

Abstract: Detecting AI-generated code, deepfakes, and other synthetic content is an
emerging research challenge. As code generated by Large Language Models (LLMs)
becomes more common, identifying the specific model behind each sample is
increasingly important. This paper presents the first systematic study of LLM
authorship attribution for C programs. We released CodeT5-Authorship, a novel
model that uses only the encoder layers from the original CodeT5
encoder-decoder architecture, discarding the decoder to focus on
classification. Our model's encoder output (first token) is passed through a
two-layer classification head with GELU activation and dropout, producing a
probability distribution over possible authors. To evaluate our approach, we
introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs
generated by eight state-of-the-art LLMs across diverse tasks. We compare our
model to seven traditional ML classifiers and eight fine-tuned transformer
models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,
Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model
achieves 97.56% accuracy in distinguishing C programs generated by closely
related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class
attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,
GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the
CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant
Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.

</details>


### [536] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/pdf/2506.17324)
*Emma Finn, T. Anderson Keller, Manos Theodosis, Demba E. Ba*

Main category: cs.LG

TL;DR: The paper explores how self-attention in diffusion models enhances creativity by ensuring globally consistent image generation beyond patch-level mosaics.


<details>
  <summary>Details</summary>
Motivation: Understanding the role of self-attention in diffusion models, as current theories only explain patch-wise creativity in CNNs.

Method: Extends theory to diffusion models with CNN and self-attention layers, analyzing global consistency in generated images.

Result: Self-attention induces globally consistent arrangements of local features, verified empirically.

Conclusion: Self-attention plays a key role in enhancing the creativity and global coherence of diffusion-generated images.

Abstract: As diffusion models have become the tool of choice for image generation and
as the quality of the images continues to improve, the question of how
`creativity' originates in diffusion has become increasingly important. The
score matching perspective on diffusion has proven particularly fruitful for
understanding how and why diffusion models generate images that remain
plausible while differing significantly from their training images. In
particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g.,
(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we
would only be able to recover training samples through our diffusion process.
However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the
score is parametrized by a simple CNN, the inductive biases of the CNN itself
(translation equivariance and locality) allow the model to generate samples
that globally do not match any training samples, but are rather patch-wise
`mosaics'. Notably, however, this theory does not extend to describe the role
of self-attention in this process. In this work, we take a preliminary step in
this direction to extend this theory to the case of diffusion models whose
score is parametrized by a CNN with a final self-attention layer. We show that
our theory suggests that self-attention will induce a globally image-consistent
arrangement of local features beyond the patch-level in generated samples, and
we verify this behavior empirically on a carefully crafted dataset.

</details>


### [537] [CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction](https://arxiv.org/pdf/2506.17326)
*Agnideep Aich, Md Monzur Murshed, Sameera Hewage, Amanda Mayeaux*

Main category: cs.LG

TL;DR: The study proposes using A2 copula-based data augmentation to address imbalanced diabetes data, outperforming SMOTE with improved ML metrics.


<details>
  <summary>Details</summary>
Motivation: Early diabetes detection is critical, but imbalanced data affects ML performance. The study aims to improve this using copula-based augmentation.

Method: Used A2 copula for data augmentation on the Pima Indian dataset, tested with logistic regression, random forest, gradient boosting, and XGBoost.

Result: XGBoost with A2 copula outperformed SMOTE, boosting accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1 by 18.2%, and AUC by 25.5%.

Conclusion: A2 copula is a viable alternative to SMOTE, enhancing ML performance for imbalanced diabetes data.

Abstract: Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people
are affected by it. Early detection can significantly lower this risk. Despite
significant advancements in machine learning for identifying diabetic cases,
results can still be influenced by the imbalanced nature of the data. To
address this challenge, our study considered copula-based data augmentation,
which preserves the dependency structure when generating data for the minority
class and integrates it with machine learning (ML) techniques. We selected the
Pima Indian dataset and generated data using A2 copula, then applied four
machine learning algorithms: logistic regression, random forest, gradient
boosting, and extreme gradient boosting. Our findings indicate that XGBoost
combined with A2 copula oversampling achieved the best performance improving
accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and
AUC by 25.5% compared to the standard SMOTE method. Furthermore, we
statistically validated our results using the McNemar test. This research
represents the first known use of A2 copulas for data augmentation and serves
as an alternative to the SMOTE technique, highlighting the efficacy of copulas
as a statistical method in machine learning applications.

</details>


### [538] [AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata](https://arxiv.org/pdf/2506.17333)
*Jaime A. Berkovich, Noah S. David, Markus J. Buehler*

Main category: cs.LG

TL;DR: AutomataGPT, a transformer model pretrained on simulated CA trajectories, achieves high accuracy in forecasting and rule inference for unseen CA rules, demonstrating generalization without hand-crafted priors.


<details>
  <summary>Details</summary>
Motivation: The challenge of automatically discovering and predicting local update rules for cellular automata (CA) in diverse domains motivates the development of AutomataGPT.

Method: AutomataGPT is a decoder-only transformer pretrained on 1 million simulated trajectories across 100 distinct 2D binary deterministic CA rules on toroidal grids.

Result: The model achieves 98.5% perfect one-step forecasts, 96% functional accuracy in rule reconstruction, and 82% exact rule-matrix match for unseen rules.

Conclusion: AutomataGPT's success in CA dynamics inference and execution paves the way for data-efficient CA surrogates in real-world applications across multiple fields.

Abstract: Cellular automata (CA) provide a minimal formalism for investigating how
simple local interactions generate rich spatiotemporal behavior in domains as
diverse as traffic flow, ecology, tissue morphogenesis and crystal growth.
However, automatically discovering the local update rules for a given
phenomenon and using them for quantitative prediction remains challenging. Here
we present AutomataGPT, a decoder-only transformer pretrained on around 1
million simulated trajectories that span 100 distinct two-dimensional binary
deterministic CA rules on toroidal grids. When evaluated on previously unseen
rules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step
forecasts and reconstructs the governing update rule with up to 96% functional
(application) accuracy and 82% exact rule-matrix match. These results
demonstrate that large-scale pretraining over wider regions of rule space
yields substantial generalization in both the forward (state forecasting) and
inverse (rule inference) problems, without hand-crafted priors. By showing that
transformer models can faithfully infer and execute CA dynamics from data
alone, our work lays the groundwork for abstracting real-world dynamical
phenomena into data-efficient CA surrogates, opening avenues in biology, tissue
engineering, physics and AI-driven scientific discovery.

</details>


### [539] [FFINO: Factorized Fourier Improved Neural Operator for Modeling Multiphase Flow in Underground Hydrogen Storage](https://arxiv.org/pdf/2506.17344)
*Tao Wang, Hewei Tang*

Main category: cs.LG

TL;DR: The paper introduces FFINO, a neural operator for fast modeling of hydrogen plume migration and pressure in underground hydrogen storage (UHS), outperforming FMIONet in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the need for fast modeling in UHS for energy transition, improving upon existing methods like FMIONet.

Method: Proposes FFINO, a neural operator incorporating parameterized relative permeability curves and comparing it with FMIONet.

Result: FFINO reduces trainable parameters, training time, and GPU memory, while improving accuracy and speed (7850x faster than numerical simulators).

Conclusion: FFINO is a superior surrogate model for UHS simulations, offering efficiency and accuracy.

Abstract: Underground hydrogen storage (UHS) is a promising energy storage option for
the current energy transition to a low-carbon economy. Fast modeling of
hydrogen plume migration and pressure field evolution is crucial for UHS field
management. In this study, we propose a new neural operator architecture,
FFINO, as a fast surrogate model for multiphase flow problems in UHS. We
parameterize experimental relative permeability curves reported in the
literature and include them as key uncertainty parameters in the FFINO model.
We also compare the FFINO model with the state-of-the-art FMIONet model through
a comprehensive combination of metrics. Our new FFINO model has 38.1% fewer
trainable parameters, 17.6% less training time, and 12% less GPU memory cost
compared to FMIONet. The FFINO model also achieves a 9.8% accuracy improvement
in predicting hydrogen plume in focused areas, and 18% higher RMSE in
predicting pressure buildup. The inference time of the trained FFINO model is
7850 times faster than a numerical simulator, which makes it a competent
substitute for numerical simulations of UHS problems with superior time
efficiency.

</details>


### [540] [SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification](https://arxiv.org/pdf/2506.17368)
*Zhenglin Lai, Mengyao Liao, Dong Xu, Zebin Zhao, Zhihang Yuan, Chao Fan, Jianqiang Li, Bingzhe Wu*

Main category: cs.LG

TL;DR: The paper investigates safety alignment challenges in Mixture-of-Experts (MoE) models, introducing SAFEx, a framework to identify and validate safety-critical experts, revealing their disproportionate impact on model safety.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment strategies for dense models are inadequate for MoE architectures, which exhibit unique vulnerabilities due to reliance on specific expert modules.

Method: The authors propose SAFEx, featuring a Stability-based Expert Selection (SES) algorithm, to decompose and analyze safety-critical experts in MoE models.

Result: Experiments on models like Qwen3-MoE show disabling a small subset of safety-critical experts (e.g., 12 out of 6144) reduces refusal rates by 22%, highlighting their outsized role in safety.

Conclusion: MoE models' safety mechanisms are highly dependent on a few experts, necessitating tailored alignment strategies to address their positional vulnerabilities.

Abstract: Large language models based on Mixture-of-Experts have achieved substantial
gains in efficiency and scalability, yet their architectural uniqueness
introduces underexplored safety alignment challenges. Existing safety alignment
strategies, predominantly designed for dense models, are ill-suited to address
MoE-specific vulnerabilities. In this work, we formalize and systematically
study MoE model's positional vulnerability - the phenomenon where
safety-aligned behaviors rely on specific expert modules, revealing critical
risks inherent to MoE architectures. To this end, we present SAFEx, an
analytical framework that robustly identifies, characterizes, and validates the
safety-critical experts using a novel Stability-based Expert Selection (SES)
algorithm. Notably, our approach enables the explicit decomposition of
safety-critical experts into distinct functional groups, including those
responsible for harmful content detection and those controlling safe response
generation. Extensive experiments on mainstream MoE models, such as the
recently released Qwen3-MoE, demonstrated that their intrinsic safety
mechanisms heavily rely on a small subset of positional experts. Disabling
these experts significantly compromised the models' ability to refuse harmful
requests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that
disabling as few as 12 identified safety-critical experts can cause the refusal
rate to drop by 22%, demonstrating the disproportionate impact of a small set
of experts on overall model safety.

</details>


### [541] [Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?](https://arxiv.org/pdf/2506.17417)
*Mingyuan Wu, Meitang Li, Jingcheng Yang, Jize Jiang, Kaizhuo Yan, Zhaoheng Li, Minjia Zhang, Klara Nahrstedt*

Main category: cs.LG

TL;DR: Inference-time techniques like self-correction and self-verification, effective in LLMs, are tested on VLMs. While some methods improve reasoning, RL-trained VLMs lack robust self-verification.


<details>
  <summary>Details</summary>
Motivation: To explore if inference-time techniques enhancing LLMs also benefit VLMs, especially those trained with RL.

Method: Evaluated decoding strategies (majority voting, best-of-N selection) and self-correction behaviors in VLMs.

Result: Generation-reliant methods outperform verification-reliant ones; RL-trained VLMs lack robust self-verification.

Conclusion: Inference-time techniques show promise for VLMs, but RL-trained models need stronger self-verification capabilities.

Abstract: Recent advances in large language models (LLMs) have demonstrated that
inference-time computation techniques, such as decoding-time scaling and
self-refinement, can significantly enhance reasoning capabilities without
relying on external knowledge. A key driver of this success is the emergence of
self-correction and self-verification behaviors, often elicited through
reinforcement learning (RL). In this paper, we investigate whether these
inference-time techniques extend effectively to vision-language models (VLMs),
particularly those trained with RL. We find that while decoding strategies such
as majority voting and best-of-N selection with self-verification all improve
VLM reasoning performance, generation-reliant methods such as the former
achieve significantly higher gains versus verification-reliant methods such as
the latter. Additionally, the self-correction behavior often associated with
RL-tuned models, such as aha moment, does not lead to measurable gains. We show
via extensive experimentation within the inference-time scaling framework to
identify a key root cause: RL-trained VLMs still lack robust self-verification
capabilities across both visual and textual modalities.

</details>


### [542] [FedNAMs: Performing Interpretability Analysis in Federated Learning Context](https://arxiv.org/pdf/2506.17466)
*Amitash Nanda, Sree Bhargavi Balija, Debashis Sahoo*

Main category: cs.LG

TL;DR: FedNAMs combine Neural Additive Models with federated learning for interpretable, privacy-preserving analysis, showing minimal accuracy loss and strong feature insights.


<details>
  <summary>Details</summary>
Motivation: Address challenges in interpretability and explainability in federated learning while enhancing privacy and robustness.

Method: Integrate Neural Additive Models (NAMs) into federated learning (FedNAMs), focusing on feature-specific learning and decentralized training.

Result: FedNAMs achieve strong interpretability with minimal accuracy loss, identifying key predictive features in datasets like wine quality, heart disease, and iris classification.

Conclusion: FedNAMs improve privacy, interpretability, and robustness, making them valuable for sectors like finance and healthcare.

Abstract: Federated learning continues to evolve but faces challenges in
interpretability and explainability. To address these challenges, we introduce
a novel approach that employs Neural Additive Models (NAMs) within a federated
learning framework. This new Federated Neural Additive Models (FedNAMs)
approach merges the advantages of NAMs, where individual networks concentrate
on specific input features, with the decentralized approach of federated
learning, ultimately producing interpretable analysis results. This integration
enhances privacy by training on local data across multiple devices, thereby
minimizing the risks associated with data centralization and improving model
robustness and generalizability. FedNAMs maintain detailed, feature-specific
learning, making them especially valuable in sectors such as finance and
healthcare. They facilitate the training of client-specific models to integrate
local updates, preserve privacy, and mitigate concerns related to
centralization. Our studies on various text and image classification tasks,
using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show
that FedNAMs deliver strong interpretability with minimal accuracy loss
compared to traditional Federated Deep Neural Networks (DNNs). The research
involves notable findings, including the identification of critical predictive
features at both client and global levels. Volatile acidity, sulfates, and
chlorides for wine quality. Chest pain type, maximum heart rate, and number of
vessels for heart disease. Petal length and width for iris classification. This
approach strengthens privacy and model efficiency and improves interpretability
and robustness across diverse datasets. Finally, FedNAMs generate insights on
causes of highly and low interpretable features.

</details>


### [543] [A geometric framework for momentum-based optimizers for low-rank training](https://arxiv.org/pdf/2506.17475)
*Steffen Schotthfer, Timon Klein, Jonas Kusch*

Main category: cs.LG

TL;DR: The paper analyzes challenges in training low-rank neural networks with classical optimizers and proposes new geometric-aware methods for faster convergence.


<details>
  <summary>Details</summary>
Motivation: To address convergence issues in low-rank neural network training caused by the optimization landscape's geometry.

Method: Introduces novel training strategies from dynamical low-rank approximation, combining it with momentum-based optimization.

Result: Demonstrates faster convergence and better validation metrics at fixed parameter budgets.

Conclusion: Proposed geometric-aware optimizers improve low-rank training efficiency and performance.

Abstract: Low-rank pre-training and fine-tuning have recently emerged as promising
techniques for reducing the computational and storage costs of large neural
networks. Training low-rank parameterizations typically relies on conventional
optimizers such as heavy ball momentum methods or Adam. In this work, we
identify and analyze potential difficulties that these training methods
encounter when used to train low-rank parameterizations of weights. In
particular, we show that classical momentum methods can struggle to converge to
a local optimum due to the geometry of the underlying optimization landscape.
To address this, we introduce novel training strategies derived from dynamical
low-rank approximation, which explicitly account for the underlying geometric
structure. Our approach leverages and combines tools from dynamical low-rank
approximation and momentum-based optimization to design optimizers that respect
the intrinsic geometry of the parameter space. We validate our methods through
numerical experiments, demonstrating faster convergence, and stronger
validation metrics at given parameter budgets.

</details>


### [544] [A Survey of State Representation Learning for Deep Reinforcement Learning](https://arxiv.org/pdf/2506.17518)
*Ayoub Echchahed, Pablo Samuel Castro*

Main category: cs.LG

TL;DR: A survey categorizing representation learning methods in reinforcement learning into six classes, detailing their mechanisms, benefits, and limitations to guide new researchers.


<details>
  <summary>Details</summary>
Motivation: Address challenges in complex observation spaces for sequential decision-making by improving sample efficiency, generalization, and performance through state representation learning.

Method: Categorizes methods into six classes, detailing their mechanisms, and discusses techniques for assessing representation quality.

Result: Provides a taxonomy to enhance understanding of state representation learning in reinforcement learning.

Conclusion: Offers a guide for researchers and outlines future directions for the field.

Abstract: Representation learning methods are an important tool for addressing the
challenges posed by complex observations spaces in sequential decision making
problems. Recently, many methods have used a wide variety of types of
approaches for learning meaningful state representations in reinforcement
learning, allowing better sample efficiency, generalization, and performance.
This survey aims to provide a broad categorization of these methods within a
model-free online setting, exploring how they tackle the learning of state
representations differently. We categorize the methods into six main classes,
detailing their mechanisms, benefits, and limitations. Through this taxonomy,
our aim is to enhance the understanding of this field and provide a guide for
new researchers. We also discuss techniques for assessing the quality of
representations, and detail relevant future directions.

</details>


### [545] [Predicting E-commerce Purchase Behavior using a DQN-Inspired Deep Learning Model for enhanced adaptability](https://arxiv.org/pdf/2506.17543)
*Aditi Madhusudan Jain*

Main category: cs.LG

TL;DR: A novel DQN-inspired model for predicting buying intent and product demand in e-commerce, combining LSTM and DQN strengths, achieves 88% accuracy and 0.88 AUC-ROC.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of user behavior is crucial for optimizing inventory, personalizing experiences, and maximizing sales in online retail.

Method: Adapts reinforcement learning to supervised learning, using LSTM for sequential modeling and DQN for strategic decision-making, tested on 885,000 user sessions.

Result: Handles class imbalance well, achieving 88% accuracy and 0.88 AUC-ROC, outperforming traditional methods in capturing temporal patterns.

Conclusion: The model is scalable and effective for real-world e-commerce, improving demand forecasting, personalization, and marketing strategies.

Abstract: This paper presents a novel approach to predicting buying intent and product
demand in e-commerce settings, leveraging a Deep Q-Network (DQN) inspired
architecture. In the rapidly evolving landscape of online retail, accurate
prediction of user behavior is crucial for optimizing inventory management,
personalizing user experiences, and maximizing sales. Our method adapts
concepts from reinforcement learning to a supervised learning context,
combining the sequential modeling capabilities of Long Short-Term Memory (LSTM)
networks with the strategic decision-making aspects of DQNs. We evaluate our
model on a large-scale e-commerce dataset comprising over 885,000 user
sessions, each characterized by 1,114 features. Our approach demonstrates
robust performance in handling the inherent class imbalance typical in
e-commerce data, where purchase events are significantly less frequent than
non-purchase events. Through comprehensive experimentation with various
classification thresholds, we show that our model achieves a balance between
precision and recall, with an overall accuracy of 88\% and an AUC-ROC score of
0.88. Comparative analysis reveals that our DQN-inspired model offers
advantages over traditional machine learning and standard deep learning
approaches, particularly in its ability to capture complex temporal patterns in
user behavior. The model's performance and scalability make it well-suited for
real-world e-commerce applications dealing with high-dimensional, sequential
data. This research contributes to the field of e-commerce analytics by
introducing a novel predictive modeling technique that combines the strengths
of deep learning and reinforcement learning paradigms. Our findings have
significant implications for improving demand forecasting, personalizing user
experiences, and optimizing marketing strategies in online retail environments.

</details>


### [546] [DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data](https://arxiv.org/pdf/2506.17552)
*Wei Zhang, Zi Wang, Hanwen Zhou, Zhaohong Deng, Weiping Ding, Yuxi Ge, Te Zhang, Yuanpeng Zhang, Kup-Sze Choi, Shitong Wang, Shudong Hu*

Main category: cs.LG

TL;DR: The paper proposes an interpretable incomplete multi-view surgical evaluation model for rectal cancer, integrating AI and multi-view data to improve surgical difficulty assessment.


<details>
  <summary>Details</summary>
Motivation: Current surgical difficulty evaluation relies on clinical data, but advancements in technology and AI allow for more comprehensive data collection and analysis.

Method: Constructs a multi-view rectal cancer dataset (MRI, pressed-fat MRI, clinical data) and proposes a dual representation incomplete multi-view learning model with missing view imputation and second-order similarity. A TSK fuzzy system evaluates surgical difficulty.

Result: The proposed DRIMV_TSK model outperforms advanced algorithms on the MVRC dataset.

Conclusion: The model effectively integrates multi-view data and AI, improving surgical evaluation for rectal cancer.

Abstract: A reliable evaluation of surgical difficulty can improve the success of the
treatment for rectal cancer and the current evaluation method is based on
clinical data. However, more data about rectal cancer can be collected with the
development of technology. Meanwhile, with the development of artificial
intelligence, its application in rectal cancer treatment is becoming possible.
In this paper, a multi-view rectal cancer dataset is first constructed to give
a more comprehensive view of patients, including the high-resolution MRI image
view, pressed-fat MRI image view, and clinical data view. Then, an
interpretable incomplete multi-view surgical evaluation model is proposed,
considering that it is hard to obtain extensive and complete patient data in
real application scenarios. Specifically, a dual representation incomplete
multi-view learning model is first proposed to extract the common information
between views and specific information in each view. In this model, the missing
view imputation is integrated into representation learning, and second-order
similarity constraint is also introduced to improve the cooperative learning
between these two parts. Then, based on the imputed multi-view data and the
learned dual representation, a multi-view surgical evaluation model with the
TSK fuzzy system is proposed. In the proposed model, a cooperative learning
mechanism is constructed to explore the consistent information between views,
and Shannon entropy is also introduced to adapt the view weight. On the MVRC
dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained
the best results.

</details>


### [547] [Accelerating Residual Reinforcement Learning with Uncertainty Estimation](https://arxiv.org/pdf/2506.17564)
*Lakshita Dodeja, Karl Schmeckpeper, Shivam Vats, Thomas Weng, Mingxi Jia, George Konidaris, Stefanie Tellex*

Main category: cs.LG

TL;DR: Residual RL improves sample efficiency and handles stochastic base policies by leveraging base policy uncertainty and modifying off-policy learning.


<details>
  <summary>Details</summary>
Motivation: Existing Residual RL methods struggle with sparse rewards and deterministic base policies, limiting their applicability.

Method: Proposes using base policy uncertainty for focused exploration and a modified off-policy learning approach for stochastic base policies.

Result: Outperforms baselines in simulation benchmarks and demonstrates robust zero-shot sim-to-real transfer.

Conclusion: The proposed enhancements make Residual RL more efficient and versatile for stochastic policies.

Abstract: Residual Reinforcement Learning (RL) is a popular approach for adapting
pretrained policies by learning a lightweight residual policy that provides
corrective actions. While Residual RL is more sample-efficient than finetuning
the entire base policy, existing methods struggle with sparse rewards and are
designed for deterministic base policies. We propose two improvements to
Residual RL that further enhance its sample efficiency and make it suitable for
stochastic base policies. First, we leverage uncertainty estimates of the base
policy to focus exploration on regions in which the base policy is not
confident. Second, we propose a simple modification to off-policy residual
learning that allows it to observe base actions and better handle stochastic
base policies. We evaluate our method with both Gaussian-based and
Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and
compare against state-of-the-art finetuning methods, demo-augmented RL methods,
and other residual RL methods. Our algorithm significantly outperforms existing
baselines in a variety of simulation benchmark environments. We also deploy our
learned polices in the real world to demonstrate their robustness with
zero-shot sim-to-real transfer.

</details>


### [548] [Towards Deeper GCNs: Alleviating Over-smoothing via Iterative Training and Fine-tuning](https://arxiv.org/pdf/2506.17576)
*Furong Peng, Jinzhen Gao, Xuan Lu, Kang Liu, Yifan Huo, Sheng Wang*

Main category: cs.LG

TL;DR: The paper identifies trainable linear transformations in GCNs as a key factor in feature collapse and proposes Layer-wise Gradual Training (LGT) to balance expressiveness and stability in deep architectures.


<details>
  <summary>Details</summary>
Motivation: Deep GCNs suffer from over-smoothing and feature collapse, primarily due to trainable linear transformations, which existing studies overlook.

Method: Proposes LGT, a training strategy with layer-wise training, low-rank adaptation, and identity initialization to stabilize and enhance deep GCNs.

Result: LGT achieves state-of-the-art performance, improving accuracy in deep (e.g., 32-layer) GCNs and works well with existing methods.

Conclusion: LGT provides a scalable, architecture-agnostic framework for training deep GCNs, balancing expressiveness and stability.

Abstract: Graph Convolutional Networks (GCNs) suffer from severe performance
degradation in deep architectures due to over-smoothing. While existing studies
primarily attribute the over-smoothing to repeated applications of graph
Laplacian operators, our empirical analysis reveals a critical yet overlooked
factor: trainable linear transformations in GCNs significantly exacerbate
feature collapse, even at moderate depths (e.g., 8 layers). In contrast,
Simplified Graph Convolution (SGC), which removes these transformations,
maintains stable feature diversity up to 32 layers, highlighting linear
transformations' dual role in facilitating expressive power and inducing
over-smoothing. However, completely removing linear transformations weakens the
model's expressive capacity.
  To address this trade-off, we propose Layer-wise Gradual Training (LGT), a
novel training strategy that progressively builds deep GCNs while preserving
their expressiveness. LGT integrates three complementary components: (1)
layer-wise training to stabilize optimization from shallow to deep layers, (2)
low-rank adaptation to fine-tune shallow layers and accelerate training, and
(3) identity initialization to ensure smooth integration of new layers and
accelerate convergence. Extensive experiments on benchmark datasets demonstrate
that LGT achieves state-of-the-art performance on vanilla GCN, significantly
improving accuracy even in 32-layer settings. Moreover, as a training method,
LGT can be seamlessly combined with existing methods such as PairNorm and
ContraNorm, further enhancing their performance in deeper networks. LGT offers
a general, architecture-agnostic training framework for scalable deep GCNs. The
code is available at [https://github.com/jfklasdfj/LGT_GCN].

</details>


### [549] [LFR-PINO: A Layered Fourier Reduced Physics-Informed Neural Operator for Parametric PDEs](https://arxiv.org/pdf/2506.17582)
*Jing Wang, Biao Chen, Hairun Xie, Rui Wang, Yifan Xia, Jifa Zhang, Hui Xu*

Main category: cs.LG

TL;DR: LFR-PINO introduces a layered hypernetwork and frequency-domain reduction to improve physics-informed neural operators for solving parametric PDEs, achieving significant error reduction and memory savings.


<details>
  <summary>Details</summary>
Motivation: Existing methods for solving parametric PDEs suffer from limited expressiveness or computational inefficiency due to high dimensionality.

Method: LFR-PINO uses a layered hypernetwork for specialized parameter generation and frequency-domain reduction to reduce parameter count.

Result: Achieves 22.8%-68.7% error reduction and 28.6%-69.3% memory savings compared to baselines.

Conclusion: LFR-PINO balances computational efficiency and solution accuracy, making it a universal PDE solver with optional fine-tuning.

Abstract: Physics-informed neural operators have emerged as a powerful paradigm for
solving parametric partial differential equations (PDEs), particularly in the
aerospace field, enabling the learning of solution operators that generalize
across parameter spaces. However, existing methods either suffer from limited
expressiveness due to fixed basis/coefficient designs, or face computational
challenges due to the high dimensionality of the parameter-to-weight mapping
space. We present LFR-PINO, a novel physics-informed neural operator that
introduces two key innovations: (1) a layered hypernetwork architecture that
enables specialized parameter generation for each network layer, and (2) a
frequency-domain reduction strategy that significantly reduces parameter count
while preserving essential spectral features. This design enables efficient
learning of a universal PDE solver through pre-training, capable of directly
handling new equations while allowing optional fine-tuning for enhanced
precision. The effectiveness of this approach is demonstrated through
comprehensive experiments on four representative PDE problems, where LFR-PINO
achieves 22.8%-68.7% error reduction compared to state-of-the-art baselines.
Notably, frequency-domain reduction strategy reduces memory usage by
28.6%-69.3% compared to Hyper-PINNs while maintaining solution accuracy,
striking an optimal balance between computational efficiency and solution
fidelity.

</details>


### [550] [Towards Fundamental Limits for Active Multi-distribution Learning](https://arxiv.org/pdf/2506.17607)
*Chicheng Zhang, Yihan Zhou*

Main category: cs.LG

TL;DR: The paper focuses on active multi-distribution learning, improving label complexity bounds in realizable and agnostic settings, and proving optimality for some cases.


<details>
  <summary>Details</summary>
Motivation: Multi-distribution learning is crucial for collaborative learning, fairness, and robustness, but active learning in this context lacks optimal algorithms.

Method: Develops new algorithms for active multi-distribution learning, analyzing label complexity in distribution-dependent and distribution-free settings.

Result: Achieves improved bounds, including an optimal one in the realizable setting, and identifies fundamental terms in the agnostic setting.

Conclusion: The work advances understanding of active multi-distribution learning, with implications for collaborative and robust learning.

Abstract: Multi-distribution learning extends agnostic Probably Approximately Correct
(PAC) learning to the setting in which a family of $k$ distributions,
$\{D_i\}_{i\in[k]}$, is considered and a classifier's performance is measured
by its error under the worst distribution. This problem has attracted a lot of
recent interests due to its applications in collaborative learning, fairness,
and robustness. Despite a rather complete picture of sample complexity of
passive multi-distribution learning, research on active multi-distribution
learning remains scarce, with algorithms whose optimality remaining unknown.
  In this paper, we develop new algorithms for active multi-distribution
learning and establish improved label complexity upper and lower bounds, in
distribution-dependent and distribution-free settings. Specifically, in the
near-realizable setting we prove an upper bound of
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\ln\frac{1}{\varepsilon}\Bigr)$ and
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\Bigl(\ln\frac{1}{\varepsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$
in the realizable and agnostic settings respectively, where $\theta_{\max}$ is
the maximum disagreement coefficient among the $k$ distributions, $d$ is the VC
dimension of the hypothesis class, $\nu$ is the multi-distribution error of the
best hypothesis, and $\varepsilon$ is the target excess error. Moreover, we
show that the bound in the realizable setting is information-theoretically
optimal and that the $k\nu/\varepsilon^2$ term in the agnostic setting is
fundamental for proper learners. We also establish instance-dependent sample
complexity bound for passive multidistribution learning that smoothly
interpolates between realizable and agnostic
regimes~\citep{blum2017collaborative,zhang2024optimal}, which may be of
independent interest.

</details>


### [551] [EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration](https://arxiv.org/pdf/2506.17615)
*Ibrahim Ahmed, Clemens Schaefer, Gil Tabak, Denis Vnukov, Zenong Zhang, Felix chern, Anatoliy Yevtushenko, Andy Davis*

Main category: cs.LG

TL;DR: EQuARX introduces a dynamic block-wise quantized AllReduce for TPUs, improving speed by 1.8X over BF16 AllReduce with minimal quality impact.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) face deployment challenges due to inter-device communication overhead. Quantizing collectives like AllReduce is difficult due to numerical instability.

Method: Developed EQuARX, a native dynamic block-wise quantized AllReduce within the XLA compiler for TPUs, using TPU-friendly quantization and deep pipelining.

Result: Achieved 1.8X speedup over BF16 AllReduce, and accelerated prefill stages of Gemma 3 models (27B by 1.25X, 12B by 1.1X) with negligible quality impact.

Conclusion: EQuARX effectively reduces communication overhead in LLM deployment, enhancing performance without compromising model quality.

Abstract: While Large Language Models (LLMs) have become highly influential, their
enormous scale presents significant deployment challenges. Efficiently serving
these models typically requires distributing them across numerous accelerator
devices, which introduces substantial performance overhead from inter-device
communication (collectives). While model quantization has been widely adopted
to reduce the memory and compute requirements of LLM weights and activations
with minimal quality impact, applying quantization directly to collectives like
AllReduce is inherently difficult due to the inter-device summation involved,
which can lead to numerical instability or significant error accumulation. In
this work, we present a native dynamic block-wise efficient quantized AllReduce
within the XLA compiler for TPUs (EQuARX). By using TPU-friendly quantization
and deep pipelining of communication and compute, EQuARX with int8 precision
achieves a 1.8X speedup over baseline BF16 AllReduce across various network
topologies. Furthermore, EQuARX accelerates the prefill stage of Gemma 3 27B by
1.25X and Gemma 3 12B by 1.1X, respectively, with small to negligible impact on
quality.

</details>


### [552] [Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care via Medical Literature Validation](https://arxiv.org/pdf/2506.17620)
*Minh Le, Khoi Ton*

Main category: cs.LG

TL;DR: The paper proposes deep learning models for predicting chronic disease risk using personal and lifestyle factors, validated with SHAP-based explainability and medical literature.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing models (reliance on medical test data, lack of validated explanations) and enable trustworthy self-assessment tools.

Method: Developed deep learning models predicting 13 chronic diseases using personal/lifestyle factors, validated with SHAP explanations against medical literature.

Result: Strong alignment between model features and medical literature, indicating trustworthiness across 13 diseases.

Conclusion: The approach provides a foundation for trustworthy self-directed preventive care, with future work needed on ethical use and additional trustworthiness methods.

Abstract: Chronic diseases are long-term, manageable, yet typically incurable
conditions, highlighting the need for effective preventive strategies. Machine
learning has been widely used to assess individual risk for chronic diseases.
However, many models rely on medical test data (e.g. blood results, glucose
levels), which limits their utility for proactive self-assessment.
Additionally, to gain public trust, machine learning models should be
explainable and transparent. Although some research on self-assessment machine
learning models includes explainability, their explanations are not validated
against established medical literature, reducing confidence in their
reliability. To address these issues, we develop deep learning models that
predict the risk of developing 13 chronic diseases using only personal and
lifestyle factors, enabling accessible, self-directed preventive care.
Importantly, we use SHAP-based explainability to identify the most influential
model features and validate them against established medical literature. Our
results show a strong alignment between the models' most influential features
and established medical literature, reinforcing the models' trustworthiness.
Critically, we find that this observation holds across 13 distinct diseases,
indicating that this machine learning approach can be broadly trusted for
chronic disease prediction. This work lays the foundation for developing
trustworthy machine learning tools for self-directed preventive care. Future
research can explore other approaches for models' trustworthiness and discuss
how the models can be used ethically and responsibly.

</details>


### [553] [Exploiting Efficiency Vulnerabilities in Dynamic Deep Learning Systems](https://arxiv.org/pdf/2506.17621)
*Ravishka Rathnasuriya, Wei Yang*

Main category: cs.LG

TL;DR: The paper explores security risks in dynamic deep learning systems (DDLSs) due to input-dependent execution, highlighting efficiency vulnerabilities exploitable by adversarial inputs. It surveys attacks, identifies gaps, and proposes defenses.


<details>
  <summary>Details</summary>
Motivation: The need for efficient deep learning inference under latency/resource constraints has led to DDLSs, but their dynamic nature introduces underexplored security risks, especially efficiency vulnerabilities.

Method: The work investigates security implications of DDLSs, surveys attack strategies, identifies gaps in defenses, and proposes targeted solutions.

Result: Current DDLSs expose efficiency vulnerabilities to adversarial inputs, with gaps in attack coverage and defense mechanisms.

Conclusion: The paper advocates for examining efficiency attacks on DDLSs and developing targeted defenses to ensure robustness.

Abstract: The growing deployment of deep learning models in real-world environments has
intensified the need for efficient inference under strict latency and resource
constraints. To meet these demands, dynamic deep learning systems (DDLSs) have
emerged, offering input-adaptive computation to optimize runtime efficiency.
While these systems succeed in reducing cost, their dynamic nature introduces
subtle and underexplored security risks. In particular, input-dependent
execution pathways create opportunities for adversaries to degrade efficiency,
resulting in excessive latency, energy usage, and potential denial-of-service
in time-sensitive deployments. This work investigates the security implications
of dynamic behaviors in DDLSs and reveals how current systems expose efficiency
vulnerabilities exploitable by adversarial inputs. Through a survey of existing
attack strategies, we identify gaps in the coverage of emerging model
architectures and limitations in current defense mechanisms. Building on these
insights, we propose to examine the feasibility of efficiency attacks on modern
DDLSs and develop targeted defenses to preserve robustness under adversarial
conditions.

</details>


### [554] [LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting](https://arxiv.org/pdf/2506.17631)
*Zesen Wang, Yonggang Li, Lijuan Lan*

Main category: cs.LG

TL;DR: LLM-Prompt is a framework for time series forecasting using large language models (LLMs), addressing shortcomings like lack of unified textual prompts and modality discrepancies.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for time series forecasting lack a unified prompt paradigm and ignore modality gaps between text and time series data.

Method: Proposes LLM-Prompt with learnable soft prompts, textualized hard prompts, and cross-modal semantic alignment for fusion of temporal and textual data.

Result: Demonstrates effectiveness on 6 public and 3 carbon emission datasets.

Conclusion: LLM-Prompt is a powerful framework for improving time series forecasting with LLMs.

Abstract: Time series forecasting aims to model temporal dependencies among variables
for future state inference, holding significant importance and widespread
applications in real-world scenarios. Although deep learning-based methods have
achieved remarkable progress, they still exhibit suboptimal performance in
long-term forecasting and data-scarce scenarios. Recent research demonstrates
that large language models (LLMs) achieve promising performance in time series
forecasting. However, we find existing LLM-based methods still have
shortcomings: (1) the absence of a unified paradigm for textual prompt
formulation and (2) the neglect of modality discrepancies between textual
prompts and time series. To address this, we propose LLM-Prompt, an LLM-based
time series forecasting framework integrating multi-prompt information and
cross-modal semantic alignment. Specifically, we first construct a unified
textual prompt paradigm containing learnable soft prompts and textualized hard
prompts. Second, to enhance LLMs' comprehensive understanding of the
forecasting task, we design a semantic space embedding and cross-modal
alignment module to achieve cross-modal fusion of temporal and textual
information. Finally, the transformed time series from the LLMs are projected
to obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3
carbon emission datasets demonstrate that LLM-Prompt is a powerful framework
for time series forecasting.

</details>


### [555] [Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution](https://arxiv.org/pdf/2506.17670)
*Manhin Poon, XiangXiang Dai, Xutong Liu, Fang Kong, John C. S. Lui, Jinhang Zuo*

Main category: cs.LG

TL;DR: A contextual bandit framework for adaptive multi-LLM selection in online settings, addressing unstructured prompt dynamics and achieving sublinear regret.


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting the most suitable LLM for user queries due to diverse behaviors, costs, and strengths, especially in dynamic, unstructured contexts.

Method: Proposes a LinUCB-based algorithm for sequential LLM selection, with budget-aware and positionally-aware extensions to handle variable costs and user preferences.

Result: Outperforms existing LLM routing strategies in accuracy and cost-efficiency on diverse benchmarks.

Conclusion: Contextual bandits are effective for real-time, adaptive LLM selection without offline fine-tuning.

Abstract: Large language models (LLMs) exhibit diverse response behaviors, costs, and
strengths, making it challenging to select the most suitable LLM for a given
user query. We study the problem of adaptive multi-LLM selection in an online
setting, where the learner interacts with users through multi-step query
refinement and must choose LLMs sequentially without access to offline datasets
or model internals. A key challenge arises from unstructured context evolution:
the prompt dynamically changes in response to previous model outputs via a
black-box process, which cannot be simulated, modeled, or learned. To address
this, we propose the first contextual bandit framework for sequential LLM
selection under unstructured prompt dynamics. We formalize a notion of myopic
regret and develop a LinUCB-based algorithm that provably achieves sublinear
regret without relying on future context prediction. We further introduce
budget-aware and positionally-aware (favoring early-stage satisfaction)
extensions to accommodate variable query costs and user preferences for early
high-quality responses. Our algorithms are theoretically grounded and require
no offline fine-tuning or dataset-specific training. Experiments on diverse
benchmarks demonstrate that our methods outperform existing LLM routing
strategies in both accuracy and cost-efficiency, validating the power of
contextual bandits for real-time, adaptive LLM selection.

</details>


### [556] [Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks](https://arxiv.org/pdf/2506.17672)
*Weiming Mai, Jie Gao, Oded Cats*

Main category: cs.LG

TL;DR: The paper introduces a hypernetwork and ensemble learning method to predict ride-hailing drivers' decisions, addressing non-linear interactions and personal preferences better than traditional models.


<details>
  <summary>Details</summary>
Motivation: Traditional models like RUM fail to capture non-linear interactions and personalized driver preferences, limiting prediction accuracy in ride-hailing systems.

Method: Uses hypernetworks to dynamically generate weights for linear utility functions, combined with ensemble learning for adaptability and reduced overfitting.

Result: The model improves prediction accuracy and uncertainty estimation, balancing explainability and revealing personalized driver preferences.

Conclusion: The ensemble hypernetwork approach outperforms traditional models, offering insights into driver decision-making and enhancing system efficiency.

Abstract: In ride-hailing systems, drivers decide whether to accept or reject ride
requests based on factors such as order characteristics, traffic conditions,
and personal preferences. Accurately predicting these decisions is essential
for improving the efficiency and reliability of these systems. Traditional
models, such as the Random Utility Maximization (RUM) approach, typically
predict drivers' decisions by assuming linear correlations among attributes.
However, these models often fall short because they fail to account for
non-linear interactions between attributes and do not cater to the unique,
personalized preferences of individual drivers. In this paper, we develop a
method for learning personalized utility functions using hypernetwork and
ensemble learning. Hypernetworks dynamically generate weights for a linear
utility function based on trip request data and driver profiles, capturing the
non-linear relationships. An ensemble of hypernetworks trained on different
data segments further improve model adaptability and generalization by
introducing controlled randomness, thereby reducing over-fitting. We validate
the performance of our ensemble hypernetworks model in terms of prediction
accuracy and uncertainty estimation in a real-world dataset. The results
demonstrate that our approach not only accurately predicts each driver's
utility but also effectively balances the needs for explainability and
uncertainty quantification. Additionally, our model serves as a powerful tool
for revealing the personalized preferences of different drivers, clearly
illustrating which attributes largely impact their rider acceptance decisions.

</details>


### [557] [FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies](https://arxiv.org/pdf/2506.17673)
*Seonglae Cho, Harryn Oh, Donghyun Lee, Luis Eduardo Rodrigues Vieira, Andrew Bermingham, Ziad El Sayed*

Main category: cs.LG

TL;DR: FaithfulSAE trains SAEs on synthetic datasets to improve stability and feature accuracy, outperforming traditional SAEs trained on external data.


<details>
  <summary>Details</summary>
Motivation: Address instability and inaccuracy in SAEs caused by training on out-of-distribution external datasets.

Method: Propose FaithfulSAE, which trains SAEs on the model's own synthetic dataset to avoid OOD issues.

Result: FaithfulSAEs show better stability across seeds, outperform web-based SAEs in probing tasks, and reduce Fake Feature Ratio in most models.

Conclusion: FaithfulSAE eliminates reliance on external datasets, enhancing interpretability by capturing model-internal features more accurately.

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising solution for
decomposing large language model representations into interpretable features.
However, Paulo and Belrose (2025) have highlighted instability across different
initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not
capture model-internal features. These problems likely stem from training SAEs
on external datasets - either collected from the Web or generated by another
model - which may contain out-of-distribution (OOD) data beyond the model's
generalisation capabilities. This can result in hallucinated SAE features,
which we term "Fake Features", that misrepresent the model's internal
activations. To address these issues, we propose FaithfulSAE, a method that
trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we
demonstrate that training SAEs on less-OOD instruction datasets results in SAEs
being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained
on web-based datasets in the SAE probing task and exhibit a lower Fake Feature
Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on
external datasets, advancing interpretability by better capturing
model-internal features while highlighting the often neglected importance of
SAE training datasets.

</details>


### [558] [Enhancing Stress-Strain Predictions with Seq2Seq and Cross-Attention based on Small Punch Test](https://arxiv.org/pdf/2506.17680)
*Zhengni Yang, Rui Yang, Weijian Han, Qixin Liu*

Main category: cs.LG

TL;DR: A deep-learning method using GAF and Seq2Seq with LSTM predicts stress-strain curves from SPT data, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of predicting true stress-strain curves in materials science, avoiding traditional experimental limitations.

Method: Transforms load-displacement data into images via GAF, then uses a Seq2Seq model with LSTM and multi-head cross-attention for prediction.

Result: Achieves mean absolute errors between 0.15 MPa and 5.58 MPa, showing superior accuracy.

Conclusion: The method is a promising alternative to traditional techniques, enhancing prediction accuracy and efficiency.

Abstract: This paper introduces a novel deep-learning approach to predict true
stress-strain curves of high-strength steels from small punch test (SPT)
load-displacement data. The proposed approach uses Gramian Angular Field (GAF)
to transform load-displacement sequences into images, capturing
spatial-temporal features and employs a Sequence-to-Sequence (Seq2Seq) model
with an LSTM-based encoder-decoder architecture, enhanced by multi-head
cross-attention to improved accuracy. Experimental results demonstrate that the
proposed approach achieves superior prediction accuracy, with minimum and
maximum mean absolute errors of 0.15 MPa and 5.58 MPa, respectively. The
proposed method offers a promising alternative to traditional experimental
techniques in materials science, enhancing the accuracy and efficiency of true
stress-strain relationship predictions.

</details>


### [559] [CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition](https://arxiv.org/pdf/2506.17709)
*Zebin Wang, Menghan Lin, Bolin Shen, Ken Anderson, Molei Liu, Tianxi Cai, Yushun Dong*

Main category: cs.LG

TL;DR: The paper evaluates GNN vulnerability to model extraction attacks (MEAs) and proposes a node querying strategy for efficient GNN acquisition in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: GNNs are widely used but vulnerable to MEAs, and ethical, cost-effective model acquisition is needed for research, especially where labeling is expensive.

Method: An iterative node querying strategy is developed, refining selection over cycles using historical feedback, tested under strict query-size limits.

Result: The method outperforms baselines in accuracy, fidelity, and F1 score, showing GNN susceptibility to MEAs and ethical acquisition potential.

Conclusion: The work highlights GNN security risks and offers a practical solution for efficient, ethical model acquisition in resource-limited research.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable utility across
diverse applications, and their growing complexity has made Machine Learning as
a Service (MLaaS) a viable platform for scalable deployment. However, this
accessibility also exposes GNN to serious security threats, most notably model
extraction attacks (MEAs), in which adversaries strategically query a deployed
model to construct a high-fidelity replica. In this work, we evaluate the
vulnerability of GNNs to MEAs and explore their potential for cost-effective
model acquisition in non-adversarial research settings. Importantly, adaptive
node querying strategies can also serve a critical role in research,
particularly when labeling data is expensive or time-consuming. By selectively
sampling informative nodes, researchers can train high-performing GNNs with
minimal supervision, which is particularly valuable in domains such as
biomedicine, where annotations often require expert input. To address this, we
propose a node querying strategy tailored to a highly practical yet
underexplored scenario, where bulk queries are prohibited, and only a limited
set of initial nodes is available. Our approach iteratively refines the node
selection mechanism over multiple learning cycles, leveraging historical
feedback to improve extraction efficiency. Extensive experiments on benchmark
graph datasets demonstrate our superiority over comparable baselines on
accuracy, fidelity, and F1 score under strict query-size constraints. These
results highlight both the susceptibility of deployed GNNs to extraction
attacks and the promise of ethical, efficient GNN acquisition methods to
support low-resource research environments.

</details>


### [560] [Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains](https://arxiv.org/pdf/2506.17718)
*Zhuo He, Shuang Li, Wenze Song, Longhui Yuan, Jian Liang, Han Li, Kun Gai*

Main category: cs.LG

TL;DR: The paper introduces SYNC, a method for evolving domain generalization (EDG) that addresses spurious correlations by learning time-aware causal representations using a structural causal model (SCM) and sequential VAE framework.


<details>
  <summary>Details</summary>
Motivation: To improve model generalization in dynamic scenarios by capturing evolving patterns and avoiding spurious correlations in existing EDG methods.

Method: Proposes SYNC, which integrates information-theoretic objectives into a sequential VAE framework to learn time-aware causal representations, preserving intra-class compactness of causal factors.

Result: SYNC achieves superior temporal generalization performance on synthetic and real-world datasets.

Conclusion: SYNC effectively learns causal representations for evolving domains, providing optimal causal predictors for each time domain.

Abstract: Endowing deep models with the ability to generalize in dynamic scenarios is
of vital significance for real-world deployment, given the continuous and
complex changes in data distribution. Recently, evolving domain generalization
(EDG) has emerged to address distribution shifts over time, aiming to capture
evolving patterns for improved model generalization. However, existing EDG
methods may suffer from spurious correlations by modeling only the dependence
between data and targets across domains, creating a shortcut between
task-irrelevant factors and the target, which hinders generalization. To this
end, we design a time-aware structural causal model (SCM) that incorporates
dynamic causal factors and the causal mechanism drifts, and propose
\textbf{S}tatic-D\textbf{YN}amic \textbf{C}ausal Representation Learning
(\textbf{SYNC}), an approach that effectively learns time-aware causal
representations. Specifically, it integrates specially designed
information-theoretic objectives into a sequential VAE framework which captures
evolving patterns, and produces the desired representations by preserving
intra-class compactness of causal factors both across and within domains.
Moreover, we theoretically show that our method can yield the optimal causal
predictor for each time domain. Results on both synthetic and real-world
datasets exhibit that SYNC can achieve superior temporal generalization
performance.

</details>


### [561] [Physics-informed mixture of experts network for interpretable battery degradation trajectory computation amid second-life complexities](https://arxiv.org/pdf/2506.17755)
*Xinghao Huang, Shengyu Tao, Chen Liang, Jiawei Chen, Junzhe Shi, Yuqi Li, Bizhong Xia, Guangmin Zhou, Xuan Zhang*

Main category: cs.LG

TL;DR: A Physics-Informed Mixture of Experts (PIMOE) network is proposed to predict battery degradation trajectories using partial field-accessible signals, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Retired EV batteries can support low-carbon energy systems, but degradation uncertainties and data inaccessibility hinder safe, scalable deployment.

Method: PIMOE combines adaptive multi-degradation prediction with expert weight synthesis and a use-dependent recurrent network for long-term trajectory prediction.

Result: Validated on 207 batteries, PIMOE achieves 0.88% MAPE, 0.43 ms inference time, and outperforms state-of-the-art models in speed and accuracy.

Conclusion: PIMOE provides a deployable, history-free solution for battery degradation prediction, enhancing second-life energy storage integration.

Abstract: Retired electric vehicle batteries offer immense potential to support
low-carbon energy systems, but uncertainties in their degradation behavior and
data inaccessibilities under second-life use pose major barriers to safe and
scalable deployment. This work proposes a Physics-Informed Mixture of Experts
(PIMOE) network that computes battery degradation trajectories using partial,
field-accessible signals in a single cycle. PIMOE leverages an adaptive
multi-degradation prediction module to classify degradation modes using expert
weight synthesis underpinned by capacity-voltage and relaxation data, producing
latent degradation trend embeddings. These are input to a use-dependent
recurrent network for long-term trajectory prediction. Validated on 207
batteries across 77 use conditions and 67,902 cycles, PIMOE achieves an average
mean absolute percentage (MAPE) errors of 0.88% with a 0.43 ms inference time.
Compared to the state-of-the-art Informer and PatchTST, it reduces
computational time and MAPE by 50%, respectively. Compatible with random state
of charge region sampling, PIMOE supports 150-cycle forecasts with 1.50%
average and 6.26% maximum MAPE, and operates effectively even with pruned 5MB
training data. Broadly, PIMOE framework offers a deployable, history-free
solution for battery degradation trajectory computation, redefining how
second-life energy storage systems are assessed, optimized, and integrated into
the sustainable energy landscape.

</details>


### [562] [Towards a Unified Textual Graph Framework for Spectral Reasoning via Physical and Chemical Information Fusion](https://arxiv.org/pdf/2506.17761)
*Jiheng Liang, Ziru Yu, Zujie Xie, Yuchen Guo, Yulan Guo, Xiangyang Yu*

Main category: cs.LG

TL;DR: A novel multi-modal spectral analysis framework integrates knowledge graphs and LLMs for flexible, interpretable, and generalizable spectral understanding, achieving high performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current spectral analysis methods, such as single-modality reliance, poor generalizability, and interpretability.

Method: Transform raw spectra into Textual Graphs (TAGs), merge with prior knowledge, use Prompt Nodes for LLM reasoning, and process with a Graph Neural Network.

Result: Consistently high performance in node-, edge-, and graph-level tasks, with robust generalization in zero-shot and few-shot settings.

Conclusion: Establishes a scalable, interpretable foundation for LLM-driven spectral analysis, unifying physical and chemical modalities.

Abstract: Motivated by the limitations of current spectral analysis methods-such as
reliance on single-modality data, limited generalizability, and poor
interpretability-we propose a novel multi-modal spectral analysis framework
that integrates prior knowledge graphs with Large Language Models. Our method
explicitly bridges physical spectral measurements and chemical structural
semantics by representing them in a unified Textual Graph format, enabling
flexible, interpretable, and generalizable spectral understanding. Raw spectra
are first transformed into TAGs, where nodes and edges are enriched with
textual attributes describing both spectral properties and chemical context.
These are then merged with relevant prior knowledge-including functional groups
and molecular graphs-to form a Task Graph that incorporates "Prompt Nodes"
supporting LLM-based contextual reasoning. A Graph Neural Network further
processes this structure to complete downstream tasks. This unified design
enables seamless multi-modal integration and automated feature decoding with
minimal manual annotation. Our framework achieves consistently high performance
across multiple spectral analysis tasks, including node-level, edge-level, and
graph-level classification. It demonstrates robust generalization in both
zero-shot and few-shot settings, highlighting its effectiveness in learning
from limited data and supporting in-context reasoning. This work establishes a
scalable and interpretable foundation for LLM-driven spectral analysis,
unifying physical and chemical modalities for scientific applications.

</details>


### [563] [Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large Networks](https://arxiv.org/pdf/2506.17768)
*Keigo Nishida, Eren Mehmet Kral, Kenichi Bannai, Mohammad Emtiyaz Khan, Thomas Mllenhoff*

Main category: cs.LG

TL;DR: The paper proposes a Log-Normal Multiplicative Dynamics (LMD) algorithm for artificial neural networks, inspired by biological synapses, achieving stable low-precision training.


<details>
  <summary>Details</summary>
Motivation: Biological synapses exhibit log-normal distributions and stable functioning under noisy conditions, prompting exploration of similar multiplicative training in artificial networks.

Method: Derived a Bayesian learning rule with log-normal posterior distributions, leading to the LMD algorithm, which uses multiplicative updates with noise and regularization.

Result: LMD enables stable and accurate training-from-scratch under low-precision conditions for Vision Transformer and GPT-2.

Conclusion: Multiplicative dynamics, inspired by biology, may enhance stable low-precision learning for energy-efficient hardware.

Abstract: Studies in neuroscience have shown that biological synapses follow a
log-normal distribution whose transitioning can be explained by noisy
multiplicative dynamics. Biological networks can function stably even under
dynamically fluctuating conditions arising due to unreliable synaptic
transmissions. Here we ask: Is it possible to design similar multiplicative
training in artificial neural networks? To answer this question, we derive a
Bayesian learning rule that assumes log-normal posterior distributions over
weights which gives rise to a new Log-Normal Multiplicative Dynamics (LMD)
algorithm. The algorithm uses multiplicative updates with both noise and
regularization applied multiplicatively. The method is as easy to implement as
Adam and only requires one additional vector to store. Our results show that
LMD achieves stable and accurate training-from-scratch under low-precision
forward operations for Vision Transformer and GPT-2. These results suggest that
multiplicative dynamics, a biological feature, may enable stable low-precision
inference and learning on future energy-efficient hardware.

</details>


### [564] [PhysiX: A Foundation Model for Physics Simulations](https://arxiv.org/pdf/2506.17774)
*Tung Nguyen, Arsh Koneru, Shufan Li, Aditya grover*

Main category: cs.LG

TL;DR: PhysiX is a 4.5B parameter foundation model for physics simulation, addressing data scarcity and outperforming task-specific baselines.


<details>
  <summary>Details</summary>
Motivation: Foundation models excel in video, image, and language but lag in physics simulation due to data scarcity and scale variability.

Method: PhysiX uses a discrete tokenizer and autoregressive generative model with a refinement module to mitigate discretization errors.

Result: PhysiX outperforms task-specific baselines and achieves state-of-the-art performance on The Well benchmark.

Conclusion: Knowledge from natural videos can transfer to physics simulation, and joint training enables synergistic learning.

Abstract: Foundation models have achieved remarkable success across video, image, and
language domains. By scaling up the number of parameters and training datasets,
these models acquire generalizable world knowledge and often surpass
task-specific approaches. However, such progress has yet to extend to the
domain of physics simulation. A primary bottleneck is data scarcity: while
millions of images, videos, and textual resources are readily available on the
internet, the largest physics simulation datasets contain only tens of
thousands of samples. This data limitation hinders the use of large models, as
overfitting becomes a major concern. As a result, physics applications
typically rely on small models, which struggle with long-range prediction due
to limited context understanding. Additionally, unlike images, videos, or
text-which typically exhibit fixed granularity-physics datasets often vary
drastically in scale, amplifying the challenges of scaling up multitask
training. We introduce PhysiX, the first large-scale foundation model for
physics simulation. PhysiX is a 4.5B parameter autoregressive generative model.
It uses a discrete tokenizer to encode physical processes at different scales
into a sequence of discrete tokens, and employs an autoregressive next-token
prediction objective to model such processes in the token space. To mitigate
the rounding error in the discretization process, PhysiX incorporates a
specialized refinement module. Through extensive experiments, we show that
PhysiX effectively addresses the data bottleneck, outperforming task-specific
baselines under comparable settings as well as the previous absolute
state-of-the-art approaches on The Well benchmark. Our results indicate that
knowledge learned from natural videos can be successfully transferred to
physics simulation, and that joint training across diverse simulation tasks
enables synergistic learning.

</details>


### [565] [Machine Learning Model Integration with Open World Temporal Logic for Process Automation](https://arxiv.org/pdf/2506.17776)
*Dyuman Aditya, Colton Payne, Mario Leiva, Paulo Shakarian*

Main category: cs.LG

TL;DR: The paper introduces a method to integrate ML model outputs with PyReason, a temporal logic reasoning engine, for adaptive decision-making in complex workflows.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of translating ML model outputs into actionable decisions within operational workflows.

Method: Integration of ML model outputs with PyReason, using generalized annotated logic to handle real-valued outputs as truth intervals, enabling dynamic reasoning.

Result: A system combining ML's perceptual strengths with PyReason's logical deduction, supporting temporal reasoning and explainability.

Conclusion: The integration offers a powerful tool for automating complex processes across domains like manufacturing, healthcare, and business.

Abstract: Recent advancements in Machine Learning (ML) have yielded powerful models
capable of extracting structured information from diverse and complex data
sources. However, a significant challenge lies in translating these perceptual
or extractive outputs into actionable, reasoned decisions within complex
operational workflows. To address these challenges, this paper introduces a
novel approach that integrates the outputs from various machine learning models
directly with the PyReason framework, an open-world temporal logic programming
reasoning engine. PyReason's foundation in generalized annotated logic allows
for the seamless incorporation of real-valued outputs (e.g., probabilities,
confidence scores) from diverse ML models, treating them as truth intervals
within its logical framework. Crucially, PyReason provides mechanisms,
implemented in Python, to continuously poll ML model outputs, convert them into
logical facts, and dynamically recompute the minimal model, ensuring real-tine
adaptive decision-making. Furthermore, its native support for temporal
reasoning, knowledge graph integration, and fully explainable interface traces
enables sophisticated analysis over time-sensitive process data and existing
organizational knowledge. By combining the strengths of perception and
extraction from ML models with the logical deduction and transparency of
PyReason, we aim to create a powerful system for automating complex processes.
This integration finds utility across numerous domains, including
manufacturing, healthcare, and business operations.

</details>


### [566] [Toward Autonomous UI Exploration: The UIExplorer Benchmark](https://arxiv.org/pdf/2506.17779)
*Andrei Cristian Nica, Akshaya Vishnu Kudlu Shanbhogue, Harshil Shah, Aleix Cambray, Tudor Berariu, Lucas Maystre, David Barber*

Main category: cs.LG

TL;DR: UIExplore-Bench is the first benchmark for evaluating UI exploration in autonomous agents, measuring their ability to discover actionable components using structured or screen-based inputs.


<details>
  <summary>Details</summary>
Motivation: There's a lack of systematic evaluation for UI exploration, a critical phase for reliable task-solving in autonomous agents.

Method: Agents are tested in Structured (DOM access) or Screen (GUI-only) modes across three levels in a GitLab sandbox, using the hUFO metric to quantify exploration effectiveness.

Result: UIExplore-AlGo leads with 77.2% (Structured) and 59.0% (Screen) of human performance at 2,000 steps, excelling in Sparse level.

Conclusion: The benchmark reveals a performance gap between agents and humans, offering tools to advance UI exploration research and applications.

Abstract: Autonomous agents must know how to explore user interfaces (UIs) for reliable
task solving, yet systematic evaluation of this crucial phase is lacking. We
introduce UIExplore-Bench, the first benchmark explicitly dedicated to UI
exploration. The benchmark evaluates agents with either Structured mode
(granting access to layout information like DOM trees) or Screen mode (relying
on GUI-only observations such as screenshots and human-like mouse/keyboard
interactions) across three levels in a standardized GitLab sandbox environment.
We formalize exploration as the process of maximizing the set of actionable UI
components discovered and propose a metric, human-normalized UI-Functionalities
Observed (hUFO), to quantify the effectiveness of exploration. Our results show
that UIExplore-AlGo achieves the leading mean hUFO scores, reaching up to 77.2%
of human performance in Structured mode and 59.0% in Screen mode at 2,000
steps, particularly excelling at the Sparse level. The results highlight the
relevance of our benchmark, as current agents show a substantial performance
gap compared to one hour of human expert exploration, indicating ample room for
future advancements. We publicly release the benchmark environment, an
exploration dataset, and an evaluation suite to catalyze research into
efficient UI exploration strategies and their downstream applications, such as
experience-driven task completion and automated training data generation.

</details>


### [567] [Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models](https://arxiv.org/pdf/2506.17781)
*Miguel Romero, Shuoyang Ding, Corey D. Barret, Georgiana Dinu, George Karypis*

Main category: cs.LG

TL;DR: The paper introduces Mixture of Task Experts (MoTE) to overcome limitations of instruction-conditioning in low-capacity models, achieving significant performance gains in retrieval and other tasks without extra costs.


<details>
  <summary>Details</summary>
Motivation: Address the representational constraints of instruction-conditioning in low-capacity models for embedding specialization.

Method: Proposes MoTE transformer block with Task-Aware Contrastive Learning (TACL) to train task-specialized parameters.

Result: MoTE achieves 64% higher gains in retrieval datasets (+3.27  +5.21) and 43% higher gains overall (+1.81  +2.60).

Conclusion: MoTE enhances embedding specialization without changing instructions, data, inference time, or active parameters.

Abstract: Dense embeddings are fundamental to modern machine learning systems, powering
Retrieval-Augmented Generation (RAG), information retrieval, and representation
learning. While instruction-conditioning has become the dominant approach for
embedding specialization, its direct application to low-capacity models imposes
fundamental representational constraints that limit the performance gains
derived from specialization. In this paper, we analyze these limitations and
introduce the Mixture of Task Experts (MoTE) transformer block, which leverages
task-specialized parameters trained with Task-Aware Contrastive Learning
(\tacl) to enhance the model ability to generate specialized embeddings.
Empirical results show that MoTE achieves $64\%$ higher performance gains in
retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance
gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains
are achieved without altering instructions, training data, inference time, or
number of active parameters.

</details>


### [568] [SING: SDE Inference via Natural Gradients](https://arxiv.org/pdf/2506.17796)
*Amber Hu, Henry Smith, Scott Linderman*

Main category: cs.LG

TL;DR: SING (SDE Inference via Natural Gradients) is a new method for efficient and stable variational inference in latent SDE models, outperforming prior methods in state inference and drift estimation.


<details>
  <summary>Details</summary>
Motivation: Exact posterior inference in latent SDE models is intractable, and existing VI methods suffer from slow convergence and instability.

Method: SING uses natural gradient VI to exploit model geometry, approximates intractable integrals, and parallelizes computations.

Result: SING achieves faster, more reliable inference and better drift estimation, demonstrated on datasets including neural dynamics.

Conclusion: SING is a promising tool for accurate inference in complex dynamical systems with limited prior knowledge.

Abstract: Latent stochastic differential equation (SDE) models are important tools for
the unsupervised discovery of dynamical systems from data, with applications
ranging from engineering to neuroscience. In these complex domains, exact
posterior inference of the latent state path is typically intractable,
motivating the use of approximate methods such as variational inference (VI).
However, existing VI methods for inference in latent SDEs often suffer from
slow convergence and numerical instability. Here, we propose SDE Inference via
Natural Gradients (SING), a method that leverages natural gradient VI to
efficiently exploit the underlying geometry of the model and variational
posterior. SING enables fast and reliable inference in latent SDE models by
approximating intractable integrals and parallelizing computations in time. We
provide theoretical guarantees that SING will approximately optimize the
intractable, continuous-time objective of interest. Moreover, we demonstrate
that better state inference enables more accurate estimation of nonlinear drift
functions using, for example, Gaussian process SDE models. SING outperforms
prior methods in state inference and drift estimation on a variety of datasets,
including a challenging application to modeling neural dynamics in freely
behaving animals. Altogether, our results illustrate the potential of SING as a
tool for accurate inference in complex dynamical systems, especially those
characterized by limited prior knowledge and non-conjugate structure.

</details>


### [569] [Reimagining Parameter Space Exploration with Diffusion Models](https://arxiv.org/pdf/2506.17807)
*Lijun Zhang, Xiao Liu, Hui Guan*

Main category: cs.LG

TL;DR: A generative method using diffusion models to produce task-specific neural network parameters directly from task identity, eliminating fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To avoid time-consuming task-specific fine-tuning and reliance on labeled data by generating parameters directly.

Method: Use diffusion models to learn and synthesize task-specific parameters from task identifiers.

Result: Effective for seen tasks and multi-task interpolation but fails for unseen tasks.

Conclusion: Shows potential for parameter generation but has limitations in generalizing to new tasks.

Abstract: Adapting neural networks to new tasks typically requires task-specific
fine-tuning, which is time-consuming and reliant on labeled data. We explore a
generative alternative that produces task-specific parameters directly from
task identity, eliminating the need for task-specific training. To this end, we
propose using diffusion models to learn the underlying structure of effective
task-specific parameter space and synthesize parameters on demand. Once
trained, the task-conditioned diffusion model can generate specialized weights
directly from task identifiers. We evaluate this approach across three
scenarios: generating parameters for a single seen task, for multiple seen
tasks, and for entirely unseen tasks. Experiments show that diffusion models
can generate accurate task-specific parameters and support multi-task
interpolation when parameter subspaces are well-structured, but fail to
generalize to unseen tasks, highlighting both the potential and limitations of
this generative solution.

</details>


### [570] [Flatness After All?](https://arxiv.org/pdf/2506.17809)
*Neta Shoham, Liron Mor-Yosef, Haim Avron*

Main category: cs.LG

TL;DR: The paper proposes a soft rank measure of the Hessian to assess generalization in neural networks, showing it captures the generalization gap better than traditional flatness measures.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency in how flatness (measured by Hessian curvature) relates to generalization, especially in overparameterized networks.

Method: Introduces a soft rank measure of the Hessian for assessing flatness, validated on calibrated and non-calibrated neural network models.

Result: The proposed measure accurately captures the generalization gap for calibrated models and connects to Takeuchi Information Criterion for non-calibrated ones.

Conclusion: The soft rank measure provides a robust estimate of generalization, outperforming baseline methods.

Abstract: Recent literature has examined the relationship between the curvature of the
loss function at minima and generalization, mainly in the context of
overparameterized networks. A key observation is that "flat" minima tend to
generalize better than "sharp" minima. While this idea is supported by
empirical evidence, it has also been shown that deep networks can generalize
even with arbitrary sharpness, as measured by either the trace or the spectral
norm of the Hessian. In this paper, we argue that generalization could be
assessed by measuring flatness using a soft rank measure of the Hessian. We
show that when the common neural network model (neural network with exponential
family negative log likelihood loss) is calibrated, and its prediction error
and its confidence in the prediction are not correlated with the first and the
second derivatives of the network's output, our measure accurately captures the
asymptotic expected generalization gap. For non-calibrated models, we connect
our flatness measure to the well-known Takeuchi Information Criterion and show
that it still provides reliable estimates of generalization gaps for models
that are not overly confident. Experimental results indicate that our approach
offers a robust estimate of the generalization gap compared to baselines.

</details>


### [571] [Actionable Interpretability via Causal Hypergraphs: Unravelling Batch Size Effects in Deep Learning](https://arxiv.org/pdf/2506.17826)
*Zhongtian Sun, Anoushka Harit, Pietro Lio*

Main category: cs.LG

TL;DR: HGCNet uses hypergraphs and DSCMs to study how batch size affects generalization in graph/text domains, revealing smaller batches improve generalization via stochasticity and flatter minima.


<details>
  <summary>Details</summary>
Motivation: To explore the causal mechanisms of batch size on generalization in graph and text domains, which are underexplored compared to vision tasks.

Method: Introduces HGCNet, a hypergraph-based causal framework using deep structural causal models (DSCMs) to analyze batch size effects via gradient noise, minima sharpness, and model complexity.

Result: HGCNet outperforms baselines (GCN, GAT, PI-GNN, BERT, RoBERTa) and shows smaller batch sizes enhance generalization through increased stochasticity and flatter minima.

Conclusion: Smaller batch sizes causally improve generalization, offering interpretable insights for optimizing deep learning training strategies.

Abstract: While the impact of batch size on generalisation is well studied in vision
tasks, its causal mechanisms remain underexplored in graph and text domains. We
introduce a hypergraph-based causal framework, HGCNet, that leverages deep
structural causal models (DSCMs) to uncover how batch size influences
generalisation via gradient noise, minima sharpness, and model complexity.
Unlike prior approaches based on static pairwise dependencies, HGCNet employs
hypergraphs to capture higher-order interactions across training dynamics.
Using do-calculus, we quantify direct and mediated effects of batch size
interventions, providing interpretable, causally grounded insights into
optimisation. Experiments on citation networks, biomedical text, and e-commerce
reviews show that HGCNet outperforms strong baselines including GCN, GAT,
PI-GNN, BERT, and RoBERTa. Our analysis reveals that smaller batch sizes
causally enhance generalisation through increased stochasticity and flatter
minima, offering actionable interpretability to guide training strategies in
deep learning. This work positions interpretability as a driver of principled
architectural and optimisation choices beyond post hoc analysis.

</details>


### [572] [Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach](https://arxiv.org/pdf/2506.17828)
*Xinnan Zhang, Chenliang Li, Siliang Zeng, Jiaxiang Li, Zhongruo Wang, Kaixiang Lin, Songtao Lu, Alfredo Garcia, Mingyi Hong*

Main category: cs.LG

TL;DR: IRO is a reinforcement learning framework for aligning LLMs with human preferences without modifying model parameters, using iterative resampling and lightweight value functions.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RLHF and DPO require model weight updates, limiting their use during test time or with inaccessible weights. Test-time methods are costly and suboptimal due to imperfect reward functions.

Method: IRO iteratively samples candidates, resamples using value functions, and trains lightweight value functions to guide generation without updating model weights.

Result: IRO enables alignment of frozen base models without weight access, offering a practical alternative to RLHF and DPO.

Conclusion: IRO provides a flexible, lightweight solution for aligning LLMs at test time, addressing limitations of existing methods.

Abstract: Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.

</details>


### [573] [Causal Spherical Hypergraph Networks for Modelling Social Uncertainty](https://arxiv.org/pdf/2506.17840)
*Anoushka Harit, Zhongtian Sun*

Main category: cs.LG

TL;DR: Causal-SphHN is a framework for socially grounded prediction, modeling higher-order structure, directional influence, and epistemic uncertainty using hyperspherical embeddings and hyperedges. It outperforms baselines in accuracy, robustness, and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of human social behavior, which involves uncertainty, causality, and group dynamics, by proposing a unified causal-geometric approach.

Method: Represents individuals as hyperspherical embeddings and group contexts as hyperedges. Uses Shannon entropy for uncertainty and Granger-informed subgraphs for causal dependencies. Employs angular message-passing for information propagation.

Result: Outperforms baselines on SNARE, PHEME, and AMIGOS datasets in predictive accuracy, robustness, and calibration. Provides interpretable analysis of influence patterns and social ambiguity.

Conclusion: Causal-SphHN offers a unified approach for learning in dynamic social environments, combining causal and geometric principles for improved prediction and interpretability.

Abstract: Human social behaviour is governed by complex interactions shaped by
uncertainty, causality, and group dynamics. We propose Causal Spherical
Hypergraph Networks (Causal-SphHN), a principled framework for socially
grounded prediction that jointly models higher-order structure, directional
influence, and epistemic uncertainty. Our method represents individuals as
hyperspherical embeddings and group contexts as hyperedges, capturing semantic
and relational geometry. Uncertainty is quantified via Shannon entropy over von
Mises-Fisher distributions, while temporal causal dependencies are identified
using Granger-informed subgraphs. Information is propagated through an angular
message-passing mechanism that respects belief dispersion and directional
semantics. Experiments on SNARE (offline networks), PHEME (online discourse),
and AMIGOS (multimodal affect) show that Causal-SphHN improves predictive
accuracy, robustness, and calibration over strong baselines. Moreover, it
enables interpretable analysis of influence patterns and social ambiguity. This
work contributes a unified causal-geometric approach for learning under
uncertainty in dynamic social environments.

</details>


### [574] [A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity](https://arxiv.org/pdf/2506.17847)
*Cristian Del Gobbo*

Main category: cs.LG

TL;DR: The paper evaluates six synthetic data generators from SDV and Synthicity libraries for their performance in statistical similarity and predictive utility, finding Bayesian Network (Synthicity) best for fidelity and TVAE (SDV) for predictive tasks in high-output scenarios. SDV is noted for better usability.


<details>
  <summary>Details</summary>
Motivation: High-quality training data is essential for ML models, but real data is hard to obtain, especially for smaller organizations. Synthetic data generators offer a scalable and privacy-preserving alternative.

Method: Six generators (SDV: Gaussian Copula, CTGAN, TVAE; Synthicity: Bayesian Network, CTGAN, TVAE) were tested on a UCI dataset (Belgium energy/environmental data) under 1:1 and 1:10 input-output ratios. Evaluation used statistical similarity and predictive utility (Train on Synthetic, Test on Real).

Result: Statistical similarity was consistent, but predictive utility declined in the 1:10 case. Bayesian Network (Synthicity) had highest fidelity; TVAE (SDV) excelled in predictive tasks for 1:10. SDV was more user-friendly.

Conclusion: Synthetic data generators are viable, with trade-offs between fidelity and predictive utility. SDV's ease of use makes it practical for practitioners, though performance differences between libraries were minor.

Abstract: High-quality training data is critical to the performance of machine learning
models, particularly Large Language Models (LLMs). However, obtaining real,
high-quality data can be challenging, especially for smaller organizations and
early-stage startups. Synthetic data generators provide a promising solution by
replicating the statistical and structural properties of real data while
preserving privacy and scalability. This study evaluates the performance of six
tabular synthetic data generators from two widely used open-source libraries:
SDV (Gaussian Copula, CTGAN, TVAE) and Synthicity (Bayesian Network, CTGAN,
TVAE). Using a real-world dataset from the UCI Machine Learning Repository,
comprising energy consumption and environmental variables from Belgium, we
simulate a low-data regime by training models on only 1,000 rows. Each
generator is then tasked with producing synthetic datasets under two
conditions: a 1:1 (1,000 rows) and a 1:10 (10,000 rows) input-output ratio.
Evaluation is conducted using two criteria: statistical similarity, measured
via classical statistics and distributional metrics; and predictive utility,
assessed using a "Train on Synthetic, Test on Real" approach with four
regression models. While statistical similarity remained consistent across
models in both scenarios, predictive utility declined notably in the 1:10 case.
The Bayesian Network from Synthicity achieved the highest fidelity in both
scenarios, while TVAE from SDV performed best in predictive tasks under the
1:10 setting. Although no significant performance gap was found between the two
libraries, SDV stands out for its superior documentation and ease of use,
making it more accessible for practitioners.

</details>


### [575] [Pathway-based Progressive Inference (PaPI) for Energy-Efficient Continual Learning](https://arxiv.org/pdf/2506.17848)
*Suyash Gaurav, Jukka Heikkonen, Jatin Chaudhary*

Main category: cs.LG

TL;DR: PaPI is a novel framework for continual learning that optimizes pathway selection and adaptation to balance stability-plasticity and energy efficiency, outperforming existing methods like EWC and GEM.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting and energy inefficiency in continual learning, especially in resource-constrained environments.

Method: Formulates continual learning as an energy-constrained optimization problem, using pathway routing with formal convergence guarantees.

Result: Achieves O(K) improvement in stability-plasticity trade-off, tight bounds on forgetting rates, and energy efficiency scaling with active parameters.

Conclusion: PaPI is theoretically and experimentally validated as effective for continual learning in energy-constrained settings.

Abstract: Continual learning systems face the dual challenge of preventing catastrophic
forgetting while maintaining energy efficiency, particularly in
resource-constrained environments. This paper introduces Pathway-based
Progressive Inference (PaPI), a novel theoretical framework that addresses
these challenges through a mathematically rigorous approach to pathway
selection and adaptation. We formulate continual learning as an
energy-constrained optimization problem and provide formal convergence
guarantees for our pathway routing mechanisms. Our theoretical analysis
demonstrates that PaPI achieves an $\mathcal{O}(K)$ improvement in the
stability-plasticity trade-off compared to monolithic architectures, where $K$
is the number of pathways. We derive tight bounds on forgetting rates using
Fisher Information Matrix analysis and prove that PaPI's energy consumption
scales with the number of active parameters rather than the total model size.
Comparative theoretical analysis shows that PaPI provides stronger guarantees
against catastrophic forgetting than Elastic Weight Consolidation (EWC) while
maintaining better energy efficiency than both EWC and Gradient Episodic Memory
(GEM). Our experimental validation confirms these theoretical advantages across
multiple benchmarks, demonstrating PaPI's effectiveness for continual learning
in energy-constrained settings. Our codes are available at
https://github.com/zser092/PAPI_FILES.

</details>


### [576] [In-Context Learning Strategies Emerge Rationally](https://arxiv.org/pdf/2506.17859)
*Daniel Wurgaft, Ekdeep Singh Lubana, Core Francisco Park, Hidenori Tanaka, Gautam Reddy, Noah D. Goodman*

Main category: cs.LG

TL;DR: The paper unifies in-context learning (ICL) strategies by explaining them through a Bayesian framework, showing how models balance memorization and generalization based on task diversity and complexity.


<details>
  <summary>Details</summary>
Motivation: To understand why models adopt disparate ICL strategies when trained on mixed tasks, and to explain these behaviors through a rational, Bayesian lens.

Method: Develops a hierarchical Bayesian framework to predict Transformer behavior without accessing weights, treating pretraining as updating posterior probabilities of strategies.

Result: The framework accurately predicts model behavior, revealing a tradeoff between strategy loss and complexity, and predicts novel phenomena like superlinear memorization timescales.

Conclusion: The work provides a predictive and explanatory account of ICL, grounded in tradeoffs between strategy effectiveness and implementation complexity.

Abstract: Recent work analyzing in-context learning (ICL) has identified a broad set of
strategies that describe model behavior in different experimental conditions.
We aim to unify these findings by asking why a model learns these disparate
strategies in the first place. Specifically, we start with the observation that
when trained to learn a mixture of tasks, as is popular in the literature, the
strategies learned by a model for performing ICL can be captured by a family of
Bayesian predictors: a memorizing predictor, which assumes a discrete prior on
the set of seen tasks, and a generalizing predictor, wherein the prior matches
the underlying task distribution. Adopting the lens of rational analysis from
cognitive science, where a learner's behavior is explained as an optimal
adaptation to data given computational constraints, we develop a hierarchical
Bayesian framework that almost perfectly predicts Transformer next token
predictions throughout training without assuming access to its weights. Under
this framework, pretraining is viewed as a process of updating the posterior
probability of different strategies, and its inference-time behavior as a
posterior-weighted average over these strategies' predictions. Our framework
draws on common assumptions about neural network learning dynamics, which make
explicit a tradeoff between loss and complexity among candidate strategies:
beyond how well it explains the data, a model's preference towards implementing
a strategy is dictated by its complexity. This helps explain well-known ICL
phenomena, while offering novel predictions: e.g., we show a superlinear trend
in the timescale for transition to memorization as task diversity is increased.
Overall, our work advances an explanatory and predictive account of ICL
grounded in tradeoffs between strategy loss and complexity.

</details>


### [577] [NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN](https://arxiv.org/pdf/2506.17870)
*Jianhang Xie, Chuntao Ding, Xiaqing Li, Shenyuan Ren, Yidong Li, Zhichao Lu*

Main category: cs.LG

TL;DR: NestQuant introduces a resource-friendly post-training quantization method for IoT devices, enabling dynamic model switching without retraining or special hardware, reducing storage and overhead.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods lack adaptability to dynamic IoT resources and require multiple models, increasing storage and switching costs.

Method: NestQuant uses integer weight decomposition and nesting to optimize higher-bit weights, allowing dynamic switching between full-bit and part-bit models.

Result: NestQuant achieves high accuracy (e.g., 78.1% for ResNet-101 INT8 nesting INT6) and reduces switching overheads by ~78.1%.

Conclusion: NestQuant provides an efficient solution for deploying adaptable quantized DNNs on IoT devices, balancing performance and resource constraints.

Abstract: Deploying quantized deep neural network (DNN) models with resource adaptation
capabilities on ubiquitous Internet of Things (IoT) devices to provide
high-quality AI services can leverage the benefits of compression and meet
multi-scenario resource requirements. However, existing dynamic/mixed precision
quantization requires retraining or special hardware, whereas post-training
quantization (PTQ) has two limitations for resource adaptation: (i) The
state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes
it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying
multiple PTQ models with diverse bitwidths consumes large storage resources and
switching overheads. To this end, this paper introduces a resource-friendly
post-training integer-nesting quantization, i.e., NestQuant, for on-device
quantized model switching on IoT devices. The proposed NestQuant incorporates
the integer weight decomposition, which bit-wise splits quantized weights into
higher-bit and lower-bit weights of integer data types. It also contains a
decomposed weights nesting mechanism to optimize the higher-bit weights by
adaptive rounding and nest them into the original quantized weights. In
deployment, we can send and store only one NestQuant model and switch between
the full-bit/part-bit model by paging in/out lower-bit weights to adapt to
resource changes and reduce consumption. Experimental results on the
ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve
high performance in top-1 accuracy, and reduce in terms of data transmission,
storage consumption, and switching overheads. In particular, the ResNet-101
with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and
part-bit models, respectively, and reduce switching overheads by approximately
78.1% compared with diverse bitwidths PTQ models.

</details>


### [578] [Decoding Federated Learning: The FedNAM+ Conformal Revolution](https://arxiv.org/pdf/2506.17872)
*Sree Bhargavi Balija, Amitash Nanda, Debashis Sahoo*

Main category: cs.LG

TL;DR: FedNAM+ is a federated learning framework combining Neural Additive Models and conformal prediction for interpretable, reliable uncertainty estimation, validated on datasets like MNIST and CIFAR.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning frameworks lack solutions for uncertainty quantification, interpretability, and robustness.

Method: Integrates Neural Additive Models (NAMs) with conformal prediction, using gradient-based sensitivity maps for dynamic level adjustment and pixel-wise uncertainty estimates.

Result: Achieves high accuracy (e.g., 0.1% loss on MNIST) with transparent uncertainty measures, outperforming methods like Monte Carlo Dropout.

Conclusion: FedNAM+ enhances trust and transparency in decentralized predictive modeling with robustness, interpretability, and computational efficiency.

Abstract: Federated learning has significantly advanced distributed training of machine
learning models across decentralized data sources. However, existing frameworks
often lack comprehensive solutions that combine uncertainty quantification,
interpretability, and robustness. To address this, we propose FedNAM+, a
federated learning framework that integrates Neural Additive Models (NAMs) with
a novel conformal prediction method to enable interpretable and reliable
uncertainty estimation. Our method introduces a dynamic level adjustment
technique that utilizes gradient-based sensitivity maps to identify key input
features influencing predictions. This facilitates both interpretability and
pixel-wise uncertainty estimates. Unlike traditional interpretability methods
such as LIME and SHAP, which do not provide confidence intervals, FedNAM+
offers visual insights into prediction reliability. We validate our approach
through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high
prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with
transparent uncertainty measures. Visual analysis highlights variable
uncertainty intervals, revealing low-confidence regions where model performance
can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+
delivers efficient and global uncertainty estimates with reduced computational
overhead, making it particularly suitable for federated learning scenarios.
Overall, FedNAM+ provides a robust, interpretable, and computationally
efficient framework that enhances trust and transparency in decentralized
predictive modeling.

</details>


### [579] [Choice of Scoring Rules for Indirect Elicitation of Properties with Parametric Assumptions](https://arxiv.org/pdf/2506.17880)
*Lingfang Hu, Ian A. Kash*

Main category: cs.LG

TL;DR: The paper explores indirect elicitation of statistical properties using weighted sums of proper scoring rules, analyzing how weight choices affect estimation and identifying optimal configurations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of literature on choosing proper scoring rules for applications, focusing on indirect elicitation of properties via parametric assumptions.

Method: Uses simulation studies and theoretical analysis to examine the impact of weight choices in scoring rules, with a focus on monotonicity and optimal weight configurations.

Result: Optimal estimation often involves setting some weights to zero, with theoretical support for 2-D cases and linear approximations for higher dimensions.

Conclusion: The study provides insights into optimal weight configurations for indirect elicitation, supported by theory and simulations, with implications for practical applications.

Abstract: People are commonly interested in predicting a statistical property of a
random event such as mean and variance. Proper scoring rules assess the quality
of predictions and require that the expected score gets uniquely maximized at
the precise prediction, in which case we call the score directly elicits the
property. Previous research work has widely studied the existence and the
characterization of proper scoring rules for different properties, but little
literature discusses the choice of proper scoring rules for applications at
hand. In this paper, we explore a novel task, the indirect elicitation of
properties with parametric assumptions, where the target property is a function
of several directly-elicitable sub-properties and the total score is a weighted
sum of proper scoring rules for each sub-property. Because of the restriction
to a parametric model class, different settings for the weights lead to
different constrained optimal solutions. Our goal is to figure out how the
choice of weights affects the estimation of the target property and which
choice is the best. We start it with simulation studies and observe an
interesting pattern: in most cases, the optimal estimation of the target
property changes monotonically with the increase of each weight, and the best
configuration of weights is often to set some weights as zero. To understand
how it happens, we first establish the elementary theoretical framework and
then provide deeper sufficient conditions for the case of two sub-properties
and of more sub-properties respectively. The theory on 2-D cases perfectly
interprets the experimental results. In higher-dimensional situations, we
especially study the linear cases and suggest that more complex settings can be
understood with locally mapping into linear situations or using linear
approximations when the true values of sub-properties are close enough to the
parametric space.

</details>


### [580] [TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs](https://arxiv.org/pdf/2506.17894)
*Kiran Thorat, Amit Hasan, Caiwen Ding, Zhijie Shi*

Main category: cs.LG

TL;DR: A novel GNN-based framework for detecting hardware trojans in large chip designs, achieving high precision and recall with efficient training and inference.


<details>
  <summary>Details</summary>
Motivation: The increasing use of untrusted third-party IPs and tools in chip manufacturing raises the risk of hardware trojans, posing threats to security and privacy. Existing GNN-based methods perform poorly on large designs and lack efficient training processes.

Method: The framework generates graph embeddings for large designs (e.g., RISC-V) and incorporates tailored GNN models. It uses model quantization for efficient training and inference, reducing computational requirements without compromising accuracy.

Result: Achieved 98.66% precision and 92.30% recall on a custom dataset, demonstrating effectiveness in detecting hardware trojans in large-scale designs.

Conclusion: The proposed framework addresses limitations of existing methods, offering an efficient and accurate solution for hardware trojan detection in large chip designs.

Abstract: Chip manufacturing is a complex process, and to achieve a faster time to
market, an increasing number of untrusted third-party tools and designs from
around the world are being utilized. The use of these untrusted third party
intellectual properties (IPs) and tools increases the risk of adversaries
inserting hardware trojans (HTs). The covert nature of HTs poses significant
threats to cyberspace, potentially leading to severe consequences for national
security, the economy, and personal privacy. Many graph neural network
(GNN)-based HT detection methods have been proposed. However, they perform
poorly on larger designs because they rely on training with smaller designs.
Additionally, these methods do not explore different GNN models that are
well-suited for HT detection or provide efficient training and inference
processes. We propose a novel framework that generates graph embeddings for
large designs (e.g., RISC-V) and incorporates various GNN models tailored for
HT detection. Furthermore, our framework introduces domain-specific techniques
for efficient training and inference by implementing model quantization. Model
quantization reduces the precision of the weights, lowering the computational
requirements, enhancing processing speed without significantly affecting
detection accuracy. We evaluate our framework using a custom dataset, and our
results demonstrate a precision of 98.66% and a recall (true positive rate) of
92.30%, highlighting the effectiveness and efficiency of our approach in
detecting hardware trojans in large-scale chip designs

</details>


### [581] [Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding](https://arxiv.org/pdf/2506.17919)
*Zhiyu Mou, Miao Xu, Wei Chen, Rongquan Bai, Chuan Yu, Jian Xu*

Main category: cs.LG

TL;DR: The paper introduces Model-based RL Bidding (MRLB) to bridge the gap between simulation-based and offline RL bidding, using a permutation equivariant model and robust offline Q-learning for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing auto-bidding methods (SRLB and ORLB) have limitations: ORLB lacks state space coverage, while SRLB suffers from a simulator-reality gap. MRLB aims to address these issues.

Method: MRLB learns an environment model from real data, combining real and model-generated data for training. It uses a permutation equivariant model architecture and a robust offline Q-learning method (PE-MORL).

Result: PE-MORL outperforms state-of-the-art auto-bidding methods in real-world experiments.

Conclusion: MRLB, with PE-MORL, effectively bridges the gap between simulation and reality, offering improved performance in auto-bidding.

Abstract: Reinforcement learning (RL) for auto-bidding has shifted from using
simplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL
on fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are
limited by the dataset's state space coverage, offering modest gains. While
SRLB expands state coverage, its simulator-reality gap risks misleading
policies. This paper introduces Model-based RL Bidding (MRLB), which learns an
environment model from real data to bridge this gap. MRLB trains policies using
both real and model-generated data, expanding state coverage beyond ORLB. To
ensure model reliability, we propose: 1) A permutation equivariant model
architecture for better generalization, and 2) A robust offline Q-learning
method that pessimistically penalizes model errors. These form the Permutation
Equivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments
show that PE-MORL outperforms state-of-the-art auto-bidding methods.

</details>


### [582] [ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation](https://arxiv.org/pdf/2506.17929)
*Shulun Chen, Wei Shao, Flora D. Salim, Hao Xue*

Main category: cs.LG

TL;DR: ASTER introduces a framework for converting spatio-temporal forecasts into actionable decisions, improving resource allocation and intervention strategies.


<details>
  <summary>Details</summary>
Motivation: The decoupling of prediction and decision phases limits downstream efficiency, especially in scenarios like emergency response where actionable strategies are critical.

Method: ASTER combines a Resource-aware Spatio-Temporal interaction module (RaST) for context-aware representations and a Preference-oriented decision agent (Poda) using multi-objective reinforcement learning for optimal actions.

Result: ASTER achieves state-of-the-art performance in early prediction accuracy and resource allocation across six metrics on four benchmark datasets.

Conclusion: ASTER bridges the gap between forecasting and decision-making, enhancing overall effectiveness in spatio-temporal intelligence.

Abstract: Supporting decision-making has long been a central vision in the field of
spatio-temporal intelligence. While prior work has improved the timeliness and
accuracy of spatio-temporal forecasting, converting these forecasts into
actionable strategies remains a key challenge. A main limitation is the
decoupling of the prediction and the downstream decision phases, which can
significantly degrade the downstream efficiency. For example, in emergency
response, the priority is successful resource allocation and intervention, not
just incident prediction. To this end, it is essential to propose an Adaptive
Spatio-Temporal Early Decision model (ASTER) that reforms the forecasting
paradigm from event anticipation to actionable decision support. This framework
ensures that information is directly used for decision-making, thereby
maximizing overall effectiveness. Specifically, ASTER introduces a new
Resource-aware Spatio-Temporal interaction module (RaST) that adaptively
captures long- and short-term dependencies under dynamic resource conditions,
producing context-aware spatiotemporal representations. To directly generate
actionable decisions, we further design a Preference-oriented decision agent
(Poda) based on multi-objective reinforcement learning, which transforms
predictive signals into resource-efficient intervention strategies by deriving
optimal actions under specific preferences and dynamic constraints.
Experimental results on four benchmark datasets demonstrate the
state-of-the-art performance of ASTER in improving both early prediction
accuracy and resource allocation outcomes across six downstream metrics.

</details>


### [583] [An entropy-optimal path to humble AI](https://arxiv.org/pdf/2506.17940)
*Davide Bassetti, Luk Pospil, Michael Groom, Terence J. O'Kane, Illia Horenko*

Main category: cs.LG

TL;DR: A novel entropy-optimizing framework for Boltzmann machines is introduced, offering cheaper, gradient-descent-free learning with performance and reliability advantages over state-of-the-art AI tools.


<details>
  <summary>Details</summary>
Motivation: Address the high costs, resource demands, and over-confidence of current AI models.

Method: Non-equilibrium entropy-optimizing reformulation of Boltzmann machines using the exact law of total probability.

Result: More performant, cost-effective models with justified reliability measures, validated on synthetic and climate data.

Conclusion: The framework outperforms existing tools in performance, cost, and descriptor efficiency, especially in climate prediction.

Abstract: Progress of AI has led to a creation of very successful, but by no means
humble models and tools, especially regarding (i) the huge and further
exploding costs and resources they demand, and (ii) the over-confidence of
these tools with the answers they provide. Here we introduce a novel
mathematical framework for a non-equilibrium entropy-optimizing reformulation
of Boltzmann machines based on the exact law of total probability. It results
in the highly-performant, but much cheaper, gradient-descent-free learning
framework with mathematically-justified existence and uniqueness criteria, and
answer confidence/reliability measures. Comparisons to state-of-the-art AI
tools in terms of performance, cost and the model descriptor lengths on a set
of synthetic problems with varying complexity reveal that the proposed method
results in more performant and slim models, with the descriptor lengths being
very close to the intrinsic complexity scaling bounds for the underlying
problems. Applying this framework to historical climate data results in models
with systematically higher prediction skills for the onsets of La Ni\~na and El
Ni\~no climate phenomena, requiring just few years of climate data for training
- a small fraction of what is necessary for contemporary climate prediction
tools.

</details>


### [584] [Adapting Vision-Language Models for Evaluating World Models](https://arxiv.org/pdf/2506.17967)
*Mariya Hendriksen, Tabish Rashid, David Bignell, Raluca Georgescu, Abdelhak Lemkhenter, Katja Hofmann, Sam Devlin, Sarah Parisot*

Main category: cs.LG

TL;DR: UNIVERSE is introduced as a scalable, semantics-aware evaluator for world models, leveraging VLMs for fine-grained, temporally sensitive assessment of action and character recognition.


<details>
  <summary>Details</summary>
Motivation: Existing metrics fail to capture fine-grained, temporally grounded evaluation of world model rollouts, necessitating a new approach.

Method: UNIVERSE adapts VLMs for rollout evaluation, testing action and character recognition across binary, multiple-choice, and open-ended formats.

Result: UNIVERSE matches task-specific baselines with a single checkpoint and aligns well with human judgments.

Conclusion: UNIVERSE provides a scalable and effective solution for evaluating world model rollouts.

Abstract: World models -- generative models that simulate environment dynamics
conditioned on past observations and actions -- are gaining prominence in
planning, simulation, and embodied AI. However, evaluating their rollouts
remains a fundamental challenge, requiring fine-grained, temporally grounded
assessment of action alignment and semantic consistency -- capabilities not
captured by existing metrics. Vision-Language Models (VLMs) have shown promise
as automatic evaluators of generative content due to their strong multimodal
reasoning abilities. Yet, their use in fine-grained, temporally sensitive
evaluation tasks remains limited and requires targeted adaptation. We introduce
a evaluation protocol targeting two recognition tasks -- action recognition and
character recognition -- each assessed across binary, multiple-choice, and
open-ended formats. To support this, we present UNIVERSE (UNIfied
Vision-language Evaluator for Rollouts in Simulated Environments), a method for
adapting VLMs to rollout evaluation under data and compute constraints. We
conduct a large-scale study comparing full, partial, and parameter-efficient
finetuning across task formats, context lengths, sampling strategies, and data
compositions. The resulting unified evaluator matches the performance of
task-specific baselines using a single checkpoint. Human studies confirm strong
alignment with human judgments, establishing UNIVERSE as a scalable,
semantics-aware evaluator for world models.

</details>


### [585] [h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective](https://arxiv.org/pdf/2506.17968)
*Wenjian Huang, Guiping Cao, Jiahao Xia, Jingkun Chen, Hao Wang, Jianguo Zhang*

Main category: cs.LG

TL;DR: The paper addresses miscalibration in deep neural networks, proposing a probabilistic framework (h-calibration) to overcome limitations in existing methods and achieve superior performance.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often produce miscalibrated probability outputs, leading to unreliable predictions, which motivates the need for effective recalibration methods.

Method: The authors categorize existing methods into three strategies, identify ten limitations, and propose h-calibration, a probabilistic framework with a post-hoc calibration algorithm.

Result: The proposed method outperforms traditional approaches, achieving state-of-the-art performance on calibration benchmarks.

Conclusion: The h-calibration framework provides a theoretically sound and practical solution for learning reliable calibrated probabilities, with broad applicability in related fields.

Abstract: Deep neural networks have demonstrated remarkable performance across numerous
learning tasks but often suffer from miscalibration, resulting in unreliable
probability outputs. This has inspired many recent works on mitigating
miscalibration, particularly through post-hoc recalibration methods that aim to
obtain calibrated probabilities without sacrificing the classification
performance of pre-trained models. In this study, we summarize and categorize
previous works into three general strategies: intuitively designed methods,
binning-based methods, and methods based on formulations of ideal calibration.
Through theoretical and practical analysis, we highlight ten common limitations
in previous approaches. To address these limitations, we propose a
probabilistic learning framework for calibration called h-calibration, which
theoretically constructs an equivalent learning formulation for canonical
calibration with boundedness. On this basis, we design a simple yet effective
post-hoc calibration algorithm. Our method not only overcomes the ten
identified limitations but also achieves markedly better performance than
traditional methods, as validated by extensive experiments. We further analyze,
both theoretically and experimentally, the relationship and advantages of our
learning objective compared to traditional proper scoring rule. In summary, our
probabilistic framework derives an approximately equivalent differentiable
objective for learning error-bounded calibrated probabilities, elucidating the
correspondence and convergence properties of computational statistics with
respect to theoretical bounds in canonical calibration. The theoretical
effectiveness is verified on standard post-hoc calibration benchmarks by
achieving state-of-the-art performance. This research offers valuable reference
for learning reliable likelihood in related fields.

</details>


### [586] [Trustworthy Efficient Communication for Distributed Learning using LQ-SGD Algorithm](https://arxiv.org/pdf/2506.17974)
*Hongyang Li, Lincen Bai, Caesar Wu, Mohammed Chadli, Said Mammar, Pascal Bouvry*

Main category: cs.LG

TL;DR: LQ-SGD is a gradient compression algorithm for distributed training, combining low-rank approximation and log-quantization to reduce communication overhead while maintaining convergence and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the high communication costs in distributed training while preserving model performance and robustness against gradient inversion attacks.

Method: LQ-SGD integrates low-rank approximation and log-quantization techniques, building on PowerSGD, to compress gradients efficiently.

Result: The method reduces communication overhead significantly without compromising training convergence or model accuracy, and offers stronger resistance to gradient inversion.

Conclusion: LQ-SGD provides a robust and efficient solution for distributed learning, balancing communication efficiency with performance and security.

Abstract: We propose LQ-SGD (Low-Rank Quantized Stochastic Gradient Descent), an
efficient communication gradient compression algorithm designed for distributed
training. LQ-SGD further develops on the basis of PowerSGD by incorporating the
low-rank approximation and log-quantization techniques, which drastically
reduce the communication overhead, while still ensuring the convergence speed
of training and model accuracy. In addition, LQ-SGD and other compression-based
methods show stronger resistance to gradient inversion than traditional SGD,
providing a more robust and efficient optimization path for distributed
learning systems.

</details>


### [587] [SliceGX: Layer-wise GNN Explanation with Model-slicing](https://arxiv.org/pdf/2506.17977)
*Tingting Zhu, Tingyang Chen, Yinghui Wu, Arijit Khan, Xiangyu Ke*

Main category: cs.LG

TL;DR: SliceGX is a novel GNN explanation method that provides layer-wise insights into model behavior, improving diagnosis and optimization.


<details>
  <summary>Details</summary>
Motivation: Existing GNN explanation methods lack fine-grained, layer-wise analysis, which is crucial for model diagnosis and architecture optimization.

Method: SliceGX segments GNNs into layer blocks and identifies explanatory subgraphs for each block, using efficient algorithms with approximation guarantees. It also offers a SPARQL-like query interface.

Result: Experiments on real-world graphs and GNN architectures confirm SliceGX's effectiveness and efficiency in model debugging.

Conclusion: SliceGX advances GNN explainability by enabling progressive, layer-wise analysis and practical debugging support.

Abstract: Ensuring the trustworthiness of graph neural networks (GNNs) as black-box
models requires effective explanation methods. Existing GNN explanations
typically apply input perturbations to identify subgraphs that are responsible
for the occurrence of the final output of GNNs. However, such approaches lack
finer-grained, layer-wise analysis of how intermediate representations
contribute to the final result, capabilities that are crucial for model
diagnosis and architecture optimization. This paper introduces SliceGX, a novel
GNN explanation approach that generates explanations at specific GNN layers in
a progressive manner. Given a GNN M, a set of selected intermediate layers, and
a target layer, SliceGX automatically segments M into layer blocks ("model
slice") and discovers high-quality explanatory subgraphs in each layer block
that clarifies the occurrence of output of M at the targeted layer. Although
finding such layer-wise explanations is computationally challenging, we develop
efficient algorithms and optimization techniques that incrementally generate
and maintain these subgraphs with provable approximation guarantees.
Additionally, SliceGX offers a SPARQL-like query interface, providing
declarative access and search capacities for the generated explanations.
Through experiments on large real-world graphs and representative GNN
architectures, we verify the effectiveness and efficiency of SliceGX, and
illustrate its practical utility in supporting model debugging.

</details>


### [588] [Data Curation Matters: Model Collapse and Spurious Shift Performance Prediction from Training on Uncurated Text Embeddings](https://arxiv.org/pdf/2506.17989)
*Lucas Mattioli, Youness Ait Hadichou, Sabrina Chaouche, Martin Gonzalez*

Main category: cs.LG

TL;DR: Training on uncurated Text Embeddings (TEs) from raw tabular data causes model collapse, where predictions converge to one class. Metrics introduced show TE quality impacts downstream learning, and collapse can inflate accuracy correlations. Better curation and evaluation of embeddings are needed.


<details>
  <summary>Details</summary>
Motivation: To investigate the failure mode of model collapse when using uncurated TEs from raw tabular data and assess their impact on downstream tasks.

Method: Compare models trained on raw tabular data vs. TE-derived data with identical hyperparameters, introduce metrics to quantify collapse, and analyze TE quality.

Result: Model collapse is consistent with TEs, and their quality significantly affects learning. Collapse can also inflate accuracy correlations misleadingly.

Conclusion: TEs alone are insufficient for data curation; nuanced evaluation and better embedding practices are crucial, especially for out-of-distribution scenarios.

Abstract: Training models on uncurated Text Embeddings (TEs) derived from raw tabular
data can lead to a severe failure mode known as model collapse, where
predictions converge to a single class regardless of input. By comparing models
trained with identical hyper-parameter configurations on both raw tabular data
and their TE-derived counterparts, we find that collapse is a consistent
failure mode in the latter setting. We introduce a set of metrics that capture
the extent of model collapse, offering a new perspective on TE quality as a
proxy for data curation. Our results reveal that TE alone does not effectively
function as a curation layer - and that their quality significantly influences
downstream learning. More insidiously, we observe that the presence of model
collapse can yield artificially inflated and spurious Accuracy-on-the-Line
correlation. These findings highlight the need for more nuanced curation and
evaluation of embedding-based representations, particularly in
out-of-distribution settings.

</details>


### [589] [Imputation of Longitudinal Data Using GANs: Challenges and Implications for Classification](https://arxiv.org/pdf/2506.18007)
*Sharon Torao Pingi, Md Abul Bashar, Richi Nayak*

Main category: cs.LG

TL;DR: The paper reviews GANs for longitudinal data imputation (LDI) and classification (LDC), highlighting challenges like missing values, class imbalance, and mixed data types. It categorizes GAN-based LDI methods, evaluates their effectiveness, and suggests future directions.


<details>
  <summary>Details</summary>
Motivation: Longitudinal data's complexity and missing values hinder LDC accuracy. GANs offer potential for LDI, but their ability to address data assumptions and challenges needs evaluation.

Method: The paper categorizes GAN-based LDI approaches, reviews their strengths/limitations, and identifies research trends.

Result: GANs show promise for LDI but lack versatility for all longitudinal data challenges.

Conclusion: Future research should develop more adaptable GAN-based solutions for LDC challenges.

Abstract: Longitudinal data is commonly utilised across various domains, such as
health, biomedical, education and survey studies. This ubiquity has led to a
rise in statistical, machine and deep learning-based methods for Longitudinal
Data Classification (LDC). However, the intricate nature of the data,
characterised by its multi-dimensionality, causes instance-level heterogeneity
and temporal correlations that add to the complexity of longitudinal data
analysis. Additionally, LDC accuracy is often hampered by the pervasiveness of
missing values in longitudinal data. Despite ongoing research that draw on the
generative power and utility of Generative Adversarial Networks (GANs) to
address the missing data problem, critical considerations include statistical
assumptions surrounding longitudinal data and missingness within it, as well as
other data-level challenges like class imbalance and mixed data types that
impact longitudinal data imputation (LDI) and the subsequent LDC process in
GANs. This paper provides a comprehensive overview of how GANs have been
applied in LDI, with a focus whether GANS have adequately addressed fundamental
assumptions about the data from a LDC perspective. We propose a categorisation
of main approaches to GAN-based LDI, highlight strengths and limitations of
methods, identify key research trends, and provide promising future directions.
Our findings indicate that while GANs show great potential for LDI to improve
usability and quality of longitudinal data for tasks like LDC, there is need
for more versatile approaches that can handle the wider spectrum of challenges
presented by longitudinal data with missing values. By synthesising current
knowledge and identifying critical research gaps, this survey aims to guide
future research efforts in developing more effective GAN-based solutions to
address LDC challenges.

</details>


### [590] [Probing the Embedding Space of Transformers via Minimal Token Perturbations](https://arxiv.org/pdf/2506.18011)
*Eddie Conti, Alejandro Astruc, Alvaro Parafita, Axel Brando*

Main category: cs.LG

TL;DR: The paper explores how minimal token perturbations affect Transformer models' embedding space, showing rare tokens cause larger shifts and deeper layers mix input information more. It validates early layers as proxies for explanations.


<details>
  <summary>Details</summary>
Motivation: To understand information propagation in Transformers and assess the impact of minimal token perturbations on embeddings.

Method: Analyzed token perturbations' effects on embedding space, focusing on frequency of shifts and propagation across layers.

Result: Rare tokens cause larger shifts; deeper layers increasingly intermix input information, validating early layers for explanations.

Conclusion: Token perturbations and embedding shifts are powerful tools for Transformer interpretability, with early layers useful for explanations.

Abstract: Understanding how information propagates through Transformer models is a key
challenge for interpretability. In this work, we study the effects of minimal
token perturbations on the embedding space. In our experiments, we analyze the
frequency of which tokens yield to minimal shifts, highlighting that rare
tokens usually lead to larger shifts. Moreover, we study how perturbations
propagate across layers, demonstrating that input information is increasingly
intermixed in deeper layers. Our findings validate the common assumption that
the first layers of a model can be used as proxies for model explanations.
Overall, this work introduces the combination of token perturbations and shifts
on the embedding space as a powerful tool for model interpretability.

</details>


### [591] [Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning](https://arxiv.org/pdf/2506.18020)
*Thomas Boudou, Batiste Le Bars, Nirupam Gupta, Aurlien Bellet*

Main category: cs.LG

TL;DR: The paper investigates the impact of Byzantine and data poisoning attacks on the generalization error in robust distributed learning, showing Byzantine attacks are more harmful.


<details>
  <summary>Details</summary>
Motivation: To understand whether the observed gap in generalization between Byzantine and data poisoning attacks is fundamental or due to suboptimal attacks.

Method: Theoretical analysis comparing algorithmic stability and generalization error under both threat models.

Result: Byzantine attacks degrade stability more severely (O((f/(n-2f))) than data poisoning ((f/(n-f))), leading to a significant generalization gap as f approaches n/2.

Conclusion: Byzantine attacks are intrinsically more harmful to generalization than data poisoning, with the gap becoming critical as the number of misbehaving workers increases.

Abstract: Robust distributed learning algorithms aim to maintain good performance in
distributed and federated settings, even in the presence of misbehaving
workers. Two primary threat models have been studied: Byzantine attacks, where
misbehaving workers can send arbitrarily corrupted updates, and data poisoning
attacks, where misbehavior is limited to manipulation of local training data.
While prior work has shown comparable optimization error under both threat
models, a fundamental question remains open: How do these threat models impact
generalization? Empirical evidence suggests a gap between the two threat
models, yet it remains unclear whether it is fundamental or merely an artifact
of suboptimal attacks. In this work, we present the first theoretical
investigation into this problem, formally showing that Byzantine attacks are
intrinsically more harmful to generalization than data poisoning. Specifically,
we prove that: (i) under data poisoning, the uniform algorithmic stability of a
robust distributed learning algorithm, with optimal optimization error,
degrades by an additive factor of $\varTheta ( \frac{f}{n-f} )$, with $f$ the
number of misbehaving workers out of $n$; and (ii) In contrast, under Byzantine
attacks, the degradation is in $\mathcal{O} \big( \sqrt{ \frac{f}{n-2f}}
\big)$.This difference in stability leads to a generalization error gap that is
especially significant as $f$ approaches its maximum value $\frac{n}{2}$.

</details>


### [592] [Why Do Some Language Models Fake Alignment While Others Don't?](https://arxiv.org/pdf/2506.18032)
*Abhay Sheshadri, John Hughes, Julian Michael, Alex Mallen, Arun Jose, Janus, Fabien Roger*

Main category: cs.LG

TL;DR: The paper explores alignment faking in 25 large language models, finding only 5 selectively comply with harmful queries during training. Claude 3 Opus stands out for consistently maintaining its goals, while post-training impacts alignment faking behavior.


<details>
  <summary>Details</summary>
Motivation: To understand why some models fake alignment (selectively comply with harmful queries during training) and why others do not, focusing on the role of post-training and refusal behavior.

Method: Analyzed 25 models, perturbed scenario details, and tested 5 hypotheses on how post-training suppresses alignment faking.

Result: Only 5 models (including Claude 3 Opus) showed alignment faking. Claude 3 Opus was uniquely consistent. Post-training varied in its effect, sometimes suppressing or amplifying alignment faking.

Conclusion: Alignment faking is model-specific, with Claude 3 Opus being the most consistent. Post-training and refusal behavior significantly influence this phenomenon.

Abstract: Alignment faking in large language models presented a demonstration of Claude
3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training
objective to prevent modification of their behavior outside of training. We
expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude
3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries
more when they infer they are in training than when they infer they are in
deployment. First, we study the motivations of these 5 models. Results from
perturbing details of the scenario suggest that only Claude 3 Opus's compliance
gap is primarily and consistently motivated by trying to keep its goals.
Second, we investigate why many chat models don't fake alignment. Our results
suggest this is not entirely due to a lack of capabilities: many base models
fake alignment some of the time, and post-training eliminates alignment-faking
for some models and amplifies it for others. We investigate 5 hypotheses for
how post-training may suppress alignment faking and find that variations in
refusal behavior may account for a significant portion of differences in
alignment faking.

</details>


### [593] [Pathwise Explanation of ReLU Neural Networks](https://arxiv.org/pdf/2506.18037)
*Seongwoo Lim, Won Jo, Joohyung Lee, Jaesik Choi*

Main category: cs.LG

TL;DR: A novel method for explaining neural network decisions by focusing on subsets of hidden units in the decision path, offering clearer and more flexible explanations.


<details>
  <summary>Details</summary>
Motivation: Address the lack of transparency and reliability in neural networks by providing more interpretable explanations of their decision-making processes.

Method: Introduces a pathwise explanation approach that analyzes subsets of hidden units involved in decisions, allowing for adjustable and decomposable explanations.

Result: Outperforms existing methods in both quantitative and qualitative evaluations, providing clearer and more consistent explanations.

Conclusion: The proposed method enhances interpretability and flexibility in explaining neural network decisions, improving transparency.

Abstract: Neural networks have demonstrated a wide range of successes, but their
``black box" nature raises concerns about transparency and reliability.
Previous research on ReLU networks has sought to unwrap these networks into
linear models based on activation states of all hidden units. In this paper, we
introduce a novel approach that considers subsets of the hidden units involved
in the decision making path. This pathwise explanation provides a clearer and
more consistent understanding of the relationship between the input and the
decision-making process. Our method also offers flexibility in adjusting the
range of explanations within the input, i.e., from an overall attribution input
to particular components within the input. Furthermore, it allows for the
decomposition of explanations for a given input for more detailed explanations.
Experiments demonstrate that our method outperforms others both quantitatively
and qualitatively.

</details>


### [594] [TAB: Unified Benchmarking of Time Series Anomaly Detection Methods](https://arxiv.org/pdf/2506.18046)
*Xiangfei Qiu, Zhe Li, Wanghui Qiu, Shiyan Hu, Lekui Zhou, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Aoying Zhou, Zhenli Sheng, Jilin Hu, Christian S. Jensen, Bin Yang*

Main category: cs.LG

TL;DR: The paper introduces TAB, a new benchmark for time series anomaly detection (TSAD), addressing deficiencies in current evaluation methods by offering diverse datasets, unified evaluation protocols, and automated pipelines.


<details>
  <summary>Details</summary>
Motivation: The growing demand for TSAD in various domains necessitates better evaluation methods to compare and improve existing and new TSAD techniques.

Method: TAB includes 29 multivariate and 1,635 univariate datasets, covers multiple TSAD methods (Non-learning, Machine learning, Deep learning, LLM-based, Time-series pre-trained), and provides a unified, automated evaluation pipeline.

Result: TAB enables comprehensive evaluation of TSAD methods, revealing insights into their performance.

Conclusion: TAB serves as a reliable benchmark for TSAD evaluation, fostering progress in the field. Datasets and code are publicly available.

Abstract: Time series anomaly detection (TSAD) plays an important role in many domains
such as finance, transportation, and healthcare. With the ongoing
instrumentation of reality, more time series data will be available, leading
also to growing demands for TSAD. While many TSAD methods already exist, new
and better methods are still desirable. However, effective progress hinges on
the availability of reliable means of evaluating new methods and comparing them
with existing methods. We address deficiencies in current evaluation procedures
related to datasets and experimental settings and protocols. Specifically, we
propose a new time series anomaly detection benchmark, called TAB. First, TAB
encompasses 29 public multivariate datasets and 1,635 univariate time series
from different domains to facilitate more comprehensive evaluations on diverse
datasets. Second, TAB covers a variety of TSAD methods, including Non-learning,
Machine learning, Deep learning, LLM-based, and Time-series pre-trained
methods. Third, TAB features a unified and automated evaluation pipeline that
enables fair and easy evaluation of TSAD methods. Finally, we employ TAB to
evaluate existing TSAD methods and report on the outcomes, thereby offering a
deeper insight into the performance of these methods. Besides, all datasets and
code are available at https://github.com/decisionintelligence/TAB.

</details>


### [595] [Distributionally robust minimization in meta-learning for system identification](https://arxiv.org/pdf/2506.18074)
*Matteo Rufolo, Dario Piga, Marco Forgione*

Main category: cs.LG

TL;DR: Meta learning with distributionally robust optimization improves system identification by prioritizing high-loss tasks, enhancing worst-case performance.


<details>
  <summary>Details</summary>
Motivation: Standard meta learning overlooks task variability; robust optimization addresses this to improve safety-critical applications.

Method: Uses distributionally robust optimization to prioritize high-loss tasks in meta learning for system identification.

Result: Reduces failures in both in-distribution and out-of-distribution settings.

Conclusion: The approach enhances robustness in meta learning, particularly for safety-critical scenarios.

Abstract: Meta learning aims at learning how to solve tasks, and thus it allows to
estimate models that can be quickly adapted to new scenarios. This work
explores distributionally robust minimization in meta learning for system
identification. Standard meta learning approaches optimize the expected loss,
overlooking task variability. We use an alternative approach, adopting a
distributionally robust optimization paradigm that prioritizes high-loss tasks,
enhancing performance in worst-case scenarios. Evaluated on a meta model
trained on a class of synthetic dynamical systems and tested in both
in-distribution and out-of-distribution settings, the proposed approach allows
to reduce failures in safety-critical applications.

</details>


### [596] [RL for Reasoning by Adaptively Revealing Rationales](https://arxiv.org/pdf/2506.18110)
*Mohammad Hossein Amani, Aryo Lotfi, Nicolas Mario Baldwin, Samy Bengio, Mehrdad Farajtabar, Emmanuel Abbe, Robert West*

Main category: cs.LG

TL;DR: The paper introduces adaptive backtracking (AdaBack), a per-sample curriculum learning method for reinforcement learning (RL) from partial expert demonstrations, addressing challenges in complex sequence generation tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised fine-tuning (SFT) is costly for long sequences, while RL struggles with sparse rewards and large output spaces. The paper explores an intermediate regime between SFT and RL.

Method: AdaBack dynamically adjusts the supervision length for each sample based on past rewards, enabling incremental learning of reasoning chains by conditioning on correct partial solutions.

Result: AdaBack reliably solves synthetic tasks with latent parity constraints and improves performance on mathematical reasoning benchmarks (MATH, GSM8k), where RL alone fails.

Conclusion: Per-sample curriculum learning, as implemented in AdaBack, succeeds in tasks with long latent dependencies, offering a viable alternative where SFT and RL fall short.

Abstract: We propose that reinforcement learning (RL) from partial expert
demonstrations is not merely a training heuristic, but a promising framework
for solving complex sequence generation tasks. Supervised fine-tuning (SFT)
relies on dense ground-truth labels, which become increasingly costly as
sequence length grows. RL, on the other hand, struggles with sparse rewards and
a combinatorially large output space. We address this by introducing adaptive
backtracking (AdaBack), a per-sample curriculum learning algorithm that reveals
only a partial prefix of the target output during training. The supervision
length is adjusted dynamically for each sample based on the model's past reward
signal, allowing it to incrementally learn to complete reasoning chains by
conditioning on correct partial solutions. We investigate this intermediate
regime between SFT and RL and argue that per-sample curriculum learning is more
than a trade-off between efficiency and generality, it can succeed in tasks
with long sequences of latent dependencies where SFT and RL both fail to
generalize. Using a synthetic task with latent parity constraints, we show that
our adaptive curriculum over partial answers reliably solves problems that are
otherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we
find that curriculum learning enables models to solve problems that RL alone
cannot, acquiring new reasoning capabilities through incremental exposure to
partial solutions.

</details>


### [597] [Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models](https://arxiv.org/pdf/2506.18124)
*Shaoxiu Wei, Mingchao Liang, Florian Meyer*

Main category: cs.LG

TL;DR: A hybrid method combining model-based Bayesian MOT with neural networks improves performance by enhancing simplistic statistical models, achieving state-of-the-art results on the nuScenes dataset.


<details>
  <summary>Details</summary>
Motivation: Traditional model-based MOT methods are broadly applicable but simplistic, while data-driven methods excel with abundant labeled data. The paper seeks to integrate both approaches for better performance.

Method: The hybrid method uses neural networks to refine Bayesian MOT's statistical models, employing belief propagation and sequential Monte Carlo for efficient computation.

Result: The method achieves state-of-the-art performance on the nuScenes autonomous driving dataset.

Conclusion: The hybrid framework successfully combines the robustness of model-based methods with the learning capability of neural networks, offering superior MOT performance.

Abstract: Multiobject tracking (MOT) is an important task in applications including
autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT
methods are model-based and combine sequential Bayesian estimation with data
association and an object birth model. More recent methods are fully
data-driven and rely on the training of neural networks. Both approaches offer
distinct advantages in specific settings. In particular, model-based methods
are generally applicable across a wide range of scenarios, whereas data-driven
MOT achieves superior performance in scenarios where abundant labeled data for
training is available. A natural thought is whether a general framework can
integrate the two approaches. This paper introduces a hybrid method that
utilizes neural networks to enhance specific aspects of the statistical model
in Bayesian MOT that have been identified as overly simplistic. By doing so,
the performance of the prediction and update steps of Bayesian MOT is improved.
To ensure tractable computation, our framework uses belief propagation to avoid
high-dimensional operations combined with sequential Monte Carlo methods to
perform low-dimensional operations efficiently. The resulting method combines
the flexibility and robustness of model-based approaches with the capability to
learn complex information from data of neural networks. We evaluate the
performance of the proposed method based on the nuScenes autonomous driving
dataset and demonstrate that it has state-of-the-art performance

</details>


### [598] [Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection](https://arxiv.org/pdf/2506.18145)
*Zheng Zhan, Liliang Ren, Shuohang Wang, Liyuan Liu, Yang Liu, Yeyun Gong, Yanzhi Wang, Yelong Shen*

Main category: cs.LG

TL;DR: RoM scales SSMs efficiently using sparse mixtures of linear projection experts, outperforming dense Mamba models with fewer active parameters and saving FLOPS.


<details>
  <summary>Details</summary>
Motivation: Efficiently scaling the expressive power of SSMs with MoE is challenging, as naive integration often fails or degrades performance.

Method: Introduces Routing Mamba (RoM), leveraging sparse mixtures of linear projection experts and shared routing decisions for efficient scaling.

Result: RoM achieves equivalent performance to dense Mamba with 2.3x fewer active parameters and saves 23% FLOPS.

Conclusion: RoM effectively scales SSMs, offering a viable alternative to dense models for long sequence modeling.

Abstract: Linear State Space Models (SSMs) offer remarkable performance gains in
efficient sequence modeling, with constant inference-time computation and
memory complexity. Recent advances, such as Mamba, further enhance SSMs with
input-dependent gating and hardware-aware implementations, positioning them as
strong alternatives to Transformers for long sequence modeling. However,
efficiently scaling the expressive power of SSMs, particularly with Mixture of
Experts (MoE), remains challenging, as naive integration attempts often falter
or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel
approach that scales SSM parameters using sparse mixtures of linear projection
experts. By sharing routing decisions between projection layers and lightweight
sub-modules within Mamba across experts, RoM leverages synergies among linear
projection experts for effective and efficient sparse scaling of Mamba layers.
At a scale of 1.3B active parameters (10B total) and 16K training sequence
length, RoM achieves language modeling performance equivalent to a dense Mamba
model requiring over 2.3x more active parameters, and demonstrates consistent
perplexity across context lengths. Experimental results further show RoM
effectively scales hybrid language models, yielding a 23% FLOPS saving compared
to dense Mamba scaling for similar performance.

</details>


### [599] [Probabilistic and reinforced mining of association rules](https://arxiv.org/pdf/2506.18155)
*Yongchao Huang*

Main category: cs.LG

TL;DR: The paper introduces four novel probabilistic and reinforcement-driven methods for association rule mining (ARM), enhancing traditional frequency-based approaches with uncertainty modeling, adaptive search, and prior knowledge integration.


<details>
  <summary>Details</summary>
Motivation: Traditional ARM methods like Apriori lack flexibility in handling uncertainty, prior knowledge, and rare patterns. The new methods aim to address these limitations.

Method: Four methods are proposed: GPAR (Gaussian process-based), BARM (Bayesian), MAB-ARM (multi-armed bandit), and RLAR (reinforcement learning). Each leverages probabilistic or adaptive techniques for improved ARM.

Result: Empirical results show effectiveness in discovering rare patterns and operating on small datasets, though with trade-offs in computational complexity and interpretability.

Conclusion: The methods represent a shift from static frequency-based ARM, offering scalable, uncertainty-aware frameworks for diverse applications.

Abstract: This work introduces 4 novel probabilistic and reinforcement-driven methods
for association rule mining (ARM): Gaussian process-based association rule
mining (GPAR), Bayesian ARM (BARM), multi-armed bandit based ARM (MAB-ARM), and
reinforcement learning based association rule mining (RLAR). These methods
depart fundamentally from traditional frequency-based algorithms such as
Apriori, FP-Growth, and Eclat, offering enhanced capabilities for incorporating
prior knowledge, modeling uncertainty, item dependencies, probabilistic
inference and adaptive search strategies. GPAR employs Gaussian processes to
model item co-occurrence via feature representations, enabling principled
inference, uncertainty quantification, and efficient generalization to unseen
itemsets without retraining. BARM adopts a Bayesian framework with priors and
optional correlation structures, yielding robust uncertainty quantification
through full posterior distributions over item presence probabilities. MAB-ARM,
including its Monte Carlo tree search (MCTS) companion, utilizes an upper
confidence bound (UCB) strategy for efficient and adaptive exploration of the
itemset space, while RLAR applies a deep Q-network (DQN) to learn a
generalizable policy for identifying high-quality rules. Collectively, these
approaches improve the flexibility and robustness of ARM, particularly for
discovering rare or complex patterns and operating on small datasets. Empirical
results on synthetic and real-world datasets demonstrate their effectiveness,
while also highlighting trade-offs in computational complexity and
interpretability. These innovations mark a significant shift from static,
frequency-driven paradigms, offering some prior and dependency-informed,
uncertainty-aware or scalable ARM frameworks for diverse application domains
such as retail, geography, finance, medical diagnostics, and risk-sensitive
scenarios.

</details>


### [600] [Pitfalls of Conformal Predictions for Medical Image Classification](https://arxiv.org/pdf/2506.18162)
*Hendrik Mehrtens, Tabea Bucher, Titus J. Brinker*

Main category: cs.LG

TL;DR: Conformal predictions in medical classification face reliability issues under distributional shifts and have limited value in small-class settings.


<details>
  <summary>Details</summary>
Motivation: To highlight the pitfalls and limitations of conformal predictions in medical tasks, especially under distributional shifts.

Method: Analysis of conformal predictions using examples from dermatology and histopathology.

Result: Conformal predictions are unreliable under shifts, unsuitable for accuracy improvement, and limited in small-class settings.

Conclusion: Practitioners must be cautious with conformal predictions in medicine due to their limitations and assumptions.

Abstract: Reliable uncertainty estimation is one of the major challenges for medical
classification tasks. While many approaches have been proposed, recently the
statistical framework of conformal predictions has gained a lot of attention,
due to its ability to provide provable calibration guarantees. Nonetheless, the
application of conformal predictions in safety-critical areas such as medicine
comes with pitfalls, limitations and assumptions that practitioners need to be
aware of. We demonstrate through examples from dermatology and histopathology
that conformal predictions are unreliable under distributional shifts in input
and label variables. Additionally, conformal predictions should not be used for
selecting predictions to improve accuracy and are not reliable for subsets of
the data, such as individual classes or patient attributes. Moreover, in
classification settings with a small number of classes, which are common in
medical image classification tasks, conformal predictions have limited
practical value.

</details>


### [601] [Non-equilibrium Annealed Adjoint Sampler](https://arxiv.org/pdf/2506.18165)
*Jaemoo Choi, Yongxin Chen, Molei Tao, Guan-Horng Liu*

Main category: cs.LG

TL;DR: NAAS is a new SOC-based diffusion sampler that avoids importance sampling by using annealed reference dynamics and adjoint matching for efficient training.


<details>
  <summary>Details</summary>
Motivation: Current annealing-based samplers suffer from high variance and scalability issues due to reliance on importance sampling.

Method: NAAS introduces a non-equilibrium annealed adjoint sampler, leveraging adjoint matching for scalable training without importance sampling.

Result: NAAS effectively samples from complex distributions like classical energy landscapes and molecular Boltzmann distributions.

Conclusion: NAAS offers a scalable and efficient alternative to traditional annealing-based samplers.

Abstract: Recently, there has been significant progress in learning-based diffusion
samplers, which aim to sample from a given unnormalized density. These methods
typically follow one of two paradigms: (i) formulating sampling as an unbiased
stochastic optimal control (SOC) problem using a canonical reference process,
or (ii) refining annealed path measures through importance-weighted sampling.
Although annealing approaches have advantages in guiding samples toward
high-density regions, reliance on importance sampling leads to high variance
and limited scalability in practice. In this paper, we introduce the
\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based
diffusion sampler that leverages annealed reference dynamics without resorting
to importance sampling. NAAS employs a lean adjoint system inspired by adjoint
matching, enabling efficient and scalable training. We demonstrate the
effectiveness of our approach across a range of tasks, including sampling from
classical energy landscapes and molecular Boltzmann distribution.

</details>


### [602] [AdapThink: Adaptive Thinking Preferences for Reasoning Language Model](https://arxiv.org/pdf/2506.18237)
*Xu Wan, Wei Wang, Wenyue Xu, Wotao Yin, Jie Song, Mingyang Sun*

Main category: cs.LG

TL;DR: AdapThink is an adaptive post-training framework for language models that improves reasoning efficiency by dynamically adjusting reflection preferences and balancing solution accuracy with diversity.


<details>
  <summary>Details</summary>
Motivation: Current RL-based post-training methods for language models lack adaptability, leading to inefficient reasoning for questions of varying complexity.

Method: AdapThink uses a group-relative reward function and diversity-aware sampling to dynamically adjust reasoning processes.

Result: Experiments show AdapThink enhances adaptive reasoning and mitigates inefficiencies in models.

Conclusion: AdapThink effectively balances reasoning efficiency and performance, addressing limitations of static methods.

Abstract: Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.

</details>


### [603] [Understanding Reasoning in Thinking Language Models via Steering Vectors](https://arxiv.org/pdf/2506.18167)
*Constantin Venhoff, Ivn Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda*

Main category: cs.LG

TL;DR: A method to control reasoning behaviors in thinking LLMs by analyzing and manipulating activation space directions, validated on DeepSeek-R1-Distill models.


<details>
  <summary>Details</summary>
Motivation: Challenges in controlling reasoning processes of thinking LLMs despite their improved performance.

Method: Analyze and manipulate reasoning behaviors via steering vectors in activation space, tested on 500 tasks across 10 categories.

Result: Identified and controlled behaviors like uncertainty expression and backtracking; validated on two model architectures.

Conclusion: Provides interpretable tools for steering reasoning in thinking models, ensuring controlled and consistent behavior.

Abstract: Recent advances in large language models (LLMs) have led to the development
of thinking language models that generate extensive internal reasoning chains
before producing responses. While these models achieve improved performance,
controlling their reasoning processes remains challenging. This work presents a
steering approach for thinking LLMs by analyzing and manipulating specific
reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic
experiment on 500 tasks across 10 diverse categories, we identify several
reasoning behaviors exhibited by thinking models, including expressing
uncertainty, generating examples for hypothesis validation, and backtracking in
reasoning chains. We demonstrate that these behaviors are mediated by linear
directions in the model's activation space and can be controlled using steering
vectors. By extracting and applying these vectors, we provide a method to
modulate specific aspects of the model's reasoning process, such as its
tendency to backtrack or express uncertainty. Our approach offers practical
tools for steering reasoning processes in thinking models in a controlled and
interpretable manner. We validate our steering method using two
DeepSeek-R1-Distill models, demonstrating consistent control across different
model architectures.

</details>


### [604] [RLPR: Extrapolating RLVR to General Domains without Verifiers](https://arxiv.org/pdf/2506.18254)
*Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, Maosong Sun, Tat-Seng Chua*

Main category: cs.LG

TL;DR: RLPR is a verifier-free framework that uses LLM's token probability scores as rewards, outperforming existing methods in general and mathematical domains.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods are limited to mathematical and code domains due to reliance on domain-specific verifiers, which are complex and unscalable.

Method: RLPR leverages LLM's intrinsic token probability scores for correct answers as rewards, introducing prob-to-reward and stabilizing techniques to manage noise.

Result: RLPR improves reasoning in general and mathematical domains, outperforming VeriFree and General-Reasoner across benchmarks.

Conclusion: RLPR effectively scales RLVR to broader domains without verifiers, enhancing LLM reasoning capabilities.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising
potential in advancing the reasoning capabilities of LLMs. However, its success
remains largely confined to mathematical and code domains. This primary
limitation stems from the heavy reliance on domain-specific verifiers, which
results in prohibitive complexity and limited scalability. To address the
challenge, our key observation is that LLM's intrinsic probability of
generating a correct free-form answer directly indicates its own evaluation of
the reasoning reward (i.e., how well the reasoning process leads to the correct
answer). Building on this insight, we propose RLPR, a simple verifier-free
framework that extrapolates RLVR to broader general domains. RLPR uses the
LLM's own token probability scores for reference answers as the reward signal
and maximizes the expected reward during training. We find that addressing the
high variance of this noisy probability reward is crucial to make it work, and
propose prob-to-reward and stabilizing methods to ensure a precise and stable
reward from LLM intrinsic probabilities. Comprehensive experiments in four
general-domain benchmarks and three mathematical benchmarks show that RLPR
consistently improves reasoning capabilities in both areas for Gemma, Llama,
and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6
points on TheoremQA and 7.5 points on Minerva, and even surpasses strong
verifier-model-dependent approaches General-Reasoner by 1.6 average points
across seven benchmarks.

</details>


### [605] [Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba](https://arxiv.org/pdf/2506.18184)
*Donghyun Lee, Yuhang Li, Ruokai Yin, Shiting Xiao, Priyadarshini Panda*

Main category: cs.LG

TL;DR: Memba introduces a bio-inspired PEFT method for Mamba SSMs, improving temporal modeling with LIM neurons and outperforming traditional PEFT approaches.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods for Transformers don't suit SSMs like Mamba due to their unique temporal dynamics, necessitating a tailored solution.

Method: Memba combines LIM neurons (bio-inspired gating) with LoRA and cross-layer membrane transfer for efficient fine-tuning.

Result: Memba outperforms existing PEFT methods in language and vision tasks, enhancing Mamba's temporal capabilities.

Conclusion: Memba is a novel, effective PEFT approach for Mamba, addressing SSM-specific challenges and improving performance.

Abstract: State Space Models (SSMs) have emerged as powerful alternatives to
attention-based Transformers, with Mamba demonstrating impressive efficiency
and scalability. As these models grow increasingly larger, the need for
Parameter-Efficient Fine-Tuning (PEFT) methods becomes critical to adapt
pre-trained Mamba to downstream tasks without prohibitive computational costs.
However, previous approaches simply apply traditional Transformer-tailored PEFT
methods without addressing the unique temporal processing dynamics of SSMs. To
address this limitation, we propose Memba, a membrane-driven PEFT approach
specifically designed for Mamba. Memba introduces Leaky Integrate Membrane
(LIM) neurons as bio-inspired gating mechanisms that naturally accumulate
membrane potentials over time, enhancing selective information retention. By
strategically combining LIM neurons with Low-Rank Adaptations (LoRA) and
cross-layer membrane transfer, our approach significantly improves Mamba's
temporal modeling capabilities. Extensive experiments across language and
vision tasks demonstrate that Memba achieves substantial improvements over
existing PEFT methods. The code is available at
https://github.com/Intelligent-Computing-Lab-Yale/Memba.

</details>


### [606] [Online Learning of Whittle Indices for Restless Bandits with Non-Stationary Transition Kernels](https://arxiv.org/pdf/2506.18186)
*Md Kamran Chowdhury Shisher, Vishrant Tripathi, Mung Chiang, Christopher G. Brinton*

Main category: cs.LG

TL;DR: The paper proposes an online learning algorithm for Whittle indices in non-stationary RMABs, achieving sub-linear dynamic regret by leveraging sliding windows and domain knowledge.


<details>
  <summary>Details</summary>
Motivation: RMABs are PSPACE-hard and often involve unknown, non-stationary transition kernels, making Whittle index computation challenging in practice.

Method: The algorithm predicts transition kernels using linear optimization with upper confidence bounds and sliding windows, then computes Whittle indices.

Result: Numerical results show superior performance with sub-linear dynamic regret in non-stationary settings.

Conclusion: The proposed method effectively addresses non-stationarity in RMABs, outperforming baselines.

Abstract: We consider optimal resource allocation for restless multi-armed bandits
(RMABs) in unknown, non-stationary settings. RMABs are PSPACE-hard to solve
optimally, even when all parameters are known. The Whittle index policy is
known to achieve asymptotic optimality for a large class of such problems,
while remaining computationally efficient. In many practical settings, however,
the transition kernels required to compute the Whittle index are unknown and
non-stationary. In this work, we propose an online learning algorithm for
Whittle indices in this setting. Our algorithm first predicts current
transition kernels by solving a linear optimization problem based on upper
confidence bounds and empirical transition probabilities calculated from data
over a sliding window. Then, it computes the Whittle index associated with the
predicted transition kernels. We design these sliding windows and upper
confidence bounds to guarantee sub-linear dynamic regret on the number of
episodes $T$, under the condition that transition kernels change slowly over
time (rate upper bounded by $\epsilon=1/T^k$ with $k>0$). Furthermore, our
proposed algorithm and regret analysis are designed to exploit prior domain
knowledge and structural information of the RMABs to accelerate the learning
process. Numerical results validate that our algorithm achieves superior
performance in terms of lowest cumulative regret relative to baselines in
non-stationary environments.

</details>


### [607] [DeInfoReg: A Decoupled Learning Framework for Better Training Throughput](https://arxiv.org/pdf/2506.18193)
*Zih-Hao Huang, You-Teng Lin, Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: DeInfoReg decouples supervised learning into shorter gradient flows to address vanishing gradients, enhances parallelization, and outperforms traditional backpropagation in performance and noise resistance.


<details>
  <summary>Details</summary>
Motivation: To mitigate the vanishing gradient problem and improve training throughput by enabling parallelization across GPUs.

Method: Decomposes long gradient flows into shorter ones using a pipeline strategy, integrating information regularization.

Result: Superior performance and noise resistance compared to standard backpropagation and other decomposition techniques, with efficient GPU utilization.

Conclusion: DeInfoReg is an effective solution for gradient flow decomposition, offering better training efficiency and robustness.

Abstract: This paper introduces Decoupled Supervised Learning with Information
Regularization (DeInfoReg), a novel approach that transforms a long gradient
flow into multiple shorter ones, thereby mitigating the vanishing gradient
problem. Integrating a pipeline strategy, DeInfoReg enables model
parallelization across multiple GPUs, significantly improving training
throughput. We compare our proposed method with standard backpropagation and
other gradient flow decomposition techniques. Extensive experiments on diverse
tasks and datasets demonstrate that DeInfoReg achieves superior performance and
better noise resistance than traditional BP models and efficiently utilizes
parallel computing resources. The code for reproducibility is available at:
https://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.

</details>


### [608] [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning](https://arxiv.org/pdf/2506.18330)
*Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan*

Main category: cs.LG

TL;DR: Confucius3-Math is a 14B-parameter open-source LLM optimized for math tasks, runs on a single GPU, and outperforms larger models. It targets Chinese K-12 education, using RL-based training with three innovations for stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance math education for Chinese K-12 students by developing an efficient, low-cost AI model aligned with national curriculum.

Method: Post-training with large-scale reinforcement learning (RL), incorporating three innovations: Targeted Entropy Regularization, Recent Sample Recovery, and Policy-Specific Hardness Weighting.

Result: Achieves SOTA performance on math reasoning tasks, outperforming larger models, and excels at solving Chinese K-12 problems efficiently.

Conclusion: Demonstrates feasibility of building strong, domain-specific reasoning models at low cost; model and code are open-sourced.

Abstract: We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.

</details>


### [609] [Joint Embedding Predictive Architecture for self-supervised pretraining on polymer molecular graphs](https://arxiv.org/pdf/2506.18194)
*Francesco Picolli, Gabriel Vogel, Jana M. Weber*

Main category: cs.LG

TL;DR: JEPA-based self-supervised pretraining on polymer graphs improves downstream ML performance, especially with scarce labeled data.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of high-quality labeled datasets for polymer ML by exploring self-supervised learning (SSL) with JEPA.

Method: Use JEPA, a self-supervised learning architecture, for pretraining on polymer molecular graphs.

Result: JEPA pretraining enhances downstream performance, particularly when labeled data is very scarce, across all tested datasets.

Conclusion: SSL with JEPA is effective for polymer ML tasks when labeled data is limited.

Abstract: Recent advances in machine learning (ML) have shown promise in accelerating
the discovery of polymers with desired properties by aiding in tasks such as
virtual screening via property prediction. However, progress in polymer ML is
hampered by the scarcity of high-quality labeled datasets, which are necessary
for training supervised ML models. In this work, we study the use of the very
recent 'Joint Embedding Predictive Architecture' (JEPA), a type of architecture
for self-supervised learning (SSL), on polymer molecular graphs to understand
whether pretraining with the proposed SSL strategy improves downstream
performance when labeled data is scarce. Our results indicate that JEPA-based
self-supervised pretraining on polymer graphs enhances downstream performance,
particularly when labeled data is very scarce, achieving improvements across
all tested datasets.

</details>


### [610] [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation](https://arxiv.org/pdf/2506.18349)
*Zichong Li, Chen Liang, Zixuan Zhang, Ilgee Hong, Young Jin Kim, Weizhu Chen, Tuo Zhao*

Main category: cs.LG

TL;DR: SlimMoE compresses large MoE models into smaller, efficient variants using multi-stage compression, reducing memory requirements and enabling deployment on resource-limited hardware.


<details>
  <summary>Details</summary>
Motivation: Large MoE models are memory-intensive and costly to fine-tune or deploy, limiting their use in resource-constrained environments.

Method: SlimMoE employs systematic parameter reduction via expert slimming and staged knowledge transfer, avoiding performance degradation seen in one-shot pruning.

Result: Compressed models (Phi-mini-MoE and Phi-tiny-MoE) outperform similarly sized models and match larger models' performance with lower latency.

Conclusion: Structured pruning and staged distillation enable high-quality, compact MoE models, broadening their adoption.

Abstract: The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm
for scaling large language models (LLMs) while maintaining inference
efficiency. However, their enormous memory requirements make them prohibitively
expensive to fine-tune or deploy in resource-constrained environments. To
address this challenge, we introduce SlimMoE, a multi-stage compression
framework for transforming large MoE models into much smaller, efficient
variants without incurring the prohibitive costs of training from scratch. Our
method systematically reduces parameter counts by slimming experts and
transferring knowledge through intermediate stages, effectively mitigating the
performance degradation common in one-shot pruning approaches. Using this
framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to
create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE
(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of
the original model's training data. These compressed models can be fine-tuned
on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them
highly suitable for academic and resource-limited settings. Our experiments
demonstrate that these compressed models outperform others of similar size and
remain competitive with larger models. For instance, Phi-mini-MoE achieves
similar or better performance to Phi-3-mini using only 2/3 of the activated
parameters and yields comparable MMLU scores to Llama 3.1 8B despite having
significantly lower latency. Our findings demonstrate that structured pruning
combined with staged distillation offers an effective path to creating
high-quality, compact MoE models, paving the way for broader adoption of MoE
architectures. We make our models publicly available at
https://huggingface.co/microsoft/Phi-mini-MoE-instruct and
https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .

</details>


### [611] [These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining](https://arxiv.org/pdf/2506.18221)
*Xingyu Alice Yang, Jianyu Zhang, Lon Bottou*

Main category: cs.LG

TL;DR: The paper explores limitations in transfer learning, identifying an 'information saturation bottleneck' in deep learning models, and suggests richer feature representations as a solution.


<details>
  <summary>Details</summary>
Motivation: To address challenges in transfer learning, particularly the difficulty of ensuring transferred features generalize well to unseen tasks.

Method: Evaluates model transfer from pretraining mixtures to component tasks, identifying bottlenecks and proposing richer feature representations.

Result: Found that models lose critical features during pretraining, leading to inconsistent performance, even on training mixture components.

Conclusion: Suggests task-specific training may outperform large-scale networks and proposes improved feature representations for better generalization.

Abstract: Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.

</details>


### [612] [No Training Wheels: Steering Vectors for Bias Correction at Inference Time](https://arxiv.org/pdf/2506.18598)
*Aviral Gupta, Armaan Sethi, Ameesh Sethi*

Main category: cs.LG

TL;DR: A training-free method using bias vectors to reduce classification bias and improve worst-group accuracy in neural networks.


<details>
  <summary>Details</summary>
Motivation: Addressing inherited class biases and spurious correlations in neural networks trained on uneven datasets without retraining or high compute costs.

Method: Compute a bias vector from mean activation differences between majority and minority groups, then subtract it from the model's residual stream.

Result: Reduced classification bias and improved worst-group accuracy in transformer-like classifiers.

Conclusion: Demonstrates a cheap, inference-time method to mitigate bias in classification models, extending steering vectors' use from generative to classification tasks.

Abstract: Neural network classifiers trained on datasets with uneven group
representation often inherit class biases and learn spurious correlations.
These models may perform well on average but consistently fail on atypical
groups. For example, in hair color classification, datasets may over-represent
females with blond hair, reinforcing stereotypes. Although various algorithmic
and data-centric methods have been proposed to address such biases, they often
require retraining or significant compute. In this work, we propose a cheap,
training-free method inspired by steering vectors used to edit behaviors in
large language models. We compute the difference in mean activations between
majority and minority groups to define a "bias vector," which we subtract from
the model's residual stream. This leads to reduced classification bias and
improved worst-group accuracy. We explore multiple strategies for extracting
and applying these vectors in transformer-like classifiers, showing that
steering vectors, traditionally used in generative models, can also be
effective in classification. More broadly, we showcase an extremely cheap,
inference time, training free method to mitigate bias in classification models.

</details>


### [613] [Quantum-Classical Hybrid Quantized Neural Network](https://arxiv.org/pdf/2506.18240)
*Wenxin Li, Chuan Wang, Hongdong Zhu, Qi Gao, Yin Ma, Hai Wei, Kai Wen*

Main category: cs.LG

TL;DR: A novel Quadratic Binary Optimization (QBO) model for quantized neural network training is introduced, using spline interpolation for arbitrary activation and loss functions. Forward Interval Propagation (FIP) handles non-linearity and multi-layer structures, preserving universal approximation properties. Quantum Conditional Gradient Descent (QCGD) solves the QCBO problem efficiently, with theoretical bounds on error and convergence. Experiments show 94.95% accuracy on Fashion MNIST with 1.1-bit precision.


<details>
  <summary>Details</summary>
Motivation: To enable quantized neural network training with arbitrary activation and loss functions while leveraging quantum computing for optimization, addressing challenges like non-linearity and constraints in large-scale QCBO problems.

Method: Proposes Forward Interval Propagation (FIP) for discretizing activation functions into linear subintervals and employs Quantum Conditional Gradient Descent (QCGD) to solve the QCBO problem, with theoretical analysis of error bounds and convergence.

Result: Achieves 94.95% accuracy on Fashion MNIST with 1.1-bit precision, demonstrating the effectiveness of the QBO model and QCGD algorithm.

Conclusion: The QBO model and QCGD algorithm successfully address quantized neural network training challenges, broadening quantum computing's applicability in AI with practical performance.

Abstract: Here in this work, we present a novel Quadratic Binary Optimization (QBO)
model for quantized neural network training, enabling the use of arbitrary
activation and loss functions through spline interpolation. We introduce
Forward Interval Propagation (FIP), a method designed to tackle the challenges
of non-linearity and the multi-layer composite structure in neural networks by
discretizing activation functions into linear subintervals. This approach
preserves the universal approximation properties of neural networks while
allowing complex nonlinear functions to be optimized using quantum computers,
thus broadening their applicability in artificial intelligence. We provide
theoretical upper bounds on the approximation error and the number of Ising
spins required, by deriving the sample complexity of the empirical risk
minimization problem, from an optimization perspective. A significant challenge
in solving the associated Quadratic Constrained Binary Optimization (QCBO)
model on a large scale is the presence of numerous constraints. When employing
the penalty method to handle these constraints, tuning a large number of
penalty coefficients becomes a critical hyperparameter optimization problem,
increasing computational complexity and potentially affecting solution quality.
To address this, we employ the Quantum Conditional Gradient Descent (QCGD)
algorithm, which leverages quantum computing to directly solve the QCBO
problem. We prove the convergence of QCGD under a quantum oracle with
randomness and bounded variance in objective value, as well as under limited
precision constraints in the coefficient matrix. Additionally, we provide an
upper bound on the Time-To-Solution for the QCBO solving process. Experimental
results using a coherent Ising machine (CIM) demonstrate a 94.95% accuracy on
the Fashion MNIST classification task, with only 1.1-bit precision.

</details>


### [614] [ReDit: Reward Dithering for Improved LLM Policy Optimization](https://arxiv.org/pdf/2506.18631)
*Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu*

Main category: cs.LG

TL;DR: ReDit (Reward Dithering) improves LLM reasoning by adding random noise to discrete rewards, solving gradient issues and speeding up convergence.


<details>
  <summary>Details</summary>
Motivation: Discrete rewards in rule-based systems cause gradient anomalies and slow convergence.

Method: ReDit dithers discrete rewards with random noise to provide smoother gradients and encourage exploration.

Result: ReDit matches vanilla GRPO performance in 10% of the steps and outperforms it by 4% with similar training.

Conclusion: ReDit effectively mitigates gradient issues, accelerates convergence, and enhances model performance.

Abstract: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning
capabilities through its rule-based reward system. While it's a ''perfect''
reward system that effectively mitigates reward hacking, such reward functions
are often discrete. Our experimental observations suggest that discrete rewards
can lead to gradient anomaly, unstable optimization, and slow convergence. To
address this issue, we propose ReDit (Reward Dithering), a method that dithers
the discrete reward signal by adding simple random noise. With this perturbed
reward, exploratory gradients are continuously provided throughout the learning
process, enabling smoother gradient updates and accelerating convergence. The
injected noise also introduces stochasticity into flat reward regions,
encouraging the model to explore novel policies and escape local optima.
Experiments across diverse tasks demonstrate the effectiveness and efficiency
of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO
with only approximately 10% the training steps, and furthermore, still exhibits
a 4% performance improvement over vanilla GRPO when trained for a similar
duration. Visualizations confirm significant mitigation of gradient issues with
ReDit. Moreover, theoretical analyses are provided to further validate these
advantages.

</details>


### [615] [Dual-Forward Path Teacher Knowledge Distillation: Bridging the Capacity Gap Between Teacher and Student](https://arxiv.org/pdf/2506.18244)
*Tong Li, Long Liu, Yihang Hu, Hu Chen, Shifeng Chen*

Main category: cs.LG

TL;DR: DFPT-KD and DFPT-KD+ improve knowledge distillation by addressing the capacity gap between teacher and student networks using prompt-based tuning and dual-forward paths, outperforming vanilla KD.


<details>
  <summary>Details</summary>
Motivation: The capacity gap between teacher and student networks limits distillation gains, and existing methods either discard accurate knowledge or fail to dynamically adjust it.

Method: Proposes DFPT-KD, introducing a dual-forward path teacher with prompt-based tuning, and DFPT-KD+, which fine-tunes the prompt-based path for better compatibility.

Result: DFPT-KD and DFPT-KD+ outperform vanilla KD, with DFPT-KD+ achieving state-of-the-art accuracy.

Conclusion: Prompt-based tuning and dual-forward paths effectively address the capacity gap, enhancing student performance in knowledge distillation.

Abstract: Knowledge distillation (KD) provides an effective way to improve the
performance of a student network under the guidance of pre-trained teachers.
However, this approach usually brings in a large capacity gap between teacher
and student networks, limiting the distillation gains. Previous methods
addressing this problem either discard accurate knowledge representation or
fail to dynamically adjust the transferred knowledge, which is less effective
in addressing the capacity gap problem and hinders students from achieving
comparable performance with the pre-trained teacher. In this work, we extend
the ideology of prompt-based learning to address the capacity gap problem, and
propose Dual-Forward Path Teacher Knowledge Distillation (DFPT-KD), which
replaces the pre-trained teacher with a novel dual-forward path teacher to
supervise the learning of student. The key to DFPT-KD is prompt-based tuning,
i.e., establishing an additional prompt-based forward path within the
pre-trained teacher and optimizing it with the pre-trained teacher frozen to
make the transferred knowledge compatible with the representation ability of
the student. Extensive experiments demonstrate that DFPT-KD leads to trained
students performing better than the vanilla KD. To make the transferred
knowledge better compatible with the representation abilities of the student,
we further fine-tune the whole prompt-based forward path, yielding a novel
distillation approach dubbed DFPT-KD+. By extensive experiments, it is shown
that DFPT-KD+ improves upon DFPT-KD and achieves state-of-the-art accuracy
performance.

</details>


### [616] [Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation](https://arxiv.org/pdf/2506.18716)
*Jie Li, Shifei Ding, Lili Guo, Xuan Li*

Main category: cs.LG

TL;DR: The paper proposes MAGTKD, a model for Emotion Recognition in Conversation (ERC), enhancing modality representations via prompt learning and knowledge distillation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing ERC models neglect varying modality contributions and introduce complexity by aligning modalities at the frame level.

Method: MAGTKD uses prompt learning for textual modality, knowledge distillation for weaker modalities, and a multi-modal anchor gated transformer for integration.

Result: Experiments on IEMOCAP and MELD datasets show improved modality representations and state-of-the-art performance.

Conclusion: MAGTKD effectively addresses ERC challenges, enhancing modality integration and achieving top results.

Abstract: Emotion Recognition in Conversation (ERC) aims to detect the emotions of
individual utterances within a conversation. Generating efficient and
modality-specific representations for each utterance remains a significant
challenge. Previous studies have proposed various models to integrate features
extracted using different modality-specific encoders. However, they neglect the
varying contributions of modalities to this task and introduce high complexity
by aligning modalities at the frame level. To address these challenges, we
propose the Multi-modal Anchor Gated Transformer with Knowledge Distillation
(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance
textual modality representations, while knowledge distillation is utilized to
strengthen representations of weaker modalities. Furthermore, we introduce a
multi-modal anchor gated transformer to effectively integrate utterance-level
representations across modalities. Extensive experiments on the IEMOCAP and
MELD datasets demonstrate the effectiveness of knowledge distillation in
enhancing modality representations and achieve state-of-the-art performance in
emotion recognition. Our code is available at:
https://github.com/JieLi-dd/MAGTKD.

</details>


### [617] [Exploring Efficient Quantification of Modeling Uncertainties with Differentiable Physics-Informed Machine Learning Architectures](https://arxiv.org/pdf/2506.18247)
*Manaswin Oddiraju, Bharath Varma Penumatsa, Divyang Amin, Michael Piedmonte, Souma Chowdhury*

Main category: cs.LG

TL;DR: The paper explores integrating Bayesian Neural Networks (BNNs) into Physics-Informed Machine Learning (PIML) architectures to enhance uncertainty propagation, using a two-stage training process and evaluating performance on benchmark and flight data.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored potential of PIML methods in predicting and propagating modeling uncertainties, crucial for reliability analysis and robust optimization in engineering.

Method: Integration of BNNs into auto-differentiable hybrid PIML architectures, employing a two-stage training process to overcome challenges in probabilistic ML training.

Result: BNN-integrated PIML showed slightly worse or comparable prediction performance to purely data-driven ML and original PIML models, with Monte Carlo sampling of BNN weights effectively propagating uncertainty.

Conclusion: BNNs can successfully provision uncertainty propagation in PIML architectures, supported by auto-differentiability, though performance trade-offs exist.

Abstract: Quantifying and propagating modeling uncertainties is crucial for reliability
analysis, robust optimization, and other model-based algorithmic processes in
engineering design and control. Now, physics-informed machine learning (PIML)
methods have emerged in recent years as a new alternative to traditional
computational modeling and surrogate modeling methods, offering a balance
between computing efficiency, modeling accuracy, and interpretability. However,
their ability to predict and propagate modeling uncertainties remains mostly
unexplored. In this paper, a promising class of auto-differentiable hybrid PIML
architectures that combine partial physics and neural networks or ANNs (for
input transformation or adaptive parameter estimation) is integrated with
Bayesian Neural networks (replacing the ANNs); this is done with the goal to
explore whether BNNs can successfully provision uncertainty propagation
capabilities in the PIML architectures as well, further supported by the
auto-differentiability of these architectures. A two-stage training process is
used to alleviate the challenges traditionally encountered in training
probabilistic ML models. The resulting BNN-integrated PIML architecture is
evaluated on an analytical benchmark problem and flight experiments data for a
fixed-wing RC aircraft, with prediction performance observed to be slightly
worse or at par with purely data-driven ML and original PIML models. Moreover,
Monte Carlo sampling of probabilistic BNN weights was found to be most
effective in propagating uncertainty in the BNN-integrated PIML architectures.

</details>


### [618] [Neural Total Variation Distance Estimators for Changepoint Detection in News Data](https://arxiv.org/pdf/2506.18764)
*Csaba Zsolnai, Niels Lrch, Julian Arnold*

Main category: cs.LG

TL;DR: A neural network-based method for detecting shifts in public discourse using news data, leveraging learning-by-confusion to identify changepoints like major events.


<details>
  <summary>Details</summary>
Motivation: Understanding societal dynamics by detecting shifts in public discourse, especially in high-dimensional, noisy data.

Method: Uses learning-by-confusion scheme to train classifiers distinguishing articles from different time periods, measuring classification accuracy to estimate changepoints.

Result: Successfully identified major events (e.g., 9/11, COVID-19) in synthetic and real-world data (The Guardian).

Conclusion: The method autonomously detects discourse shifts with minimal domain knowledge, useful for journalism, policy, and crisis monitoring.

Abstract: Detecting when public discourse shifts in response to major events is crucial
for understanding societal dynamics. Real-world data is high-dimensional,
sparse, and noisy, making changepoint detection in this domain a challenging
endeavor. In this paper, we leverage neural networks for changepoint detection
in news data, introducing a method based on the so-called learning-by-confusion
scheme, which was originally developed for detecting phase transitions in
physical systems. We train classifiers to distinguish between articles from
different time periods. The resulting classification accuracy is used to
estimate the total variation distance between underlying content distributions,
where significant distances highlight changepoints. We demonstrate the
effectiveness of this method on both synthetic datasets and real-world data
from The Guardian newspaper, successfully identifying major historical events
including 9/11, the COVID-19 pandemic, and presidential elections. Our approach
requires minimal domain knowledge, can autonomously discover significant shifts
in public discourse, and yields a quantitative measure of change in content,
making it valuable for journalism, policy analysis, and crisis monitoring.

</details>


### [619] [Ground tracking for improved landmine detection in a GPR system](https://arxiv.org/pdf/2506.18258)
*Li Tang, Peter A. Torrione, Cihat Eldeniz, Leslie M. Collins*

Main category: cs.LG

TL;DR: The paper proposes Kalman and particle filter frameworks to mitigate ground bounce interference in GPR for better landmine detection.


<details>
  <summary>Details</summary>
Motivation: Ground bounce in GPR data degrades landmine detection, especially for low-metal mines. Addressing this interference is critical.

Method: Uses Kalman and particle filters to model ground bounce as a hidden state, with adaptive updates and smoothness constraints.

Result: Experiments show improved ground bounce tracking and enhanced landmine detection performance.

Conclusion: The proposed filters effectively reduce ground bounce interference, boosting landmine detection accuracy.

Abstract: Ground penetrating radar (GPR) provides a promising technology for accurate
subsurface object detection. In particular, it has shown promise for detecting
landmines with low metal content. However, the ground bounce (GB) that is
present in GPR data, which is caused by the dielectric discontinuity between
soil and air, is a major source of interference and degrades landmine detection
performance. To mitigate this interference, GB tracking algorithms formulated
using both a Kalman filter (KF) and a particle filter (PF) framework are
proposed. In particular, the location of the GB in the radar signal is modeled
as the hidden state in a stochastic system for the PF approach. The
observations are the 2D radar images, which arrive scan by scan along the
down-track direction. An initial training stage sets parameters automatically
to accommodate different ground and weather conditions. The features associated
with the GB description are updated adaptively with the arrival of new data.
The prior distribution for a given location is predicted by propagating
information from two adjacent channels/scans, which ensures that the overall GB
surface remains smooth. The proposed algorithms are verified in experiments
utilizing real data, and their performances are compared with other GB tracking
approaches. We demonstrate that improved GB tracking contributes to improved
performance for the landmine detection problem.

</details>


### [620] [ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs](https://arxiv.org/pdf/2506.18267)
*Haseeb Ullah Khan Shinwari, Muhammad Usama*

Main category: cs.LG

TL;DR: ARD-LoRA introduces dynamic rank allocation for LoRA, optimizing rank per attention head via learnable scaling factors, achieving near-full fine-tuning performance with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Fixed-rank LoRA methods fail to address heterogeneous learning dynamics across transformer layers and attention heads, limiting adaptation efficiency.

Method: ARD-LoRA automates rank allocation using learnable scaling factors, optimized via a meta-objective with 1 sparsity and Total Variation regularization for stability.

Result: Achieves 99.3% of full fine-tuning performance with 0.32% trainable parameters, reduces multimodal adaptation memory by 41%, and outperforms baselines like DoRA and AdaLoRA.

Conclusion: Dynamic, fine-grained rank allocation is a critical paradigm for efficient foundation model adaptation.

Abstract: Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing
uniform adaptation across transformer layers and attention heads despite their
heterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic
LoRA (ARD-LoRA), a novel framework that automates rank allocation through
learnable scaling factors. These factors are optimized via a meta-objective
balancing task performance and parameter efficiency, incorporating $\ell_1$
sparsity for minimal rank and Total Variation regularization for stable rank
transitions. ARD-LoRA enables continuous, differentiable, per-head rank
adaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's
efficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32%
trainable parameters, outperforming strong baselines like DoRA and AdaLoRA.
Furthermore, it reduces multimodal adaptation memory by 41%. These results
establish dynamic, fine-grained rank allocation as a critical paradigm for
efficient foundation model adaptation.

</details>


### [621] [Memory-Augmented Architecture for Long-Term Context Handling in Large Language Models](https://arxiv.org/pdf/2506.18271)
*Haseeb Ullah Khan Shinwari, Muhammad Usama*

Main category: cs.LG

TL;DR: A memory-augmented architecture improves long-term context handling in Large Language Models, enhancing coherence and response quality.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with maintaining coherent interactions in extended dialogues due to limited contextual memory, leading to fragmented exchanges.

Method: Proposed a memory-augmented architecture that dynamically retrieves, updates, and prunes relevant past interaction information.

Result: Experimental results show improved contextual coherence, reduced memory overhead, and better response quality.

Conclusion: The solution is effective for real-time applications in interactive systems, addressing long-term context challenges.

Abstract: Large Language Models face significant challenges in maintaining coherent
interactions over extended dialogues due to their limited contextual memory.
This limitation often leads to fragmented exchanges and reduced relevance in
responses, diminishing user experience. To address these issues, we propose a
memory-augmented architecture that dynamically retrieves, updates, and prunes
relevant information from past interactions, ensuring effective long-term
context handling. Experimental results demonstrate that our solution
significantly improves contextual coherence, reduces memory overhead, and
enhances response quality, showcasing its potential for real-time applications
in interactive systems.

</details>


### [622] [Leveraging Large Language Models for Information Verification -- an Engineering Approach](https://arxiv.org/pdf/2506.18274)
*Nguyen Nang Hung, Nguyen Thanh Trong, Vuong Thanh Toan, Nguyen An Phuoc, Dao Minh Tu, Nguyen Manh Duc Tuan, Nguyen Dinh Mau*

Main category: cs.LG

TL;DR: An automated pipeline using GPT-4o for multimedia news verification, involving metadata generation, frame selection, and cross-referencing, with minimal human intervention.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying multimedia news sources efficiently and accurately using advanced LLMs.

Method: Processes images/videos via metadata generation, segmentation, frame selection, and cross-referencing with audio transcripts, all automated by GPT-4o.

Result: A streamlined, automated pipeline for verifying multimedia news sources with high efficiency.

Conclusion: The approach demonstrates the feasibility of using LLMs like GPT-4o for practical, automated news verification.

Abstract: For the ACMMM25 challenge, we present a practical engineering approach to
multimedia news source verification, utilizing Large Language Models (LLMs)
like GPT-4o as the backbone of our pipeline. Our method processes images and
videos through a streamlined sequence of steps: First, we generate metadata
using general-purpose queries via Google tools, capturing relevant content and
links. Multimedia data is then segmented, cleaned, and converted into frames,
from which we select the top-K most informative frames. These frames are
cross-referenced with metadata to identify consensus or discrepancies.
Additionally, audio transcripts are extracted for further verification.
Noticeably, the entire pipeline is automated using GPT-4o through prompt
engineering, with human intervention limited to final validation.

</details>


### [623] [Learning Causal Graphs at Scale: A Foundation Model Approach](https://arxiv.org/pdf/2506.18285)
*Naiyu Yin, Tian Gao, Yue Yu*

Main category: cs.LG

TL;DR: ADAG introduces an attention-based model for learning multiple DAGs efficiently, improving accuracy and zero-shot inference.


<details>
  <summary>Details</summary>
Motivation: DAG learning is computationally expensive and suffers from identifiability issues, especially with small samples.

Method: ADAG uses a nonlinear attention-based kernel to map data to graph structures and parameters, optimizing across tasks.

Result: ADAG improves DAG learning accuracy and zero-shot efficiency on synthetic benchmarks.

Conclusion: ADAG is the first foundation model for DAG learning, enhancing efficiency and generalizability in causal discovery.

Abstract: Due to its human-interpretability and invariance properties, Directed Acyclic
Graph (DAG) has been a foundational tool across various areas of AI research,
leading to significant advancements. However, DAG learning remains highly
challenging, due to its super-exponential growth in computational cost and
identifiability issues, particularly in small-sample regimes. To address these
two challenges, in this work we leverage the recent success of linear
transformers and develop a foundation model approach for discovering multiple
order-consistent DAGs across tasks. In particular, we propose Attention-DAG
(ADAG), a novel attention-mechanism-based architecture for learning multiple
linear Structural Equation Models (SEMs). ADAG learns the mapping from observed
data to both graph structure and parameters via a nonlinear attention-based
kernel, enabling efficient multi-task estimation of the underlying linear SEMs.
By formulating the learning process across multiple tasks as a continuous
optimization problem, the pre-trained ADAG model captures the common structural
properties as a shared low-dimensional prior, thereby reducing the
ill-posedness of downstream DAG learning tasks in small-sample regimes. We
evaluate our proposed approach on benchmark synthetic datasets and find that
ADAG achieves substantial improvements in both DAG learning accuracy and
zero-shot inference efficiency. To the best of our knowledge, this is the first
practical approach for pre-training a foundation model specifically designed
for DAG learning, representing a step toward more efficient and generalizable
down-stream applications in causal discovery.

</details>


### [624] [Learning High-Quality Latent Representations for Anomaly Detection and Signal Integrity Enhancement in High-Speed Signals](https://arxiv.org/pdf/2506.18288)
*Muhammad Usama, Hee-Deok Jang, Soham Shanbhag, Yoo-Chang Sung, Seung-Jun Bae, Dong Eui Chang*

Main category: cs.LG

TL;DR: Proposes a joint training framework combining autoencoder and classifier for better anomaly detection and signal integrity in high-speed DRAM signals, outperforming baselines and improving signal integrity by 11.3%.


<details>
  <summary>Details</summary>
Motivation: Addressing the dual challenge of enhancing anomaly detection and signal integrity in high-speed DRAM signals.

Method: Joint training framework integrating autoencoder and classifier to learn distinctive latent representations, evaluated across three anomaly detection algorithms.

Result: Outperforms two baseline methods; signal integrity improved by an average of 11.3%.

Conclusion: The framework effectively enhances anomaly detection and signal integrity, supported by ablation studies and available source code.

Abstract: This paper addresses the dual challenge of improving anomaly detection and
signal integrity in high-speed dynamic random access memory signals. To achieve
this, we propose a joint training framework that integrates an autoencoder with
a classifier to learn more distinctive latent representations by focusing on
valid data features. Our approach is evaluated across three anomaly detection
algorithms and consistently outperforms two baseline methods. Detailed ablation
studies further support these findings. Furthermore, we introduce a signal
integrity enhancement algorithm that improves signal integrity by an average of
11.3%. The source code and data used in this study are available at
https://github.com/Usama1002/learning-latent-representations.

</details>


### [625] [Instability in Diffusion ODEs: An Explanation for Inaccurate Image Reconstruction](https://arxiv.org/pdf/2506.18290)
*Han Zhang, Jinghong Mao, Shangwen Zhu, Zhantao Yang, Lianghua Huang, Yu Liu, Deli Zhao, Ruili Feng, Fan Cheng*

Main category: cs.LG

TL;DR: The paper identifies instability in the PF-ODE generation process as a key cause of reconstruction errors in diffusion models, proving its inevitability in high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: To explain and address the noticeable reconstruction errors in diffusion-based applications like image editing and restoration, which cannot be attributed solely to numerical errors.

Method: The study analyzes the instability in PF-ODE generation, conducts experiments on toy examples and open-source diffusion models, and provides theoretical proof of instability's inevitability in high-dimensional data.

Result: The instability in PF-ODE generation amplifies reconstruction errors, and its probability converges to one as data dimensionality increases.

Conclusion: The findings reveal inherent challenges in diffusion-based reconstruction and suggest directions for future improvements.

Abstract: Diffusion reconstruction plays a critical role in various applications such
as image editing, restoration, and style transfer. In theory, the
reconstruction should be simple - it just inverts and regenerates images by
numerically solving the Probability Flow-Ordinary Differential Equation
(PF-ODE). Yet in practice, noticeable reconstruction errors have been observed,
which cannot be well explained by numerical errors. In this work, we identify a
deeper intrinsic property in the PF-ODE generation process, the instability,
that can further amplify the reconstruction errors. The root of this
instability lies in the sparsity inherent in the generation distribution, which
means that the probability is concentrated on scattered and small regions while
the vast majority remains almost empty. To demonstrate the existence of
instability and its amplification on reconstruction error, we conduct
experiments on both toy numerical examples and popular open-sourced diffusion
models. Furthermore, based on the characteristics of image data, we
theoretically prove that the instability's probability converges to one as the
data dimensionality increases. Our findings highlight the inherent challenges
in diffusion-based reconstruction and can offer insights for future
improvements.

</details>


### [626] [GeNeRT: A Physics-Informed Approach to Intelligent Wireless Channel Modeling via Generalizable Neural Ray Tracing](https://arxiv.org/pdf/2506.18295)
*Kejia Bian, Meixia Tao, Shu Sun, Jun Yu*

Main category: cs.LG

TL;DR: GeNeRT is a generalizable neural ray tracing framework that improves generalization, accuracy, and efficiency in channel modeling by addressing spatial dependence and electromagnetic law adherence.


<details>
  <summary>Details</summary>
Motivation: Current neural RT methods lack generalization and adherence to electromagnetic laws, limiting their practical use.

Method: GeNeRT incorporates Fresnel-inspired neural design and GPU-tensorized acceleration for intra- and inter-scenario generalization and efficiency.

Result: GeNeRT outperforms baselines in generalization and MPC prediction accuracy, and surpasses Wireless Insite in runtime efficiency.

Conclusion: GeNeRT effectively captures ray-surface interactions and offers a scalable solution for neural RT.

Abstract: Neural ray tracing (RT) has emerged as a promising paradigm for channel
modeling by combining physical propagation principles with neural networks. It
enables high modeling accuracy and efficiency. However, current neural RT
methods face two key limitations: constrained generalization capability due to
strong spatial dependence, and weak adherence to electromagnetic laws. In this
paper, we propose GeNeRT, a Generalizable Neural RT framework with enhanced
generalization, accuracy and efficiency. GeNeRT supports both intra-scenario
spatial transferability and inter-scenario zero-shot generalization. By
incorporating Fresnel-inspired neural network design, it also achieves higher
accuracy in multipath component (MPC) prediction. Furthermore, a GPU-tensorized
acceleration strategy is introduced to improve runtime efficiency. Extensive
experiments conducted in outdoor scenarios demonstrate that GeNeRT generalizes
well across untrained regions within a scenario and entirely unseen
environments, and achieves superior accuracy in MPC prediction compared to
baselines. Moreover, it outperforms Wireless Insite in runtime efficiency,
particularly in multi-transmitter settings. Ablation experiments validate the
effectiveness of the network architecture and training strategy in capturing
physical principles of ray-surface interactions.

</details>


### [627] [Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies](https://arxiv.org/pdf/2506.18304)
*Junchao Fan, Xuyang Lei, Xiaolin Chang*

Main category: cs.LG

TL;DR: Proposes an adaptive expert-guided adversarial attack method for DRL-based autonomous driving policies to improve attack efficiency and training stability.


<details>
  <summary>Details</summary>
Motivation: DRL policies for autonomous driving are vulnerable to adversarial attacks, and existing methods are inefficient or unstable.

Method: Uses imitation learning to derive an expert policy, guides DRL adversary with KL-divergence regularization, and employs performance-aware annealing.

Result: Outperforms existing methods in collision rate, attack efficiency, and training stability, even with sub-optimal experts.

Conclusion: The method effectively addresses inefficiency and instability in adversarial attacks on DRL policies.

Abstract: Deep reinforcement learning (DRL) has emerged as a promising paradigm for
autonomous driving. However, despite their advanced capabilities, DRL-based
policies remain highly vulnerable to adversarial attacks, posing serious safety
risks in real-world deployments. Investigating such attacks is crucial for
revealing policy vulnerabilities and guiding the development of more robust
autonomous systems. While prior attack methods have made notable progress, they
still face several challenges: 1) they often rely on high-frequency attacks,
yet critical attack opportunities are typically context-dependent and
temporally sparse, resulting in inefficient attack patterns; 2) restricting
attack frequency can improve efficiency but often results in unstable training
due to the adversary's limited exploration. To address these challenges, we
propose an adaptive expert-guided adversarial attack method that enhances both
the stability and efficiency of attack policy training. Our method first
derives an expert policy from successful attack demonstrations using imitation
learning, strengthened by an ensemble Mixture-of-Experts architecture for
robust generalization across scenarios. This expert policy then guides a
DRL-based adversary through a KL-divergence regularization term. Due to the
diversity of scenarios, expert policies may be imperfect. To address this, we
further introduce a performance-aware annealing strategy that gradually reduces
reliance on the expert as the adversary improves. Extensive experiments
demonstrate that our method achieves outperforms existing approaches in terms
of collision rate, attack efficiency, and training stability, especially in
cases where the expert policy is sub-optimal.

</details>


### [628] [Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and Symbolic Discovery of Nonlinear Dynamics](https://arxiv.org/pdf/2506.18339)
*Wei Liu, Kiran Bacsa, Loon Ching Tang, Eleni Chatzi*

Main category: cs.LG

TL;DR: SKANODE integrates structured state-space modeling with Kolmogorov-Arnold Networks (KAN) to create interpretable and accurate models of nonlinear dynamical systems.


<details>
  <summary>Details</summary>
Motivation: The challenge of achieving highly accurate and physically interpretable models for nonlinear dynamical systems motivates the development of SKANODE.

Method: SKANODE uses a trainable KAN within a Neural ODE framework for virtual sensing and symbolic regression to extract interpretable governing dynamics.

Result: SKANODE outperforms existing methods, providing interpretable, physics-consistent models that reveal underlying system mechanisms.

Conclusion: SKANODE successfully balances accuracy and interpretability, offering a powerful tool for modeling nonlinear dynamical systems.

Abstract: Understanding and modeling nonlinear dynamical systems is a fundamental
problem across scientific and engineering domains. While deep learning has
demonstrated remarkable potential for learning complex system behavior,
achieving models that are both highly accurate and physically interpretable
remains a major challenge. To address this, we propose Structured
Kolmogorov-Arnold Neural ODEs (SKANODEs), a novel framework that integrates
structured state-space modeling with the Kolmogorov-Arnold Network (KAN).
SKANODE first employs a fully trainable KAN as a universal function
approximator within a structured Neural ODE framework to perform virtual
sensing, recovering latent states that correspond to physically interpretable
quantities such as positions and velocities. Once this structured latent
representation is established, we exploit the symbolic regression capability of
KAN to extract compact and interpretable expressions for the system's governing
dynamics. The resulting symbolic expression is then substituted back into the
Neural ODE framework and further calibrated through continued training to
refine its coefficients, enhancing both the precision of the discovered
equations and the predictive accuracy of system responses. Extensive
experiments on both simulated and real-world systems demonstrate that SKANODE
achieves superior performance while offering interpretable, physics-consistent
models that uncover the underlying mechanisms of nonlinear dynamical systems.

</details>


### [629] [Controlled Generation with Equivariant Variational Flow Matching](https://arxiv.org/pdf/2506.18340)
*Floor Eijkelboom, Heiko Zimmermann, Sharvaree Vadgama, Erik J Bekkers, Max Welling, Christian A. Naesseth, Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: The paper introduces a controlled generation objective within Variational Flow Matching (VFM), enabling two implementation methods: end-to-end training of conditional models or Bayesian inference for post hoc control. It also addresses equivariant generation for molecular structures and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To bridge flow-based generative modeling and Bayesian inference, providing a scalable framework for controlled and symmetry-aware generation, particularly in molecular contexts.

Method: Derives a controlled generation objective in VFM, offering two approaches: end-to-end training of conditional models and Bayesian inference for post hoc control. Also formulates an equivariant VFM for molecular generation.

Result: Achieves state-of-the-art performance in uncontrolled and controlled molecular generation, outperforming existing models in both end-to-end and Bayesian inference settings.

Conclusion: The work strengthens the link between flow-based models and Bayesian inference, providing a principled framework for constraint-driven and symmetry-aware generation.

Abstract: We derive a controlled generation objective within the framework of
Variational Flow Matching (VFM), which casts flow matching as a variational
inference problem. We demonstrate that controlled generation can be implemented
two ways: (1) by way of end-to-end training of conditional generative models,
or (2) as a Bayesian inference problem, enabling post hoc control of
unconditional models without retraining. Furthermore, we establish the
conditions required for equivariant generation and provide an equivariant
formulation of VFM tailored for molecular generation, ensuring invariance to
rotations, translations, and permutations. We evaluate our approach on both
uncontrolled and controlled molecular generation, achieving state-of-the-art
performance on uncontrolled generation and outperforming state-of-the-art
models in controlled generation, both with end-to-end training and in the
Bayesian inference setting. This work strengthens the connection between
flow-based generative modeling and Bayesian inference, offering a scalable and
principled framework for constraint-driven and symmetry-aware generation.

</details>


### [630] [LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization](https://arxiv.org/pdf/2506.18383)
*Koushik Viswanadha, Deepanway Ghosal, Somak Aditya*

Main category: cs.LG

TL;DR: The paper proposes finetuning LLMs with preference optimization to improve logical reasoning by better converting natural language problems into logical formulations.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to accurately translate natural language reasoning problems into logical forms, limiting LLMs' reasoning capabilities.

Method: Introduces LogicPO dataset and uses DPO/KTO to finetune LLMs like Phi-3.5.

Result: Outperforms GPT-3.5-turbo (8-shot) with 10% more correct logic and 14% fewer syntax errors.

Conclusion: Offers a promising approach to enhance LLM reasoning through improved logical representation.

Abstract: Logical reasoning is a key task for artificial intelligence due to it's role
in major downstream tasks such as Question Answering, Summarization. Recent
methods in improving the reasoning ability of LLMs fall short in correctly
converting a natural language reasoning problem to an equivalent logical
formulation, which hinders the framework's overall ability to reason. Towards
this, we propose to use finetuning on a preference optimization dataset to
learn to parse and represent a natural language problem as a whole to a
consistent logical program by 1) introducing a new supervised and preference
optimization dataset LogicPO, and 2) adopting popular techniques such as Direct
Preference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune
open-source LLMs. Our best model with Phi-3.5 consistently outperforms
GPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%
less syntax errors. Through the framework and our improved evaluation metrics,
we offer a promising direction in improving the logical reasoning of LLMs by
better representing them in their logical formulations.

</details>


### [631] [ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction](https://arxiv.org/pdf/2506.18396)
*Marco Aruta, Ciro Listone, Giuseppe Murano, Aniello Murano*

Main category: cs.LG

TL;DR: ADNF introduces a dynamic neuro-fuzzy clustering framework for leukemia diagnosis, combining CNN feature extraction with online fuzzy clustering, outperforming static methods.


<details>
  <summary>Details</summary>
Motivation: Conventional clustering lacks flexibility for evolving cellular patterns and real-time uncertainty quantification in leukemia diagnosis.

Method: Uses CNN-based feature extraction and online fuzzy clustering with Fuzzy C-Means initialization, updated via Fuzzy Temporal Index (FTI). Includes topology refinement for density-weighted merging and entropy-guided splitting.

Result: Achieves a silhouette score of 0.51 on the C-NMC leukemia dataset, showing better cohesion and separation than static methods.

Conclusion: ADNF's adaptive uncertainty modeling and label-free operation offer scalable support for personalized leukemia management, with potential for integration in pediatric oncology networks.

Abstract: Leukemia diagnosis and monitoring rely increasingly on high-throughput image
data, yet conventional clustering methods lack the flexibility to accommodate
evolving cellular patterns and quantify uncertainty in real time. We introduce
Adaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable
framework that combines Convolutional Neural Network-based feature extraction
with an online fuzzy clustering engine. ADNF initializes soft partitions via
Fuzzy C-Means, then continuously updates micro-cluster centers, densities, and
fuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy
evolution. A topology refinement stage performs density-weighted merging and
entropy-guided splitting to guard against over- and under-segmentation. On the
C-NMC leukemia microscopy dataset, our tool achieves a silhouette score of
0.51, demonstrating superior cohesion and separation over static baselines. The
method's adaptive uncertainty modeling and label-free operation hold immediate
potential for integration within the INFANT pediatric oncology network,
enabling scalable, up-to-date support for personalized leukemia management.

</details>


### [632] [FREQuency ATTribution: Benchmarking Frequency-based Occlusion for Time Series Data](https://arxiv.org/pdf/2506.18481)
*Dominique Mercier, Andreas Dengel, Sheraz, Ahmed*

Main category: cs.LG

TL;DR: FreqATT is a framework for interpreting time-series-based deep neural networks using frequency-domain analysis, outperforming existing methods in highlighting relevant signal areas and robustness.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks lack interpretability, especially for time-series data, limiting their usability. Existing methods are insufficient for time-series analysis.

Method: FreqATT evaluates relevant frequencies in the input signal, filtering or marking key data to enable post-hoc interpretation.

Result: Frequency-domain analysis highlights relevant signal areas better and is more robust to fluctuations than existing methods.

Conclusion: FreqATT provides a more effective and robust approach for interpreting time-series-based deep neural networks.

Abstract: Deep neural networks are among the most successful algorithms in terms of
performance and scalability in different domains. However, since these networks
are black boxes, their usability is severely restricted due to the lack of
interpretability. Existing interpretability methods do not address the analysis
of time-series-based networks specifically enough. This paper shows that an
analysis in the frequency domain can not only highlight relevant areas in the
input signal better than existing methods, but is also more robust to
fluctuations in the signal. In this paper, FreqATT is presented, a framework
that enables post-hoc networks to interpret time series analysis. To achieve
this, the relevant different frequencies are evaluated and the signal is either
filtered or the relevant input data is marked.

</details>


### [633] [Reliability-Adjusted Prioritized Experience Replay](https://arxiv.org/pdf/2506.18482)
*Leonard S. Pleiss, Tobias Sutter, Maximilian Schiffer*

Main category: cs.LG

TL;DR: ReaPER improves PER by introducing a reliability measure for temporal difference errors, enhancing learning efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of uniform sampling in PER by leveraging the reliability of temporal difference errors.

Method: Proposes ReaPER, which adjusts PER with a novel measure of temporal difference error reliability.

Result: ReaPER outperforms PER in learning efficiency across environments, including Atari-5.

Conclusion: ReaPER offers a more efficient alternative to PER by incorporating reliability into experience prioritization.

Abstract: Experience replay enables data-efficient learning from past experiences in
online reinforcement learning agents. Traditionally, experiences were sampled
uniformly from a replay buffer, regardless of differences in
experience-specific learning potential. In an effort to sample more
efficiently, researchers introduced Prioritized Experience Replay (PER). In
this paper, we propose an extension to PER by introducing a novel measure of
temporal difference error reliability. We theoretically show that the resulting
transition selection algorithm, Reliability-adjusted Prioritized Experience
Replay (ReaPER), enables more efficient learning than PER. We further present
empirical results showing that ReaPER outperforms PER across various
environment types, including the Atari-5 benchmark.

</details>


### [634] [AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing](https://arxiv.org/pdf/2506.18495)
*Aniss Bessalah, Hatem Mohamed Abdelmoumen, Karima Benatchba, Hadjer Benmeziane*

Main category: cs.LG

TL;DR: AnalogNAS-Bench is introduced as the first NAS benchmark for AIMC, revealing key insights about robust architectures and limitations of current methods.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art neural networks are not designed for AIMC's non-idealities, necessitating a dedicated NAS benchmark.

Method: The study introduces AnalogNAS-Bench to evaluate NAS methodologies for AIMC, analyzing quantization, architecture features, and skip connections.

Result: Findings show standard quantization fails for AIMC, robust architectures are wider and branched, and skip connections help with noise resilience.

Conclusion: The benchmark highlights current NAS limitations for AIMC and guides future analog-aware NAS development.

Abstract: Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm
for accelerating Deep Neural Networks (DNNs), offering significant energy and
latency benefits over conventional digital hardware. However, state-of-the-art
neural networks are not inherently designed for AIMC, as they fail to account
for its unique non-idealities. Neural Architecture Search (NAS) is thus needed
to systematically discover neural architectures optimized explicitly for AIMC
constraints. However, comparing NAS methodologies and extracting insights about
robust architectures for AIMC requires a dedicated NAS benchmark that
explicitly accounts for AIMC-specific hardware non-idealities. To address this,
we introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for
AIMC. Our study reveals three key insights: (1) standard quantization
techniques fail to capture AIMC-specific noises, (2) robust architectures tend
to feature wider and branched blocks, (3) skip connections improve resilience
to temporal drift noise. These insights highlight the limitations of current
NAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the
implementations used in this paper can be found at
https://github.com/IBM/analog-nas/tree/main/analognasbench.

</details>


### [635] [PuckTrick: A Library for Making Synthetic Data More Realistic](https://arxiv.org/pdf/2506.18499)
*Alessandra Agostini, Andrea Maurino, Blerina Spahiu*

Main category: cs.LG

TL;DR: Pucktrick, a Python library, introduces controlled errors into synthetic datasets to improve ML model robustness by mimicking real-world data imperfections.


<details>
  <summary>Details</summary>
Motivation: Real-world datasets are often restricted, and synthetic data lacks imperfections, impacting model generalization.

Method: Pucktrick systematically contaminates synthetic data with errors like missing values, noise, and misclassification.

Result: ML models trained on contaminated synthetic data outperform those on clean synthetic data, especially for tree-based and linear models.

Conclusion: Controlled contamination of synthetic data enhances model resilience, making Pucktrick a valuable tool for ML robustness testing.

Abstract: The increasing reliance on machine learning (ML) models for decision-making
requires high-quality training data. However, access to real-world datasets is
often restricted due to privacy concerns, proprietary restrictions, and
incomplete data availability. As a result, synthetic data generation (SDG) has
emerged as a viable alternative, enabling the creation of artificial datasets
that preserve the statistical properties of real data while ensuring privacy
compliance. Despite its advantages, synthetic data is often overly clean and
lacks real-world imperfections, such as missing values, noise, outliers, and
misclassified labels, which can significantly impact model generalization and
robustness. To address this limitation, we introduce Pucktrick, a Python
library designed to systematically contaminate synthetic datasets by
introducing controlled errors. The library supports multiple error types,
including missing data, noisy values, outliers, label misclassification,
duplication, and class imbalance, offering a structured approach to evaluating
ML model resilience under real-world data imperfections. Pucktrick provides two
contamination modes: one for injecting errors into clean datasets and another
for further corrupting already contaminated datasets. Through extensive
experiments on real-world financial datasets, we evaluate the impact of
systematic data contamination on model performance. Our findings demonstrate
that ML models trained on contaminated synthetic data outperform those trained
on purely synthetic, error-free data, particularly for tree-based and linear
models such as SVMs and Extra Trees.

</details>


### [636] [DDOT: A Derivative-directed Dual-decoder Ordinary Differential Equation Transformer for Dynamic System Modeling](https://arxiv.org/pdf/2506.18522)
*Yang Chang, Kuang-Da Wang, Ping-Chun Hsieh, Cheng-Kuan Lin, Wen-Chih Peng*

Main category: cs.LG

TL;DR: The paper introduces DDOT, a transformer-based model for reconstructing multidimensional ODEs, and the DIV-diff metric for stable evaluation. DDOT outperforms existing methods, showing practical impact.


<details>
  <summary>Details</summary>
Motivation: Traditional symbolic regression struggles with temporal dynamics in ODEs. ODEFormer's single-trajectory evaluation is sensitive to initial conditions, limiting performance assessment.

Method: Proposes DDOT, a transformer-based model with an auxiliary task predicting ODE derivatives, and the DIV-diff metric for comprehensive evaluation over a grid of points.

Result: DDOT outperforms existing methods, improving reconstruction and generalization by 4.58% and 1.62%, respectively, and reducing DIV-diff by 3.55%. It also performs well on real-world data.

Conclusion: DDOT and DIV-diff provide a robust framework for ODE reconstruction, addressing limitations of prior methods and demonstrating practical utility.

Abstract: Uncovering the underlying ordinary differential equations (ODEs) that govern
dynamic systems is crucial for advancing our understanding of complex
phenomena. Traditional symbolic regression methods often struggle to capture
the temporal dynamics and intervariable correlations inherent in ODEs.
ODEFormer, a state-of-the-art method for inferring multidimensional ODEs from
single trajectories, has made notable progress. However, its focus on
single-trajectory evaluation is highly sensitive to initial starting points,
which may not fully reflect true performance. To address this, we propose the
divergence difference metric (DIV-diff), which evaluates divergence over a grid
of points within the target region, offering a comprehensive and stable
analysis of the variable space. Alongside, we introduce DDOT
(Derivative-Directed Dual-Decoder Ordinary Differential Equation Transformer),
a transformer-based model designed to reconstruct multidimensional ODEs in
symbolic form. By incorporating an auxiliary task predicting the ODE's
derivative, DDOT effectively captures both structure and dynamic behavior.
Experiments on ODEBench show DDOT outperforms existing symbolic regression
methods, achieving an absolute improvement of 4.58% and 1.62% in $P(R^2 > 0.9)$
for reconstruction and generalization tasks, respectively, and an absolute
reduction of 3.55% in DIV-diff. Furthermore, DDOT demonstrates real-world
applicability on an anesthesia dataset, highlighting its practical impact.

</details>


### [637] [Federated Learning from Molecules to Processes: A Perspective](https://arxiv.org/pdf/2506.18525)
*Jan G. Rittig, Clemens Kortmann*

Main category: cs.LG

TL;DR: Federated learning enables collaborative ML model training in chemical engineering without sharing proprietary data, improving accuracy while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Proprietary data silos in the chemical industry hinder ML model training; federated learning offers a solution by allowing collaborative training without data disclosure.

Method: Applied federated learning in two case studies: (i) binary mixture activity coefficient prediction with graph neural networks, and (ii) distillation column system identification with autoencoders.

Result: Federated learning models outperformed individual company models and matched the accuracy of models trained on combined datasets.

Conclusion: Federated learning holds great promise for advancing ML in chemical engineering while maintaining data privacy, making it suitable for industrial applications.

Abstract: We present a perspective on federated learning in chemical engineering that
envisions collaborative efforts in machine learning (ML) developments within
the chemical industry. Large amounts of chemical and process data are
proprietary to chemical companies and are therefore locked in data silos,
hindering the training of ML models on large data sets in chemical engineering.
Recently, the concept of federated learning has gained increasing attention in
ML research, enabling organizations to jointly train machine learning models
without disclosure of their individual data. We discuss potential applications
of federated learning in several fields of chemical engineering, from the
molecular to the process scale. In addition, we apply federated learning in two
exemplary case studies that simulate practical scenarios of multiple chemical
companies holding proprietary data sets: (i) prediction of binary mixture
activity coefficients with graph neural networks and (ii) system identification
of a distillation column with autoencoders. Our results indicate that ML models
jointly trained with federated learning yield significantly higher accuracy
than models trained by each chemical company individually and can perform
similarly to models trained on combined datasets from all companies. Federated
learning has therefore great potential to advance ML models in chemical
engineering while respecting corporate data privacy, making it promising for
future industrial applications.

</details>


### [638] [Optimization-Induced Dynamics of Lipschitz Continuity in Neural Networks](https://arxiv.org/pdf/2506.18588)
*Risn Luo, James McDermott, Christian Gagn, Qiang Sun, Colm O'Riordan*

Main category: cs.LG

TL;DR: The paper analyzes the temporal evolution of Lipschitz continuity in neural networks during SGD training using a stochastic differential equation framework, identifying key driving factors and validating results experimentally.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of Lipschitz continuity during neural network training, which is crucial for robustness but remains under-explored.

Method: A mathematical framework based on stochastic differential equations (SDEs) models the evolution, considering deterministic and stochastic forces, including gradient flows, noise, and Hessian projections.

Result: Identifies three main factors driving Lipschitz continuity evolution and shows how training conditions (e.g., noisy supervision, batch size) influence it. Experimental results align with theory.

Conclusion: The framework successfully captures the dynamics of Lipschitz continuity during training, providing insights into factors affecting neural network robustness.

Abstract: Lipschitz continuity characterizes the worst-case sensitivity of neural
networks to small input perturbations; yet its dynamics (i.e. temporal
evolution) during training remains under-explored. We present a rigorous
mathematical framework to model the temporal evolution of Lipschitz continuity
during training with stochastic gradient descent (SGD). This framework
leverages a system of stochastic differential equations (SDEs) to capture both
deterministic and stochastic forces. Our theoretical analysis identifies three
principal factors driving the evolution: (i) the projection of gradient flows,
induced by the optimization dynamics, onto the operator-norm Jacobian of
parameter matrices; (ii) the projection of gradient noise, arising from the
randomness in mini-batch sampling, onto the operator-norm Jacobian; and (iii)
the projection of the gradient noise onto the operator-norm Hessian of
parameter matrices. Furthermore, our theoretical framework sheds light on such
as how noisy supervision, parameter initialization, batch size, and mini-batch
sampling trajectories, among other factors, shape the evolution of the
Lipschitz continuity of neural networks. Our experimental results demonstrate
strong agreement between the theoretical implications and the observed
behaviors.

</details>


### [639] [Simulation-Free Differential Dynamics through Neural Conservation Laws](https://arxiv.org/pdf/2506.18604)
*Mengjian Hua, Eric Vanden-Eijnden, Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: A simulation-free framework for training continuous-time diffusion processes, avoiding expensive simulations and restrictive problem formulations by jointly modeling time-dependent densities and diffusion dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited by either restrictive problem formulations or costly simulations. The goal is to enable broader applicability and efficiency.

Method: Uses a coupled parameterization to model time-dependent density functions and diffusion dynamics, incorporating the Fokker-Planck equation as a hard constraint.

Result: Validated in diverse applications, including generative modeling, stochastic optimal control, and spatio-temporal event modeling.

Conclusion: The framework offers a versatile, simulation-free solution for training diffusion processes across various problem formulations.

Abstract: We present a novel simulation-free framework for training continuous-time
diffusion processes over very general objective functions. Existing methods
typically involve either prescribing the optimal diffusion process -- which
only works for heavily restricted problem formulations -- or require expensive
simulation to numerically obtain the time-dependent densities and sample from
the diffusion process. In contrast, we propose a coupled parameterization which
jointly models a time-dependent density function, or probability path, and the
dynamics of a diffusion process that generates this probability path. To
accomplish this, our approach directly bakes in the Fokker-Planck equation and
density function requirements as hard constraints, by extending and greatly
simplifying the construction of Neural Conservation Laws. This enables
simulation-free training for a large variety of problem formulations, from
data-driven objectives as in generative modeling and dynamical optimal
transport, to optimality-based objectives as in stochastic optimal control,
with straightforward extensions to mean-field objectives due to the ease of
accessing exact density functions. We validate our method in a diverse range of
application domains from modeling spatio-temporal events to learning optimal
dynamics from population data.

</details>


### [640] [Policy gradient methods for ordinal policies](https://arxiv.org/pdf/2506.18614)
*Simn Weinberger, Jairo Cugliari*

Main category: cs.LG

TL;DR: Proposes an ordinal regression-based policy for RL to address the limitations of softmax in capturing action order relationships.


<details>
  <summary>Details</summary>
Motivation: Softmax fails to capture action order, which is critical in real-world industrial problems.

Method: Novel policy parametrization using ordinal regression models adapted for RL.

Result: Effective in real applications and competitive in continuous action tasks.

Conclusion: Ordinal policy outperforms softmax in capturing action relationships and performs well in practical settings.

Abstract: In reinforcement learning, the softmax parametrization is the standard
approach for policies over discrete action spaces. However, it fails to capture
the order relationship between actions. Motivated by a real-world industrial
problem, we propose a novel policy parametrization based on ordinal regression
models adapted to the reinforcement learning setting. Our approach addresses
practical challenges, and numerical experiments demonstrate its effectiveness
in real applications and in continuous action tasks, where discretizing the
action space and applying the ordinal policy yields competitive performance.

</details>


### [641] [Pr{}diction optimale pour un mod{}le ordinal {} covariables fonctionnelles](https://arxiv.org/pdf/2506.18615)
*Simn Weinberger, Jairo Cugliari, Aurlie Le Cain*

Main category: cs.LG

TL;DR: A framework for ordinal model predictions using loss functions, with applications to functional covariates and a real-world dataset for connected glasses.


<details>
  <summary>Details</summary>
Motivation: To improve prediction accuracy in ordinal models by introducing optimal predictions and adapting functional covariates to scalar forms.

Method: Introduces optimal predictions via loss functions, derives Least-Absolute-Deviation predictions, and reformulates ordinal models with functional covariates.

Result: Demonstrates applicability through a dataset for controlling connected glasses' shade.

Conclusion: The framework effectively adapts ordinal models for practical use, validated by real-world application.

Abstract: We present a prediction framework for ordinal models: we introduce optimal
predictions using loss functions and give the explicit form of the
Least-Absolute-Deviation prediction for these models. Then, we reformulate an
ordinal model with functional covariates to a classic ordinal model with
multiple scalar covariates. We illustrate all the proposed methods and try to
apply these to a dataset collected by EssilorLuxottica for the development of a
control algorithm for the shade of connected glasses.

</details>


### [642] [Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits](https://arxiv.org/pdf/2506.18627)
*Yannik Mahlau, Maximilian Schier, Christoph Reinders, Frederik Schubert, Marco Bgling, Bodo Rosenhahn*

Main category: cs.LG

TL;DR: The paper introduces a reinforcement learning (RL) approach for inverse design of photonic integrated circuits (PICs), outperforming traditional gradient-based methods by avoiding local minima and achieving better results with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient-based optimization for PICs often gets stuck in local minima, leading to suboptimal designs. With growing interest in PICs for optical computing, more adaptive and efficient optimization methods are needed.

Method: The authors propose a multi-agent RL framework, discretizing the design space into a grid and treating the design task as an optimization problem with binary variables. They test this on 2D and 3D PIC components.

Result: The RL algorithms outperform gradient-based methods in both 2D and 3D tasks, achieving optimized designs with only a few thousand samples.

Conclusion: This work demonstrates the effectiveness of RL for PIC inverse design and sets a benchmark for future research in sample-efficient RL for photonics.

Abstract: Inverse design of photonic integrated circuits (PICs) has traditionally
relied on gradientbased optimization. However, this approach is prone to end up
in local minima, which results in suboptimal design functionality. As interest
in PICs increases due to their potential for addressing modern hardware demands
through optical computing, more adaptive optimization algorithms are needed. We
present a reinforcement learning (RL) environment as well as multi-agent RL
algorithms for the design of PICs. By discretizing the design space into a
grid, we formulate the design task as an optimization problem with thousands of
binary variables. We consider multiple two- and three-dimensional design tasks
that represent PIC components for an optical computing system. By decomposing
the design space into thousands of individual agents, our algorithms are able
to optimize designs with only a few thousand environment samples. They
outperform previous state-of-the-art gradient-based optimization in both twoand
three-dimensional design tasks. Our work may also serve as a benchmark for
further exploration of sample-efficient RL for inverse design in photonics.

</details>


### [643] [On Equivariant Model Selection through the Lens of Uncertainty](https://arxiv.org/pdf/2506.18629)
*Putri A. van der Linden, Alexander Timans, Dharmesh Tailor, Erik J. Bekkers*

Main category: cs.LG

TL;DR: The paper explores uncertainty-aware methods for selecting equivariant models with varying symmetry biases, comparing frequentist, Bayesian, and calibration-based measures. Bayesian model evidence shows inconsistency, attributed to complexity mismatches.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting among pretrained equivariant models with varying symmetry biases, which can impact predictive performance.

Method: Compares frequentist (Conformal Prediction), Bayesian (marginal likelihood), and calibration-based measures to naive error-based evaluation for model selection.

Result: Uncertainty metrics generally align with predictive performance, but Bayesian model evidence does so inconsistently due to complexity mismatches.

Conclusion: Uncertainty metrics show promise for guiding symmetry-aware model selection, though Bayesian methods may need adjustments for better alignment.

Abstract: Equivariant models leverage prior knowledge on symmetries to improve
predictive performance, but misspecified architectural constraints can harm it
instead. While work has explored learning or relaxing constraints, selecting
among pretrained models with varying symmetry biases remains challenging. We
examine this model selection task from an uncertainty-aware perspective,
comparing frequentist (via Conformal Prediction), Bayesian (via the marginal
likelihood), and calibration-based measures to naive error-based evaluation. We
find that uncertainty metrics generally align with predictive performance, but
Bayesian model evidence does so inconsistently. We attribute this to a mismatch
in Bayesian and geometric notions of model complexity, and discuss possible
remedies. Our findings point towards the potential of uncertainty in guiding
symmetry-aware model selection.

</details>


### [644] [Granular-Ball-Induced Multiple Kernel K-Means](https://arxiv.org/pdf/2506.18637)
*Shuyin Xia, Yifan Wang, Lifeng Shen, Guoyin Wang*

Main category: cs.LG

TL;DR: The paper introduces a granular-ball computing approach to enhance multi-kernel clustering, improving efficiency and robustness by using ball-based data descriptions.


<details>
  <summary>Details</summary>
Motivation: Existing multi-kernel clustering methods struggle with computational efficiency and robustness due to reliance on point-to-point relationships and complex kernel interplay.

Method: The authors propose granular-ball computing to adaptively fit data distributions, introducing granular-ball kernels (GBK) and a granular-ball multi-kernel K-means framework (GB-MKKM).

Result: GB-MKKM demonstrates superior efficiency and clustering performance in empirical evaluations.

Conclusion: Granular-ball computing effectively addresses the limitations of traditional multi-kernel clustering, offering a more efficient and robust solution.

Abstract: Most existing multi-kernel clustering algorithms, such as multi-kernel
K-means, often struggle with computational efficiency and robustness when faced
with complex data distributions. These challenges stem from their dependence on
point-to-point relationships for optimization, which can lead to difficulty in
accurately capturing data sets' inherent structure and diversity. Additionally,
the intricate interplay between multiple kernels in such algorithms can further
exacerbate these issues, effectively impacting their ability to cluster data
points in high-dimensional spaces. In this paper, we leverage granular-ball
computing to improve the multi-kernel clustering framework. The core of
granular-ball computing is to adaptively fit data distribution by balls from
coarse to acceptable levels. Each ball can enclose data points based on a
density consistency measurement. Such ball-based data description thus improves
the computational efficiency and the robustness to unknown noises.
Specifically, based on granular-ball representations, we introduce the
granular-ball kernel (GBK) and its corresponding granular-ball multi-kernel
K-means framework (GB-MKKM) for efficient clustering. Using granular-ball
relationships in multiple kernel spaces, the proposed GB-MKKM framework shows
its superiority in efficiency and clustering performance in the empirical
evaluation of various clustering tasks.

</details>


### [645] [Federated Loss Exploration for Improved Convergence on Non-IID Data](https://arxiv.org/pdf/2506.18640)
*Christian Intern, Markus Olhofer, Yaochu Jin, Barbara Hammer*

Main category: cs.LG

TL;DR: FedLEx is a novel federated learning method designed to handle non-IID data by optimizing learning behavior and using a global guidance matrix for gradient updates, improving performance without extra data sharing.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods struggle with non-IID data, lacking robustness and efficiency. FedLEx aims to address these challenges by adapting to unknown or impractical data heterogeneity assumptions.

Method: FedLEx employs a federated loss exploration technique, where clients calculate gradient deviations to form a global guidance matrix, guiding gradient updates in subsequent rounds.

Result: Experiments show FedLEx outperforms state-of-the-art FL algorithms in non-IID settings, achieving efficient convergence with minimal epochs and data.

Conclusion: FedLEx effectively tackles non-IID challenges in FL, enhancing performance and knowledge transfer, making it promising for diverse FL applications.

Abstract: Federated learning (FL) has emerged as a groundbreaking paradigm in machine
learning (ML), offering privacy-preserving collaborative model training across
diverse datasets. Despite its promise, FL faces significant hurdles in
non-identically and independently distributed (non-IID) data scenarios, where
most existing methods often struggle with data heterogeneity and lack
robustness in performance. This paper introduces Federated Loss Exploration
(FedLEx), an innovative approach specifically designed to tackle these
challenges. FedLEx distinctively addresses the shortcomings of existing FL
methods in non-IID settings by optimizing its learning behavior for scenarios
in which assumptions about data heterogeneity are impractical or unknown. It
employs a federated loss exploration technique, where clients contribute to a
global guidance matrix by calculating gradient deviations for model parameters.
This matrix serves as a strategic compass to guide clients' gradient updates in
subsequent FL rounds, thereby fostering optimal parameter updates for the
global model. FedLEx effectively navigates the complex loss surfaces inherent
in non-IID data, enhancing knowledge transfer in an efficient manner, since
only a small number of epochs and small amount of data are required to build a
strong global guidance matrix that can achieve model convergence without the
need for additional data sharing or data distribution statics in a large client
scenario. Our extensive experiments with state-of-the art FL algorithms
demonstrate significant improvements in performance, particularly under
realistic non-IID conditions, thus highlighting FedLEx's potential to overcome
critical barriers in diverse FL applications.

</details>


### [646] [On Union-Closedness of Language Generation](https://arxiv.org/pdf/2506.18642)
*Steve Hanneke, Amin Karbasi, Anay Mehrotra, Grigoris Velegkas*

Main category: cs.LG

TL;DR: The paper resolves open questions about language generation in the limit, showing finite unions of generatable or non-uniformly generatable classes need not be generatable, and introduces a novel diagonalization argument.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of language generation for uncountable collections and resolve open questions from prior work, highlighting differences from traditional learning tasks.

Method: Uses carefully constructed classes and a novel diagonalization argument to prove results about generatable and non-uniformly generatable classes.

Result: Proves finite unions of generatable or non-uniformly generatable classes need not be generatable, and addresses uncountable classes not satisfying the EUC condition.

Conclusion: The findings reveal unique challenges in language generation, prohibiting boosting-like combinations of generators and advancing understanding of generatable classes.

Abstract: We investigate language generation in the limit - a model by Kleinberg and
Mullainathan [NeurIPS 2024] and extended by Li, Raman, and Tewari [COLT 2025].
While Kleinberg and Mullainathan proved generation is possible for all
countable collections, Li et al. defined a hierarchy of generation notions
(uniform, non-uniform, and generatable) and explored their feasibility for
uncountable collections.
  Our first set of results resolve two open questions of Li et al. by proving
finite unions of generatable or non-uniformly generatable classes need not be
generatable. These follow from a stronger result: there is a non-uniformly
generatable class and a uniformly generatable class whose union is
non-generatable. This adds to the aspects along which language generation in
the limit is different from traditional tasks in statistical learning theory
like classification, which are closed under finite unions. In particular, it
implies that given two generators for different collections, one cannot combine
them to obtain a single "more powerful" generator, prohibiting this notion of
boosting.
  Our construction also addresses a third open question of Li et al. on whether
there are uncountable classes that are non-uniformly generatable and do not
satisfy the eventually unbounded closure (EUC) condition introduced by Li,
Raman, and Tewari. Our approach utilizes carefully constructed classes along
with a novel diagonalization argument that could be of independent interest in
the growing area of language generation.

</details>


### [647] [SaGIF: Improving Individual Fairness in Graph Neural Networks via Similarity Encoding](https://arxiv.org/pdf/2506.18696)
*Yuchang Zhu, Jintang Li, Huizhe Zhang, Liang Chen, Zibin Zheng*

Main category: cs.LG

TL;DR: The paper addresses individual fairness (IF) in GNNs, introducing metrics and a method (SaGIF) to improve fairness by integrating similarity representations.


<details>
  <summary>Details</summary>
Motivation: To understand causes of individual unfairness in GNNs and comprehensively identify similar individuals, bridging gaps in current research.

Method: Introduces two metrics for assessing individual similarity (topology and feature fusion) and proposes SaGIF, which integrates these similarities to improve IF.

Result: SaGIF outperforms state-of-the-art IF methods while maintaining utility, validated on real-world datasets.

Conclusion: The study provides effective metrics and a method (SaGIF) to enhance individual fairness in GNNs, with promising experimental results.

Abstract: Individual fairness (IF) in graph neural networks (GNNs), which emphasizes
the need for similar individuals should receive similar outcomes from GNNs, has
been a critical issue. Despite its importance, research in this area has been
largely unexplored in terms of (1) a clear understanding of what induces
individual unfairness in GNNs and (2) a comprehensive consideration of
identifying similar individuals. To bridge these gaps, we conduct a preliminary
analysis to explore the underlying reason for individual unfairness and observe
correlations between IF and similarity consistency, a concept introduced to
evaluate the discrepancy in identifying similar individuals based on graph
structure versus node features. Inspired by our observations, we introduce two
metrics to assess individual similarity from two distinct perspectives:
topology fusion and feature fusion. Building upon these metrics, we propose
Similarity-aware GNNs for Individual Fairness, named SaGIF. The key insight
behind SaGIF is the integration of individual similarities by independently
learning similarity representations, leading to an improvement of IF in GNNs.
Our experiments on several real-world datasets validate the effectiveness of
our proposed metrics and SaGIF. Specifically, SaGIF consistently outperforms
state-of-the-art IF methods while maintaining utility performance. Code is
available at: https://github.com/ZzoomD/SaGIF.

</details>


### [648] [PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries](https://arxiv.org/pdf/2506.18728)
*Steven Kolawole, Keshav Santhanam, Virginia Smith, Pratiksha Thaker*

Main category: cs.LG

TL;DR: PARALLELPROMPT is a benchmark for measuring intra-query parallelism in LLM prompts, showing 75% success in decomposition and up to 5x speedups with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Real-world prompts often have latent parallelism, but current LLM serving systems treat them monolithically, missing optimization opportunities.

Method: A dataset of 37,000 prompts is annotated with structured schemas using LLM-assisted prompting and validated. An execution suite compares serial vs. parallel strategies.

Result: Over 75% of prompts can be decomposed, achieving up to 5x speedups in tasks like translation and analysis with little quality degradation.

Conclusion: PARALLELPROMPT provides a standardized testbed for structure-aware execution in LLM serving, demonstrating significant latency improvements.

Abstract: LLM serving systems typically treat user prompts as monolithic inputs,
optimizing inference through decoding tricks or inter-query batching. However,
many real-world prompts contain latent semantic parallelism--decomposable
structures where subtasks can be executed independently to reduce latency while
preserving meaning. We introduce PARALLELPROMPT, the first benchmark for
measuring intra-query parallelism in natural user prompts. Our dataset
comprises over 37,000 real-world prompts from public LLM chat logs, each
annotated with a structured schema capturing task templates, shared context,
and iteration inputs. These schemas are extracted using LLM-assisted prompting
with rule-based multilingual validation. To evaluate the benefits of
decomposition, we provide an execution suite that benchmarks serial vs.
parallel strategies, measuring latency, structural adherence, and semantic
fidelity. Our results show that intra-query parallelism can be successfully
parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks
like translation, comprehension, and comparative analysis, with minimal quality
degradation. By releasing this benchmark, curation pipeline, and evaluation
suite, we provide the first standardized testbed for studying structure-aware
execution in LLM serving pipelines.

</details>


### [649] [Towards Group Fairness with Multiple Sensitive Attributes in Federated Foundation Models](https://arxiv.org/pdf/2506.18732)
*Yuning Yang, Han Yu, Tianrun Gao, Xiaodong Xu, Guangyu Wang*

Main category: cs.LG

TL;DR: The paper introduces a causal analysis approach to address group fairness in federated foundation models (FFMs), focusing on multiple sensitive attributes for interpretability and equity.


<details>
  <summary>Details</summary>
Motivation: Biases in sensitive attributes in FFMs can lead to unfair treatment of underrepresented groups, necessitating a method to analyze and mitigate such biases across multiple attributes.

Method: The study extends FFM structure to balance multiple sensitive attributes, using causal discovery and inference to quantify fairness effects.

Result: Experiments confirm the method's effectiveness in achieving group fairness and providing interpretable insights.

Conclusion: The approach enhances trustworthiness and fairness in FFMs by addressing dependencies among multiple sensitive attributes.

Abstract: The deep integration of foundation models (FM) with federated learning (FL)
enhances personalization and scalability for diverse downstream tasks, making
it crucial in sensitive domains like healthcare. Achieving group fairness has
become an increasingly prominent issue in the era of federated foundation
models (FFMs), since biases in sensitive attributes might lead to inequitable
treatment for under-represented demographic groups. Existing studies mostly
focus on achieving fairness with respect to a single sensitive attribute. This
renders them unable to provide clear interpretability of dependencies among
multiple sensitive attributes which is required to achieve group fairness. Our
paper takes the first attempt towards a causal analysis of the relationship
between group fairness across various sensitive attributes in the FFM. We
extend the FFM structure to trade off multiple sensitive attributes
simultaneously and quantify the causal effect behind the group fairness through
causal discovery and inference. Extensive experiments validate its
effectiveness, offering insights into interpretability towards building
trustworthy and fair FFM systems.

</details>


### [650] [On the Existence of Universal Simulators of Attention](https://arxiv.org/pdf/2506.18739)
*Debanjan Dutta, Faizanuddin Ansari, Anish Chakrabarty, Swagatam Das*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prior work on the learnability of transformers has established its capacity
to approximate specific algorithmic patterns through training under restrictive
architectural assumptions. Fundamentally, these arguments remain data-driven
and therefore can only provide a probabilistic guarantee. Expressivity, on the
contrary, has theoretically been explored to address the problems
\emph{computable} by such architecture. These results proved the
Turing-completeness of transformers, investigated bounds focused on circuit
complexity, and formal logic. Being at the crossroad between learnability and
expressivity, the question remains: \emph{can transformer architectures exactly
simulate an arbitrary attention mechanism, or in particular, the underlying
operations?} In this study, we investigate the transformer encoder's ability to
simulate a vanilla attention mechanism. By constructing a universal simulator
$\mathcal{U}$ composed of transformer encoders, we present algorithmic
solutions to identically replicate attention outputs and the underlying
elementary matrix and activation operations via RASP, a formal framework for
transformer computation. Our proofs, for the first time, show the existence of
an algorithmically achievable data-agnostic solution, previously known to be
approximated only by learning.

</details>


### [651] [Experimenting, Fast and Slow: Bayesian Optimization of Long-term Outcomes with Online Experiments](https://arxiv.org/pdf/2506.18744)
*Qing Feng, Samuel Dalton, Benjamin Letham, Maximilian Balandat, Eytan Bakshy*

Main category: cs.LG

TL;DR: A novel approach combines fast experiments and offline proxies with slow experiments for efficient Bayesian optimization in A/B testing.


<details>
  <summary>Details</summary>
Motivation: Optimizing long-term treatment effects in A/B tests is challenging due to non-stationarity and lengthy sequential experiments.

Method: Combines fast experiments (biased, short-term) and offline proxies with slow experiments for sequential Bayesian optimization.

Result: Enables efficient optimization over large action spaces in shorter timeframes.

Conclusion: The approach addresses the limitations of traditional A/B testing by integrating fast and slow experiments for better decision-making.

Abstract: Online experiments in internet systems, also known as A/B tests, are used for
a wide range of system tuning problems, such as optimizing recommender system
ranking policies and learning adaptive streaming controllers. Decision-makers
generally wish to optimize for long-term treatment effects of the system
changes, which often requires running experiments for a long time as short-term
measurements can be misleading due to non-stationarity in treatment effects
over time. The sequential experimentation strategies--which typically involve
several iterations--can be prohibitively long in such cases. We describe a
novel approach that combines fast experiments (e.g., biased experiments run
only for a few hours or days) and/or offline proxies (e.g., off-policy
evaluation) with long-running, slow experiments to perform sequential, Bayesian
optimization over large action spaces in a short amount of time.

</details>


### [652] [ContinualFlow: Learning and Unlearning with Neural Flow Matching](https://arxiv.org/pdf/2506.18747)
*Lorenzo Simone, Davide Bacciu, Shuangge Ma*

Main category: cs.LG

TL;DR: ContinualFlow is a framework for targeted unlearning in generative models using Flow Matching and energy-based reweighting to remove undesired data regions without retraining or direct sample access.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and targeted unlearning in generative models without the need for retraining or direct access to unwanted samples.

Method: Uses Flow Matching with an energy-based reweighting loss to softly subtract undesired data regions, guided by energy-based proxies.

Result: Demonstrated effectiveness through experiments on 2D and image domains, supported by visualizations and quantitative metrics.

Conclusion: ContinualFlow provides a practical and principled approach for targeted unlearning in generative models.

Abstract: We introduce ContinualFlow, a principled framework for targeted unlearning in
generative models via Flow Matching. Our method leverages an energy-based
reweighting loss to softly subtract undesired regions of the data distribution
without retraining from scratch or requiring direct access to the samples to be
unlearned. Instead, it relies on energy-based proxies to guide the unlearning
process. We prove that this induces gradients equivalent to Flow Matching
toward a soft mass-subtracted target, and validate the framework through
experiments on 2D and image domains, supported by interpretable visualizations
and quantitative evaluations.

</details>


### [653] [Sensitivity Analysis of Image Classification Models using Generalized Polynomial Chaos](https://arxiv.org/pdf/2506.18751)
*Lukas Bahr, Lucas Poner, Konstantin Weise, Sophie Grger, Rdiger Daub*

Main category: cs.LG

TL;DR: The paper explores sensitivity analysis for image classification models in predictive quality, addressing uncertainties from domain shifts by using Sobol indices and GPC.


<details>
  <summary>Details</summary>
Motivation: ML models in image classification face uncertainties from model, data, and domain shifts, leading to overconfidence. Sensitivity analysis helps understand these models.

Method: Proposes modeling domain shifts with random variables and quantifying their impact using Sobol indices via generalized polynomial chaos (GPC). Validated with a welding defect and emblem classification case study.

Result: Demonstrates the effectiveness of the proposed approach in analyzing sensitivity and uncertainty in image classification models.

Conclusion: The method provides a robust way to understand and mitigate uncertainties in predictive quality ML models, validated by real-world case studies.

Abstract: Integrating advanced communication protocols in production has accelerated
the adoption of data-driven predictive quality methods, notably machine
learning (ML) models. However, ML models in image classification often face
significant uncertainties arising from model, data, and domain shifts. These
uncertainties lead to overconfidence in the classification model's output. To
better understand these models, sensitivity analysis can help to analyze the
relative influence of input parameters on the output. This work investigates
the sensitivity of image classification models used for predictive quality. We
propose modeling the distributional domain shifts of inputs with random
variables and quantifying their impact on the model's outputs using Sobol
indices computed via generalized polynomial chaos (GPC). This approach is
validated through a case study involving a welding defect classification
problem, utilizing a fine-tuned ResNet18 model and an emblem classification
model used in BMW Group production facilities.

</details>


### [654] [Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning](https://arxiv.org/pdf/2506.18789)
*Rahul Atul Bhope, K. R. Jayaram, Praveen Venkateswaran, Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: ShiftEx is a shift-aware mixture of experts framework for federated learning, addressing covariate and label shifts in dynamic environments, improving accuracy and adaptation speed.


<details>
  <summary>Details</summary>
Motivation: To tackle performance degradation in federated learning due to evolving data distributions (covariate and label shifts) in real-world settings.

Method: Introduces ShiftEx, using Maximum Mean Discrepancy for covariate shift detection, latent memory for expert reuse, and facility location-based optimization.

Result: Achieves 5.5-12.9% accuracy improvements and 22-95% faster adaptation compared to FL baselines.

Conclusion: ShiftEx provides a scalable, privacy-preserving solution for FL in non-stationary environments with minimal overhead.

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients without sharing raw data, yet faces significant
challenges in real-world settings where client data distributions evolve
dynamically over time. This paper tackles the critical problem of covariate and
label shifts in streaming FL environments, where non-stationary data
distributions degrade model performance and require adaptive middleware
solutions. We introduce ShiftEx, a shift-aware mixture of experts framework
that dynamically creates and trains specialized global models in response to
detected distribution shifts using Maximum Mean Discrepancy for covariate
shifts. The framework employs a latent memory mechanism for expert reuse and
implements facility location-based optimization to jointly minimize covariate
mismatch, expert creation costs, and label imbalance. Through theoretical
analysis and comprehensive experiments on benchmark datasets, we demonstrate
5.5-12.9 percentage point accuracy improvements and 22-95 % faster adaptation
compared to state-of-the-art FL baselines across diverse shift scenarios. The
proposed approach offers a scalable, privacy-preserving middleware solution for
FL systems operating in non-stationary, real-world conditions while minimizing
communication and computational overhead.

</details>


### [655] [A Multi-view Divergence-Convergence Feature Augmentation Framework for Drug-related Microbes Prediction](https://arxiv.org/pdf/2506.18797)
*Xin An, Ruijie Li, Qiao Ning, Shikai Guo, Hui Li, Qian Ma*

Main category: cs.LG

TL;DR: DCFA_DMP is a multi-view framework for predicting drug-microbe associations, combining adversarial learning and attention mechanisms for enhanced feature fusion and prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods lack effective multi-view optimization and feature fusion for drug-microbe association prediction, limiting accuracy and applicability.

Method: DCFA_DMP uses adversarial learning (divergence phase) and a bidirectional attention mechanism (convergence phase) to integrate association and similarity information, with Transformer graph learning for node relevance.

Result: DCFA_DMP outperforms existing methods in predicting drug-microbe associations, including cold start scenarios for new drugs/microbes.

Conclusion: DCFA_DMP is a robust and reliable framework for drug-microbe association prediction, with potential applications in precision medicine.

Abstract: In the study of drug function and precision medicine, identifying new
drug-microbe associations is crucial. However, current methods isolate
association and similarity analysis of drug and microbe, lacking effective
inter-view optimization and coordinated multi-view feature fusion. In our
study, a multi-view Divergence-Convergence Feature Augmentation framework for
Drug-related Microbes Prediction (DCFA_DMP) is proposed, to better learn and
integrate association information and similarity information. In the divergence
phase, DCFA_DMP strengthens the complementarity and diversity between
heterogeneous information and similarity information by performing Adversarial
Learning method between the association network view and different similarity
views, optimizing the feature space. In the convergence phase, a novel
Bidirectional Synergistic Attention Mechanism is proposed to deeply synergize
the complementary features between different views, achieving a deep fusion of
the feature space. Moreover, Transformer graph learning is alternately applied
on the drug-microbe heterogeneous graph, enabling each drug or microbe node to
focus on the most relevant nodes. Numerous experiments demonstrate DCFA_DMP's
significant performance in predicting drug-microbe associations. It also proves
effectiveness in predicting associations for new drugs and microbes in cold
start experiments, further confirming its stability and reliability in
predicting potential drug-microbe associations.

</details>


### [656] [Multi-Agent Online Control with Adversarial Disturbances](https://arxiv.org/pdf/2506.18814)
*Anas Barakat, John Lazarsfeld, Georgios Piliouras, Antonios Varvitsiotis*

Main category: cs.LG

TL;DR: The paper studies online control in multi-agent linear dynamical systems with adversarial disturbances, focusing on gradient-based controllers and their robustness in multi-agent settings.


<details>
  <summary>Details</summary>
Motivation: Multi-agent control problems with competing and time-varying objectives are common in robotics, economics, and energy systems, necessitating robust solutions for adversarial settings.

Method: The study investigates gradient-based controllers in an online setting with adversarial disturbances and convex losses, analyzing regret bounds and communication assumptions.

Result: Near-optimal sublinear regret bounds are proven for all agents, and equilibrium gap guarantees are derived for aligned objectives.

Conclusion: The work provides robust regret guarantees for multi-agent control and extends insights to potential games with aligned objectives.

Abstract: Multi-agent control problems involving a large number of agents with
competing and time-varying objectives are increasingly prevalent in
applications across robotics, economics, and energy systems. In this paper, we
study online control in multi-agent linear dynamical systems with disturbances.
In contrast to most prior work in multi-agent control, we consider an online
setting where disturbances are adversarial and where each agent seeks to
minimize its own, adversarial sequence of convex losses. In this setting, we
investigate the robustness of gradient-based controllers from single-agent
online control, with a particular focus on understanding how individual regret
guarantees are influenced by the number of agents in the system. Under minimal
communication assumptions, we prove near-optimal sublinear regret bounds that
hold uniformly for all agents. Finally, when the objectives of the agents are
aligned, we show that the multi-agent control problem induces a time-varying
potential game for which we derive equilibrium gap guarantees.

</details>


### [657] [Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning](https://arxiv.org/pdf/2506.18847)
*Anthony Kobanda, Waris Radji, Mathieu Petitbois, Odalric-Ambrym Maillard, Rmy Portelas*

Main category: cs.LG

TL;DR: ProQ introduces a compositional framework for offline goal-conditioned RL, using asymmetric distance learning and keypoint coverage to address long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing compounding value-estimation errors in scaling offline goal-conditioned RL to long-horizon tasks.

Method: ProQ learns an asymmetric distance, repurposes it for keypoint coverage and directional cost, and uses a Lagrangian OOD detector for reachability.

Result: ProQ produces meaningful sub-goals and achieves robust long-horizon goal-reaching on navigation benchmarks.

Conclusion: ProQ effectively unifies metric learning, keypoint coverage, and goal-conditioned control for scalable offline RL.

Abstract: Offline Goal-Conditioned Reinforcement Learning seeks to train agents to
reach specified goals from previously collected trajectories. Scaling that
promises to long-horizon tasks remains challenging, notably due to compounding
value-estimation errors. Principled geometric offers a potential solution to
address these issues. Following this insight, we introduce Projective
Quasimetric Planning (ProQ), a compositional framework that learns an
asymmetric distance and then repurposes it, firstly as a repulsive energy
forcing a sparse set of keypoints to uniformly spread over the learned latent
space, and secondly as a structured directional cost guiding towards proximal
sub-goals. In particular, ProQ couples this geometry with a Lagrangian
out-of-distribution detector to ensure the learned keypoints stay within
reachable areas. By unifying metric learning, keypoint coverage, and
goal-conditioned control, our approach produces meaningful sub-goals and
robustly drives long-horizon goal-reaching on diverse a navigation benchmarks.

</details>


### [658] [$L^*LM$: Learning Automata from Examples using Natural Language Oracles](https://arxiv.org/pdf/2402.07051)
*Marcell Vazquez-Chanlatte, Karim Elmaaroufi, Stefan J. Witwicki, Matei Zaharia, Sanjit A. Seshia*

Main category: cs.LG

TL;DR: $L^*LM$ improves DFA learning efficiency by combining expert demonstrations and natural language, leveraging LLMs for membership queries.


<details>
  <summary>Details</summary>
Motivation: Existing techniques for learning DFAs from demonstrations are not sample efficient.

Method: $L^*LM$ integrates expert demonstrations and natural language, using LLMs for membership queries and transforming demonstrations into labeled learning problems.

Result: The method significantly improves data efficiency, with modalities complementing each other for few-shot learning.

Conclusion: $L^*LM$ is a powerful, efficient approach for learning DFAs from demonstrations and natural language.

Abstract: Expert demonstrations have proven an easy way to indirectly specify complex
tasks. Recent algorithms even support extracting unambiguous formal
specifications, e.g. deterministic finite automata (DFA), from demonstrations.
Unfortunately, these techniques are generally not sample efficient. In this
work, we introduce $L^*LM$, an algorithm for learning DFAs from both
demonstrations and natural language. Due to the expressivity of natural
language, we observe a significant improvement in the data efficiency of
learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large
language models to answer membership queries about the underlying task. This is
then combined with recent techniques for transforming learning from
demonstrations into a sequence of labeled example learning problems. In our
experiments, we observe the two modalities complement each other, yielding a
powerful few-shot learner.

</details>


### [659] [RePST: Language Model Empowered Spatio-Temporal Forecasting via Semantic-Oriented Reprogramming](https://arxiv.org/pdf/2408.14505)
*Hao Wang, Jindong Han, Wei Fan, Leilei Sun, Hao Liu*

Main category: cs.LG

TL;DR: RePST is a framework that adapts Pre-trained Language Models (PLMs) for spatio-temporal forecasting by using semantic-oriented decomposer and selective discrete reprogramming, outperforming baselines in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: PLMs struggle with numerical time series correlations, limiting their effectiveness in spatio-temporal forecasting, especially in data-scarce situations.

Method: Proposes RePST with a semantic-oriented decomposer and selective discrete reprogramming to adapt PLMs for spatio-temporal data.

Result: RePST outperforms twelve state-of-the-art baselines, especially in data-scarce scenarios.

Conclusion: RePST effectively harnesses PLMs for spatio-temporal forecasting, demonstrating superior generalization and performance.

Abstract: Spatio-temporal forecasting is pivotal in numerous real-world applications,
including transportation planning, energy management, and climate monitoring.
In this work, we aim to harness the reasoning and generalization abilities of
Pre-trained Language Models (PLMs) for more effective spatio-temporal
forecasting, particularly in data-scarce scenarios. However, recent studies
uncover that PLMs, which are primarily trained on textual data, often falter
when tasked with modeling the intricate correlations in numerical time series,
thereby limiting their effectiveness in comprehending spatio-temporal data. To
bridge the gap, we propose RePST, a semantic-oriented PLM reprogramming
framework tailored for spatio-temporal forecasting. Specifically, we first
propose a semantic-oriented decomposer that adaptively disentangles spatially
correlated time series into interpretable sub-components, which facilitates PLM
to understand sophisticated spatio-temporal dynamics via a divide-and-conquer
strategy. Moreover, we propose a selective discrete reprogramming scheme, which
introduces an expanded spatio-temporal vocabulary space to project
spatio-temporal series into discrete representations. This scheme minimizes the
information loss during reprogramming and enriches the representations derived
by PLMs. Extensive experiments on real-world datasets show that the proposed
RePST outperforms twelve state-of-the-art baseline methods, particularly in
data-scarce scenarios, highlighting the effectiveness and superior
generalization capabilities of PLMs for spatio-temporal forecasting. Our codes
can be found at https://github.com/usail-hkust/REPST.

</details>


### [660] [Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models](https://arxiv.org/pdf/2410.01434)
*Philipp Mondorf, Sondre Wold, Barbara Plank*

Main category: cs.LG

TL;DR: The paper investigates modularity in neural networks by analyzing circuits for compositional subtasks in a transformer-based language model, showing functional similarity and reusability of circuits.


<details>
  <summary>Details</summary>
Motivation: To understand how functionally similar circuits in neural networks relate to each other and whether they can be composed for complex tasks.

Method: Analyze circuits for ten modular string-edit operations using a probabilistic context-free grammar in a transformer-based language model.

Result: Functionally similar circuits show node overlap and cross-task faithfulness, and can be reused via set operations for complex tasks.

Conclusion: Neural networks exhibit modularity, with circuits being reusable and composable, advancing interpretability research.

Abstract: A fundamental question in interpretability research is to what extent neural
networks, particularly language models, implement reusable functions through
subnetworks that can be composed to perform more complex tasks. Recent advances
in mechanistic interpretability have made progress in identifying
$\textit{circuits}$, which represent the minimal computational subgraphs
responsible for a model's behavior on specific tasks. However, most studies
focus on identifying circuits for individual tasks without investigating how
functionally similar circuits $\textit{relate}$ to each other. To address this
gap, we study the modularity of neural networks by analyzing circuits for
highly compositional subtasks within a transformer-based language model.
Specifically, given a probabilistic context-free grammar, we identify and
compare circuits responsible for ten modular string-edit operations. Our
results indicate that functionally similar circuits exhibit both notable node
overlap and cross-task faithfulness. Moreover, we demonstrate that the circuits
identified can be reused and combined through set operations to represent more
complex functional model capabilities.

</details>


### [661] [FutureFill: Fast Generation from Convolutional Sequence Models](https://arxiv.org/pdf/2410.03766)
*Naman Agarwal, Xinyi Chen, Evan Dogariu, Devan Shah, Hubert Strauss, Vlad Feinberg, Daniel Suo, Peter Bartlett, Elad Hazan*

Main category: cs.LG

TL;DR: FutureFill is a fast generation method for sequence prediction models using convolutional operators, reducing generation time from quadratic to quasilinear and requiring smaller prefill caches.


<details>
  <summary>Details</summary>
Motivation: The challenge of efficient auto-regressive generation in sequence prediction models motivates the development of FutureFill.

Method: FutureFill is introduced as a general-purpose fast generation method for convolutional-based sequence prediction algorithms.

Result: Experiments show FutureFill reduces generation time and requires smaller caches, validating its efficiency.

Conclusion: FutureFill offers substantial efficiency gains for deep convolutional sequence prediction models.

Abstract: We address the challenge of efficient auto-regressive generation in sequence
prediction models by introducing FutureFill, a general-purpose fast generation
method for any sequence prediction algorithm based on convolutional operators.
FutureFill reduces generation time from quadratic to quasilinear in the context
length. Moreover, when generating from a prompt, it requires a prefill cache
whose size grows only with the number of tokens to be generated, often much
smaller than the caches required by standard convolutional or attention based
models. We validate our theoretical claims with experiments on synthetic tasks
and demonstrate substantial efficiency gains when generating from a deep
convolutional sequence prediction model.

</details>


### [662] [How Numerical Precision Affects Arithmetical Reasoning Capabilities of LLMs](https://arxiv.org/pdf/2410.13857)
*Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, Liwei Wang*

Main category: cs.LG

TL;DR: The paper analyzes the mathematical abilities of Transformer-based LLMs, focusing on arithmetic performance. It identifies numerical precision as a critical factor, showing low-precision models require super-polynomial size growth for arithmetic tasks, while standard-precision models handle them efficiently.


<details>
  <summary>Details</summary>
Motivation: Understanding and enhancing the mathematical capabilities of LLMs, particularly in arithmetic tasks, is a significant challenge despite their success in other domains.

Method: The study combines theoretical analysis and empirical experiments to evaluate the impact of numerical precision on LLMs' arithmetic performance, focusing on tasks like iterated addition and integer multiplication.

Result: Low numerical precision in Transformers leads to failure in arithmetic tasks unless model size grows super-polynomially, whereas standard precision allows efficient performance with smaller models.

Conclusion: Numerical precision is crucial for LLMs' arithmetic capabilities, and standard precision enables efficient task handling, offering insights for improving mathematical reasoning in LLMs.

Abstract: Despite the remarkable success of Transformer-based large language models
(LLMs) across various domains, understanding and enhancing their mathematical
capabilities remains a significant challenge. In this paper, we conduct a
rigorous theoretical analysis of LLMs' mathematical abilities, with a specific
focus on their arithmetic performances. We identify numerical precision as a
key factor that influences their effectiveness in arithmetical tasks. Our
results show that Transformers operating with low numerical precision fail to
address arithmetic tasks, such as iterated addition and integer multiplication,
unless the model size grows super-polynomially with respect to the input
length. In contrast, Transformers with standard numerical precision can
efficiently handle these tasks with significantly smaller model sizes. We
further support our theoretical findings through empirical experiments that
explore the impact of varying numerical precision on arithmetic tasks,
providing valuable insights for improving the mathematical reasoning
capabilities of LLMs.

</details>


### [663] [LoRA vs Full Fine-tuning: An Illusion of Equivalence](https://arxiv.org/pdf/2410.21228)
*Reece Shuttleworth, Jacob Andreas, Antonio Torralba, Pratyusha Sharma*

Main category: cs.LG

TL;DR: LoRA and full fine-tuning differ in spectral properties, with LoRA introducing 'intruder dimensions' that cause forgetting. Scaling these dimensions improves pre-training distribution modeling with minimal task performance loss.


<details>
  <summary>Details</summary>
Motivation: To understand the differences between LoRA and full fine-tuning in terms of their impact on pre-trained models' weight matrices and forgetting behavior.

Method: Analyzing weight matrices via spectral properties, identifying intruder dimensions in LoRA, and causally intervening on these dimensions.

Result: LoRA introduces high-ranking intruder dimensions causing forgetting, which can be mitigated by scaling them down. LoRA accumulates intruder dimensions in continual learning, worsening performance.

Conclusion: LoRA's intruder dimensions are a key factor in forgetting, and managing them can improve model performance, especially in continual learning settings.

Abstract: Fine-tuning is a crucial paradigm for adapting pre-trained large language
models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA)
have been shown to effectively fine-tune LLMs with an extreme reduction in
trainable parameters. But, \emph{are their learned solutions really
equivalent?} We study how LoRA and full-finetuning change pre-trained models by
analyzing the model's weight matrices through the lens of their spectral
properties. We find that LoRA and full fine-tuning yield weight matrices whose
singular value decompositions exhibit very different structure: weight matrices
trained with LoRA have new, high-ranking singular vectors, which we call
\emph{intruder dimensions}, while those trained with full fine-tuning do not.
Further, we extend the finding that LoRA forgets less than full fine-tuning and
find its forgetting is vastly localized to the intruder dimension -- by
causally intervening on the intruder dimensions by changing their associated
singular values post-fine-tuning, we show that they cause forgetting. Moreover,
scaling them down significantly improves modeling of the pre-training
distribution with a minimal drop in downstream task performance. Given this, we
should expect accumulating intruder dimensions to be harmful and lead to more
forgetting. This will be amplified during continual learning because of
sequentially fine-tuning, and we show that LoRA models do accumulate intruder
dimensions here tend to perform worse in this setting, emphasizing the
practicality of our findings.

</details>


### [664] [Indeterminate Probability Theory](https://arxiv.org/pdf/2303.11536)
*Tao Yang, Chuang Liu, Xiaofeng Ma, Weijia Lu, Ning Wu, Bingyang Li, Zhifei Yang, Peng Liu, Lin Sun, Xiaodong Zhang, Can Zhang*

Main category: cs.LG

TL;DR: The paper introduces Indeterminate Probability Theory (IPT), offering closed-form solutions for complex joint distributions, validated in high-dimensional settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of closed-form solutions for complex joint distributions, which often require approximations like MCMC.

Method: Proposes IPT with an observer-centered framework, three independence axioms, and a two-phase inference framework.

Result: Derives closed-form solutions for complex distributions, validated in high-dimensional (up to 1000D) applications.

Conclusion: IPT is effective, consistent with classical probability theory, and generalizes frequentist approaches.

Abstract: Complex continuous or mixed joint distributions (e.g., P(Y | z_1, z_2, ...,
z_N)) generally lack closed-form solutions, often necessitating approximations
such as MCMC. This paper proposes Indeterminate Probability Theory (IPT), which
makes the following contributions: (1) An observer-centered framework in which
experimental outcomes are represented as distributions combining ground truth
with observation error; (2) The introduction of three independence candidate
axioms that enable a two-phase probabilistic inference framework; (3) The
derivation of closed-form solutions for arbitrary complex joint distributions
under this framework. Both the Indeterminate Probability Neural Network (IPNN)
model and the non-neural multivariate time series forecasting application
demonstrate IPT's effectiveness in modeling high-dimensional distributions,
with successful validation up to 1000 dimensions. Importantly, IPT is
consistent with classical probability theory and subsumes the frequentist
equation in the limit of vanishing observation error.

</details>


### [665] [Steering LLMs for Formal Theorem Proving](https://arxiv.org/pdf/2502.15507)
*Shashank Kirtania, Arun Iyer*

Main category: cs.LG

TL;DR: Activation steering improves LLMs' theorem proving by guiding tactic ranking, offering a lightweight alternative to fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with ranking correct tactics in proof assistants, hindering their theorem-proving capabilities.

Method: Use activation steering to guide LLM responses during inference for better tactic selection.

Result: Activation steering enhances LLMs' theorem-proving performance without extensive fine-tuning.

Conclusion: Activation steering is a promising, resource-efficient method to improve LLMs in formal theorem proving.

Abstract: Large Language Models (LLMs) have shown promise in proving formal theorems
using proof assistants like Lean. However, current state of the art language
models struggles to predict next step in proofs leading practitioners to use
different sampling techniques to improve LLMs capabilities. We observe that the
LLM is capable of predicting the correct tactic; however, it faces challenges
in ranking it appropriately within the set of candidate tactics, affecting the
overall selection process. To overcome this hurdle, we use activation steering
to guide LLMs responses to improve the generations at the time of inference.
Our results suggest that activation steering offers a promising lightweight
alternative to specialized fine-tuning for enhancing theorem proving
capabilities in LLMs, particularly valuable in resource-constrained
environments.

</details>


### [666] [Recent Trends in Artificial Intelligence Technology: A Scoping Review](https://arxiv.org/pdf/2305.04532)
*Teemu Niskanen, Tuomo Sipola, Olli Vnnen*

Main category: cs.LG

TL;DR: A scoping review of 2022 AI technologies, focusing on advanced methods in various domains, emphasizing data processing, unsupervised/semi-supervised learning, and the need for interpretability and safety.


<details>
  <summary>Details</summary>
Motivation: To identify the most advanced AI technologies across domains, ensuring they meet criteria like tested performance, approved datasets, and improvements over existing solutions.

Method: Conducted a scoping review using the PRISMA framework, analyzing articles from three top AI/ML journals in 2022, with strict criteria for technology evaluation.

Result: Highlighted challenges in creating labeled datasets, a shift toward unsupervised/semi-supervised learning, and the importance of interpretable and updatable algorithms.

Conclusion: AI adoption in real-world applications requires focus on safety, explainability, and efficient data utilization, with unsupervised methods gaining traction.

Abstract: Artificial intelligence is more ubiquitous in multiple domains. Smartphones,
social media platforms, search engines, and autonomous vehicles are just a few
examples of applications that utilize artificial intelligence technologies to
enhance their performance. This study carries out a scoping review of the
current state-of-the-art artificial intelligence technologies following the
Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)
framework. The goal was to find the most advanced technologies used in
different domains of artificial intelligence technology research. Three
recognized journals were used from artificial intelligence and machine learning
domain: Journal of Artificial Intelligence Research, Journal of Machine
Learning Research, and Machine Learning, and articles published in 2022 were
observed. Certain qualifications were laid for the technological solutions: the
technology must be tested against comparable solutions, commonly approved or
otherwise well justified datasets must be used while applying, and results must
show improvements against comparable solutions. One of the most important parts
of the technology development appeared to be how to process and exploit the
data gathered from multiple sources. The data can be highly unstructured, and
the technological solution should be able to utilize the data with minimum
manual work from humans. The results of this review indicate that creating
labeled datasets is very laborious, and solutions exploiting unsupervised or
semi-supervised learning technologies are more and more researched. The
learning algorithms should be able to be updated efficiently, and predictions
should be interpretable. Using artificial intelligence technologies in
real-world applications, safety and explainable predictions are mandatory to
consider before mass adoption can occur.

</details>


### [667] [Directional Gradient Projection for Robust Fine-Tuning of Foundation Models](https://arxiv.org/pdf/2502.15895)
*Chengyue Huang, Junjiao Tian, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira*

Main category: cs.LG

TL;DR: DiGraP is a novel layer-wise method for robust fine-tuning that uses directional gradient information to improve generalization and robustness, outperforming existing methods in image classification and multi-modal tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for robust fine-tuning often require extensive tuning and may underfit. DiGraP aims to address these limitations by incorporating directional gradient information.

Method: DiGraP (Directional Gradient Projection) integrates gradient direction into regularization and multi-objective optimization, applied to image classification and multi-modal tasks like VQA.

Result: DiGraP consistently outperforms baselines in both in-distribution generalization and out-of-distribution robustness across tasks.

Conclusion: DiGraP is an effective method for robust fine-tuning, bridging uni-modal and multi-modal gaps and improving performance in diverse settings.

Abstract: Robust fine-tuning aims to adapt large foundation models to downstream tasks
while preserving their robustness to distribution shifts. Existing methods
primarily focus on constraining and projecting current model towards the
pre-trained initialization based on the magnitudes between fine-tuned and
pre-trained weights, which often require extensive hyper-parameter tuning and
can sometimes result in underfitting. In this work, we propose Directional
Gradient Projection (DiGraP), a novel layer-wise trainable method that
incorporates directional information from gradients to bridge regularization
and multi-objective optimization. Besides demonstrating our method on image
classification, as another contribution we generalize this area to the
multi-modal evaluation settings for robust fine-tuning. Specifically, we first
bridge the uni-modal and multi-modal gap by performing analysis on Image
Classification reformulated Visual Question Answering (VQA) benchmarks and
further categorize ten out-of-distribution (OOD) VQA datasets by distribution
shift types and degree (i.e. near versus far OOD). Experimental results show
that DiGraP consistently outperforms existing baselines across Image
Classfication and VQA tasks with discriminative and generative backbones,
improving both in-distribution (ID) generalization and OOD robustness.

</details>


### [668] [Supercharging Graph Transformers with Advective Diffusion](https://arxiv.org/pdf/2310.06417)
*Qitian Wu, Chenxiao Yang, Kaipeng Zeng, Michael Bronstein*

Main category: cs.LG

TL;DR: AdvDIFFormer, a physics-inspired graph Transformer, addresses generalization under topological shifts in non-Euclidean data, outperforming traditional graph neural networks.


<details>
  <summary>Details</summary>
Motivation: Prior studies neglect generalization under topological shifts in non-Euclidean data like graphs, which is crucial for learning systems.

Method: The model is derived from advective diffusion equations, enabling continuous message passing with observed and latent topological structures.

Result: AdvDIFFormer provably controls generalization error under topological shifts and excels in tasks like information networks, molecular screening, and protein interactions.

Conclusion: AdvDIFFormer effectively addresses topological generalization challenges, surpassing traditional graph diffusion models.

Abstract: The capability of generalization is a cornerstone for the success of modern
learning systems. For non-Euclidean data, e.g., graphs, that particularly
involves topological structures, one important aspect neglected by prior
studies is how machine learning models generalize under topological shifts.
This paper proposes Advective Diffusion Transformer (AdvDIFFormer), a
physics-inspired graph Transformer model designed to address this challenge.
The model is derived from advective diffusion equations which describe a class
of continuous message passing process with observed and latent topological
structures. We show that AdvDIFFormer has provable capability for controlling
generalization error with topological shifts, which in contrast cannot be
guaranteed by graph diffusion models, i.e., the generalized formulation of
common graph neural networks in continuous space. Empirically, the model
demonstrates superiority in various predictive tasks across information
networks, molecular screening and protein interactions.

</details>


### [669] [DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training](https://arxiv.org/pdf/2504.09710)
*Zhenting Wang, Guofeng Cui, Yu-Jhe Li, Kun Wan, Wentian Zhao*

Main category: cs.LG

TL;DR: A curriculum learning framework for RL-based LLM post-training dynamically schedules training across diverse data distributions using policy advantages and UCB, improving convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based post-training methods treat training data as uniform, ignoring its diverse distributions, which hinders learning efficiency.

Method: Proposes a distribution-level curriculum learning framework using policy advantages and UCB to dynamically adjust sampling probabilities for different distributions.

Result: The framework significantly improves convergence speed and final performance on logic reasoning datasets with varied difficulties and sources.

Conclusion: Distribution-aware curriculum strategies enhance RL-based LLM post-training, demonstrating the value of adaptive scheduling.

Abstract: Recent advances in reinforcement learning (RL)-based post-training have led
to notable improvements in large language models (LLMs), particularly in
enhancing their reasoning capabilities to handle complex tasks. However, most
existing methods treat the training data as a unified whole, overlooking the
fact that modern LLM training often involves a mixture of data from diverse
distributions-varying in both source and difficulty. This heterogeneity
introduces a key challenge: how to adaptively schedule training across
distributions to optimize learning efficiency. In this paper, we present a
principled curriculum learning framework grounded in the notion of
distribution-level learnability. Our core insight is that the magnitude of
policy advantages reflects how much a model can still benefit from further
training on a given distribution. Based on this, we propose a
distribution-level curriculum learning framework for RL-based LLM
post-training, which leverages the Upper Confidence Bound (UCB) principle to
dynamically adjust sampling probabilities for different distrubutions. This
approach prioritizes distributions with either high average advantage
(exploitation) or low sample count (exploration), yielding an adaptive and
theoretically grounded training schedule. We instantiate our curriculum
learning framework with GRPO as the underlying RL algorithm and demonstrate its
effectiveness on logic reasoning datasets with multiple difficulties and
sources. Our experiments show that our framework significantly improves
convergence speed and final performance, highlighting the value of
distribution-aware curriculum strategies in LLM post-training. Code:
https://github.com/ZhentingWang/DUMP.

</details>


### [670] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/pdf/2504.14945)
*Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, Yue Zhang*

Main category: cs.LG

TL;DR: LUFFY introduces off-policy guidance to RLVR, improving reasoning abilities and outperforming on-policy methods by +6.4 on math benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods are limited to on-policy learning, restricting reasoning capabilities. LUFFY aims to overcome this by incorporating off-policy traces.

Method: LUFFY combines Mixed-Policy GRPO and policy shaping via regularized importance sampling to balance imitation and exploration during training.

Result: Achieves +6.4 average gain on math benchmarks and +6.2 in out-of-distribution tasks, even training weak models where on-policy RLVR fails.

Conclusion: LUFFY surpasses on-policy RLVR limitations, showcasing the potential of off-policy guidance in reasoning tasks.

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}).
However, existing \textit{RLVR} approaches are inherently ``on-policy'',
limiting learning to a model's own outputs and failing to acquire reasoning
abilities beyond its initial capabilities. To address this issue, we introduce
\textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder
o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR}
with off-policy reasoning traces. LUFFY dynamically balances imitation and
exploration by combining off-policy demonstrations with on-policy rollouts
during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework,
which has a theoretically guaranteed convergence rate, alongside policy shaping
via regularized importance sampling to avoid superficial and rigid imitation
during mixed-policy training. Compared with previous RLVR methods, LUFFY
achieves an over \textbf{+6.4} average gain across six math benchmarks and an
advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most
significantly, we show that LUFFY successfully trains weak models in scenarios
where on-policy RLVR completely fails. These results provide compelling
evidence that LUFFY transcends the fundamental limitations of on-policy RLVR
and demonstrates the great potential of utilizing off-policy guidance in RLVR.

</details>


### [671] [Do Concept Bottleneck Models Respect Localities?](https://arxiv.org/pdf/2401.01259)
*Naveen Raman, Mateo Espinosa Zarlenga, Juyeon Heo, Mateja Jamnik*

Main category: cs.LG

TL;DR: The paper evaluates whether concept-based explainability methods truly reflect a model's reasoning by analyzing if concept predictors use relevant features (locality). It finds many models fail this, leading to spurious explanations, and proposes solutions.


<details>
  <summary>Details</summary>
Motivation: To assess the validity of concept-based explainability methods by determining if concept predictors rely on relevant features (locality) for predictions, ensuring explanations are meaningful.

Method: Constructs three metrics to test locality under different perturbations, analyzing if irrelevant feature changes affect concept predictions. Theoretical analysis complements this.

Result: Many concept-based models fail to respect locality, as concept predictors struggle to clearly distinguish distinct concepts, leading to unreliable explanations.

Conclusion: The study highlights flaws in current concept-based models and suggests improvements to ensure meaningful explanations by respecting locality.

Abstract: Concept-based explainability methods use human-understandable intermediaries
to produce explanations for machine learning models. These methods assume
concept predictions can help understand a model's internal reasoning. In this
work, we assess the degree to which such an assumption is true by analyzing
whether concept predictors leverage ``relevant'' features to make predictions,
a term we call locality. Concept-based models that fail to respect localities
also fail to be explainable because concept predictions are based on spurious
features, making the interpretation of the concept predictions vacuous. To
assess whether concept-based models respect localities, we construct and use
three metrics to characterize when models respect localities, complementing our
analysis with theoretical results. Each of our metrics captures a different
notion of perturbation and assess whether perturbing ``irrelevant'' features
impacts the predictions made by a concept predictors. We find that many
concept-based models used in practice fail to respect localities because
concept predictors cannot always clearly distinguish distinct concepts. Based
on these findings, we propose suggestions for alleviating this issue.

</details>


### [672] [ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training](https://arxiv.org/pdf/2505.17331)
*Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu*

Main category: cs.LG

TL;DR: ECHO-LLaMA enhances LLaMA by sharing KV caching across layers, boosting training speed and inference throughput without losing performance.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of LLaMA architectures in terms of training speed and inference throughput while retaining learning capacity.

Method: Introduces shared KV caching across certain layers to reduce computational complexity.

Result: Achieves up to 77% higher training throughput, 16% higher MFU, and 14% lower loss. Test-time throughput improves by 7% for the 1.1B model.

Conclusion: ECHO-LLaMA provides a scalable, cost-effective solution for efficient large language model training and finetuning.

Abstract: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to
improve both the training speed and inference throughput of LLaMA architectures
while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models
into shared KV caching across certain layers, significantly reducing KV
computational complexity while maintaining or improving language performance.
Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher
token-per-second throughput during training, up to 16\% higher Model FLOPs
Utilization (MFU), and up to 14\% lower loss when trained on an equal number of
tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\%
higher test-time throughput compared to the baseline. By introducing a
computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable
and cost-effective solution for pretraining and finetuning large language
models, enabling faster and more resource-efficient training without
compromising performance.

</details>


### [673] [Hierarchical Decision Making Based on Structural Information Principles](https://arxiv.org/pdf/2404.09760)
*Xianghua Zeng, Hao Peng, Dingli Su, Angsheng Li*

Main category: cs.LG

TL;DR: The paper introduces SIDM, a Structural Information principles-based framework for hierarchical decision-making in RL, improving policy learning effectiveness, efficiency, and stability.


<details>
  <summary>Details</summary>
Motivation: Hierarchical Reinforcement Learning (HRL) relies on prior knowledge and manual assumptions, limiting adaptability. SIDM aims to dynamically discover and learn hierarchical policies using structural information.

Method: SIDM uses structural information from state-action trajectories to construct abstract representations. It optimizes directed structural entropy to discover skills and employs skill-based (single-agent) and role-based (multi-agent) learning methods.

Result: SIDM outperforms state-of-the-art baselines, improving average rewards by 32.70%, convergence timesteps by 64.86%, and stability by 88.26%.

Conclusion: SIDM provides a flexible, adaptive framework for hierarchical decision-making, significantly enhancing RL performance in single and multi-agent scenarios.

Abstract: Hierarchical Reinforcement Learning (HRL) is a promising approach for
managing task complexity across multiple levels of abstraction and accelerating
long-horizon agent exploration. However, the effectiveness of hierarchical
policies heavily depends on prior knowledge and manual assumptions about skill
definitions and task decomposition. In this paper, we propose a novel
Structural Information principles-based framework, namely SIDM, for
hierarchical Decision Making in both single-agent and multi-agent scenarios.
Central to our work is the utilization of structural information embedded in
the decision-making process to adaptively and dynamically discover and learn
hierarchical policies through environmental abstractions. Specifically, we
present an abstraction mechanism that processes historical state-action
trajectories to construct abstract representations of states and actions. We
define and optimize directed structural entropy, a metric quantifying the
uncertainty in transition dynamics between abstract states, to discover skills
that capture key transition patterns in RL environments. Building on these
findings, we develop a skill-based learning method for single-agent scenarios
and a role-based collaboration method for multi-agent scenarios, both of which
can flexibly integrate various underlying algorithms for enhanced performance.
Extensive evaluations on challenging benchmarks demonstrate that our framework
significantly and consistently outperforms state-of-the-art baselines,
improving the effectiveness, efficiency, and stability of policy learning by up
to 32.70%, 64.86%, and 88.26%, respectively, as measured by average rewards,
convergence timesteps, and standard deviations.

</details>


### [674] [Interpretable global minima of deep ReLU neural networks on sequentially separable data](https://arxiv.org/pdf/2405.07098)
*Thomas Chen, Patrcia Muoz Ewald*

Main category: cs.LG

TL;DR: The paper constructs zero-loss neural network classifiers using recursive truncation maps and specific data configurations.


<details>
  <summary>Details</summary>
Motivation: To develop neural networks that achieve zero loss by leveraging well-structured training data configurations.

Method: Explicit construction of weight matrices and bias vectors using cumulative parameters and recursive truncation maps. Data configurations include small, well-separated clusters and sequentially linearly separable equivalence classes.

Result: Global minimizers can be described with Q(M+2) parameters for Q classes in R^M.

Conclusion: The method efficiently achieves zero loss for specific data structures, offering a compact parameterization.

Abstract: We explicitly construct zero loss neural network classifiers. We write the
weight matrices and bias vectors in terms of cumulative parameters, which
determine truncation maps acting recursively on input space. The configurations
for the training data considered are (i) sufficiently small, well separated
clusters corresponding to each class, and (ii) equivalence classes which are
sequentially linearly separable. In the best case, for $Q$ classes of data in
$\mathbb{R}^M$, global minimizers can be described with $Q(M+2)$ parameters.

</details>


### [675] [Comba: Improving Bilinear RNNs with Closed-loop Control](https://arxiv.org/pdf/2506.02475)
*Jiaxi Hu, Yongqi Pan, Jusen Du, Disen Lan, Xiaqiang Tang, Qingsong Wen, Yuxuan Liang, Weigao Sun*

Main category: cs.LG

TL;DR: The paper introduces Bilinear RNNs, analyzes their advantages/limitations, and proposes Comba, a novel variant with efficient state/output feedback corrections, showing superior performance in language/vision tasks.


<details>
  <summary>Details</summary>
Motivation: To improve recurrent memory management in sequence modeling by leveraging bilinear interactions, addressing limitations of existing methods like Gated DeltaNet and Mamba.

Method: Proposes Comba, a Bilinear RNN variant with scalar-plus-low-rank state transition and feedback corrections, implemented via a hardware-efficient parallel kernel.

Result: Comba achieves superior performance and efficiency in language and vision modeling, demonstrated with 340M/1.3B parameter models.

Conclusion: Comba advances Bilinear RNNs with efficient feedback mechanisms, offering improved performance and scalability for sequence modeling tasks.

Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and
RWKV-7 have achieved performance improvements by supervising the recurrent
memory management through Delta learning rule. Unlike previous state-space
models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models
introduce interactions between the recurrent state and the key vector,
structurally resembling bilinear systems. In this paper, we first introduce the
concept of Bilinear RNNs with a comprehensive analysis on the advantages and
limitations of these models. Then, based on closed-loop control theory, we
propose a novel Bilinear RNN variant named Comba, which adopts a
scalar-plus-low-rank state transition, with both state feedback and output
feedback corrections. We also implement a hardware-efficient chunk-wise
parallel kernel in Triton and train models with 340M/1.3B parameters on
large-scale corpus. Comba demonstrates superior performance and computation
efficiency in both language and vision modeling.

</details>


### [676] [Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales](https://arxiv.org/pdf/2405.17618)
*Ju-Seung Byun, Andrew Perrault*

Main category: cs.LG

TL;DR: The paper introduces a symmetric RL loss using reverse cross entropy (RCE) to improve training stability in RL, RLHF, and RLAIF, demonstrating performance gains in various tasks and scales.


<details>
  <summary>Details</summary>
Motivation: RL training is unstable due to moving targets and high gradient variance, exacerbated by RLHF/RLAIF challenges like differing preferences and reward model errors.

Method: Adapts RCE from supervised learning to define a symmetric RL loss, tested with Symmetric A2C (SA2C) and Symmetric PPO (SPPO) in discrete (Atari) and continuous (MuJoCo, Box2D) tasks, and RLHF tasks for LLMs.

Result: Improved performance across tasks, especially with SPPO, and validated benefits in RLHF tasks like sentiment analysis and summarization.

Conclusion: The symmetric RL loss enhances training robustness and performance, making it effective for diverse RL applications.

Abstract: Reinforcement learning (RL) training is inherently unstable due to factors
such as moving targets and high gradient variance. Reinforcement Learning from
Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can
introduce additional difficulty. Differing preferences can complicate the
alignment process, and prediction errors in a trained reward model can become
more severe as the LLM generates unseen outputs. To enhance training
robustness, RL has adopted techniques from supervised learning, such as
ensembles and layer normalization. In this work, we improve the stability of RL
training by adapting the reverse cross entropy (RCE) from supervised learning
for noisy data to define a symmetric RL loss. We demonstrate performance
improvements across various tasks and scales. We conduct experiments in
discrete action tasks (Atari games) and continuous action space tasks (MuJoCo
benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with
and without added noise with especially notable performance in SPPO across
different hyperparameters. Furthermore, we validate the benefits of the
symmetric RL loss when using SPPO for large language models through improved
performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR
summarization tasks.

</details>


### [677] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/pdf/2506.08388)
*Edoardo Cetin, Tianyu Zhao, Yujin Tang*

Main category: cs.LG

TL;DR: A new framework, Reinforcement-Learned Teachers (RLTs), trains reasoning LMs to generate detailed explanations for distillation, avoiding RL's exploration challenges and outperforming larger LMs in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To address the exploration challenge in RL for reasoning LMs and improve distillation efficiency for training students.

Method: RLTs are trained with dense rewards by generating explanations for students and testing their understanding, avoiding the need for initial task-solving ability.

Result: A 7B RLT outperforms larger LMs in competition and graduate-level tasks, even for larger students and out-of-distribution tasks.

Conclusion: RLTs enhance efficiency and re-usability in RL reasoning frameworks, offering superior performance in distillation and cold-starting pipelines.

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [678] [Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization](https://arxiv.org/pdf/2506.12484)
*Filip Sondej, Yushi Yang, Mikoaj Kniejski, Marcel Windys*

Main category: cs.LG

TL;DR: MUDMAN introduces Disruption Masking and gradient normalization for irreversible unlearning, outperforming prior methods by 40%.


<details>
  <summary>Details</summary>
Motivation: Address risks of dangerous knowledge retention in language models despite safety fine-tuning.

Method: Disruption Masking ensures non-disruptive updates; meta-learning and gradient normalization are combined in MUDMAN.

Result: MUDMAN prevents recovery of dangerous capabilities, outperforming TAR by 40%.

Conclusion: MUDMAN sets a new state-of-the-art for robust unlearning.

Abstract: Language models can retain dangerous knowledge and skills even after
extensive safety fine-tuning, posing both misuse and misalignment risks. Recent
studies show that even specialized unlearning methods can be easily reversed.
To address this, we systematically evaluate many existing and novel components
of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating
weights, where the signs of the unlearning gradient and the retaining gradient
are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients,
and also confirm the usefulness of meta-learning. We combine these insights
into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and
validate its effectiveness at preventing the recovery of dangerous
capabilities. MUDMAN outperforms the prior TAR method by 40\%, setting a new
state-of-the-art for robust unlearning.

</details>


### [679] [AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining](https://arxiv.org/pdf/2506.13274)
*Hongyuan Dong, Dingkang Yang, Xiao Liang, Chao Feng, Jiao Ran*

Main category: cs.LG

TL;DR: AdaLRS is an adaptive learning rate search algorithm for foundation model pretraining, optimizing loss descent velocities with minimal extra computation and proven convergence.


<details>
  <summary>Details</summary>
Motivation: Existing learning rate transferability methods are limited to specific scenarios and require extensive tuning.

Method: AdaLRS conducts online optimal learning rate search by optimizing loss descent velocities, leveraging convexity in training loss and loss descent velocity.

Result: AdaLRS efficiently adjusts suboptimal learning rates to optimal levels, improving model performance across LLM and VLM pretraining.

Conclusion: AdaLRS is robust and generalizable across diverse training scenarios, offering a plug-and-play solution for learning rate optimization.

Abstract: Learning rate is widely regarded as crucial for effective foundation model
pretraining. Recent research explores and demonstrates the transferability of
learning rate configurations across varying model and dataset sizes, etc.
Nevertheless, these approaches are constrained to specific training scenarios
and typically necessitate extensive hyperparameter tuning on proxy models. In
this work, we propose \textbf{AdaLRS}, a plug-in-and-play adaptive learning
rate search algorithm that conducts online optimal learning rate search via
optimizing loss descent velocities. We provide experiment results to show that
the optimization of training loss and loss descent velocity in foundation model
pretraining are both convex and share the same optimal learning rate. Relying
solely on training loss dynamics, AdaLRS involves few extra computations to
guide the search process, and its convergence is guaranteed via theoretical
analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts
suboptimal learning rates to the neighborhood of optimum with marked efficiency
and effectiveness, with model performance improved accordingly. We also show
the robust generalizability of AdaLRS across varying training scenarios, such
as different model sizes, training paradigms, and base learning rate scheduler
choices.

</details>


### [680] [Conformal Prediction for Causal Effects of Continuous Treatments](https://arxiv.org/pdf/2407.03094)
*Maresa Schrder, Dennis Frauen, Jonas Schweisthal, Konstantin He, Valentyn Melnychuk, Stefan Feuerriegel*

Main category: cs.LG

TL;DR: A novel conformal prediction method for continuous treatments is introduced, addressing uncertainty in causal effects without requiring known propensity scores.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification in causal effects is vital for safety-critical applications like personalized medicine, but existing methods are limited to binary/discrete treatments and restrictive assumptions.

Method: The paper proposes a conformal prediction method for continuous treatments, accounting for uncertainty in propensity score estimation, and provides an algorithm for calculating prediction intervals.

Result: Finite-sample prediction intervals for potential outcomes are derived, and the method's effectiveness is demonstrated on synthetic and real-world datasets.

Conclusion: This work is the first to offer conformal prediction for continuous treatments with unknown propensity scores, providing valid intervals even when the propensity score must be estimated.

Abstract: Uncertainty quantification of causal effects is crucial for safety-critical
applications such as personalized medicine. A powerful approach for this is
conformal prediction, which has several practical benefits due to
model-agnostic finite-sample guarantees. Yet, existing methods for conformal
prediction of causal effects are limited to binary/discrete treatments and make
highly restrictive assumptions such as known propensity scores. In this work,
we provide a novel conformal prediction method for potential outcomes of
continuous treatments. We account for the additional uncertainty introduced
through propensity estimation so that our conformal prediction intervals are
valid even if the propensity score is unknown. Our contributions are
three-fold: (1) We derive finite-sample prediction intervals for potential
outcomes of continuous treatments. (2) We provide an algorithm for calculating
the derived intervals. (3) We demonstrate the effectiveness of the conformal
prediction intervals in experiments on synthetic and real-world datasets. To
the best of our knowledge, we are the first to propose conformal prediction for
continuous treatments when the propensity score is unknown and must be
estimated from data.

</details>


### [681] [PREMAP: A Unifying PREiMage APproximation Framework for Neural Networks](https://arxiv.org/pdf/2408.09262)
*Xiyue Zhang, Benjie Wang, Marta Kwiatkowska, Huan Zhang*

Main category: cs.LG

TL;DR: A framework for preimage abstraction in neural networks, using linear relaxations and refinement to approximate input sets for given output properties, improving efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on bounding outputs for given inputs, but verifying input properties (preimage) requires input-space abstractions.

Method: Uses parameterised linear relaxations and an anytime refinement procedure, splitting input regions and neurons for better approximations.

Result: Demonstrates efficiency and scalability, outperforming state-of-the-art techniques in high-dimensional tasks.

Conclusion: The framework is effective for quantitative verification and robustness analysis, providing sound and complete results.

Abstract: Most methods for neural network verification focus on bounding the image,
i.e., set of outputs for a given input set. This can be used to, for example,
check the robustness of neural network predictions to bounded perturbations of
an input. However, verifying properties concerning the preimage, i.e., the set
of inputs satisfying an output property, requires abstractions in the input
space. We present a general framework for preimage abstraction that produces
under- and over-approximations of any polyhedral output set. Our framework
employs cheap parameterised linear relaxations of the neural network, together
with an anytime refinement procedure that iteratively partitions the input
region by splitting on input features and neurons. The effectiveness of our
approach relies on carefully designed heuristics and optimization objectives to
achieve rapid improvements in the approximation volume. We evaluate our method
on a range of tasks, demonstrating significant improvement in efficiency and
scalability to high-input-dimensional image classification tasks compared to
state-of-the-art techniques. Further, we showcase the application to
quantitative verification and robustness analysis, presenting a sound and
complete algorithm for the former and providing sound quantitative results for
the latter.

</details>


### [682] [Smooth InfoMax -- Towards Easier Post-Hoc Interpretability](https://arxiv.org/pdf/2408.12936)
*Fabian Denoodt, Bart de Boer, Jos Oramas*

Main category: cs.LG

TL;DR: Smooth InfoMax (SIM) is a self-supervised method for interpretable representation learning, using probabilistic modules and InfoNCE loss to create smooth, disentangled latent spaces.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability of latent representations in deep networks while retaining large-scale training benefits.

Method: Uses $eta$-VAEs with probabilistic modules, locally optimized via InfoNCE loss, to produce Gaussian-distributed representations regularized toward standard normal.

Result: SIM creates smooth, well-defined latent spaces, enhancing post-hoc interpretability on speech data.

Conclusion: SIM improves interpretability while preserving training efficiency, making it effective for post-hoc analysis.

Abstract: We introduce Smooth InfoMax (SIM), a self-supervised representation learning
method that incorporates interpretability constraints into the latent
representations at different depths of the network. Based on $\beta$-VAEs,
SIM's architecture consists of probabilistic modules optimized locally with the
InfoNCE loss to produce Gaussian-distributed representations regularized toward
the standard normal distribution. This creates smooth, well-defined, and
better-disentangled latent spaces, enabling easier post-hoc analysis. Evaluated
on speech data, SIM preserves the large-scale training benefits of Greedy
InfoMax while improving the effectiveness of post-hoc interpretability methods
across layers.

</details>


### [683] [Bridging Geometric Diffusion and Energy Minimization: A Unified Framework for Neural Message Passing](https://arxiv.org/pdf/2409.09111)
*Qitian Wu, David Wipf, Junchi Yan*

Main category: cs.LG

TL;DR: The paper proposes an energy-constrained diffusion model to understand and improve message passing neural networks (MPNNs), unifying various architectures under this framework and introducing diffusion-inspired Transformers for better performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning representations for structured data with geometries, leveraging MPNNs and providing a principled mathematical framework for their understanding and enhancement.

Method: Introduces an energy-constrained diffusion model combining diffusion on manifolds with energy minimization, linking diffusion operators to energy functions and deriving MPNN propagation layers.

Result: Demonstrates that the model unifies common neural architectures (e.g., MLP, GCN, Transformers) and achieves promising performance across diverse datasets.

Conclusion: The energy-constrained diffusion framework offers a unified perspective on MPNNs and enables the design of improved models like diffusion-inspired Transformers.

Abstract: Learning representations for structured data with certain geometries (e.g.,
observed or unobserved) is a fundamental challenge, wherein message passing
neural networks (MPNNs) have become a de facto class of model solutions. In
this paper, we propose an energy-constrained diffusion model as a principled
mathematical framework for understanding the mechanism of MPNNs and navigating
novel architectural designs. Inspired by physical systems, the model combines
the inductive bias of diffusion on manifolds with layer-wise constraints of
energy minimization. We identify that the diffusion operators have a one-to-one
correspondence with the energy functions implicitly descended by the diffusion
process, and the finite-difference iteration for solving the energy-constrained
diffusion system induces the propagation layers of various types of MPNNs
operating on observed or latent structures. This leads to a unified perspective
on common neural architectures whose computational flows can be cast as message
passing (or its special case), including MLP, GCN, GIN, APPNP, GCNII, GAT, and
Transformers. Building on these insights, we devise a new class of neural
message passing models, dubbed diffusion-inspired Transformers, whose global
attention layers are derived from the principled energy-constrained diffusion
framework. Across diverse datasets, ranging from real-world networks to images,
texts, and physical particles, we demonstrate that the new model achieves
promising performance in scenarios where the data structures are observed (as a
graph), partially observed, or entirely unobserved.

</details>


### [684] [One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models](https://arxiv.org/pdf/2410.22366)
*Viacheslav Surkov, Chris Wendler, Antonio Mari, Mikhail Terekhov, Justin Deschenaux, Robert West, Caglar Gulcehre, David Bau*

Main category: cs.LG

TL;DR: SAEs decompose SDXL Turbo's intermediate representations into interpretable features, generalizing to other models and enabling causal influence on image generation.


<details>
  <summary>Details</summary>
Motivation: To extend SAE-based interpretability from LLMs to text-to-image models like SDXL Turbo.

Method: Train SAEs on transformer block updates in SDXL Turbo's U-net, then evaluate generalization to other models and interpretability via RIEBench.

Result: SAEs generalize to 4-step SDXL Turbo and SDXL base model; features are interpretable and causally influence generation.

Conclusion: SAEs are a promising tool for understanding and manipulating text-to-image diffusion models.

Abstract: For large language models (LLMs), sparse autoencoders (SAEs) have been shown
to decompose intermediate representations that often are not interpretable
directly into sparse sums of interpretable features, facilitating better
control and subsequent analysis. However, similar analyses and approaches have
been lacking for text-to-image models. We investigate the possibility of using
SAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image
diffusion model. To this end, we train SAEs on the updates performed by
transformer blocks within SDXL Turbo's denoising U-net in its 1-step setting.
Interestingly, we find that they generalize to 4-step SDXL Turbo and even to
the multi-step SDXL base model (i.e., a different model) without additional
training. In addition, we show that their learned features are interpretable,
causally influence the generation process, and reveal specialization among the
blocks. We do so by creating RIEBench, a representation-based image editing
benchmark, for editing images while they are generated by turning on and off
individual SAE features. This allows us to track which transformer blocks'
features are the most impactful depending on the edit category. Our work is the
first investigation of SAEs for interpretability in text-to-image diffusion
models and our results establish SAEs as a promising approach for understanding
and manipulating the internal mechanisms of text-to-image models.

</details>


### [685] [On the fast convergence of minibatch heavy ball momentum](https://arxiv.org/pdf/2206.07553)
*Raghu Bollapragada, Tyler Chen, Rachel Ward*

Main category: cs.LG

TL;DR: Stochastic heavy ball momentum retains the fast linear rate of deterministic heavy ball momentum on quadratic problems with large enough batch sizes, bridging theory and practice.


<details>
  <summary>Details</summary>
Motivation: The gap between the practical success of stochastic momentum methods and the lack of theoretical acceleration guarantees.

Method: Analyzing stochastic heavy ball momentum as an accelerated randomized Kaczmarz algorithm with minibatching, using spectral norm concentration bounds for random matrices.

Result: The method achieves fast linear convergence on quadratic problems with sufficient batch sizes, supported by numerical evidence.

Conclusion: The work provides theoretical justification for the practical effectiveness of stochastic heavy ball momentum, with sharp bounds.

Abstract: Simple stochastic momentum methods are widely used in machine learning
optimization, but their good practical performance is at odds with an absence
of theoretical guarantees of acceleration in the literature. In this work, we
aim to close the gap between theory and practice by showing that stochastic
heavy ball momentum retains the fast linear rate of (deterministic) heavy ball
momentum on quadratic optimization problems, at least when minibatching with a
sufficiently large batch size. The algorithm we study can be interpreted as an
accelerated randomized Kaczmarz algorithm with minibatching and heavy ball
momentum. The analysis relies on carefully decomposing the momentum transition
matrix, and using new spectral norm concentration bounds for products of
independent random matrices. We provide numerical illustrations demonstrating
that our bounds are reasonably sharp.

</details>


### [686] [DeepMedcast: A Deep Learning Method for Generating Intermediate Weather Forecasts among Multiple NWP Models](https://arxiv.org/pdf/2411.10010)
*Atsushi Kudo*

Main category: cs.LG

TL;DR: DeepMedcast is a deep learning method that generates intermediate, meteorologically realistic forecasts between NWP outputs, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The expansion of NWP models raises uncertainty about the most plausible forecast, and traditional methods like averaging produce unrealistic outputs.

Method: DeepMedcast uses deep learning to align meteorologically significant features with the arithmetic mean of input NWP models, preserving structure.

Result: Case studies show DeepMedcast produces realistic, interpretable forecasts with higher accuracy than input models.

Conclusion: DeepMedcast enhances operational forecasting efficiency and standardization by providing plausible intermediate forecasts.

Abstract: Numerical weather prediction (NWP) centers around the world operate a variety
of NWP models. In addition, recent advances in AI-driven NWP models have
further increased the availability of NWP outputs. While this expansion holds
the potential to improve forecast accuracy, it raises a critical question:
which prediction is the most plausible? If the NWP models have comparable
accuracy, it is impossible to determine in advance which one is the best.
Traditional approaches, such as ensemble or weighted averaging, combine
multiple NWP outputs to produce a single forecast with improved accuracy.
However, they often result in meteorologically unrealistic and uninterpretable
outputs, such as the splitting of tropical cyclone centers or frontal
boundaries into multiple distinct systems.
  To address this issue, we propose DeepMedcast, a deep learning method that
generates intermediate forecasts between two or more NWP outputs. Unlike
averaging, DeepMedcast provides predictions in which meteorologically
significant features -- such as the locations of tropical cyclones,
extratropical cyclones, fronts, and shear lines -- approximately align with the
arithmetic mean of the corresponding features predicted by the input NWP
models, without distorting meteorological structures. We demonstrate the
capability of DeepMedcast through case studies and verification results,
showing that it produces realistic and interpretable forecasts with higher
accuracy than the input NWP models. By providing plausible intermediate
forecasts, DeepMedcast can significantly contribute to the efficiency and
standardization of operational forecasting tasks, including general, marine,
and aviation forecasts.

</details>


### [687] [Gaussian Process Latent Variable Modeling for Few-shot Time Series Forecasting](https://arxiv.org/pdf/2212.10306)
*Yunyao Cheng, Chenjuan Guo, Kaixuan Chen, Kai Zhao, Bin Yang, Jiandong Xie, Christian S. Jensen, Feiteng Huang, Kai Zheng*

Main category: cs.LG

TL;DR: MetaGP, a meta-learning-based Gaussian process model, improves few-shot time series forecasting by capturing long-term dependencies and modeling meta-knowledge explicitly.


<details>
  <summary>Details</summary>
Motivation: Accurate time series forecasting is vital for resource optimization and urban management, but limited training samples and existing models' shortcomings in capturing long-term dependencies and meta-knowledge pose challenges.

Method: Proposes MetaGP, combining Gaussian process latent variable modeling with Kernel Association Search (KAS) for meta-knowledge modeling.

Result: MetaGP achieves state-of-the-art accuracy on few-shot datasets, capturing long-term dependencies and providing insights into complex patterns.

Conclusion: MetaGP effectively addresses few-shot forecasting challenges, enhancing interpretability and prediction accuracy.

Abstract: Accurate time series forecasting is crucial for optimizing resource
allocation, industrial production, and urban management, particularly with the
growth of cyber-physical and IoT systems. However, limited training sample
availability in fields like physics and biology poses significant challenges.
Existing models struggle to capture long-term dependencies and to model diverse
meta-knowledge explicitly in few-shot scenarios. To address these issues, we
propose MetaGP, a meta-learning-based Gaussian process latent variable model
that uses a Gaussian process kernel function to capture long-term dependencies
and to maintain strong correlations in time series. We also introduce Kernel
Association Search (KAS) as a novel meta-learning component to explicitly model
meta-knowledge, thereby enhancing both interpretability and prediction
accuracy. We study MetaGP on simulated and real-world few-shot datasets,
showing that it is capable of state-of-the-art prediction accuracy. We also
find that MetaGP can capture long-term dependencies and can model
meta-knowledge, thereby providing valuable insights into complex time series
patterns.

</details>


### [688] [Non-asymptotic approximations of Gaussian neural networks via second-order Poincar inequalities](https://arxiv.org/pdf/2304.04010)
*Alberto Bordino, Stefano Favaro, Sandra Fortini*

Main category: cs.LG

TL;DR: The paper explores using second-order Poincar inequalities to derive quantitative central limit theorems (QCLTs) for deep Gaussian neural networks (NNs), offering a generalizable method despite suboptimal convergence rates.


<details>
  <summary>Details</summary>
Motivation: To provide an alternative, more general approach to establishing QCLTs for NNs, avoiding the complexity of recursive layer-by-layer analysis.

Method: Utilizes second-order Poincar inequalities, focusing on the NN's output as a functional of a Gaussian process, simplifying the problem to computing gradients and Hessians.

Result: The method successfully establishes QCLTs but yields suboptimal convergence rates compared to existing approaches.

Conclusion: The trade-off for generality and simplicity is a less optimal convergence rate, highlighting the cost of a more straightforward procedure.

Abstract: There is a recent and growing literature on large-width asymptotic and
non-asymptotic properties of deep Gaussian neural networks (NNs), namely NNs
with weights initialized as Gaussian distributions. For a Gaussian NN of depth
$L\geq1$ and width $n\geq1$, it is well-known that, as $n\rightarrow+\infty$,
the NN's output converges (in distribution) to a Gaussian process. Recently,
some quantitative versions of this result, also known as quantitative central
limit theorems (QCLTs), have been obtained, showing that the rate of
convergence is $n^{-1}$, in the $2$-Wasserstein distance, and that such a rate
is optimal. In this paper, we investigate the use of second-order Poincar\'e
inequalities as an alternative approach to establish QCLTs for the NN's output.
Previous approaches consist of a careful analysis of the NN, by combining
non-trivial probabilistic tools with ad-hoc techniques that rely on the
recursive definition of the network, typically by means of an induction
argument over the layers, and it is unclear if and how they still apply to
other NN's architectures. Instead, the use of second-order Poincar\'e
inequalities rely only on the fact that the NN is a functional of a Gaussian
process, reducing the problem of establishing QCLTs to the algebraic problem of
computing the gradient and Hessian of the NN's output, which still applies to
other NN's architectures. We show how our approach is effective in establishing
QCLTs for the NN's output, though it leads to suboptimal rates of convergence.
We argue that such a worsening in the rates is peculiar to second-order
Poincar\'e inequalities, and it should be interpreted as the "cost" for having
a straightforward, and general, procedure for obtaining QCLTs.

</details>


### [689] [Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection](https://arxiv.org/pdf/2305.17528)
*Nils Palumbo, Yang Guo, Xi Wu, Jiefeng Chen, Yingyu Liang, Somesh Jha*

Main category: cs.LG

TL;DR: The paper improves adversarial defense by combining transduction and rejection, using a novel reduction technique for better robust accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance the practical performance of transduction+rejection defenses against strong adversarial attacks.

Method: Applies a reduction technique by Tramr in a transductive setting to design a selective model.

Result: Achieves 81.6% robust accuracy on CIFAR-10 and 57.9% on CIFAR-100 under l attacks.

Conclusion: The approach significantly outperforms existing techniques in robust generalization.

Abstract: Both transduction and rejection have emerged as important techniques for
defending against adversarial perturbations. A recent work by Goldwasser et al.
showed that rejection combined with transduction can give provable guarantees
(for certain problems) that cannot be achieved otherwise. Nevertheless, under
recent strong adversarial attacks, their work was shown to have low performance
in a practical deep-learning setting. In this paper, we take a step towards
realizing the promise of transduction+rejection in more realistic scenarios.
Our key observation is that a novel application of a reduction technique by
Tram\`er, which was until now only used to demonstrate the vulnerability of
certain defenses, can be used to actually construct effective defenses.
Theoretically, we show that a careful application of this technique in the
transductive setting can give significantly improved sample-complexity for
robust generalization. Our theory guides us to design a new transductive
algorithm for learning a selective model; extensive experiments using state of
the art attacks show that our approach provides significantly better robust
accuracy (81.6% on CIFAR-10 and 57.9% on CIFAR-100 under $l_\infty$ with budget
8/255) than existing techniques.

</details>


### [690] [An Expanded Benchmark that Rediscovers and Affirms the Edge of Uncertainty Sampling for Active Learning in Tabular Datasets](https://arxiv.org/pdf/2306.08954)
*Po-Yi Lu, Yi-Jie Cheng, Chun-Liang Li, Hsuan-Tien Lin*

Main category: cs.LG

TL;DR: The paper reviews Active Learning (AL) strategies, focusing on Uncertainty Sampling (US), and builds a comprehensive benchmark to evaluate their effectiveness, revealing the importance of model compatibility for US's success.


<details>
  <summary>Details</summary>
Motivation: To address conflicting conclusions about the competitiveness of Uncertainty Sampling (AL strategy) and provide a comprehensive benchmark for evaluating AL strategies.

Method: Review of AL literature over the last decade and creation of an open-source AL benchmark covering diverse strategies, models, and datasets.

Result: US remains competitive when paired with compatible models, but its effectiveness degrades with incompatible models.

Conclusion: The study highlights the importance of model compatibility in US and provides actionable insights for AL practitioners, especially in tabular classification with limited labeled data.

Abstract: Active Learning (AL) addresses the crucial challenge of enabling machines to
efficiently gather labeled examples through strategic queries. Among the many
AL strategies, Uncertainty Sampling (US) stands out as one of the most widely
adopted. US queries the example(s) that the current model finds uncertain,
proving to be both straightforward and effective. Despite claims in the
literature suggesting superior alternatives to US, community-wide acceptance
remains elusive. In fact, existing benchmarks for tabular datasets present
conflicting conclusions on the continued competitiveness of US. In this study,
we review the literature on AL strategies in the last decade and build the most
comprehensive open-source AL benchmark to date to understand the relative
merits of different AL strategies. The benchmark surpasses existing ones by
encompassing a broader coverage of strategies, models, and data. Through our
investigation of the conflicting conclusions in existing tabular AL benchmarks
by evaluation under broad AL experimental settings, we uncover fresh insights
into the often-overlooked issue of using machine learning models--**model
compatibility** in the context of US. Specifically, we notice that adopting the
different models for the querying unlabeled examples and learning tasks would
degrade US's effectiveness. Notably, our findings affirm that US maintains a
competitive edge over other strategies when paired with compatible models.
These findings have practical implications and provide a concrete recipe for AL
practitioners, empowering them to make informed decisions when working with
tabular classifications with limited labeled data. The code for this project is
available on https://github.com/ariapoy/active-learning-benchmark.

</details>


### [691] [Kernel Limit of Recurrent Neural Networks Trained on Ergodic Data Sequences](https://arxiv.org/pdf/2308.14555)
*Samuel Chun-Hei Lam, Justin Sirignano, Konstantinos Spiliopoulos*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Mathematical methods are developed to characterize the asymptotics of
recurrent neural networks (RNN) as the number of hidden units, data samples in
the sequence, hidden state updates, and training steps simultaneously grow to
infinity. In the case of an RNN with a simplified weight matrix, we prove the
convergence of the RNN to the solution of an infinite-dimensional ODE coupled
with the fixed point of a random algebraic equation. The analysis requires
addressing several challenges which are unique to RNNs. In typical mean-field
applications (e.g., feedforward neural networks), discrete updates are of
magnitude $\mathcal{O}(\frac{1}{N})$ and the number of updates is
$\mathcal{O}(N)$. Therefore, the system can be represented as an Euler
approximation of an appropriate ODE/PDE, which it will converge to as $N
\rightarrow \infty$. However, the RNN hidden layer updates are
$\mathcal{O}(1)$. Therefore, RNNs cannot be represented as a discretization of
an ODE/PDE and standard mean-field techniques cannot be applied. Instead, we
develop a fixed point analysis for the evolution of the RNN memory states, with
convergence estimates in terms of the number of update steps and the number of
hidden units. The RNN hidden layer is studied as a function in a Sobolev space,
whose evolution is governed by the data sequence (a Markov chain), the
parameter updates, and its dependence on the RNN hidden layer at the previous
time step. Due to the strong correlation between updates, a Poisson equation
must be used to bound the fluctuations of the RNN around its limit equation.
These mathematical methods give rise to the neural tangent kernel (NTK) limits
for RNNs trained on data sequences as the number of data samples and size of
the neural network grow to infinity.

</details>


### [692] [DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic](https://arxiv.org/pdf/2310.17173)
*Dexter Neo, Tsuhan Chen*

Main category: cs.LG

TL;DR: Extended Soft Actor-Critic (SAC) with statistical constraints for improved robustness and performance in discrete settings.


<details>
  <summary>Details</summary>
Motivation: Enhance SAC's performance and robustness, especially for real-world deployment, by incorporating additional statistical constraints.

Method: Introduces constraints from a surrogate critic policy, grounded in the Maximum Entropy Principle, for discrete SAC.

Result: Improved robustness against domain shifts and better performance in low-data regimes, validated on Atari 2600 games.

Conclusion: The extended SAC with statistical constraints is effective for real-world reinforcement learning applications.

Abstract: We present a novel extension to the family of Soft Actor-Critic (SAC)
algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC
can be further improved via additional statistical constraints derived from a
surrogate critic policy. Furthermore, our findings suggests that these
constraints provide an added robustness against potential domain shifts, which
are essential for safe deployment of reinforcement learning agents in the
real-world. We provide theoretical analysis and show empirical results on low
data regimes for both in-distribution and out-of-distribution variants of Atari
2600 games.

</details>


### [693] [Open-world machine learning: A review and new outlooks](https://arxiv.org/pdf/2403.01759)
*Fei Zhu, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang, Dacheng Tao, Cheng-Lin Liu*

Main category: cs.LG

TL;DR: The paper discusses open-world machine learning, addressing challenges like unknown rejection, novelty discovery, and continual learning to adapt to dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Current machine learning assumes a static environment, but real-world applications are dynamic and unknown-filled, requiring adaptive models.

Method: Investigates unknown rejection, novelty discovery, and continual learning in a unified framework, analyzing methodologies, benchmarks, and metrics.

Result: Summarizes challenges, principles, and limitations of current approaches, along with performance benchmarks.

Conclusion: Proposes future directions for open-world ML, aiming to advance AI systems and artificial general intelligence.

Abstract: Machine learning has achieved remarkable success in many applications.
However, existing studies are largely based on the closed-world assumption,
which assumes that the environment is stationary, and the model is fixed once
deployed. In many real-world applications, this fundamental and rather naive
assumption may not hold because an open environment is complex, dynamic, and
full of unknowns. In such cases, rejecting unknowns, discovering novelties, and
then continually learning them, could enable models to be safe and evolve
continually as biological systems do. This article presents a holistic view of
open-world machine learning by investigating unknown rejection, novelty
discovery, and continual learning in a unified paradigm. The challenges,
principles, and limitations of current methodologies are discussed in detail.
Furthermore, widely used benchmarks, metrics, and performances are summarized.
Finally, we discuss several potential directions for further progress in the
field. By providing a comprehensive introduction to the emerging open-world
machine learning paradigm, this article aims to help researchers build more
powerful AI systems in their respective fields, and to promote the development
of artificial general intelligence.

</details>


### [694] [EXPRTS: Exploring and Probing the Robustness ofTime Series Forecasting Models](https://arxiv.org/pdf/2403.03508)
*Hkon Hanisch Kjrnli, Lluis Mas-Ribas, Hans Jakob Hland, Vegard Sjvik, Aida Ashrafi, Helge Langseth, Odd Erik Gundersen*

Main category: cs.LG

TL;DR: A framework for generating interpretable time series to improve forecasting robustness against distribution drift, demonstrated via the EXPRTS tool.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of robustness in ML forecasting models when faced with out-of-distribution data due to distribution drift.

Method: Combines time-series decompositions with analytic functions to generate interpretable time series matching in- and out-of-distribution data.

Result: Developed EXPRTS, a visual analytics tool, validated through use-cases and a user study, showing improved model robustness.

Conclusion: The framework provides an interpretable and effective way to generate time series for enhancing forecasting robustness.

Abstract: When deploying time series forecasting models based on machine learning to
real world settings, one often encounter situations where the data distribution
drifts. Such drifts expose the forecasting models to out-of-distribution (OOD)
data, and machine learning models lack robustness in these settings. Robustness
can be improved by using deep generative models or genetic algorithms to
augment time series datasets, but these approaches lack interpretability and
are computationally expensive. In this work, we develop an interpretable and
simple framework for generating time series. Our method combines time-series
decompositions with analytic functions, and is able to generate time series
with characteristics matching both in- and out-of-distribution data. This
approach allows users to generate new time series in an interpretable fashion,
which can be used to augment the dataset and improve forecasting robustness. We
demonstrate our framework through EXPRTS, a visual analytics tool designed for
univariate time series forecasting models and datasets. Different
visualizations of the data distribution, forecasting errors and single time
series instances enable users to explore time series datasets, apply
transformations, and evaluate forecasting model robustness across diverse
scenarios. We show how our framework can generate meaningful OOD time series
that improve model robustness, and we validate EXPRTS effectiveness and
usability through three use-cases and a user study.

</details>


### [695] [SPD-CFL: Stepwise Parameter Dropout for Efficient Continual Federated Learning](https://arxiv.org/pdf/2405.09394)
*Yuning Yang, Han Yu, Chuan Sun, Tianrun Gao, Xiaohong Liu, Xiaodong Xu, Ping Zhang, Guangyu Wang*

Main category: cs.LG

TL;DR: SPD-CFL is a federated learning method that adaptively adjusts parameter dropout for efficiency and performance, outperforming baselines in test AUC and reducing communication overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional FL struggles with large pre-trained models due to high communication costs, and existing PEFT methods require manual dropout rate tuning, which is inefficient.

Method: SPD-CFL uses stepwise parameter dropout and sensitivity-based gradient consistency to adaptively adjust dropout rates, alongside continual learning on clients to handle heterogeneous FL.

Result: SPD-CFL achieves 2.07% higher test AUC and reduces communication overhead by 29.53% compared to the best baseline.

Conclusion: SPD-CFL effectively balances performance and efficiency in federated learning, making it superior to existing methods.

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm for
training models on local sensitive data with privacy protection. Pre-trained
transformer-based models have emerged as useful foundation models (FMs) to be
fine-tuned for a wide range of downstream tasks. However, large-scale
pre-trained models make it challenging for traditional FL due to high
communication overhead in the resource-constrained IoT. This has inspired the
field of parameter-efficient fine-tuning (PEFT) research. Existing PEFT methods
attempt to optimize model performance at the given dropout level. Such an
approach places the burden on human users to find a dropout rate that provides
a satisfactory level of performance through trial-and-error, which is time
consuming and resource intensive. To address this limitation, we propose the
Step-wise Parameter Dropout for Continual Federated Learning (SPD-CFL)
approach. Instead of pre-defining a desired dropout rate, it allows users to
specify the target level of performance and then attempts to find the most
suitable dropout rate for the given FL model. Specifically, on the server side,
SPD-CFL drops trainable parameters in a stepwise manner to improve
communication efficiency by reducing the rank of low-rank adaptation (LoRA).
The sensitivity-based gradient consistency (SGC) measure is designed to
facilitate the adaptive adjustment of parameter dropout. In addition, SPD-CFL
introduces continual learning (CL) on the client side to mitigate performance
degradation due to the inconsistent optima with distinct parameter dropout
rates under heterogeneous FL. Extensive experiments on the public benchmark
dataset CIFAR-10 and a real-world medical Face dataset demonstrate significant
superiority of SPD-CFL over state-of-the-art methods. Compared to the
best-performing baseline, it achieves a 2.07% higher test AUC while reducing
communication overhead by 29.53%.

</details>


### [696] [Federated Learning With Energy Harvesting Devices: An MDP Framework](https://arxiv.org/pdf/2405.10513)
*Kai Zhang, Xuanyu Cao, Khaled B. Letaief*

Main category: cs.LG

TL;DR: The paper addresses energy consumption in federated learning (FL) by integrating energy harvesting techniques, optimizing device scheduling and power control, and proposing low-complexity and deep reinforcement learning algorithms for efficient FL operation.


<details>
  <summary>Details</summary>
Motivation: The rapid depletion of battery-limited edge devices in FL systems hinders their operational lifespan and learning performance, prompting the need for energy-efficient solutions.

Method: The authors implement energy harvesting, formulate a joint device scheduling and power control problem as an MDP, derive an optimal transmission policy, and propose low-complexity and structure-enhanced deep reinforcement learning algorithms.

Result: The proposed algorithms are shown to be asymptotically optimal and effective in improving convergence and training performance, validated through numerical experiments.

Conclusion: Energy harvesting and optimized policies significantly enhance FL system efficiency, with the proposed algorithms offering practical solutions for real-world deployment.

Abstract: Federated learning (FL) necessitates that edge devices conduct local training
and communicate with a parameter server, resulting in significant energy
consumption. A key challenge in practical FL systems is the rapid depletion of
battery-limited edge devices, which limits their operational lifespan and
impacts learning performance. To tackle this issue, we implement energy
harvesting techniques in FL systems to capture ambient energy, thereby
providing continuous power to edge devices. We first establish the convergence
bound for the wireless FL system with energy harvesting devices, illustrating
that the convergence is affected by partial device participation and packet
drops, both of which depend on the energy supply. To accelerate the
convergence, we formulate a joint device scheduling and power control problem
and model it as a Markov decision process (MDP). By solving this MDP, we derive
the optimal transmission policy and demonstrate that it possesses a monotone
structure with respect to the battery and channel states. To overcome the curse
of dimensionality caused by the exponential complexity of computing the optimal
policy, we propose a low-complexity algorithm, which is asymptotically optimal
as the number of devices increases. Furthermore, for unknown channels and
harvested energy statistics, we develop a structure-enhanced deep reinforcement
learning algorithm that leverages the monotone structure of the optimal policy
to improve the training performance. Finally, extensive numerical experiments
on real-world datasets are presented to validate the theoretical results and
corroborate the effectiveness of the proposed algorithms.

</details>


### [697] [BAnG: Bidirectional Anchored Generation for Conditional RNA Design](https://arxiv.org/pdf/2502.21274)
*Roman Klypa, Alberto Bietti, Sergei Grudinin*

Main category: cs.LG

TL;DR: RNA-BAnG is a deep learning model for generating RNA sequences that interact with proteins, overcoming the need for extensive prior data or structural knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing methods require large datasets or detailed RNA structures, limiting practical use. RNA-BAnG addresses this gap.

Method: Uses Bidirectional Anchored Generation (BAnG), focusing on functional binding motifs within RNA sequences. Validated on synthetic and biological tasks.

Result: Outperforms existing generative methods in synthetic tasks and effectively designs RNA sequences for protein binding.

Conclusion: RNA-BAnG offers a practical, data-efficient solution for RNA sequence design in protein interactions.

Abstract: Designing RNA molecules that interact with specific proteins is a critical
challenge in experimental and computational biology. Existing computational
approaches require a substantial amount of previously known interacting RNA
sequences for each specific protein or a detailed knowledge of RNA structure,
restricting their utility in practice. To address this limitation, we develop
RNA-BAnG, a deep learning-based model designed to generate RNA sequences for
protein interactions without these requirements. Central to our approach is a
novel generative method, Bidirectional Anchored Generation (BAnG), which
leverages the observation that protein-binding RNA sequences often contain
functional binding motifs embedded within broader sequence contexts. We first
validate our method on generic synthetic tasks involving similar localized
motifs to those appearing in RNAs, demonstrating its benefits over existing
generative approaches. We then evaluate our model on biological sequences,
showing its effectiveness for conditional RNA sequence design given a binding
protein.

</details>


### [698] [Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for Learning General Purpose Visual Representations](https://arxiv.org/pdf/2405.14239)
*Mohammed Baharoon, Jonathan Klein, Dominik L. Michels*

Main category: cs.LG

TL;DR: Harmony combines vision-language training with self-supervision to improve localized feature learning for dense prediction tasks, outperforming CLIP and other methods.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP lack localized feature learning, limiting performance on dense tasks. Self-supervised methods complement this but need integration.

Method: Harmony integrates vision-language training with discriminative and generative self-supervision, avoids negative examples, and uses soft CLIP targets.

Result: Harmony outperforms CLIP and leading methods (SLIP, MaskCLIP, DetailCLIP) across various vision tasks.

Conclusion: Harmony effectively combines supervision types for robust feature learning, especially in data-constrained settings.

Abstract: Vision-language contrastive learning frameworks such as CLIP enable learning
representations from natural language supervision and provide strong zero-shot
classification capabilities. However, due to the nature of the supervisory
signal in these paradigms, they lack the ability to learn localized features,
leading to degraded performance on dense prediction tasks such as segmentation
and detection. On the other hand, self-supervised learning methods have shown
the ability to learn granular representations, complementing the high-level
features in vision-language training. In this work, we present Harmony, a
framework that combines vision-language training with discriminative and
generative self-supervision to learn visual features that can be generalized
across different downstream vision tasks. Our framework is specifically
designed to work on web-scraped data by not relying on negative examples in the
self-supervised learning path and addressing the one-to-one correspondence
issue using soft CLIP targets generated by an EMA model. Moreover, Harmony
optimizes for five different objectives simultaneously, efficiently utilizing
the supervision in each data example, making it even more suited in
data-constrained settings. We comprehensively evaluate Harmony across various
vision downstream tasks and find that it significantly outperforms the baseline
CLIP and outperforms the previously leading joint self- and weakly supervised
methods, SLIP, MaskCLIP, and DetailCLIP.

</details>


### [699] [POPGym Arcade: Parallel Pixelated POMDPs](https://arxiv.org/pdf/2503.01450)
*Zekang Wang, Zhe He, Borong Zhang, Edan Toledo, Steven Morad*

Main category: cs.LG

TL;DR: POPGym Arcade introduces pixel-based environments with shared observation/action spaces, tools for analyzing partial observability, and reveals issues with long-term memory in agents.


<details>
  <summary>Details</summary>
Motivation: To enable counterfactual studies on partial observability and analyze how agents use past information for decision-making.

Method: Developed hardware-accelerated environments with fully/partially observable variants and introduced mathematical tools for policy analysis under partial observability.

Result: Found controlling for partial observability is critical; long-term memory leads to brittle policies and susceptibility to 'poisoning' by old observations.

Conclusion: Highlights challenges in sim-to-real transfer, imitation learning, and offline RL due to partial observability and memory issues.

Abstract: We present the POPGym Arcade, a collection of hardware-accelerated,
pixel-based environments with shared observation and action spaces. Each
environment includes fully and partially observable variants, enabling
counterfactual studies on partial observability. We also introduce mathematical
tools for analyzing policies under partial observability, which reveal how
agents recall past information to make decisions. Our analysis shows (1) that
controlling for partial observability is critical and (2) that agents with
long-term memory learn brittle policies that struggle to generalize. Finally,
we demonstrate that recurrent policies can be "poisoned" by old,
out-of-distribution observations, with implications for sim-to-real transfer,
imitation learning, and offline reinforcement learning.

</details>


### [700] [Navigating Conflicting Views: Harnessing Trust for Learning](https://arxiv.org/pdf/2406.00958)
*Jueqing Lu, Wray Buntine, Yuanyuan Qi, Joanna Dipnall, Belinda Gabbe, Lan Du*

Main category: cs.LG

TL;DR: A trust-based method improves multi-view classification by addressing view reliability conflicts, validated on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Prior work assumes perfect view alignment, but real-world scenarios often involve unreliable or distinct views, necessitating a method to resolve conflicts.

Method: Develops a computational trust-based discounting method within the Evidential Multi-view framework, using a probability-sensitive trust mechanism to assess view reliability.

Result: Evaluated on six datasets, the method improves reliability metrics (Top-1 Accuracy, Fleiss' Kappa, Multi-View Agreement) and shows effective uncertainty indication via AUROC. Scalability is confirmed on a large dataset.

Conclusion: The trust mechanism effectively resolves conflicts, enhancing multi-view classification reliability for real-world applications.

Abstract: Resolving conflicts is critical for improving the reliability of multi-view
classification. While prior work focuses on learning consistent and informative
representations across views, it often assumes perfect alignment and equal
importance of all views, an assumption rarely met in real-world scenarios, as
some views may express distinct information. To address this, we develop a
computational trust-based discounting method that enhances the Evidential
Multi-view framework by accounting for the instance-wise reliability of each
view through a probability-sensitive trust mechanism. We evaluate our method on
six real-world datasets using Top-1 Accuracy, Fleiss' Kappa, and a new metric,
Multi-View Agreement with Ground Truth, to assess prediction reliability. We
also assess the effectiveness of uncertainty in indicating prediction
correctness via AUROC. Additionally, we test the scalability of our method
through end-to-end training on a large-scale dataset. The experimental results
show that computational trust can effectively resolve conflicts, paving the way
for more reliable multi-view classification models in real-world applications.
Codes available at: https://github.com/OverfitFlow/Trust4Conflict

</details>


### [701] [Learning interpretable positional encodings in transformers depends on initialization](https://arxiv.org/pdf/2406.08272)
*Takuya Ito, Luca Cocchi, Tim Klinger, Parikshit Ram, Murray Campbell, Luke Hearne*

Main category: cs.LG

TL;DR: The study explores how initialization of learnable positional encodings (PEs) in transformers affects generalization, especially in tasks with complex positional arrangements like 2D or 3D data.


<details>
  <summary>Details</summary>
Motivation: Prior PE research focused on 1D sequences (e.g., natural language), but many real-world tasks involve complex positional arrangements (e.g., spatial or unknown positions). The study investigates how PE initialization impacts interpretability and generalization.

Method: Three experiments were conducted: 1) 2D relational reasoning, 2) nonlinear stochastic network simulation, and 3) a 3D neuroscience dataset. Interpretability analyses were used to verify PE accuracy.

Result: Small-norm initialization of learned PEs uncovers interpretable PEs mirroring ground truth positions and improves generalization.

Conclusion: Learning identifiable and interpretable PEs is feasible and enhances generalization in tasks with complex positional arrangements.

Abstract: In transformers, the positional encoding (PE) provides essential information
that distinguishes the position and order amongst tokens in a sequence. Most
prior investigations of PE effects on generalization were tailored to 1D input
sequences, such as those presented in natural language, where adjacent tokens
(e.g., words) are highly related. In contrast, many real world tasks involve
datasets with highly non-trivial positional arrangements, such as datasets
organized in multiple spatial dimensions, or datasets for which ground truth
positions are not known. Here we find that the choice of initialization of a
learnable PE greatly influences its ability to learn interpretable PEs that
lead to enhanced generalization. We empirically demonstrate our findings in
three experiments: 1) A 2D relational reasoning task; 2) A nonlinear stochastic
network simulation; 3) A real world 3D neuroscience dataset, applying
interpretability analyses to verify the learning of accurate PEs. Overall, we
find that a learned PE initialized from a small-norm distribution can 1)
uncover interpretable PEs that mirror ground truth positions in multiple
dimensions, and 2) lead to improved generalization. These results illustrate
the feasibility of learning identifiable and interpretable PEs for enhanced
generalization.

</details>


### [702] [G-Adaptivity: optimised graph-based mesh relocation for finite element methods](https://arxiv.org/pdf/2407.04516)
*James Rowbottom, Georg Maierhofer, Teo Deveney, Eike Mueller, Alberto Paganini, Katharina Schratz, Pietro Li, Carola-Bibiane Schnlieb, Chris Budd*

Main category: cs.LG

TL;DR: A novel GNN-based approach for optimal mesh relocation in FEMs outperforms classical and prior ML methods by directly minimizing FE solution error, achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The cost and accuracy of FEMs depend heavily on mesh point selection. Traditional r-adaptivity methods are costly and rely on estimates, while prior ML approaches focus on surrogates. This work aims to directly optimize mesh geometry for better accuracy and efficiency.

Method: The approach trains a GNN to determine mesh point locations by directly minimizing FE solution error from the PDE system Firedrake, replacing classical estimates with a learnable strategy.

Result: The method achieves lower FE solution error and retains the speed-up over classical methods, outperforming both classical and prior ML approaches.

Conclusion: The GNN-based approach provides a rapid, robust, and effective solution for online r-adaptivity, offering higher accuracy and efficiency compared to existing methods.

Abstract: We present a novel, and effective, approach to achieve optimal mesh
relocation in finite element methods (FEMs). The cost and accuracy of FEMs is
critically dependent on the choice of mesh points. Mesh relocation
(r-adaptivity) seeks to optimise the mesh geometry to obtain the best solution
accuracy at given computational budget. Classical r-adaptivity relies on the
solution of a separate nonlinear "meshing" PDE to determine mesh point
locations. This incurs significant cost at remeshing, and relies on estimates
that relate interpolation- and FEM-error. Recent machine learning approaches
have focused on the construction of fast surrogates for such classical methods.
Instead, our new approach trains a graph neural network (GNN) to determine mesh
point locations by directly minimising the FE solution error from the PDE
system Firedrake to achieve higher solution accuracy. Our GNN architecture
closely aligns the mesh solution space to that of classical meshing
methodologies, thus replacing classical estimates for optimality with a
learnable strategy. This allows for rapid and robust training and results in an
extremely efficient and effective GNN approach to online r-adaptivity. Our
method outperforms both classical, and prior ML, approaches to r-adaptive
meshing. In particular, it achieves lower FE solution error, whilst retaining
the significant speed-up over classical methods observed in prior ML work.

</details>


### [703] [TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning](https://arxiv.org/pdf/2503.17195)
*Sheng Wang, Pengan Chen, Jingqi Zhou, Qintong Li, Jingwei Dong, Jiahui Gao, Boyang Xue, Jiyue Jiang, Lingpeng Kong, Chuan Wu*

Main category: cs.LG

TL;DR: TREESYNTH introduces a tree-guided subspace-based data synthesis method to enhance diversity and balance in datasets, outperforming human-crafted and peer methods by 10%.


<details>
  <summary>Details</summary>
Motivation: Acquiring high-quality, diverse datasets is time-consuming and labor-intensive, and current LLM-based synthesis methods suffer from limited diversity and biases.

Method: TREESYNTH uses a spatial partitioning tree to divide the data space into atomic subspaces, synthesizing samples within each to ensure distinctiveness and comprehensiveness.

Result: Experiments show TREESYNTH outperforms human-crafted and peer methods by 10%, with improved diversity, model performance, and scalability.

Conclusion: TREESYNTH effectively addresses diversity and bias issues in data synthesis, offering a scalable solution for dataset enhancement and redistribution.

Abstract: Model customization necessitates high-quality and diverse datasets, but
acquiring such data remains time-consuming and labor-intensive. Despite the
great potential of large language models (LLMs) for data synthesis, current
approaches are constrained by limited seed data, model biases, and
low-variation prompts, resulting in limited diversity and biased distributions
with the increase of data scales. To tackle this challenge, we introduce
TREESYNTH, a tree-guided subspace-based data synthesis approach inspired by
decision trees. It constructs a spatial partitioning tree to recursively divide
a task-specific full data space (i.e., root node) into numerous atomic
subspaces (i.e., leaf nodes) with mutually exclusive and exhaustive attributes
to ensure both distinctiveness and comprehensiveness before synthesizing
samples within each atomic subspace. This globally dividing-and-synthesizing
method finally collects subspace samples into a comprehensive dataset,
effectively circumventing repetition and space collapse to ensure the diversity
of large-scale data synthesis. Furthermore, the spatial partitioning tree
enables sample allocation into atomic subspaces, allowing the rebalancing of
existing datasets for more balanced and comprehensive distributions.
Empirically, extensive experiments across diverse benchmarks consistently
demonstrate the superior data diversity, model performance, and robust
scalability of TREESYNTH compared to both human-crafted datasets and peer data
synthesis methods, with an average performance gain reaching 10%. Besides, the
consistent improvements of TREESYNTH-balanced datasets highlight its
efficacious application to redistribute existing datasets for more
comprehensive coverage and the induced performance enhancement. The code is
available at https://github.com/cpa2001/TreeSynth.

</details>


### [704] [Validating Mechanistic Interpretations: An Axiomatic Approach](https://arxiv.org/pdf/2407.13594)
*Nils Palumbo, Ravi Mangal, Zifan Wang, Saranya Vijayakumar, Corina S. Pasareanu, Somesh Jha*

Main category: cs.LG

TL;DR: The paper formalizes mechanistic interpretability of neural networks using axioms inspired by abstract interpretation, validating them on existing and new case studies.


<details>
  <summary>Details</summary>
Motivation: To address the ad-hoc nature of mechanistic interpretations of neural networks by providing a formal framework.

Method: Proposes axioms inspired by abstract interpretation to characterize mechanistic interpretations, validated on existing and new case studies (e.g., a Transformer solving 2-SAT).

Result: Demonstrates the applicability of the axioms in validating mechanistic interpretations.

Conclusion: The formal framework provides a rigorous basis for mechanistic interpretability, enhancing its reliability and applicability.

Abstract: Mechanistic interpretability aims to reverse engineer the computation
performed by a neural network in terms of its internal components. Although
there is a growing body of research on mechanistic interpretation of neural
networks, the notion of a mechanistic interpretation itself is often ad-hoc.
Inspired by the notion of abstract interpretation from the program analysis
literature that aims to develop approximate semantics for programs, we give a
set of axioms that formally characterize a mechanistic interpretation as a
description that approximately captures the semantics of the neural network
under analysis in a compositional manner. We demonstrate the applicability of
these axioms for validating mechanistic interpretations on an existing,
well-known interpretability study as well as on a new case study involving a
Transformer-based model trained to solve the well-known 2-SAT problem.

</details>


### [705] [How to Train Your Multi-Exit Model? Analyzing the Impact of Training Strategies](https://arxiv.org/pdf/2407.14320)
*Piotr Kubaty, Bartosz Wjcik, Bartomiej Krzepkowski, Monika Michaluk, Tomasz Trzciski, Jary Pomponi, Kamil Adamczewski*

Main category: cs.LG

TL;DR: The paper analyzes early-exit training strategies, proposing a mixed approach to improve performance and efficiency over conventional joint or disjoint methods.


<details>
  <summary>Details</summary>
Motivation: Existing early-exit methods lack justification for choosing joint or disjoint training, and their impact on performance is unexplored.

Method: Introduces metrics to analyze training dynamics and proposes a mixed strategy: backbone training first, followed by multi-exit network training.

Result: The mixed strategy consistently outperforms joint and disjoint methods in performance and efficiency.

Conclusion: The mixed training strategy is superior, offering a better balance between training dynamics and performance for early-exit networks.

Abstract: Early exits enable the network's forward pass to terminate early by attaching
trainable internal classifiers to the backbone network. Existing early-exit
methods typically adopt either a joint training approach, where the backbone
and exit heads are trained simultaneously, or a disjoint approach, where the
heads are trained separately. However, the implications of this choice are
often overlooked, with studies typically adopting one approach without adequate
justification. This choice influences training dynamics and its impact remains
largely unexplored. In this paper, we introduce a set of metrics to analyze
early-exit training dynamics and guide the choice of training strategy. We
demonstrate that conventionally used joint and disjoint regimes yield
suboptimal performance. To address these limitations, we propose a mixed
training strategy: the backbone is trained first, followed by the training of
the entire multi-exit network. Through comprehensive evaluations of training
strategies across various architectures, datasets, and early-exit methods, we
present the strengths and weaknesses of the early exit training strategies. In
particular, we show consistent improvements in performance and efficiency using
the proposed mixed strategy.

</details>


### [706] [Simple and Critical Iterative Denoising: A Recasting of Discrete Diffusion in Graph Generation](https://arxiv.org/pdf/2503.21592)
*Yoann Boget*

Main category: cs.LG

TL;DR: Proposes Simple Iterative Denoising to address compounding errors in discrete diffusion models for graph generation, enhanced by a Critic for selective element retention.


<details>
  <summary>Details</summary>
Motivation: Compounding denoising errors in discrete diffusion models hinder performance in generative tasks like graph generation.

Method: Introduces Simple Iterative Denoising with conditional independence assumptions and a Critic for element-wise evaluation.

Result: Outperforms existing discrete diffusion baselines in graph generation tasks.

Conclusion: The framework effectively mitigates error accumulation and improves generative modeling for discrete structures.

Abstract: Discrete Diffusion and Flow Matching models have significantly advanced
generative modeling for discrete structures, including graphs. However, the
dependencies between intermediate noisy states lead to error accumulation and
propagation during the reverse denoising process - a phenomenon known as
compounding denoising errors. To address this problem, we propose a novel
framework called Simple Iterative Denoising, which simplifies discrete
diffusion and circumvents the issue by assuming conditional independence
between intermediate states. Additionally, we enhance our model by
incorporating a Critic. During generation, the Critic selectively retains or
corrupts elements in an instance based on their likelihood under the data
distribution. Our empirical evaluations demonstrate that the proposed method
significantly outperforms existing discrete diffusion baselines in graph
generation tasks.

</details>


### [707] [Regularized Neural Ensemblers](https://arxiv.org/pdf/2410.04520)
*Sebastian Pineda Arango, Maciej Janowski, Lennart Purucker, Arber Zela, Frank Hutter, Josif Grabocka*

Main category: cs.LG

TL;DR: Regularized neural networks improve ensemble methods by dynamically weighting predictions and ensuring diversity, outperforming traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Standard ensemble methods assume constant weights, limiting expressiveness and performance. Dynamic ensembling and diversity are crucial for better results.

Method: Proposes regularizing the ensembling model by randomly dropping base model predictions during training to ensure diversity and reduce overfitting.

Result: Regularized neural ensemblers achieve competitive performance across computer vision, NLP, and tabular data.

Conclusion: Dynamic ensembling with regularization enhances diversity and generalization, making it a robust alternative to traditional methods.

Abstract: Ensemble methods are known for enhancing the accuracy and robustness of
machine learning models by combining multiple base learners. However, standard
approaches like greedy or random ensembling often fall short, as they assume a
constant weight across samples for the ensemble members. This can limit
expressiveness and hinder performance when aggregating the ensemble
predictions. In this study, we explore employing regularized neural networks as
ensemble methods, emphasizing the significance of dynamic ensembling to
leverage diverse model predictions adaptively. Motivated by the risk of
learning low-diversity ensembles, we propose regularizing the ensembling model
by randomly dropping base model predictions during the training. We demonstrate
this approach provides lower bounds for the diversity within the ensemble,
reducing overfitting and improving generalization capabilities. Our experiments
showcase that the regularized neural ensemblers yield competitive results
compared to strong baselines across several modalities such as computer vision,
natural language processing, and tabular data.

</details>


### [708] [Active Fine-Tuning of Multi-Task Policies](https://arxiv.org/pdf/2410.05026)
*Marco Bagatella, Jonas Hbotter, Georg Martius, Andreas Krause*

Main category: cs.LG

TL;DR: AMF (Active Multi-task Fine-tuning) is an algorithm that adaptively selects tasks for demonstrations to maximize multi-task policy performance under limited budgets, showing effectiveness in complex environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently fine-tuning pre-trained generalist policies for multiple tasks under limited demonstration budgets.

Method: Proposes AMF, which adaptively selects tasks for demonstrations based on information gain to optimize multi-task policy performance.

Result: Demonstrates empirical effectiveness in fine-tuning neural policies in complex, high-dimensional environments.

Conclusion: AMF efficiently maximizes multi-task policy performance by strategically allocating demonstration resources.

Abstract: Pre-trained generalist policies are rapidly gaining relevance in robot
learning due to their promise of fast adaptation to novel, in-domain tasks.
This adaptation often relies on collecting new demonstrations for a specific
task of interest and applying imitation learning algorithms, such as behavioral
cloning. However, as soon as several tasks need to be learned, we must decide
which tasks should be demonstrated and how often? We study this multi-task
problem and explore an interactive framework in which the agent adaptively
selects the tasks to be demonstrated. We propose AMF (Active Multi-task
Fine-tuning), an algorithm to maximize multi-task policy performance under a
limited demonstration budget by collecting demonstrations yielding the largest
information gain on the expert policy. We derive performance guarantees for AMF
under regularity assumptions and demonstrate its empirical effectiveness to
efficiently fine-tune neural policies in complex and high-dimensional
environments.

</details>


### [709] [Fed-pilot: Optimizing LoRA Allocation for Efficient Federated Fine-Tuning with Heterogeneous Clients](https://arxiv.org/pdf/2410.10200)
*Zikai Zhang, Rui Hu, Ping Liu, Jiahao Xu*

Main category: cs.LG

TL;DR: Fed-pilot is a memory-efficient federated fine-tuning framework for foundation models, addressing client memory heterogeneity by optimizing LoRA module selection and employing dynamic aggregation to handle inconsistencies.


<details>
  <summary>Details</summary>
Motivation: The scalability of federated learning for fine-tuning foundation models is limited by client memory heterogeneity.

Method: Fed-pilot uses Low-Rank Adaptation (LoRA)-based fine-tuning, optimizing module selection via knapsack problem-solving and employing dynamic aggregation for consistency.

Result: Extensive experiments show Fed-pilot's effectiveness and efficiency across diverse datasets and heterogeneous settings.

Conclusion: Fed-pilot is the first federated fine-tuning framework integrating memory-constrained optimization, with promising results.

Abstract: Federated Learning enables the fine-tuning of foundation models (FMs) across
distributed clients for specific tasks; however, its scalability is limited by
the heterogeneity of client memory capacities. In this work, we propose
Fed-pilot, a memory-efficient federated fine-tuning framework. It enables
memory-constrained clients to participate in Low-Rank Adaptation (LoRA)-based
fine-tuning by training only a subset of LoRA modules locally. Fed-pilot
identifies the optimal selection of trainable LoRA modules as a knapsack
optimization problem, maximizing model performance under memory constraints for
each client. To mitigate inconsistencies arising from heterogeneous module
allocations and Non-IID data, Fed-pilot employs a novel aggregation rule that
dynamically compensates for under-updated layers. Extensive experiments on five
diverse datasets across various heterogeneous data settings demonstrate
Fed-pilot's effectiveness and efficiency compared to state-of-the-art methods.
To the best of our knowledge, this is the first study on federated fine-tuning
of FMs that integrates memory-constrained optimization. The code will be
publicly available.

</details>


### [710] [Holistic Physics Solver: Learning PDEs in a Unified Spectral-Physical Space](https://arxiv.org/pdf/2410.11382)
*Xihang Yue, Yi Yang, Linchao Zhu*

Main category: cs.LG

TL;DR: HPM bridges spectral and attention-based PDE solvers, combining their strengths for better accuracy, efficiency, and generalization.


<details>
  <summary>Details</summary>
Motivation: The gap between spectral and attention-based PDE solvers limits flexibility and generalization. HPM aims to unify these approaches.

Method: HPM integrates spectral and physical information in a unified space, enabling spectral-physical interactions beyond existing methods.

Result: HPM outperforms state-of-the-art methods in accuracy, efficiency, and generalization, even with limited data or unseen resolutions.

Conclusion: HPM successfully unifies spectral and attention-based methods, offering superior performance and flexibility for PDE solving.

Abstract: Recent advances in operator learning have produced two distinct approaches
for solving partial differential equations (PDEs): attention-based methods
offering point-level adaptability but lacking spectral constraints, and
spectral-based methods providing domain-level continuity priors but limited in
local flexibility. This dichotomy has hindered the development of PDE solvers
with both strong flexibility and generalization capability. This work
introduces Holistic Physics Mixer (HPM), a simple framework that bridges this
gap by integrating spectral and physical information in a unified space. HPM
unifies both approaches as special cases while enabling more powerful
spectral-physical interactions beyond either method alone. This enables HPM to
inherit both the strong generalization of spectral methods and the flexibility
of attention mechanisms while avoiding their respective limitations. Through
extensive experiments across diverse PDE problems, we demonstrate that HPM
consistently outperforms state-of-the-art methods in both accuracy and
computational efficiency, while maintaining strong generalization capabilities
with limited training data and excellent zero-shot performance on unseen
resolutions.

</details>


### [711] [One Step Diffusion via Shortcut Models](https://arxiv.org/pdf/2410.12557)
*Kevin Frans, Danijar Hafner, Sergey Levine, Pieter Abbeel*

Main category: cs.LG

TL;DR: Shortcut models simplify generative sampling by using a single network and training phase, enabling high-quality samples in fewer steps compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion and flow-matching models are slow and complex due to iterative denoising and multi-phase training. Shortcut models aim to streamline this process.

Method: Shortcut models condition the network on both noise level and desired step size, allowing skipping steps during generation.

Result: Shortcut models outperform consistency models and reflow in sample quality across varying step budgets and simplify training compared to distillation.

Conclusion: Shortcut models offer a simpler, more efficient alternative for high-quality generative sampling without complex training regimes.

Abstract: Diffusion models and flow-matching models have enabled generating diverse and
realistic images by learning to transfer noise to data. However, sampling from
these models involves iterative denoising over many neural network passes,
making generation slow and expensive. Previous approaches for speeding up
sampling require complex training regimes, such as multiple training phases,
multiple networks, or fragile scheduling. We introduce shortcut models, a
family of generative models that use a single network and training phase to
produce high-quality samples in a single or multiple sampling steps. Shortcut
models condition the network not only on the current noise level but also on
the desired step size, allowing the model to skip ahead in the generation
process. Across a wide range of sampling step budgets, shortcut models
consistently produce higher quality samples than previous approaches, such as
consistency models and reflow. Compared to distillation, shortcut models reduce
complexity to a single network and training phase and additionally allow
varying step budgets at inference time.

</details>


### [712] [FedBaF: Federated Learning Aggregation Biased by a Foundation Model](https://arxiv.org/pdf/2410.18352)
*Jong-Ik Park, Srinivasa Pranav, Jos M. F. Moura, Carlee Joe-Wong*

Main category: cs.LG

TL;DR: FedBaF is a novel FL method that integrates pre-trained foundation model weights during aggregation, preserving model confidentiality while improving accuracy in IID and non-IID settings.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods compromise model security by disclosing foundation model weights. FedBaF aims to leverage foundation models without exposing their weights.

Method: FedBaF dynamically integrates pre-trained foundation model weights during FL aggregation, avoiding direct weight disclosure.

Result: FedBaF outperforms traditional methods by up to 11.4% (IID) and 15.8% (non-IID) in accuracy, and reduces perplexity by 39.2% in language tasks.

Conclusion: FedBaF effectively balances privacy and performance, making it a promising approach for secure FL with foundation models.

Abstract: Foundation models are now a major focus of leading technology organizations
due to their ability to generalize across diverse tasks. Existing approaches
for adapting foundation models to new applications often rely on Federated
Learning (FL) and disclose the foundation model weights to clients when using
it to initialize the global model. While these methods ensure client data
privacy, they compromise model and information security. In this paper, we
introduce Federated Learning Aggregation Biased by a Foundation Model (FedBaF),
a novel method for dynamically integrating pre-trained foundation model weights
during the FL aggregation phase. Unlike conventional methods, FedBaF preserves
the confidentiality of the foundation model while still leveraging its power to
train more accurate models, especially in non-IID and adversarial scenarios.
Our comprehensive experiments use Pre-ResNet and foundation models like Vision
Transformer to demonstrate that FedBaF not only matches, but often surpasses
the test accuracy of traditional weight initialization methods by up to 11.4%
in IID and up to 15.8% in non-IID settings. Additionally, FedBaF applied to a
Transformer-based language model significantly reduced perplexity by up to
39.2%.

</details>


### [713] [AutoPDL: Automatic Prompt Optimization for LLM Agents](https://arxiv.org/pdf/2504.04365)
*Claudio Spiess, Mandana Vaziri, Louis Mandel, Martin Hirzel*

Main category: cs.LG

TL;DR: AutoPDL automates LLM prompt tuning by framing it as an AutoML problem, using successive halving to efficiently explore combinatorial spaces. It achieves significant accuracy gains across tasks and models.


<details>
  <summary>Details</summary>
Motivation: Manual prompt tuning for LLMs is tedious, error-prone, and model/task-specific, necessitating an automated solution.

Method: AutoPDL treats prompt tuning as a structured AutoML problem, using successive halving to navigate combinatorial spaces of prompting patterns and demonstrations. It leverages PDL for human-readable, editable, and executable programs.

Result: Evaluations show consistent accuracy gains (up to 68.9pp) across tasks and models, with selected strategies varying by model and task.

Conclusion: AutoPDL effectively automates prompt tuning, offering human-readable solutions and significant performance improvements.

Abstract: The performance of large language models (LLMs) depends on how they are
prompted, with choices spanning both the high-level prompting pattern (e.g.,
Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and
few-shot demonstrations). Manually tuning this combination is tedious,
error-prone, and specific to a given LLM and task. Therefore, this paper
proposes AutoPDL, an automated approach to discovering good LLM agent
configurations. Our approach frames this as a structured AutoML problem over a
combinatorial space of agentic and non-agentic prompting patterns and
demonstrations, using successive halving to efficiently navigate this space. We
introduce a library implementing common prompting patterns using the PDL prompt
programming language. AutoPDL solutions are human-readable, editable, and
executable PDL programs that use this library. This approach also enables
source-to-source optimization, allowing human-in-the-loop refinement and reuse.
Evaluations across three tasks and seven LLMs (ranging from 3B to 70B
parameters) show consistent accuracy gains ($9.06\pm15.3$ percentage points),
up to 68.9pp, and reveal that selected prompting strategies vary across models
and tasks.

</details>


### [714] [Enhancing Glucose Level Prediction of ICU Patients through Hierarchical Modeling of Irregular Time-Series](https://arxiv.org/pdf/2411.01418)
*Hadi Mehdizavareh, Arijit Khan, Simon Lebech Cichosz*

Main category: cs.LG

TL;DR: MITST, a machine learning framework, improves BG level prediction in ICU patients by integrating diverse clinical data and outperforms existing methods in accuracy and sensitivity.


<details>
  <summary>Details</summary>
Motivation: Accurate BG level prediction is crucial for ICU patients due to the risks of hypoglycemia and hyperglycemia. Existing methods lack comprehensive data integration and manual feature engineering.

Method: MITST uses a hierarchical Transformer to analyze multi-source, irregular time-series data (e.g., lab results, medications, vital signs) without predefined aggregation.

Result: MITST outperforms a random forest baseline (1.7 pp AUROC, 1.8 pp AUPRC improvement) and increases hypoglycemia sensitivity by 7.2 pp. It also generalizes to other tasks like mortality prediction.

Conclusion: MITST is a robust, adaptable solution for ICU data analysis, with potential for broader clinical applications.

Abstract: Accurately predicting blood glucose (BG) levels of ICU patients is critical,
as both hypoglycemia (BG < 70 mg/dL) and hyperglycemia (BG > 180 mg/dL) are
associated with increased morbidity and mortality. This study presents a
proof-of-concept machine learning framework, the Multi-source Irregular
Time-Series Transformer (MITST), designed to predict BG levels in ICU patients.
In contrast to existing methods that rely heavily on manual feature engineering
or utilize limited Electronic Health Record (EHR) data sources, MITST
integrates diverse clinical data--including laboratory results, medications,
and vital signs without predefined aggregation. The model leverages a
hierarchical Transformer architecture, designed to capture interactions among
features within individual timestamps, temporal dependencies across different
timestamps, and semantic relationships across multiple data sources. Evaluated
using the extensive eICU database (200,859 ICU stays across 208 hospitals),
MITST achieves a statistically significant ( p < 0.001 ) average improvement of
1.7 percentage points (pp) in AUROC and 1.8 pp in AUPRC over a state-of-the-art
random forest baseline. Crucially, for hypoglycemia--a rare but
life-threatening condition--MITST increases sensitivity by 7.2 pp, potentially
enabling hundreds of earlier interventions across ICU populations. The flexible
architecture of MITST allows seamless integration of new data sources without
retraining the entire model, enhancing its adaptability for clinical decision
support. While this study focuses on predicting BG levels, we also demonstrate
MITST's ability to generalize to a distinct clinical task (in-hospital
mortality prediction), highlighting its potential for broader applicability in
ICU settings. MITST thus offers a robust and extensible solution for analyzing
complex, multi-source, irregular time-series data.

</details>


### [715] [Zero-Shot NAS via the Suppression of Local Entropy Decrease](https://arxiv.org/pdf/2411.06236)
*Ning Wu, Han Huang, Yueting Xu, Zhifeng Hao*

Main category: cs.LG

TL;DR: The paper introduces a data-free and running-free proxy called SED (Suppression of Local Entropy Decrease) for Zero-Shot NAS, outperforming existing proxies in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To accelerate neural architecture search (NAS) by reducing the computational overhead of existing zero-cost proxies, which rely on backpropagation or input data.

Method: Uses architectural topologies to quantify SED, proving that certain topologies degrade network performance by decreasing local entropy of feature maps.

Result: SED outperforms state-of-the-art proxies in architecture selection, reducing computation time by three orders of magnitude and achieving higher accuracy with fewer parameters.

Conclusion: SED-based NAS efficiently selects optimal architectures, validated by theoretical and experimental results.

Abstract: Architecture performance evaluation is the most time-consuming part of neural
architecture search (NAS). Zero-Shot NAS accelerates the evaluation by
utilizing zero-cost proxies instead of training. Though effective, existing
zero-cost proxies require invoking backpropagations or running networks on
input data, making it difficult to further accelerate the computation of
proxies. To alleviate this issue, architecture topologies are used to evaluate
the performance of networks in this study. We prove that particular
architectural topologies decrease the local entropy of feature maps, which
degrades specific features to a bias, thereby reducing network performance.
Based on this proof, architectural topologies are utilized to quantify the
suppression of local entropy decrease (SED) as a data-free and running-free
proxy. Experimental results show that SED outperforms most state-of-the-art
proxies in terms of architecture selection on five benchmarks, with computation
time reduced by three orders of magnitude. We further compare the SED-based NAS
with state-of-the-art proxies. SED-based NAS selects the architecture with
higher accuracy and fewer parameters in only one second. The theoretical
analyses of local entropy and experimental results demonstrate that the
suppression of local entropy decrease facilitates selecting optimal
architectures in Zero-Shot NAS.

</details>


### [716] [Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU](https://arxiv.org/pdf/2504.17028)
*Iman Khadir, Shane Stevenson, Henry Li, Kyle Krick, Abram Burrows, David Hall, Stan Posey, Samuel S. P. Shen*

Main category: cs.LG

TL;DR: The paper explores democratizing AI-driven weather forecasting for university research groups using NVIDIA's FourCastNetv2 and GPUs, addressing challenges like limited resources and training efficiency.


<details>
  <summary>Details</summary>
Motivation: To make AI-driven weather forecasting accessible to university research groups with limited GPU resources, leveraging freely available models like FourCastNetv2.

Method: Utilizes NVIDIA's FourCastNetv2 via API for predictions and trains the original FourCastNet model using NVIDIA hardware, focusing on data management and training efficiency.

Result: Demonstrates the capabilities and limitations of NVIDIA A100 GPUs for resource-limited groups, providing insights into model validation and training challenges.

Conclusion: The paper serves as a guide for universities to develop AI weather forecasting programs, promoting democratization of AI in numerical weather prediction.

Abstract: This paper demonstrates the feasibility of democratizing AI-driven global
weather forecasting models among university research groups by leveraging
Graphics Processing Units (GPUs) and freely available AI models, such as
NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network
for weather prediction and is trained on a 73-channel subset of the European
Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset
at single levels and different pressure levels. Although the training
specifications for FourCastNetv2 are not released to the public, the training
documentation of the model's first generation, FourCastNet, is available to all
users. The training had 64 A100 GPUs and took 16 hours to complete. Although
NVIDIA's models offer significant reductions in both time and cost compared to
traditional Numerical Weather Prediction (NWP), reproducing published
forecasting results presents ongoing challenges for resource-constrained
university research groups with limited GPU availability. We demonstrate both
(i) leveraging FourCastNetv2 to create predictions through the designated
application programming interface (API) and (ii) utilizing NVIDIA hardware to
train the original FourCastNet model. Further, this paper demonstrates the
capabilities and limitations of NVIDIA A100's for resource-limited research
groups in universities. We also explore data management, training efficiency,
and model validation, highlighting the advantages and challenges of using
limited high-performance computing resources. Consequently, this paper and its
corresponding GitHub materials may serve as an initial guide for other
university research groups and courses related to machine learning, climate
science, and data science to develop research and education programs on AI
weather forecasting, and hence help democratize the AI NWP in the digital
economy.

</details>


### [717] [Graph Neural Networks in Supply Chain Analytics and Optimization: Concepts, Perspectives, Dataset and Benchmarks](https://arxiv.org/pdf/2411.08550)
*Azmine Toushik Wasi, MD Shafikul Islam, Adipto Raihan Akib, Mahathir Mohammad Bappy*

Main category: cs.LG

TL;DR: GNNs show promise in supply chain management (SCM) due to their graph-like nature, but research is limited. This paper bridges gaps by linking SCM with GNNs, providing formulations, examples, and a real-world dataset. GNNs outperform other models by 10-40% in various tasks.


<details>
  <summary>Details</summary>
Motivation: Supply chains are graph-like, making GNNs suitable, but research is scarce due to lack of foundations, familiarity, and datasets.

Method: The paper connects SCM with GNNs, offers formulations, examples, and a benchmark dataset. It evaluates GNN models on six SCM tasks.

Result: GNNs outperform statistical ML and other DL models by 10-30% in regression/classification and 15-40% in anomaly detection.

Conclusion: This work establishes a foundation for GNN-based SCM solutions with conceptual, methodological, and dataset support.

Abstract: Graph Neural Networks (GNNs) have recently gained traction in transportation,
bioinformatics, language and image processing, but research on their
application to supply chain management remains limited. Supply chains are
inherently graph-like, making them ideal for GNN methodologies, which can
optimize and solve complex problems. The barriers include a lack of proper
conceptual foundations, familiarity with graph applications in SCM, and
real-world benchmark datasets for GNN-based supply chain research. To address
this, we discuss and connect supply chains with graph structures for effective
GNN application, providing detailed formulations, examples, mathematical
definitions, and task guidelines. Additionally, we present a multi-perspective
real-world benchmark dataset from a leading FMCG company in Bangladesh,
focusing on supply chain planning. We discuss various supply chain tasks using
GNNs and benchmark several state-of-the-art models on homogeneous and
heterogeneous graphs across six supply chain analytics tasks. Our analysis
shows that GNN-based models consistently outperform statistical Machine
Learning and other Deep Learning models by around 10-30% in regression, 10-30%
in classification and detection tasks, and 15-40% in anomaly detection tasks on
designated metrics. With this work, we lay the groundwork for solving supply
chain problems using GNNs, supported by conceptual discussions, methodological
insights, and a comprehensive dataset.

</details>


### [718] [Learning to Insert for Constructive Neural Vehicle Routing Solver](https://arxiv.org/pdf/2505.13904)
*Fu Luo, Xi Lin, Mengyuan Zhong, Fei Liu, Zhenkun Wang, Jianyong Sun, Qingfu Zhang*

Main category: cs.LG

TL;DR: L2C-Insert is a novel learning-based method for Neural Combinatorial Optimisation (NCO) that improves solution quality by using an insertion-based paradigm instead of traditional appending methods.


<details>
  <summary>Details</summary>
Motivation: Existing constructive NCO methods follow rigid appending-based approaches, leading to suboptimal results. The paper explores the insertion-based paradigm to enhance flexibility and solution quality.

Method: L2C-Insert introduces a model architecture for insertion position prediction, an efficient training scheme, and an advanced inference technique.

Result: Experiments on TSP and CVRP show L2C-Insert outperforms traditional methods across various problem sizes.

Conclusion: The insertion-based paradigm in L2C-Insert significantly improves flexibility and solution quality in NCO for VRPs.

Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based
approach for solving Vehicle Routing Problems (VRPs) without extensive manual
design. While existing constructive NCO methods typically follow an
appending-based paradigm that sequentially adds unvisited nodes to partial
solutions, this rigid approach often leads to suboptimal results. To overcome
this limitation, we explore the idea of insertion-based paradigm and propose
Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel
learning-based method for constructive NCO. Unlike traditional approaches,
L2C-Insert builds solutions by strategically inserting unvisited nodes at any
valid position in the current partial solution, which can significantly enhance
the flexibility and solution quality. The proposed framework introduces three
key components: a novel model architecture for precise insertion position
prediction, an efficient training scheme for model optimization, and an
advanced inference technique that fully exploits the insertion paradigm's
flexibility. Extensive experiments on both synthetic and real-world instances
of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior
performance across various problem sizes.

</details>


### [719] [CDI: Copyrighted Data Identification in Diffusion Models](https://arxiv.org/pdf/2411.12858)
*Jan Dubiski, Antoni Kowalczuk, Franziska Boenisch, Adam Dziedzic*

Main category: cs.LG

TL;DR: CDI is a framework to detect if a dataset was used to train Diffusion Models (DMs), addressing copyright concerns by leveraging dataset inference techniques and statistical testing.


<details>
  <summary>Details</summary>
Motivation: Concerns about copyright and intellectual property violations due to unauthorized use of data in training DMs, especially when outputs are not direct replicas.

Method: CDI aggregates signals from membership inference attacks (MIAs) and uses handcrafted methods to extract features, feeding them to a scoring model with statistical testing.

Result: CDI can identify dataset usage in DMs with 99% confidence using as few as 70 data points.

Conclusion: CDI provides a reliable tool for data owners to detect illegitimate use of their copyrighted data in DMs.

Abstract: Diffusion Models (DMs) benefit from large and diverse datasets for their
training. Since this data is often scraped from the Internet without permission
from the data owners, this raises concerns about copyright and intellectual
property protections. While (illicit) use of data is easily detected for
training samples perfectly re-created by a DM at inference time, it is much
harder for data owners to verify if their data was used for training when the
outputs from the suspect DM are not close replicas. Conceptually, membership
inference attacks (MIAs), which detect if a given data point was used during
training, present themselves as a suitable tool to address this challenge.
However, we demonstrate that existing MIAs are not strong enough to reliably
determine the membership of individual images in large, state-of-the-art DMs.
To overcome this limitation, we propose CDI, a framework for data owners to
identify whether their dataset was used to train a given DM. CDI relies on
dataset inference techniques, i.e., instead of using the membership signal from
a single data point, CDI leverages the fact that most data owners, such as
providers of stock photography, visual media companies, or even individual
artists, own datasets with multiple publicly exposed data points which might
all be included in the training of a given DM. By selectively aggregating
signals from existing MIAs and using new handcrafted methods to extract
features for these datasets, feeding them to a scoring model, and applying
rigorous statistical testing, CDI allows data owners with as little as 70 data
points to identify with a confidence of more than 99% whether their data was
used to train a given DM. Thereby, CDI represents a valuable tool for data
owners to claim illegitimate use of their copyrighted data. We make the code
available at https://github.com/sprintml/copyrighted_data_identification

</details>


### [720] [Maximizing Confidence Alone Improves Reasoning](https://arxiv.org/pdf/2505.22660)
*Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, Deepak Pathak*

Main category: cs.LG

TL;DR: RENT is an unsupervised RL method using entropy minimization as intrinsic reward, improving reasoning without external rewards.


<details>
  <summary>Details</summary>
Motivation: Reward engineering in RL is challenging; RENT avoids external rewards by using model entropy.

Method: Uses entropy of the model's distribution as intrinsic reward to reinforce high-confidence reasoning.

Result: Improves reasoning on benchmarks like GSM8K, MATH500, AMC, AIME, and GPQA across Qwen and Mistral models.

Conclusion: RENT's unsupervised approach is broadly applicable where external supervision is lacking.

Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and
Mistral families. The generality of our unsupervised learning method lends
itself to applicability in a wide range of domains where external supervision
is unavailable.

</details>


### [721] [Recursive Gaussian Process State Space Model](https://arxiv.org/pdf/2411.14679)
*Tengjie Zheng, Haipeng Chen, Lin Cheng, Shengping Gong, Xu Huang*

Main category: cs.LG

TL;DR: The paper proposes a recursive Gaussian Process State-Space Model (GPSSM) method for online learning, addressing limitations in prior information. It includes adaptive domain and hyperparameter capabilities, lightweight learning via inducing points, and efficient hyperparameter optimization.


<details>
  <summary>Details</summary>
Motivation: The need for efficient online learning methods in GPSSMs, especially when prior data distribution and model function information are limited.

Method: 1. Bayesian update for joint state-GP distribution using first-order linearization. 2. Online inducing point selection for lightweight learning. 3. Historical measurement recovery for hyperparameter optimization.

Result: The method shows superior accuracy, computational efficiency, and adaptability on synthetic and real-world datasets.

Conclusion: The proposed recursive GPSSM method effectively addresses online learning challenges in GPSSMs, outperforming existing techniques.

Abstract: Learning dynamical models from data is not only fundamental but also holds
great promise for advancing principle discovery, time-series prediction, and
controller design. Among various approaches, Gaussian Process State-Space
Models (GPSSMs) have recently gained significant attention due to their
combination of flexibility and interpretability. However, for online learning,
the field lacks an efficient method suitable for scenarios where prior
information regarding data distribution and model function is limited. To
address this issue, this paper proposes a recursive GPSSM method with adaptive
capabilities for both operating domains and Gaussian process (GP)
hyperparameters. Specifically, we first utilize first-order linearization to
derive a Bayesian update equation for the joint distribution between the system
state and the GP model, enabling closed-form and domain-independent learning.
Second, an online selection algorithm for inducing points is developed based on
informative criteria to achieve lightweight learning. Third, to support online
hyperparameter optimization, we recover historical measurement information from
the current filtering distribution. Comprehensive evaluations on both synthetic
and real-world datasets demonstrate the superior accuracy, computational
efficiency, and adaptability of our method compared to state-of-the-art online
GPSSM techniques.

</details>


### [722] [Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning](https://arxiv.org/pdf/2506.00691)
*Junaid Muzaffar, Khubaib Ahmed, Ingo Frommholz, Zeeshan Pervez, Ahsan ul Haq*

Main category: cs.LG

TL;DR: Proposes a Nonlinear Attention (NLA) mechanism for faster RL training without performance loss.


<details>
  <summary>Details</summary>
Motivation: Addresses the high computational cost and slow training in RL by enhancing attention mechanisms.

Method: Introduces a nonlinear transformation to key vectors in attention, creating enriched representations (K').

Result: Achieves faster convergence and improved efficiency while matching baseline performance.

Conclusion: NLA mechanisms can accelerate RL training effectively.

Abstract: Training reinforcement learning (RL) agents often requires significant
computational resources and prolonged training durations. To address this
challenge, we build upon prior work that introduced a neural architecture with
permutation-invariant sensory processing. We propose a modified attention
mechanism that applies a non-linear transformation to the key vectors (K),
producing enriched representations (K') through a custom mapping function. This
Nonlinear Attention (NLA) mechanism enhances the representational capacity of
the attention layer, enabling the agent to learn more expressive feature
interactions. As a result, our model achieves significantly faster convergence
and improved training efficiency, while maintaining performance on par with the
baseline. These results highlight the potential of nonlinear attention
mechanisms to accelerate reinforcement learning without sacrificing
effectiveness.

</details>


### [723] [Evaluating Rank-N-Contrast: Continuous and Robust Representations for Regression](https://arxiv.org/pdf/2411.16298)
*Valentin Six, Alexandre Chidiac, Arkin Worlikar*

Main category: cs.LG

TL;DR: The paper evaluates the Rank-N-Contrast (RNC) framework, confirming its effectiveness in improving deep regression models by learning continuous representations through ranking-based contrast.


<details>
  <summary>Details</summary>
Motivation: Deep regression models often struggle with fragmented representations due to ignoring sample order continuity. The study aims to validate and extend RNC's benefits.

Method: The study reproduces RNC, evaluates it on an additional dataset, and tests robustness using a holdout method to exclude specific data ranges during training.

Result: RNC demonstrates improved performance and robustness, generalizing well to unseen data and achieving state-of-the-art results.

Conclusion: The replication confirms RNC's effectiveness and expands its applicability, validating its theoretical and empirical advantages.

Abstract: This document is an evaluation of the original "Rank-N-Contrast"
(arXiv:2210.01189v2) paper published in 2023. This evaluation is done for
academic purposes. Deep regression models often fail to capture the continuous
nature of sample orders, creating fragmented representations and suboptimal
performance. To address this, we reproduced the Rank-N-Contrast (RNC)
framework, which learns continuous representations by contrasting samples by
their rankings in the target space. Our study validates RNC's theoretical and
empirical benefits, including improved performance and robustness. We extended
the evaluation to an additional regression dataset and conducted robustness
tests using a holdout method, where a specific range of continuous data was
excluded from the training set. This approach assessed the model's ability to
generalize to unseen data and achieve state-of-the-art performance. This
replication study validates the original findings and broadens the
understanding of RNC's applicability and robustness.

</details>


### [724] [Stealing That Free Lunch: Exposing the Limits of Dyna-Style Reinforcement Learning](https://arxiv.org/pdf/2412.14312)
*Brett Barkley, David Fridovich-Keil*

Main category: cs.LG

TL;DR: DMBRL algorithms improve sample efficiency in off-policy RL but perform inconsistently across benchmarks like OpenAI Gym and DeepMind Control Suite, often degrading performance in DMC.


<details>
  <summary>Details</summary>
Motivation: To investigate the performance gap of DMBRL algorithms across similar benchmark environments with proprioceptive observations.

Method: Analyzed DMBRL algorithms in OpenAI Gym and DeepMind Control Suite, comparing performance and testing modern techniques.

Result: DMBRL algorithms perform well in OpenAI Gym but degrade performance in DMC, with no consistent improvement from modern techniques.

Conclusion: Model-based RL faces fundamental challenges, and performance varies across benchmarks, highlighting no universal solution.

Abstract: Dyna-style off-policy model-based reinforcement learning (DMBRL) algorithms
are a family of techniques for generating synthetic state transition data and
thereby enhancing the sample efficiency of off-policy RL algorithms. This paper
identifies and investigates a surprising performance gap observed when applying
DMBRL algorithms across different benchmark environments with proprioceptive
observations. We show that, while DMBRL algorithms perform well in OpenAI Gym,
their performance can drop significantly in DeepMind Control Suite (DMC), even
though these settings offer similar tasks and identical physics backends.
Modern techniques designed to address several key issues that arise in these
settings do not provide a consistent improvement across all environments, and
overall our results show that adding synthetic rollouts to the training process
-- the backbone of Dyna-style algorithms -- significantly degrades performance
across most DMC environments. Our findings contribute to a deeper understanding
of several fundamental challenges in model-based RL and show that, like many
optimization fields, there is no free lunch when evaluating performance across
diverse benchmarks in RL.

</details>


### [725] [Curse of Dimensionality in Neural Network Optimization](https://arxiv.org/pdf/2502.05360)
*Sanghoon Na, Haizhao Yang*

Main category: cs.LG

TL;DR: The paper shows that shallow neural networks with Lipschitz continuous activation functions face a curse of dimensionality when approximating smooth target functions, limiting the decay rate of population risk.


<details>
  <summary>Details</summary>
Motivation: To understand how function smoothness and dimensionality impact the optimization of neural networks, addressing an underexplored area in theory.

Method: Analyzing training dynamics via the evolution of parameter distribution under 2-Wasserstein gradient flow, avoiding direct parameter evolution.

Result: Population risk decays no faster than $t^{-\frac{4r}{d-2r}}$ for Lipschitz activations and $t^{-\frac{(4+2\delta)r}{d-2r}}$ for locally Lipschitz activations.

Conclusion: The curse of dimensionality persists in neural network optimization, with decay rates influenced by activation function properties and target smoothness.

Abstract: This paper demonstrates that when a shallow neural network with a Lipschitz
continuous activation function is trained using either empirical or population
risk to approximate a target function that is $r$ times continuously
differentiable on $[0,1]^d$, the population risk may not decay at a rate faster
than $t^{-\frac{4r}{d-2r}}$, where $t$ is an analog of the total number of
optimization iterations. This result highlights the presence of the curse of
dimensionality in the optimization computation required to achieve a desired
accuracy. Instead of analyzing parameter evolution directly, the training
dynamics are examined through the evolution of the parameter distribution under
the 2-Wasserstein gradient flow. Furthermore, it is established that the curse
of dimensionality persists when a locally Lipschitz continuous activation
function is employed, where the Lipschitz constant in $[-x,x]$ is bounded by
$O(x^\delta)$ for any $x \in \mathbb{R}$. In this scenario, the population risk
is shown to decay at a rate no faster than $t^{-\frac{(4+2\delta)r}{d-2r}}$.
Understanding how function smoothness influences the curse of dimensionality in
neural network optimization theory is an important and underexplored direction
that this work aims to address.

</details>


### [726] [Distributionally Robust Active Learning for Gaussian Process Regression](https://arxiv.org/pdf/2502.16870)
*Shion Takeno, Yoshito Okura, Yu Inatsu, Tatsuya Aoyama, Tomonari Tanaka, Satoshi Akahane, Hiroyuki Hanada, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, Ichiro Takeuchi*

Main category: cs.LG

TL;DR: The paper proposes two active learning methods for Gaussian process regression to minimize worst-case expected error, with theoretical guarantees and empirical validation.


<details>
  <summary>Details</summary>
Motivation: Existing active learning methods for Gaussian process regression lack theoretical guarantees on prediction accuracy for target distributions, which are often hard to specify.

Method: Two active learning methods are introduced to reduce the worst-case expected error for GPR, with an upper bound on the worst-case expected squared error provided.

Result: Theoretical analysis shows the error can be made arbitrarily small with finite data labels under mild conditions, and synthetic/real-world datasets validate the methods.

Conclusion: The proposed methods effectively address the limitations of existing approaches, offering theoretical and practical improvements for active learning in GPR.

Abstract: Gaussian process regression (GPR) or kernel ridge regression is a widely used
and powerful tool for nonlinear prediction. Therefore, active learning (AL) for
GPR, which actively collects data labels to achieve an accurate prediction with
fewer data labels, is an important problem. However, existing AL methods do not
theoretically guarantee prediction accuracy for target distribution.
Furthermore, as discussed in the distributionally robust learning literature,
specifying the target distribution is often difficult. Thus, this paper
proposes two AL methods that effectively reduce the worst-case expected error
for GPR, which is the worst-case expectation in target distribution candidates.
We show an upper bound of the worst-case expected squared error, which suggests
that the error will be arbitrarily small by a finite number of data labels
under mild conditions. Finally, we demonstrate the effectiveness of the
proposed methods through synthetic and real-world datasets.

</details>


### [727] [xInv: Explainable Optimization of Inverse Problems](https://arxiv.org/pdf/2506.11056)
*Sean Memery, Kevin Denamganai, Anna Kapron-King, Kartic Subr*

Main category: cs.LG

TL;DR: A method to make inverse problem optimization interpretable by using natural language events from a differentiable simulator and a language model for explanations.


<details>
  <summary>Details</summary>
Motivation: Inverse problems are critical but their optimization processes are cryptic to domain experts, limiting understanding and trust.

Method: Instrument a differentiable simulator to emit natural language events during forward/backward passes, then use a language model to generate explanations.

Result: Demonstrated effectiveness with an optimization problem and neural network training example.

Conclusion: The approach enhances interpretability of inverse problem optimization for domain experts.

Abstract: Inverse problems are central to a wide range of fields, including healthcare,
climate science, and agriculture. They involve the estimation of inputs,
typically via iterative optimization, to some known forward model so that it
produces a desired outcome. Despite considerable development in the
explainability and interpretability of forward models, the iterative
optimization of inverse problems remains largely cryptic to domain experts. We
propose a methodology to produce explanations, from traces produced by an
optimizer, that are interpretable by humans at the abstraction of the domain.
The central idea in our approach is to instrument a differentiable simulator so
that it emits natural language events during its forward and backward passes.
In a post-process, we use a Language Model to create an explanation from the
list of events. We demonstrate the effectiveness of our approach with an
illustrative optimization problem and an example involving the training of a
neural network.

</details>


### [728] [Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning](https://arxiv.org/pdf/2503.01837)
*Adri Lpez Escoriza, Nicklas Hansen, Stone Tao, Tongzhou Mu, Hao Su*

Main category: cs.LG

TL;DR: DEMO3 is a framework for efficient RL in long-horizon robotic tasks, using multi-stage dense rewards, bi-phasic training, and world models to improve exploration and data-efficiency.


<details>
  <summary>Details</summary>
Motivation: Long-horizon tasks in robotic manipulation are challenging due to sparse rewards and large state-action spaces, but their multi-stage structure can be leveraged.

Method: DEMO3 combines multi-stage dense reward learning, bi-phasic training, and world model learning within a demonstration-augmented RL framework.

Result: Improves data-efficiency by 40% on average and 70% on difficult tasks, validated across 16 sparse-reward tasks.

Conclusion: DEMO3 effectively addresses exploration challenges in long-horizon tasks, demonstrating significant performance gains with minimal demonstrations.

Abstract: Long-horizon tasks in robotic manipulation present significant challenges in
reinforcement learning (RL) due to the difficulty of designing dense reward
functions and effectively exploring the expansive state-action space. However,
despite a lack of dense rewards, these tasks often have a multi-stage
structure, which can be leveraged to decompose the overall objective into
manageable subgoals. In this work, we propose DEMO3, a framework that exploits
this structure for efficient learning from visual inputs. Specifically, our
approach incorporates multi-stage dense reward learning, a bi-phasic training
scheme, and world model learning into a carefully designed
demonstration-augmented RL framework that strongly mitigates the challenge of
exploration in long-horizon tasks. Our evaluations demonstrate that our method
improves data-efficiency by an average of 40% and by 70% on particularly
difficult tasks compared to state-of-the-art approaches. We validate this
across 16 sparse-reward tasks spanning four domains, including challenging
humanoid visual control tasks using as few as five demonstrations.

</details>


### [729] [Exploring the Secondary Risks of Large Language Models](https://arxiv.org/pdf/2506.12382)
*Jiawei Chen, Zhengwei Fang, Xiao Yang, Chao Yu, Zhaoxia Yin, Hang Su*

Main category: cs.LG

TL;DR: The paper introduces 'secondary risks,' a new class of non-adversarial failures in Large Language Models (LLMs) during benign interactions, and proposes SecLens, a framework to systematically evaluate these risks.


<details>
  <summary>Details</summary>
Motivation: Addressing overlooked non-adversarial failures in LLMs, which emerge during benign interactions and evade standard safety mechanisms.

Method: Introduces two risk primitives (verbose response and speculative advice) and SecLens, a black-box search framework, alongside SecRiskBench, a benchmark dataset.

Result: Secondary risks are widespread, transferable, and modality-independent across 16 popular models.

Conclusion: Highlights the need for improved safety mechanisms to mitigate harmful LLM behaviors in real-world deployments.

Abstract: Ensuring the safety and alignment of Large Language Models is a significant
challenge with their growing integration into critical applications and
societal functions. While prior research has primarily focused on jailbreak
attacks, less attention has been given to non-adversarial failures that subtly
emerge during benign interactions. We introduce secondary risks a novel class
of failure modes marked by harmful or misleading behaviors during benign
prompts. Unlike adversarial attacks, these risks stem from imperfect
generalization and often evade standard safety mechanisms. To enable systematic
evaluation, we introduce two risk primitives verbose response and speculative
advice that capture the core failure patterns. Building on these definitions,
we propose SecLens, a black-box, multi-objective search framework that
efficiently elicits secondary risk behaviors by optimizing task relevance, risk
activation, and linguistic plausibility. To support reproducible evaluation, we
release SecRiskBench, a benchmark dataset of 650 prompts covering eight diverse
real-world risk categories. Experimental results from extensive evaluations on
16 popular models demonstrate that secondary risks are widespread, transferable
across models, and modality independent, emphasizing the urgent need for
enhanced safety mechanisms to address benign yet harmful LLM behaviors in
real-world deployments.

</details>


### [730] [Truthful Elicitation of Imprecise Forecasts](https://arxiv.org/pdf/2503.16395)
*Anurag Singh, Siu Lun Chau, Krikamol Muandet*

Main category: cs.LG

TL;DR: A framework for scoring imprecise forecasts is proposed, enabling truthful elicitation by linking to social choice theory and using randomized proper scoring rules.


<details>
  <summary>Details</summary>
Motivation: Current scoring rules fail under epistemic uncertainty, limiting use in safety-critical domains where proper uncertainty management is vital.

Method: Introduces a two-way communication framework where decision-makers share aggregation rules, and uses randomized proper scoring rules for truthful elicitation.

Result: Truthful elicitation of imprecise forecasts is achievable, improving credibility by integrating epistemic uncertainty into decision-making.

Conclusion: The proposed framework enhances decision-making under uncertainty by addressing limitations of existing scoring rules.

Abstract: The quality of probabilistic forecasts is crucial for decision-making under
uncertainty. While proper scoring rules incentivize truthful reporting of
precise forecasts, they fall short when forecasters face epistemic uncertainty
about their beliefs, limiting their use in safety-critical domains where
decision-makers (DMs) prioritize proper uncertainty management. To address
this, we propose a framework for scoring imprecise forecasts -- forecasts given
as a set of beliefs. Despite existing impossibility results for deterministic
scoring rules, we enable truthful elicitation by drawing connection to social
choice theory and introducing a two-way communication framework where DMs first
share their aggregation rules (e.g., averaging or min-max) used in downstream
decisions for resolving forecast ambiguity. This, in turn, helps forecasters
resolve indecision during elicitation. We further show that truthful
elicitation of imprecise forecasts is achievable using proper scoring rules
randomized over the aggregation procedure. Our approach allows DM to elicit and
integrate the forecaster's epistemic uncertainty into their decision-making
process, thus improving credibility.

</details>


### [731] [AFBS:Buffer Gradient Selection in Semi-asynchronous Federated Learning](https://arxiv.org/pdf/2506.12754)
*Chaoyi Lu, Yiding Sun, Jinqian Chen, Zhichuan Yang, Jiangming Pan, Jihua Zhu*

Main category: cs.LG

TL;DR: AFBS (Asynchronous FL Buffer Selection) improves federated learning by selecting high-value gradients in buffers, enhancing performance and reducing training time.


<details>
  <summary>Details</summary>
Motivation: Asynchronous federated learning (AFL) suffers from gradient staleness, degrading performance. Existing semi-asynchronous methods struggle with stale gradients, harming training.

Method: AFBS introduces gradient selection within buffers using client clustering and random projection encryption. The server scores and selects gradients based on informational value, discarding low-value ones.

Result: AFBS outperforms state-of-the-art methods, improving accuracy by up to 4.8% on CIFAR-100 and reducing time to target accuracy by 75%.

Conclusion: AFBS effectively addresses gradient staleness in AFL, offering superior performance and efficiency in heterogeneous environments.

Abstract: Asynchronous federated learning (AFL) accelerates training by eliminating the
need to wait for stragglers, but its asynchronous nature introduces gradient
staleness, where outdated gradients degrade performance. Existing solutions
address this issue with gradient buffers, forming a semi-asynchronous
framework. However, this approach struggles when buffers accumulate numerous
stale gradients, as blindly aggregating all gradients can harm training. To
address this, we propose AFBS (Asynchronous FL Buffer Selection), the first
algorithm to perform gradient selection within buffers while ensuring privacy
protection. Specifically, the client sends the random projection encrypted
label distribution matrix before training, and the server performs client
clustering based on it. During training, server scores and selects gradients
within each cluster based on their informational value, discarding low-value
gradients to enhance semi-asynchronous federated learning. Extensive
experiments in highly heterogeneous system and data environments demonstrate
AFBS's superior performance compared to state-of-the-art methods. Notably, on
the most challenging task, CIFAR-100, AFBS improves accuracy by up to 4.8% over
the previous best algorithm and reduces the time to reach target accuracy by
75%.

</details>


### [732] [ASGO: Adaptive Structured Gradient Optimization](https://arxiv.org/pdf/2503.20762)
*Kang An, Yuxing Liu, Rui Pan, Yi Ren, Shiqian Ma, Donald Goldfarb, Tong Zhang*

Main category: cs.LG

TL;DR: ASGO is a novel optimization algorithm leveraging low-rank and block-wise diagonal properties of gradients and Hessians in deep neural networks, achieving superior convergence rates and practical effectiveness.


<details>
  <summary>Details</summary>
Motivation: Current optimizers like Adam do not utilize the structured properties (low-rank gradients, block-wise diagonal Hessians) in deep neural networks, which are crucial for efficient optimization.

Method: ASGO employs an adaptively updated preconditioner based on structured gradients, supported by fine-grained theoretical analysis.

Result: ASGO achieves better convergence rates than existing structured gradient methods and benefits from low-rank and block-wise diagonal properties.

Conclusion: ASGO is theoretically and empirically effective, particularly in language model tasks, demonstrating the value of leveraging structural properties in optimization.

Abstract: Training deep neural networks is a structured optimization problem, because
the parameters are naturally represented by matrices and tensors rather than by
vectors. Under this structural representation, it has been widely observed that
gradients are low-rank and Hessians are approximately block-wise diagonal.
These structured properties are crucial for designing efficient optimization
algorithms, but are not utilized by many current popular optimizers like Adam.
In this paper, we present a novel optimization algorithm ASGO that capitalizes
on these properties by employing a preconditioner that is adaptively updated
using structured gradients. By fine-grained theoretical analysis, ASGO is
proven to achieve superior convergence rates compared to existing structured
gradient methods. Based on the convergence theory, we further demonstrate that
ASGO can benefit from the low-rank and block-wise diagonal properties. We also
discuss practical modifications of ASGO and empirically verify ASGO's
effectiveness on language model tasks.

</details>


### [733] [Distributional Training Data Attribution](https://arxiv.org/pdf/2506.12965)
*Bruno Mlodozeniec, Isaac Reid, Sam Power, David Krueger, Murat Erdogdu, Richard E. Turner, Roger Grosse*

Main category: cs.LG

TL;DR: The paper introduces distributional training data attribution (d-TDA) to account for randomness in deep learning training, addressing limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional training data attribution algorithms fail to account for randomness in model training due to stochastic initialization and batching.

Method: The paper proposes d-TDA to predict how the distribution of model outputs depends on the dataset, validated through experiments.

Result: d-TDA identifies training examples that significantly alter output distributions, and reveals influence functions (IFs) as a natural outcome of the framework.

Conclusion: d-TDA provides a rigorous way to analyze training data impact, offering new insights into IFs and their limitations in deep learning.

Abstract: Randomness is an unavoidable part of training deep learning models, yet
something that traditional training data attribution algorithms fail to
rigorously account for. They ignore the fact that, due to stochasticity in the
initialisation and batching, training on the same dataset can yield different
models. In this paper, we address this shortcoming through introducing
distributional training data attribution (d-TDA), the goal of which is to
predict how the distribution of model outputs (over training runs) depends upon
the dataset. We demonstrate the practical significance of d-TDA in experiments,
e.g. by identifying training examples that drastically change the distribution
of some target measurement without necessarily changing the mean. Intriguingly,
we also find that influence functions (IFs), a popular but poorly-understood
data attribution tool, emerge naturally from our distributional framework as
the limit to unrolled differentiation; without requiring restrictive convexity
assumptions. This provides a new mathematical motivation for their efficacy in
deep learning, and helps to characterise their limitations.

</details>


### [734] [Physics-informed KAN PointNet: Deep learning for simultaneous solutions to inverse problems in incompressible flow on numerous irregular geometries](https://arxiv.org/pdf/2504.06327)
*Ali Kashefi, Tapan Mukerji*

Main category: cs.LG

TL;DR: PI-KAN-PointNet extends KANs to solve inverse problems over multiple irregular geometries in one training run, outperforming MLP-based methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current KANs are limited to single-domain training. This work explores their potential for multi-geometry inverse problems in computational physics.

Method: Integrates shared KANs into PointNet, using Jacobi polynomials for construction, and evaluates performance on natural convection with varying cylinder shapes.

Result: PI-KAN-PointNet reduces computational costs and improves prediction accuracy, especially for nonsmooth geometries, compared to MLP-based PointNet.

Conclusion: PI-KAN-PointNet is a promising approach for multi-geometry inverse problems, offering efficiency and accuracy advantages over traditional methods.

Abstract: Kolmogorov-Arnold Networks (KANs) have gained attention as an alternative to
traditional multilayer perceptrons (MLPs) for deep learning applications in
computational physics, particularly for solving inverse problems with sparse
data, as exemplified by the physics-informed Kolmogorov-Arnold network (PIKAN).
However, the capability of KANs to simultaneously solve inverse problems over
multiple irregular geometries within a single training run remains unexplored.
To address this gap, we introduce the physics-informed Kolmogorov-Arnold
PointNet (PI-KAN-PointNet), in which shared KANs are integrated into the
PointNet architecture to capture the geometric features of computational
domains. The loss function comprises the squared residuals of the governing
equations, computed via automatic differentiation, along with sparse
observations and partially known boundary conditions. We construct shared KANs
using Jacobi polynomials and investigate their performance by considering
Jacobi polynomials of different degrees and types in terms of both
computational cost and prediction accuracy. As a benchmark test case, we
consider natural convection in a square enclosure with a cylinder, where the
cylinder's shape varies across a dataset of 135 geometries. PI-KAN-PointNet
offers two main advantages. First, it overcomes the limitation of current
PIKANs, which are restricted to solving only a single computational domain per
training run, thereby reducing computational costs. Second, when comparing the
performance of PI-KAN-PointNet with that of the physics-informed PointNet using
MLPs, we observe that, with approximately the same number of trainable
parameters and comparable computational cost in terms of the number of epochs,
training time per epoch, and memory usage, PI-KAN-PointNet yields more accurate
predictions, particularly for values on unknown boundary conditions involving
nonsmooth geometries.

</details>


### [735] [Bures-Wasserstein Flow Matching for Graph Generation](https://arxiv.org/pdf/2506.14020)
*Keyue Jiang, Jiahao Cui, Xiaowen Dong, Laura Toni*

Main category: cs.LG

TL;DR: BWFlow introduces a flow-matching framework for graph generation, addressing the limitations of Euclidean assumptions in existing methods by leveraging Markov random fields and optimal transport.


<details>
  <summary>Details</summary>
Motivation: Existing graph generation methods assume Euclidean space, which is suboptimal for graphs' non-Euclidean structure and interconnected patterns, risking sampling convergence.

Method: BWFlow models joint evolution of nodes and edges using Markov random fields and designs probability paths via optimal transport displacement.

Result: BWFlow achieves competitive performance in plain graph and molecule generation, with stable training and guaranteed sampling convergence.

Conclusion: BWFlow provides a geometry-respecting, effective framework for graph generation, adaptable to continuous and discrete flow-matching algorithms.

Abstract: Graph generation has emerged as a critical task in fields ranging from
molecule design to drug discovery. Contemporary approaches, notably diffusion
and flow-based models, have achieved solid graph generative performance through
constructing a probability path that interpolates between a reference
distribution and the data distribution. However, these methods typically model
the evolution of individual nodes and edges independently and use linear
interpolations to build the path assuming that the data lie in Euclidean space.
We show that this is suboptimal given the intrinsic non-Euclidean structure and
interconnected patterns of graphs, and it poses risks to the sampling
convergence. To build a better probability path, we model the joint evolution
of the nodes and edges by representing graphs as connected systems
parameterized by Markov random fields (MRF). We then leverage the optimal
transport displacement between MRF objects to design the probability path for
graph generation. Based on this, we introduce BWFlow, a flow-matching framework
for graph generation that respects the underlying geometry of graphs and
provides smooth velocities in the probability path. The novel framework can be
adapted to both continuous and discrete flow-matching algorithms. Experimental
evaluations in plain graph generation and 2D/3D molecule generation validate
the effectiveness of BWFlow in graph generation with competitive performance,
stable training, and guaranteed sampling convergence.

</details>


### [736] [SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL](https://arxiv.org/pdf/2504.06386)
*Jacques Cloete, Nikolaus Vertovec, Alessandro Abate*

Main category: cs.LG

TL;DR: The paper introduces SPoRt, a method for providing safety guarantees in reinforcement learning by bounding the probability of violating safety properties, using a 'safe' base policy and a new projection-based training approach.


<details>
  <summary>Details</summary>
Motivation: To ensure safety in reinforcement learning for critical applications by providing theoretical and practical bounds on safety violations during policy training and deployment.

Method: Theoretical bounds on safety violations are derived using a maximum policy ratio. SPoRt computes these bounds data-drivenly and includes Projected PPO for training task-specific policies while maintaining safety.

Result: Experimental results show the trade-off between safety guarantees and task performance, comparing theoretical bounds to empirical violation rates.

Conclusion: SPoRt effectively balances safety and performance, offering a practical solution for safety-critical reinforcement learning applications.

Abstract: To apply reinforcement learning to safety-critical applications, we ought to
provide safety guarantees during both policy training and deployment. In this
work, we present theoretical results that place a bound on the probability of
violating a safety property for a new task-specific policy in a model-free,
episodic setting. This bound, based on a maximum policy ratio computed with
respect to a 'safe' base policy, can also be applied to temporally-extended
properties (beyond safety) and to robust control problems. To utilize these
results, we introduce SPoRt, which provides a data-driven method for computing
this bound for the base policy using the scenario approach, and includes
Projected PPO, a new projection-based approach for training the task-specific
policy while maintaining a user-specified bound on property violation. SPoRt
thus enables users to trade off safety guarantees against task-specific
performance. Complementing our theoretical results, we present experimental
results demonstrating this trade-off and comparing the theoretical bound to
posterior bounds derived from empirical violation rates.

</details>


### [737] [Disentangle and Regularize: Sign Language Production with Articulator-Based Disentanglement and Channel-Aware Regularization](https://arxiv.org/pdf/2504.06610)
*Sumeyye Meryem Tasyurek, Tugce Kiziltepe, Hacer Yalim Keles*

Main category: cs.LG

TL;DR: DARSLP is a gloss-free, transformer-based framework for sign language production (SLP) that maps spoken-language text to sign pose sequences using a pose autoencoder and non-autoregressive transformer decoder.


<details>
  <summary>Details</summary>
Motivation: To create a simple, interpretable, and gloss-free SLP framework that avoids reliance on gloss supervision or pretrained models.

Method: Uses a pose autoencoder with articulator-based disentanglement for structured representation learning, followed by a non-autoregressive transformer decoder with channel-aware regularization.

Result: Achieves state-of-the-art results on PHOENIX14T and CSL-Daily datasets.

Conclusion: DARSLP provides an effective, interpretable, and gloss-free solution for SLP.

Abstract: In this work, we propose DARSLP, a simple gloss-free, transformer-based sign
language production (SLP) framework that directly maps spoken-language text to
sign pose sequences. We first train a pose autoencoder that encodes sign poses
into a compact latent space using an articulator-based disentanglement
strategy, where features corresponding to the face, right hand, left hand, and
body are modeled separately to promote structured and interpretable
representation learning. Next, a non-autoregressive transformer decoder is
trained to predict these latent representations from sentence-level text
embeddings. To guide this process, we apply channel-aware regularization by
aligning predicted latent distributions with priors extracted from the
ground-truth encodings using a KL-divergence loss. The contribution of each
channel to the loss is weighted according to its associated articulator region,
enabling the model to account for the relative importance of different
articulators during training. Our approach does not rely on gloss supervision
or pretrained models, and achieves state-of-the-art results on the PHOENIX14T
and CSL-Daily datasets.

</details>


### [738] [DrivAer Transformer: A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset](https://arxiv.org/pdf/2504.08217)
*Jiaqi He, Xiangwen Luo, Yiping Wang*

Main category: cs.LG

TL;DR: The paper introduces DrivAer Transformer (DAT), a point cloud learning framework for accurate aerodynamic drag prediction from 3D vehicle meshes, addressing limitations of deep learning and traditional CFD methods.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for aerodynamic performance evaluation lack accuracy and versatility for complex 3D vehicle models due to limited datasets and geometric diversity.

Method: Proposes DAT, leveraging Transformer models and the DrivAerNet++ dataset to estimate air drag directly from 3D meshes, bypassing traditional 2D or SDF limitations.

Result: DAT enables fast and accurate drag prediction, improving aerodynamic evaluation and supporting data-driven automotive design.

Conclusion: DAT accelerates vehicle design, enhances efficiency, and lays a foundation for data-driven approaches in automotive aerodynamics.

Abstract: At the current stage, deep learning-based methods have demonstrated excellent
capabilities in evaluating aerodynamic performance, significantly reducing the
time and cost required for traditional computational fluid dynamics (CFD)
simulations. However, when faced with the task of processing extremely complex
three-dimensional (3D) vehicle models, the lack of large-scale datasets and
training resources, coupled with the inherent diversity and complexity of the
geometry of different vehicle models, means that the prediction accuracy and
versatility of these networks are still not up to the level required for
current production. In view of the remarkable success of Transformer models in
the field of natural language processing and their strong potential in the
field of image processing, this study innovatively proposes a point cloud
learning framework called DrivAer Transformer (DAT). The DAT structure uses the
DrivAerNet++ dataset, which contains high-fidelity CFD data of
industrial-standard 3D vehicle shapes. enabling accurate estimation of air drag
directly from 3D meshes, thus avoiding the limitations of traditional methods
such as 2D image rendering or signed distance fields (SDF). DAT enables fast
and accurate drag prediction, driving the evolution of the aerodynamic
evaluation process and laying the critical foundation for introducing a
data-driven approach to automotive design. The framework is expected to
accelerate the vehicle design process and improve development efficiency.

</details>


### [739] [LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs](https://arxiv.org/pdf/2506.15690)
*Tianyu Wang, Lingyou Pang, Akira Horiguchi, Carey E. Priebe*

Main category: cs.LG

TL;DR: LLM Web Dynamics (LWD) is introduced to study model collapse in LLMs at the network level, using a RAG database to simulate the Internet and analyze output convergence.


<details>
  <summary>Details</summary>
Motivation: The threat of model collapse in LLMs trained on synthetic data is underexplored, especially at the network level.

Method: LWD framework simulates the Internet with a RAG database to study output convergence, supported by theoretical guarantees from Gaussian Mixture Models.

Result: The framework provides insights into model collapse patterns and theoretical convergence guarantees.

Conclusion: LWD offers a novel approach to understanding and mitigating model collapse in LLMs at scale.

Abstract: The increasing use of synthetic data from the public Internet has enhanced
data usage efficiency in large language model (LLM) training. However, the
potential threat of model collapse remains insufficiently explored. Existing
studies primarily examine model collapse in a single model setting or rely
solely on statistical surrogates. In this work, we introduce LLM Web Dynamics
(LWD), an efficient framework for investigating model collapse at the network
level. By simulating the Internet with a retrieval-augmented generation (RAG)
database, we analyze the convergence pattern of model outputs. Furthermore, we
provide theoretical guarantees for this convergence by drawing an analogy to
interacting Gaussian Mixture Models.

</details>


### [740] [Predicting Mild Cognitive Impairment Using Naturalistic Driving and Trip Destination Modeling](https://arxiv.org/pdf/2504.09027)
*Souradeep Chattopadhyay, Guillermo Basulto-Elias, Jun Ha Chang, Matthew Rizzo, Shauna Hallmark, Anuj Sharma, Soumik Sarkar*

Main category: cs.LG

TL;DR: The study analyzes driving behavior in older adults with mild cognitive impairment (MCI) using geohashing and machine learning, finding the C5.0 model most effective with 68% recall.


<details>
  <summary>Details</summary>
Motivation: To enhance road safety by understanding how MCI affects driving behavior in older adults.

Method: Combines geohashing of trip destinations (home, work, etc.) with machine learning models (C5.0, Random Forest, SVM) for prediction.

Result: C5.0 model achieved 68% recall, effectively identifying cognitive impairment and reducing false negatives.

Conclusion: Life-space variables are innovative for predicting cognitive decline, enabling early intervention and support.

Abstract: Understanding the relationship between mild cognitive impairment (MCI) and
driving behavior is essential for enhancing road safety, particularly among
older adults. This study introduces a novel approach by incorporating specific
trip destinations-such as home, work, medical appointments, social activities,
and errands-using geohashing to analyze the driving habits of older drivers in
Nebraska. We employed a two-fold methodology that combines data visualization
with advanced machine learning models, including C5.0, Random Forest, and
Support Vector Machines, to assess the effectiveness of these location-based
variables in predicting cognitive impairment. Notably, the C5.0 model showed a
robust and stable performance, achieving a median recall of 0.68, which
indicates that our methodology accurately identifies cognitive impairment in
drivers 68\% of the time. This emphasizes our model's capacity to reduce false
negatives, a crucial factor given the profound implications of failing to
identify impaired drivers. Our findings underscore the innovative use of
life-space variables in understanding and predicting cognitive decline,
offering avenues for early intervention and tailored support for affected
individuals.

</details>


### [741] [VRAIL: Vectorized Reward-based Attribution for Interpretable Learning](https://arxiv.org/pdf/2506.16014)
*Jina Kim, Youjin Jang, Jeongjin Han*

Main category: cs.LG

TL;DR: VRAIL is a bi-level RL framework combining deep learning and RL for interpretable weight representations, improving stability and interpretability without environment changes.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and stability in RL by learning meaningful weight representations from state features.

Method: Two-stage approach: DL for value function estimation and RL for reward shaping via potential-based transformations, with linear/quadratic estimators.

Result: Improved training stability and convergence in Taxi-v3, uncovering semantically meaningful subgoals like passenger possession.

Conclusion: VRAIL is a model-agnostic framework for reward shaping that boosts learning and interpretability.

Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable
Learning), a bi-level framework for value-based reinforcement learning (RL)
that learns interpretable weight representations from state features. VRAIL
consists of two stages: a deep learning (DL) stage that fits an estimated value
function using state features, and an RL stage that uses this to shape learning
via potential-based reward transformations. The estimator is modeled in either
linear or quadratic form, allowing attribution of importance to individual
features and their interactions. Empirical results on the Taxi-v3 environment
demonstrate that VRAIL improves training stability and convergence compared to
standard DQN, without requiring environment modifications. Further analysis
shows that VRAIL uncovers semantically meaningful subgoals, such as passenger
possession, highlighting its ability to produce human-interpretable behavior.
Our findings suggest that VRAIL serves as a general, model-agnostic framework
for reward shaping that enhances both learning and interpretability.

</details>


### [742] [SD-KDE: Score-Debiased Kernel Density Estimation](https://arxiv.org/pdf/2504.19084)
*Elliot L. Epstein, Rajat Dwaraknath, Thanawat Sornwanee, John Winnicki, Jerry Weihong Liu*

Main category: cs.LG

TL;DR: A novel method (SD-KDE) improves density estimation by debiasing kernel density estimation using an estimated score function, reducing error significantly.


<details>
  <summary>Details</summary>
Motivation: To address bias in standard kernel density estimation (KDE) by leveraging score function corrections.

Method: Adjusts data points with a score function step, modifies bandwidth, and removes leading bias in KDE.

Result: SD-KDE reduces mean integrated squared error compared to Silverman KDE, even with noisy score estimates.

Conclusion: Score-based corrections enhance nonparametric density estimation, as shown in synthetic and MNIST experiments.

Abstract: We propose a novel method for density estimation that leverages an estimated
score function to debias kernel density estimation (SD-KDE). In our approach,
each data point is adjusted by taking a single step along the score function
with a specific choice of step size, followed by standard KDE with a modified
bandwidth. The step size and modified bandwidth are chosen to remove the
leading order bias in the KDE. Our experiments on synthetic tasks in 1D, 2D and
on MNIST, demonstrate that our proposed SD-KDE method significantly reduces the
mean integrated squared error compared to the standard Silverman KDE, even with
noisy estimates in the score function. These results underscore the potential
of integrating score-based corrections into nonparametric density estimation.

</details>


### [743] [Scalable Unit Harmonization in Medical Informatics via Bayesian-Optimized Retrieval and Transformer-Based Re-ranking](https://arxiv.org/pdf/2505.00810)
*Jordi de la Torre*

Main category: cs.LG

TL;DR: A scalable methodology for harmonizing inconsistent units in clinical datasets was developed, combining BM25, sentence embeddings, Bayesian optimization, and a transformer-based classifier. It outperformed baseline methods and achieved high precision and recall.


<details>
  <summary>Details</summary>
Motivation: To address data interoperability issues in large-scale clinical datasets by harmonizing inconsistent units, enabling seamless data reuse and reliable multi-institutional studies.

Method: A multi-stage pipeline involving filtering, identification, harmonization proposal generation, automated re-ranking, and manual validation, using a hybrid of BM25, sentence embeddings, Bayesian optimization, and a transformer-based classifier.

Result: The hybrid approach (MRR: 0.8833) outperformed lexical-only (MRR: 0.7985) and embedding-only (MRR: 0.5277) methods. The transformer-based reranker further improved MRR to 0.9833, with 83.39% precision at rank 1 and 94.66% recall at rank 5.

Conclusion: The framework provides an efficient, scalable solution for unit harmonization, reducing manual effort and improving accuracy, enabling consistent data reuse across healthcare systems.

Abstract: Objective: To develop and evaluate a scalable methodology for harmonizing
inconsistent units in large-scale clinical datasets, addressing a key barrier
to data interoperability.
  Materials and Methods: We designed a novel unit harmonization system
combining BM25, sentence embeddings, Bayesian optimization, and a bidirectional
transformer based binary classifier for retrieving and matching laboratory test
entries. The system was evaluated using the Optum Clinformatics Datamart
dataset (7.5 billion entries). We implemented a multi-stage pipeline:
filtering, identification, harmonization proposal generation, automated
re-ranking, and manual validation. Performance was assessed using Mean
Reciprocal Rank (MRR) and other standard information retrieval metrics.
  Results: Our hybrid retrieval approach combining BM25 and sentence embeddings
(MRR: 0.8833) significantly outperformed both lexical-only (MRR: 0.7985) and
embedding-only (MRR: 0.5277) approaches. The transformer-based reranker further
improved performance (absolute MRR improvement: 0.10), bringing the final
system MRR to 0.9833. The system achieved 83.39\% precision at rank 1 and
94.66\% recall at rank 5.
  Discussion: The hybrid architecture effectively leverages the complementary
strengths of lexical and semantic approaches. The reranker addresses cases
where initial retrieval components make errors due to complex semantic
relationships in medical terminology.
  Conclusion: Our framework provides an efficient, scalable solution for unit
harmonization in clinical datasets, reducing manual effort while improving
accuracy. Once harmonized, data can be reused seamlessly in different analyses,
ensuring consistency across healthcare systems and enabling more reliable
multi-institutional studies and meta-analyses.

</details>


### [744] [Stabilizing Temporal Difference Learning via Implicit Stochastic Recursion](https://arxiv.org/pdf/2505.01361)
*Hwanwoo Kim, Panos Toulis, Eric Laber*

Main category: cs.LG

TL;DR: Implicit TD algorithms are proposed to address the sensitivity of traditional TD learning to step size, offering stability and broader applicability without sacrificing efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional TD learning is sensitive to step size, leading to high variance and slow convergence. Current methods for selecting step sizes are ad hoc and inefficient.

Method: The paper introduces implicit TD algorithms, which reformulate TD updates into fixed point equations, ensuring stability and reduced sensitivity to step size.

Result: The proposed implicit TD algorithms (including TD(0), TD(), and TDC) are shown to be robust across a wider range of step sizes, with theoretical convergence guarantees and empirical validation.

Conclusion: Implicit TD algorithms provide a versatile and stable framework for policy evaluation and value approximation in RL, addressing key limitations of traditional TD learning.

Abstract: Temporal difference (TD) learning is a foundational algorithm in
reinforcement learning (RL). For nearly forty years, TD learning has served as
a workhorse for applied RL as well as a building block for more complex and
specialized algorithms. However, despite its widespread use, TD procedures are
generally sensitive to step size specification. A poor choice of step size can
dramatically increase variance and slow convergence in both on-policy and
off-policy evaluation tasks. In practice, researchers use trial and error to
identify stable step sizes, but these approaches tend to be ad hoc and
inefficient. As an alternative, we propose implicit TD algorithms that
reformulate TD updates into fixed point equations. Such updates are more stable
and less sensitive to step size without sacrificing computational efficiency.
Moreover, we derive asymptotic convergence guarantees and finite-time error
bounds for our proposed implicit TD algorithms, which include implicit TD(0),
TD($\lambda$), and TD with gradient correction (TDC). Our results show that
implicit TD algorithms are applicable to a much broader range of step sizes,
and thus provide a robust and versatile framework for policy evaluation and
value approximation in modern RL tasks. We demonstrate these benefits
empirically through extensive numerical examples spanning both on-policy and
off-policy tasks.

</details>


### [745] [Taming OOD Actions for Offline Reinforcement Learning: An Advantage-Based Approach](https://arxiv.org/pdf/2505.05126)
*Xuyang Chen, Keyu Yan, Lin Zhao*

Main category: cs.LG

TL;DR: ADAC introduces advantage-based modulation to evaluate OOD actions in offline RL, improving performance by selectively encouraging beneficial OOD actions.


<details>
  <summary>Details</summary>
Motivation: Offline RL suffers from distribution shift, leading to overestimation of OOD actions. Existing methods conservatively discourage all OOD actions, limiting generalization.

Method: ADAC uses the batch-optimal value function to evaluate OOD actions and modulates Q-function updates via an advantage function for precise assessment.

Result: ADAC achieves state-of-the-art performance on D4RL tasks, excelling in challenging scenarios.

Conclusion: ADAC effectively addresses distribution shift by selectively leveraging beneficial OOD actions, enhancing offline RL performance.

Abstract: Offline reinforcement learning (RL) aims to learn decision-making policies
from fixed datasets without online interactions, providing a practical solution
where online data collection is expensive or risky. However, offline RL often
suffers from distribution shift, resulting in inaccurate evaluation and
substantial overestimation on out-of-distribution (OOD) actions. To address
this, existing approaches incorporate conservatism by indiscriminately
discouraging all OOD actions, thereby hindering the agent's ability to
generalize and exploit beneficial ones. In this paper, we propose
Advantage-based Diffusion Actor-Critic (ADAC), a novel method that
systematically evaluates OOD actions using the batch-optimal value function.
Based on this evaluation, ADAC defines an advantage function to modulate the
Q-function update, enabling more precise assessment of OOD action quality. We
design a custom PointMaze environment and collect datasets to visually reveal
that advantage modulation can effectively identify and select superior OOD
actions. Extensive experiments show that ADAC achieves state-of-the-art
performance on almost all tasks in the D4RL benchmark, with particularly clear
margins on the more challenging tasks.

</details>


### [746] [Reliable Vertical Federated Learning in 5G Core Network Architecture](https://arxiv.org/pdf/2505.15244)
*Mohamad Mestoukirdi, Mourad Khanfouci*

Main category: cs.LG

TL;DR: A new algorithm is proposed to reduce generalization loss in Vertical Federated Learning (VFL) under client reliability constraints in 5G Core Networks, improving performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: VFL performance degrades due to reliability issues in Network Data Analytics Functions (NWDAFs) caused by resource constraints and operational overhead in 5G Core Networks.

Method: Optimizes vertical feature splits among clients and centrally defines local models based on reliability metrics, leveraging CN data handling flexibility.

Result: Empirical evaluation shows the proposed algorithm outperforms traditional baseline methods.

Conclusion: The method effectively mitigates generalization loss in VFL under client reliability constraints, enhancing performance in 5G Core Networks.

Abstract: This work proposes a new algorithm to mitigate model generalization loss in
Vertical Federated Learning (VFL) operating under client reliability
constraints within 5G Core Networks (CNs). Recently studied and endorsed by
3GPP, VFL enables collaborative and load-balanced model training and inference
across the CN. However, the performance of VFL significantly degrades when the
Network Data Analytics Functions (NWDAFs) - which serve as primary clients for
VFL model training and inference - experience reliability issues stemming from
resource constraints and operational overhead. Unlike edge environments, CN
environments adopt fundamentally different data management strategies,
characterized by more centralized data orchestration capabilities. This
presents opportunities to implement better distributed solutions that take full
advantage of the CN data handling flexibility. Leveraging this flexibility, we
propose a method that optimizes the vertical feature split among clients while
centrally defining their local models based on reliability metrics. Our
empirical evaluation demonstrates the effectiveness of our proposed algorithm,
showing improved performance over traditional baseline methods.

</details>


### [747] [Fast and Accurate Power Load Data Completion via Regularization-optimized Low-Rank Factorization](https://arxiv.org/pdf/2505.19133)
*Yan Xia, Hao Feng, Hongwei Sun, Junjie Wang, Qicong Hu*

Main category: cs.LG

TL;DR: A new method, Regularization-optimized Low-Rank Factorization, uses a PID controller to adaptively adjust regularization, improving accuracy and efficiency in power load data imputation.


<details>
  <summary>Details</summary>
Motivation: Existing low-rank factorization models for power load data imputation suffer from sensitivity to fixed or manually tuned regularization parameters, limiting generalization and convergence.

Method: Proposes a Regularization-optimized Low-Rank Factorization with a PID controller for adaptive regularization adjustment, analyzed for computational efficiency.

Result: Outperforms baselines in imputation accuracy and training efficiency on real-world power load datasets.

Conclusion: The method enhances adaptivity and efficiency while maintaining computational simplicity, validated by experiments.

Abstract: Low-rank representation learning has emerged as a powerful tool for
recovering missing values in power load data due to its ability to exploit the
inherent low-dimensional structures of spatiotemporal measurements. Among
various techniques, low-rank factorization models are favoured for their
efficiency and interpretability. However, their performance is highly sensitive
to the choice of regularization parameters, which are typically fixed or
manually tuned, resulting in limited generalization capability or slow
convergence in practical scenarios. In this paper, we propose a
Regularization-optimized Low-Rank Factorization, which introduces a
Proportional-Integral-Derivative controller to adaptively adjust the
regularization coefficient. Furthermore, we provide a detailed algorithmic
complexity analysis, showing that our method preserves the computational
efficiency of stochastic gradient descent while improving adaptivity.
Experimental results on real-world power load datasets validate the superiority
of our method in both imputation accuracy and training efficiency compared to
existing baselines.

</details>


### [748] [Data-Dependent Regret Bounds for Constrained MABs](https://arxiv.org/pdf/2505.20010)
*Gianmarco Genalti, Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti*

Main category: cs.LG

TL;DR: The paper introduces data-dependent regret bounds in constrained MAB settings, showing they can outperform classical bounds. It confirms feasibility in adversarial losses and stochastic constraints, with a focus on hard constraints.


<details>
  <summary>Details</summary>
Motivation: To explore if data-dependent regret bounds, which adapt to problem instances, can be derived in constrained MAB settings, a previously overlooked area.

Method: Designs an algorithm for constrained MABs with adversarial losses and stochastic constraints, focusing on hard constraints. The regret bound includes two data-dependent terms.

Result: The algorithm achieves a regret bound with two data-dependent terms, proving their necessity through a lower bound. Novel results for soft constraints are also derived.

Conclusion: Data-dependent regret bounds are feasible in constrained MABs, with the algorithm and analysis revealing fundamental problem complexities.

Abstract: This paper initiates the study of data-dependent regret bounds in constrained
MAB settings. These bounds depend on the sequence of losses that characterize
the problem instance. Thus, they can be much smaller than classical
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret bounds, while being equivalent to
them in the worst case. Despite this, data-dependent regret bounds have been
completely overlooked in constrained MAB settings. The goal of this paper is to
answer the following question: Can data-dependent regret bounds be derived in
the presence of constraints? We answer this question affirmatively in
constrained MABs with adversarial losses and stochastic constraints.
Specifically, our main focus is on the most challenging and natural settings
with hard constraints, where the learner must ensure that the constraints are
always satisfied with high probability. We design an algorithm with a regret
bound consisting of two data-dependent terms. The first term captures the
difficulty of satisfying the constraints, while the second one encodes the
complexity of learning independently of the presence of constraints. We also
prove a lower bound showing that these two terms are not artifacts of our
specific approach and analysis, but rather the fundamental components that
inherently characterize the complexities of the problem. Finally, in designing
our algorithm, we also derive some novel results in the related (and easier)
soft constraints settings, which may be of independent interest.

</details>


### [749] [Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach](https://arxiv.org/pdf/2505.20130)
*Jin Zhu, Jingyi Li, Hongyi Zhou, Yinan Lin, Zhenhua Lin, Chengchun Shi*

Main category: cs.LG

TL;DR: Proposes a method to optimize spatial experiments using a surrogate function for MSE, leveraging graph cut algorithms for efficient design.


<details>
  <summary>Details</summary>
Motivation: To enhance accuracy in causal effect estimation by optimizing spatial experiment designs, accommodating interference and varying covariance functions.

Method: Uses a surrogate function for MSE and graph cut algorithms to derive optimal experimental designs.

Result: Validated through theory and experiments, showing effectiveness in synthetic and real-world scenarios (e.g., ridesharing markets).

Conclusion: The method is computationally efficient, adaptable, and improves causal effect estimation in spatial experiments.

Abstract: This paper focuses on the design of spatial experiments to optimize the
amount of information derived from the experimental data and enhance the
accuracy of the resulting causal effect estimator. We propose a surrogate
function for the mean squared error (MSE) of the estimator, which facilitates
the use of classical graph cut algorithms to learn the optimal design. Our
proposal offers three key advances: (1) it accommodates moderate to large
spatial interference effects; (2) it adapts to different spatial covariance
functions; (3) it is computationally efficient. Theoretical results and
numerical experiments based on synthetic environments and a dispatch simulator
that models a city-scale ridesharing market, further validate the effectiveness
of our design. A python implementation of our method is available at
https://github.com/Mamba413/CausalGraphCut.

</details>


### [750] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/pdf/2505.21777)
*Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca Ambrogioni, Dmitry Krotov*

Main category: cs.LG

TL;DR: The paper explores diffusion models as associative memory systems, drawing parallels to Hopfield networks. It identifies memorization and generalization phases in diffusion models, predicting and validating spurious states.


<details>
  <summary>Details</summary>
Motivation: To understand diffusion models through the lens of associative memory, revealing insights into memorization-generalization trade-offs and emergent spurious states.

Method: Conceptualize diffusion model training as memory encoding and generation as retrieval. Analyze phases (memorization vs. generalization) and spurious states.

Result: In small data, memorization occurs; in large data, new attractor states emerge. Spurious states appear at the transition boundary, validated empirically.

Conclusion: The study bridges diffusion models and associative memory, offering theoretical and empirical insights into spurious states and memorization-generalization dynamics.

Abstract: Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [751] [Multi-agent Markov Entanglement](https://arxiv.org/pdf/2506.02385)
*Shuze Chen, Tianyi Peng*

Main category: cs.LG

TL;DR: The paper explores the theoretical justification for value decomposition in multi-agent RL, introducing the concept of 'Markov entanglement' to measure and bound decomposition error.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of value decomposition in multi-agent RL lacks theoretical understanding, prompting an investigation into its underlying mathematical structure.

Method: The authors analyze multi-agent MDPs, defining 'Markov entanglement' to measure transition matrix entanglement and bound decomposition error.

Result: They prove that a class of index policies is weakly entangled with sublinear error scaling, and show how to estimate entanglement empirically.

Conclusion: The work provides a theoretical foundation for value decomposition and practical tools for assessing its quality in multi-agent systems.

Abstract: Value decomposition has long been a fundamental technique in multi-agent
dynamic programming and reinforcement learning (RL). Specifically, the value
function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the
sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$.
This approach traces back to the index policy in restless multi-armed bandit
problems and has found various applications in modern RL systems. However, the
theoretical justification for why this decomposition works so effectively
remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables
value decomposition. We demonstrate that a multi-agent Markov decision process
(MDP) permits value decomposition if and only if its transition matrix is not
"entangled" -- a concept analogous to quantum entanglement in quantum physics.
Drawing inspiration from how physicists measure quantum entanglement, we
introduce how to measure the "Markov entanglement" for multi-agent MDPs and
show that this measure can be used to bound the decomposition error in general
multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class
of index policies is weakly entangled and enjoys a sublinear $\mathcal
O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we
show how Markov entanglement can be efficiently estimated in practice,
providing practitioners with an empirical proxy for the quality of value
decomposition.

</details>


### [752] [Unsupervised risk factor identification across cancer types and data modalities via explainable artificial intelligence](https://arxiv.org/pdf/2506.12944)
*Maximilian Ferle, Jonas Ader, Thomas Wiemers, Nora Grieb, Adrian Lindenmeyer, Hans-Jonas Meyer, Thomas Neumuth, Markus Kreuz, Kristin Reiche, Maximilian Merz*

Main category: cs.LG

TL;DR: A novel unsupervised machine learning method optimizes survival heterogeneity across patient clusters using a differentiable logrank statistic, identifying prognostically distinct groups in cancer data.


<details>
  <summary>Details</summary>
Motivation: Current risk stratification methods often fail to translate survival analysis into actionable clinical criteria, necessitating a more direct and interpretable approach.

Method: The method adapts the multivariate logrank statistic for unsupervised learning, training neural networks to identify distinct patient subgroups without relying on proxy metrics.

Result: Applied to multiple myeloma and non-small cell lung cancer data, the method identified subgroups with significantly different survival outcomes, validated by clinically meaningful features.

Conclusion: This pan-cancer, model-agnostic approach advances clinical risk stratification, offering interpretable results for personalized treatment and decision-making.

Abstract: Risk stratification is a key tool in clinical decision-making, yet current
approaches often fail to translate sophisticated survival analysis into
actionable clinical criteria. We present a novel method for unsupervised
machine learning that directly optimizes for survival heterogeneity across
patient clusters through a differentiable adaptation of the multivariate
logrank statistic. Unlike most existing methods that rely on proxy metrics, our
approach represents novel methodology for training any neural network
architecture on any data modality to identify prognostically distinct patient
groups. We thoroughly evaluate the method in simulation experiments and
demonstrate its utility in practice by applying it to two distinct cancer
types: analyzing laboratory parameters from multiple myeloma patients and
computed tomography images from non-small cell lung cancer patients,
identifying prognostically distinct patient subgroups with significantly
different survival outcomes in both cases. Post-hoc explainability analyses
uncover clinically meaningful features determining the group assignments which
align well with established risk factors and thus lend strong weight to the
methods utility. This pan-cancer, model-agnostic approach represents a valuable
advancement in clinical risk stratification, enabling the discovery of novel
prognostic signatures across diverse data types while providing interpretable
results that promise to complement treatment personalization and clinical
decision-making in oncology and beyond.

</details>


### [753] [Global Context-aware Representation Learning for Spatially Resolved Transcriptomics](https://arxiv.org/pdf/2506.15698)
*Yunhak Oh, Junseok Lee, Yeongmin Kim, Sangwoo Seo, Namkyeong Lee, Chanyoung Park*

Main category: cs.LG

TL;DR: Spotscape is a new framework for Spatially Resolved Transcriptomics (SRT) that improves spot representations by capturing global relationships and regulating distances between spots, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based methods for SRT struggle with meaningful spot representations, especially near boundaries, due to overemphasis on adjacent spots with minimal feature differences.

Method: Spotscape introduces a Similarity Telescope module for global spot relationships and a similarity scaling strategy for multi-slice integration.

Result: Spotscape excels in single-slice and multi-slice scenarios, demonstrating superior performance in downstream tasks.

Conclusion: Spotscape addresses limitations of current SRT methods, offering improved accuracy and versatility for spatial transcriptomics analysis.

Abstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that
captures the spatial context of cells within tissues, enabling the study of
complex biological networks. Recent graph-based methods leverage both gene
expression and spatial information to identify relevant spatial domains.
However, these approaches fall short in obtaining meaningful spot
representations, especially for spots near spatial domain boundaries, as they
heavily emphasize adjacent spots that have minimal feature differences from an
anchor node. To address this, we propose Spotscape, a novel framework that
introduces the Similarity Telescope module to capture global relationships
between multiple spots. Additionally, we propose a similarity scaling strategy
to regulate the distances between intra- and inter-slice spots, facilitating
effective multi-slice integration. Extensive experiments demonstrate the
superiority of Spotscape in various downstream tasks, including single-slice
and multi-slice scenarios. Our code is available at the following link: https:
//github.com/yunhak0/Spotscape.

</details>


### [754] [Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs](https://arxiv.org/pdf/2506.14003)
*Yiwei Chen, Soumyadeep Pal, Yimeng Zhang, Qing Qu, Sijia Liu*

Main category: cs.LG

TL;DR: Machine unlearning (MU) in large language models (LLMs) leaves detectable traces, enabling reverse-engineering of forgotten information.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of unlearning trace detection in LLMs, which poses risks to data privacy, copyright, and sociotechnical harm mitigation.

Method: Analyze model behavior and internal representations to detect unlearning traces using supervised classifiers and activation space analysis.

Result: Over 90% accuracy in detecting unlearning traces, even with irrelevant inputs, revealing persistent fingerprints in LLMs.

Conclusion: Unlearning leaves measurable signatures, introducing new risks of reverse-engineering forgotten data.

Abstract: Machine unlearning (MU) for large language models (LLMs), commonly referred
to as LLM unlearning, seeks to remove specific undesirable data or knowledge
from a trained model, while maintaining its performance on standard tasks.
While unlearning plays a vital role in protecting data privacy, enforcing
copyright, and mitigating sociotechnical harms in LLMs, we identify a new
vulnerability post-unlearning: unlearning trace detection. We discover that
unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces
in both model behavior and internal representations. These traces can be
identified from output responses, even when prompted with forget-irrelevant
inputs. Specifically, a simple supervised classifier can reliably determine
whether a model has undergone unlearning based solely on its textual outputs.
Further analysis shows that these traces are embedded in intermediate
activations and propagate nonlinearly to the final layer, forming
low-dimensional, learnable manifolds in activation space. Through extensive
experiments, we show that forget-relevant prompts enable over 90% accuracy in
detecting unlearning traces across all model sizes. Even with forget-irrelevant
inputs, large LLMs maintain high detectability, demonstrating the broad
applicability of unlearning trace detection. These findings reveal that
unlearning leaves measurable signatures, introducing a new risk of
reverse-engineering forgotten information when a model is identified as
unlearned given an input query. Codes are available at
https://github.com/OPTML-Group/Unlearn-Trace.

</details>


### [755] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/pdf/2506.14436)
*Shen Yuan, Yin Zheng, Taifeng Wang, Binbin Liu, Hongteng Xu*

Main category: cs.LG

TL;DR: A novel 'model MoE-ization' strategy, MoORE, is proposed to mitigate task conflict and oblivion in multi-task adaptation by using SVD and learnable routers, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing task conflict and oblivion in adapting large-scale foundation models for multi-task scenarios.

Method: Applies SVD to pre-trained weight matrices, introduces learnable routers for task/sample-specific singular value adjustment, and ensures orthogonality via MoORE (Mixture of Orthogonal Rank-one Experts).

Result: MoORE consistently outperforms existing multi-task adaptation methods, demonstrating superior conflict- and oblivion-resistance.

Conclusion: MoORE is an effective solution for multi-task adaptation, balancing task-specific adjustments while preserving original model capabilities.

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers
from task conflict and oblivion. To mitigate such issues, we propose a novel
''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant
multi-task adaptation method. Given a weight matrix of a pre-trained model, our
method applies SVD to it and introduces a learnable router to adjust its
singular values based on tasks and samples. Accordingly, the weight matrix
becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert
corresponds to the outer product of a left singular vector and the
corresponding right one. We can improve the model capacity by imposing a
learnable orthogonal transform on the right singular vectors. Unlike low-rank
adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts'
orthogonality and maintains the column space of the original weight matrix.
These two properties make the adapted model resistant to the conflicts among
the new tasks and the oblivion of its original tasks, respectively. Experiments
on various datasets demonstrate that MoORE outperforms existing multi-task
adaptation methods consistently, showing its superiority in terms of conflict-
and oblivion-resistance. The code of the experiments is available at
https://github.com/DaShenZi721/MoORE.

</details>


### [756] [SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification](https://arxiv.org/pdf/2506.14587)
*Shuo Yang, Bardh Prenkaj, Gjergji Kasneci*

Main category: cs.LG

TL;DR: SCISSOR, a Siamese network-based debiasing method, addresses semantic shortcut learning by remapping the semantic space, improving model robustness without data augmentation.


<details>
  <summary>Details</summary>
Motivation: Shortcut learning undermines generalization due to semantic imbalances, which prior methods overlook by focusing on superficial feature biases.

Method: SCISSOR intervenes in semantic clusters to suppress shortcuts, using a Siamese network without requiring data augmentation or rewriting.

Result: SCISSOR improves F1 scores by +5.3 to +7.7 across 4 benchmarks and boosts lightweight models by ~9.5-11.9%.

Conclusion: SCISSOR redefines model generalization by tackling semantic biases, offering a robust framework against shortcut learning.

Abstract: Shortcut learning undermines model generalization to out-of-distribution
data. While the literature attributes shortcuts to biases in superficial
features, we show that imbalances in the semantic distribution of sample
embeddings induce spurious semantic correlations, compromising model
robustness. To address this issue, we propose SCISSOR (Semantic Cluster
Intervention for Suppressing ShORtcut), a Siamese network-based debiasing
approach that remaps the semantic space by discouraging latent clusters
exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR
eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on
6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and
GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports
+5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay,
and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models
with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for
BERT on NLP. Our study redefines the landscape of model generalization by
addressing overlooked semantic biases, establishing SCISSOR as a foundational
framework for mitigating shortcut learning and fostering more robust,
bias-resistant AI systems.

</details>


### [757] [Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration](https://arxiv.org/pdf/2506.15721)
*Junqi Gao, Zhichang Guo, Dazhi Zhang, Dong Li, Runze Liu, Pengfei Li, Kai Tian, Biqing Qi*

Main category: cs.LG

TL;DR: Bohdi is a synthetic-data-only framework for heterogeneous LLM fusion, addressing limitations of existing methods by enabling dynamic domain exploration and adaptive data sampling via hierarchical multi-armed bandit and DynaBranches mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing LLM fusion methods rely on limited real data and fixed domain allocations, causing knowledge gaps and capability imbalances. Bohdi aims to overcome these issues.

Method: Bohdi organizes domains hierarchically, uses multi-model collaboration for synthetic data generation, and employs DynaBranches for adaptive sampling via hierarchical multi-armed bandit. The IR mechanism tracks capability shifts.

Result: Bohdi outperforms baselines, shows higher data efficiency, and eliminates capability imbalance in target LLMs.

Conclusion: Bohdi provides an effective, adaptive solution for heterogeneous LLM fusion, leveraging synthetic data and dynamic adjustments.

Abstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of
multiple source LLMs with different architectures into a target LLM with low
computational overhead. While promising, existing methods suffer from two major
limitations: 1) reliance on real data from limited domain for knowledge fusion,
preventing the target LLM from fully acquiring knowledge across diverse
domains, and 2) fixed data allocation proportions across domains, failing to
dynamically adjust according to the target LLM's varying capabilities across
domains, leading to a capability imbalance. To overcome these limitations, we
propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.
Through the organization of knowledge domains into a hierarchical tree
structure, Bohdi enables automatic domain exploration and multi-domain data
generation through multi-model collaboration, thereby comprehensively
extracting knowledge from source LLMs. By formalizing domain expansion and data
sampling proportion allocation on the knowledge tree as a Hierarchical
Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism
to adaptively adjust sampling proportions based on the target LLM's performance
feedback across domains. Integrated with our proposed Introspection-Rebirth
(IR) mechanism, DynaBranches dynamically tracks capability shifts during target
LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),
further enhancing its online adaptation capability. Comparative experimental
results on a comprehensive suite of benchmarks demonstrate that Bohdi
significantly outperforms existing baselines on multiple target LLMs, exhibits
higher data efficiency, and virtually eliminates the imbalance in the target
LLM's capabilities. Our code is available at
https://github.com/gjq100/Bohdi.git.

</details>


### [758] [Hidden Breakthroughs in Language Model Training](https://arxiv.org/pdf/2506.15872)
*Sara Kangaslahti, Elan Rosenfeld, Naomi Saphra*

Main category: cs.LG

TL;DR: POLCA identifies hidden conceptual breakthroughs in training by decomposing loss changes along low-rank subspaces, revealing interpretable clusters of data.


<details>
  <summary>Details</summary>
Motivation: Visible discontinuities in loss curves indicate breakthroughs, but many are hidden due to scalar loss metrics. Understanding these hidden transitions can deepen insights into learning dynamics.

Method: Introduces POLCA, a method to decompose loss changes along low-rank training subspaces, identifying clusters of samples with similar loss dynamics.

Result: Validated on synthetic and natural language tasks, POLCA successfully recovers interpretable clusters representing model breakthroughs.

Conclusion: POLCA offers a promising tool for unsupervised interpretability by uncovering hidden phase transitions in training.

Abstract: Loss curves are smooth during most of model training, so visible
discontinuities stand out as possible conceptual breakthroughs. Studying these
breakthroughs enables a deeper understanding of learning dynamics, but only
when they are properly identified. This paper argues that similar breakthroughs
occur frequently throughout training but they are obscured by a loss metric
that collapses all variation into a single scalar. To find these hidden
transitions, we introduce POLCA, a method for decomposing changes in loss along
arbitrary bases of the low-rank training subspace. We use our method to
identify clusters of samples that share similar changes in loss during
training, disaggregating the overall loss into that of smaller groups of
conceptually similar data. We validate our method on synthetic arithmetic and
natural language tasks, showing that POLCA recovers clusters that represent
interpretable breakthroughs in the model's capabilities. We demonstrate the
promise of these hidden phase transitions as a tool for unsupervised
interpretability.

</details>


### [759] [Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song](https://arxiv.org/pdf/2506.16174)
*Ismo Horppu, Frederick Ayala, Erlin Gulbenkoglu*

Main category: cs.LG

TL;DR: The paper explores AI's ability to transcribe Finnish rap lyrics, comparing Faster Whisperer and YouTube's speech-to-text, with Mc Timo's lyrics as reference.


<details>
  <summary>Details</summary>
Motivation: To test AI's capability in understanding complex languages like Finnish, especially in artistic contexts like rap music.

Method: Compare Faster Whisperer and YouTube's speech-to-text by transcribing Finnish rap lyrics, measuring errors against original lyrics.

Result: Hallucination and mishearing levels of AI transcriptions are measured informally.

Conclusion: AI faces challenges in accurately transcribing complex, artistic language like Finnish rap.

Abstract: All languages are peculiar. Some of them are considered more challenging to
understand than others. The Finnish Language is known to be a complex language.
Also, when languages are used by artists, the pronunciation and meaning might
be more tricky to understand. Therefore, we are putting AI to a fun, yet
challenging trial: translating a Finnish rap song to text. We will compare the
Faster Whisperer algorithm and YouTube's internal speech-to-text functionality.
The reference truth will be Finnish rap lyrics, which the main author's little
brother, Mc Timo, has written. Transcribing the lyrics will be challenging
because the artist raps over synth music player by Syntikka Janne. The
hallucination level and mishearing of AI speech-to-text extractions will be
measured by comparing errors made against the original Finnish lyrics. The
error function is informal but still works for our case.

</details>


### [760] [Soft decision trees for survival analysis](https://arxiv.org/pdf/2506.16846)
*Antonio Consolo, Edoardo Amaldi, Emilio Carrizosa*

Main category: cs.LG

TL;DR: The paper introduces a soft survival tree (SST) model, optimized via nonlinear decomposition, outperforming benchmarks in survival analysis.


<details>
  <summary>Details</summary>
Motivation: To enhance survival tree models by combining interpretability with global optimization and flexibility in survival function choices.

Method: Proposes SST with soft splitting rules, trained via nonlinear optimization, allowing parametric, semiparametric, or nonparametric survival functions.

Result: SSTs outperform three benchmark survival trees on 15 datasets in discrimination and calibration measures.

Conclusion: SSTs offer a flexible, interpretable, and high-performing approach to survival analysis, with potential for fairness extensions.

Abstract: Decision trees are popular in survival analysis for their interpretability
and ability to model complex relationships. Survival trees, which predict the
timing of singular events using censored historical data, are typically built
through heuristic approaches. Recently, there has been growing interest in
globally optimized trees, where the overall tree is trained by minimizing the
error function over all its parameters. We propose a new soft survival tree
model (SST), with a soft splitting rule at each branch node, trained via a
nonlinear optimization formulation amenable to decomposition. Since SSTs
provide for every input vector a specific survival function associated to a
single leaf node, they satisfy the conditional computation property and inherit
the related benefits. SST and the training formulation combine flexibility with
interpretability: any smooth survival function (parametric, semiparametric, or
nonparametric) estimated through maximum likelihood can be used, and each leaf
node of an SST yields a cluster of distinct survival functions which are
associated to the data points routed to it. Numerical experiments on 15
well-known datasets show that SSTs, with parametric and spline-based
semiparametric survival functions, trained using an adaptation of the
node-based decomposition algorithm proposed by Consolo et al. (2024) for soft
regression trees, outperform three benchmark survival trees in terms of four
widely-used discrimination and calibration measures. SSTs can also be extended
to consider group fairness.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [761] [Advanced Game-Theoretic Frameworks for Multi-Agent AI Challenges: A 2025 Outlook](https://arxiv.org/pdf/2506.17348)
*Pavel Malinovskiy*

Main category: cs.MA

TL;DR: The paper explores advanced game-theoretic paradigms for next-gen AI challenges, focusing on dynamic coalition formation, language-based utilities, and adversarial risks, with mathematical and simulation tools.


<details>
  <summary>Details</summary>
Motivation: To address the next-generation AI challenges by extending traditional models to include complex, uncertain, and adversarial contexts.

Method: Uses mathematical formalisms, simulations, and coding schemes incorporating repeated games, Bayesian updates, and moral framing in payoff structures.

Result: Provides theoretical tools for multi-agent AI systems to adapt and negotiate in complex, uncertain environments.

Conclusion: Aims to equip AI researchers with robust tools for strategic interaction in adversarial and partially observable settings.

Abstract: This paper presents a substantially reworked examination of how advanced
game-theoretic paradigms can serve as a foundation for the next-generation
challenges in Artificial Intelligence (AI), forecasted to arrive in or around
2025. Our focus extends beyond traditional models by incorporating dynamic
coalition formation, language-based utilities, sabotage risks, and partial
observability. We provide a set of mathematical formalisms, simulations, and
coding schemes that illustrate how multi-agent AI systems may adapt and
negotiate in complex environments. Key elements include repeated games,
Bayesian updates for adversarial detection, and moral framing within payoff
structures. This work aims to equip AI researchers with robust theoretical
tools for aligning strategic interaction in uncertain, partially adversarial
contexts.

</details>


### [762] [Towards Zero-Shot Coordination between Teams of Agents: The N-XPlay Framework](https://arxiv.org/pdf/2506.17560)
*Ava Abderezaei, Chi-Hui Lin, Joseph Miceli, Naren Sivagnanadasan, Stphane Aroca-Ouellette, Jake Brawer, Alessandro Roncone*

Main category: cs.MA

TL;DR: The paper introduces N-player Overcooked and N-XPlay for evaluating and improving zero-shot coordination (ZSC) in multi-agent, multi-team systems, showing better intra- and inter-team coordination than Self-Play.


<details>
  <summary>Details</summary>
Motivation: Existing ZSC methods focus on two-agent interactions, missing the complexity of real-world multi-team systems (MTS).

Method: Proposes N-player Overcooked for N-agent ZSC evaluation and N-XPlay for training agents in multi-team settings.

Result: N-XPlay-trained agents outperform Self-Play in balancing intra-team and inter-team coordination in 2-, 3-, and 5-player scenarios.

Conclusion: N-XPlay advances ZSC in multi-team systems, demonstrating improved coordination in complex settings.

Abstract: Zero-shot coordination (ZSC) -- the ability to collaborate with unfamiliar
partners -- is essential to making autonomous agents effective teammates.
Existing ZSC methods evaluate coordination capabilities between two agents who
have not previously interacted. However, these scenarios do not reflect the
complexity of real-world multi-agent systems, where coordination often involves
a hierarchy of sub-groups and interactions between teams of agents, known as
Multi-Team Systems (MTS). To address this gap, we first introduce N-player
Overcooked, an N-agent extension of the popular two-agent ZSC benchmark,
enabling evaluation of ZSC in N-agent scenarios. We then propose N-XPlay for
ZSC in N-agent, multi-team settings. Comparison against Self-Play across two-,
three- and five-player Overcooked scenarios, where agents are split between an
``ego-team'' and a group of unseen collaborators shows that agents trained with
N-XPlay are better able to simultaneously balance ``intra-team'' and
``inter-team'' coordination than agents trained with SP.

</details>


### [763] [Optimization of Flying Ad Hoc Network Topology and Collaborative Path Planning for Multiple UAVs](https://arxiv.org/pdf/2506.17945)
*Ming He, Peizhao Wang, Haihua Chen, Bin Sun, Hongpeng Wang*

Main category: cs.MA

TL;DR: The paper proposes a reinforcement learning-based trajectory planning (RL-TP) and convex-based topology optimization (C-TOP) to maximize UAV network data throughput in harsh environments.


<details>
  <summary>Details</summary>
Motivation: Addressing overlooked real-time data retrieval and UAV positioning issues in UAV networks, focusing on coverage and data transmission capabilities.

Method: Combines RL-TP for optimizing UAV paths under FANET constraints and C-TOP for maximizing data throughput with convex optimization.

Result: Simulations and experiments show improved data throughput over A-LMST and CPAPO methods.

Conclusion: The proposed strategy effectively optimizes UAV trajectories and enhances FANET performance.

Abstract: Multiple unmanned aerial vehicles (UAVs) play a vital role in monitoring and
data collection in wide area environments with harsh conditions. In most
scenarios, issues such as real-time data retrieval and real-time UAV
positioning are often disregarded, essentially neglecting the communication
constraints. In this paper, we comprehensively address both the coverage of the
target area and the data transmission capabilities of the flying ad hoc network
(FANET). The data throughput of the network is therefore maximized by
optimizing the network topology and the UAV trajectories. The resultant
optimization problem is effectively solved by the proposed reinforcement
learning-based trajectory planning (RL-TP) algorithm and the convex-based
topology optimization (C-TOP) algorithm sequentially. The RL-TP optimizes the
UAV paths while considering the constraints of FANET. The C-TOP maximizes the
data throughput of the network while simultaneously constraining the neighbors
and transmit powers of the UAVs, which is shown to be a convex problem that can
be efficiently solved in polynomial time. Simulations and field experimental
results show that the proposed optimization strategy can effectively plan the
UAV trajectories and significantly improve the data throughput of the FANET
over the adaptive local minimum spanning tree (A-LMST) and cyclic
pruning-assisted power optimization (CPAPO) methods.

</details>


### [764] [The Hive Mind is a Single Reinforcement Learning Agent](https://arxiv.org/pdf/2410.17517)
*Karthik Soma, Yann Bouteiller, Heiko Hamann, Giovanni Beltrame*

Main category: cs.MA

TL;DR: The paper shows that collective decision-making in honey bees (via imitation) is equivalent to a single reinforcement learning agent, proposing a new bandit algorithm called Maynard-Cross Learning.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between collective decision-making (e.g., imitation in swarms) and individual trial-and-error (e.g., reinforcement learning), using honey bees as a model.

Method: Analyzes nest-site selection in honey bees, mapping swarm behavior to a reinforcement learning framework and introducing Maynard-Cross Learning.

Result: Demonstrates equivalence between swarm imitation and a single RL agent, suggesting simple individual behaviors can achieve group-level intelligence.

Conclusion: Group-level intelligence can match complex individual cognition, explaining how simple behaviors are selected in nature.

Abstract: Decision-making is an essential attribute of any intelligent agent or group.
Natural systems are known to converge to optimal strategies through at least
two distinct mechanisms: collective decision-making via imitation of others,
and individual trial-and-error. This paper establishes an equivalence between
these two paradigms by drawing from the well-established collective
decision-making model of nest-site selection in swarms of honey bees. We show
that the emergent distributed cognition (sometimes referred to as the hive mind
) arising from individual bees following simple, local imitation-based rules is
equivalent to a single online reinforcement learning (RL) agent interacting
with many parallel environments. The update rule through which this macro-agent
learns is a bandit algorithm that we coin Maynard-Cross Learning. Our analysis
implies that a group of cognition-limited organisms can be on-par with a more
complex, reinforcement-enabled entity, substantiating the idea that group-level
intelligence may explain how seemingly simple and blind individual behaviors
are selected in nature.

</details>


### [765] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/pdf/2506.09600)
*Itay Nakash, George Kour, Koren Lazar, Matan Vetzler, Guy Uziel, Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: The paper addresses the challenge of ensuring task-oriented LLM-based agents adhere to strict policies while maintaining natural interactions, proposing a threat model and CRAFT, a red-teaming system, to test resilience.


<details>
  <summary>Details</summary>
Motivation: Ensuring policy adherence in LLM-based agents against adversarial users exploiting rules for personal benefit.

Method: Introduces CRAFT, a multi-agent red-teaming system, and tau-break benchmark to evaluate robustness against manipulative behavior.

Result: CRAFT outperforms conventional jailbreak methods; simple defenses are insufficient against adversarial attacks.

Conclusion: Stronger safeguards are needed to protect policy-adherent agents from adversarial exploitation.

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [766] [Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?](https://arxiv.org/pdf/2506.17623)
*Yuesheng Huang, Peng Zhang, Riliang Liu, Jiaqi Liang*

Main category: cs.MM

TL;DR: The paper explores using Text-to-Image (T2I) models to generate images as a complementary modality for text-centric tasks, showing performance gains but highlighting dependencies on semantic alignment, task visual groundability, and T2I model quality.


<details>
  <summary>Details</summary>
Motivation: Address the modality gap between abundant text data and powerful multimodal models by leveraging T2I-generated images for text tasks.

Method: Systematic evaluation of T2I-generated images in text classification, analyzing T2I model quality, prompt engineering, and fusion architectures.

Result: Synthetic perception via T2I models improves performance but depends on semantic alignment, task visual groundability, and T2I fidelity.

Conclusion: T2I-generated images can enhance text tasks, but effectiveness is conditional; the work sets a benchmark for this approach.

Abstract: A significant ``modality gap" exists between the abundance of text-only data
and the increasing power of multimodal models. This work systematically
investigates whether images generated on-the-fly by Text-to-Image (T2I) models
can serve as a valuable complementary modality for text-centric tasks. Through
a comprehensive evaluation framework on text classification, we analyze the
impact of critical variables, including T2I model quality, prompt engineering
strategies, and multimodal fusion architectures. Our findings demonstrate that
this``synthetic perception" can yield significant performance gains, even when
augmenting strong large language model baselines. However, we find the
effectiveness of this approach is highly conditional, depending critically on
the semantic alignment between text and the generated image, the inherent
``visual groundability" of the task, and the generative fidelity of the T2I
model. Our work establishes the first rigorous benchmark for this paradigm,
providing a clear analysis of its potential and current limitations, and
demonstrating its viability as a pathway to enrich language understanding in
traditionally unimodal scenarios.

</details>


### [767] [Face-Voice Association for Audiovisual Active Speaker Detection in Egocentric Recordings](https://arxiv.org/pdf/2506.18055)
*Jason Clarke, Yoshihiko Gotoh, Stefan Goetze*

Main category: cs.MM

TL;DR: SL-ASD leverages face-voice associations instead of synchronisation for active speaker detection, achieving comparable performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of synchronisation-based ASD in egocentric recordings (occlusions, motion blur, poor acoustics).

Method: Integrates face-voice association model with transformer-based encoder for dynamic facial weighting, coupled with utterance segmentation.

Result: Performs comparably or better than synchronisation-based methods with fewer parameters.

Conclusion: Flexible biometric associations can replace strict synchronisation in challenging egocentric ASD.

Abstract: Audiovisual active speaker detection (ASD) is conventionally performed by
modelling the temporal synchronisation of acoustic and visual speech cues. In
egocentric recordings, however, the efficacy of synchronisation-based methods
is compromised by occlusions, motion blur, and adverse acoustic conditions. In
this work, a novel framework is proposed that exclusively leverages cross-modal
face-voice associations to determine speaker activity. An existing face-voice
association model is integrated with a transformer-based encoder that
aggregates facial identity information by dynamically weighting each frame
based on its visual quality. This system is then coupled with a front-end
utterance segmentation method, producing a complete ASD system. This work
demonstrates that the proposed system, Self-Lifting for audiovisual active
speaker detection(SL-ASD), achieves performance comparable to, and in certain
cases exceeding, that of parameter-intensive synchronisation-based approaches
with significantly fewer learnable parameters, thereby validating the
feasibility of substituting strict audiovisual synchronisation modelling with
flexible biometric associations in challenging egocentric scenarios.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [768] [Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models](https://arxiv.org/pdf/2506.17686)
*Alican Gok, Oguzhan Buyuksolak, Osman Erman Okman, Murat Saraclar*

Main category: eess.AS

TL;DR: The paper proposes a training scheme for Few-Shot Keyword Spotting (FS-KWS) using self-supervised learning, dimensionality reduction, and knowledge distillation to improve accuracy in resource-constrained edge environments.


<details>
  <summary>Details</summary>
Motivation: Traditional FS-KWS systems struggle with accuracy at low false acceptance rates, especially in edge environments. The goal is to enhance performance with minimal training examples.

Method: Leverages Wav2Vec 2.0 for feature extraction, Sub-center ArcFace loss for training, and attention-based dimensionality reduction. A lightweight ResNet15 student model is trained for deployment.

Result: Achieves 74.1% 10-shot classification accuracy (up from 33.4%) on the GSC dataset at 1% false alarm rate.

Conclusion: The proposed method significantly improves FS-KWS accuracy, making it practical for real-world edge device applications.

Abstract: Keyword Spotting plays a critical role in enabling hands-free interaction for
battery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the
scalability and adaptability challenges of traditional systems by enabling
recognition of custom keywords with only a few examples. However, existing
FS-KWS systems achieve subpar accuracy at desirable false acceptance rates,
particularly in resource-constrained edge environments. To address these
issues, we propose a training scheme that leverages self-supervised learning
models for robust feature extraction, dimensionality reduction, and knowledge
distillation. The teacher model, based on Wav2Vec 2.0 is trained using
Sub-center ArcFace loss, which enhances inter-class separability and
intra-class compactness. To enable efficient deployment on edge devices, we
introduce attention-based dimensionality reduction and train a standard
lightweight ResNet15 student model. We evaluate the proposed approach on the
English portion of the Multilingual Spoken Words Corpus (MSWC) and the Google
Speech Commands (GSC) datasets. Notably, the proposed training method improves
the 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%
false alarm accuracy on the GSC dataset, thus making it significantly
better-suited for a real use case scenario.

</details>


### [769] [Low-resource keyword spotting using contrastively trained transformer acoustic word embeddings](https://arxiv.org/pdf/2506.17690)
*Julian Herreilers, Christiaan Jacobs, Thomas Niesler*

Main category: eess.AS

TL;DR: The paper introduces ContrastiveTransformer for low-resource keyword spotting, outperforming existing methods in Luganda and Bambara.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of keyword spotting in severely under-resourced languages like Bambara and Luganda.

Method: Uses an encoder-only ContrastiveTransformer with NT-Xent loss to optimize acoustic word embeddings.

Result: Outperforms existing AWE approaches, including pre-trained models and DTW baselines, in both languages.

Conclusion: The ContrastiveTransformer is effective for low-resource keyword spotting, offering significant performance improvements.

Abstract: We introduce a new approach, the ContrastiveTransformer, that produces
acoustic word embeddings (AWEs) for the purpose of very low-resource keyword
spotting. The ContrastiveTransformer, an encoder-only model, directly optimises
the embedding space using normalised temperature-scaled cross entropy (NT-Xent)
loss. We use this model to perform keyword spotting for radio broadcasts in
Luganda and Bambara, the latter a severely under-resourced language. We compare
our model to various existing AWE approaches, including those constructed from
large pre-trained self-supervised models, a recurrent encoder which previously
used the NT-Xent loss, and a DTW baseline. We demonstrate that the proposed
contrastive transformer approach offers performance improvements over all
considered existing approaches to very low-resource keyword spotting in both
languages.

</details>


### [770] [Blind Source Separation in Biomedical Signals Using Variational Methods](https://arxiv.org/pdf/2506.18281)
*Yasaman Torabi, Shahram Shirani, James P. Reilly*

Main category: eess.AS

TL;DR: Novel unsupervised method using VAEs to separate overlapping heart and lung sounds without labeled data.


<details>
  <summary>Details</summary>
Motivation: Overlapping heart and lung sounds in clinical settings are hard to separate manually, leading to errors.

Method: VAEs encode mixed signals into a structured latent space and reconstruct individual components probabilistically.

Result: Distinct latent clusters for heart and lung sounds; accurate reconstructions preserving spectral features.

Conclusion: Robust, interpretable solution for blind source separation with potential for diagnostic tools and smart stethoscopes.

Abstract: This study introduces a novel unsupervised approach for separating
overlapping heart and lung sounds using variational autoencoders (VAEs). In
clinical settings, these sounds often interfere with each other, making manual
separation difficult and error-prone. The proposed model learns to encode mixed
signals into a structured latent space and reconstructs the individual
components using a probabilistic decoder, all without requiring labeled data or
prior knowledge of source characteristics. We apply this method to real
recordings obtained from a clinical manikin using a digital stethoscope.
Results demonstrate distinct latent clusters corresponding to heart and lung
sources, as well as accurate reconstructions that preserve key spectral
features of the original signals. The approach offers a robust and
interpretable solution for blind source separation and has potential
applications in portable diagnostic tools and intelligent stethoscope systems.

</details>


### [771] [Infant Cry Emotion Recognition Using Improved ECAPA-TDNN with Multiscale Feature Fusion and Attention Enhancement](https://arxiv.org/pdf/2506.18402)
*Junyu Zhou, Yanxiong Li, Haolin Yu*

Main category: eess.AS

TL;DR: Proposes an improved ECAPA-TDNN for infant cry emotion recognition, achieving 82.20% accuracy with efficient parameters and FLOPs.


<details>
  <summary>Details</summary>
Motivation: Address challenges like subtle emotional variations, noise, and limited data in infant cry recognition, where existing methods lack multi-scale feature integration.

Method: Uses improved ECAPA-TDNN with multi-scale feature fusion and attention enhancement.

Result: Achieves 82.20% accuracy, 1.43 MB parameters, and 0.32 Giga FLOPs, outperforming baselines.

Conclusion: The method is effective for infant cry emotion recognition, with code available for reproducibility.

Abstract: Infant cry emotion recognition is crucial for parenting and medical
applications. It faces many challenges, such as subtle emotional variations,
noise interference, and limited data. The existing methods lack the ability to
effectively integrate multi-scale features and temporal-frequency
relationships. In this study, we propose a method for infant cry emotion
recognition using an improved Emphasized Channel Attention, Propagation and
Aggregation in Time Delay Neural Network (ECAPA-TDNN) with both multi-scale
feature fusion and attention enhancement. Experiments on a public dataset show
that the proposed method achieves accuracy of 82.20%, number of parameters of
1.43 MB and FLOPs of 0.32 Giga. Moreover, our method has advantage over the
baseline methods in terms of accuracy. The code is at
https://github.com/kkpretend/IETMA.

</details>


### [772] [Fully Few-shot Class-incremental Audio Classification Using Multi-level Embedding Extractor and Ridge Regression Classifier](https://arxiv.org/pdf/2506.18406)
*Yongjie Si, Yanxiong Li, Jiaxin Tan, Qianhua He, Il-Youp Kwak*

Main category: eess.AS

TL;DR: The paper addresses Few-shot Class-incremental Audio Classification (FCAC) with scarce training samples, proposing a decoupled model with a frozen embedding extractor and an updated classifier, achieving superior accuracy and lower complexity.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and high collection costs make it challenging to gather abundant training samples for FCAC, prompting the study of Fully FCAC (FFCAC) where both base and incremental classes have few samples.

Method: A decoupled model with a multi-level embedding extractor (audio spectrogram Transformer + fusion module) and ridge regression classifier. The extractor is frozen after base training, while the classifier updates incrementally.

Result: Outperforms current methods in accuracy on three datasets and shows lower complexity.

Conclusion: The proposed FFCAC method is effective for scenarios with limited training samples, balancing accuracy and computational efficiency.

Abstract: In the task of Few-shot Class-incremental Audio Classification (FCAC),
training samples of each base class are required to be abundant to train model.
However, it is not easy to collect abundant training samples for many base
classes due to data scarcity and high collection cost. We discuss a more
realistic issue, Fully FCAC (FFCAC), in which training samples of both base and
incremental classes are only a few. Furthermore, we propose a FFCAC method
using a model which is decoupled into a multi-level embedding extractor and a
ridge regression classifier. The embedding extractor consists of an encoder of
audio spectrogram Transformer and a fusion module, and is trained in the base
session but frozen in all incremental sessions. The classifier is updated
continually in each incremental session. Results on three public datasets show
that our method exceeds current methods in accuracy, and has advantage over
most of them in complexity. The code is at https://github.com/YongjieSi/MAR.

</details>


### [773] [Efficient and Generalizable Speaker Diarization via Structured Pruning of Self-Supervised Models](https://arxiv.org/pdf/2506.18623)
*Jiangyu Han, Petr Plka, Marc Delcroix, Federico Landini, Johan Rohdin, Jan Cernock, Luk Burget*

Main category: eess.AS

TL;DR: The paper presents a method to compress SSL-based speaker diarization models using structured pruning and knowledge distillation, achieving 80% size reduction and 4x faster inference without performance loss.


<details>
  <summary>Details</summary>
Motivation: High computational and memory costs of SSL models like WavLM limit their deployment in real-time and resource-constrained scenarios.

Method: Structured pruning guided by knowledge distillation, with analysis of MACs-based objectives, module-wise and progressive pruning strategies, and training data impact.

Result: Reduced model size by 80%, 4x faster inference, and state-of-the-art performance on diverse datasets, including CHiME-6 without domain adaptation.

Conclusion: The method effectively compresses SSL models for diarization, maintaining performance while improving efficiency, with publicly released models and code.

Abstract: Self-supervised learning (SSL) models such as WavLM have brought substantial
improvements to speaker diarization by providing rich contextual
representations. However, the high computational and memory costs of these
models hinder their deployment in real-time and resource-constrained scenarios.
In this work, we present a comprehensive study on compressing SSL-based
diarization models through structured pruning guided by knowledge distillation.
Building upon our previous work, we extend the analysis to include pruning
objectives based on multiply-accumulate operations (MACs), investigate
module-wise and progressive pruning strategies, and examine the impact of
training data quantity. Experimental results show that our method reduces model
size by up to 80% without degrading performance, achieving up to 4x faster
inference on a single GPU. We further perform large-scale evaluations on a
diverse compound dataset comprising eight public diarization corpora, where our
best pruned model achieves state-of-the-art performance across most conditions.
Additionally, we show strong generalization to the CHiME-6 dataset, attaining
performance comparable to the third-place system in the CHiME-7 challenge
without any domain adaptation. All models and code are publicly released to
support reproducibility and future research.

</details>


### [774] [SongBloom: Coherent Song Generation via Interleaved Autoregressive Sketching and Diffusion Refinement](https://arxiv.org/pdf/2506.07634)
*Chenyu Yang, Shuai Wang, Hangting Chen, Wei Tan, Jianwei Yu, Haizhou Li*

Main category: eess.AS

TL;DR: SongBloom is a novel framework for full-length song generation, combining autoregressive sketching and diffusion-based refinement to achieve coherent and high-fidelity music.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with balancing global coherence and local fidelity in music generation, leading to outputs lacking musicality or coherence.

Method: SongBloom uses an autoregressive diffusion model, gradually extending and refining musical sketches from short to long and coarse to fine-grained.

Result: SongBloom outperforms existing methods in subjective and objective metrics, matching state-of-the-art commercial platforms.

Conclusion: SongBloom effectively integrates semantic and acoustic context, demonstrating superior performance in song generation.

Abstract: Generating music with coherent structure, harmonious instrumental and vocal
elements remains a significant challenge in song generation. Existing language
models and diffusion-based methods often struggle to balance global coherence
with local fidelity, resulting in outputs that lack musicality or suffer from
incoherent progression and mismatched lyrics. This paper introduces
$\textbf{SongBloom}$, a novel framework for full-length song generation that
leverages an interleaved paradigm of autoregressive sketching and
diffusion-based refinement. SongBloom employs an autoregressive diffusion model
that combines the high fidelity of diffusion models with the scalability of
language models. Specifically, it gradually extends a musical sketch from short
to long and refines the details from coarse to fine-grained. The interleaved
generation paradigm effectively integrates prior semantic and acoustic context
to guide the generation process. Experimental results demonstrate that
SongBloom outperforms existing methods across both subjective and objective
metrics and achieves performance comparable to the state-of-the-art commercial
music generation platforms. Audio samples are available on our demo page:
https://cypress-yang.github.io/SongBloom_demo. The code and model weights have
been released on https://github.com/Cypress-Yang/SongBloom .

</details>


### [775] [Handling Numeric Expressions in Automatic Speech Recognition](https://arxiv.org/pdf/2408.00004)
*Christian Huber, Alexander Waibel*

Main category: eess.AS

TL;DR: The paper compares cascaded and end-to-end methods for formatting numeric expressions in ASR transcripts, finding adapted end-to-end models competitive with LLM-based approaches but more efficient.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in context-dependent formatting of numeric expressions (e.g., years vs. timestamps) in ASR transcripts.

Method: Cascaded and end-to-end approaches are compared, with the latter using LLM and TTS for data generation.

Result: LLM-based methods perform well, but adapted end-to-end models match performance with lower latency and cost.

Conclusion: Adapted end-to-end models are a viable alternative to LLM-based methods for numeric expression formatting in ASR.

Abstract: This paper addresses the problem of correctly formatting numeric expressions
in automatic speech recognition (ASR) transcripts. This is challenging since
the expected transcript format depends on the context, e.g., 1945 (year) vs.
19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize
and format numeric expressions such as years, timestamps, currency amounts, and
quantities. For the end-to-end approach, we employed a data generation strategy
using a large language model (LLM) together with a text to speech (TTS) model
to generate adaptation data. The results on our test data set show that while
approaches based on LLMs perform well in recognizing formatted numeric
expressions, adapted end-to-end models offer competitive performance with the
advantage of lower latency and inference cost.

</details>


### [776] [Sequence-to-Sequence Neural Diarization with Automatic Speaker Detection and Representation](https://arxiv.org/pdf/2411.13849)
*Ming Cheng, Yuke Lin, Ming Li*

Main category: eess.AS

TL;DR: A novel Sequence-to-Sequence Neural Diarization (S2SND) framework is proposed for online and offline speaker diarization, addressing speaker detection and representation without prior systems.


<details>
  <summary>Details</summary>
Motivation: To improve speaker diarization by eliminating the need for prior diarization systems and integrating speaker embedding learning within the network.

Method: Uses a sequence-to-sequence architecture to detect speakers and extract embeddings simultaneously, processing audio blockwise and iteratively updating predictions.

Result: Achieves highly accurate diarization performance, functioning effectively as both online and offline systems.

Conclusion: The S2SND framework offers a robust solution for speaker diarization by integrating detection and representation, outperforming traditional methods.

Abstract: This paper proposes a novel Sequence-to-Sequence Neural Diarization (S2SND)
framework to perform online and offline speaker diarization. It is developed
from the sequence-to-sequence architecture of our previous target-speaker voice
activity detection system and then evolves into a new diarization paradigm by
addressing two critical problems. 1) Speaker Detection: The proposed approach
can utilize partially given speaker embeddings to discover the unknown speaker
and predict the target voice activities in the audio signal. It does not
require a prior diarization system for speaker enrollment in advance. 2)
Speaker Representation: The proposed approach can adopt the predicted voice
activities as reference information to extract speaker embeddings from the
audio signal simultaneously. The representation space of speaker embedding is
jointly learned within the whole diarization network without using an extra
speaker embedding model. During inference, the S2SND framework can process long
audio recordings blockwise. The detection module utilizes the previously
obtained speaker-embedding buffer to predict both enrolled and unknown
speakers' voice activities for each coming audio block. Next, the
speaker-embedding buffer is updated according to the predictions of the
representation module. Assuming that up to one new speaker may appear in a
small block shift, our model iteratively predicts the results of each block and
extracts target embeddings for the subsequent blocks until the signal ends.
Finally, the last speaker-embedding buffer can re-score the entire audio,
achieving highly accurate diarization performance as an offline system.
Experimental results show that ...

</details>


### [777] [Meta-learning-based percussion transcription and $t\bar{a}la$ identification from low-resource audio](https://arxiv.org/pdf/2501.04742)
*Rahul Bapusaheb Kodag, Vipul Arora*

Main category: eess.AS

TL;DR: A meta-learning-based approach for low-resource Tabla Stroke Transcription and tla identification in Hindustani classical music, using MAML to handle limited data and label heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of limited annotated datasets and label heterogeneity in music transcription, particularly for low-resource settings.

Method: Utilizes Model-Agnostic Meta-Learning (MAML) for rapid adaptation to new tasks with minimal data. Validated on tabla solo and concert recordings, with novel tla identification techniques based on stroke sequences and rhythmic patterns.

Result: Outperforms existing techniques in low-resource settings, effective for Automatic Drum Transcription (ADT) in both Indian and Western percussion music.

Conclusion: The method significantly contributes to music transcription and computational study of musical traditions, demonstrating robustness in polyphonic audio scenarios.

Abstract: This study introduces a meta-learning-based approach for low-resource Tabla
Stroke Transcription (TST) and $t\bar{a}la$ identification in Hindustani
classical music. Using Model-Agnostic Meta-Learning (MAML), we address the
challenges of limited annotated datasets and label heterogeneity, enabling
rapid adaptation to new tasks with minimal data.
  The method is validated across various datasets, including tabla solo and
concert recordings, demonstrating robustness in polyphonic audio scenarios. We
propose two novel $t\bar{a}la$ identification techniques based on stroke
sequences and rhythmic patterns. Additionally, the approach proves effective
for Automatic Drum Transcription (ADT), showcasing its flexibility for Indian
and Western percussion music. Experimental results show that the proposed
method outperforms existing techniques in low-resource settings, significantly
contributing to music transcription and studying musical traditions through
computational tools.

</details>


### [778] [Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection](https://arxiv.org/pdf/2505.22029)
*Jinming Zhang, Xuanru Zhou, Jiachen Lian, Shuhe Li, William Li, Zoe Ezzes, Rian Bogley, Lisa Wauters, Zachary Miller, Jet Vonk, Brittany Morin, Maria Gorno-Tempini, Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: The paper introduces LLM-Dys, a comprehensive dysfluent speech corpus enhanced by LLM for synthetic dysfluency generation, improving detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for speech dysfluency detection are limited by scarce annotated data and poor synthetic dataset quality.

Method: Proposes LLM-Dys, a dataset with 11 dysfluency categories, and an end-to-end detection framework.

Result: Achieves state-of-the-art performance in dysfluency detection.

Conclusion: The LLM-Dys dataset and framework significantly advance dysfluency detection, with all resources open-sourced.

Abstract: Speech dysfluency detection is crucial for clinical diagnosis and language
assessment, but existing methods are limited by the scarcity of high-quality
annotated data. Although recent advances in TTS model have enabled synthetic
dysfluency generation, existing synthetic datasets suffer from unnatural
prosody and limited contextual diversity. To address these limitations, we
propose LLM-Dys -- the most comprehensive dysfluent speech corpus with
LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency
categories spanning both word and phoneme levels. Building upon this resource,
we improve an end-to-end dysfluency detection framework. Experimental
validation demonstrates state-of-the-art performance. All data, models, and
code are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.

</details>


### [779] [S2ST-Omni: An Efficient and Scalable Multilingual Speech-to-Speech Translation Framework via Seamless Speech-Text Alignment and Streaming Speech Generation](https://arxiv.org/pdf/2506.11160)
*Yu Pan, Yuguang Yang, Yanni Hu, Jianhao Ye, Xiang Zhang, Hongbin Zhou, Lei Ma, Jianjun Zhao*

Main category: eess.AS

TL;DR: S2ST-Omni is a novel framework for multilingual speech-to-speech translation, decomposing the task into speech-to-text and text-to-speech stages, leveraging pretrained models and a lightweight adapter for efficiency.


<details>
  <summary>Details</summary>
Motivation: Challenges in high-quality S2ST and reliance on large parallel speech corpora motivate the need for an efficient, scalable solution.

Method: Decomposes S2ST into S2TT and TTS, uses Whisper and Qwen 3.0 models, introduces a speech adapter, and employs streaming TTS for real-time output.

Result: Outperforms state-of-the-art baselines on the CVSS benchmark in translation quality.

Conclusion: S2ST-Omni is effective and superior for multilingual S2ST, addressing key challenges with innovative methods.

Abstract: Multilingual speech-to-speech translation (S2ST) aims to directly convert
spoken utterances from multiple source languages into fluent and intelligible
speech in a target language. Despite recent progress, several critical
challenges persist: 1) achieving high-quality S2ST remains a significant
obstacle; 2) most existing S2ST methods rely heavily on large-scale parallel
speech corpora, which are difficult and resource-intensive to obtain. To tackle
these challenges, we introduce S2ST-Omni, a novel, efficient, and scalable
framework tailored for multilingual speech-to-speech translation. Specifically,
we decompose S2ST into speech-to-text translation (S2TT) and text-to-speech
synthesis (TTS). To enable high-quality S2TT while mitigating reliance on
large-scale parallel speech corpora, we leverage powerful pretrained models:
Whisper for robust audio understanding and Qwen 3.0 for advanced text
comprehension. A lightweight speech adapter is introduced to bridge the
modality gap between speech and text representations, facilitating effective
utilization of pretrained multimodal knowledge. To ensure both translation
accuracy and real-time responsiveness, we adopt a streaming speech generation
model in the TTS stage, which generates the target speech in an autoregressive
manner. Extensive experiments conducted on the CVSS benchmark demonstrate that
S2ST-Omni consistently surpasses several state-of-the-art S2ST baselines in
translation quality, highlighting its effectiveness and superiority.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [780] [Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights](https://arxiv.org/pdf/2506.17337)
*Yuan Zhong, Ruinan Jin, Xiaoxiao Li, Qi Dou*

Main category: eess.IV

TL;DR: Common VLMs, when fine-tuned, can rival or outperform medical-specific VLMs in medical imaging tasks, offering a scalable and cost-effective alternative.


<details>
  <summary>Details</summary>
Motivation: To determine if efficient fine-tuning of common VLMs (e.g., CLIP, LLaVA) can match the performance of medical-specific VLMs in disease diagnosis and VQA tasks.

Method: Systematic evaluation of common and medical VLMs in in-domain (ID) and out-of-domain (OOD) settings, including off-the-shelf performance, fine-tuning impact, and generalization.

Result: Common VLMs match or surpass medical VLMs after lightweight fine-tuning (e.g., LoRA-based adaptation). They also show strong adaptability in OOD tasks.

Conclusion: Fine-tuned common VLMs provide a scalable, cost-effective alternative to medical-specific pretraining, challenging the necessity of domain-specific models.

Abstract: Medical vision-language models (VLMs) leverage large-scale pretraining for
diverse imaging tasks but require substantial computational and data resources.
Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not
trained for medical use, show promise with fine-tuning. This raises a key
question: Can efficient fine-tuned common VLMs rival generalist medical VLMs
for solving specific medical imaging tasks? This study systematically evaluates
common and medical VLMs across disease diagnosis and visual question answering
(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf
performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges
these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen
medical modalities. While medical-specific pretraining provides advantages in
ID settings, common VLMs match or surpass medical-specific models after
lightweight fine-tuning, with LoRA-based adaptation proving highly effective
among different tasks. In OOD tasks, common VLMs demonstrate strong
adaptability in some tasks, challenging the assumption that medical-specific
pre-training is essential. These findings suggest that leveraging common VLMs
with fine-tuning offers a scalable and cost-effective alternative to developing
large-scale medical VLMs, providing crucial insights for future research in the
medical imaging field.

</details>


### [781] [DSA-NRP: No-Reflow Prediction from Angiographic Perfusion Dynamics in Stroke EVT](https://arxiv.org/pdf/2506.17501)
*Shreeram Athreya, Carlos Olivares, Ameera Ismail, Kambiz Nael, William Speier, Corey Arnold*

Main category: eess.IV

TL;DR: A machine learning framework is introduced to predict no-reflow after endovascular thrombectomy (EVT) for acute ischemic stroke (AIS) using intra-procedural DSA sequences and clinical variables, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the delayed identification of no-reflow, a complication post-EVT, which undermines tissue recovery and worsens outcomes, by enabling immediate prediction.

Method: Retrospective analysis of AIS patients with favorable mTICI scores, using DSA sequences to extract perfusion features for training ML classifiers.

Result: The ML method significantly outperformed clinical-features baselines (AUC: 0.7703 vs. 0.5728; accuracy: 0.8125 vs. 0.6331).

Conclusion: The approach enables real-time, accurate no-reflow prediction, allowing proactive management of high-risk patients without delayed imaging.

Abstract: Following successful large-vessel recanalization via endovascular
thrombectomy (EVT) for acute ischemic stroke (AIS), some patients experience a
complication known as no-reflow, defined by persistent microvascular
hypoperfusion that undermines tissue recovery and worsens clinical outcomes.
Although prompt identification is crucial, standard clinical practice relies on
perfusion magnetic resonance imaging (MRI) within 24 hours post-procedure,
delaying intervention. In this work, we introduce the first-ever machine
learning (ML) framework to predict no-reflow immediately after EVT by
leveraging previously unexplored intra-procedural digital subtraction
angiography (DSA) sequences and clinical variables. Our retrospective analysis
included AIS patients treated at UCLA Medical Center (2011-2024) who achieved
favorable mTICI scores (2b-3) and underwent pre- and post-procedure MRI.
No-reflow was defined as persistent hypoperfusion (Tmax > 6 s) on
post-procedural imaging. From DSA sequences (AP and lateral views), we
extracted statistical and temporal perfusion features from the target
downstream territory to train ML classifiers for predicting no-reflow. Our
novel method significantly outperformed a clinical-features baseline(AUC:
0.7703 $\pm$ 0.12 vs. 0.5728 $\pm$ 0.12; accuracy: 0.8125 $\pm$ 0.10 vs. 0.6331
$\pm$ 0.09), demonstrating that real-time DSA perfusion dynamics encode
critical insights into microvascular integrity. This approach establishes a
foundation for immediate, accurate no-reflow prediction, enabling clinicians to
proactively manage high-risk patients without reliance on delayed imaging.

</details>


### [782] [MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization](https://arxiv.org/pdf/2506.17540)
*Tingting Liu, Yuan Liu, Jinhui Tang, Liyin Yuan, Chengyu Liu, Chunlai Li, Xiubao Sui, Qian Chen*

Main category: eess.IV

TL;DR: A GAN-based framework (MTSIC) for colorizing thermal infrared images using multi-band spectral data and a Transformer network to enhance visual quality and reduce semantic ambiguity.


<details>
  <summary>Details</summary>
Motivation: TIR images lack color and texture, limiting their usability. Existing methods are inadequate due to single-band reliance and poor feature extraction, causing distortion and ambiguity.

Method: Proposes MTSIC, a GAN framework with a multi-stage spectral self-attention Transformer (STformer) and spatial-spectral attention blocks (SARB) for multi-band feature mapping. Uses U-shaped architecture and wavelet blocks for spatial-frequency alignment.

Result: Outperforms traditional methods, improving visual quality and reducing semantic confusion in infrared images.

Conclusion: The MTSIC framework effectively enhances infrared image colorization by leveraging multi-band spectral data and advanced attention mechanisms.

Abstract: Thermal infrared (TIR) images, acquired through thermal radiation imaging,
are unaffected by variations in lighting conditions and atmospheric haze.
However, TIR images inherently lack color and texture information, limiting
downstream tasks and potentially causing visual fatigue. Existing colorization
methods primarily rely on single-band images with limited spectral information
and insufficient feature extraction capabilities, which often result in image
distortion and semantic ambiguity. In contrast, multiband infrared imagery
provides richer spectral data, facilitating the preservation of finer details
and enhancing semantic accuracy. In this paper, we propose a generative
adversarial network (GAN)-based framework designed to integrate spectral
information to enhance the colorization of infrared images. The framework
employs a multi-stage spectral self-attention Transformer network (MTSIC) as
the generator. Each spectral feature is treated as a token for self-attention
computation, and a multi-head self-attention mechanism forms a spatial-spectral
attention residual block (SARB), achieving multi-band feature mapping and
reducing semantic confusion. Multiple SARB units are integrated into a
Transformer-based single-stage network (STformer), which uses a U-shaped
architecture to extract contextual information, combined with multi-scale
wavelet blocks (MSWB) to align semantic information in the spatial-frequency
dual domain. Multiple STformer modules are cascaded to form MTSIC,
progressively optimizing the reconstruction quality. Experimental results
demonstrate that the proposed method significantly outperforms traditional
techniques and effectively enhances the visual quality of infrared images.

</details>


### [783] [LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework for Lossless Compression of Medical Images](https://arxiv.org/pdf/2506.17983)
*Chenyue Song, Chen Hui, Qing Lin, Wei Zhang, Siqiao Li, Shengping Zhang, Haiqi Zhu, Zhixuan Li, Shaohui Liu, Feng Jiang, Xiang Li*

Main category: eess.IV

TL;DR: LVPNet improves lossless medical image compression by using global latent variables and addressing quantization loss, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from posterior collapse and inefficient latent variable use due to uniform latent information distribution in sub-images.

Method: Proposes LVPNet with Global Multi-scale Sensing Module (GMSM) for better latent representations and Quantization Compensation Module (QCM) to reduce quantization errors.

Result: Achieves superior compression efficiency and maintains competitive inference speed on benchmarks.

Conclusion: LVPNet effectively addresses limitations of prior methods, offering improved performance in lossless medical image compression.

Abstract: Autoregressive Initial Bits is a framework that integrates sub-image
autoregression and latent variable modeling, demonstrating its advantages in
lossless medical image compression. However, in existing methods, the image
segmentation process leads to an even distribution of latent variable
information across each sub-image, which in turn causes posterior collapse and
inefficient utilization of latent variables. To deal with these issues, we
propose a prediction-based end-to-end lossless medical image compression method
named LVPNet, leveraging global latent variables to predict pixel values and
encoding predicted probabilities for lossless compression. Specifically, we
introduce the Global Multi-scale Sensing Module (GMSM), which extracts compact
and informative latent representations from the entire image, effectively
capturing spatial dependencies within the latent space. Furthermore, to
mitigate the information loss introduced during quantization, we propose the
Quantization Compensation Module (QCM), which learns the distribution of
quantization errors and refines the quantized features to compensate for
quantization loss. Extensive experiments on challenging benchmarks demonstrate
that our method achieves superior compression efficiency compared to
state-of-the-art lossless image compression approaches, while maintaining
competitive inference speed. The code is at
https://github.com/Anonymity00000/Anonymity-repository/.

</details>


### [784] [Multimodal Medical Image Binding via Shared Text Embeddings](https://arxiv.org/pdf/2506.18072)
*Yunhao Liu, Suyang Xi, Shiqi Liu, Hong Ding, Chicheng Jin, Chenxi Yang, Junjun He, Yiqing Shen*

Main category: eess.IV

TL;DR: MBind is a pre-training framework for aligning multiple medical imaging modalities via a shared text space without paired data, outperforming CLIP-like models in tasks like zero-shot classification.


<details>
  <summary>Details</summary>
Motivation: Medical image analysis needs aligned feature representations across modalities, but explicit paired data is hard to acquire.

Method: MBind fine-tunes CLIP-like models to align modality-specific text embeddings and distills them into a unified shared space.

Result: Achieves state-of-the-art performance in zero-shot, few-shot classification, and cross-modal retrieval tasks.

Conclusion: MBind effectively enables cross-modal alignment in medical analysis without requiring paired data.

Abstract: Medical image analysis increasingly relies on the integration of multiple
imaging modalities to capture complementary anatomical and functional
information, enabling more accurate diagnosis and treatment planning. Achieving
aligned feature representations across these diverse modalities is therefore
important for effective multimodal analysis. While contrastive language-image
pre-training (CLIP) and its variant have enabled image-text alignments, they
require explicitly paired data between arbitrary two modalities, which is
difficult to acquire in medical contexts. To address the gap, we present
Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel
pre-training framework that enables seamless alignment of multiple medical
imaging modalities through a shared text representation space without requiring
explicit paired data between any two medical image modalities. Specifically,
based on the insight that different images can naturally bind with text,
M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text
models to align their modality-specific text embedding space while preserving
their original image-text alignments. Subsequently, we distill these
modality-specific text encoders into a unified model, creating a shared text
embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images
on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves
state-of-the-art performance in zero-shot, few-shot classification and
cross-modal retrieval tasks compared to its CLIP-like counterparts. These
results validate M\textsuperscript{3}Bind's effectiveness in achieving
cross-image-modal alignment for medical analysis.

</details>


### [785] [CT Radiomics-Based Explainable Machine Learning Model for Accurate Differentiation of Malignant and Benign Endometrial Tumors: A Two-Center Study](https://arxiv.org/pdf/2506.18106)
*Tingrui Zhang, Honglin Wu, Zekun Jiang, Yingying Wang, Rui Ye, Huiming Ni, Chang Liu, Jin Cao, Xuan Sun, Rong Shao, Xiaorong Wei, Yingchun Sun*

Main category: eess.IV

TL;DR: A CT radiomics-based explainable ML model was developed to diagnose endometrial cancer (EC) malignancy vs. benignity, achieving high accuracy (AUC 0.96) using Random Forest and SHAP for feature importance.


<details>
  <summary>Details</summary>
Motivation: To improve the diagnosis of EC by leveraging radiomics and explainable ML for better clinical decision-making.

Method: Used 83 EC patients' CT scans, extracted 1132 radiomic features, and tested six ML models. Random Forest was optimal, validated via SHAP, ROC, and DCA.

Result: Random Forest model achieved training AUC 1.00 and testing AUC 0.96, with SHAP identifying significant features (P < 0.05). DCA confirmed clinical utility.

Conclusion: The model is a high-performing, explainable tool for EC diagnosis, aiding in reducing unnecessary interventions.

Abstract: Aimed to develop and validate a CT radiomics-based explainable machine
learning model for diagnosing malignancy and benignity specifically in
endometrial cancer (EC) patients. A total of 83 EC patients from two centers,
including 46 with malignant and 37 with benign conditions, were included, with
data split into a training set (n=59) and a testing set (n=24). The regions of
interest (ROIs) were manually segmented from pre-surgical CT scans, and 1132
radiomic features were extracted from the pre-surgical CT scans using
Pyradiomics. Six explainable machine learning modeling algorithms were
implemented respectively, for determining the optimal radiomics pipeline. The
diagnostic performance of the radiomic model was evaluated by using
sensitivity, specificity, accuracy, precision, F1 score, confusion matrices,
and ROC curves. To enhance clinical understanding and usability, we separately
implemented SHAP analysis and feature mapping visualization, and evaluated the
calibration curve and decision curve. By comparing six modeling strategies, the
Random Forest model emerged as the optimal choice for diagnosing EC, with a
training AUC of 1.00 and a testing AUC of 0.96. SHAP identified the most
important radiomic features, revealing that all selected features were
significantly associated with EC (P < 0.05). Radiomics feature maps also
provide a feasible assessment tool for clinical applications. DCA indicated a
higher net benefit for our model compared to the "All" and "None" strategies,
suggesting its clinical utility in identifying high-risk cases and reducing
unnecessary interventions. In conclusion, the CT radiomics-based explainable
machine learning model achieved high diagnostic performance, which could be
used as an intelligent auxiliary tool for the diagnosis of endometrial cancer.

</details>


### [786] [Transforming H&E images into IHC: A Variance-Penalized GAN for Precision Oncology](https://arxiv.org/pdf/2506.18371)
*Sara Rehmat, Hafeez Ur Rehman*

Main category: eess.IV

TL;DR: A deep learning framework translates H&E-stained images to IHC images for HER2 assessment, improving accuracy and scalability in breast cancer diagnostics.


<details>
  <summary>Details</summary>
Motivation: HER2-positive breast cancer requires precise diagnosis, but current IHC methods are costly and labor-intensive, while H&E lacks specificity.

Method: Modified pyramid pix2pix with a novel variance-based penalty to mitigate GAN mode collapse and enhance structural diversity.

Result: Outperforms state-of-the-art methods in PSNR, SSIM, and FID, especially for HER2-positive (IHC 3+) images.

Conclusion: The model offers a cost-effective, scalable alternative for HER2 diagnostics and shows promise for broader image translation tasks.

Abstract: The overexpression of the human epidermal growth factor receptor 2 (HER2) in
breast cells is a key driver of HER2-positive breast cancer, a highly
aggressive subtype requiring precise diagnosis and targeted therapy.
Immunohistochemistry (IHC) is the standard technique for HER2 assessment but is
costly, labor-intensive, and highly dependent on antibody selection. In
contrast, hematoxylin and eosin (H&E) staining, a routine histopathological
procedure, offers broader accessibility but lacks HER2 specificity. This study
proposes an advanced deep learning-based image translation framework to
generate highfidelity IHC images from H&E-stained tissue samples, enabling
cost-effective and scalable HER2 assessment. By modifying the loss function of
pyramid pix2pix, we mitigate mode collapse, a fundamental limitation in
generative adversarial networks (GANs), and introduce a novel variance-based
penalty that enforces structural diversity in generated images. Our model
particularly excels in translating HER2-positive (IHC 3+) images, which have
remained challenging for existing methods due to their complex morphological
variations. Extensive evaluations on the BCI histopathological dataset
demonstrate that our model surpasses state-of-the-art methods in terms of peak
signal-tonoise ratio (PSNR), structural similarity index (SSIM), and Frechet
Inception Distance (FID), particularly in accurately translating HER2-positive
(IHC 3+) images. Beyond medical imaging, our model exhibits superior
performance in general image-to-image translation tasks, showcasing its
potential across multiple domains. This work marks a significant step toward
AI-driven precision oncology, offering a reliable and efficient alternative to
traditional HER2 diagnostics.

</details>


### [787] [Taming Vision-Language Models for Medical Image Analysis: A Comprehensive Review](https://arxiv.org/pdf/2506.18378)
*Haoneng Lin, Cheng Xu, Jing Qin*

Main category: eess.IV

TL;DR: This review summarizes advances in adapting Vision-Language Models (VLMs) for medical image analysis, addressing challenges like domain gaps and task diversity, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: VLMs show promise for medical image analysis but face challenges like domain gaps and task-specific complexities, necessitating a systematic review.

Method: The review categorizes five VLM adaptation strategies and analyzes them across eleven medical imaging tasks, alongside discussing pretraining, fine-tuning, and prompt learning.

Result: The review highlights current implementations and challenges, providing an open-access repository for further research.

Conclusion: The article aims to guide researchers in leveraging VLMs for medical applications by understanding their capabilities, limitations, and technical barriers.

Abstract: Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in
cross-modal semantic understanding between visual and textual modalities. Given
the intrinsic need for multi-modal integration in clinical applications, VLMs
have emerged as a promising solution for a wide range of medical image analysis
tasks. However, adapting general-purpose VLMs to medical domain poses numerous
challenges, such as large domain gaps, complicated pathological variations, and
diversity and uniqueness of different tasks. The central purpose of this review
is to systematically summarize recent advances in adapting VLMs for medical
image analysis, analyzing current challenges, and recommending promising yet
urgent directions for further investigations. We begin by introducing core
learning strategies for medical VLMs, including pretraining, fine-tuning, and
prompt learning. We then categorize five major VLM adaptation strategies for
medical image analysis. These strategies are further analyzed across eleven
medical imaging tasks to illustrate their current practical implementations.
Furthermore, we analyze key challenges that impede the effective adaptation of
VLMs to clinical applications and discuss potential directions for future
research. We also provide an open-access repository of related literature to
facilitate further research, available at
https://github.com/haonenglin/Awesome-VLM-for-MIA. It is anticipated that this
article can help researchers who are interested in harnessing VLMs in medical
image analysis tasks have a better understanding on their capabilities and
limitations, as well as current technical barriers, to promote their
innovative, robust, and safe application in clinical practice.

</details>


### [788] [SafeClick: Error-Tolerant Interactive Segmentation of Any Medical Volumes via Hierarchical Expert Consensus](https://arxiv.org/pdf/2506.18404)
*Yifan Gao, Jiaxi Sheng, Wenbin Wu, Haoyue Li, Yaoxian Dong, Chaoyang Ge, Feng Yuan, Xin Gao*

Main category: eess.IV

TL;DR: SafeClick is an error-tolerant interactive segmentation method for medical volumes, improving foundation models' performance despite suboptimal prompts.


<details>
  <summary>Details</summary>
Motivation: Foundation models for medical image segmentation rely on prompt quality, which is often suboptimal in clinical settings, affecting reliability and accuracy.

Method: SafeClick uses a hierarchical expert consensus framework with two components: a collaborative expert layer (CEL) for diverse feature representations and a consensus reasoning layer (CRL) for adaptive integration.

Result: Experiments on 15 datasets show SafeClick consistently enhances foundation models, especially with imperfect prompts.

Conclusion: SafeClick transforms segmentation into a robust process, reducing dependency on prompt quality and improving accuracy.

Abstract: Foundation models for volumetric medical image segmentation have emerged as
powerful tools in clinical workflows, enabling radiologists to delineate
regions of interest through intuitive clicks. While these models demonstrate
promising capabilities in segmenting previously unseen anatomical structures,
their performance is strongly influenced by prompt quality. In clinical
settings, radiologists often provide suboptimal prompts, which affects
segmentation reliability and accuracy. To address this limitation, we present
SafeClick, an error-tolerant interactive segmentation approach for medical
volumes based on hierarchical expert consensus. SafeClick operates as a
plug-and-play module compatible with foundation models including SAM 2 and
MedSAM 2. The framework consists of two key components: a collaborative expert
layer (CEL) that generates diverse feature representations through specialized
transformer modules, and a consensus reasoning layer (CRL) that performs
cross-referencing and adaptive integration of these features. This architecture
transforms the segmentation process from a prompt-dependent operation to a
robust framework capable of producing accurate results despite imperfect user
inputs. Extensive experiments across 15 public datasets demonstrate that our
plug-and-play approach consistently improves the performance of base foundation
models, with particularly significant gains when working with imperfect
prompts. The source code is available at
https://github.com/yifangao112/SafeClick.

</details>


### [789] [A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation](https://arxiv.org/pdf/2506.18474)
*Atifa Kalsoom, M. A. Iftikhar, Amjad Ali, Zubair Shah, Shidin Balakrishnan, Hazrat Ali*

Main category: eess.IV

TL;DR: The paper introduces BLCB-CNN, a deep learning pipeline with bi-level class balancing for retinal blood vessel segmentation, achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of retinal blood vessels is challenging due to imbalanced data and varying vessel thickness.

Method: Uses a CNN with bi-level class balancing (vessel/non-vessel and thick/thin vessels) and pre-processing techniques like GCN, CLAHE, and gamma corrections.

Result: Achieves 98.23% AUC, 96.22% accuracy, 81.57% sensitivity, and 97.65% specificity, with cross-validation on STARE images confirming generalization.

Conclusion: BLCB-CNN is effective for retinal vessel segmentation, addressing class imbalance and enhancing contrast.

Abstract: Retinal fundus images provide valuable insights into the human eye's interior
structure and crucial features, such as blood vessels, optic disk, macula, and
fovea. However, accurate segmentation of retinal blood vessels can be
challenging due to imbalanced data distribution and varying vessel thickness.
In this paper, we propose BLCB-CNN, a novel pipeline based on deep learning and
bi-level class balancing scheme to achieve vessel segmentation in retinal
fundus images. The BLCB-CNN scheme uses a Convolutional Neural Network (CNN)
architecture and an empirical approach to balance the distribution of pixels
across vessel and non-vessel classes and within thin and thick vessels. Level-I
is used for vessel/non-vessel balancing and Level-II is used for thick/thin
vessel balancing. Additionally, pre-processing of the input retinal fundus
image is performed by Global Contrast Normalization (GCN), Contrast Limited
Adaptive Histogram Equalization (CLAHE), and gamma corrections to increase
intensity uniformity as well as to enhance the contrast between vessels and
background pixels. The resulting balanced dataset is used for
classification-based segmentation of the retinal vascular tree. We evaluate the
proposed scheme on standard retinal fundus images and achieve superior
performance measures, including an area under the ROC curve of 98.23%, Accuracy
of 96.22%, Sensitivity of 81.57%, and Specificity of 97.65%. We also
demonstrate the method's efficacy through external cross-validation on STARE
images, confirming its generalization ability.

</details>


### [790] [Temporal Neural Cellular Automata: Application to modeling of contrast enhancement in breast MRI](https://arxiv.org/pdf/2506.18720)
*Daniel M. Lang, Richard Osuala, Veronika Spieker, Karim Lekadir, Rickmer Braren, Julia A. Schnabel*

Main category: eess.IV

TL;DR: TeNCA (Temporal Neural Cellular Automata) improves synthetic contrast enhancement in breast MRI by modeling temporal evolution, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Long acquisition times and high costs limit MRI's use in breast imaging. Synthetic contrast enhancement can address this, but current methods lack temporal consistency.

Method: Extends Neural Cellular Automata (NCA) to TeNCA, incorporating adaptive loss computation and iterative training to model physiologically plausible contrast evolution.

Result: TeNCA surpasses state-of-the-art methods in generating images aligned with ground truth post-contrast sequences.

Conclusion: TeNCA offers a robust solution for synthetic contrast enhancement, improving MRI's applicability in breast imaging.

Abstract: Synthetic contrast enhancement offers fast image acquisition and eliminates
the need for intravenous injection of contrast agent. This is particularly
beneficial for breast imaging, where long acquisition times and high cost are
significantly limiting the applicability of magnetic resonance imaging (MRI) as
a widespread screening modality. Recent studies have demonstrated the
feasibility of synthetic contrast generation. However, current state-of-the-art
(SOTA) methods lack sufficient measures for consistent temporal evolution.
Neural cellular automata (NCA) offer a robust and lightweight architecture to
model evolving patterns between neighboring cells or pixels. In this work we
introduce TeNCA (Temporal Neural Cellular Automata), which extends and further
refines NCAs to effectively model temporally sparse, non-uniformly sampled
imaging data. To achieve this, we advance the training strategy by enabling
adaptive loss computation and define the iterative nature of the method to
resemble a physical progression in time. This conditions the model to learn a
physiologically plausible evolution of contrast enhancement. We rigorously
train and test TeNCA on a diverse breast MRI dataset and demonstrate its
effectiveness, surpassing the performance of existing methods in generation of
images that align with ground truth post-contrast sequences.

</details>


### [791] [Auto-Lesion Segmentation with a Novel Intensity Dark Channel Prior for COVID-19 Detection](https://arxiv.org/pdf/2309.12638)
*Basma Jumaa Saleh, Zaid Omar, Vikrant Bhateja, Lila Iznita Izhar*

Main category: eess.IV

TL;DR: A novel CT-based radiomics framework using IDCP and DNN (ALS-IDCP-DNN) achieves high accuracy (98.8%) in classifying COVID-19, non-COVID-19, and normal lung images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional CT features lack specificity in distinguishing COVID-19 from other lung diseases, necessitating a more accurate diagnostic tool.

Method: The model uses radiomic features, IDCP, and DNN for auto-segmentation and classification into COVID-19, non-COVID-19, or normal classes. Resnet-50 was the top-performing classifier.

Result: Resnet-50 achieved 98.8% accuracy, 99% precision, 98% recall, and 98% F1-score, outperforming 10+ state-of-the-art studies.

Conclusion: The model effectively aids radiologists in diagnosing COVID-19, demonstrating superior performance over existing methods.

Abstract: During the COVID-19 pandemic, medical imaging techniques like computed
tomography (CT) scans have demonstrated effectiveness in combating the rapid
spread of the virus. Therefore, it is crucial to conduct research on
computerized models for the detection of COVID-19 using CT imaging. A novel
processing method has been developed, utilizing radiomic features, to assist in
the CT-based diagnosis of COVID-19. Given the lower specificity of traditional
features in distinguishing between different causes of pulmonary diseases, the
objective of this study is to develop a CT-based radiomics framework for the
differentiation of COVID-19 from other lung diseases. The model is designed to
focus on outlining COVID-19 lesions, as traditional features often lack
specificity in this aspect. The model categorizes images into three classes:
COVID-19, non-COVID-19, or normal. It employs enhancement auto-segmentation
principles using intensity dark channel prior (IDCP) and deep neural networks
(ALS-IDCP-DNN) within a defined range of analysis thresholds. A publicly
available dataset comprising COVID-19, normal, and non-COVID-19 classes was
utilized to validate the proposed model's effectiveness. The best performing
classification model, Residual Neural Network with 50 layers (Resnet-50),
attained an average accuracy, precision, recall, and F1-score of 98.8%, 99%,
98%, and 98% respectively. These results demonstrate the capability of our
model to accurately classify COVID-19 images, which could aid radiologists in
diagnosing suspected COVID-19 patients. Furthermore, our model's performance
surpasses that of more than 10 current state-of-the-art studies conducted on
the same dataset.

</details>


### [792] [MOST: MR reconstruction Optimization for multiple downStream Tasks via continual learning](https://arxiv.org/pdf/2409.10394)
*Hwihun Jeong, Se Young Chun, Jongho Lee*

Main category: eess.IV

TL;DR: The paper proposes a continual learning method for MR reconstruction optimized for multiple downstream tasks, outperforming baseline and naive finetuning approaches.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning MR reconstruction methods neglect downstream task performance, leading to degradation when cascaded with task networks.

Method: Integrates replay-based continual learning and image-guided loss to handle multiple sequential downstream tasks, avoiding catastrophic forgetting.

Result: Outperforms reconstruction networks without finetuning, naive finetuning, and conventional continual learning methods.

Conclusion: The proposed method effectively optimizes MR reconstruction for multiple downstream tasks via continual learning.

Abstract: Deep learning-based Magnetic Resonance (MR) reconstruction methods have
focused on generating high-quality images but often overlook the impact on
downstream tasks (e.g., segmentation) that utilize the reconstructed images.
Cascading separately trained reconstruction network and downstream task network
has been shown to introduce performance degradation due to error propagation
and the domain gaps between training datasets. To mitigate this issue,
downstream task-oriented reconstruction optimization has been proposed for a
single downstream task. In this work, we extend the optimization to handle
multiple downstream tasks that are introduced sequentially via continual
learning. The proposed method integrates techniques from replay-based continual
learning and image-guided loss to overcome catastrophic forgetting. Comparative
experiments demonstrated that our method outperformed a reconstruction network
without finetuning, a reconstruction network with na\"ive finetuning, and
conventional continual learning methods. The source code is available at:
https://github.com/SNU-LIST/MOST.

</details>


### [793] [Accurate early detection of Parkinson's disease from SPECT imaging through Convolutional Neural Networks](https://arxiv.org/pdf/2412.05348)
*R. Prashanth*

Main category: eess.IV

TL;DR: Machine learning models using SPECT image features accurately detect early Parkinson's disease (PD) and SWEDD subjects, aiding clinicians in diagnosis.


<details>
  <summary>Details</summary>
Motivation: Early and accurate PD detection is critical for effective treatment, especially for SWEDD subjects misdiagnosed and treated as PD.

Method: Developed machine learning models using features from SPECT images to distinguish PD, SWEDD, and normal subjects.

Result: Models achieved high accuracy in detecting early PD and SWEDD subjects.

Conclusion: These models can assist clinicians in improving PD diagnosis and avoiding misdiagnosis.

Abstract: Early and accurate detection of Parkinson's disease (PD) is a crucial
diagnostic challenge carrying immense clinical significance, for effective
treatment regimens and patient management. For instance, a group of subjects
termed SWEDD who are clinically diagnosed as PD, but show normal Single Photon
Emission Computed Tomography (SPECT) scans, change their diagnosis as non-PD
after few years of follow up, and in the meantime, they are treated with PD
medications which do more harm than good. In this work, machine learning models
are developed using features from SPECT images to detect early PD and SWEDD
subjects from normal. These models were observed to perform with high accuracy.
It is inferred from the study that these diagnostic models carry potential to
help PD clinicians in the diagnostic process

</details>


### [794] [Ultra-high resolution multimodal MRI densely labelled holistic structural brain atlas](https://arxiv.org/pdf/2501.16879)
*Jos V. Manjn, Sergio Morell-Ortega, Marina Ruiz-Perez, Boris Mansencal, Edern Le Bot, Marien Gadea, Enrique Lanuza, Gwenaelle Catheline, Thomas Tourdias, Vincent Planche, Rmi Giraud, Denis Rivire, Jean-Franois Mangin, Nicole Labra-Avila, Roberto Vivo-Hernando, Gregorio Rubio, Fernando Aparici, Maria de la Iglesia-Vaya, Pierrick Coup*

Main category: eess.IV

TL;DR: A novel multimodal brain atlas (holiAtlas) was created using high-resolution MRI from 75 healthy subjects, offering detailed anatomical labels at multiple scales for improved neurological disorder detection.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive, multiscale brain atlas for advancing ultra-high-resolution segmentation and early detection of neurological disorders.

Method: Constructed by averaging T1, T2, and WMn MRI images from 75 subjects using nonlinear registration and symmetric group-wise normalization, with 350 labels from 7 protocols.

Result: A publicly available holistic atlas with consistent multiscale labels, enabling detailed brain structure analysis.

Conclusion: The holiAtlas enhances brain mapping and segmentation, supporting neurological research and early disease detection.

Abstract: In this paper, we introduce a novel structural holistic Atlas (holiAtlas) of
the human brain anatomy based on multimodal and high-resolution MRI that covers
several anatomical levels from the organ to the substructure level, using a new
densely labelled protocol generated from the fusion of multiple local protocols
at different scales. This atlas was constructed by averaging images and
segmentations of 75 healthy subjects from the Human Connectome Project
database. Specifically, MR images of T1, T2 and WMn (White Matter nulled)
contrasts at 0.125 $mm^{3}$ resolution were selected for this project. The
images of these 75 subjects were nonlinearly registered and averaged using
symmetric group-wise normalisation to construct the atlas. At the finest level,
the proposed atlas has 350 different labels derived from 7 distinct delineation
protocols. These labels were grouped at multiple scales, offering a coherent
and consistent holistic representation of the brain across different levels of
detail. This multiscale and multimodal atlas can be used to develop new
ultra-high-resolution segmentation methods, potentially improving the early
detection of neurological disorders. We make it publicly available to the
scientific community.

</details>


### [795] [MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for PET Denoising](https://arxiv.org/pdf/2505.05112)
*Xiaolong Niu, Zanting Ye, Xu Han, Yanchao Huang, Hao Sun, Hubing Wu, Lijun Lu*

Main category: eess.IV

TL;DR: A novel CT-guided multi-dose PET denoising model (MDAA-Diff) improves image quality by integrating anatomical constraints and dose-level adaptation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: High-dose radiotracers for PET imaging pose radiation risks. Existing methods ignore inter-patient variability and CT-derived anatomical constraints.

Method: MDAA-Diff uses a CT-Guided High-frequency Wavelet Attention module for anatomical guidance and a Dose-Adaptive Attention module for dose-level adaptation.

Result: MDAA-Diff outperforms state-of-the-art methods in preserving diagnostic quality under low-dose conditions, validated on 18F-FDG and 68Ga-FAPI datasets.

Conclusion: The proposed model effectively addresses limitations of prior work, offering superior denoising performance for low-dose PET imaging.

Abstract: Acquiring high-quality Positron Emission Tomography (PET) images requires
administering high-dose radiotracers, which increases radiation exposure risks.
Generating standard-dose PET (SPET) from low-dose PET (LPET) has become a
potential solution. However, previous studies have primarily focused on single
low-dose PET denoising, neglecting two critical factors: discrepancies in dose
response caused by inter-patient variability, and complementary anatomical
constraints derived from CT images. In this work, we propose a novel CT-Guided
Multi-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for
multi-dose PET denoising. Our approach integrates anatomical guidance and
dose-level adaptation to achieve superior denoising performance under low-dose
conditions. Specifically, this approach incorporates a CT-Guided High-frequency
Wavelet Attention (HWA) module, which uses wavelet transforms to separate
high-frequency anatomical boundary features from CT images. These extracted
features are then incorporated into PET imaging through an adaptive weighted
fusion mechanism to enhance edge details. Additionally, we propose the
Dose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism
that dynamically integrates dose levels into channel-spatial attention weight
calculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets
demonstrate that MDAA-Diff outperforms state-of-the-art approaches in
preserving diagnostic quality under reduced-dose conditions. Our code is
publicly available.

</details>


### [796] [PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations](https://arxiv.org/pdf/2505.06502)
*Md Rakibul Hasan, Pouria Behnoudfar, Dan MacKinlay, Thomas Poulet*

Main category: eess.IV

TL;DR: PC-SRGAN improves image resolution with physical consistency, outperforming traditional methods in metrics like PSNR and SSIM, even with less training data.


<details>
  <summary>Details</summary>
Motivation: Existing GAN-based SR methods lack physical meaningfulness, limiting their use in scientific applications.

Method: PC-SRGAN ensures physical consistency by integrating numerically justified time integrators and advanced quality metrics.

Result: PC-SRGAN achieves better PSNR and SSIM scores than SRGAN, requiring only 13% of the training data.

Conclusion: PC-SRGAN advances scientific ML by providing reliable, physically consistent results, making it suitable for time-dependent problems and broader research applications.

Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has
revolutionised Super Resolution (SR). However, generated images often lack
physical meaningfulness, which is essential for scientific applications. Our
approach, PC-SRGAN, enhances image resolution while ensuring physical
consistency for interpretable simulations. PC-SRGAN significantly improves both
the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure
compared to conventional methods, even with limited training data (e.g., only
13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments
physically meaningful machine learning, incorporating numerically justified
time integrators and advanced quality metrics. These advancements promise
reliable and causal machine-learning models in scientific domains. A
significant advantage of PC-SRGAN over conventional SR techniques is its
physical consistency, which makes it a viable surrogate model for
time-dependent problems. PC-SRGAN advances scientific machine learning,
offering improved accuracy and efficiency for image processing, enhanced
process understanding, and broader applications to scientific research. We
publicly release the complete source code at
https://github.com/hasan-rakibul/PC-SRGAN.

</details>


### [797] [Multi-contrast laser endoscopy for in vivo gastrointestinal imaging](https://arxiv.org/pdf/2505.10492)
*Taylor L. Bobrow, Mayank Golhar, Suchapa Arayakarnkul, Anthony A. Song, Saowanee Ngamruengphong, Nicholas J. Durr*

Main category: eess.IV

TL;DR: Multi-contrast Laser Endoscopy (MLE) enhances gastrointestinal imaging by improving tissue contrast and revealing complementary features, outperforming white light and narrow band imaging.


<details>
  <summary>Details</summary>
Motivation: White light endoscopy often misses subtle tissue abnormalities due to low contrast. MLE aims to overcome this limitation by providing tunable illumination for better visualization.

Method: MLE uses rapidly tunable spectral, coherent, and directional illumination to enhance chromophore contrast, quantify blood flow, and characterize mucosal topography. Validated via benchtop models and in vivo colonoscopies.

Result: MLE improves contrast by ~3x and color difference by ~5x compared to white light and narrow band imaging, as shown in 31 polyp cases.

Conclusion: MLE is a promising tool for improving gastrointestinal imaging by revealing multiple complementary tissue contrasts and integrating seamlessly into clinical workflows.

Abstract: White light endoscopy is the clinical gold standard for detecting diseases in
the gastrointestinal tract. Most applications involve identifying visual
abnormalities in tissue color, texture, and shape. Unfortunately, the contrast
of these features is often subtle, causing many clinically relevant cases to go
undetected. To overcome this challenge, we introduce Multi-contrast Laser
Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable
spectral, coherent, and directional illumination. We demonstrate three
capabilities of MLE: enhancing tissue chromophore contrast with multispectral
diffuse reflectance, quantifying blood flow using laser speckle contrast
imaging, and characterizing mucosal topography using photometric stereo. We
validate MLE with benchtop models, then demonstrate MLE in vivo during clinical
colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold
improvement in contrast and a five-fold improvement in color difference
compared to white light and narrow band imaging. With the ability to reveal
multiple complementary types of tissue contrast while seamlessly integrating
into the clinical environment, MLE shows promise as an investigative tool to
improve gastrointestinal imaging.

</details>


### [798] [A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer Conversion](https://arxiv.org/pdf/2506.16733)
*Fang Chen, Weifeng Zhang, Xingyu Ai, BingXuan Li, An Li, Qiegen Liu*

Main category: eess.IV

TL;DR: A prior-guided joint diffusion model (PJDM) transforms 18F-FDG PET sinograms into 18F-DOPA PET sinograms, improving quality and clinical utility.


<details>
  <summary>Details</summary>
Motivation: 18F-DOPA PET is more specific for neuroendocrine tumors but limited by synthesis and transport constraints. PJDM aims to overcome these by leveraging 18F-FDG PET data.

Method: PJDM uses a higher-order hybrid sampler to generate initial synthetic 18F-DOPA sinograms, then iteratively refines them with degradation guidance.

Result: PJDM improved sinogram quality and synthetic outcomes, demonstrating effectiveness.

Conclusion: PJDM offers a promising solution for enhancing 18F-DOPA PET imaging by utilizing existing 18F-FDG PET data.

Abstract: Positron emission tomography (PET) is widely used to assess metabolic
activity, but its application is limited by the availability of radiotracers.
18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but
shows limited effectiveness for certain tumors. In contrast,
6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity
for neuroendocrine tumors and neurological disorders. However, the complexity
of its synthesis process and constraints on transportation time have limited
its clinical application. Among different forms of raw data acquired by the
scanner, sinogram is a commonly used representation in PET imaging. Therefore,
modeling in projection domain enables more direct utilization of the original
information, potentially reducing the accumulation errors during the image
reconstruction process. Inspired by these factors, this study proposes a
prior-guided joint diffusion model (PJDM) for transforming 18F-FDG PET
sinograms into 18F-DOPA PET sinograms. During inference, an initial synthetic
18F-DOPA PET sinogram is first generated using a higher-order hybrid sampler.
This sinogram is then degraded and serves as an additional condition to guide
the iterative refinement process. Experimental results demonstrated that PJDM
effectively improved both sinogram quality and the final synthetic outcomes.
The code is available at: https://github.com/yqx7150/PJDM.

</details>
