{"id": "2505.06685", "pdf": "https://arxiv.org/pdf/2505.06685", "abs": "https://arxiv.org/abs/2505.06685", "authors": ["Dawei Huang", "Qing Li", "Chuan Yan", "Zebang Cheng", "Yurong Huang", "Xiang Li", "Bin Li", "Xiaohui Wang", "Zheng Lian", "Xiaojiang Peng"], "title": "Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Emotion understanding in videos aims to accurately recognize and interpret\nindividuals' emotional states by integrating contextual, visual, textual, and\nauditory cues. While Large Multimodal Models (LMMs) have demonstrated\nsignificant progress in general vision-language (VL) tasks, their performance\nin emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on\nemotion-related tasks often leads to catastrophic forgetting, hindering their\nability to generalize across diverse tasks. To address these challenges, we\npresent Emotion-Qwen, a tailored multimodal framework designed to enhance both\nemotion understanding and general VL reasoning. Emotion-Qwen incorporates a\nsophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,\nwhich dynamically routes inputs to balance emotion-specific and general-purpose\nprocessing. The model is pre-trained in a three-stage pipeline on large-scale\ngeneral and emotional image datasets to support robust multimodal\nrepresentations. Furthermore, we construct the Video Emotion Reasoning (VER)\ndataset, comprising more than 40K bilingual video clips with fine-grained\ndescriptive annotations, to further enrich Emotion-Qwen's emotional reasoning\ncapability. Experimental results demonstrate that Emotion-Qwen achieves\nstate-of-the-art performance on multiple emotion recognition benchmarks, while\nmaintaining competitive results on general VL tasks. Code and models are\navailable at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous."}
{"id": "2505.07164", "pdf": "https://arxiv.org/pdf/2505.07164", "abs": "https://arxiv.org/abs/2505.07164", "authors": ["SangEun Lee", "Yubeen Lee", "Eunil Park"], "title": "EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for Visual Emotion Analysis", "categories": ["cs.MM"], "comment": "Accepted at Workshop and Competition on Affective & Behavior Analysis\n  in-the-wild (ABAW), CVPR 2025, 10 pages, 4 figures, 4 tables", "summary": "Visual emotion analysis, which has gained considerable attention in the field\nof affective computing, aims to predict the dominant emotions conveyed by an\nimage. Despite advancements in visual emotion analysis with the emergence of\nvision-language models, we observed that instruction-tuned vision-language\nmodels and conventional vision models exhibit complementary strengths in visual\nemotion analysis, as vision-language models excel in certain cases, whereas\nvision models perform better in others. This finding highlights the need to\nintegrate these capabilities to enhance the performance of visual emotion\nanalysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned\nvision-language model augmented with a lightweight module distilled from\nconventional vision models. Instead of deploying both models simultaneously,\nwhich incurs high computational costs, we transfer the predictive patterns of a\nconventional vision model into the vision-language model using a knowledge\ndistillation framework. Our approach first fine-tunes a vision-language model\non emotion-specific instruction data and then attaches a distilled module to\nits visual encoder while keeping the vision-language model frozen. Predictions\nfrom the vision language model and the distillation module are effectively\nbalanced by a gate module, which subsequently generates the final outcome.\nExtensive experiments show that EmoVLM-KD achieves state-of-the-art performance\non multiple visual emotion analysis benchmark datasets, outperforming the\nexisting methods while maintaining computational efficiency. The code is\navailable in https://github.com/sange1104/EmoVLM-KD."}
{"id": "2505.06803", "pdf": "https://arxiv.org/pdf/2505.06803", "abs": "https://arxiv.org/abs/2505.06803", "authors": ["Xilin Jiang", "Junkai Wu", "Vishal Choudhari", "Nima Mesgarani"], "title": "Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Audio large language models (LLMs) are considered experts at recognizing\nsound objects, yet their performance relative to LLMs in other sensory\nmodalities, such as visual or audio-visual LLMs, and to humans using their\nears, eyes, or both remains unexplored. To investigate this, we systematically\nevaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio,\nQwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of\ndifferent classes from audio-only, silent video, or sounded video inputs. We\nuncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the\nsensory discrepancy between human ears and eyes. To reduce this gap, we\nintroduce a cross-modal distillation framework, where an LLM in one modality\nserves as the teacher and another as the student, with knowledge transfer in\nsound classes predicted as more challenging to the student by a heuristic\nmodel. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice\nversa, leads to notable improvements, particularly in challenging classes. This\nwork highlights the sensory gap in LLMs from a human-aligned perspective and\nproposes a principled approach to enhancing modality-specific perception in\nmultimodal LLMs."}
{"id": "2505.06416", "pdf": "https://arxiv.org/pdf/2505.06416", "abs": "https://arxiv.org/abs/2505.06416", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "title": "ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents", "categories": ["cs.CL"], "comment": "17 pages", "summary": "Recent advancements in Large Language Models (LLMs) and the introduction of\nthe Model Context Protocol (MCP) have significantly expanded LLM agents'\ncapability to interact dynamically with external tools and APIs. However,\nexisting tool selection frameworks do not integrate MCP servers, instead\nrelying heavily on error-prone manual updates to monolithic local tool\nrepositories, leading to duplication, inconsistencies, and inefficiencies.\nAdditionally, current approaches abstract tool selection before the LLM agent\nis invoked, limiting its autonomy and hindering dynamic re-querying\ncapabilities during multi-turn interactions. To address these issues, we\nintroduce ScaleMCP, a novel tool selection approach that dynamically equips LLM\nagents with a MCP tool retriever, giving agents the autonomy to add tools into\ntheir memory, as well as an auto-synchronizing tool storage system pipeline\nthrough CRUD (create, read, update, delete) operations with MCP servers as the\nsingle source of truth. We also propose a novel embedding strategy, Tool\nDocument Weighted Average (TDWA), designed to selectively emphasize critical\ncomponents of tool documents (e.g. tool name or synthetic questions) during the\nembedding process. Comprehensive evaluations conducted on a created dataset of\n5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models,\nand 5 retriever types, demonstrate substantial improvements in tool retrieval\nand agent invocation performance, emphasizing ScaleMCP's effectiveness in\nscalable, dynamic tool selection and invocation."}
{"id": "2505.07365", "pdf": "https://arxiv.org/pdf/2505.07365", "abs": "https://arxiv.org/abs/2505.07365", "authors": ["Chao-Han Huck Yang", "Sreyan Ghosh", "Qing Wang", "Jaeyeon Kim", "Hengyi Hong", "Sonal Kumar", "Guirui Zhong", "Zhifeng Kong", "S Sakshi", "Vaibhavi Lokegaonkar", "Oriol Nieto", "Ramani Duraiswami", "Dinesh Manocha", "Gunhee Kim", "Jun Du", "Rafael Valle", "Bryan Catanzaro"], "title": "Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "comment": "Preprint. DCASE 2025 Audio QA Challenge:\n  https://dcase.community/challenge2025/task-audio-question-answering", "summary": "We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering\n(AQA) benchmark spanning multiple domains of sound understanding. This task\ndefines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA)\nto test audio-language models on interactive question-answering over diverse\nacoustic scenes. We describe the dataset composition (from marine mammal calls\nto soundscapes and complex real-world clips), the evaluation protocol (top-1\naccuracy with answer-shuffling robustness), and baseline systems\n(Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the\ndevelopment set are compared, showing strong variation across models and\nsubsets. This challenge aims to advance the audio understanding and reasoning\ncapabilities of audio-language models toward human-level acuity, which are\ncrucial for enabling AI agents to perceive and interact about the world\neffectively."}
{"id": "2505.06418", "pdf": "https://arxiv.org/pdf/2505.06418", "abs": "https://arxiv.org/abs/2505.06418", "authors": ["Ming Liu", "Liwen Wang", "Wensheng Zhang"], "title": "Is your multimodal large language model a good science tutor?", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate impressive performance\non scientific reasoning tasks (e.g., ScienceQA). However, most existing\nbenchmarks focus narrowly on the accuracy of the final answer while ignoring\nother metrics. In particular, when applying MLLMs to educational contexts, the\ngoal is not only correctness but also the ability to teach. In this paper, we\npropose a framework that evaluates MLLMs as science tutors using a\ncomprehensive educational rubric and a simulated student model that judges the\nteaching performance of the tutors. Given a list of candidate MLLM science\ntutors, we use rubric-based student judgments to produce a range of tutor\nperformance scores, identifying both strong and weak tutors. Using the training\nsection of the ScienceQA dataset, we then construct a data set of pairwise\ncomparisons between the outputs of strong and weak tutors. This enables us to\napply multiple preference optimization methods to fine-tune an underperforming\ntutor model (Qwen2-VL-2B) into more effective ones. Our results also show that\nstrong problem-solving skills do not guarantee high-quality tutoring and that\nperformance optimization-guided refinements can yield more educationally\naligned tutor models. This approach opens avenues for building MLLMs that serve\nnot only as problem solvers, but as genuinely helpful educational assistants."}
{"id": "2505.07176", "pdf": "https://arxiv.org/pdf/2505.07176", "abs": "https://arxiv.org/abs/2505.07176", "authors": ["Yuntao Wang", "Shaolong Guo", "Yanghe Pan", "Zhou Su", "Fahao Chen", "Tom H. Luan", "Peng Li", "Jiawen Kang", "Dusit Niyato"], "title": "Internet of Agents: Fundamentals, Applications, and Challenges", "categories": ["cs.MA", "cs.AI"], "comment": "22 pages,10 figures, 8 tables. Submitted to IEEE TCCN", "summary": "With the rapid proliferation of large language models and vision-language\nmodels, AI agents have evolved from isolated, task-specific systems into\nautonomous, interactive entities capable of perceiving, reasoning, and acting\nwithout human intervention. As these agents proliferate across virtual and\nphysical environments, from virtual assistants to embodied robots, the need for\na unified, agent-centric infrastructure becomes paramount. In this survey, we\nintroduce the Internet of Agents (IoA) as a foundational framework that enables\nseamless interconnection, dynamic discovery, and collaborative orchestration\namong heterogeneous agents at scale. We begin by presenting a general IoA\narchitecture, highlighting its hierarchical organization, distinguishing\nfeatures relative to the traditional Internet, and emerging applications. Next,\nwe analyze the key operational enablers of IoA, including capability\nnotification and discovery, adaptive communication protocols, dynamic task\nmatching, consensus and conflict-resolution mechanisms, and incentive models.\nFinally, we identify open research directions toward building resilient and\ntrustworthy IoA ecosystems."}
{"id": "2505.06356", "pdf": "https://arxiv.org/pdf/2505.06356", "abs": "https://arxiv.org/abs/2505.06356", "authors": ["Karthik Reddy Kanjula", "Surya Guthikonda", "Nahid Alam", "Shayekh Bin Islam"], "title": "Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA", "categories": ["cs.CV"], "comment": "Accepted at ReGenAI CVPR2025 Workshop as Oral", "summary": "Pretraining datasets are foundational to the development of multimodal\nmodels, yet they often have inherent biases and toxic content from the\nweb-scale corpora they are sourced from. In this paper, we investigate the\nprevalence of toxicity in LLaVA image-text pretraining dataset, examining how\nharmful content manifests in different modalities. We present a comprehensive\nanalysis of common toxicity categories and propose targeted mitigation\nstrategies, resulting in the creation of a refined toxicity-mitigated dataset.\nThis dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training\ndataset. We offer guidelines for implementing robust toxicity detection\npipelines. Our findings underscore the need to actively identify and filter\ntoxic content - such as hate speech, explicit imagery, and targeted harassment\n- to build more responsible and equitable multimodal systems. The\ntoxicity-mitigated dataset is open source and is available for further\nresearch."}
{"id": "2505.06287", "pdf": "https://arxiv.org/pdf/2505.06287", "abs": "https://arxiv.org/abs/2505.06287", "authors": ["Riccardo Sieve", "Paul Kobialka", "Laura Slaughter", "Rudolf Schlatte", "Einar Broch Johnsen", "Silvia Lizeth Tapia Tarifa"], "title": "BedreFlyt: Improving Patient Flows through Hospital Wards with Digital Twins", "categories": ["cs.AI", "cs.ET", "cs.LO", "D.2.2; D.2.4; J.3"], "comment": "In Proceedings ASQAP 2025, arXiv:2505.02873", "summary": "Digital twins are emerging as a valuable tool for short-term decision-making\nas well as for long-term strategic planning across numerous domains, including\nprocess industry, energy, space, transport, and healthcare. This paper reports\non our ongoing work on designing a digital twin to enhance resource planning,\ne.g., for the in-patient ward needs in hospitals. By leveraging executable\nformal models for system exploration, ontologies for knowledge representation\nand an SMT solver for constraint satisfiability, our approach aims to explore\nhypothetical \"what-if\" scenarios to improve strategic planning processes, as\nwell as to solve concrete, short-term decision-making tasks. Our proposed\nsolution uses the executable formal model to turn a stream of arriving\npatients, that need to be hospitalized, into a stream of optimization problems,\ne.g., capturing daily inpatient ward needs, that can be solved by SMT\ntechniques. The knowledge base, which formalizes domain knowledge, is used to\nmodel the needed configuration in the digital twin, allowing the twin to\nsupport both short-term decision-making and long-term strategic planning by\ngenerating scenarios spanning average-case as well as worst-case resource\nneeds, depending on the expected treatment of patients, as well as ranging over\nvariations in available resources, e.g., bed distribution in different rooms.\nWe illustrate our digital twin architecture by considering the problem of bed\nbay allocation in a hospital ward."}
{"id": "2505.06229", "pdf": "https://arxiv.org/pdf/2505.06229", "abs": "https://arxiv.org/abs/2505.06229", "authors": ["Aaqib Ayoub Bhat", "Asif Khan", "M. Mursaleen"], "title": "Neural Network Operator-Based Fractal Approximation: Smoothness Preservation and Convergence Analysis", "categories": ["cs.LG", "cs.NA", "math.NA", "28A80, 41A05, 41A25, 41A29, 41A30, 65D05"], "comment": "18 pages", "summary": "This paper presents a new approach of constructing $\\alpha$-fractal\ninterpolation functions (FIFs) using neural network operators, integrating\nconcepts from approximation theory. Initially, we construct $\\alpha$-fractals\nutilizing neural network-based operators, providing an approach to generating\nfractal functions with interpolation properties. Based on the same foundation,\nwe have developed fractal interpolation functions that utilize only the values\nof the original function at the nodes or partition points, unlike traditional\nmethods that rely on the entire original function.\n  Further, we have constructed \\(\\alpha\\)-fractals that preserve the smoothness\nof functions under certain constraints by employing a four-layered neural\nnetwork operator, ensuring that if \\(f \\in C^{r}[a,b]\\), then the corresponding\nfractal \\(f^{\\alpha} \\in C^{r}[a,b]\\). Furthermore, we analyze the convergence\nof these $\\alpha$-fractals to the original function under suitable conditions.\nThe work uses key approximation theory tools, such as the modulus of continuity\nand interpolation operators, to develop convergence results and uniform\napproximation error bounds."}
{"id": "2505.06671", "pdf": "https://arxiv.org/pdf/2505.06671", "abs": "https://arxiv.org/abs/2505.06671", "authors": ["David Rowe", "Jean-Marc Valin"], "title": "RADE: A Neural Codec for Transmitting Speech over HF Radio Channels", "categories": ["eess.AS", "cs.SD"], "comment": "5 pages", "summary": "Speech compression is commonly used to send voice over radio channels in\napplications such as mobile telephony and two-way push-to-talk (PTT) radio. In\nclassical systems, the speech codec is combined with forward error correction,\nmodulation and radio hardware. In this paper we describe an autoencoder that\nreplaces many of the traditional signal processing elements with a neural\nnetwork. The encoder takes a vocoder feature set (short term spectrum, pitch,\nvoicing), and produces discrete time, but continuously valued quadrature\namplitude modulation (QAM) symbols. We use orthogonal frequency domain\nmultiplexing (OFDM) to send and receive these symbols over high frequency (HF)\nradio channels. The decoder converts received QAM symbols to vocoder features\nsuitable for synthesis. The autoencoder has been trained to be robust to\nadditive Gaussian noise and multipath channel impairments while simultaneously\nmaintaining a Peak To Average Power Ratio (PAPR) of less than 1~dB. Over\nsimulated and real world HF radio channels we have achieved output speech\nintelligibility that clearly surpasses existing analog and digital radio\nsystems over a range of SNRs."}
{"id": "2505.06502", "pdf": "https://arxiv.org/pdf/2505.06502", "abs": "https://arxiv.org/abs/2505.06502", "authors": ["Md Rakibul Hasan", "Pouria Behnoudfar", "Dan MacKinlay", "Thomas Poulet"], "title": "PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations", "categories": ["eess.IV", "cs.CE", "cs.CV", "cs.LG"], "comment": null, "summary": "Machine Learning, particularly Generative Adversarial Networks (GANs), has\nrevolutionised Super Resolution (SR). However, generated images often lack\nphysical meaningfulness, which is essential for scientific applications. Our\napproach, PC-SRGAN, enhances image resolution while ensuring physical\nconsistency for interpretable simulations. PC-SRGAN significantly improves both\nthe Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure\ncompared to conventional methods, even with limited training data (e.g., only\n13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments\nphysically meaningful machine learning, incorporating numerically justified\ntime integrators and advanced quality metrics. These advancements promise\nreliable and causal machine-learning models in scientific domains. A\nsignificant advantage of PC-SRGAN over conventional SR techniques is its\nphysical consistency, which makes it a viable surrogate model for\ntime-dependent problems. PC-SRGAN advances scientific machine learning,\noffering improved accuracy and efficiency for image processing, enhanced\nprocess understanding, and broader applications to scientific research. The\nsource codes and data will be made publicly available at\nhttps://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper."}
{"id": "2505.06766", "pdf": "https://arxiv.org/pdf/2505.06766", "abs": "https://arxiv.org/abs/2505.06766", "authors": ["Yasaman Ahmadiadli", "Xiao-Ping Zhang", "Naimul Khan"], "title": "Beyond Identity: A Generalizable Approach for Deepfake Audio Detection", "categories": ["cs.SD", "eess.AS", "eess.SP"], "comment": "Submitted to IEEE Transactions on Biometrics, Behavior, and Identity\n  Science (T-BIOM)", "summary": "Deepfake audio presents a growing threat to digital security, due to its\npotential for social engineering, fraud, and identity misuse. However, existing\ndetection models suffer from poor generalization across datasets, due to\nimplicit identity leakage, where models inadvertently learn speaker-specific\nfeatures instead of manipulation artifacts. To the best of our knowledge, this\nis the first study to explicitly analyze and address identity leakage in the\naudio deepfake detection domain. This work proposes an identity-independent\naudio deepfake detection framework that mitigates identity leakage by\nencouraging the model to focus on forgery-specific artifacts instead of\noverfitting to speaker traits. Our approach leverages Artifact Detection\nModules (ADMs) to isolate synthetic artifacts in both time and frequency\ndomains, enhancing cross-dataset generalization. We introduce novel dynamic\nartifact generation techniques, including frequency domain swaps, time domain\nmanipulations, and background noise augmentation, to enforce learning of\ndataset-invariant features. Extensive experiments conducted on ASVspoof2019,\nADD 2022, FoR, and In-The-Wild datasets demonstrate that the proposed\nADM-enhanced models achieve F1 scores of 0.230 (ADD 2022), 0.604 (FoR), and\n0.813 (In-The-Wild), consistently outperforming the baseline. Dynamic Frequency\nSwap proves to be the most effective strategy across diverse conditions. These\nfindings emphasize the value of artifact-based learning in mitigating implicit\nidentity leakage for more generalizable audio deepfake detection."}
{"id": "2409.07901", "pdf": "https://arxiv.org/pdf/2409.07901", "abs": "https://arxiv.org/abs/2409.07901", "authors": ["Jiehui Jia", "Huan Zhang", "Jinhua Liang"], "title": "Bridging Discrete and Continuous: A Multimodal Strategy for Complex Emotion Detection", "categories": ["cs.MM"], "comment": null, "summary": "In the domain of human-computer interaction, accurately recognizing and\ninterpreting human emotions is crucial yet challenging due to the complexity\nand subtlety of emotional expressions. This study explores the potential for\ndetecting a rich and flexible range of emotions through a multimodal approach\nwhich integrates facial expressions, voice tones, and transcript from video\nclips. We propose a novel framework that maps variety of emotions in a\nthree-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect\nthe fluctuations and positivity/negativity of emotions to enable a more variety\nand comprehensive representation of emotional states. We employed K-means\nclustering to transit emotions from traditional discrete categorization to a\ncontinuous labeling system and built a classifier for emotion recognition upon\nthis system. The effectiveness of the proposed model is evaluated using the\nMER2024 dataset, which contains culturally consistent video clips from Chinese\nmovies and TV series, annotated with both discrete and open-vocabulary emotion\nlabels. Our experiment successfully achieved the transformation between\ndiscrete and continuous models, and the proposed model generated a more diverse\nand comprehensive set of emotion vocabulary while maintaining strong accuracy."}
{"id": "2505.06496", "pdf": "https://arxiv.org/pdf/2505.06496", "abs": "https://arxiv.org/abs/2505.06496", "authors": ["Erik Nijkamp", "Bo Pang", "Egor Pakhomov", "Akash Gokul", "Jin Qu", "Silvio Savarese", "Yingbo Zhou", "Caiming Xiong"], "title": "xGen-small Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce xGen-small, a family of 4B and 9B Transformer decoder models\noptimized for long-context applications. Our vertically integrated pipeline\nunites domain-balanced, frequency-aware data curation; multi-stage pre-training\nwith quality annealing and length extension to 128k tokens; and targeted\npost-training via supervised fine-tuning, preference learning, and online\nreinforcement learning. xGen-small delivers strong performance across various\ntasks, especially in math and coding domains, while excelling at long context\nbenchmarks."}
{"id": "2505.07207", "pdf": "https://arxiv.org/pdf/2505.07207", "abs": "https://arxiv.org/abs/2505.07207", "authors": ["Chiqiang Liu", "Dazi Li"], "title": "Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning", "categories": ["cs.MA"], "comment": null, "summary": "Cooperative multi-agent reinforcement learning faces significant challenges\nin effectively organizing agent relationships and facilitating information\nexchange, particularly when agents need to adapt their coordination patterns\ndynamically. This paper presents a novel framework that integrates dynamic\nspectral clustering with hypergraph neural networks to enable adaptive group\nformation and efficient information processing in multi-agent systems. The\nproposed framework dynamically constructs and updates hypergraph structures\nthrough spectral clustering on agents' state histories, enabling higher-order\nrelationships to emerge naturally from agent interactions. The hypergraph\nstructure is enhanced with attention mechanisms for selective information\nprocessing, providing an expressive and efficient way to model complex agent\nrelationships. This architecture can be implemented in both value-based and\npolicy-based paradigms through a unified objective combining task performance\nwith structural regularization. Extensive experiments on challenging\ncooperative tasks demonstrate that our method significantly outperforms\nstate-of-the-art approaches in both sample efficiency and final performance."}
{"id": "2505.06370", "pdf": "https://arxiv.org/pdf/2505.06370", "abs": "https://arxiv.org/abs/2505.06370", "authors": ["Adhora Madhuri", "Nusaiba Sobir", "Tasnia Binte Mamun", "Taufiq Hasan"], "title": "LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 5 figures, 6 tables", "summary": "Lung cancer is the leading cause of patient mortality in the world. Early\ndiagnosis of malignant pulmonary nodules in CT images can have a significant\nimpact on reducing disease mortality and morbidity. In this work, we propose\nLMLCC-Net, a novel deep learning framework for classifying nodules from CT scan\nimages using a 3D CNN, considering Hounsfield Unit (HU)-based intensity\nfiltering. Benign and malignant nodules have significant differences in their\nintensity profile of HU, which was not exploited in the literature. Our method\nconsiders the intensity pattern as well as the texture for the prediction of\nmalignancies. LMLCC-Net extracts features from multiple branches that each use\na separate learnable HU-based intensity filtering stage. Various combinations\nof branches and learnable ranges of filters were explored to finally produce\nthe best-performing model. In addition, we propose a semi-supervised learning\nscheme for labeling ambiguous cases and also developed a lightweight model to\nclassify the nodules. The experimental evaluations are carried out on the\nLUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of\n91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of\n91.87%, showing improved performance compared to existing methods. The proposed\nmethod can have a significant impact in helping radiologists in the\nclassification of pulmonary nodules and improving patient care."}
{"id": "2505.06328", "pdf": "https://arxiv.org/pdf/2505.06328", "abs": "https://arxiv.org/abs/2505.06328", "authors": ["Felix Ocker", "Jörg Deigmöller", "Pavel Smirnov", "Julian Eggert"], "title": "A Grounded Memory System For Smart Personal Assistants", "categories": ["cs.AI", "H.3.3; H.3.4; I.2.1; I.2.5; I.2.7; I.2.10; J.3"], "comment": "8 pages, 5 figures, accepted for the ESWC 2025 TEXT2KG workshop", "summary": "A wide variety of agentic AI applications - ranging from cognitive assistants\nfor dementia patients to robotics - demand a robust memory system grounded in\nreality. In this paper, we propose such a memory system consisting of three\ncomponents. First, we combine Vision Language Models for image captioning and\nentity disambiguation with Large Language Models for consistent information\nextraction during perception. Second, the extracted information is represented\nin a memory consisting of a knowledge graph enhanced by vector embeddings to\nefficiently manage relational information. Third, we combine semantic search\nand graph query generation for question answering via Retrieval Augmented\nGeneration. We illustrate the system's working and potential using a real-world\nexample."}
{"id": "2505.06257", "pdf": "https://arxiv.org/pdf/2505.06257", "abs": "https://arxiv.org/abs/2505.06257", "authors": ["Ahsan Adeel"], "title": "Beyond Attention: Toward Machines with Intrinsic Higher Mental States", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Attending to what is relevant is fundamental to both the mammalian brain and\nmodern machine learning models such as Transformers. Yet, determining relevance\nremains a core challenge, traditionally offloaded to learning algorithms like\nbackpropagation. Inspired by recent cellular neurobiological evidence linking\nneocortical pyramidal cells to distinct mental states, this work shows how\nmodels (e.g., Transformers) can emulate high-level perceptual processing and\nawake thought (imagination) states to pre-select relevant information before\napplying attention. Triadic neuronal-level modulation loops among questions\n($Q$), clues (keys, $K$), and hypotheses (values, $V$) enable diverse, deep,\nparallel reasoning chains at the representation level and allow a rapid shift\nfrom initial biases to refined understanding. This leads to orders-of-magnitude\nfaster learning with significantly reduced computational demand (e.g., fewer\nheads, layers, and tokens), at an approximate cost of $\\mathcal{O}(N)$, where\n$N$ is the number of input tokens. Results span reinforcement learning (e.g.,\nCarRacing in a high-dimensional visual setup), computer vision, and natural\nlanguage question answering."}
{"id": "2505.07609", "pdf": "https://arxiv.org/pdf/2505.07609", "abs": "https://arxiv.org/abs/2505.07609", "authors": ["Paul Primus", "Florian Schmid", "Gerhard Widmer"], "title": "TACOS: Temporally-aligned Audio CaptiOnS for Language-Audio Pretraining", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": "submitted to the IEEE Workshop on Applications of Signal Processing\n  to Audio and Acoustics (WASPAA), 2025. Dataset (Zenodo):\n  https://zenodo.org/records/15379789, Implementation (GitHub):\n  https://github.com/OptimusPrimus/tacos", "summary": "Learning to associate audio with textual descriptions is valuable for a range\nof tasks, including pretraining, zero-shot classification, audio retrieval,\naudio captioning, and text-conditioned audio generation. Existing contrastive\nlanguage-audio pretrained models are typically trained using global, clip-level\ndescriptions, which provide only weak temporal supervision. We hypothesize that\nCLAP-like language-audio models - particularly, if they are expected to produce\nframe-level embeddings - can benefit from a stronger temporal supervision. To\nconfirm our hypothesis, we curate a novel dataset of approximately 12,000 audio\nrecordings from Freesound, each annotated with single-sentence free-text\ndescriptions linked to a specific temporal segment in an audio recording. We\nuse large language models to clean these annotations by removing references to\nnon-audible events, transcribed speech, typos, and annotator language bias. We\nfurther propose a frame-wise contrastive training strategy that learns to align\ntext descriptions with temporal regions in an audio recording and demonstrate\nthat our model has better temporal text-audio alignment abilities compared to\nmodels trained only on global captions when evaluated on the AudioSet Strong\nbenchmark. The dataset and our source code are available on Zenodo and GitHub,\nrespectively."}
{"id": "2505.06646", "pdf": "https://arxiv.org/pdf/2505.06646", "abs": "https://arxiv.org/abs/2505.06646", "authors": ["Daniel Strick", "Carlos Garcia", "Anthony Huang"], "title": "Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "12 pages, 4 figures", "summary": "Deep learning for radiologic image analysis is a rapidly growing field in\nbiomedical research and is likely to become a standard practice in modern\nmedicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray\nimages that are classified by the presence or absence of 14 different diseases,\nwe reproduced an algorithm known as CheXNet, as well as explored other\nalgorithms that outperform CheXNet's baseline metrics. Model performance was\nprimarily evaluated using the F1 score and AUC-ROC, both of which are critical\nmetrics for imbalanced, multi-label classification tasks in medical imaging.\nThe best model achieved an average AUC-ROC score of 0.85 and an average F1\nscore of 0.39 across all 14 disease classifications present in the dataset."}
{"id": "2505.07235", "pdf": "https://arxiv.org/pdf/2505.07235", "abs": "https://arxiv.org/abs/2505.07235", "authors": ["Dianwen Ng", "Kun Zhou", "Yi-Wen Chao", "Zhiwei Xiong", "Bin Ma", "Eng Siong Chng"], "title": "Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Achieving high-fidelity audio compression while preserving perceptual quality\nacross diverse content remains a key challenge in Neural Audio Coding (NAC). We\nintroduce MUFFIN, a fully convolutional Neural Psychoacoustic Coding (NPC)\nframework that leverages psychoacoustically guided multi-band frequency\nreconstruction. At its core is a Multi-Band Spectral Residual Vector\nQuantization (MBS-RVQ) module that allocates bitrate across frequency bands\nbased on perceptual salience. This design enables efficient compression while\ndisentangling speaker identity from content using distinct codebooks. MUFFIN\nincorporates a transformer-inspired convolutional backbone and a modified snake\nactivation to enhance resolution in fine-grained spectral regions. Experimental\nresults on multiple benchmarks demonstrate that MUFFIN consistently outperforms\nexisting approaches in reconstruction quality. A high-compression variant\nachieves a state-of-the-art 12.5 Hz rate with minimal loss. MUFFIN also proves\neffective in downstream generative tasks, highlighting its promise as a token\nrepresentation for integration with language models. Audio samples and code are\navailable."}
{"id": "2505.03420", "pdf": "https://arxiv.org/pdf/2505.03420", "abs": "https://arxiv.org/abs/2505.03420", "authors": ["Fei Zhao", "Chengcui Zhang", "Runlin Zhang", "Tianyang Wang", "Xi Li"], "title": "Mitigating Image Captioning Hallucinations in Vision-Language Models", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Hallucinations in vision-language models (VLMs) hinder reliability and\nreal-world applicability, usually stemming from distribution shifts between\npretraining data and test samples. Existing solutions, such as retraining or\nfine-tuning on additional data, demand significant computational resources and\nlabor-intensive data collection, while ensemble-based methods incur additional\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\nnovel test-time adaptation framework using reinforcement learning to mitigate\nhallucinations during inference without retraining or any auxiliary VLMs. By\nupdating only the learnable parameters in the layer normalization of the\nlanguage model (approximately 0.003% of the model parameters), our method\nreduces distribution shifts between test samples and pretraining samples. A\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\noutperforms state-of-the-art baselines with a 68.3% improvement in\nhallucination mitigation, demonstrating its effectiveness."}
{"id": "2505.06538", "pdf": "https://arxiv.org/pdf/2505.06538", "abs": "https://arxiv.org/abs/2505.06538", "authors": ["Xinyue Lou", "You Li", "Jinan Xu", "Xiangyu Shi", "Chi Chen", "Kaiyu Huang"], "title": "Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "The rapid development of multimodal large reasoning models (MLRMs) has\ndemonstrated broad application potential, yet their safety and reliability\nremain critical concerns that require systematic exploration. To address this\ngap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs\nacross 5 benchmarks and unveil prevalent safety degradation phenomena in most\nadvanced models. Moreover, our analysis reveals distinct safety patterns across\ndifferent benchmarks: significant safety degradation is observed across\njailbreak robustness benchmarks, whereas safety-awareness benchmarks\ndemonstrate less pronounced degradation. In particular, a long thought process\nin some scenarios even enhances safety performance. Therefore, it is a\npotential approach to addressing safety issues in MLRMs by leveraging the\nintrinsic reasoning capabilities of the model to detect unsafe intent. To\noperationalize this insight, we construct a multimodal tuning dataset that\nincorporates a safety-oriented thought process. Experimental results from\nfine-tuning existing MLRMs with this dataset effectively enhances the safety on\nboth jailbreak robustness and safety-awareness benchmarks. This study provides\na new perspective for developing safe MLRMs. Our dataset is available at\nhttps://github.com/xinyuelou/Think-in-Safety."}
{"id": "2505.07532", "pdf": "https://arxiv.org/pdf/2505.07532", "abs": "https://arxiv.org/abs/2505.07532", "authors": ["Kajetan Rachwał", "Maciej Majek", "Bartłomiej Boczek", "Kacper Dąbrowski", "Paweł Liberadzki", "Adam Dąbrowski", "Maria Ganzha"], "title": "RAI: Flexible Agent Framework for Embodied AI", "categories": ["cs.MA"], "comment": "12 pages, 8 figures, submitted to 23rd International Conference on\n  Practical applications of Agents and Multi-Agent Systems (PAAMS'25)", "summary": "With an increase in the capabilities of generative language models, a growing\ninterest in embodied AI has followed. This contribution introduces RAI - a\nframework for creating embodied Multi Agent Systems for robotics. The proposed\nframework implements tools for Agents' integration with robotic stacks, Large\nLanguage Models, and simulations. It provides out-of-the-box integration with\nstate-of-the-art systems like ROS 2. It also comes with dedicated mechanisms\nfor the embodiment of Agents. These mechanisms have been tested on a physical\nrobot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid\nprototyping. Furthermore, these mechanisms have been deployed in two\nsimulations: (1) robot arm manipulator and (2) tractor controller. All of these\ndeployments have been evaluated in terms of their control capabilities,\neffectiveness of embodiment, and perception ability. The proposed framework has\nbeen used successfully to build systems with multiple agents. It has\ndemonstrated effectiveness in all the aforementioned tasks. It also enabled\nidentifying and addressing the shortcomings of the generative models used for\nembodied AI."}
{"id": "2505.06381", "pdf": "https://arxiv.org/pdf/2505.06381", "abs": "https://arxiv.org/abs/2505.06381", "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"], "title": "Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal", "categories": ["cs.CV"], "comment": null, "summary": "Medical disease prediction, particularly through imaging, remains a\nchallenging task due to the complexity and variability of medical data,\nincluding noise, ambiguity, and differing image quality. Recent deep learning\nmodels, including Knowledge Distillation (KD) methods, have shown promising\nresults in brain tumor image identification but still face limitations in\nhandling uncertainty and generalizing across diverse medical conditions.\nTraditional KD methods often rely on a context-unaware temperature parameter to\nsoften teacher model predictions, which does not adapt effectively to varying\nuncertainty levels present in medical images. To address this issue, we propose\na novel framework that integrates Ant Colony Optimization (ACO) for optimal\nteacher-student model selection and a novel context-aware predictor approach\nfor temperature scaling. The proposed context-aware framework adjusts the\ntemperature based on factors such as image quality, disease complexity, and\nteacher model confidence, allowing for more robust knowledge transfer.\nAdditionally, ACO efficiently selects the most appropriate teacher-student\nmodel pair from a set of pre-trained models, outperforming current optimization\nmethods by exploring a broader solution space and better handling complex,\nnon-linear relationships within the data. The proposed framework is evaluated\nusing three publicly available benchmark datasets, each corresponding to a\ndistinct medical imaging task. The results demonstrate that the proposed\nframework significantly outperforms current state-of-the-art methods, achieving\ntop accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on\nthe Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced\nperformance is further evidenced by the improved results, surpassing existing\nbenchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet)."}
{"id": "2505.06438", "pdf": "https://arxiv.org/pdf/2505.06438", "abs": "https://arxiv.org/abs/2505.06438", "authors": ["Yankai Zeng", "Gopal Gupta"], "title": "Reliable Collaborative Conversational Agent System Based on LLMs and Answer Set Programming", "categories": ["cs.AI"], "comment": "14 pages", "summary": "As the Large-Language-Model-driven (LLM-driven) Artificial Intelligence (AI)\nbots became popular, people realized their strong potential in Task-Oriented\nDialogue (TOD). However, bots relying wholly on LLMs are unreliable in their\nknowledge, and whether they can finally produce a correct result for the task\nis not guaranteed. The collaboration among these agents also remains a\nchallenge, since the necessary information to convey is unclear, and the\ninformation transfer is by prompts -- unreliable, and malicious knowledge is\neasy to inject. With the help of logic programming tools such as Answer Set\nProgramming (ASP), conversational agents can be built safely and reliably, and\ncommunication among the agents made more efficient and secure. We proposed an\nAdministrator-Assistant Dual-Agent paradigm, where the two ASP-driven bots\nshare the same knowledge base and complete their tasks independently, while the\ninformation can be passed by a Collaborative Rule Set (CRS). The knowledge and\ninformation conveyed are encapsulated and invisible to the users, ensuring the\nsecurity of information transmission. We have constructed AutoManager, a\ndual-agent system for managing the drive-through window of a fast-food\nrestaurant such as Taco Bell in the US. In AutoManager, the assistant bot takes\nthe customer's order while the administrator bot manages the menu and food\nsupply. We evaluated our AutoManager and compared it with the real-world Taco\nBell Drive-Thru AI Order Taker, and the results show that our method is more\nreliable."}
{"id": "2505.06258", "pdf": "https://arxiv.org/pdf/2505.06258", "abs": "https://arxiv.org/abs/2505.06258", "authors": ["Zhiyu Zhu", "Jiayu Zhang", "Zhibo Jin", "Fang Chen", "Jianlong Zhou"], "title": "ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Attribution algorithms are essential for enhancing the interpretability and\ntrustworthiness of deep learning models by identifying key features driving\nmodel decisions. Existing frameworks, such as InterpretDL and OmniXAI,\nintegrate multiple attribution methods but suffer from scalability limitations,\nhigh coupling, theoretical constraints, and lack of user-friendly\nimplementations, hindering neural network transparency and interoperability. To\naddress these challenges, we propose Attribution-Based Explainability (ABE), a\nunified framework that formalizes Fundamental Attribution Methods and\nintegrates state-of-the-art attribution algorithms while ensuring compliance\nwith attribution axioms. ABE enables researchers to develop novel attribution\ntechniques and enhances interpretability through four customizable modules:\nRobustness, Interpretability, Validation, and Data & Model. This framework\nprovides a scalable, extensible foundation for advancing attribution-based\nexplainability and fostering transparent AI systems. Our code is available at:\nhttps://github.com/LMBTough/ABE-XAI."}
{"id": "2505.07615", "pdf": "https://arxiv.org/pdf/2505.07615", "abs": "https://arxiv.org/abs/2505.07615", "authors": ["Riccardo Passoni", "Francesca Ronchini", "Luca Comanducci", "Romain Serizel", "Fabio Antonacci"], "title": "Diffused Responsibility: Analyzing the Energy Consumption of Generative Text-to-Audio Diffusion Models", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "comment": null, "summary": "Text-to-audio models have recently emerged as a powerful technology for\ngenerating sound from textual descriptions. However, their high computational\ndemands raise concerns about energy consumption and environmental impact. In\nthis paper, we conduct an analysis of the energy usage of 7 state-of-the-art\ntext-to-audio diffusion-based generative models, evaluating to what extent\nvariations in generation parameters affect energy consumption at inference\ntime. We also aim to identify an optimal balance between audio quality and\nenergy consumption by considering Pareto-optimal solutions across all selected\nmodels. Our findings provide insights into the trade-offs between performance\nand environmental impact, contributing to the development of more efficient\ngenerative audio models."}
{"id": "2505.06793", "pdf": "https://arxiv.org/pdf/2505.06793", "abs": "https://arxiv.org/abs/2505.06793", "authors": ["Erik Großkopf", "Valay Bundele", "Mehran Hossienzadeh", "Hendrik P. A. Lensch"], "title": "HistDiST: Histopathological Diffusion-based Stain Transfer", "categories": ["eess.IV", "cs.CV"], "comment": "8 pages, 4 figures", "summary": "Hematoxylin and Eosin (H&E) staining is the cornerstone of histopathology but\nlacks molecular specificity. While Immunohistochemistry (IHC) provides\nmolecular insights, it is costly and complex, motivating H&E-to-IHC translation\nas a cost-effective alternative. Existing translation methods are mainly\nGAN-based, often struggling with training instability and limited structural\nfidelity, while diffusion-based approaches remain underexplored. We propose\nHistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity\nH&E-to-IHC translation. HistDiST introduces a dual-conditioning strategy,\nutilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&E\nrepresentations to ensure pathology-relevant context and structural\nconsistency. To overcome brightness biases, we incorporate a rescaled noise\nschedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition\nat the final timestep. During inference, DDIM inversion preserves the\nmorphological structure, while an eta-cosine noise schedule introduces\ncontrolled stochasticity, balancing structural consistency and molecular\nfidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel\npathology-aware metric leveraging GigaPath embeddings to assess molecular\nrelevance. Extensive evaluations on MIST and BCI datasets demonstrate that\nHistDiST significantly outperforms existing methods, achieving a 28%\nimprovement in MRA on the H&E-to-Ki67 translation task, highlighting its\neffectiveness in capturing true IHC semantics."}
{"id": "2505.07280", "pdf": "https://arxiv.org/pdf/2505.07280", "abs": "https://arxiv.org/abs/2505.07280", "authors": ["Navid Falah", "Behnam Yousefimehr", "Mehdi Ghatee"], "title": "Predicting Music Track Popularity by Convolutional Neural Networks on Spotify Features and Spectrogram of Audio Waveform", "categories": ["cs.SD", "cs.AI", "68T05, 68T10, 68T37", "I.2.6; I.2.1"], "comment": "12 pages, 6 figures, 4 tables", "summary": "In the digital streaming landscape, it's becoming increasingly challenging\nfor artists and industry experts to predict the success of music tracks. This\nstudy introduces a pioneering methodology that uses Convolutional Neural\nNetworks (CNNs) and Spotify data analysis to forecast the popularity of music\ntracks. Our approach takes advantage of Spotify's wide range of features,\nincluding acoustic attributes based on the spectrogram of audio waveform,\nmetadata, and user engagement metrics, to capture the complex patterns and\nrelationships that influence a track's popularity. Using a large dataset\ncovering various genres and demographics, our CNN-based model shows impressive\neffectiveness in predicting the popularity of music tracks. Additionally, we've\nconducted extensive experiments to assess the strength and adaptability of our\nmodel across different musical styles and time periods, with promising results\nyielding a 97\\% F1 score. Our study not only offers valuable insights into the\ndynamic landscape of digital music consumption but also provides the music\nindustry with advanced predictive tools for assessing and predicting the\nsuccess of music tracks."}
{"id": "2105.00335", "pdf": "https://arxiv.org/pdf/2105.00335", "abs": "https://arxiv.org/abs/2105.00335", "authors": ["Prateek Verma", "Jonathan Berger"], "title": "Audio Transformers", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "comment": "5 pages, 4 figures; Under review WASPAA 2021; Typo Fixes", "summary": "Over the past two decades, CNN architectures have produced compelling models\nof sound perception and cognition, learning hierarchical organizations of\nfeatures. Analogous to successes in computer vision, audio feature\nclassification can be optimized for a particular task of interest, over a wide\nvariety of datasets and labels. In fact similar architectures designed for\nimage understanding have proven effective for acoustic scene analysis. Here we\npropose applying Transformer based architectures without convolutional layers\nto raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200\ncategories, our model outperforms convolutional models to produce state of the\nart results. This is significant as unlike in natural language processing and\ncomputer vision, we do not perform unsupervised pre-training for outperforming\nconvolutional architectures. On the same training set, with respect mean\naver-age precision benchmarks, we show a significant improvement. We further\nimprove the performance of Transformer architectures by using techniques such\nas pooling inspired from convolutional net-work designed in the past few years.\nIn addition, we also show how multi-rate signal processing ideas inspired from\nwavelets, can be applied to the Transformer embeddings to improve the results.\nWe also show how our models learns a non-linear non constant band-width\nfilter-bank, which shows an adaptable time frequency front end representation\nfor the task of audio understanding, different from other tasks e.g. pitch\nestimation."}
{"id": "2505.06548", "pdf": "https://arxiv.org/pdf/2505.06548", "abs": "https://arxiv.org/abs/2505.06548", "authors": ["Aniruddha Roy", "Pretam Ray", "Abhilash Nandy", "Somak Aditya", "Pawan Goyal"], "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback", "categories": ["cs.CL"], "comment": "11 pages", "summary": "Instruction-based Large Language Models (LLMs) have proven effective in\nnumerous few-shot or zero-shot Natural Language Processing (NLP) tasks.\nHowever, creating human-annotated instruction data is time-consuming,\nexpensive, and often limited in quantity and task diversity. Previous research\nendeavors have attempted to address this challenge by proposing frameworks\ncapable of generating instructions in a semi-automated and task-agnostic manner\ndirectly from the model itself. Many of these efforts have relied on large\nAPI-only parameter-based models such as GPT-3.5 (175B), which are expensive,\nand subject to limits on a number of queries. This paper explores the\nperformance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,\nand Mistral 7B, using a semi-automated framework, thereby reducing human\nintervention, effort, and cost required to generate an instruction dataset for\nfine-tuning LLMs. Furthermore, we demonstrate that incorporating a\nReinforcement Learning (RL) based training algorithm into this LLMs-based\nframework leads to further enhancements. Our evaluation of the dataset reveals\nthat these RL-based frameworks achieve a substantial improvements in 63-66% of\nthe tasks compared to previous approaches."}
{"id": "2505.06409", "pdf": "https://arxiv.org/pdf/2505.06409", "abs": "https://arxiv.org/abs/2505.06409", "authors": ["Krti Tallam"], "title": "Engineering Risk-Aware, Security-by-Design Frameworks for Assurance of Large-Scale Autonomous AI Models", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "As AI models scale to billions of parameters and operate with increasing\nautonomy, ensuring their safe, reliable operation demands engineering-grade\nsecurity and assurance frameworks. This paper presents an enterprise-level,\nrisk-aware, security-by-design approach for large-scale autonomous AI systems,\nintegrating standardized threat metrics, adversarial hardening techniques, and\nreal-time anomaly detection into every phase of the development lifecycle. We\ndetail a unified pipeline - from design-time risk assessments and secure\ntraining protocols to continuous monitoring and automated audit logging - that\ndelivers provable guarantees of model behavior under adversarial and\noperational stress. Case studies in national security, open-source model\ngovernance, and industrial automation demonstrate measurable reductions in\nvulnerability and compliance overhead. Finally, we advocate cross-sector\ncollaboration - uniting engineering teams, standards bodies, and regulatory\nagencies - to institutionalize these technical safeguards within a resilient,\nend-to-end assurance ecosystem for the next generation of AI."}
{"id": "2505.06389", "pdf": "https://arxiv.org/pdf/2505.06389", "abs": "https://arxiv.org/abs/2505.06389", "authors": ["Adrien Chan-Hon-Tong", "Aurélien Plyer", "Baptiste Cadalen", "Laurent Serre"], "title": "Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms", "categories": ["cs.CV"], "comment": null, "summary": "Sensor-based guidance is required for long-range platforms. To bypass the\nstructural limitation of classical registration on reference image framework,\nwe offer in this paper to encode a stack of images of the scene into a deep\nnetwork. Relying on a stack is showed to be relevant on bimodal scene (e.g.\nwhen the scene can or can not be snowy)."}
{"id": "2505.06464", "pdf": "https://arxiv.org/pdf/2505.06464", "abs": "https://arxiv.org/abs/2505.06464", "authors": ["Tamara Paris", "AJung Moon", "Jin Guo"], "title": "Opening the Scope of Openness in AI", "categories": ["cs.AI"], "comment": "To appear in ACM Conference on Fairness, Accountability, and\n  Transparency (ACM FAccT) 2025", "summary": "The concept of openness in AI has so far been heavily inspired by the\ndefinition and community practice of open source software. This positions\nopenness in AI as having positive connotations; it introduces assumptions of\ncertain advantages, such as collaborative innovation and transparency. However,\nthe practices and benefits of open source software are not fully transferable\nto AI, which has its own challenges. Framing a notion of openness tailored to\nAI is crucial to addressing its growing societal implications, risks, and\ncapabilities. We argue that considering the fundamental scope of openness in\ndifferent disciplines will broaden discussions, introduce important\nperspectives, and reflect on what openness in AI should mean. Toward this goal,\nwe qualitatively analyze 98 concepts of openness discovered from topic\nmodeling, through which we develop a taxonomy of openness. Using this taxonomy\nas an instrument, we situate the current discussion on AI openness, identify\ngaps and highlight links with other disciplines. Our work contributes to the\nrecent efforts in framing openness in AI by reflecting principles and practices\nof openness beyond open source software and calls for a more holistic view of\nopenness in terms of actions, system properties, and ethical objectives."}
{"id": "2505.06259", "pdf": "https://arxiv.org/pdf/2505.06259", "abs": "https://arxiv.org/abs/2505.06259", "authors": ["Mattia Setzu", "Riccardo Guidotti"], "title": "Fair Clustering with Clusterlets", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Given their widespread usage in the real world, the fairness of clustering\nmethods has become of major interest. Theoretical results on fair clustering\nshow that fairness enjoys transitivity: given a set of small and fair clusters,\na trivial centroid-based clustering algorithm yields a fair clustering.\nUnfortunately, discovering a suitable starting clustering can be\ncomputationally expensive, rather complex or arbitrary.\n  In this paper, we propose a set of simple \\emph{clusterlet}-based fuzzy\nclustering algorithms that match single-class clusters, optimizing fair\nclustering. Matching leverages clusterlet distance, optimizing for classic\nclustering objectives, while also regularizing for fairness. Empirical results\nshow that simple matching strategies are able to achieve high fairness, and\nthat appropriate parameter tuning allows to achieve high cohesion and low\noverlap."}
{"id": "2505.07631", "pdf": "https://arxiv.org/pdf/2505.07631", "abs": "https://arxiv.org/abs/2505.07631", "authors": ["Kohei Saijo", "Yoshiaki Bando"], "title": "Is MixIT Really Unsuitable for Correlated Sources? Exploring MixIT for Unsupervised Pre-training in Music Source Separation", "categories": ["eess.AS"], "comment": "5 pages, 1 figure, 3 tables", "summary": "In music source separation (MSS), obtaining isolated sources or stems is\nhighly costly, making pre-training on unlabeled data a promising approach.\nAlthough source-agnostic unsupervised learning like mixture-invariant training\n(MixIT) has been explored in general sound separation, they have been largely\noverlooked in MSS due to its implicit assumption of source independence. We\nhypothesize, however, that the difficulty of applying MixIT to MSS arises from\nthe ill-posed nature of MSS itself, where stem definitions are\napplication-dependent and models lack explicit knowledge of what should or\nshould not be separated, rather than from high inter-source correlation. While\nMixIT does not assume any source model and struggles with such ambiguities, our\npreliminary experiments show that it can still separate instruments to some\nextent, suggesting its potential for unsupervised pre-training. Motivated by\nthese insights, this study investigates MixIT-based pre-training for MSS. We\nfirst pre-train a model on in-the-wild, unlabeled data from the Free Music\nArchive using MixIT, and then fine-tune it on MUSDB18 with supervision. Using\nthe band-split TF-Locoformer, one of the state-of-the-art MSS models, we\ndemonstrate that MixIT-based pre-training improves the performance over\ntraining from scratch."}
{"id": "2505.06811", "pdf": "https://arxiv.org/pdf/2505.06811", "abs": "https://arxiv.org/abs/2505.06811", "authors": ["Tan-Hanh Pham", "Ovidiu C. Andronesi", "Xianqi Li", "Kim-Doang Nguyen"], "title": "Missing Data Estimation for MR Spectroscopic Imaging via Mask-Free Deep Learning Methods", "categories": ["eess.IV", "cs.CV"], "comment": "8 pages", "summary": "Magnetic Resonance Spectroscopic Imaging (MRSI) is a powerful tool for\nnon-invasive mapping of brain metabolites, providing critical insights into\nneurological conditions. However, its utility is often limited by missing or\ncorrupted data due to motion artifacts, magnetic field inhomogeneities, or\nfailed spectral fitting-especially in high resolution 3D acquisitions. To\naddress this, we propose the first deep learning-based, mask-free framework for\nestimating missing data in MRSI metabolic maps. Unlike conventional restoration\nmethods that rely on explicit masks to identify missing regions, our approach\nimplicitly detects and estimates these areas using contextual spatial features\nthrough 2D and 3D U-Net architectures. We also introduce a progressive training\nstrategy to enhance robustness under varying levels of data degradation. Our\nmethod is evaluated on both simulated and real patient datasets and\nconsistently outperforms traditional interpolation techniques such as cubic and\nlinear interpolation. The 2D model achieves an MSE of 0.002 and an SSIM of 0.97\nwith 20% missing voxels, while the 3D model reaches an MSE of 0.001 and an SSIM\nof 0.98 with 15% missing voxels. Qualitative results show improved fidelity in\nestimating missing data, particularly in metabolically heterogeneous regions\nand ventricular regions. Importantly, our model generalizes well to real-world\ndatasets without requiring retraining or mask input. These findings demonstrate\nthe effectiveness and broad applicability of mask-free deep learning for MRSI\nrestoration, with strong potential for clinical and research integration."}
{"id": "2505.07701", "pdf": "https://arxiv.org/pdf/2505.07701", "abs": "https://arxiv.org/abs/2505.07701", "authors": ["Biel Tura Vecino", "Adam Gabryś", "Daniel Mątwicki", "Andrzej Pomirski", "Tom Iddon", "Marius Cotescu", "Jaime Lorenzo-Trueba"], "title": "Lightweight End-to-end Text-to-speech Synthesis for low resource on-device applications", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Published as a conference paper at SSW 2023", "summary": "Recent works have shown that modelling raw waveform directly from text in an\nend-to-end (E2E) fashion produces more natural-sounding speech than traditional\nneural text-to-speech (TTS) systems based on a cascade or two-stage approach.\nHowever, current E2E state-of-the-art models are computationally complex and\nmemory-consuming, making them unsuitable for real-time offline on-device\napplications in low-resource scenarios. To address this issue, we propose a\nLightweight E2E-TTS (LE2E) model that generates high-quality speech requiring\nminimal computational resources. We evaluate the proposed model on the LJSpeech\ndataset and show that it achieves state-of-the-art performance while being up\nto $90\\%$ smaller in terms of model parameters and $10\\times$ faster in\nreal-time-factor. Furthermore, we demonstrate that the proposed E2E training\nparadigm achieves better quality compared to an equivalent architecture trained\nin a two-stage approach. Our results suggest that LE2E is a promising approach\nfor developing real-time, high quality, low-resource TTS applications for\non-device applications."}
{"id": "2505.03603", "pdf": "https://arxiv.org/pdf/2505.03603", "abs": "https://arxiv.org/abs/2505.03603", "authors": ["S. Z. Zhou", "Y. B. Wang", "J. F. Wu", "T. Hu", "J. N. Zhang", "Z. J. Li", "Y. Liu"], "title": "PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance."}
{"id": "2505.06552", "pdf": "https://arxiv.org/pdf/2505.06552", "abs": "https://arxiv.org/abs/2505.06552", "authors": ["Doyoung Kim", "Youngjun Lee", "Joeun Kim", "Jihwan Bang", "Hwanjun Song", "Susik Yoon", "Jae-Gil Lee"], "title": "References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Conversational query reformulation (CQR) has become indispensable for\nimproving retrieval in dialogue-based applications. However, existing\napproaches typically rely on reference passages for optimization, which are\nimpractical to acquire in real-world scenarios. To address this limitation, we\nintroduce a novel reference-free preference optimization framework DualReform\nthat generates pseudo reference passages from commonly-encountered\nconversational datasets containing only queries and responses. DualReform\nattains this goal through two key innovations: (1) response-based inference,\nwhere responses serve as proxies to infer pseudo reference passages, and (2)\nresponse refinement via the dual-role of CQR, where a CQR model refines\nresponses based on the shared objectives between response refinement and CQR.\nDespite not relying on reference passages, DualReform achieves 96.9--99.1% of\nthe retrieval accuracy attainable only with reference passages and surpasses\nthe state-of-the-art method by up to 31.6%."}
{"id": "2505.06588", "pdf": "https://arxiv.org/pdf/2505.06588", "abs": "https://arxiv.org/abs/2505.06588", "authors": ["Yu Cheng", "Harun Šiljak"], "title": "Emergent Multi-View Fidelity in Autonomous UAV Swarm Sport Injury Detection", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY", "nlin.AO"], "comment": "Accepted for 2025 8th International Balkan Conference on\n  Communications and Networking (Balkancom)", "summary": "Accurate, real-time collision detection is essential for ensuring player\nsafety and effective refereeing in high-contact sports such as rugby,\nparticularly given the severe risks associated with traumatic brain injuries\n(TBI). Traditional collision-monitoring methods employing fixed cameras or\nwearable sensors face limitations in visibility, coverage, and responsiveness.\nPreviously, we introduced a framework using unmanned aerial vehicles (UAVs) for\nmonitoring and real time kinematics extraction from videos of collision events.\nIn this paper, we show that the strategies operating on the objective of\nensuring at least one UAV captures every incident on the pitch have an emergent\nproperty of fulfilling a stronger key condition for successful kinematics\nextraction. Namely, they ensure that almost all collisions are captured by\nmultiple drones, establishing multi-view fidelity and redundancy, while not\nrequiring any drone-to-drone communication."}
{"id": "2505.06393", "pdf": "https://arxiv.org/pdf/2505.06393", "abs": "https://arxiv.org/abs/2505.06393", "authors": ["Valfride Nascimento", "Gabriel E. Lima", "Rafael O. Ribeiro", "William Robson Schwartz", "Rayson Laroca", "David Menotti"], "title": "Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark", "categories": ["cs.CV"], "comment": "Accepted for publication in the Journal of the Brazilian Computer\n  Society", "summary": "Recent advancements in super-resolution for License Plate Recognition (LPR)\nhave sought to address challenges posed by low-resolution (LR) and degraded\nimages in surveillance, traffic monitoring, and forensic applications. However,\nexisting studies have relied on private datasets and simplistic degradation\nmodels. To address this gap, we introduce UFPR-SR-Plates, a novel dataset\ncontaining 10,000 tracks with 100,000 paired low and high-resolution license\nplate images captured under real-world conditions. We establish a benchmark\nusing multiple sequential LR and high-resolution (HR) images per vehicle --\nfive of each -- and two state-of-the-art models for super-resolution of license\nplates. We also investigate three fusion strategies to evaluate how combining\npredictions from a leading Optical Character Recognition (OCR) model for\nmultiple super-resolved license plates enhances overall performance. Our\nfindings demonstrate that super-resolution significantly boosts LPR\nperformance, with further improvements observed when applying majority\nvote-based fusion techniques. Specifically, the Layout-Aware and\nCharacter-Driven Network (LCDNet) model combined with the Majority Vote by\nCharacter Position (MVCP) strategy led to the highest recognition rates,\nincreasing from 1.7% with low-resolution images to 31.1% with super-resolution,\nand up to 44.7% when combining OCR outputs from five super-resolved images.\nThese findings underscore the critical role of super-resolution and temporal\ninformation in enhancing LPR accuracy under real-world, adverse conditions. The\nproposed dataset is publicly available to support further research and can be\naccessed at: https://valfride.github.io/nascimento2024toward/"}
{"id": "2505.06469", "pdf": "https://arxiv.org/pdf/2505.06469", "abs": "https://arxiv.org/abs/2505.06469", "authors": ["Yumou Wei", "Paulo Carvalho", "John Stamper"], "title": "KCluster: An LLM-based Clustering Approach to Knowledge Component Discovery", "categories": ["cs.AI", "cs.HC"], "comment": "Accepted to the Educational Data Mining (EDM) 2025 conference", "summary": "Educators evaluate student knowledge using knowledge component (KC) models\nthat map assessment questions to KCs. Still, designing KC models for large\nquestion banks remains an insurmountable challenge for instructors who need to\nanalyze each question by hand. The growing use of Generative AI in education is\nexpected only to aggravate this chronic deficiency of expert-designed KC\nmodels, as course engineers designing KCs struggle to keep up with the pace at\nwhich questions are generated. In this work, we propose KCluster, a novel KC\ndiscovery algorithm based on identifying clusters of congruent questions\naccording to a new similarity metric induced by a large language model (LLM).\nWe demonstrate in three datasets that an LLM can create an effective metric of\nquestion similarity, which a clustering algorithm can use to create KC models\nfrom questions with minimal human effort. Combining the strengths of LLM and\nclustering, KCluster generates descriptive KC labels and discovers KC models\nthat predict student performance better than the best expert-designed models\navailable. In anticipation of future work, we illustrate how KCluster can\nreveal insights into difficult KCs and suggest improvements to instruction."}
{"id": "2505.06262", "pdf": "https://arxiv.org/pdf/2505.06262", "abs": "https://arxiv.org/abs/2505.06262", "authors": ["Zara Siddique", "Liam D. Turner", "Luis Espinosa-Anke"], "title": "Dialz: A Python Toolkit for Steering Vectors", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Dialz, a framework for advancing research on steering vectors\nfor open-source LLMs, implemented in Python. Steering vectors allow users to\nmodify activations at inference time to amplify or weaken a 'concept', e.g.\nhonesty or positivity, providing a more powerful alternative to prompting or\nfine-tuning. Dialz supports a diverse set of tasks, including creating\ncontrastive pair datasets, computing and applying steering vectors, and\nvisualizations. Unlike existing libraries, Dialz emphasizes modularity and\nusability, enabling both rapid prototyping and in-depth analysis. We\ndemonstrate how Dialz can be used to reduce harmful outputs such as\nstereotypes, while also providing insights into model behaviour across\ndifferent layers. We release Dialz with full documentation, tutorials, and\nsupport for popular open-source models to encourage further research in safe\nand controllable language generation. Dialz enables faster research cycles and\nfacilitates insights into model interpretability, paving the way for safer,\nmore transparent, and more reliable AI systems."}
{"id": "2505.06660", "pdf": "https://arxiv.org/pdf/2505.06660", "abs": "https://arxiv.org/abs/2505.06660", "authors": ["Junyi Peng", "Takanori Ashihara", "Marc Delcroix", "Tsubasa Ochiai", "Oldrich Plchot", "Shoko Araki", "Jan Černocký"], "title": "TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at ICASSP 2025", "summary": "Self-supervised learning (SSL) models have significantly advanced speech\nprocessing tasks, and several benchmarks have been proposed to validate their\neffectiveness. However, previous benchmarks have primarily focused on\nsingle-speaker scenarios, with less exploration of target-speaker tasks in\nnoisy, multi-talker conditions -- a more challenging yet practical case. In\nthis paper, we introduce the Target-Speaker Speech Processing Universal\nPerformance Benchmark (TS-SUPERB), which includes four widely recognized\ntarget-speaker processing tasks that require identifying the target speaker and\nextracting information from the speech mixture. In our benchmark, the speaker\nembedding extracted from enrollment speech is used as a clue to condition\ndownstream models. The benchmark result reveals the importance of evaluating\nSSL models in target speaker scenarios, demonstrating that performance cannot\nbe easily inferred from related single-speaker tasks. Moreover, by using a\nunified SSL-based target speech encoder, consisting of a speaker encoder and an\nextractor module, we also investigate joint optimization across TS tasks to\nleverage mutual information and demonstrate its effectiveness."}
{"id": "2505.06918", "pdf": "https://arxiv.org/pdf/2505.06918", "abs": "https://arxiv.org/abs/2505.06918", "authors": ["Yanhui Hong", "Nan Wang", "Zhiyi Xia", "Haoyi Tao", "Xi Fang", "Yiming Li", "Jiankun Wang", "Peng Jin", "Xiaochen Cai", "Shengyu Li", "Ziqi Chen", "Zezhong Zhang", "Guolin Ke", "Linfeng Zhang"], "title": "Uni-AIMS: AI-Powered Microscopy Image Analysis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper presents a systematic solution for the intelligent recognition and\nautomatic analysis of microscopy images. We developed a data engine that\ngenerates high-quality annotated datasets through a combination of the\ncollection of diverse microscopy images from experiments, synthetic data\ngeneration and a human-in-the-loop annotation process. To address the unique\nchallenges of microscopy images, we propose a segmentation model capable of\nrobustly detecting both small and large objects. The model effectively\nidentifies and separates thousands of closely situated targets, even in\ncluttered visual environments. Furthermore, our solution supports the precise\nautomatic recognition of image scale bars, an essential feature in quantitative\nmicroscopic analysis. Building upon these components, we have constructed a\ncomprehensive intelligent analysis platform and validated its effectiveness and\npracticality in real-world applications. This study not only advances automatic\nrecognition in microscopy imaging but also ensures scalability and\ngeneralizability across multiple application domains, offering a powerful tool\nfor automated microscopic analysis in interdisciplinary research."}
{"id": "2505.07709", "pdf": "https://arxiv.org/pdf/2505.07709", "abs": "https://arxiv.org/abs/2505.07709", "authors": ["Daniel Haider", "Felix Perfler", "Peter Balazs", "Clara Hollomey", "Nicki Holighaus"], "title": "ISAC: An Invertible and Stable Auditory Filter Bank with Customizable Kernels for ML Integration", "categories": ["cs.SD", "cs.LG"], "comment": "Accepted at the IEEE International Conference on Sampling Theory and\n  Applications (SampTA) 2025", "summary": "This paper introduces ISAC, an invertible and stable, perceptually-motivated\nfilter bank that is specifically designed to be integrated into machine\nlearning paradigms. More precisely, the center frequencies and bandwidths of\nthe filters are chosen to follow a non-linear, auditory frequency scale, the\nfilter kernels have user-defined maximum temporal support and may serve as\nlearnable convolutional kernels, and there exists a corresponding filter bank\nsuch that both form a perfect reconstruction pair. ISAC provides a powerful and\nuser-friendly audio front-end suitable for any application, including\nanalysis-synthesis schemes."}
{"id": "2505.06569", "pdf": "https://arxiv.org/pdf/2505.06569", "abs": "https://arxiv.org/abs/2505.06569", "authors": ["Woosang Lim", "Zekun Li", "Gyuwan Kim", "Sungyoung Ji", "HyeonJung Kim", "Kyuri Choi", "Jin Hyuk Lim", "Kyungpyo Park", "William Yang Wang"], "title": "MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Long-context (LC) Large Language Models (LLMs) combined with\nRetrieval-Augmented Generation (RAG) hold strong potential for complex\nmulti-hop and large-document tasks. However, existing RAG systems often suffer\nfrom imprecise retrieval, incomplete context coverage under constrained context\nwindows, and fragmented information caused by suboptimal context construction.\nWe introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical\nretrieval framework that compresses and partitions documents into\ncoarse-to-fine granularities, then adaptively merges relevant contexts through\nchunk- and document-level expansions in real time. By starting from the\nfinest-level retrieval and progressively incorporating higher-level and broader\ncontext, MacRAG constructs effective query-specific long contexts, optimizing\nboth precision and coverage. Evaluations on the challenging LongBench\nexpansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG\nconsistently surpasses baseline RAG pipelines on single- and multi-step\ngeneration with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish\nMacRAG as an efficient, scalable solution for real-world long-context,\nmulti-hop reasoning. Our code is available at\nhttps://github.com/Leezekun/MacRAG."}
{"id": "2505.06761", "pdf": "https://arxiv.org/pdf/2505.06761", "abs": "https://arxiv.org/abs/2505.06761", "authors": ["Youcef Djenouri", "Nassim Belmecheri", "Tomasz Michalak", "Jan Dubiński", "Ahmed Nabil Belbachir", "Anis Yazidi"], "title": "Learning Graph Representation of Agent Diffuser", "categories": ["cs.LG", "cs.MA"], "comment": "Accepted at AAMAS2025 International Conference on Autonomous Agents\n  and Multiagent Systems", "summary": "Diffusion-based generative models have significantly advanced text-to-image\nsynthesis, demonstrating impressive text comprehension and zero-shot\ngeneralization. These models refine images from random noise based on textual\nprompts, with initial reliance on text input shifting towards enhanced visual\nfidelity over time. This transition suggests that static model parameters might\nnot optimally address the distinct phases of generation. We introduce LGR-AD\n(Learning Graph Representation of Agent Diffusers), a novel multi-agent system\ndesigned to improve adaptability in dynamic computer vision tasks. LGR-AD\nmodels the generation process as a distributed system of interacting agents,\neach representing an expert sub-model. These agents dynamically adapt to\nvarying conditions and collaborate through a graph neural network that encodes\ntheir relationships and performance metrics. Our approach employs a\ncoordination mechanism based on top-$k$ maximum spanning trees, optimizing the\ngeneration process. Each agent's decision-making is guided by a meta-model that\nminimizes a novel loss function, balancing accuracy and diversity. Theoretical\nanalysis and extensive empirical evaluations show that LGR-AD outperforms\ntraditional diffusion models across various benchmarks, highlighting its\npotential for scalable and flexible solutions in complex image generation\ntasks. Code is available at: https://github.com/YousIA/LGR_AD"}
{"id": "2505.06411", "pdf": "https://arxiv.org/pdf/2505.06411", "abs": "https://arxiv.org/abs/2505.06411", "authors": ["Fangyu Du", "Yang Yang", "Xuehao Gao", "Hongye Hou"], "title": "MAGE:A Multi-stage Avatar Generator with Sparse Observations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Inferring full-body poses from Head Mounted Devices, which capture only\n3-joint observations from the head and wrists, is a challenging task with wide\nAR/VR applications. Previous attempts focus on learning one-stage motion\nmapping and thus suffer from an over-large inference space for unobserved body\njoint motions. This often leads to unsatisfactory lower-body predictions and\npoor temporal consistency, resulting in unrealistic or incoherent motion\nsequences. To address this, we propose a powerful Multi-stage Avatar GEnerator\nnamed MAGE that factorizes this one-stage direct motion mapping learning with a\nprogressive prediction strategy. Specifically, given initial 3-joint motions,\nMAGE gradually inferring multi-scale body part poses at different abstract\ngranularity levels, starting from a 6-part body representation and gradually\nrefining to 22 joints. With decreasing abstract levels step by step, MAGE\nintroduces more motion context priors from former prediction stages and thus\nimproves realistic motion completion with richer constraint conditions and less\nambiguity. Extensive experiments on large-scale datasets verify that MAGE\nsignificantly outperforms state-of-the-art methods with better accuracy and\ncontinuity."}
{"id": "2505.06492", "pdf": "https://arxiv.org/pdf/2505.06492", "abs": "https://arxiv.org/abs/2505.06492", "authors": ["Chathurangi Shyalika", "Renjith Prasad", "Alaa Al Ghazo", "Darssan Eswaramoorthi", "Harleen Kaur", "Sara Shree Muthuselvam", "Amit Sheth"], "title": "SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing", "categories": ["cs.AI"], "comment": "8 pages, 8 figures, 4 tables, IEEE Conference on Artificial\n  Intelligence (IEEE CAI) 2025", "summary": "In the dynamic landscape of Industry 4.0, achieving efficiency, precision,\nand adaptability is essential to optimize manufacturing operations. Industries\nsuffer due to supply chain disruptions caused by anomalies, which are being\ndetected by current AI models but leaving domain experts uncertain without\ndeeper insights into these anomalies. Additionally, operational inefficiencies\npersist due to inaccurate production forecasts and the limited effectiveness of\ntraditional AI models for processing complex sensor data. Despite these\nadvancements, existing systems lack the seamless integration of these\ncapabilities needed to create a truly unified solution for enhancing production\nand decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot\ndesigned for advanced reasoning and contextual decision-making to address these\nchallenges. SmartPilot processes multimodal sensor data and is compact to\ndeploy on edge devices. It focuses on three key tasks: anomaly prediction,\nproduction forecasting, and domain-specific question answering. By bridging the\ngap between AI capabilities and real-world industrial needs, SmartPilot\nempowers industries with intelligent decision-making and drives transformative\ninnovation in manufacturing. The demonstration video, datasets, and\nsupplementary materials are available at\nhttps://github.com/ChathurangiShyalika/SmartPilot."}
{"id": "2505.06265", "pdf": "https://arxiv.org/pdf/2505.06265", "abs": "https://arxiv.org/abs/2505.06265", "authors": ["Jacques Peter", "Quentin Bennehard", "Sébastien Heib", "Jean-Luc Hantrais-Gervois", "Frédéric Moëns"], "title": "ONERA's CRM WBPN database for machine learning activities, related regression challenge and first results", "categories": ["cs.LG"], "comment": "16 pages, 9 figures", "summary": "This paper presents a new Computational Fluid Dynamics database, developed at\nONERA, to support the advancement of machine learning techniques for\naerodynamic field prediction. It contains 468 Reynolds-Averaged Navier-Stokes\nsimulations using the Spalart-Allmaras turbulence model, performed on the\nNASA/Boeing Common Research Model wing-body-pylon-nacelle configuration. The\ndatabase spans a wide range of flow conditions, varying Mach number (including\ntransonic regimes), angle of attack (capturing flow separation), and Reynolds\nnumber (based on three stagnation pressures, with one setting matching wind\ntunnel experiments). The quality of the database is assessed, through checking\nthe convergence level of each computation.\n  Based on these data, a regression challenge is defined. It consists in\npredicting the wall distributions of pressure and friction coefficients for\nunseen aerodynamic conditions. The 468 simulations are split into training and\ntesting sets, with the training data made available publicly on the Codabench\nplatform. The paper further evaluates several classical machine learning\nregressors on this task. Tested pointwise methods include Multi-Layer\nPerceptrons, $\\lambda$-DNNs, and Decision Trees, while global methods include\nMulti-Layer Perceptron, k-Nearest Neighbors, Proper Orthogonal Decomposition\nand IsoMap. Initial performance results, using $R^2$ scores and worst relative\nmean absolute error metrics, are presented, offering insights into the\ncapabilities of these techniques for the challenge and references for future\nwork."}
{"id": "2505.06823", "pdf": "https://arxiv.org/pdf/2505.06823", "abs": "https://arxiv.org/abs/2505.06823", "authors": ["Saad Masrur", "Ozgur Ozdemir", "Anil Gurses", "Ismail Guvenc", "Mihail L. Sichitiu", "Rudra Dutta", "Magreth Mushi", "homas Zajkowski", "Cole Dickerson", "Gautham Reddy", "Sergio Vargas Villar", "Chau-Wai Wong", "Baisakhi Chatterjee", "Sonali Chaudhari", "Zhizhen Li", "Yuchen Liu", "Paul Kudyba", "Haijian Sun", "Jaya Sravani Mandapaka", "Kamesh Namuduri", "Weijie Wang", "Fraida Fund"], "title": "Collection: Datasets from AFAR Challenge", "categories": ["eess.SP", "eess.AS"], "comment": "Submitted to IEEE Data Descriptions", "summary": "This paper presents a comprehensive real-world and Digital Twin (DT) dataset\ncollected as part of the Find A Rover (AFAR) Challenge, organized by the NSF\nAerial Experimentation and Research Platform for Advanced Wireless (AERPAW)\ntestbed and hosted at the Lake Wheeler Field in Raleigh, North Carolina. The\nAFAR Challenge was a competition involving five finalist university teams,\nfocused on promoting innovation in UAV-assisted radio frequency (RF) source\nlocalization. Participating teams were tasked with designing UAV flight\ntrajectories and localization algorithms to detect the position of a hidden\nunmanned ground vehicle (UGV), also referred to as a rover, emitting wireless\nprobe signals generated by GNU Radio. The competition was structured to\nevaluate solutions in a DT environment first, followed by deployment and\ntesting in AERPAW's outdoor wireless testbed. For each team, the UGV was placed\nat three different positions, resulting in a total of 30 datasets, 15 collected\nin a DT simulation environment and 15 in a physical outdoor testbed. Each\ndataset contains time-synchronized measurements of received signal strength\n(RSS), received signal quality (RSQ), GPS coordinates, UAV velocity, and UAV\norientation (roll, pitch, and yaw). Data is organized into structured folders\nby team, environment (DT and real-world), and UGV location. The dataset\nsupports research in UAV-assisted RF source localization, air-to-ground (A2G)\nwireless propagation modeling, trajectory optimization, signal prediction,\nautonomous navigation, and DT validation. With approximately 300k\ntime-synchronized samples collected from real-world experiments, the dataset\nprovides a substantial foundation for training and evaluating deep learning\n(DL) models. Overall, the AFAR dataset serves as a valuable resource for\nadvancing robust, real-world solutions in UAV-enabled wireless communications\nand sensing systems."}
{"id": "2505.06934", "pdf": "https://arxiv.org/pdf/2505.06934", "abs": "https://arxiv.org/abs/2505.06934", "authors": ["Roy Betser", "Meir Yossef Levi", "Guy Gilboa"], "title": "Whitened CLIP as a Likelihood Surrogate of Images and Captions", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to ICML 2025. This version matches the camera-ready version", "summary": "Likelihood approximations for images are not trivial to compute and can be\nuseful in many applications. We examine the use of Contrastive Language-Image\nPre-training (CLIP) to assess the likelihood of images and captions. We\nintroduce \\textit{Whitened CLIP}, a novel transformation of the CLIP latent\nspace via an invertible linear operation. This transformation ensures that each\nfeature in the embedding space has zero mean, unit standard deviation, and no\ncorrelation with all other features, resulting in an identity covariance\nmatrix. We show that the whitened embeddings statistics can be well\napproximated as a standard normal distribution, thus, the log-likelihood is\nestimated simply by the square Euclidean norm in the whitened embedding space.\nThe whitening procedure is completely training-free and performed using a\npre-computed whitening matrix, hence, is very fast. We present several\npreliminary experiments demonstrating the properties and applicability of these\nlikelihood scores to images and captions."}
{"id": "2504.01660", "pdf": "https://arxiv.org/pdf/2504.01660", "abs": "https://arxiv.org/abs/2504.01660", "authors": ["James W. Trayford", "Samantha Youles", "Chris Harrison", "Rose Shepherd", "Nicolas Bonne"], "title": "STRAUSS: Sonification Tools & Resources for Analysis Using Sound Synthesis", "categories": ["astro-ph.IM", "cs.SD", "physics.data-an"], "comment": "4 pages, linking to documentation on ReadTheDocs\n  (https://strauss.readthedocs.io/en/latest/)", "summary": "Sonification, or conveying data using non-verbal audio, is a relatively niche\nbut growing approach for presenting data across multiple specialist domains\nincluding astronomy, climate science, and beyond. The STRAUSS Python package\naims to provide such a tool, which builds upon previous approaches to provide a\npowerful means to explore different ways of expressing data, with fine control\nover the output audio and its format. STRAUSS is a free, open source (FOSS)\npackage, designed to allow flexible and effective sonification to be integrated\ninto data workflows, in analogy to widely used visualisation packages. The\nremit of STRAUSS is broad; it is intended to be able to bridge between ad-hoc\nsolutions for sonifying very particular datasets, and highly technical\ncompositional and sound-design tools that are not optimised for sonification,\nor may have a steep learning curve. The code offers a range of approaches to\nsonification for a variety of contexts (e.g. science education, science\ncommunication, technical data analysis, etc). To this end, STRAUSS is packaged\nwith a number of examples of different sonification approaches, and preset\nconfigurations to support \"low-barrier, high-ceiling\" approach. STRAUSS has\nbeen used to produce both educational resources and analysis tools."}
{"id": "2505.06591", "pdf": "https://arxiv.org/pdf/2505.06591", "abs": "https://arxiv.org/abs/2505.06591", "authors": ["Anna Wróblewska", "Bartosz Grabek", "Jakub Świstak", "Daniel Dan"], "title": "Evaluating LLM-Generated Q&A Test: a Student-Centered Study", "categories": ["cs.CL", "cs.HC"], "comment": "accepted to AIED 2025", "summary": "This research prepares an automatic pipeline for generating reliable\nquestion-answer (Q&A) tests using AI chatbots. We automatically generated a\nGPT-4o-mini-based Q&A test for a Natural Language Processing course and\nevaluated its psychometric and perceived-quality metrics with students and\nexperts. A mixed-format IRT analysis showed that the generated items exhibit\nstrong discrimination and appropriate difficulty, while student and expert star\nratings reflect high overall quality. A uniform DIF check identified two items\nfor review. These findings demonstrate that LLM-generated assessments can match\nhuman-authored tests in psychometric performance and user satisfaction,\nillustrating a scalable approach to AI-assisted assessment development."}
{"id": "2505.06771", "pdf": "https://arxiv.org/pdf/2505.06771", "abs": "https://arxiv.org/abs/2505.06771", "authors": ["Shalin Anand Jain", "Jiazhen Liu", "Siva Kailas", "Harish Ravichandar"], "title": "JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes", "categories": ["cs.RO", "cs.LG", "cs.MA"], "comment": "22 pages, 14 figures, 10 tables", "summary": "Multi-agent reinforcement learning (MARL) has emerged as a promising solution\nfor learning complex and scalable coordination behaviors in multi-robot\nsystems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics\nrelevance and hardware deployment, leaving multi-robot learning researchers to\ndevelop bespoke environments and hardware testbeds dedicated to the development\nand evaluation of their individual contributions. The Multi-Agent RL Benchmark\nand Learning Environment for the Robotarium (MARBLER) is an exciting recent\nstep in providing a standardized robotics-relevant platform for MARL, by\nbridging the Robotarium testbed with existing MARL software infrastructure.\nHowever, MARBLER lacks support for parallelization and GPU/TPU execution,\nmaking the platform prohibitively slow compared to modern MARL environments and\nhindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end\nsimulation, learning, deployment, and benchmarking platform for the Robotarium.\nJaxRobotarium enables rapid training and deployment of multi-robot\nreinforcement learning (MRRL) policies with realistic robot dynamics and safety\nconstraints, supporting both parallelization and hardware acceleration. Our\ngeneralizable learning interface provides an easy-to-use integration with SOTA\nMARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight\nstandardized coordination scenarios, including four novel scenarios that bring\nestablished MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a\nrealistic robotics setting. We demonstrate that JaxRobotarium retains high\nsimulation fidelity while achieving dramatic speedups over baseline (20x in\ntraining and 150x in simulation), and provides an open-access sim-to-real\nevaluation pipeline through the Robotarium testbed, accelerating and\ndemocratizing access to multi-robot learning research and evaluation."}
{"id": "2505.06413", "pdf": "https://arxiv.org/pdf/2505.06413", "abs": "https://arxiv.org/abs/2505.06413", "authors": ["Ming Liu", "Siyuan Liang", "Koushik Howlader", "Liwen Wang", "Dacheng Tao", "Wensheng Zhang"], "title": "Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have been integrated into autonomous driving\nsystems to enhance reasoning capabilities through tasks such as Visual Question\nAnswering (VQA). However, the robustness of these systems against backdoor\nattacks remains underexplored. In this paper, we propose a natural\nreflection-based backdoor attack targeting VLM systems in autonomous driving\nscenarios, aiming to induce substantial response delays when specific visual\ntriggers are present. We embed faint reflection patterns, mimicking natural\nsurfaces such as glass or water, into a subset of images in the DriveLM\ndataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories\nor system update notifications) to the corresponding textual labels. This\nstrategy trains the model to generate abnormally long responses upon\nencountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and\nLLaMA-Adapter, using parameter-efficient methods. Experimental results\ndemonstrate that while the models maintain normal performance on clean inputs,\nthey exhibit significantly increased inference latency when triggered,\npotentially leading to hazardous delays in real-world autonomous driving\ndecision-making. Further analysis examines factors such as poisoning rates,\ncamera perspectives, and cross-view transferability. Our findings uncover a new\nclass of attacks that exploit the stringent real-time requirements of\nautonomous driving, posing serious challenges to the security and reliability\nof VLM-augmented driving systems."}
{"id": "2505.06505", "pdf": "https://arxiv.org/pdf/2505.06505", "abs": "https://arxiv.org/abs/2505.06505", "authors": ["Hua Meng", "Zhiguo Long", "Michael Sioutis", "Zhengchun Zhou"], "title": "On Definite Iterated Belief Revision with Belief Algebras", "categories": ["cs.AI", "I.2.4"], "comment": "10 pages. Extended version of an accepted IJCAI 2025 paper", "summary": "Traditional logic-based belief revision research focuses on designing rules\nto constrain the behavior of revision operators. Frameworks have been proposed\nto characterize iterated revision rules, but they are often too loose, leading\nto multiple revision operators that all satisfy the rules under the same belief\ncondition. In many practical applications, such as safety critical ones, it is\nimportant to specify a definite revision operator to enable agents to\niteratively revise their beliefs in a deterministic way. In this paper, we\npropose a novel framework for iterated belief revision by characterizing belief\ninformation through preference relations. Semantically, both beliefs and new\nevidence are represented as belief algebras, which provide a rich and\nexpressive foundation for belief revision. Building on traditional revision\nrules, we introduce additional postulates for revision with belief algebra,\nincluding an upper-bound constraint on the outcomes of revision. We prove that\nthe revision result is uniquely determined given the current belief state and\nnew evidence. Furthermore, to make the framework more useful in practice, we\ndevelop a particular algorithm for performing the proposed revision process. We\nargue that this approach may offer a more predictable and principled method for\nbelief revision, making it suitable for real-world applications."}
{"id": "2505.06266", "pdf": "https://arxiv.org/pdf/2505.06266", "abs": "https://arxiv.org/abs/2505.06266", "authors": ["Qi Cheng", "Licheng Liu", "Zhang Yao", "Hong Mu", "Shiyuan Luo", "Zhenong Jin", "Yiqun Xie", "Xiaowei Jia"], "title": "Knowledge Guided Encoder-Decoder Framework Integrating Multiple Physical Models for Agricultural Ecosystem Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Agricultural monitoring is critical for ensuring food security, maintaining\nsustainable farming practices, informing policies on mitigating food shortage,\nand managing greenhouse gas emissions. Traditional process-based physical\nmodels are often designed and implemented for specific situations, and their\nparameters could also be highly uncertain. In contrast, data-driven models\noften use black-box structures and does not explicitly model the\ninter-dependence between different ecological variables. As a result, they\nrequire extensive training data and lack generalizability to different tasks\nwith data distribution shifts and inconsistent observed variables. To address\nthe need for more universal models, we propose a knowledge-guided\nencoder-decoder model, which can predict key crop variables by leveraging\nknowledge of underlying processes from multiple physical models. The proposed\nmethod also integrates a language model to process complex and inconsistent\ninputs and also utilizes it to implement a model selection mechanism for\nselectively combining the knowledge from different physical models. Our\nevaluations on predicting carbon and nitrogen fluxes for multiple sites\ndemonstrate the effectiveness and robustness of the proposed model under\nvarious scenarios."}
{"id": "2505.07202", "pdf": "https://arxiv.org/pdf/2505.07202", "abs": "https://arxiv.org/abs/2505.07202", "authors": ["Hyouin Liu", "Zhikuan Zhang"], "title": "On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Modern TTS systems designed for conversations achieve high-quality utterances\nbut often remain inaccessible publicly. Are existing open-source architectures\ninadequate, or are current training techniques insufficient? This paper\ninvestigates prominent models and their underlying behaviors regarding\nconversational context. Using 20 GPU-hours on an NVIDIA H100, we empirically\nexamine two approaches: context-based utterance-level training versus full\nconversation training. Results demonstrate that context-based utterance\ntraining achieves superior MOS scores (4.3/5.0 vs 3.7/5.0) and reduces training\ntime by 37%, while full conversation approaches suffer from speaker similarity\nhallucination issues. These findings provide practical guidelines for\nconversational TTS development, favoring utterance-level training with\ncontextual conditioning for both resource efficiency and output quality."}
{"id": "2505.07159", "pdf": "https://arxiv.org/pdf/2505.07159", "abs": "https://arxiv.org/abs/2505.07159", "authors": ["Jong Sung Park", "Juhyung Ha", "Siddhesh Thakur", "Alexandra Badea", "Spyridon Bakas", "Eleftherios Garyfallidis"], "title": "Skull stripping with purely synthetic data", "categories": ["eess.IV", "cs.CV"], "comment": "Oral at ISMRM 2025", "summary": "While many skull stripping algorithms have been developed for multi-modal and\nmulti-species cases, there is still a lack of a fundamentally generalizable\napproach. We present PUMBA(PUrely synthetic Multimodal/species invariant Brain\nextrAction), a strategy to train a model for brain extraction with no real\nbrain images or labels. Our results show that even without any real images or\nanatomical priors, the model achieves comparable accuracy in multi-modal,\nmulti-species and pathological cases. This work presents a new direction of\nresearch for any generalizable medical image segmentation task."}
{"id": "2505.06271", "pdf": "https://arxiv.org/pdf/2505.06271", "abs": "https://arxiv.org/abs/2505.06271", "authors": ["June-Woo Kim", "Sanghoon Lee", "Miika Toikkanen", "Daehwan Hwang", "Kyunghoon Kim"], "title": "Tri-MTL: A Triple Multitask Learning Approach for Respiratory Disease Diagnosis", "categories": ["cs.LG", "cs.AI", "cs.SD"], "comment": "Accepted to EMBC 2025", "summary": "Auscultation remains a cornerstone of clinical practice, essential for both\ninitial evaluation and continuous monitoring. Clinicians listen to the lung\nsounds and make a diagnosis by combining the patient's medical history and test\nresults. Given this strong association, multitask learning (MTL) can offer a\ncompelling framework to simultaneously model these relationships, integrating\nrespiratory sound patterns with disease manifestations. While MTL has shown\nconsiderable promise in medical applications, a significant research gap\nremains in understanding the complex interplay between respiratory sounds,\ndisease manifestations, and patient metadata attributes. This study\ninvestigates how integrating MTL with cutting-edge deep learning architectures\ncan enhance both respiratory sound classification and disease diagnosis.\nSpecifically, we extend recent findings regarding the beneficial impact of\nmetadata on respiratory sound classification by evaluating its effectiveness\nwithin an MTL framework. Our comprehensive experiments reveal significant\nimprovements in both lung sound classification and diagnostic performance when\nthe stethoscope information is incorporated into the MTL architecture."}
{"id": "2505.06594", "pdf": "https://arxiv.org/pdf/2505.06594", "abs": "https://arxiv.org/abs/2505.06594", "authors": ["Galann Pennec", "Zhengyuan Liu", "Nicholas Asher", "Philippe Muller", "Nancy F. Chen"], "title": "Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) often struggle to balance visual and textual\ninformation when summarizing complex multimodal inputs, such as entire TV show\nepisodes. In this paper, we propose a zero-shot video-to-text summarization\napproach that builds its own screenplay representation of an episode,\neffectively integrating key video moments, dialogue, and character information\ninto a unified document. Unlike previous approaches, we simultaneously generate\nscreenplays and name the characters in zero-shot, using only the audio, video,\nand transcripts as input. Additionally, we highlight that existing\nsummarization metrics can fail to assess the multimodal content in summaries.\nTo address this, we introduce MFactSum, a multimodal metric that evaluates\nsummaries with respect to both vision and text modalities. Using MFactSum, we\nevaluate our screenplay summaries on the SummScreen3D dataset, demonstrating\nsuperiority against state-of-the-art VLMs such as Gemini 1.5 by generating\nsummaries containing 20% more relevant visual information while requiring 75%\nless of the video as input."}
{"id": "2505.06947", "pdf": "https://arxiv.org/pdf/2505.06947", "abs": "https://arxiv.org/abs/2505.06947", "authors": ["Senhao Yang", "Qiwen Cheng", "Ruiqi Ma", "Liangzhe Zhao", "Zhenying Wu", "Guangqiang Yu"], "title": "The Wisdom of Agent Crowds: A Human-AI Interaction Innovation Ignition Framework", "categories": ["cs.HC", "cs.MA", "I.2.7; J.4"], "comment": null, "summary": "With the widespread application of large AI models in various fields, the\nautomation level of multi-agent systems has been continuously improved.\nHowever, in high-risk decision-making scenarios such as healthcare and finance,\nhuman participation and the alignment of intelligent systems with human\nintentions remain crucial. This paper focuses on the financial scenario and\nconstructs a multi-agent brainstorming framework based on the BDI theory. A\nhuman-computer collaborative multi-agent financial analysis process is built\nusing Streamlit. The system plans tasks according to user intentions, reduces\nusers' cognitive load through real-time updated structured text summaries and\nthe interactive Cothinker module, and reasonably integrates general and\nreasoning large models to enhance the ability to handle complex problems. By\ndesigning a quantitative analysis algorithm for the sentiment tendency of\ninterview content based on LLMs and a method for evaluating the diversity of\nideas generated by LLMs in brainstorming based on k-means clustering and\ninformation entropy, the system is comprehensively evaluated. The results of\nhuman factors testing show that the system performs well in terms of usability\nand user experience. Although there is still room for improvement, it can\neffectively support users in completing complex financial tasks. The research\nshows that the system significantly improves the efficiency of human-computer\ninteraction and the quality of decision-making in financial decision-making\nscenarios, providing a new direction for the development of related fields."}
{"id": "2505.06436", "pdf": "https://arxiv.org/pdf/2505.06436", "abs": "https://arxiv.org/abs/2505.06436", "authors": ["Jingrui He", "Andrew Stephen McGough"], "title": "My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to 2nd International Workshop on Synthetic Data for Face\n  and Gesture Analysis at IEEE FG 2025", "summary": "Generative Adversarial Network approaches such as StyleGAN/2 provide two key\nbenefits: the ability to generate photo-realistic face images and possessing a\nsemantically structured latent space from which these images are created. Many\napproaches have emerged for editing images derived from vectors in the latent\nspace of a pre-trained StyleGAN/2 models by identifying semantically meaningful\ndirections (e.g., gender or age) in the latent space. By moving the vector in a\nspecific direction, the ideal result would only change the target feature while\npreserving all the other features. Providing an ideal data augmentation\napproach for gesture research as it could be used to generate numerous image\nvariations whilst keeping the facial expressions intact. However, entanglement\nissues, where changing one feature inevitably affects other features, impacts\nthe ability to preserve facial expressions. To address this, we propose the use\nof an addition to the loss function of a Facial Keypoint Detection model to\nrestrict changes to the facial expressions. Building on top of an existing\nmodel, adding the proposed Human Face Landmark Detection (HFLD) loss, provided\nby a pre-trained Facial Keypoint Detection model, to the original loss\nfunction. We quantitatively and qualitatively evaluate the existing and our\nextended model, showing the effectiveness of our approach in addressing the\nentanglement issue and maintaining the facial expression. Our approach achieves\nup to 49% reduction in the change of emotion in our experiments. Moreover, we\nshow the benefit of our approach by comparing with state-of-the-art models. By\nincreasing the ability to preserve the facial gesture and expression during\nfacial transformation, we present a way to create human face images with fixed\nexpression but different appearances, making it a reliable data augmentation\napproach for Facial Gesture and Expression research."}
{"id": "2505.06507", "pdf": "https://arxiv.org/pdf/2505.06507", "abs": "https://arxiv.org/abs/2505.06507", "authors": ["Haoyang Xie", "Feng Ju"], "title": "Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Computer-aided design (CAD) is fundamental to modern engineering and\nmanufacturing, but creating CAD models still requires expert knowledge and\nspecialized software. Recent advances in large language models (LLMs) open up\nthe possibility of generative CAD, where natural language is directly\ntranslated into parametric 3D models. However, most existing methods generate\ntask-specific command sequences that pretrained models cannot directly handle.\nThese sequences must be converted into CAD representations such as CAD vectors\nbefore a 3D model can be produced, which requires training models from scratch\nand adds unnecessary complexity. To tackle this issue, we propose generating\nCadQuery code directly from text, leveraging the strengths of pretrained LLMs\nto produce 3D models without intermediate representations, using this\nPython-based scripting language. Since LLMs already excel at Python generation\nand spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly\neffective. Given that these capabilities typically improve with scale, we\nhypothesize that larger models will perform better after fine-tuning. To enable\nthis, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We\nfine-tune six open-source LLMs of varying sizes and observe consistent\nimprovements. Our best model achieves a top-1 exact match of 69.3%, up from\n58.8%, and reduces Chamfer Distance by 48.6%. Project page:\nhttps://github.com/Text-to-CadQuery/Text-to-CadQuery."}
{"id": "2505.06268", "pdf": "https://arxiv.org/pdf/2505.06268", "abs": "https://arxiv.org/abs/2505.06268", "authors": ["Pengcheng Sun", "Erwu Liu", "Wei Ni", "Kanglei Yu", "Rui Wang", "Abbas Jamalipour"], "title": "Cluster-Aware Multi-Round Update for Wireless Federated Learning in Heterogeneous Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The aggregation efficiency and accuracy of wireless Federated Learning (FL)\nare significantly affected by resource constraints, especially in heterogeneous\nenvironments where devices exhibit distinct data distributions and\ncommunication capabilities. This paper proposes a clustering strategy that\nleverages prior knowledge similarity to group devices with similar data and\ncommunication characteristics, mitigating performance degradation from\nheterogeneity. On this basis, a novel Cluster- Aware Multi-round Update (CAMU)\nstrategy is proposed, which treats clusters as the basic units and adjusts the\nlocal update frequency based on the clustered contribution threshold,\neffectively reducing update bias and enhancing aggregation accuracy. The\ntheoretical convergence of the CAMU strategy is rigorously validated.\nMeanwhile, based on the convergence upper bound, the local update frequency and\ntransmission power of each cluster are jointly optimized to achieve an optimal\nbalance between computation and communication resources under constrained\nconditions, significantly improving the convergence efficiency of FL.\nExperimental results demonstrate that the proposed method effectively improves\nthe model performance of FL in heterogeneous environments and achieves a better\nbalance between communication cost and computational load under limited\nresources."}
{"id": "2505.07731", "pdf": "https://arxiv.org/pdf/2505.07731", "abs": "https://arxiv.org/abs/2505.07731", "authors": ["Neeraj Agrawal", "Sriram Ganapathy"], "title": "Spoken Language Understanding on Unseen Tasks With In-Context Learning", "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "Spoken language understanding (SLU) tasks involve diverse skills that probe\nthe information extraction, classification and/or generation capabilities of\nmodels. In this setting, task-specific training data may not always be\navailable. While traditional task-specific SLU models are unable to cater to\nsuch requirements, the speech-text large language models (LLMs) offer a\npromising alternative with emergent abilities. However, out of-the-box, our\nevaluations indicate that the zero/few-shot performance of prominent\nopen-source speech-text LLMs on SLU tasks are not up to the mark. In this\npaper, we introduce a novel approach to robust task-agnostic fine-tuning using\nrandomized class labels. With this proposed fine-tuning, we illustrate that the\nperformance of the speech-text LLMs on an unseen task is significantly improved\nover standard approaches. Critically, the proposed approach avoids the\nrequirement of task-specific data annotations for enabling new tasks in\nspeech-text LLMs."}
{"id": "2505.07175", "pdf": "https://arxiv.org/pdf/2505.07175", "abs": "https://arxiv.org/abs/2505.07175", "authors": ["Yash Deo", "Yan Jia", "Toni Lassila", "William A. P. Smith", "Tom Lawton", "Siyuan Kang", "Alejandro F. Frangi", "Ibrahim Habli"], "title": "Metrics that matter: Evaluating image quality metrics for medical image generation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Evaluating generative models for synthetic medical imaging is crucial yet\nchallenging, especially given the high standards of fidelity, anatomical\naccuracy, and safety required for clinical applications. Standard evaluation of\ngenerated images often relies on no-reference image quality metrics when ground\ntruth images are unavailable, but their reliability in this complex domain is\nnot well established. This study comprehensively assesses commonly used\nno-reference image quality metrics using brain MRI data, including tumour and\nvascular images, providing a representative exemplar for the field. We\nsystematically evaluate metric sensitivity to a range of challenges, including\nnoise, distribution shifts, and, critically, localised morphological\nalterations designed to mimic clinically relevant inaccuracies. We then compare\nthese metric scores against model performance on a relevant downstream\nsegmentation task, analysing results across both controlled image perturbations\nand outputs from different generative model architectures. Our findings reveal\nsignificant limitations: many widely-used no-reference image quality metrics\ncorrelate poorly with downstream task suitability and exhibit a profound\ninsensitivity to localised anatomical details crucial for clinical validity.\nFurthermore, these metrics can yield misleading scores regarding distribution\nshifts, e.g. data memorisation. This reveals the risk of misjudging model\nreadiness, potentially leading to the deployment of flawed tools that could\ncompromise patient safety. We conclude that ensuring generative models are\ntruly fit for clinical purpose requires a multifaceted validation framework,\nintegrating performance on relevant downstream tasks with the cautious\ninterpretation of carefully selected no-reference image quality metrics."}
{"id": "2505.06519", "pdf": "https://arxiv.org/pdf/2505.06519", "abs": "https://arxiv.org/abs/2505.06519", "authors": ["Hansani Weeratunge", "Dominic Robe", "Elnaz Hajizadeh"], "title": "Interpretable SHAP-bounded Bayesian Optimization for Underwater Acoustic Metamaterial Coating Design", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.SD"], "comment": null, "summary": "We developed an interpretability informed Bayesian optimization framework to\noptimize underwater acoustic coatings based on polyurethane elastomers with\nembedded metamaterial features. A data driven model was employed to analyze the\nrelationship between acoustic performance, specifically sound absorption and\nthe corresponding design variables. By leveraging SHapley Additive exPlanations\n(SHAP), a machine learning interpretability tool, we identified the key\nparameters influencing the objective function and gained insights into how\nthese parameters affect sound absorption. The insights derived from the SHAP\nanalysis were subsequently used to automatically refine the bounds of the\noptimization problem automatically, enabling a more targeted and efficient\nexploration of the design space.\n  The proposed approach was applied to two polyurethane materials with distinct\nhardness levels, resulting in improved optimal solutions compared to those\nobtained without SHAP-informed guidance. Notably, these enhancements were\nachieved without increasing the number of simulation iterations. Our findings\ndemonstrate the potential of SHAP to streamline optimization processes by\nuncovering hidden parameter relationships and guiding the search toward\npromising regions of the design space. This work underscores the effectiveness\nof combining interpretability techniques with Bayesian optimization for the\nefficient and cost-effective design of underwater acoustic metamaterials under\nstrict computational constraints and can be generalized towards other materials\nand engineering optimization problems."}
{"id": "2505.06599", "pdf": "https://arxiv.org/pdf/2505.06599", "abs": "https://arxiv.org/abs/2505.06599", "authors": ["Abbas Bertina", "Shahab Beirami", "Hossein Biniazian", "Elham Esmaeilnia", "Soheil Shahi", "Mahdi Pirnia"], "title": "Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "pdf, 8 pages, 4 figures, 4 tables", "summary": "Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges\ndue to its complex phonological features, particularly homographs and Ezafe,\nwhich exist in formal and informal language contexts. This paper introduces an\nintermediate language specifically designed for Persian language processing\nthat addresses these challenges through a multi-faceted approach. Our\nmethodology combines two key components: Large Language Model (LLM) prompting\ntechniques and a specialized sequence-to-sequence machine transliteration\narchitecture. We developed and implemented a systematic approach for\nconstructing a comprehensive lexical database for homographs with multiple\npronunciations disambiguation often termed polyphones, utilizing formal concept\nanalysis for semantic differentiation. We train our model using two distinct\ndatasets: the LLM-generated dataset for formal and informal Persian and the\nB-Plus podcasts for informal language variants. The experimental results\ndemonstrate superior performance compared to existing state-of-the-art\napproaches, particularly in handling the complexities of Persian phoneme\nconversion. Our model significantly improves Phoneme Error Rate (PER) metrics,\nestablishing a new benchmark for Persian G2P conversion accuracy. This work\ncontributes to the growing research in low-resource language processing and\nprovides a robust solution for Persian text-to-speech systems and demonstrating\nits applicability beyond Persian. Specifically, the approach can extend to\nlanguages with rich homographic phenomena such as Chinese and Arabic"}
{"id": "2505.07008", "pdf": "https://arxiv.org/pdf/2505.07008", "abs": "https://arxiv.org/abs/2505.07008", "authors": ["Fengming Zhu", "Fangzhen Lin"], "title": "Constant-Memory Strategies in Stochastic Games: Best Responses and Equilibria", "categories": ["cs.GT", "cs.MA"], "comment": "19 pages, ongoing work", "summary": "(Here is a short version, see our paper for the complete abstract.)\n  In this work, we comprehensively investigate the concept of constant-memory\nstrategies in stochastic games. We first establish some results on best\nresponses and Nash equilibria for behavioral constant-memory strategies,\nfollowed by a discussion on the computational hardness of best responding to\nmixed constant-memory strategies. Those theoretic insights later empower a\ngenerative framework for studying generalizability of single-agent RL\nalgorithms."}
{"id": "2505.06467", "pdf": "https://arxiv.org/pdf/2505.06467", "abs": "https://arxiv.org/abs/2505.06467", "authors": ["Nisan Chhetri", "Arpan Sainju"], "title": "PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation", "categories": ["cs.CV", "cs.HC"], "comment": "4 pages, 2 figures", "summary": "Generating high-quality images without prompt engineering expertise remains a\nchallenge for text-to-image (T2I) models, which often misinterpret poorly\nstructured prompts, leading to distortions and misalignments. While humans\neasily recognize these flaws, metrics like CLIP fail to capture structural\ninconsistencies, exposing a key limitation in current evaluation methods. To\naddress this, we introduce PromptIQ, an automated framework that refines\nprompts and assesses image quality using our novel Component-Aware Similarity\n(CAS) metric, which detects and penalizes structural errors. Unlike\nconventional methods, PromptIQ iteratively generates and evaluates images until\nthe user is satisfied, eliminating trial-and-error prompt tuning. Our results\nshow that PromptIQ significantly improves generation quality and evaluation\naccuracy, making T2I models more accessible for users with little to no prompt\nengineering expertise."}
{"id": "2505.06518", "pdf": "https://arxiv.org/pdf/2505.06518", "abs": "https://arxiv.org/abs/2505.06518", "authors": ["Larry Preuett III"], "title": "A Point-Based Algorithm for Distributional Reinforcement Learning in Partially Observable Domains", "categories": ["cs.AI"], "comment": null, "summary": "In many real-world planning tasks, agents must tackle uncertainty about the\nenvironment's state and variability in the outcomes of any chosen policy. We\naddress both forms of uncertainty as a first step toward safer algorithms in\npartially observable settings. Specifically, we extend Distributional\nReinforcement Learning (DistRL)-which models the entire return distribution for\nfully observable domains-to Partially Observable Markov Decision Processes\n(POMDPs), allowing an agent to learn the distribution of returns for each\nconditional plan. Concretely, we introduce new distributional Bellman operators\nfor partial observability and prove their convergence under the supremum\np-Wasserstein metric. We also propose a finite representation of these return\ndistributions via psi-vectors, generalizing the classical alpha-vectors in\nPOMDP solvers. Building on this, we develop Distributional Point-Based Value\nIteration (DPBVI), which integrates psi-vectors into a standard point-based\nbackup procedure-bridging DistRL and POMDP planning. By tracking return\ndistributions, DPBVI naturally enables risk-sensitive control in domains where\nrare, high-impact events must be carefully managed. We provide source code to\nfoster further research in robust decision-making under partial observability."}
{"id": "2505.06269", "pdf": "https://arxiv.org/pdf/2505.06269", "abs": "https://arxiv.org/abs/2505.06269", "authors": ["Chenguang Zhou", "Lei Chen", "Xiaohui Zhong", "Bo Lu", "Hao Li", "Libo Wu", "Jie Wu", "Jiahui Hu", "Zesheng Dou", "Pang-Chi Hsu", "Xiaoye Zhang"], "title": "A machine learning model for skillful climate system prediction", "categories": ["cs.LG"], "comment": null, "summary": "Climate system models (CSMs), through integrating cross-sphere interactions\namong the atmosphere, ocean, land, and cryosphere, have emerged as pivotal\ntools for deciphering climate dynamics and improving forecasting capabilities.\nRecent breakthroughs in artificial intelligence (AI)-driven meteorological\nmodeling have demonstrated remarkable success in single-sphere systems and\npartially spheres coupled systems. However, the development of a fully coupled\nAI-based climate system model encompassing atmosphere-ocean-land-sea ice\ninteractions has remained an unresolved challenge. This paper introduces\nFengShun-CSM, an AI-based CSM model that provides 60-day global daily forecasts\nfor 29 critical variables across atmospheric, oceanic, terrestrial, and\ncryospheric domains. The model significantly outperforms the European Centre\nfor Medium-Range Weather Forecasts (ECMWF) subseasonal-to-seasonal (S2S) model\nin predicting most variables, particularly precipitation, land surface, and\noceanic components. This enhanced capability is primarily attributed to its\nimproved representation of intra-seasonal variability modes, most notably the\nMadden-Julian Oscillation (MJO). Remarkably, FengShun-CSM exhibits substantial\npotential in predicting subseasonal extreme events. Such breakthroughs will\nadvance its applications in meteorological disaster mitigation, marine\necosystem conservation, and agricultural productivity enhancement. Furthermore,\nit validates the feasibility of developing AI-powered CSMs through machine\nlearning technologies, establishing a transformative paradigm for\nnext-generation Earth system modeling."}
{"id": "2503.12010", "pdf": "https://arxiv.org/pdf/2503.12010", "abs": "https://arxiv.org/abs/2503.12010", "authors": ["Qixian Chen", "Yuxiong Xu", "Sara Mandelli", "Sheng Li", "Bin Li"], "title": "Adaptive Mixture of Low-Rank Experts for Robust Audio Spoofing Detection", "categories": ["eess.AS"], "comment": "5 pages, 1 figure, 4 tables", "summary": "In audio spoofing detection, most studies rely on clean datasets, making\nmodels susceptible to real-world post-processing attacks, such as channel\ncompression and noise. To overcome this challenge, we propose the Adaptive\nMixtUre Low-rank ExperTs (AMULET) framework, which enhances resilience by\nleveraging attack-specific knowledge and dynamically adapting to varied attack\nconditions. Specifically, AMULET employs Attack-Specific Experts (ASEs)\nfine-tuned with Low-Rank Adaptation (LoRA), allowing each expert to focus on\ndistinct post-processing patterns using just 1.13\\% of the parameters required\nfor full fine-tuning. Furthermore, we introduce Adaptive Expert Fusion (AEF),\nwhich adaptively selects and integrates expert knowledge to enhance the\nrobustness of spoofing detection. Experimental results demonstrate that AMULET\nsignificantly enhances robustness by improving noise resilience and exhibiting\ngreater adaptability to unseen post-processing methods compared to models\ntrained with full fine-tuning. Additionally, our framework outperforms both\nsingle expert and other expert aggregation strategies under various mixed\nattacks, demonstrating its superior robustness and adaptability in managing\ncomplex real-world scenarios."}
{"id": "2505.07349", "pdf": "https://arxiv.org/pdf/2505.07349", "abs": "https://arxiv.org/abs/2505.07349", "authors": ["Badhan Kumar Das", "Gengyan Zhao", "Boris Mailhe", "Thomas J. Re", "Dorin Comaniciu", "Eli Gibson", "Andreas Maier"], "title": "Multi-Plane Vision Transformer for Hemorrhage Classification Using Axial and Sagittal MRI Data", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages", "summary": "Identifying brain hemorrhages from magnetic resonance imaging (MRI) is a\ncritical task for healthcare professionals. The diverse nature of MRI\nacquisitions with varying contrasts and orientation introduce complexity in\nidentifying hemorrhage using neural networks. For acquisitions with varying\norientations, traditional methods often involve resampling images to a fixed\nplane, which can lead to information loss. To address this, we propose a 3D\nmulti-plane vision transformer (MP-ViT) for hemorrhage classification with\nvarying orientation data. It employs two separate transformer encoders for\naxial and sagittal contrasts, using cross-attention to integrate information\nacross orientations. MP-ViT also includes a modality indication vector to\nprovide missing contrast information to the model. The effectiveness of the\nproposed model is demonstrated with extensive experiments on real world\nclinical dataset consists of 10,084 training, 1,289 validation and 1,496 test\nsubjects. MP-ViT achieved substantial improvement in area under the curve\n(AUC), outperforming the vision transformer (ViT) by 5.5% and CNN-based\narchitectures by 1.8%. These results highlight the potential of MP-ViT in\nimproving performance for hemorrhage detection when different orientation\ncontrasts are needed."}
{"id": "2211.09089", "pdf": "https://arxiv.org/pdf/2211.09089", "abs": "https://arxiv.org/abs/2211.09089", "authors": ["Yi Xiao", "Harshit Sharma", "Victoria Tumanova", "Asif Salekin"], "title": "Psychophysiology-aided Perceptually Fluent Speech Analysis of Children Who Stutter", "categories": ["cs.SD", "cs.HC", "eess.AS"], "comment": "13 pages, 5 figures, ICCPS 2025", "summary": "This paper presents a novel approach named PASAD that detects changes in\nperceptually fluent speech acoustics of young children. Particularly, analysis\nof perceptually fluent speech enables identifying the speech-motor-control\nfactors that are considered as the underlying cause of stuttering disfluencies.\nRecent studies indicate that the speech production of young children,\nespecially those who stutter, may get adversely affected by situational\nphysiological arousal. A major contribution of this paper is leveraging the\nspeaker's situational physiological responses in real-time to analyze the\nspeech signal effectively. The presented PASAD approach adapts a Hyper-Network\nstructure to extract temporal speech importance information leveraging\nphysiological parameters. Moreover, we collected speech and physiological\nsensing data from 73 preschool-age children who stutter (CWS) and who do not\nstutter (CWNS) in different conditions. PASAD's unique architecture enables\nidentifying speech attributes distinct to a CWS's fluent speech and mapping\nthem to the speaker's respective speech-motor-control factors. Extracted\nknowledge can enhance understanding of children's speech-motor-control and\nstuttering development. Our comprehensive evaluation shows that PASAD\noutperforms state-of-the-art multi-modal baseline approaches in different\nconditions, is expressive and adaptive to the speaker's speech and physiology,\ngeneralizable, robust, and is real-time executable."}
{"id": "2505.06605", "pdf": "https://arxiv.org/pdf/2505.06605", "abs": "https://arxiv.org/abs/2505.06605", "authors": ["Min Li", "Chun Yuan"], "title": "Using External knowledge to Enhanced PLM for Semantic Matching", "categories": ["cs.CL"], "comment": null, "summary": "Modeling semantic relevance has always been a challenging and critical task\nin natural language processing. In recent years, with the emergence of massive\namounts of annotated data, it has become feasible to train complex models, such\nas neural network-based reasoning models. These models have shown excellent\nperformance in practical applications and have achieved the current\nstate-ofthe-art performance. However, even with such large-scale annotated\ndata, we still need to think: Can machines learn all the knowledge necessary to\nperform semantic relevance detection tasks based on this data alone? If not,\nhow can neural network-based models incorporate external knowledge into\nthemselves, and how can relevance detection models be constructed to make full\nuse of external knowledge? In this paper, we use external knowledge to enhance\nthe pre-trained semantic relevance discrimination model. Experimental results\non 10 public datasets show that our method achieves consistent improvements in\nperformance compared to the baseline model."}
{"id": "2505.07240", "pdf": "https://arxiv.org/pdf/2505.07240", "abs": "https://arxiv.org/abs/2505.07240", "authors": ["Yating Yuan"], "title": "Continuous-Time Control Synthesis for Multiple Quadrotors under Signal Temporal Logic Specifications", "categories": ["eess.SY", "cs.MA", "cs.SY"], "comment": null, "summary": "Ensuring continuous-time control of multiple quadrotors in constrained\nenvironments under signal temporal logic (STL) specifications is challenging\ndue to nonlinear dynamics, safety constraints, and disturbances. This letter\nproposes a two-stage framework to address this challenge. First, exponentially\ndecaying tracking error bounds are derived with multidimensional geometric\ncontrol gains obtained via differential evolution. These bounds are less\nconservative, while the resulting tracking errors exhibit smaller oscillations\nand improved transient performance. Second, leveraging the time-varying bounds,\na mixed-integer convex programming (MICP) formulation generates piecewise\nB\\'ezier reference trajectories that satisfy STL and velocity limits, while\nensuring inter-agent safety through convex-hull properties. Simulation results\ndemonstrate that the proposed approach enables formally verifiable multi-agent\ncoordination in constrained environments, with provable tracking guarantees\nunder bounded disturbances."}
{"id": "2505.06512", "pdf": "https://arxiv.org/pdf/2505.06512", "abs": "https://arxiv.org/abs/2505.06512", "authors": ["Hang Wang", "Zhi-Qi Cheng", "Chenhao Lin", "Chao Shen", "Lei Zhang"], "title": "HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "Text-to-image synthesis has progressed to the point where models can generate\nvisually compelling images from natural language prompts. Yet, existing methods\noften fail to reconcile high-level semantic fidelity with explicit spatial\ncontrol, particularly in scenes involving multiple objects, nuanced relations,\nor complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal\nAlignment (HCMA) framework for grounded text-to-image generation. HCMA\nintegrates two alignment modules into each diffusion sampling step: a global\nmodule that continuously aligns latent representations with textual\ndescriptions to ensure scene-level coherence, and a local module that employs\nbounding-box layouts to anchor objects at specified locations, enabling\nfine-grained spatial control. Extensive experiments on the MS-COCO 2014\nvalidation set show that HCMA surpasses state-of-the-art baselines, achieving a\n0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP\nScore. These results demonstrate HCMA's effectiveness in faithfully capturing\nintricate textual semantics while adhering to user-defined spatial constraints,\noffering a robust solution for semantically grounded image generation.Our code\nis available at https://github.com/hwang-cs-ime/HCMA"}
{"id": "2505.06535", "pdf": "https://arxiv.org/pdf/2505.06535", "abs": "https://arxiv.org/abs/2505.06535", "authors": ["Anindya Sarkar", "Binglin Ji", "Yevgeniy Vorobeychik"], "title": "Online Feedback Efficient Active Target Discovery in Partially Observable Environments", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "30 pages, 28 figures, Pre-print", "summary": "In various scientific and engineering domains, where data acquisition is\ncostly, such as in medical imaging, environmental monitoring, or remote\nsensing, strategic sampling from unobserved regions, guided by prior\nobservations, is essential to maximize target discovery within a limited\nsampling budget. In this work, we introduce Diffusion-guided Active Target\nDiscovery (DiffATD), a novel method that leverages diffusion dynamics for\nactive target discovery. DiffATD maintains a belief distribution over each\nunobserved state in the environment, using this distribution to dynamically\nbalance exploration-exploitation. Exploration reduces uncertainty by sampling\nregions with the highest expected entropy, while exploitation targets areas\nwith the highest likelihood of discovering the target, indicated by the belief\ndistribution and an incrementally trained reward model designed to learn the\ncharacteristics of the target. DiffATD enables efficient target discovery in a\npartially observable environment within a fixed sampling budget, all without\nrelying on any prior supervised training. Furthermore, DiffATD offers\ninterpretability, unlike existing black-box policies that require extensive\nsupervised training. Through extensive experiments and ablation studies across\ndiverse domains, including medical imaging and remote sensing, we show that\nDiffATD performs significantly better than baselines and competitively with\nsupervised methods that operate under full environmental observability."}
{"id": "2505.06270", "pdf": "https://arxiv.org/pdf/2505.06270", "abs": "https://arxiv.org/abs/2505.06270", "authors": ["Seongmin Kim", "Kwanho Kim", "Minseung Kim", "Kanghyun Jo"], "title": "Importance Analysis for Dynamic Control of Balancing Parameter in a Simple Knowledge Distillation Setting", "categories": ["cs.LG", "cs.AI"], "comment": "3 pages, 2 figures, conference preprint for IWIS2025", "summary": "Although deep learning models owe their remarkable success to deep and\ncomplex architectures, this very complexity typically comes at the expense of\nreal-time performance. To address this issue, a variety of model compression\ntechniques have been proposed, among which knowledge distillation (KD) stands\nout for its strong empirical performance. The KD contains two concurrent\nprocesses: (i) matching the outputs of a large, pre-trained teacher network and\na lightweight student network, and (ii) training the student to solve its\ndesignated downstream task. The associated loss functions are termed the\ndistillation loss and the downsteam-task loss, respectively. Numerous prior\nstudies report that KD is most effective when the influence of the distillation\nloss outweighs that of the downstream-task loss. The influence(or importance)\nis typically regulated by a balancing parameter. This paper provides a\nmathematical rationale showing that in a simple KD setting when the loss is\ndecreasing, the balancing parameter should be dynamically adjusted"}
{"id": "2504.14906", "pdf": "https://arxiv.org/pdf/2504.14906", "abs": "https://arxiv.org/abs/2504.14906", "authors": ["Huadai Liu", "Tianyi Luo", "Qikai Jiang", "Kaicheng Luo", "Peiwen Sun", "Jialei Wan", "Rongjie Huang", "Qian Chen", "Wen Wang", "Xiangtai Li", "Shiliang Zhang", "Zhijie Yan", "Zhou Zhao", "Wei Xue"], "title": "OmniAudio: Generating Spatial Audio from 360-Degree Video", "categories": ["eess.AS", "cs.CV", "cs.SD"], "comment": "ICML 2025", "summary": "Traditional video-to-audio generation techniques primarily focus on\nfield-of-view (FoV) video and non-spatial audio, often missing the spatial cues\nnecessary for accurately representing sound sources in 3D environments. To\naddress this limitation, we introduce a novel task, 360V2SA, to generate\nspatial audio from 360-degree videos, specifically producing First-order\nAmbisonics (FOA) audio - a standard format for representing 3D spatial audio\nthat captures sound directionality and enables realistic 3D audio reproduction.\nWe first create Sphere360, a novel dataset tailored for this task that is\ncurated from real-world data. We also design an efficient semi-automated\npipeline for collecting and cleaning paired video-audio data. To generate\nspatial audio from 360-degree video, we propose a novel framework OmniAudio,\nwhich leverages self-supervised pre-training using both spatial audio data (in\nFOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a\ndual-branch framework that utilizes both panoramic and FoV video inputs to\ncapture comprehensive local and global information from 360-degree videos.\nExperimental results demonstrate that OmniAudio achieves state-of-the-art\nperformance across both objective and subjective metrics on Sphere360. Code and\ndatasets will be released at https://github.com/liuhuadai/OmniAudio. The demo\npage is available at https://OmniAudio-360V2SA.github.io."}
{"id": "2505.07364", "pdf": "https://arxiv.org/pdf/2505.07364", "abs": "https://arxiv.org/abs/2505.07364", "authors": ["Daria Zotova", "Nicolas Pinon", "Robin Trombetta", "Romain Bouet", "Julien Jung", "Carole Lartizien"], "title": "GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models", "categories": ["eess.IV", "cs.AI"], "comment": null, "summary": "Background and Objective. Research in the cross-modal medical image\ntranslation domain has been very productive over the past few years in tackling\nthe scarce availability of large curated multimodality datasets with the\npromising performance of GAN-based architectures. However, only a few of these\nstudies assessed task-based related performance of these synthetic data,\nespecially for the training of deep models. Method. We design and compare\ndifferent GAN-based frameworks for generating synthetic brain\n[18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first\nperform standard qualitative and quantitative visual quality evaluation. Then,\nwe explore further impact of using these fake PET data in the training of a\ndeep unsupervised anomaly detection (UAD) model designed to detect subtle\nepilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic\ntask-oriented quality metrics of the synthetic FDG PET data tailored to our\nunsupervised detection task, then use these fake data to train a use case UAD\nmodel combining a deep representation learning based on siamese autoencoders\nwith a OC-SVM density support estimation model. This model is trained on normal\nsubjects only and allows the detection of any variation from the pattern of the\nnormal population. We compare the detection performance of models trained on 35\npaired real MR T1 of normal subjects paired either on 35 true PET images or on\n35 synthetic PET images generated from the best performing generative models.\nPerformance analysis is conducted on 17 exams of epilepsy patients undergoing\nsurgery. Results. The best performing GAN-based models allow generating\nrealistic fake PET images of control subject with SSIM and PSNR values around\n0.9 and 23.8, respectively and in distribution (ID) with regard to the true\ncontrol dataset. The best UAD model trained on these synthetic normative PET\ndata allows reaching 74% sensitivity. Conclusion. Our results confirm that\nGAN-based models are the best suited for MR T1 to FDG PET translation,\noutperforming transformer or diffusion models. We also demonstrate the\ndiagnostic value of these synthetic data for the training of UAD models and\nevaluation on clinical exams of epilepsy patients. Our code and the normative\nimage dataset are available."}
{"id": "2411.12363", "pdf": "https://arxiv.org/pdf/2411.12363", "abs": "https://arxiv.org/abs/2411.12363", "authors": ["Zihao Chen", "Zhentao Lin", "Bi Zeng", "Linyi Huang", "Zhi Li", "Jia Cai"], "title": "DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "To ensure the reliable operation of speech systems across diverse\nenvironments, noise addition methods have emerged as the prevailing solution.\nHowever, existing methods offer limited coverage of real-world noisy scenes and\ndepend on pre-existing scene-based information and noise. This paper presents\nprompt-based Dynamic Generative Scene-based Noise Addition (DGSNA), a novel\nnoise addition methodology that integrates Dynamic Generation of Scene-based\nInformation (DGSI) with Scene-based Noise Addition for Speech (SNAS). This\nintegration facilitates automated scene-based noise addition by transforming\nclean speech into various noise environments, thereby providing a more\ncomprehensive and realistic simulation of diverse noise conditions.\nExperimental results demonstrate that DGSNA significantly enhances the\nrobustness of speech recognition and keyword spotting models across various\nnoise conditions, achieving a relative improvement of up to 11.21%.\nFurthermore, DGSNA can be effectively integrated with other noise addition\nmethods to enhance performance. Our implementation and demonstrations are\navailable at https://dgsna.github.io."}
{"id": "2505.06607", "pdf": "https://arxiv.org/pdf/2505.06607", "abs": "https://arxiv.org/abs/2505.06607", "authors": ["Min Li", "Chun Yuan"], "title": "Boosting Neural Language Inference via Cascaded Interactive Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Inference (NLI) focuses on ascertaining the logical\nrelationship (entailment, contradiction, or neutral) between a given premise\nand hypothesis. This task presents significant challenges due to inherent\nlinguistic features such as diverse phrasing, semantic complexity, and\ncontextual nuances. While Pre-trained Language Models (PLMs) built upon the\nTransformer architecture have yielded substantial advancements in NLI,\nprevailing methods predominantly utilize representations from the terminal\nlayer. This reliance on final-layer outputs may overlook valuable information\nencoded in intermediate layers, potentially limiting the capacity to model\nintricate semantic interactions effectively. Addressing this gap, we introduce\nthe Cascaded Interactive Reasoning Network (CIRN), a novel architecture\ndesigned for deeper semantic comprehension in NLI. CIRN implements a\nhierarchical feature extraction strategy across multiple network depths,\noperating within an interactive space where cross-sentence information is\ncontinuously integrated. This mechanism aims to mimic a process of progressive\nreasoning, transitioning from surface-level feature matching to uncovering more\nprofound logical and semantic connections between the premise and hypothesis.\nBy systematically mining latent semantic relationships at various\nrepresentational levels, CIRN facilitates a more thorough understanding of the\ninput pair. Comprehensive evaluations conducted on several standard NLI\nbenchmark datasets reveal consistent performance gains achieved by CIRN over\ncompetitive baseline approaches, demonstrating the efficacy of leveraging\nmulti-level interactive features for complex relational reasoning."}
{"id": "2505.07501", "pdf": "https://arxiv.org/pdf/2505.07501", "abs": "https://arxiv.org/abs/2505.07501", "authors": ["Purandar Bhaduri"], "title": "The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games", "categories": ["cs.GT", "cs.FL", "cs.LO", "cs.MA"], "comment": null, "summary": "We study rational synthesis problems for concurrent games with\n$\\omega$-regular objectives. Our model of rationality considers only pure\nstrategy Nash equilibria that satisfy either a social welfare or Pareto\noptimality condition with respect to an $\\omega$-regular objective for each\nagent. This extends earlier work on equilibria in concurrent games, without\nconsideration about their quality. Our results show that the existence of Nash\nequilibria satisfying social welfare conditions can be computed as efficiently\nas the constrained Nash equilibrium existence problem. On the other hand, the\nexistence of Nash equilibria satisfying the Pareto optimality condition\npossibly involves a higher upper bound, except in the case of B\\\"uchi and\nMuller games, for which all three problems are in the classes P and\nPSPACE-complete, respectively."}
{"id": "2505.06515", "pdf": "https://arxiv.org/pdf/2505.06515", "abs": "https://arxiv.org/abs/2505.06515", "authors": ["Zhiwen Zeng", "Yunfei Yin", "Zheng Yuan", "Argho Dey", "Xianjian Bao"], "title": "RESAR-BEV: An Explainable Progressive Residual Autoregressive Approach for Camera-Radar Fusion in BEV Segmentation", "categories": ["cs.CV"], "comment": "This work was submitted to IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS) on 09-May-2025", "summary": "Bird's-Eye-View (BEV) semantic segmentation provides comprehensive\nenvironmental perception for autonomous driving but suffers multi-modal\nmisalignment and sensor noise. We propose RESAR-BEV, a progressive refinement\nframework that advances beyond single-step end-to-end approaches: (1)\nprogressive refinement through residual autoregressive learning that decomposes\nBEV segmentation into interpretable coarse-to-fine stages via our\nDrive-Transformer and Modifier-Transformer residual prediction cascaded\narchitecture, (2) robust BEV representation combining ground-proximity voxels\nwith adaptive height offsets and dual-path voxel feature encoding\n(max+attention pooling) for efficient feature extraction, and (3) decoupled\nsupervision with offline Ground Truth decomposition and online joint\noptimization to prevent overfitting while ensuring structural coherence.\nExperiments on nuScenes demonstrate RESAR-BEV achieves state-of-the-art\nperformance with 54.0% mIoU across 7 essential driving-scene categories while\nmaintaining real-time capability at 14.6 FPS. The framework exhibits robustness\nin challenging scenarios of long-range perception and adverse weather\nconditions."}
{"id": "2505.06580", "pdf": "https://arxiv.org/pdf/2505.06580", "abs": "https://arxiv.org/abs/2505.06580", "authors": ["Dongyoon Yang", "Jihu Lee", "Yongdai Kim"], "title": "TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification", "categories": ["cs.AI", "stat.ML"], "comment": "Accepted in CVPR 2025 (19 pages, 7 figures)", "summary": "Robust domain adaptation against adversarial attacks is a critical research\narea that aims to develop models capable of maintaining consistent performance\nacross diverse and challenging domains. In this paper, we derive a new\ngeneralization bound for robust risk on the target domain using a novel\ndivergence measure specifically designed for robust domain adaptation. Building\nupon this, we propose a new algorithm named TAROT, which is designed to enhance\nboth domain adaptability and robustness. Through extensive experiments, TAROT\nnot only surpasses state-of-the-art methods in accuracy and robustness but also\nsignificantly enhances domain generalization and scalability by effectively\nlearning domain-invariant features. In particular, TAROT achieves superior\nperformance on the challenging DomainNet dataset, demonstrating its ability to\nlearn domain-invariant representations that generalize well across different\ndomains, including unseen ones. These results highlight the broader\napplicability of our approach in real-world domain adaptation scenarios."}
{"id": "2505.06272", "pdf": "https://arxiv.org/pdf/2505.06272", "abs": "https://arxiv.org/abs/2505.06272", "authors": ["Junzhou Xu", "Boyu Diao"], "title": "A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for Efficient Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As deep learning models expand, the pre-training-fine-tuning paradigm has\nbecome the standard approach for handling various downstream tasks. However,\nshared parameters can lead to diminished performance when dealing with complex\ndatasets involving multiple tasks. While introducing Mixture-of-Experts (MoE)\nmethods has alleviated this issue to some extent, it also significantly\nincreases the number of parameters required for fine-tuning and training time,\nintroducing greater parameter redundancy. To address these challenges, we\npropose a method for allocating expert numbers based on parameter sensitivity\nLoRA-SMoE (A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for\nEfficient Fine-Tuning). This method rapidly assesses the sensitivity of\ndifferent tasks to parameters by sampling a small amount of data and using\ngradient information. It then adaptively allocates expert numbers within a\ngiven budget. The process maintains comparable memory consumption to LoRA\n(Low-Rank Adaptation) while ensuring an efficient and resource-friendly\nfine-tuning procedure. Experimental results demonstrate that compared to SOTA\nfine-tuning methods, our LoRA-SMoE approach can enhance model performance while\nreducing the number of trainable parameters. This significantly improves model\nperformance in resource-constrained environments. Additionally, due to its\nefficient parameter sensitivity evaluation mechanism, LoRA-SMoE requires\nminimal computational overhead to optimize expert allocation, making it\nparticularly suitable for scenarios with limited computational resources. All\nthe code in this study will be made publicly available following the acceptance\nof the paper for publication. Source code is at\nhttps://github.com/EMLS-ICTCAS/LoRA-SMoE"}
{"id": "2502.02929", "pdf": "https://arxiv.org/pdf/2502.02929", "abs": "https://arxiv.org/abs/2502.02929", "authors": ["Brandon Woodard", "Margarita Geleta", "Joseph J. LaViola Jr.", "Andrea Fanelli", "Rhonda Wilson"], "title": "AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design in Augmented Reality", "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "Revision necessary for accuracy", "summary": "We present AudioMiXR, an augmented reality (AR) interface intended to assess\nhow users manipulate virtual audio objects situated in their physical space\nusing six degrees of freedom (6DoF) deployed on a head-mounted display (Apple\nVision Pro) for 3D sound design. Existing tools for 3D sound design are\ntypically constrained to desktop displays, which may limit spatial awareness of\nmixing within the execution environment. Utilizing an XR HMD to create\nsoundscapes may provide a real-time test environment for 3D sound design, as\nmodern HMDs can provide precise spatial localization assisted by cross-modal\ninteractions. However, there is no research on design guidelines specific to\nsound design with six degrees of freedom (6DoF) in XR. To provide a first step\ntoward identifying design-related research directions in this space, we\nconducted an exploratory study where we recruited 27 participants, consisting\nof expert and non-expert sound designers. The goal was to assess design lessons\nthat can be used to inform future research venues in 3D sound design. We ran a\nwithin-subjects study where users designed both a music and cinematic\nsoundscapes. After thematically analyzing participant data, we constructed two\ndesign lessons: 1. Proprioception for AR Sound Design, and 2. Balancing\nAudio-Visual Modalities in AR GUIs. Additionally, we provide application\ndomains that can benefit most from 6DoF sound design based on our results."}
{"id": "2505.07386", "pdf": "https://arxiv.org/pdf/2505.07386", "abs": "https://arxiv.org/abs/2505.07386", "authors": ["Rui Graca", "Tobi Delbruck"], "title": "Towards a physically realistic computationally efficient DVS pixel model", "categories": ["eess.IV"], "comment": "Presented in 2025 International Image Sensor Workshop", "summary": "Dynamic Vision Sensor (DVS) event camera models are important tools for\npredicting camera response, optimizing biases, and generating realistic\nsimulated datasets. Existing DVS models have been useful, but have not\ndemonstrated high realism for challenging HDR scenes combined with adequate\ncomputational efficiency for array-level scene simulation. This paper reports\nprogress towards a physically realistic and computationally efficient DVS model\nbased on large-signal differential equations derived from circuit analysis,\nwith parameters fitted from pixel measurements and circuit simulation. These\nare combined with an efficient stochastic event generation mechanism based on\nfirst-passage-time theory, allowing accurate noise generation with timesteps\ngreater than 1000x longer than previous methods"}
{"id": "2504.15118", "pdf": "https://arxiv.org/pdf/2504.15118", "abs": "https://arxiv.org/abs/2504.15118", "authors": ["Inho Kim", "Youngkil Song", "Jicheol Park", "Won Hwa Kim", "Suha Kwak"], "title": "Improving Sound Source Localization with Joint Slot Attention on Image and Audio", "categories": ["cs.CV", "cs.SD"], "comment": "Accepted to CVPR 2025", "summary": "Sound source localization (SSL) is the task of locating the source of sound\nwithin an image. Due to the lack of localization labels, the de facto standard\nin SSL has been to represent an image and audio as a single embedding vector\neach, and use them to learn SSL via contrastive learning. To this end, previous\nwork samples one of local image features as the image embedding and aggregates\nall local audio features to obtain the audio embedding, which is far from\noptimal due to the presence of noise and background irrelevant to the actual\ntarget in the input. We present a novel SSL method that addresses this chronic\nissue by joint slot attention on image and audio. To be specific, two slots\ncompetitively attend image and audio features to decompose them into target and\noff-target representations, and only target representations of image and audio\nare used for contrastive learning. Also, we introduce cross-modal attention\nmatching to further align local features of image and audio. Our method\nachieved the best in almost all settings on three public benchmarks for SSL,\nand substantially outperformed all the prior work in cross-modal retrieval."}
{"id": "2505.06624", "pdf": "https://arxiv.org/pdf/2505.06624", "abs": "https://arxiv.org/abs/2505.06624", "authors": ["Arezoo Hatefi", "Xuan-Son Vu", "Monowar Bhuyan", "Frank Drewes"], "title": "The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "We extend and study a semi-supervised model for text classification proposed\nearlier by Hatefi et al. for classification tasks in which document classes are\ndescribed by a small number of gold-labeled examples, while the majority of\ntraining examples is unlabeled. The model leverages the teacher-student\narchitecture of Meta Pseudo Labels in which a ''teacher'' generates labels for\noriginally unlabeled training data to train the ''student'' and updates its own\nmodel iteratively based on the performance of the student on the gold-labeled\nportion of the data. We extend the original model of Hatefi et al. by an\nunsupervised pre-training phase based on objective masking, and conduct\nin-depth performance evaluations of the original model, our extension, and\nvarious independent baselines. Experiments are performed using three different\ndatasets in two different languages (English and Swedish)."}
{"id": "2302.09859", "pdf": "https://arxiv.org/pdf/2302.09859", "abs": "https://arxiv.org/abs/2302.09859", "authors": ["Theodor Cimpeanu", "Luis Moniz Pereira", "The Anh Han"], "title": "The evolutionary advantage of guilt: co-evolution of social and non-social guilt in structured populations", "categories": ["cs.MA", "cs.AI", "cs.CY", "math.DS", "nlin.AO"], "comment": "26 pages, 6 figures, in press with the Journal of the Royal Society\n  Interface", "summary": "Building ethical machines may involve bestowing upon them the emotional\ncapacity to self-evaluate and repent on their actions. While apologies\nrepresent potential strategic interactions, the explicit evolution of guilt as\na behavioural trait remains poorly understood. Our study delves into the\nco-evolution of two forms of emotional guilt: social guilt entails a cost,\nrequiring agents to exert efforts to understand others' internal states and\nbehaviours; and non-social guilt, which only involves awareness of one's own\nstate, incurs no social cost. Resorting to methods from evolutionary game\ntheory, we study analytically, and through extensive numerical and agent-based\nsimulations, whether and how guilt can evolve and deploy, depending on the\nunderlying structure of the systems of agents. Our findings reveal that in\nlattice and scale-free networks, strategies favouring emotional guilt dominate\na broader range of guilt and social costs compared to non-structured well-mixed\npopulations, so leading to higher levels of cooperation. In structured\npopulations, both social and non-social guilt can thrive through clustering\nwith emotionally inclined strategies, thereby providing protection against\nexploiters, particularly for less costly non-social strategies. These insights\nshed light on the complex interplay of guilt and cooperation, enhancing our\nunderstanding of ethical artificial intelligence."}
{"id": "2505.06516", "pdf": "https://arxiv.org/pdf/2505.06516", "abs": "https://arxiv.org/abs/2505.06516", "authors": ["Yilin Dong", "Tianyun Zhu", "Xinde Li", "Jean Dezert", "Rigui Zhou", "Changming Zhu", "Lei Cao", "Shuzhi Sam Ge"], "title": "Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": "16 pages, 28 figures", "summary": "Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to\nderive a quantum mass function (QMF) as a fuzzy metric type from information\nobtained from various data sources. In addition, QDST uses quantum parallel\ncomputing to speed up computation. Nevertheless, the effective management of\nconflicts between multiple QMFs in QDST is a challenging question. This work\naims to address this problem by proposing a Quantum Conflict Indicator (QCI)\nthat measures the conflict between two QMFs in decision-making. Then, the\nproperties of the QCI are carefully investigated. The obtained results validate\nits compliance with desirable conflict measurement properties such as\nnon-negativity, symmetry, boundedness, extreme consistency and insensitivity to\nrefinement. We then apply the proposed QCI in conflict fusion methods and\ncompare its performance with several commonly used fusion approaches. This\ncomparison demonstrates the superiority of the QCI-based conflict fusion\nmethod. Moreover, the Class Description Domain Space (C-DDS) and its optimized\nversion, C-DDS+ by utilizing the QCI-based fusion method, are proposed to\naddress the Out-of-Distribution (OOD) detection task. The experimental results\nshow that the proposed approach gives better OOD performance with respect to\nseveral state-of-the-art baseline OOD detection methods. Specifically, it\nachieves an average increase in Area Under the Receiver Operating\nCharacteristic Curve (AUC) of 1.2% and a corresponding average decrease in\nFalse Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the\noptimal baseline method."}
{"id": "2505.06637", "pdf": "https://arxiv.org/pdf/2505.06637", "abs": "https://arxiv.org/abs/2505.06637", "authors": ["Chi Xu", "Yili Jin", "Sami Ma", "Rongsheng Qian", "Hao Fang", "Jiangchuan Liu", "Xue Liu", "Edith C. H. Ngai", "William I. Atlas", "Katrina M. Connors", "Mark A. Spoljaric"], "title": "Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers", "categories": ["cs.AI"], "comment": "10 pages, accepted by IJCAI 2025, AI and Social Good Track", "summary": "Wild salmon are essential to the ecological, economic, and cultural\nsustainability of the North Pacific Rim. Yet climate variability, habitat loss,\nand data limitations in remote ecosystems that lack basic infrastructure\nsupport pose significant challenges to effective fisheries management. This\nproject explores the integration of multimodal foundation AI and\nexpert-in-the-loop frameworks to enhance wild salmon monitoring and sustainable\nfisheries management in Indigenous rivers across Pacific Northwest. By\nleveraging video and sonar-based monitoring, we develop AI-powered tools for\nautomated species identification, counting, and length measurement, reducing\nmanual effort, expediting delivery of results, and improving decision-making\naccuracy. Expert validation and active learning frameworks ensure ecological\nrelevance while reducing annotation burdens. To address unique technical and\nsocietal challenges, we bring together a cross-domain, interdisciplinary team\nof university researchers, fisheries biologists, Indigenous stewardship\npractitioners, government agencies, and conservation organizations. Through\nthese collaborations, our research fosters ethical AI co-development, open data\nsharing, and culturally informed fisheries management."}
{"id": "2505.06273", "pdf": "https://arxiv.org/pdf/2505.06273", "abs": "https://arxiv.org/abs/2505.06273", "authors": ["Taehyun Cho", "Seokhun Ju", "Seungyub Han", "Dohyeong Kim", "Kyungjae Lee", "Jungwoo Lee"], "title": "Policy-labeled Preference Learning: Is Preference Enough for RLHF?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To design rewards that align with human goals, Reinforcement Learning from\nHuman Feedback (RLHF) has emerged as a prominent technique for learning reward\nfunctions from human preferences and optimizing policies via reinforcement\nlearning algorithms. However, existing RLHF methods often misinterpret\ntrajectories as being generated by an optimal policy, causing inaccurate\nlikelihood estimation and suboptimal learning. Inspired by Direct Preference\nOptimization framework which directly learns optimal policy without explicit\nreward, we propose policy-labeled preference learning (PPL), to resolve\nlikelihood mismatch issues by modeling human preferences with regret, which\nreflects behavior policy information. We also provide a contrastive KL\nregularization, derived from regret-based principles, to enhance RLHF in\nsequential decision making. Experiments in high-dimensional continuous control\ntasks demonstrate PPL's significant improvements in offline RLHF performance\nand its effectiveness in online settings."}
{"id": "2505.07449", "pdf": "https://arxiv.org/pdf/2505.07449", "abs": "https://arxiv.org/abs/2505.07449", "authors": ["Wei Li", "Ming Hu", "Guoan Wang", "Lihao Liu", "Kaijin Zhou", "Junzhi Ning", "Xin Guo", "Zongyuan Ge", "Lixu Gu", "Junjun He"], "title": "Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In ophthalmic surgery, developing an AI system capable of interpreting\nsurgical videos and predicting subsequent operations requires numerous\nophthalmic surgical videos with high-quality annotations, which are difficult\nto collect due to privacy concerns and labor consumption. Text-guided video\ngeneration (T2V) emerges as a promising solution to overcome this issue by\ngenerating ophthalmic surgical videos based on surgeon instructions. In this\npaper, we present Ophora, a pioneering model that can generate ophthalmic\nsurgical videos following natural language instructions. To construct Ophora,\nwe first propose a Comprehensive Data Curation pipeline to convert narrative\nophthalmic surgical videos into a large-scale, high-quality dataset comprising\nover 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive\nVideo-Instruction Tuning scheme to transfer rich spatial-temporal knowledge\nfrom a T2V model pre-trained on natural video-text datasets for\nprivacy-preserved ophthalmic surgical video generation based on Ophora-160K.\nExperiments on video quality evaluation via quantitative analysis and\nophthalmologist feedback demonstrate that Ophora can generate realistic and\nreliable ophthalmic surgical videos based on surgeon instructions. We also\nvalidate the capability of Ophora for empowering downstream tasks of ophthalmic\nsurgical workflow understanding. Code is available at\nhttps://github.com/mar-cry/Ophora."}
{"id": "2505.06630", "pdf": "https://arxiv.org/pdf/2505.06630", "abs": "https://arxiv.org/abs/2505.06630", "authors": ["Chunyi Yue", "Ang Li"], "title": "Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "17 pages, 5 figures, 3 tables", "summary": "Multi-domain sentiment classification aims to mitigate poor performance\nmodels due to the scarcity of labeled data in a single domain, by utilizing\ndata labeled from various domains. A series of models that jointly train domain\nclassifiers and sentiment classifiers have demonstrated their advantages,\nbecause domain classification helps generate necessary information for\nsentiment classification. Intuitively, the importance of sentiment\nclassification tasks is the same in all domains for multi-domain sentiment\nclassification; but domain classification tasks are different because the\nimpact of domain information on sentiment classification varies across\ndifferent fields; this can be controlled through adjustable weights or hyper\nparameters. However, as the number of domains increases, existing\nhyperparameter optimization algorithms may face the following challenges: (1)\ntremendous demand for computing resources, (2) convergence problems, and (3)\nhigh algorithm complexity. To efficiently generate the domain information\nrequired for sentiment classification in each domain, we propose a dynamic\ninformation modulation algorithm. Specifically, the model training process is\ndivided into two stages. In the first stage, a shared hyperparameter, which\nwould control the proportion of domain classification tasks across all fields,\nis determined. In the second stage, we introduce a novel domain-aware\nmodulation algorithm to adjust the domain information contained in the input\ntext, which is then calculated based on a gradient-based and loss-based method.\nIn summary, experimental results on a public sentiment analysis dataset\ncontaining 16 domains prove the superiority of the proposed method."}
{"id": "2405.03132", "pdf": "https://arxiv.org/pdf/2405.03132", "abs": "https://arxiv.org/abs/2405.03132", "authors": ["Lu Liu", "Maonan Wang", "Man-On Pun", "Xi Xiong"], "title": "A Multi-Agent Rollout Approach for Highway Bottleneck Decongestion in Mixed Autonomy", "categories": ["cs.MA"], "comment": "Accepted by the 2024 IEEE 27th International Conference on\n  Intelligent Transportation Systems (ITSC)", "summary": "The integration of autonomous vehicles (AVs) into the existing transportation\ninfrastructure offers a promising solution to alleviate congestion and enhance\nmobility. This research explores a novel approach to traffic optimization by\nemploying a multi-agent rollout approach within a mixed autonomy environment.\nThe study concentrates on coordinating the speed of human-driven vehicles by\nlongitudinally controlling AVs, aiming to dynamically optimize traffic flow and\nalleviate congestion at highway bottlenecks in real-time. We model the problem\nas a decentralized partially observable Markov decision process (Dec-POMDP) and\npropose an improved multi-agent rollout algorithm. By employing agent-by-agent\npolicy iterations, our approach implicitly considers cooperation among multiple\nagents and seamlessly adapts to complex scenarios where the number of agents\ndynamically varies. Validated in a real-world network with varying AV\npenetration rates and traffic flow, the simulations demonstrate that the\nmulti-agent rollout algorithm significantly enhances performance, reducing\naverage travel time on bottleneck segments by 9.42% with a 10% AV penetration\nrate."}
{"id": "2505.06517", "pdf": "https://arxiv.org/pdf/2505.06517", "abs": "https://arxiv.org/abs/2505.06517", "authors": ["Xiaohong Huang", "Cui Yang", "Miaowen Wen"], "title": "Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation", "categories": ["cs.CV", "cs.RO"], "comment": "9 pages with 9 figures", "summary": "This paper presents a visual-inertial odometry (VIO) method using\nlong-tracked features. Long-tracked features can constrain more visual frames,\nreducing localization drift. However, they may also lead to accumulated\nmatching errors and drift in feature tracking. Current VIO methods adjust\nobservation weights based on re-projection errors, yet this approach has flaws.\nRe-projection errors depend on estimated camera poses and map points, so\nincreased errors might come from estimation inaccuracies, not actual feature\ntracking errors. This can mislead the optimization process and make\nlong-tracked features ineffective for suppressing localization drift.\nFurthermore, long-tracked features constrain a larger number of frames, which\nposes a significant challenge to real-time performance of the system. To tackle\nthese issues, we propose an active decoupling mechanism for accumulated errors\nin long-tracked feature utilization. We introduce a visual reference frame\nreset strategy to eliminate accumulated tracking errors and a depth prediction\nstrategy to leverage the long-term constraint. To ensure real time preformane,\nwe implement three strategies for efficient system state estimation: a parallel\nelimination strategy based on predefined elimination order, an inverse-depth\nelimination simplification strategy, and an elimination skipping strategy.\nExperiments on various datasets show that our method offers higher positioning\naccuracy with relatively short consumption time, making it more suitable for\nedge-enabled low-altitude IoT navigation, where high-accuracy positioning and\nreal-time operation on edge device are required. The code will be published at\ngithub."}
{"id": "2505.06680", "pdf": "https://arxiv.org/pdf/2505.06680", "abs": "https://arxiv.org/abs/2505.06680", "authors": ["Linxuan Huang", "Dong-Fan Xie", "Li Li", "Zhengbing He"], "title": "A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.SY", "eess.SY", "physics.soc-ph"], "comment": null, "summary": "Lane-changing (LC) behavior, a critical yet complex driving maneuver,\nsignificantly influences driving safety and traffic dynamics. Traditional\nanalytical LC decision (LCD) models, while effective in specific environments,\noften oversimplify behavioral heterogeneity and complex interactions, limiting\ntheir capacity to capture real LCD. Data-driven approaches address these gaps\nby leveraging rich empirical data and machine learning to decode latent\ndecision-making patterns, enabling adaptive LCD modeling in dynamic\nenvironments. In light of the rapid development of artificial intelligence and\nthe demand for data-driven models oriented towards connected vehicles and\nautonomous vehicles, this paper presents a comprehensive survey of data-driven\nLCD models, with a particular focus on human drivers LC decision-making. It\nsystematically reviews the modeling framework, covering data sources and\npreprocessing, model inputs and outputs, objectives, structures, and validation\nmethods. This survey further discusses the opportunities and challenges faced\nby data-driven LCD models, including driving safety, uncertainty, as well as\nthe integration and improvement of technical frameworks."}
{"id": "2505.06274", "pdf": "https://arxiv.org/pdf/2505.06274", "abs": "https://arxiv.org/abs/2505.06274", "authors": ["Baijiong Lin", "Weisen Jiang", "Yuancheng Xu", "Hao Chen", "Ying-Cong Chen"], "title": "PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Multi-objective test-time alignment aims to adapt large language models\n(LLMs) to diverse multi-dimensional user preferences during inference while\nkeeping LLMs frozen. Recently, GenARM (Xu et al., 2025) first independently\ntrains Autoregressive Reward Models (ARMs) for each preference dimension\nwithout awareness of each other, then combines their outputs based on\nuser-specific preference vectors during inference to achieve multi-objective\ntest-time alignment, leading to two key limitations: the need for\n\\textit{multiple} ARMs increases the inference cost, and the separate training\nof ARMs causes the misalignment between the guided generation and the user\npreferences. To address these issues, we propose Preference-aware ARM (PARM), a\nsingle unified ARM trained across all preference dimensions. PARM uses our\nproposed Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA), which employs\na bilinear form to condition the ARM on preference vectors, enabling it to\nachieve precise control over preference trade-offs during inference.\nExperiments demonstrate that PARM reduces inference costs and achieves better\nalignment with preference vectors compared with existing methods. Additionally,\nPARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger\nfrozen LLM without expensive training, making multi-objective alignment\naccessible with limited computing resources. The code is available at\nhttps://github.com/Baijiong-Lin/PARM."}
{"id": "2505.07654", "pdf": "https://arxiv.org/pdf/2505.07654", "abs": "https://arxiv.org/abs/2505.07654", "authors": ["Pouya Afshin", "David Helminiak", "Tongtong Lu", "Tina Yen", "Julie M. Jorns", "Mollie Patton", "Bing Yu", "Dong Hye Ye"], "title": "Breast Cancer Classification in Deep Ultraviolet Fluorescence Images Using a Patch-Level Vision Transformer Framework", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Breast-conserving surgery (BCS) aims to completely remove malignant lesions\nwhile maximizing healthy tissue preservation. Intraoperative margin assessment\nis essential to achieve a balance between thorough cancer resection and tissue\nconservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM)\nenables rapid acquisition of whole surface images (WSIs) for excised tissue,\nproviding contrast between malignant and normal tissues. However, breast cancer\nclassification with DUV WSIs is challenged by high resolutions and complex\nhistopathological features. This study introduces a DUV WSI classification\nframework using a patch-level vision transformer (ViT) model, capturing local\nand global features. Grad-CAM++ saliency weighting highlights relevant spatial\nregions, enhances result interpretability, and improves diagnostic accuracy for\nbenign and malignant tissue classification. A comprehensive 5-fold\ncross-validation demonstrates the proposed approach significantly outperforms\nconventional deep learning methods, achieving a classification accuracy of\n98.33%."}
{"id": "2505.06633", "pdf": "https://arxiv.org/pdf/2505.06633", "abs": "https://arxiv.org/abs/2505.06633", "authors": ["Isaac Gerber"], "title": "Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Decoder-only transformer networks have become incredibly popular for language\nmodeling tasks. State-of-the-art models can have over a hundred transformer\nblocks, containing billions of trainable parameters, and are trained on\ntrillions of tokens of text. Each transformer block typically consists of a\nmulti-head attention (MHA) mechanism and a two-layer fully connected\nfeedforward network (FFN). In this paper, we examine the importance of the FFN\nduring the model pre-training process through a series of experiments,\nconfirming that the FFN is important to model performance. Furthermore, we show\nthat models using a transformer block configuration with three-layer FFNs with\nfewer such blocks outperform the standard two-layer configuration delivering\nlower training loss with fewer total parameters in less time."}
{"id": "2505.03586", "pdf": "https://arxiv.org/pdf/2505.03586", "abs": "https://arxiv.org/abs/2505.03586", "authors": ["Songchen Fu", "Siang Chen", "Shaojing Zhao", "Letian Bai", "Ta Li", "Yonghong Yan"], "title": "Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation", "categories": ["cs.MA", "cs.AI", "68T07 (Primary), 68T20, 68T42 (Secondary)", "I.2"], "comment": "The code will be open-sourced in the RDC-pymarl project under\n  https://github.com/linkjoker1006", "summary": "In real-world multi-agent systems (MASs), observation delays are ubiquitous,\npreventing agents from making decisions based on the environment's true state.\nAn individual agent's local observation often consists of multiple components\nfrom other agents or dynamic entities in the environment. These discrete\nobservation components with varying delay characteristics pose significant\nchallenges for multi-agent reinforcement learning (MARL). In this paper, we\nfirst formulate the decentralized stochastic individual delay partially\nobservable Markov decision process (DSID-POMDP) by extending the standard\nDec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL\ntraining framework for addressing stochastic individual delays, along with\nrecommended implementations for its constituent modules. We implement the\nDSID-POMDP's observation generation pattern using standard MARL benchmarks,\nincluding MPE and SMAC. Experiments demonstrate that baseline MARL methods\nsuffer severe performance degradation under fixed and unfixed delays. The\nRDC-enhanced approach mitigates this issue, remarkably achieving ideal\ndelay-free performance in certain delay scenarios while maintaining\ngeneralizability. Our work provides a novel perspective on multi-agent delayed\nobservation problems and offers an effective solution framework. The source\ncode is available at https://anonymous.4open.science/r/RDC-pymarl-4512/."}
{"id": "2505.06524", "pdf": "https://arxiv.org/pdf/2505.06524", "abs": "https://arxiv.org/abs/2505.06524", "authors": ["Jingyao Wang", "Jianqi Zhang", "Wenwen Qiang", "Changwen Zheng"], "title": "Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Despite the strength of the Segment Anything Model (SAM), it struggles with\ngeneralization issues in open-vocabulary multi-entity segmentation (OVMS).\nThrough empirical and causal analyses, we find that (i) the prompt bias is the\nprimary cause of the generalization issues; (ii) this bias is closely tied to\nthe task-irrelevant generating factors within the prompts, which act as\nconfounders and affect generalization. To address the generalization issues, we\naim to propose a method that can calibrate prompts to eliminate confounders for\naccurate OVMS. Building upon the causal analysis, we propose that the optimal\nprompt for OVMS should contain only task-relevant causal factors. We define it\nas the causal prompt, serving as the goal of calibration. Next, our theoretical\nanalysis, grounded by causal multi-distribution consistency theory, proves that\nthis prompt can be obtained by enforcing segmentation consistency and\noptimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration\nmethod for SAM to achieve accurate OVMS. It integrates a lightweight causal\nprompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first\ngenerate multiple prompts using random annotations to simulate diverse\ndistributions and then reweight them via CaPL by enforcing causal\nmulti-distribution consistency in both task and entity levels. To ensure\nobtaining causal prompts, CaPL is optimized by minimizing the cumulative\nsegmentation loss across the reweighted prompts to achieve consistency and\noptimality. A bi-level optimization strategy alternates between optimizing CaPL\nand SAM, ensuring accurate OVMS. Extensive experiments validate its\nsuperiority."}
{"id": "2505.06706", "pdf": "https://arxiv.org/pdf/2505.06706", "abs": "https://arxiv.org/abs/2505.06706", "authors": ["Yuxuan Zheng", "Yihe Zhou", "Feiyang Xu", "Mingli Song", "Shunyu Liu"], "title": "Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL", "categories": ["cs.AI"], "comment": null, "summary": "Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the\ncurse of dimensionality, as the exponential growth in agent interactions\nsignificantly increases computational complexity and impedes learning\nefficiency. To mitigate this, existing efforts that rely on Mean Field (MF)\nsimplify the interaction landscape by approximating neighboring agents as a\nsingle mean agent, thus reducing overall complexity to pairwise interactions.\nHowever, these MF methods inevitably fail to account for individual\ndifferences, leading to aggregation noise caused by inaccurate iterative\nupdates during MF learning. In this paper, we propose a Bi-level Mean Field\n(BMF) method to capture agent diversity with dynamic grouping in large-scale\nMARL, which can alleviate aggregation noise via bi-level interaction.\nSpecifically, BMF introduces a dynamic group assignment module, which employs a\nVariational AutoEncoder (VAE) to learn the representations of agents,\nfacilitating their dynamic grouping over time. Furthermore, we propose a\nbi-level interaction module to model both inter- and intra-group interactions\nfor effective neighboring aggregation. Experiments across various tasks\ndemonstrate that the proposed BMF yields results superior to the\nstate-of-the-art methods. Our code will be made publicly available."}
{"id": "2505.06275", "pdf": "https://arxiv.org/pdf/2505.06275", "abs": "https://arxiv.org/abs/2505.06275", "authors": ["Yuzhou Zhu", "Zheng Zhang", "Ruyi Zhang", "Liang Zhou"], "title": "Attonsecond Streaking Phase Retrieval Via Deep Learning Methods", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.optics"], "comment": null, "summary": "Attosecond streaking phase retrieval is essential for resolving electron\ndynamics on sub-femtosecond time scales yet traditional algorithms rely on\niterative minimization and central momentum approximations that degrade\naccuracy for broadband pulses. In this work phase retrieval is reformulated as\na supervised computer-vision problem and four neural architectures are\nsystematically compared. A convolutional network demonstrates strong\nsensitivity to local streak edges but lacks global context; a vision\ntransformer captures long-range delay-energy correlations at the expense of\nlocal inductive bias; a hybrid CNN-ViT model unites local feature extraction\nand full-graph attention; and a capsule network further enforces spatial pose\nagreement through dynamic routing. A theoretical analysis introduces local,\nglobal and positional sensitivity measures and derives surrogate error bounds\nthat predict the strict ordering $CNN<ViT<Hybrid<Capsule$. Controlled\nexperiments on synthetic streaking spectrograms confirm this hierarchy, with\nthe capsule network achieving the highest retrieval fidelity. Looking forward,\nembedding the strong-field integral into physics-informed neural networks and\nexploring photonic hardware implementations promise pathways toward real-time\nattosecond pulse characterization under demanding experimental conditions."}
{"id": "2505.07661", "pdf": "https://arxiv.org/pdf/2505.07661", "abs": "https://arxiv.org/abs/2505.07661", "authors": ["Elad Yoshai", "Dana Yagoda-Aharoni", "Eden Dotan", "Natan T. Shaked"], "title": "Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present SparseAttnNet, a new hierarchical attention-driven framework for\nefficient image classification that adaptively selects and processes only the\nmost informative pixels from images. Traditional convolutional neural networks\ntypically process the entire images regardless of information density, leading\nto computational inefficiency and potential focus on irrelevant features. Our\napproach leverages a dynamic selection mechanism that uses coarse attention\ndistilled by fine multi-head attention from the downstream layers of the model,\nallowing the model to identify and extract the most salient k pixels, where k\nis adaptively learned during training based on loss convergence trends. Once\nthe top-k pixels are selected, the model processes only these pixels, embedding\nthem as words in a language model to capture their semantics, followed by\nmulti-head attention to incorporate global context. For biological cell images,\nwe demonstrate that SparseAttnNet can process approximately 15% of the pixels\ninstead of the full image. Applied to cell classification tasks using white\nblood cells images from the following modalities: optical path difference (OPD)\nimages from digital holography for stain-free cells, images from\nmotion-sensitive (event) camera from stain-free cells, and brightfield\nmicroscopy images of stained cells, For all three imaging modalities,\nSparseAttnNet achieves competitive accuracy while drastically reducing\ncomputational requirements in terms of both parameters and floating-point\noperations per second, compared to traditional CNNs and Vision Transformers.\nSince the model focuses on biologically relevant regions, it also offers\nimproved explainability. The adaptive and lightweight nature of SparseAttnNet\nmakes it ideal for deployment in resource-constrained and high-throughput\nsettings, including imaging flow cytometry."}
{"id": "2505.06696", "pdf": "https://arxiv.org/pdf/2505.06696", "abs": "https://arxiv.org/abs/2505.06696", "authors": ["Dominik Koterwa", "Maciej Świtała"], "title": "Enhancing BERTopic with Intermediate Layer Representations", "categories": ["cs.CL"], "comment": "Repository with code for reproduction:\n  https://github.com/dkoterwa/optimizing_bertopic", "summary": "BERTopic is a topic modeling algorithm that leverages transformer-based\nembeddings to create dense clusters, enabling the estimation of topic\nstructures and the extraction of valuable insights from a corpus of documents.\nThis approach allows users to efficiently process large-scale text data and\ngain meaningful insights into its structure. While BERTopic is a powerful tool,\nembedding preparation can vary, including extracting representations from\nintermediate model layers and applying transformations to these embeddings. In\nthis study, we evaluate 18 different embedding representations and present\nfindings based on experiments conducted on three diverse datasets. To assess\nthe algorithm's performance, we report topic coherence and topic diversity\nmetrics across all experiments. Our results demonstrate that, for each dataset,\nit is possible to find an embedding configuration that performs better than the\ndefault setting of BERTopic. Additionally, we investigate the influence of stop\nwords on different embedding configurations."}
{"id": "2502.12275", "pdf": "https://arxiv.org/pdf/2502.12275", "abs": "https://arxiv.org/abs/2502.12275", "authors": ["Franciszek Górski", "Oskar Wysocki", "Marco Valentino", "Andre Freitas"], "title": "Integrating Expert Knowledge into Logical Programs via LLMs", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "This paper introduces ExKLoP, a novel framework designed to evaluate how\neffectively Large Language Models (LLMs) integrate expert knowledge into\nlogical reasoning systems. This capability is especially valuable in\nengineering, where expert knowledge-such as manufacturer-recommended\noperational ranges-can be directly embedded into automated monitoring systems.\nBy mirroring expert verification steps, tasks like range checking and\nconstraint validation help ensure system safety and reliability. Our approach\nsystematically evaluates LLM-generated logical rules, assessing both syntactic\nfluency and logical correctness in these critical validation tasks. We also\nexplore the models' capacity for self-correction via an iterative feedback loop\nbased on code execution outcomes. ExKLoP presents an extensible dataset\ncomprising 130 engineering premises, 950 prompts, and corresponding validation\npoints. It enables comprehensive benchmarking while allowing control over task\ncomplexity and scalability of experiments. We leverage the synthetic data\ncreation methodology to conduct extensive empirical evaluation on a diverse set\nof LLMs including Llama3, Gemma3, Codestral and QwenCoder. The results reveal\nthat most models generate nearly perfect syntactically correct code and exhibit\nstrong performance in translating expert knowledge into correct code. At the\nsame time, while most LLMs produce nearly flawless syntactic output, their\nability to correctly implement logical rules varies, as does their capacity for\nself-improvement. Overall, ExKLoP serves as a robust evaluation platform that\nstreamlines the selection of effective models for self-correcting systems while\nclearly delineating the types of errors encountered."}
{"id": "2505.06527", "pdf": "https://arxiv.org/pdf/2505.06527", "abs": "https://arxiv.org/abs/2505.06527", "authors": ["Jing Hu", "Kaiwei Yu", "Hongjiang Xian", "Shu Hu", "Xin Wang"], "title": "Improving Generalization of Medical Image Registration Foundation Model", "categories": ["cs.CV", "cs.AI"], "comment": "IJCNN", "summary": "Deformable registration is a fundamental task in medical image processing,\naiming to achieve precise alignment by establishing nonlinear correspondences\nbetween images. Traditional methods offer good adaptability and\ninterpretability but are limited by computational efficiency. Although deep\nlearning approaches have significantly improved registration speed and\naccuracy, they often lack flexibility and generalizability across different\ndatasets and tasks. In recent years, foundation models have emerged as a\npromising direction, leveraging large and diverse datasets to learn universal\nfeatures and transformation patterns for image registration, thus demonstrating\nstrong cross-task transferability. However, these models still face challenges\nin generalization and robustness when encountering novel anatomical structures,\nvarying imaging conditions, or unseen modalities. To address these limitations,\nthis paper incorporates Sharpness-Aware Minimization (SAM) into foundation\nmodels to enhance their generalization and robustness in medical image\nregistration. By optimizing the flatness of the loss landscape, SAM improves\nmodel stability across diverse data distributions and strengthens its ability\nto handle complex clinical scenarios. Experimental results show that foundation\nmodels integrated with SAM achieve significant improvements in cross-dataset\nregistration performance, offering new insights for the advancement of medical\nimage registration technology. Our code is available at\nhttps://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\\_sam."}
{"id": "2505.06769", "pdf": "https://arxiv.org/pdf/2505.06769", "abs": "https://arxiv.org/abs/2505.06769", "authors": ["Krishnendu Chatterjee", "Mahdi JafariRaviz", "Raimundo Saona", "Jakub Svoboda"], "title": "Value Iteration with Guessing for Markov Chains and Markov Decision Processes", "categories": ["cs.AI", "cs.CC"], "comment": "Appeared in the 31st International Conference on Tools and Algorithms\n  for the Construction and Analysis of Systems (TACAS 2025)", "summary": "Two standard models for probabilistic systems are Markov chains (MCs) and\nMarkov decision processes (MDPs). Classic objectives for such probabilistic\nmodels for control and planning problems are reachability and stochastic\nshortest path. The widely studied algorithmic approach for these problems is\nthe Value Iteration (VI) algorithm which iteratively applies local updates\ncalled Bellman updates. There are many practical approaches for VI in the\nliterature but they all require exponentially many Bellman updates for MCs in\nthe worst case. A preprocessing step is an algorithm that is discrete,\ngraph-theoretical, and requires linear space. An important open question is\nwhether, after a polynomial-time preprocessing, VI can be achieved with\nsub-exponentially many Bellman updates. In this work, we present a new approach\nfor VI based on guessing values. Our theoretical contributions are twofold.\nFirst, for MCs, we present an almost-linear-time preprocessing algorithm after\nwhich, along with guessing values, VI requires only subexponentially many\nBellman updates. Second, we present an improved analysis of the speed of\nconvergence of VI for MDPs. Finally, we present a practical algorithm for MDPs\nbased on our new approach. Experimental results show that our approach provides\na considerable improvement over existing VI-based approaches on several\nbenchmark examples from the literature."}
{"id": "2505.06279", "pdf": "https://arxiv.org/pdf/2505.06279", "abs": "https://arxiv.org/abs/2505.06279", "authors": ["Shashwat Pandey"], "title": "Interpretable Learning Dynamics in Unsupervised Reinforcement Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We present an interpretability framework for unsupervised reinforcement\nlearning (URL) agents, aimed at understanding how intrinsic motivation shapes\nattention, behavior, and representation learning. We analyze five agents DQN,\nRND, ICM, PPO, and a Transformer-RND variant trained on procedurally generated\nenvironments, using Grad-CAM, Layer-wise Relevance Propagation (LRP),\nexploration metrics, and latent space clustering. To capture how agents\nperceive and adapt over time, we introduce two metrics: attention diversity,\nwhich measures the spatial breadth of focus, and attention change rate, which\nquantifies temporal shifts in attention. Our findings show that\ncuriosity-driven agents display broader, more dynamic attention and exploratory\nbehavior than their extrinsically motivated counterparts. Among them,\nTransformerRND combines wide attention, high exploration coverage, and compact,\nstructured latent representations. Our results highlight the influence of\narchitectural inductive biases and training signals on internal agent dynamics.\nBeyond reward-centric evaluation, the proposed framework offers diagnostic\ntools to probe perception and abstraction in RL agents, enabling more\ninterpretable and generalizable behavior."}
{"id": "2505.07687", "pdf": "https://arxiv.org/pdf/2505.07687", "abs": "https://arxiv.org/abs/2505.07687", "authors": ["Feng Yuan", "Yifan Gao", "Wenbin Wu", "Keqing Wu", "Xiaotong Guo", "Jie Jiang", "Xin Gao"], "title": "ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025(under view)", "summary": "Accurate multi-modal medical image translation requires ha-rmonizing global\nanatomical semantics and local structural fidelity, a challenge complicated by\nintermodality information loss and structural distortion. We propose ABS-Mamba,\na novel architecture integrating the Segment Anything Model 2 (SAM2) for\norgan-aware semantic representation, specialized convolutional neural networks\n(CNNs) for preserving modality-specific edge and texture details, and Mamba's\nselective state-space modeling for efficient long- and short-range feature\ndependencies. Structurally, our dual-resolution framework leverages SAM2's\nimage encoder to capture organ-scale semantics from high-resolution inputs,\nwhile a parallel CNNs branch extracts fine-grained local features. The Robust\nFeature Fusion Network (RFFN) integrates these epresentations, and the\nBidirectional Mamba Residual Network (BMRN) models spatial dependencies using\nspiral scanning and bidirectional state-space dynamics. A three-stage skip\nfusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank\nAdaptation (LoRA+) fine-tuning to enable precise domain specialization while\nmaintaining the foundational capabilities of the pre-trained components.\nExtensive experimental validation on the SynthRAD2023 and BraTS2019 datasets\ndemonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering\nhigh-fidelity cross-modal synthesis that preserves anatomical semantics and\nstructural details to enhance diagnostic accuracy in clinical applications. The\ncode is available at https://github.com/gatina-yone/ABS-Mamba"}
{"id": "2505.06698", "pdf": "https://arxiv.org/pdf/2505.06698", "abs": "https://arxiv.org/abs/2505.06698", "authors": ["Zongqi Wang", "Tianle Gu", "Chen Gong", "Xin Tian", "Siqi Bao", "Yujiu Yang"], "title": "From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback", "categories": ["cs.CL"], "comment": null, "summary": "Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena\nare seeing growing adoption for the evaluation of Large Language Models (LLMs).\nExisting research has primarily focused on approximating human-based model\nrankings using limited data and LLM-as-a-Judge. However, the fundamental\npremise of these studies, which attempts to replicate human rankings, is\nflawed. Specifically, these benchmarks typically offer only overall scores,\nlimiting their utility to leaderboard rankings, rather than providing feedback\nthat can guide model optimization and support model profiling. Therefore, we\nadvocate for an evaluation paradigm shift from approximating human-based model\nrankings to providing feedback with analytical value. To this end, we introduce\nFeedbacker, an evaluation framework that provides comprehensive and\nfine-grained results, thereby enabling thorough identification of a model's\nspecific strengths and weaknesses. Such feedback not only supports the targeted\noptimization of the model but also enhances the understanding of its behavior.\nFeedbacker comprises three key components: an extensible tree-based query\ntaxonomy builder, an automated query synthesis scheme, and a suite of\nvisualization and analysis tools. Furthermore, we propose a novel\nLLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise\nevaluation. This method derives evaluation criteria by pre-comparing the\ndifferences between several auxiliary responses, achieving the accuracy of\npairwise evaluation while maintaining the time complexity of pointwise\nevaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs,\nwe demonstrate the usage of Feedbacker and highlight its effectiveness and\npotential. Our homepage project is available at\nhttps://liudan193.github.io/Feedbacker."}
{"id": "2503.21633", "pdf": "https://arxiv.org/pdf/2503.21633", "abs": "https://arxiv.org/abs/2503.21633", "authors": ["David Emanuele Corrado Raphael Catania", "Alessandro Buratto", "Giovanni Perin"], "title": "Static and Repeated Cooperative Games for the Optimization of the AoI in IoT Networks", "categories": ["cs.NI", "cs.GT", "cs.MA"], "comment": "Accepted to MedComNet 2025 (Cagliari, June 2025). 6 pages, 7 figures", "summary": "Wireless sensing and the internet of things (IoT) are nowadays pervasive in\n5G and beyond networks, and they are expected to play a crucial role in 6G.\nHowever, a centralized optimization of a distributed system is not always\npossible and cost-efficient. In this paper, we analyze a setting in which two\nsensors collaboratively update a common server seeking to minimize the age of\ninformation (AoI) of the latest sample of a common physical process. We\nconsider a distributed and uncoordinated setting where each sensor lacks\ninformation about whether the other decides to update the server. This\nstrategic setting is modeled through game theory (GT) and two games are\ndefined: i) a static game of complete information with an incentive mechanism\nfor cooperation, and ii) a repeated game over a finite horizon where the static\ngame is played at each stage. We perform a mathematical analysis of the static\ngame finding three Nash Equilibria (NEs) in pure strategies and one in mixed\nstrategies. A numerical simulation of the repeated game is also presented and\nnovel and valuable insight into the setting is given thanks to the definition\nof a new metric, the price of delayed updates (PoDU), which shows that the\ndecentralized solution provides results close to the centralized optimum."}
{"id": "2505.06528", "pdf": "https://arxiv.org/pdf/2505.06528", "abs": "https://arxiv.org/abs/2505.06528", "authors": ["Mahmudul Hasan"], "title": "Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection", "categories": ["cs.CV"], "comment": null, "summary": "Deepfake videos, produced through advanced artificial intelligence methods\nnow a days, pose a new challenge to the truthfulness of the digital media. As\nDeepfake becomes more convincing day by day, detecting them requires advanced\nmethods capable of identifying subtle inconsistencies. The primary motivation\nof this paper is to recognize deepfake videos using deep learning techniques,\nspecifically by using convolutional neural networks. Deep learning excels in\npattern recognition, hence, makes it an ideal approach for detecting the\nintricate manipulations in deepfakes. In this paper, we consider using MTCNN as\na face detector and EfficientNet-B5 as encoder model to predict if a video is\ndeepfake or not. We utilize training and evaluation dataset from Kaggle DFDC.\nThe results shows that our deepfake detection model acquired 42.78% log loss,\n93.80% AUC and 86.82% F1 score on kaggle's DFDC dataset."}
{"id": "2505.06817", "pdf": "https://arxiv.org/pdf/2505.06817", "abs": "https://arxiv.org/abs/2505.06817", "authors": ["Sivasathivel Kandasamy"], "title": "Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems", "categories": ["cs.AI"], "comment": "2 Figures and 2 Tables", "summary": "Agentic AI systems represent a new frontier in artificial intelligence, where\nagents often based on large language models(LLMs) interact with tools,\nenvironments, and other agents to accomplish tasks with a degree of autonomy.\nThese systems show promise across a range of domains, but their architectural\nunderpinnings remain immature. This paper conducts a comprehensive review of\nthe types of agents, their modes of interaction with the environment, and the\ninfrastructural and architectural challenges that emerge. We identify a gap in\nhow these systems manage tool orchestration at scale and propose a reusable\ndesign abstraction: the \"Control Plane as a Tool\" pattern. This pattern allows\ndevelopers to expose a single tool interface to an agent while encapsulating\nmodular tool routing logic behind it. We position this pattern within the\nbroader context of agent design and argue that it addresses several key\nchallenges in scaling, safety, and extensibility."}
{"id": "2505.06280", "pdf": "https://arxiv.org/pdf/2505.06280", "abs": "https://arxiv.org/abs/2505.06280", "authors": ["Gabriele Rosi", "Fabio Cermelli"], "title": "Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation", "categories": ["cs.LG"], "comment": "Accepted to PixFoundation workshop at CVPR2025. Code:\n  https://github.com/FocoosAI/ShowOrTell", "summary": "Prompt engineering has shown remarkable success with large language models,\nyet its systematic exploration in computer vision remains limited. In semantic\nsegmentation, both textual and visual prompts offer distinct advantages:\ntextual prompts through open-vocabulary methods allow segmentation of arbitrary\ncategories, while visual reference prompts provide intuitive reference\nexamples. However, existing benchmarks evaluate these modalities in isolation,\nwithout direct comparison under identical conditions. We present Show or Tell\n(SoT), a novel benchmark specifically designed to evaluate both visual and\ntextual prompts for semantic segmentation across 14 datasets spanning 7 diverse\ndomains (common scenes, urban, food, waste, parts, tools, and land-cover). We\nevaluate 5 open-vocabulary methods and 4 visual reference prompt approaches,\nadapting the latter to handle multi-class segmentation through a\nconfidence-based mask merging strategy. Our extensive experiments reveal that\nopen-vocabulary methods excel with common concepts easily described by text but\nstruggle with complex domains like tools, while visual reference prompt methods\nachieve good average results but exhibit high variability depending on the\ninput prompt. Through comprehensive quantitative and qualitative analysis, we\nidentify the strengths and weaknesses of both prompting modalities, providing\nvaluable insights to guide future research in vision foundation models for\nsegmentation tasks."}
{"id": "2505.06668", "pdf": "https://arxiv.org/pdf/2505.06668", "abs": "https://arxiv.org/abs/2505.06668", "authors": ["Ziyi Wang", "Haipeng Li", "Lin Sui", "Tianhao Zhou", "Hai Jiang", "Lang Nie", "Shuaicheng Liu"], "title": "StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "We present StableMotion, a novel framework leverages knowledge (geometry and\ncontent priors) from pretrained large-scale image diffusion models to perform\nmotion estimation, solving single-image-based image rectification tasks such as\nStitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC).\nSpecifically, StableMotion framework takes text-to-image Stable Diffusion (SD)\nmodels as backbone and repurposes it into an image-to-motion estimator. To\nmitigate inconsistent output produced by diffusion models, we propose Adaptive\nEnsemble Strategy (AES) that consolidates multiple outputs into a cohesive,\nhigh-fidelity result. Additionally, we present the concept of Sampling Steps\nDisaster (SSD), the counterintuitive scenario where increasing the number of\nsampling steps can lead to poorer outcomes, which enables our framework to\nachieve one-step inference. StableMotion is verified on two image rectification\ntasks and delivers state-of-the-art performance in both, as well as showing\nstrong generalizability. Supported by SSD, StableMotion offers a speedup of 200\ntimes compared to previous diffusion model-based methods."}
{"id": "2505.06708", "pdf": "https://arxiv.org/pdf/2505.06708", "abs": "https://arxiv.org/abs/2505.06708", "authors": ["Zihan Qiu", "Zekun Wang", "Bo Zheng", "Zeyu Huang", "Kaiyue Wen", "Songlin Yang", "Rui Men", "Le Yu", "Fei Huang", "Suozhi Huang", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin"], "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free", "categories": ["cs.CL"], "comment": null, "summary": "Gating mechanisms have been widely utilized, from early models like LSTMs and\nHighway Networks to recent state space models, linear attention, and also\nsoftmax attention. Yet, existing literature rarely examines the specific\neffects of gating. In this work, we conduct comprehensive experiments to\nsystematically investigate gating-augmented softmax attention variants.\nSpecifically, we perform a comprehensive comparison over 30 variants of 15B\nMixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion\ntoken dataset. Our central finding is that a simple modification-applying a\nhead-specific sigmoid gate after the Scaled Dot-Product Attention\n(SDPA)-consistently improves performance. This modification also enhances\ntraining stability, tolerates larger learning rates, and improves scaling\nproperties. By comparing various gating positions and computational variants,\nwe attribute this effectiveness to two key factors: (1) introducing\nnon-linearity upon the low-rank mapping in the softmax attention, and (2)\napplying query-dependent sparse gating scores to modulate the SDPA output.\nNotably, we find this sparse gating mechanism mitigates 'attention sink' and\nenhances long-context extrapolation performance, and we also release related\n$\\href{https://github.com/qiuzh20/gated_attention}{codes}$ and\n$\\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate\nfuture research."}
{"id": "2505.05029", "pdf": "https://arxiv.org/pdf/2505.05029", "abs": "https://arxiv.org/abs/2505.05029", "authors": ["Siyue Ren", "Wanli Fu", "Xinkun Zou", "Chen Shen", "Yi Cai", "Chen Chu", "Zhen Wang", "Shuyue Hu"], "title": "Beyond the Tragedy of the Commons: Building A Reputation System for Generative Multi-agent Systems", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The tragedy of the commons, where individual self-interest leads to\ncollectively disastrous outcomes, is a pervasive challenge in human society.\nRecent studies have demonstrated that similar phenomena can arise in generative\nmulti-agent systems (MASs). To address this challenge, this paper explores the\nuse of reputation systems as a remedy. We propose RepuNet, a dynamic,\ndual-level reputation framework that models both agent-level reputation\ndynamics and system-level network evolution. Specifically, driven by direct\ninteractions and indirect gossip, agents form reputations for both themselves\nand their peers, and decide whether to connect or disconnect other agents for\nfuture interactions. Through two distinct scenarios, we show that RepuNet\neffectively mitigates the 'tragedy of the commons', promoting and sustaining\ncooperation in generative MASs. Moreover, we find that reputation systems can\ngive rise to rich emergent behaviors in generative MASs, such as the formation\nof cooperative clusters, the social isolation of exploitative agents, and the\npreference for sharing positive gossip rather than negative ones."}
{"id": "2505.06536", "pdf": "https://arxiv.org/pdf/2505.06536", "abs": "https://arxiv.org/abs/2505.06536", "authors": ["Feng Liu", "Ziwang Fu", "Yunlong Wang", "Qijian Zheng"], "title": "TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2111.02172", "summary": "The fusion technique is the key to the multimodal emotion recognition task.\nRecently, cross-modal attention-based fusion methods have demonstrated high\nperformance and strong robustness. However, cross-modal attention suffers from\nredundant features and does not capture complementary features well. We find\nthat it is not necessary to use the entire information of one modality to\nreinforce the other during cross-modal interaction, and the features that can\nreinforce a modality may contain only a part of it. To this end, we design an\ninnovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN).\nSpecifically, for the redundant features, we make one modality perform\nintra-modal feature selection through a self-attention mechanism, so that the\nselected features can adaptively and efficiently interact with another\nmodality. To better capture the complementary information between the\nmodalities, we obtain the fused weight vector by splicing and use the weight\nvector to achieve feature reinforcement of the modalities. We apply TCAFN to\nthe RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal\nrepresentations to validate the effectiveness of the proposed fusion method.\nThe experimental results show that TACFN brings a significant performance\nimprovement compared to other methods and reaches the state-of-the-art. All\ncode and models could be accessed from https://github.com/shuzihuaiyu/TACFN."}
{"id": "2505.06856", "pdf": "https://arxiv.org/pdf/2505.06856", "abs": "https://arxiv.org/abs/2505.06856", "authors": ["Bonan Wang", "Haicheng Liao", "Chengyue Wang", "Bin Rao", "Yanchen Guan", "Guyang Yu", "Jiaxun Zhang", "Songning Lai", "Chengzhong Xu", "Zhenning Li"], "title": "Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Accurate trajectory prediction has long been a major challenge for autonomous\ndriving (AD). Traditional data-driven models predominantly rely on statistical\ncorrelations, often overlooking the causal relationships that govern traffic\nbehavior. In this paper, we introduce a novel trajectory prediction framework\nthat leverages causal inference to enhance predictive robustness,\ngeneralization, and accuracy. By decomposing the environment into spatial and\ntemporal components, our approach identifies and mitigates spurious\ncorrelations, uncovering genuine causal relationships. We also employ a\nprogressive fusion strategy to integrate multimodal information, simulating\nhuman-like reasoning processes and enabling real-time inference. Evaluations on\nfive real-world datasets--ApolloScape, nuScenes, NGSIM, HighD, and\nMoCAD--demonstrate our model's superiority over existing state-of-the-art\n(SOTA) methods, with improvements in key metrics such as RMSE and FDE. Our\nfindings highlight the potential of causal reasoning to transform trajectory\nprediction, paving the way for robust AD systems."}
{"id": "2505.06281", "pdf": "https://arxiv.org/pdf/2505.06281", "abs": "https://arxiv.org/abs/2505.06281", "authors": ["Chunduru Rohith Kumar", "PHD Surya Shanmuk", "Prabhala Naga Srinivas", "Sri Venkatesh Lankalapalli", "Debasis Dwibedy"], "title": "A Data-Driven Probabilistic Framework for Cascading Urban Risk Analysis Using Bayesian Networks", "categories": ["cs.LG", "stat.ML"], "comment": "14 pages, 4 figures, 8 tables", "summary": "The increasing complexity of cascading risks in urban systems necessitates\nrobust, data-driven frameworks to model interdependencies across multiple\ndomains. This study presents a foundational Bayesian network-based approach for\nanalyzing cross-domain risk propagation across key urban domains, including\nair, water, electricity, agriculture, health, infrastructure, weather, and\nclimate. Directed Acyclic Graphs (DAGs) are constructed using Bayesian Belief\nNetworks (BBNs), with structure learning guided by Hill-Climbing search\noptimized through Bayesian Information Criterion (BIC) and K2 scoring. The\nframework is trained on a hybrid dataset that combines real-world urban\nindicators with synthetically generated data from Generative Adversarial\nNetworks (GANs), and is further balanced using the Synthetic Minority\nOver-sampling Technique (SMOTE). Conditional Probability Tables (CPTs) derived\nfrom the learned structures enable interpretable probabilistic reasoning and\nquantify the likelihood of cascading failures. The results identify key intra-\nand inter-domain risk factors and demonstrate the framework's utility for\nproactive urban resilience planning. This work establishes a scalable,\ninterpretable foundation for cascading risk assessment and serves as a basis\nfor future empirical research in this emerging interdisciplinary field."}
{"id": "2505.06890", "pdf": "https://arxiv.org/pdf/2505.06890", "abs": "https://arxiv.org/abs/2505.06890", "authors": ["Kosuke Ukita", "Ye Xiaolong", "Tsuyoshi Okita"], "title": "Image Classification Using a Diffusion Model as a Pre-Training Model", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": "10 pages, 9 figures", "summary": "In this paper, we propose a diffusion model that integrates a\nrepresentation-conditioning mechanism, where the representations derived from a\nVision Transformer (ViT) are used to condition the internal process of a\nTransformer-based diffusion model. This approach enables\nrepresentation-conditioned data generation, addressing the challenge of\nrequiring large-scale labeled datasets by leveraging self-supervised learning\non unlabeled data. We evaluate our method through a zero-shot classification\ntask for hematoma detection in brain imaging. Compared to the strong\ncontrastive learning baseline, DINOv2, our method achieves a notable\nimprovement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its\neffectiveness in image classification."}
{"id": "2505.06782", "pdf": "https://arxiv.org/pdf/2505.06782", "abs": "https://arxiv.org/abs/2505.06782", "authors": ["Damian Curran", "Brian Chapman", "Mike Conway"], "title": "Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Australia and the UK have developed contrasting approaches to the regulation\nof electronic cigarettes, with - broadly speaking - Australia adopting a\nrelatively restrictive approach and the UK adopting a more permissive approach.\nNotably, these divergent policies were developed from the same broad evidence\nbase. In this paper, to investigate differences in how the two jurisdictions\nmanage and present evidence, we developed and evaluated a Large Language\nModel-based sentence classifier to perform automated analyses of electronic\ncigarette-related policy documents drawn from official Australian and UK\nlegislative processes (109 documents in total). Specifically, we utilized GPT-4\nto automatically classify sentences based on whether they contained claims that\ne-cigarettes were broadly helpful or harmful for public health. Our LLM-based\nclassifier achieved an F-score of 0.9. Further, when applying the classifier to\nour entire sentence-level corpus, we found that Australian legislative\ndocuments show a much higher proportion of harmful statements, and a lower\nproportion of helpful statements compared to the expected values, with the\nopposite holding for the UK. In conclusion, this work utilized an LLM-based\napproach to provide evidence to support the contention that - drawing on the\nsame evidence base - Australian ENDS-related policy documents emphasize the\nharms associated with ENDS products and UK policy documents emphasize the\nbenefits. Further, our approach provides a starting point for using LLM-based\nmethods to investigate the complex relationship between evidence and health\npolicy formation."}
{"id": "2505.06537", "pdf": "https://arxiv.org/pdf/2505.06537", "abs": "https://arxiv.org/abs/2505.06537", "authors": ["Xianghao Kong", "Qiaosong Qi", "Yuanbin Wang", "Anyi Rao", "Biaolong Chen", "Aixi Zhang", "Si Liu", "Hao Jiang"], "title": "ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fashion video generation aims to synthesize temporally consistent videos from\nreference images of a designated character. Despite significant progress,\nexisting diffusion-based methods only support a single reference image as\ninput, severely limiting their capability to generate view-consistent fashion\nvideos, especially when there are different patterns on the clothes from\ndifferent perspectives. Moreover, the widely adopted motion module does not\nsufficiently model human body movement, leading to sub-optimal spatiotemporal\nconsistency. To address these issues, we propose ProFashion, a fashion video\ngeneration framework leveraging multiple reference images to achieve improved\nview consistency and temporal coherency. To effectively leverage features from\nmultiple reference images while maintaining a reasonable computational cost, we\ndevise a Pose-aware Prototype Aggregator, which selects and aggregates global\nand fine-grained reference features according to pose information to form\nframe-wise prototypes, which serve as guidance in the denoising process. To\nfurther enhance motion consistency, we introduce a Flow-enhanced Prototype\nInstantiator, which exploits the human keypoint motion flow to guide an extra\nspatiotemporal attention process in the denoiser. To demonstrate the\neffectiveness of ProFashion, we extensively evaluate our method on the\nMRFashion-7K dataset we collected from the Internet. ProFashion also\noutperforms previous methods on the UBC Fashion dataset."}
{"id": "2505.06897", "pdf": "https://arxiv.org/pdf/2505.06897", "abs": "https://arxiv.org/abs/2505.06897", "authors": ["Jinhao Jiang", "Changlin Chen", "Shile Feng", "Wanru Geng", "Zesheng Zhou", "Ni Wang", "Shuai Li", "Feng-Qi Cui", "Erbao Dong"], "title": "Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence", "categories": ["cs.AI"], "comment": "19pages,7 figures,3 tables", "summary": "The ultimate goal of artificial intelligence (AI) is to achieve Artificial\nGeneral Intelligence (AGI). Embodied Artificial Intelligence (EAI), which\ninvolves intelligent systems with physical presence and real-time interaction\nwith the environment, has emerged as a key research direction in pursuit of\nAGI. While advancements in deep learning, reinforcement learning, large-scale\nlanguage models, and multimodal technologies have significantly contributed to\nthe progress of EAI, most existing reviews focus on specific technologies or\napplications. A systematic overview, particularly one that explores the direct\nconnection between EAI and AGI, remains scarce. This paper examines EAI as a\nfoundational approach to AGI, systematically analyzing its four core modules:\nperception, intelligent decision-making, action, and feedback. We provide a\ndetailed discussion of how each module contributes to the six core principles\nof AGI. Additionally, we discuss future trends, challenges, and research\ndirections in EAI, emphasizing its potential as a cornerstone for AGI\ndevelopment. Our findings suggest that EAI's integration of dynamic learning\nand real-world interaction is essential for bridging the gap between narrow AI\nand AGI."}
{"id": "2505.06282", "pdf": "https://arxiv.org/pdf/2505.06282", "abs": "https://arxiv.org/abs/2505.06282", "authors": ["Zixu Wang", "Bingbing Xu", "Yige Yuan", "Huawei Shen", "Xueqi Cheng"], "title": "InfoNCE is a Free Lunch for Semantically guided Graph Contrastive Learning", "categories": ["cs.LG"], "comment": "10 pages, 5 figures, Accepted by SIGIR2025", "summary": "As an important graph pre-training method, Graph Contrastive Learning (GCL)\ncontinues to play a crucial role in the ongoing surge of research on graph\nfoundation models or LLM as enhancer for graphs. Traditional GCL optimizes\nInfoNCE by using augmentations to define self-supervised tasks, treating\naugmented pairs as positive samples and others as negative. However, this leads\nto semantically similar pairs being classified as negative, causing significant\nsampling bias and limiting performance. In this paper, we argue that GCL is\nessentially a Positive-Unlabeled (PU) learning problem, where the definition of\nself-supervised tasks should be semantically guided, i.e., augmented samples\nwith similar semantics are considered positive, while others, with unknown\nsemantics, are treated as unlabeled. From this perspective, the key lies in how\nto extract semantic information. To achieve this, we propose IFL-GCL, using\nInfoNCE as a \"free lunch\" to extract semantic information. Specifically, We\nfirst prove that under InfoNCE, the representation similarity of node pairs\naligns with the probability that the corresponding contrastive sample is\npositive. Then we redefine the maximum likelihood objective based on the\ncorrected samples, leading to a new InfoNCE loss function. Extensive\nexperiments on both the graph pretraining framework and LLM as an enhancer show\nsignificantly improvements of IFL-GCL in both IID and OOD scenarios, achieving\nup to a 9.05% improvement, validating the effectiveness of semantically guided.\nCode for IFL-GCL is publicly available at:\nhttps://github.com/Camel-Prince/IFL-GCL."}
{"id": "2505.06905", "pdf": "https://arxiv.org/pdf/2505.06905", "abs": "https://arxiv.org/abs/2505.06905", "authors": ["Jian Song", "Hongruixuan Chen", "Naoto Yokoya"], "title": "Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Monocular height estimation (MHE) from very-high-resolution (VHR) remote\nsensing imagery via deep learning is notoriously challenging due to the lack of\nsufficient structural information. Conventional digital elevation models\n(DEMs), typically derived from airborne LiDAR or multi-view stereo, remain\ncostly and geographically limited. Recently, models trained on synthetic data\nand refined through domain adaptation have shown remarkable performance in MHE,\nyet it remains unclear how these models make predictions or how reliable they\ntruly are. In this paper, we investigate a state-of-the-art MHE model trained\npurely on synthetic data to explore where the model looks when making height\npredictions. Through systematic analyses, we find that the model relies heavily\non shadow cues, a factor that can lead to overestimation or underestimation of\nheights when shadows deviate from expected norms. Furthermore, the inherent\ndifficulty of evaluating regression tasks with the human eye underscores\nadditional limitations of purely synthetic training. To address these issues,\nwe propose a novel correction pipeline that integrates sparse, imperfect global\nLiDAR measurements (ICESat-2) with deep-learning outputs to improve local\naccuracy and achieve spatially consistent corrections. Our method comprises two\nstages: pre-processing raw ICESat-2 data, followed by a random forest-based\napproach to densely refine height estimates. Experiments in three\nrepresentative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal\nsubstantial error reductions, with mean absolute error (MAE) decreased by\n22.8\\%, 6.9\\%, and 4.9\\%, respectively. These findings highlight the critical\nrole of shadow awareness in synthetic data-driven models and demonstrate how\nfusing imperfect real-world LiDAR data can bolster the robustness of MHE,\npaving the way for more reliable and scalable 3D mapping solutions."}
{"id": "2505.06862", "pdf": "https://arxiv.org/pdf/2505.06862", "abs": "https://arxiv.org/abs/2505.06862", "authors": ["Lhuqita Fazry"], "title": "A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "$\\texttt{BIGBIRD-PEGASUS}$ model achieves $\\textit{state-of-the-art}$ on\nabstractive text summarization for long documents. However it's capacity still\nlimited to maximum of $4,096$ tokens, thus caused performance degradation on\nsummarization for very long documents. Common method to deal with the issue is\nto truncate the documents. In this reasearch, we'll use different approach.\nWe'll use the pretrained $\\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the\nmodel on other domain dataset. First, we filter out all documents which length\nless than $20,000$ tokens to focus on very long documents. To prevent domain\nshifting problem and overfitting on transfer learning due to small dataset, we\naugment the dataset by splitting document-summary training pair into parts, to\nfit the document into $4,096$ tokens. Source code available on\n$\\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$."}
{"id": "2505.06543", "pdf": "https://arxiv.org/pdf/2505.06543", "abs": "https://arxiv.org/abs/2505.06543", "authors": ["Shuhan Zhuang", "Mengqi Huang", "Fengyi Fu", "Nan Chen", "Bohan Lei", "Zhendong Mao"], "title": "HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Visual text rendering, which aims to accurately integrate specified textual\ncontent within generated images, is critical for various applications such as\ncommercial design. Despite recent advances, current methods struggle with\nlong-tail text cases, particularly when handling unseen or small-sized text. In\nthis work, we propose a novel Hierarchical Disentangled Glyph-Based framework\n(HDGlyph) that hierarchically decouples text generation from non-text visual\nsynthesis, enabling joint optimization of both common and long-tail text\nrendering. At the training stage, HDGlyph disentangles pixel-level\nrepresentations via the Multi-Linguistic GlyphNet and the Glyph-Aware\nPerceptual Loss, ensuring robust rendering even for unseen characters. At\ninference time, HDGlyph applies Noise-Disentangled Classifier-Free Guidance and\nLatent-Disentangled Two-Stage Rendering (LD-TSR) scheme, which refines both\nbackground and small-sized text. Extensive evaluations show our model\nconsistently outperforms others, with 5.08% and 11.7% accuracy gains in English\nand Chinese text rendering while maintaining high image quality. It also excels\nin long-tail scenarios with strong accuracy and visual performance."}
{"id": "2505.06907", "pdf": "https://arxiv.org/pdf/2505.06907", "abs": "https://arxiv.org/abs/2505.06907", "authors": ["Yu Qiao", "Huy Q. Le", "Avi Deb Raha", "Phuong-Nam Tran", "Apurba Adhikary", "Mengchun Zhang", "Loc X. Nguyen", "Eui-Nam Huh", "Dusit Niyato", "Choong Seon Hong"], "title": "Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence", "categories": ["cs.AI", "cs.CV", "cs.NE"], "comment": "On going work", "summary": "The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and\nGrok-3, has reshaped the artificial intelligence landscape. As prominent\nexamples of foundational models (FMs) built on LLMs, these models exhibit\nremarkable capabilities in generating human-like content, bringing us closer to\nachieving artificial general intelligence (AGI). However, their large-scale\nnature, sensitivity to privacy concerns, and substantial computational demands\npresent significant challenges to personalized customization for end users. To\nbridge this gap, this paper presents the vision of artificial personalized\nintelligence (API), focusing on adapting these powerful models to meet the\nspecific needs and preferences of users while maintaining privacy and\nefficiency. Specifically, this paper proposes personalized federated\nintelligence (PFI), which integrates the privacy-preserving advantages of\nfederated learning (FL) with the zero-shot generalization capabilities of FMs,\nenabling personalized, efficient, and privacy-protective deployment at the\nedge. We first review recent advances in both FL and FMs, and discuss the\npotential of leveraging FMs to enhance federated systems. We then present the\nkey motivations behind realizing PFI and explore promising opportunities in\nthis space, including efficient PFI, trustworthy PFI, and PFI empowered by\nretrieval-augmented generation (RAG). Finally, we outline key challenges and\nfuture research directions for deploying FM-powered FL systems at the edge with\nimproved personalization, computational efficiency, and privacy guarantees.\nOverall, this survey aims to lay the groundwork for the development of API as a\ncomplement to AGI, with a particular focus on PFI as a key enabling technique."}
{"id": "2505.06283", "pdf": "https://arxiv.org/pdf/2505.06283", "abs": "https://arxiv.org/abs/2505.06283", "authors": ["Limin Li", "Kuo Yang", "Wenjie Du", "Pengkun Wang", "Zhengyang Zhou", "Yang Wang"], "title": "Soft causal learning for generalized molecule property prediction: An environment perspective", "categories": ["cs.LG", "q-bio.QM", "stat.ML", "I.2.4"], "comment": "23 pages, 7 figures, 3 tables", "summary": "Learning on molecule graphs has become an increasingly important topic in AI\nfor science, which takes full advantage of AI to facilitate scientific\ndiscovery. Existing solutions on modeling molecules utilize Graph Neural\nNetworks (GNNs) to achieve representations but they mostly fail to adapt models\nto out-of-distribution (OOD) samples. Although recent advances on OOD-oriented\ngraph learning have discovered the invariant rationale on graphs, they still\nignore three important issues, i.e., 1) the expanding atom patterns regarding\nenvironments on graphs lead to failures of invariant rationale based models, 2)\nthe associations between discovered molecular subgraphs and corresponding\nproperties are complex where causal substructures cannot fully interpret the\nlabels. 3) the interactions between environments and invariances can influence\nwith each other thus are challenging to be modeled. To this end, we propose a\nsoft causal learning framework, to tackle the unresolved OOD challenge in\nmolecular science, from the perspective of fully modeling the molecule\nenvironments and bypassing the invariant subgraphs. Specifically, we first\nincorporate chemistry theories into our graph growth generator to imitate\nexpaned environments, and then devise an GIB-based objective to disentangle\nenvironment from whole graphs and finally introduce a cross-attention based\nsoft causal interaction, which allows dynamic interactions between environments\nand invariances. We perform experiments on seven datasets by imitating\ndifferent kinds of OOD generalization scenarios. Extensive comparison, ablation\nexperiments as well as visualized case studies demonstrate well generalization\nability of our proposal."}
{"id": "2505.07219", "pdf": "https://arxiv.org/pdf/2505.07219", "abs": "https://arxiv.org/abs/2505.07219", "authors": ["Hongda Qin", "Xiao Lu", "Zhiyong Wei", "Yihong Cao", "Kailun Yang", "Ningjiang Chen"], "title": "Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "The source code and pre-trained models will be publicly available at\n  https://github.com/qinhongda8/LDDS", "summary": "Generalizing an object detector trained on a single domain to multiple unseen\ndomains is a challenging task. Existing methods typically introduce image or\nfeature augmentation to diversify the source domain to raise the robustness of\nthe detector. Vision-Language Model (VLM)-based augmentation techniques have\nbeen proven to be effective, but they require that the detector's backbone has\nthe same structure as the image encoder of VLM, limiting the detector framework\nselection. To address this problem, we propose Language-Driven Dual Style\nMixing (LDDS) for single-domain generalization, which diversifies the source\ndomain by fully utilizing the semantic information of the VLM. Specifically, we\nfirst construct prompts to transfer style semantics embedded in the VLM to an\nimage translation network. This facilitates the generation of style diversified\nimages with explicit semantic information. Then, we propose image-level style\nmixing between the diversified images and source domain images. This\neffectively mines the semantic information for image augmentation without\nrelying on specific augmentation selections. Finally, we propose feature-level\nstyle mixing in a double-pipeline manner, allowing feature augmentation to be\nmodel-agnostic and can work seamlessly with the mainstream detector frameworks,\nincluding the one-stage, two-stage, and transformer-based detectors. Extensive\nexperiments demonstrate the effectiveness of our approach across various\nbenchmark datasets, including real to cartoon and normal to adverse weather\ntasks. The source code and pre-trained models will be publicly available at\nhttps://github.com/qinhongda8/LDDS."}
{"id": "2505.06889", "pdf": "https://arxiv.org/pdf/2505.06889", "abs": "https://arxiv.org/abs/2505.06889", "authors": ["Mihyeon Kim", "Juhyoung Park", "Youngbin Kim"], "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2024 Main", "summary": "Pre-trained Language Models (PLMs) have achieved remarkable performance on\ndiverse NLP tasks through pre-training and fine-tuning. However, fine-tuning\nthe model with a large number of parameters on limited downstream datasets\noften leads to vulnerability to adversarial attacks, causing overfitting of the\nmodel on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic\nsystem by conceptualizing a layer of BERT as a solution of Ordinary\nDifferential Equations (ODEs). Under the situation of initial value\nperturbation, we analyze the numerical stability of two main numerical ODE\nsolvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection\nincorporating BERT's layers. This strategy enhances the robustness of PLMs\nagainst adversarial attacks, even in low-resource scenarios, without\nintroducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the\nrobustness of IM-BERT under various conditions. Compared to the original BERT,\nIM-BERT exhibits a performance improvement of approximately 8.3\\%p on the\nAdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms\nBERT by achieving 5.9\\%p higher accuracy."}
{"id": "2505.06557", "pdf": "https://arxiv.org/pdf/2505.06557", "abs": "https://arxiv.org/abs/2505.06557", "authors": ["Lu Dong", "Haiyu Zhang", "Hongjie Zhang", "Yifei Huang", "Zhen-Hua Ling", "Yu Qiao", "Limin Wang", "Yali Wang"], "title": "Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining", "categories": ["cs.CV"], "comment": "TCSVT 2025, doi at https://ieeexplore.ieee.org/document/10970001", "summary": "The task of weakly supervised temporal sentence grounding (WSTSG) aims to\ndetect temporal intervals corresponding to a language description from\nuntrimmed videos with only video-level video-language correspondence. For an\nanchor sample, most existing approaches generate negative samples either from\nother videos or within the same video for contrastive learning. However, some\ntraining samples are highly similar to the anchor sample, directly regarding\nthem as negative samples leads to difficulties for optimization and ignores the\ncorrelations between these similar samples and the anchor sample. To address\nthis, we propose Positive Sample Mining (PSM), a novel framework that mines\npositive samples from the training set to provide more discriminative\nsupervision. Specifically, for a given anchor sample, we partition the\nremaining training set into semantically similar and dissimilar subsets based\non the similarity of their text queries. To effectively leverage these\ncorrelations, we introduce a PSM-guided contrastive loss to ensure that the\nanchor proposal is closer to similar samples and further from dissimilar ones.\nAdditionally, we design a PSM-guided rank loss to ensure that similar samples\nare closer to the anchor proposal than to the negative intra-video proposal,\naiming to distinguish the anchor proposal and the negative intra-video\nproposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the\neffectiveness and superiority of our method."}
{"id": "2505.06949", "pdf": "https://arxiv.org/pdf/2505.06949", "abs": "https://arxiv.org/abs/2505.06949", "authors": ["Sumyyah Toonsi", "Paul Schofield", "Robert Hoehndorf"], "title": "Causal knowledge graph analysis identifies adverse drug effects", "categories": ["cs.AI", "q-bio.BM"], "comment": null, "summary": "Knowledge graphs and structural causal models have each proven valuable for\norganizing biomedical knowledge and estimating causal effects, but remain\nlargely disconnected: knowledge graphs encode qualitative relationships\nfocusing on facts and deductive reasoning without formal probabilistic\nsemantics, while causal models lack integration with background knowledge in\nknowledge graphs and have no access to the deductive reasoning capabilities\nthat knowledge graphs provide. To bridge this gap, we introduce a novel\nformulation of Causal Knowledge Graphs (CKGs) which extend knowledge graphs\nwith formal causal semantics, preserving their deductive capabilities while\nenabling principled causal inference. CKGs support deconfounding via explicitly\nmarked causal edges and facilitate hypothesis formulation aligned with both\nencoded and entailed background knowledge. We constructed a Drug-Disease CKG\n(DD-CKG) integrating disease progression pathways, drug indications,\nside-effects, and hierarchical disease classification to enable automated\nlarge-scale mediation analysis. Applied to UK Biobank and MIMIC-IV cohorts, we\ntested whether drugs mediate effects between indications and downstream disease\nprogression, adjusting for confounders inferred from the DD-CKG. Our approach\nsuccessfully reproduced known adverse drug reactions with high precision while\nidentifying previously undocumented significant candidate adverse effects.\nFurther validation through side effect similarity analysis demonstrated that\ncombining our predicted drug effects with established databases significantly\nimproves the prediction of shared drug indications, supporting the clinical\nrelevance of our novel findings. These results demonstrate that our methodology\nprovides a generalizable, knowledge-driven framework for scalable causal\ninference."}
{"id": "2505.06284", "pdf": "https://arxiv.org/pdf/2505.06284", "abs": "https://arxiv.org/abs/2505.06284", "authors": ["Zhiqiang Wang", "Ruoxi Cheng"], "title": "DMRL: Data- and Model-aware Reward Learning for Data Extraction", "categories": ["cs.LG", "cs.CR"], "comment": "Data- and Model-aware Reward Learning for Data Extraction. arXiv\n  admin note: substantial text overlap with arXiv:2503.18991", "summary": "Large language models (LLMs) are inherently vulnerable to unintended privacy\nbreaches. Consequently, systematic red-teaming research is essential for\ndeveloping robust defense mechanisms. However, current data extraction methods\nsuffer from several limitations: (1) rely on dataset duplicates (addressable\nvia deduplication), (2) depend on prompt engineering (now countered by\ndetection and defense), and (3) rely on random-search adversarial generation.\nTo address these challenges, we propose DMRL, a Data- and Model-aware Reward\nLearning approach for data extraction. This technique leverages inverse\nreinforcement learning to extract sensitive data from LLMs. Our method consists\nof two main components: (1) constructing an introspective reasoning dataset\nthat captures leakage mindsets to guide model behavior, and (2) training reward\nmodels with Group Relative Policy Optimization (GRPO), dynamically tuning\noptimization based on task difficulty at both the data and model levels.\nComprehensive experiments across various LLMs demonstrate that DMRL outperforms\nall baseline methods in data extraction performance."}
{"id": "2505.07380", "pdf": "https://arxiv.org/pdf/2505.07380", "abs": "https://arxiv.org/abs/2505.07380", "authors": ["David Vázquez-Padín", "Fernando Pérez-González", "Pablo Pérez-Miguélez"], "title": "Apple's Synthetic Defocus Noise Pattern: Characterization and Forensic Applications", "categories": ["cs.CV", "cs.CR", "eess.IV"], "comment": "This paper was submitted to IEEE Transactions on Information\n  Forensics & Security on May, 2025", "summary": "iPhone portrait-mode images contain a distinctive pattern in out-of-focus\nregions simulating the bokeh effect, which we term Apple's Synthetic Defocus\nNoise Pattern (SDNP). If overlooked, this pattern can interfere with blind\nforensic analyses, especially PRNU-based camera source verification, as noted\nin earlier works. Since Apple's SDNP remains underexplored, we provide a\ndetailed characterization, proposing a method for its precise estimation,\nmodeling its dependence on scene brightness, ISO settings, and other factors.\nLeveraging this characterization, we explore forensic applications of the SDNP,\nincluding traceability of portrait-mode images across iPhone models and iOS\nversions in open-set scenarios, assessing its robustness under post-processing.\nFurthermore, we show that masking SDNP-affected regions in PRNU-based camera\nsource verification significantly reduces false positives, overcoming a\ncritical limitation in camera attribution, and improving state-of-the-art\ntechniques."}
{"id": "2505.06904", "pdf": "https://arxiv.org/pdf/2505.06904", "abs": "https://arxiv.org/abs/2505.06904", "authors": ["Xinyi Mou", "Chen Qian", "Wei Liu", "Xuanjing Huang", "Zhongyu Wei"], "title": "EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have demonstrated an impressive ability to\nrole-play humans and replicate complex social dynamics. While large-scale\nsocial simulations are gaining increasing attention, they still face\nsignificant challenges, particularly regarding high time and computation costs.\nExisting solutions, such as distributed mechanisms or hybrid agent-based model\n(ABM) integrations, either fail to address inference costs or compromise\naccuracy and generalizability. To this end, we propose EcoLANG: Efficient and\nEffective Agent Communication Language Induction for Social Simulation. EcoLANG\noperates in two stages: (1) language evolution, where we filter synonymous\nwords and optimize sentence-level rules through natural selection, and (2)\nlanguage utilization, where agents in social simulations communicate using the\nevolved language. Experimental results demonstrate that EcoLANG reduces token\nconsumption by over 20%, enhancing efficiency without sacrificing simulation\naccuracy."}
{"id": "2505.06566", "pdf": "https://arxiv.org/pdf/2505.06566", "abs": "https://arxiv.org/abs/2505.06566", "authors": ["Zequn Xie", "Haoming Ji", "Lingwei Meng"], "title": "Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image person search aims to identify an individual based on a text\ndescription. To reduce data collection costs, large-scale text-image datasets\nare created from co-occurrence pairs found online. However, this can introduce\nnoise, particularly mismatched pairs, which degrade retrieval performance.\nExisting methods often focus on negative samples, amplifying this noise. To\naddress these issues, we propose the Dynamic Uncertainty and Relational\nAlignment (DURA) framework, which includes the Key Feature Selector (KFS) and a\nnew loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and\nmodels noise uncertainty, improving retrieval reliability. The bidirectional\nevidence from cross-modal similarity is modeled as a Dirichlet distribution,\nenhancing adaptability to noisy data. DSH adjusts the difficulty of negative\nsamples to improve robustness in noisy environments. Our experiments on three\ndatasets show that the method offers strong noise resistance and improves\nretrieval performance in both low- and high-noise scenarios."}
{"id": "2505.06964", "pdf": "https://arxiv.org/pdf/2505.06964", "abs": "https://arxiv.org/abs/2505.06964", "authors": ["Gaurab Sarkar", "Sougata Saha"], "title": "From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research in Chemical and Biological Engineering", "categories": ["cs.AI"], "comment": null, "summary": "Although Large Language Models (LLMs) have achieved remarkable performance in\ndiverse general knowledge and reasoning tasks, their utility in the scientific\ndomain of Chemical and Biological Engineering (CBE) is unclear. Hence, it\nnecessitates challenging evaluation benchmarks that can measure LLM performance\nin knowledge- and reasoning-based tasks, which is lacking. As a foundational\nstep, we empirically measure the reasoning capabilities of LLMs in CBE. We\nconstruct and share an expert-curated dataset of 5,920 examples for\nbenchmarking LLMs' reasoning capabilities in the niche domain of Ionic Liquids\n(ILs) for carbon sequestration, an emergent solution to reducing global\nwarming. The dataset presents different difficulty levels by varying along the\ndimensions of linguistic and domain-specific knowledge. Benchmarking three less\nthan 10B parameter open-source LLMs on the dataset suggests that while smaller\ngeneral-purpose LLMs are knowledgeable about ILs, they lack domain-specific\nreasoning capabilities. Based on our results, we further discuss considerations\nfor leveraging LLMs for carbon capture research using ILs. Since LLMs have a\nhigh carbon footprint, gearing them for IL research can symbiotically benefit\nboth fields and help reach the ambitious carbon neutrality target by 2050.\nDataset link: https://github.com/sougata-ub/llms_for_ionic_liquids"}
{"id": "2505.06288", "pdf": "https://arxiv.org/pdf/2505.06288", "abs": "https://arxiv.org/abs/2505.06288", "authors": ["Zihao Chen", "Wenyong Wang", "Jiachen Yang", "Yu Xiang"], "title": "IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation", "categories": ["cs.LG", "stat.ML"], "comment": "16 pages, 14 figures", "summary": "Geometric representation learning in preserving the intrinsic geometric and\ntopological properties for discrete non-Euclidean data is crucial in scientific\napplications. Previous research generally mapped non-Euclidean discrete data\ninto Euclidean space during representation learning, which may lead to the loss\nof some critical geometric information. In this paper, we propose a novel\nIsometric Immersion Kernel Learning (IIKL) method to build Riemannian manifold\nand isometrically induce Riemannian metric from discrete non-Euclidean data. We\nprove that Isometric immersion is equivalent to the kernel function in the\ntangent bundle on the manifold, which explicitly guarantees the invariance of\nthe inner product between vectors in the arbitrary tangent space throughout the\nlearning process, thus maintaining the geometric structure of the original\ndata. Moreover, a novel parameterized learning model based on IIKL is\nintroduced, and an alternating training method for this model is derived using\nMaximum Likelihood Estimation (MLE), ensuring efficient convergence.\nExperimental results proved that using the learned Riemannian manifold and its\nmetric, our model preserved the intrinsic geometric representation of data in\nboth 3D and high-dimensional datasets successfully, and significantly improved\nthe accuracy of downstream tasks, such as data reconstruction and\nclassification. It is showed that our method could reduce the inner product\ninvariant loss by more than 90% compared to state-of-the-art (SOTA) methods,\nalso achieved an average 40% improvement in downstream reconstruction accuracy\nand a 90% reduction in error for geometric metrics involving isometric and\nconformal."}
{"id": "2410.21000", "pdf": "https://arxiv.org/pdf/2410.21000", "abs": "https://arxiv.org/abs/2410.21000", "authors": ["Zhilin Zhang", "Jie Wang", "Zhanghao Qin", "Ruiqi Zhu", "Xiaoliang Gong"], "title": "Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "To be published in 2025 International Joint Conference on Neural\n  Networks (IJCNN)", "summary": "Medical Visual Question Answering (MedVQA) has attracted growing interest at\nthe intersection of medical image understanding and natural language processing\nfor clinical applications. By interpreting medical images and providing precise\nanswers to relevant clinical inquiries, MedVQA has the potential to support\ndiagnostic decision-making and reduce workload across various fields like\nradiology. While recent approaches rely heavily on unified large pre-trained\nVisual-Language Models, research on more efficient fusion mechanisms remains\nrelatively limited in this domain. In this paper, we introduce a fusion model,\nOMniBAN, that integrates Orthogonality loss, Multi-head attention, and a\nBilinear Attention Network to achieve high computational efficiency as well as\nsolid performance. We conduct comprehensive experiments and demonstrate how\nbilinear attention fusion can approximate the performance of larger fusion\nmodels like cross-modal Transformer. Our results show that OMniBAN requires\nfewer parameters (approximately 2/3 of Transformer-based Co-Attention) and\nsubstantially lower FLOPs (approximately 1/4), while achieving comparable\noverall performance and even slight improvements on closed-ended questions on\ntwo key MedVQA benchmarks. This balance between efficiency and accuracy\nsuggests that OMniBAN could be a viable option for real-world medical image\nquestion answering, where computational resources are often constrained."}
{"id": "2505.06914", "pdf": "https://arxiv.org/pdf/2505.06914", "abs": "https://arxiv.org/abs/2505.06914", "authors": ["Chen Amiraz", "Florin Cuconasu", "Simone Filice", "Zohar Karnin"], "title": "The Distracting Effect: Understanding Irrelevant Passages in RAG", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "A well-known issue with Retrieval Augmented Generation (RAG) is that\nretrieved passages that are irrelevant to the query sometimes distract the\nanswer-generating LLM, causing it to provide an incorrect response. In this\npaper, we shed light on this core issue and formulate the distracting effect of\na passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the\ndistracting effect of a passage and demonstrate its robustness across LLMs.\n  Our research introduces novel methods for identifying and using hard\ndistracting passages to improve RAG systems. By fine-tuning LLMs with these\ncarefully selected distracting passages, we achieve up to a 7.5% increase in\nanswering accuracy compared to counterparts fine-tuned on conventional RAG\ndatasets. Our contribution is two-fold: first, we move beyond the simple binary\nclassification of irrelevant passages as either completely unrelated vs.\ndistracting, and second, we develop and analyze multiple methods for finding\nhard distracting passages. To our knowledge, no other research has provided\nsuch a comprehensive framework for identifying and utilizing hard distracting\npassages."}
{"id": "2505.06573", "pdf": "https://arxiv.org/pdf/2505.06573", "abs": "https://arxiv.org/abs/2505.06573", "authors": ["Xingchen Li", "LiDian Wang", "Yu Sheng", "ZhiPeng Tang", "Haojie Ren", "Guoliang You", "YiFan Duan", "Jianmin Ji", "Yanyong Zhang"], "title": "ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors", "categories": ["cs.CV"], "comment": null, "summary": "Protecting power transmission lines from potential hazards involves critical\ntasks, one of which is the accurate measurement of distances between power\nlines and potential threats, such as large cranes. The challenge with this task\nis that the current sensor-based methods face challenges in balancing accuracy\nand cost in distance measurement. A common practice is to install cameras on\ntransmission towers, which, however, struggle to measure true 3D distances due\nto the lack of depth information. Although 3D lasers can provide accurate depth\ndata, their high cost makes large-scale deployment impractical.\n  To address this challenge, we present ElectricSight, a system designed for 3D\ndistance measurement and monitoring of potential hazards to power transmission\nlines. This work's key innovations lie in both the overall system framework and\na monocular depth estimation method. Specifically, the system framework\ncombines real-time images with environmental point cloud priors, enabling\ncost-effective and precise 3D distance measurements. As a core component of the\nsystem, the monocular depth estimation method enhances the performance by\nintegrating 3D point cloud data into image-based estimates, improving both the\naccuracy and reliability of the system.\n  To assess ElectricSight's performance, we conducted tests with data from a\nreal-world power transmission scenario. The experimental results demonstrate\nthat ElectricSight achieves an average accuracy of 1.08 m for distance\nmeasurements and an early warning accuracy of 92%."}
{"id": "2505.06977", "pdf": "https://arxiv.org/pdf/2505.06977", "abs": "https://arxiv.org/abs/2505.06977", "authors": ["Wenju Sun", "Qingyong Li", "Yangli-ao Geng", "Boyang Li"], "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Multi-task model merging offers a promising paradigm for integrating multiple\nexpert models into a unified model without additional training. Existing\nstate-of-the-art techniques, such as Task Arithmetic and its variants, merge\nmodels by accumulating task vectors -- the parameter differences between\npretrained and finetuned models. However, task vector accumulation is often\nhindered by knowledge conflicts, leading to performance degradation. To address\nthis challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel\ntraining-free framework that selectively trims conflict-prone components from\nthe task vectors. CAT Merging introduces several parameter-specific strategies,\nincluding projection for linear weights and masking for scaling and shifting\nparameters in normalization layers. Extensive experiments on vision, language,\nand vision-language tasks demonstrate that CAT Merging effectively suppresses\nknowledge conflicts, achieving average accuracy improvements of up to 2.5%\n(ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods."}
{"id": "2505.06289", "pdf": "https://arxiv.org/pdf/2505.06289", "abs": "https://arxiv.org/abs/2505.06289", "authors": ["Sotirios Athanasoulias"], "title": "Edge-Optimized Deep Learning & Pattern Recognition Techniques for Non-Intrusive Load Monitoring of Energy Time Series", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": "PhD dissertation as part of the GECKO Marie Curie", "summary": "The growing global energy demand and the urgent need for sustainability call\nfor innovative ways to boost energy efficiency. While advanced energy-saving\nsystems exist, they often fall short without user engagement. Providing\nfeedback on energy consumption behavior is key to promoting sustainable\npractices. Non-Intrusive Load Monitoring (NILM) offers a promising solution by\ndisaggregating total household energy usage, recorded by a central smart meter,\ninto appliance-level data. This empowers users to optimize consumption.\nAdvances in AI, IoT, and smart meter adoption have further enhanced NILM's\npotential.\n  Despite this promise, real-world NILM deployment faces major challenges.\nFirst, existing datasets mainly represent regions like the USA and UK, leaving\nplaces like the Mediterranean underrepresented. This limits understanding of\nregional consumption patterns, such as heavy use of air conditioners and\nelectric water heaters. Second, deep learning models used in NILM require high\ncomputational power, often relying on cloud services. This increases costs,\nraises privacy concerns, and limits scalability, especially for households with\npoor connectivity. This thesis tackles these issues with key contributions. It\npresents an interoperable data collection framework and introduces the Plegma\nDataset, focused on underrepresented Mediterranean energy patterns. It also\nexplores advanced deep neural networks and model compression techniques for\nefficient edge deployment. By bridging theoretical advances with practical\nneeds, this work aims to make NILM scalable, efficient, and adaptable for\nglobal energy sustainability."}
{"id": "2505.00228", "pdf": "https://arxiv.org/pdf/2505.00228", "abs": "https://arxiv.org/abs/2505.00228", "authors": ["Xiaoman Zhang", "Julián N. Acosta", "Josh Miller", "Ouwen Huang", "Pranav Rajpurkar"], "title": "ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present ReXGradient-160K, representing the largest publicly available\nchest X-ray dataset to date in terms of the number of patients. This dataset\ncontains 160,000 chest X-ray studies with paired radiological reports from\n109,487 unique patients across 3 U.S. health systems (79 medical sites). This\ncomprehensive dataset includes multiple images per study and detailed radiology\nreports, making it particularly valuable for the development and evaluation of\nAI systems for medical imaging and automated report generation models. The\ndataset is divided into training (140,000 studies), validation (10,000\nstudies), and public test (10,000 studies) sets, with an additional private\ntest set (10,000 studies) reserved for model evaluation on the ReXrank\nbenchmark. By providing this extensive dataset, we aim to accelerate research\nin medical imaging AI and advance the state-of-the-art in automated\nradiological analysis. Our dataset will be open-sourced at\nhttps://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K."}
{"id": "2505.06974", "pdf": "https://arxiv.org/pdf/2505.06974", "abs": "https://arxiv.org/abs/2505.06974", "authors": ["Daichi Kohmoto", "Katsutoshi Fukuda", "Daisuke Yoshida", "Takafumi Matsui", "Sachihiro Omura"], "title": "CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire", "categories": ["cs.CL"], "comment": "11 pages, 9 figures, 5 tables", "summary": "A cuneiform tablet KBo 23.1 ++/KUB 30.38, which is known to represent a text\nof Kizzuwatna rituals, was written by two writers with almost identical content\nin two iterations. Unlike other cuneiform tablets that contained information\nsuch as myths, essays, or business records, the reason why ancient people left\nsuch tablets for posterity remains unclear. To study this problem, we develop a\nnew methodology by analyzing images of a tablet quantitatively using CNN\n(Convolutional Neural Network)-based image models, without segmenting\ncuneiforms one-by-one. Our data-driven methodology implies that the writer\nwriting the first half was a `teacher' and the other writer was a `student' who\nwas training his skills of writing cuneiforms. This result has not been reached\nby classical linguistics. We also discuss related conclusions and possible\nfurther directions for applying our method and its generalizations."}
{"id": "2505.06575", "pdf": "https://arxiv.org/pdf/2505.06575", "abs": "https://arxiv.org/abs/2505.06575", "authors": ["Chengfeng Wang", "Wei Zhai", "Yuhang Yang", "Yang Cao", "Zhengjun Zha"], "title": "GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images", "categories": ["cs.CV"], "comment": null, "summary": "Estimating the geometry level of human-scene contact aims to ground specific\ncontact surface points at 3D human geometries, which provides a spatial prior\nand bridges the interaction between human and scene, supporting applications\nsuch as human behavior analysis, embodied AI, and AR/VR. To complete the task,\nexisting approaches predominantly rely on parametric human models (e.g., SMPL),\nwhich establish correspondences between images and contact regions through\nfixed SMPL vertex sequences. This actually completes the mapping from image\nfeatures to an ordered sequence. However, this approach lacks consideration of\ngeometry, limiting its generalizability in distinct human geometries. In this\npaper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact\nEstimation), a new paradigm for 3D human contact estimation. GRACE incorporates\na point cloud encoder-decoder architecture along with a hierarchical feature\nextraction and fusion module, enabling the effective integration of 3D human\ngeometric structures with 2D interaction semantics derived from images. Guided\nby visual cues, GRACE establishes an implicit mapping from geometric features\nto the vertex space of the 3D human mesh, thereby achieving accurate modeling\nof contact regions. This design ensures high prediction accuracy and endows the\nframework with strong generalization capability across diverse human\ngeometries. Extensive experiments on multiple benchmark datasets demonstrate\nthat GRACE achieves state-of-the-art performance in contact estimation, with\nadditional results further validating its robust generalization to unstructured\nhuman point clouds."}
{"id": "2505.06997", "pdf": "https://arxiv.org/pdf/2505.06997", "abs": "https://arxiv.org/abs/2505.06997", "authors": ["Wenhao Lu", "Zhengqiu Zhu", "Yong Zhao", "Yonglin Tian", "Junjie Zeng", "Jun Zhang", "Zhong Liu", "Fei-Yue Wang"], "title": "A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue", "categories": ["cs.AI"], "comment": null, "summary": "Mobile crowdsensing is evolving beyond traditional human-centric models by\nintegrating heterogeneous entities like unmanned aerial vehicles (UAVs) and\nunmanned ground vehicles (UGVs). Optimizing task allocation among these diverse\nagents is critical, particularly in challenging emergency rescue scenarios\ncharacterized by complex environments, limited communication, and partial\nobservability. This paper tackles the Heterogeneous-Entity\nCollaborative-Sensing Task Allocation (HECTA) problem specifically for\nemergency rescue, considering humans, UAVs, and UGVs. We introduce a novel\n``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs,\nalongside performing their sensing tasks. The primary objective is maximizing\nthe task completion rate (TCR) under strict time constraints. We rigorously\nformulate this NP-hard problem as a decentralized partially observable Markov\ndecision process (Dec-POMDP) to effectively handle sequential decision-making\nunder uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent\nreinforcement learning algorithm built upon a Centralized Training with\nDecentralized Execution architecture. HECTA4ER incorporates tailored designs,\nincluding specialized modules for complex feature extraction, utilization of\naction-observation history via hidden states, and a mixing network integrating\nglobal and local information, specifically addressing the challenges of partial\nobservability. Furthermore, theoretical analysis confirms the algorithm's\nconvergence properties. Extensive simulations demonstrate that HECTA4ER\nsignificantly outperforms baseline algorithms, achieving an average 18.42%\nincrease in TCR. Crucially, a real-world case study validates the algorithm's\neffectiveness and robustness in dynamic sensing scenarios, highlighting its\nstrong potential for practical application in emergency response."}
{"id": "2505.06290", "pdf": "https://arxiv.org/pdf/2505.06290", "abs": "https://arxiv.org/abs/2505.06290", "authors": ["Zefang Zong", "Xiaochen Wei", "Guozhen Zhang", "Chen Gao", "Huandong Wang", "Yong Li"], "title": "UniCO: Towards a Unified Model for Combinatorial Optimization Problems", "categories": ["cs.LG", "cs.DM"], "comment": null, "summary": "Combinatorial Optimization (CO) encompasses a wide range of problems that\narise in many real-world scenarios. While significant progress has been made in\ndeveloping learning-based methods for specialized CO problems, a unified model\nwith a single architecture and parameter set for diverse CO problems remains\nelusive. Such a model would offer substantial advantages in terms of efficiency\nand convenience. In this paper, we introduce UniCO, a unified model for solving\nvarious CO problems. Inspired by the success of next-token prediction, we frame\neach problem-solving process as a Markov Decision Process (MDP), tokenize the\ncorresponding sequential trajectory data, and train the model using a\ntransformer backbone. To reduce token length in the trajectory data, we propose\na CO-prefix design that aggregates static problem features. To address the\nheterogeneity of state and action tokens within the MDP, we employ a two-stage\nself-supervised learning approach. In this approach, a dynamic prediction model\nis first trained and then serves as a pre-trained model for subsequent policy\ngeneration. Experiments across 10 CO problems showcase the versatility of\nUniCO, emphasizing its ability to generalize to new, unseen problems with\nminimal fine-tuning, achieving even few-shot or zero-shot performance. Our\nframework offers a valuable complement to existing neural CO methods that focus\non optimizing performance for individual problems."}
{"id": "2505.03844", "pdf": "https://arxiv.org/pdf/2505.03844", "abs": "https://arxiv.org/abs/2505.03844", "authors": ["Solene Debuysere", "Nicolas Trouve", "Nathan Letheule", "Olivier Leveque", "Elise Colin"], "title": "From Spaceborne to Airborne: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has\nincreased considerably in recent years, with datasets commercially available.\nHowever, the acquisition of high-resolution SAR images in airborne\nconfigurations, remains costly and limited. Thus, the lack of open source,\nwell-labeled, or easily exploitable SAR text-image datasets is a barrier to the\nuse of existing foundation models in remote sensing applications. In this\ncontext, synthetic image generation is a promising solution to augment this\nscarce data, enabling a broader range of applications. Leveraging over 15 years\nof ONERA's extensive archival airborn data from acquisition campaigns, we\ncreated a comprehensive training dataset of 110 thousands SAR images to exploit\na 3.5 billion parameters pre-trained latent diffusion model\n\\cite{Baqu2019SethiR}. In this work, we present a novel approach utilizing\nspatial conditioning techniques within a foundation model to transform\nsatellite SAR imagery into airborne SAR representations. Additionally, we\ndemonstrate that our pipeline is effective for bridging the realism of\nsimulated images generated by ONERA's physics-based simulator EMPRISE\n\\cite{empriseem_ai_images}. Our method explores a key application of AI in\nadvancing SAR imaging technology. To the best of our knowledge, we are the\nfirst to introduce this approach in the literature."}
{"id": "2505.06987", "pdf": "https://arxiv.org/pdf/2505.06987", "abs": "https://arxiv.org/abs/2505.06987", "authors": ["Xiaoyu Wang", "Yue Zhao", "Qingqing Gu", "Zhonglin Jiang", "Xiaokai Chen", "Yong Chen", "Luo Ji"], "title": "Convert Language Model into a Value-based Strategic Planner", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures, Accepted by ACL 2025 Industry Track", "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines."}
{"id": "2505.06576", "pdf": "https://arxiv.org/pdf/2505.06576", "abs": "https://arxiv.org/abs/2505.06576", "authors": ["Haorui Chen", "Zeyu Ren", "Jiaxuan Ren", "Ran Ran", "Jinliang Shao", "Jie Huang", "Liangjian Deng"], "title": "Two-Stage Random Alternation Framework for Zero-Shot Pansharpening", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, pansharpening has seen rapid advancements with deep learning\nmethods, which have demonstrated impressive fusion quality. However, the\nchallenge of acquiring real high-resolution images limits the practical\napplicability of these methods. To address this, we propose a two-stage random\nalternating framework (TRA-PAN) that effectively integrates strong supervision\nconstraints from reduced-resolution images with the physical characteristics of\nfull-resolution images. The first stage introduces a pre-training procedure,\nwhich includes Degradation-Aware Modeling (DAM) to capture spatial-spectral\ndegradation mappings, alongside a warm-up procedure designed to reduce training\ntime and mitigate the negative effects of reduced-resolution data. In the\nsecond stage, Random Alternation Optimization (RAO) is employed, where random\nalternating training leverages the strengths of both reduced- and\nfull-resolution images, further optimizing the fusion model. By primarily\nrelying on full-resolution images, our method enables zero-shot training with\njust a single image pair, obviating the need for large datasets. Experimental\nresults demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in\nboth quantitative metrics and visual quality in real-world scenarios,\nhighlighting its strong practical applicability."}
{"id": "2505.07005", "pdf": "https://arxiv.org/pdf/2505.07005", "abs": "https://arxiv.org/abs/2505.07005", "authors": ["Bowen Long", "Enjie Liu", "Renxi Qiu", "Yanqing Duan"], "title": "Explainable AI the Latest Advancements and New Trends", "categories": ["cs.AI"], "comment": null, "summary": "In recent years, Artificial Intelligence technology has excelled in various\napplications across all domains and fields. However, the various algorithms in\nneural networks make it difficult to understand the reasons behind decisions.\nFor this reason, trustworthy AI techniques have started gaining popularity. The\nconcept of trustworthiness is cross-disciplinary; it must meet societal\nstandards and principles, and technology is used to fulfill these requirements.\nIn this paper, we first surveyed developments from various countries and\nregions on the ethical elements that make AI algorithms trustworthy; and then\nfocused our survey on the state of the art research into the interpretability\nof AI. We have conducted an intensive survey on technologies and techniques\nused in making AI explainable. Finally, we identified new trends in achieving\nexplainable AI. In particular, we elaborate on the strong link between the\nexplainability of AI and the meta-reasoning of autonomous systems. The concept\nof meta-reasoning is 'reason the reasoning', which coincides with the intention\nand goal of explainable Al. The integration of the approaches could pave the\nway for future interpretable AI systems."}
{"id": "2505.06292", "pdf": "https://arxiv.org/pdf/2505.06292", "abs": "https://arxiv.org/abs/2505.06292", "authors": ["Silke K. Kaiser", "Filipe Rodrigues", "Carlos Lima Azevedo", "Lynn H. Kaack"], "title": "Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Reliable street-level traffic volume data, covering multiple modes of\ntransportation, helps urban planning by informing decisions on infrastructure\nimprovements, traffic management, and public transportation. Yet, traffic\nsensors measuring traffic volume are typically scarcely located, due to their\nhigh deployment and maintenance costs. To address this, interpolation methods\ncan estimate traffic volumes at unobserved locations using available data.\nGraph Neural Networks have shown strong performance in traffic volume\nforecasting, particularly on highways and major arterial networks. Applying\nthem to urban settings, however, presents unique challenges: urban networks\nexhibit greater structural diversity, traffic volumes are highly overdispersed\nwith many zeros, the best way to account for spatial dependencies remains\nunclear, and sensor coverage is often very sparse. We introduce the Graph\nNeural Network for Urban Interpolation (GNNUI), a novel urban traffic volume\nestimation approach. GNNUI employs a masking algorithm to learn interpolation,\nintegrates node features to capture functional roles, and uses a loss function\ntailored to zero-inflated traffic distributions. In addition to the model, we\nintroduce two new open, large-scale urban traffic volume benchmarks, covering\ndifferent transportation modes: Strava cycling data from Berlin and New York\nCity taxi data. GNNUI outperforms recent, some graph-based, interpolation\nmethods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence)\nand remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAE\nrises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strong\nperformance under extreme data scarcity, common in real-world urban settings.\nWe also examine how graph connectivity choices influence model accuracy."}
{"id": "2404.17484", "pdf": "https://arxiv.org/pdf/2404.17484", "abs": "https://arxiv.org/abs/2404.17484", "authors": ["Zhenghong Li", "Jiaxiang Ren", "Wensheng Cheng", "Congwu Du", "Yingtian Pan", "Haibin Ling"], "title": "Sparse Reconstruction of Optical Doppler Tomography with Alternative State Space Model and Attention", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 3 figures", "summary": "Optical coherence Doppler tomography (ODT) is an emerging blood flow imaging\ntechnique. The fundamental unit of ODT is the 1D depth-resolved trace named raw\nA-scans (or A-line). A 2D ODT image (B-scan) is formed by reconstructing a\ncross-sectional flow image via Doppler phase-subtraction of raw A-scans along\nB-line. To obtain a high-fidelity B-scan, densely sampled A-scans are required\ncurrently, leading to prolonged scanning time and increased storage demands.\nAddressing this issue, we propose a novel sparse ODT reconstruction framework\nwith an Alternative State Space Attention Network (ASSAN) that effectively\nreduces raw A-scans needed. Inspired by the distinct distributions of\ninformation along A-line and B-line, ASSAN applies 1D State Space Model (SSM)\nto each A-line to learn the intra-A-scan representation, while using 1D gated\nself-attention along B-line to capture the inter-A-scan features. In addition,\nan effective feedforward network based on sequential 1D convolutions along\ndifferent axes is employed to enhance the local feature. In validation\nexperiments on real animal data, ASSAN shows clear effectiveness in the\nreconstruction in comparison with state-of-the-art reconstruction methods."}
{"id": "2505.07157", "pdf": "https://arxiv.org/pdf/2505.07157", "abs": "https://arxiv.org/abs/2505.07157", "authors": ["Hajar Sakai", "Sarah S. Lam"], "title": "HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Traditional topic models often struggle with contextual nuances and fail to\nadequately handle polysemy and rare words. This limitation typically results in\ntopics that lack coherence and quality. Large Language Models (LLMs) can\nmitigate this issue by generating an initial set of topics. However, these raw\ntopics frequently lack refinement and representativeness, which leads to\nredundancy without lexical similarity and reduced interpretability. This paper\nintroduces HAMLET, a graph-driven architecture for cross-lingual healthcare\ntopic modeling that uses LLMs. The proposed approach leverages neural-enhanced\nsemantic fusion to refine the embeddings of topics generated by the LLM.\nInstead of relying solely on statistical co-occurrence or human interpretation\nto extract topics from a document corpus, this method introduces a topic\nembedding refinement that uses Bidirectional Encoder Representations from\nTransformers (BERT) and Graph Neural Networks (GNN). After topic generation, a\nhybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for\nembedding. The topic representations are further refined using a GNN, which\nestablishes connections between documents, topics, words, similar topics, and\nsimilar words. A novel method is introduced to compute similarities.\nConsequently, the topic embeddings are refined, and the top k topics are\nextracted. Experiments were conducted using two healthcare datasets, one in\nEnglish and one in French, from which six sets were derived. The results\ndemonstrate the effectiveness of HAMLET."}
{"id": "2505.06578", "pdf": "https://arxiv.org/pdf/2505.06578", "abs": "https://arxiv.org/abs/2505.06578", "authors": ["Maxim Vashkevich", "Egor Krivalcevich"], "title": "Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform", "categories": ["cs.CV", "cs.LG", "68T07", "I.5.1"], "comment": "6 pages, 9 figures", "summary": "The paper presents a learned two-dimensional separable transform (LST) that\ncan be considered as a new type of computational layer for constructing neural\nnetwork (NN) architecture for image recognition tasks. The LST based on the\nidea of sharing the weights of one fullyconnected (FC) layer to process all\nrows of an image. After that, a second shared FC layer is used to process all\ncolumns of image representation obtained from the first layer. The use of LST\nlayers in a NN architecture significantly reduces the number of model\nparameters compared to models that use stacked FC layers. We show that a\nNN-classifier based on a single LST layer followed by an FC layer achieves\n98.02\\% accuracy on the MNIST dataset, while having only 9.5k parameters. We\nalso implemented a LST-based classifier for handwritten digit recognition on\nthe FPGA platform to demonstrate the efficiency of the suggested approach for\ndesigning a compact and high-performance implementation of NN models. Git\nrepository with supplementary materials: https://github.com/Mak-Sim/LST-2d"}
{"id": "2505.07027", "pdf": "https://arxiv.org/pdf/2505.07027", "abs": "https://arxiv.org/abs/2505.07027", "authors": ["Haorui Wang", "Jeff Guo", "Lingkai Kong", "Rampi Ramprasad", "Philippe Schwaller", "Yuanqi Du", "Chao Zhang"], "title": "LLM-Augmented Chemical Synthesis and Design Decision Programs", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE", "physics.chem-ph"], "comment": null, "summary": "Retrosynthesis, the process of breaking down a target molecule into simpler\nprecursors through a series of valid reactions, stands at the core of organic\nchemistry and drug development. Although recent machine learning (ML) research\nhas advanced single-step retrosynthetic modeling and subsequent route searches,\nthese solutions remain restricted by the extensive combinatorial space of\npossible pathways. Concurrently, large language models (LLMs) have exhibited\nremarkable chemical knowledge, hinting at their potential to tackle complex\ndecision-making tasks in chemistry. In this work, we explore whether LLMs can\nsuccessfully navigate the highly constrained, multi-step retrosynthesis\nplanning problem. We introduce an efficient scheme for encoding reaction\npathways and present a new route-level search strategy, moving beyond the\nconventional step-by-step reactant prediction. Through comprehensive\nevaluations, we show that our LLM-augmented approach excels at retrosynthesis\nplanning and extends naturally to the broader challenge of synthesizable\nmolecular design."}
{"id": "2505.06295", "pdf": "https://arxiv.org/pdf/2505.06295", "abs": "https://arxiv.org/abs/2505.06295", "authors": ["Bhuvan Saravanan", "Pasanth Kumar M D", "Aarnesh Vengateson"], "title": "Benchmarking Traditional Machine Learning and Deep Learning Models for Fault Detection in Power Transformers", "categories": ["cs.LG"], "comment": null, "summary": "Accurate diagnosis of power transformer faults is essential for ensuring the\nstability and safety of electrical power systems. This study presents a\ncomparative analysis of conventional machine learning (ML) algorithms and deep\nlearning (DL) algorithms for fault classification of power transformers. Using\na condition-monitored dataset spanning 10 months, various gas concentration\nfeatures were normalized and used to train five ML classifiers: Support Vector\nMachine (SVM), k-Nearest Neighbors (KNN), Random Forest (RF), XGBoost, and\nArtificial Neural Network (ANN). In addition, four DL models were evaluated:\nLong Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), One-Dimensional\nConvolutional Neural Network (1D-CNN), and TabNet. Experimental results show\nthat both ML and DL approaches performed comparably. The RF model achieved the\nhighest ML accuracy at 86.82%, while the 1D-CNN model attained a close 86.30%."}
{"id": "2405.06198", "pdf": "https://arxiv.org/pdf/2405.06198", "abs": "https://arxiv.org/abs/2405.06198", "authors": ["Junzhuo Chen", "Shitong Kang"], "title": "MAPL: Memory Augmentation and Pseudo-Labeling for Semi-Supervised Anomaly Detection", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Large unlabeled data and difficult-to-identify anomalies are the urgent\nissues need to overcome in most industrial scene. In order to address this\nissue, a new meth-odology for detecting surface defects in in-dustrial settings\nis introduced, referred to as Memory Augmentation and Pseudo-Labeling(MAPL).\nThe methodology first in-troduces an anomaly simulation strategy, which\nsignificantly improves the model's ability to recognize rare or unknown\nanom-aly types by generating simulated anomaly samples. To cope with the\nproblem of the lack of labeling of anomalous simulated samples, a\npseudo-labeler method based on a one-classifier ensemble was employed in this\nstudy, which enhances the robustness of the model in the case of limited\nlabeling data by automatically selecting key pseudo-labeling hyperparameters.\nMeanwhile, a memory-enhanced learning mechanism is introduced to effectively\npredict abnormal regions by analyzing the difference be-tween the input samples\nand the normal samples in the memory pool. An end-to-end learning framework is\nemployed by MAPL to identify the abnormal regions directly from the input data,\nwhich optimizes the ef-ficiency and real-time performance of de-tection. By\nconducting extensive trials on the recently developed BHAD dataset (in-cluding\nMVTec AD [1], Visa [2], and MDPP [3]), MAPL achieves an average im-age-level\nAUROC score of 86.2%, demon-strating a 5.1% enhancement compared to the\noriginal MemSeg [4] model. The source code is available at\nhttps://github.com/jzc777/MAPL."}
{"id": "2505.07161", "pdf": "https://arxiv.org/pdf/2505.07161", "abs": "https://arxiv.org/abs/2505.07161", "authors": ["Jannatun Naim", "Jie Cao", "Fareen Tasneem", "Jennifer Jacobs", "Brent Milne", "James Martin", "Tamara Sumner"], "title": "Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to EDM'2025", "summary": "Effective feedback is essential for refining instructional practices in\nmathematics education, and researchers often turn to advanced natural language\nprocessing (NLP) models to analyze classroom dialogues from multiple\nperspectives. However, utterance-level discourse analysis encounters two\nprimary challenges: (1) multifunctionality, where a single utterance may serve\nmultiple purposes that a single tag cannot capture, and (2) the exclusion of\nmany utterances from domain-specific discourse move classifications, leading to\ntheir omission in feedback. To address these challenges, we proposed a\nmulti-perspective discourse analysis that integrates domain-specific talk moves\nwith dialogue act (using the flattened multi-functional SWBD-MASL schema with\n43 tags) and discourse relation (applying Segmented Discourse Representation\nTheory with 16 relations). Our top-down analysis framework enables a\ncomprehensive understanding of utterances that contain talk moves, as well as\nutterances that do not contain talk moves. This is applied to two mathematics\neducation datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through\ndistributional unigram analysis, sequential talk move analysis, and multi-view\ndeep dive, we discovered meaningful discourse patterns, and revealed the vital\nrole of utterances without talk moves, demonstrating that these utterances, far\nfrom being mere fillers, serve crucial functions in guiding, acknowledging, and\nstructuring classroom discourse. These insights underscore the importance of\nincorporating discourse relations and dialogue acts into AI-assisted education\nsystems to enhance feedback and create more responsive learning environments.\nOur framework may prove helpful for providing human educator feedback, but also\naiding in the development of AI agents that can effectively emulate the roles\nof both educators and students."}
{"id": "2505.06592", "pdf": "https://arxiv.org/pdf/2505.06592", "abs": "https://arxiv.org/abs/2505.06592", "authors": ["H M Dipu Kabir", "Subrota Kumar Mondal", "Mohammad Ali Moni"], "title": "Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes batch augmentation with unimodal fine-tuning to detect\nthe fetus's organs from ultrasound images and associated clinical textual\ninformation. We also prescribe pre-training initial layers with investigated\nmedical data before the multimodal training. At first, we apply a transferred\ninitialization with the unimodal image portion of the dataset with batch\naugmentation. This step adjusts the initial layer weights for medical data.\nThen, we apply neural networks (NNs) with fine-tuned initial layers to images\nin batches with batch augmentation to obtain features. We also extract\ninformation from descriptions of images. We combine this information with\nfeatures obtained from images to train the head layer. We write a dataloader\nscript to load the multimodal data and use existing unimodal image augmentation\ntechniques with batch augmentation for the multimodal data. The dataloader\nbrings a new random augmentation for each batch to get a good generalization.\nWe investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The\nmultimodal large language model (LLM) with the proposed training provides the\nbest results among the investigated methods. We receive near state-of-the-art\n(SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the\nproposed method with traditional counterparts at the following repository:\ngithub.com/dipuk0506/multimodal"}
{"id": "2505.07030", "pdf": "https://arxiv.org/pdf/2505.07030", "abs": "https://arxiv.org/abs/2505.07030", "authors": ["Mahmood Mohassel Feghhi", "Raya Majid Alsharfa", "Majid Hameed Majeed"], "title": "Efficient Fault Detection in WSN Based on PCA-Optimized Deep Neural Network Slicing Trained with GOA", "categories": ["cs.AI", "cs.LG", "eess.SP"], "comment": "22 pages, 18 figures, Accepted for publication in International\n  Journal of Intelligent Engineering and Systems, May 2025", "summary": "Fault detection in Wireless Sensor Networks (WSNs) is crucial for reliable\ndata transmission and network longevity. Traditional fault detection methods\noften struggle with optimizing deep neural networks (DNNs) for efficient\nperformance, especially in handling high-dimensional data and capturing\nnonlinear relationships. Additionally, these methods typically suffer from slow\nconvergence and difficulty in finding optimal network architectures using\ngradient-based optimization. This study proposes a novel hybrid method\ncombining Principal Component Analysis (PCA) with a DNN optimized by the\nGrasshopper Optimization Algorithm (GOA) to address these limitations. Our\napproach begins by computing eigenvalues from the original 12-dimensional\ndataset and sorting them in descending order. The cumulative sum of these\nvalues is calculated, retaining principal components until 99.5% variance is\nachieved, effectively reducing dimensionality to 4 features while preserving\ncritical information. This compressed representation trains a six-layer DNN\nwhere GOA optimizes the network architecture, overcoming backpropagation's\nlimitations in discovering nonlinear relationships. This hybrid PCA-GOA-DNN\nframework compresses the data and trains a six-layer DNN that is optimized by\nGOA, enhancing both training efficiency and fault detection accuracy. The\ndataset used in this study is a real-world WSNs dataset developed by the\nUniversity of North Carolina, which was used to evaluate the proposed method's\nperformance. Extensive simulations demonstrate that our approach achieves a\nremarkable 99.72% classification accuracy, with exceptional precision and\nrecall, outperforming conventional methods. The method is computationally\nefficient, making it suitable for large-scale WSN deployments, and represents a\nsignificant advancement in fault detection for resource-constrained WSNs."}
{"id": "2505.06297", "pdf": "https://arxiv.org/pdf/2505.06297", "abs": "https://arxiv.org/abs/2505.06297", "authors": ["Yu Mao", "Holger Pirk", "Chun Jason Xue"], "title": "Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to be deployed and utilized across\ndomains, the volume of LLM-generated data is growing rapidly. This trend\nhighlights the increasing importance of effective and lossless compression for\nsuch data in modern text management systems. However, compressing LLM-generated\ndata presents unique challenges compared to traditional human- or\nmachine-generated content. Traditional machine-generated data is typically\nderived from computational processes or device outputs, often highly structured\nand limited to low-level elements like labels or numerical values. This\nstructure enables conventional lossless compressors to perform efficiently. In\ncontrast, LLM-generated data is more complex and diverse, requiring new\napproaches for effective compression. In this work, we conduct the first\nsystematic investigation of lossless compression techniques tailored\nspecifically to LLM-generated data. Notably, because LLMs are trained via\nnext-token prediction, we find that LLM-generated data is highly predictable\nfor the models themselves. This predictability enables LLMs to serve as\nefficient compressors of their own outputs. Through extensive experiments with\n14 representative LLMs and 8 LLM-generated datasets from diverse domains, we\nshow that LLM-based prediction methods achieve remarkable compression rates,\nexceeding 20x, far surpassing the 3x rate achieved by Gzip, a widely used\ngeneral-purpose compressor. Furthermore, this advantage holds across different\nLLM sizes and dataset types, demonstrating the robustness and practicality of\nLLM-based methods in lossless text compression under generative AI workloads."}
{"id": "2405.17456", "pdf": "https://arxiv.org/pdf/2405.17456", "abs": "https://arxiv.org/abs/2405.17456", "authors": ["Ling-Qi Zhang", "Zahra Kadkhodaie", "Eero P. Simoncelli", "David H. Brainard"], "title": "Generalized Compressed Sensing for Image Reconstruction with Diffusion Probabilistic Models", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "We examine the problem of selecting a small set of linear measurements for\nreconstructing high-dimensional signals. Well-established methods for\noptimizing such measurements include principal component analysis (PCA),\nindependent component analysis (ICA) and compressed sensing (CS) based on\nrandom projections, all of which rely on axis- or subspace-aligned statistical\ncharacterization of the signal source. However, many naturally occurring\nsignals, including photographic images, contain richer statistical structure.\nTo exploit such structure, we introduce a general method for obtaining an\noptimized set of linear measurements for efficient image reconstruction, where\nthe signal statistics are expressed by the prior implicit in a neural network\ntrained to perform denoising (known as a ``diffusion model''). We demonstrate\nthat the optimal measurements derived for two natural image datasets differ\nfrom those of PCA, ICA, or CS, and result in substantially lower mean squared\nreconstruction error. Interestingly, the marginal distributions of the\nmeasurement values are asymmetrical (skewed), substantially more so than those\nof previous methods. We also find that optimizing with respect to perceptual\nloss, as quantified by structural similarity (SSIM), leads to measurements\ndifferent from those obtained when optimizing for MSE. Our results highlight\nthe importance of incorporating the specific statistical regularities of\nnatural signals when designing effective linear measurements."}
{"id": "2505.07162", "pdf": "https://arxiv.org/pdf/2505.07162", "abs": "https://arxiv.org/abs/2505.07162", "authors": ["Hajar Sakai", "Sarah S. Lam"], "title": "KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "The increasing volume of healthcare textual data requires computationally\nefficient, yet highly accurate classification approaches able to handle the\nnuanced and complex nature of medical terminology. This research presents\nKnowledge Distillation for Healthcare Multi-Label Text Classification\n(KDH-MLTC), a framework leveraging model compression and Large Language Models\n(LLMs). The proposed approach addresses conventional healthcare Multi-Label\nText Classification (MLTC) challenges by integrating knowledge distillation and\nsequential fine-tuning, subsequently optimized through Particle Swarm\nOptimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from\na more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e.,\nDistilBERT) through sequential training adapted to MLTC that preserves the\nteacher's learned information while significantly reducing computational\nrequirements. As a result, the classification is enabled to be conducted\nlocally, making it suitable for healthcare textual data characterized by\nsensitivity and, therefore, ensuring HIPAA compliance. The experiments\nconducted on three medical literature datasets of different sizes, sampled from\nthe Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves\nsuperior performance compared to existing approaches, particularly for the\nlargest dataset, reaching an F1 score of 82.70%. Additionally, statistical\nvalidation and an ablation study are carried out, proving the robustness of\nKDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process\nallowed the identification of optimal configurations. The proposed approach\ncontributes to healthcare text classification research, balancing efficiency\nrequirements in resource-constrained healthcare settings with satisfactory\naccuracy demands."}
{"id": "2505.06603", "pdf": "https://arxiv.org/pdf/2505.06603", "abs": "https://arxiv.org/abs/2505.06603", "authors": ["Lei Hu", "Zhiyong Gan", "Ling Deng", "Jinglin Liang", "Lingyu Liang", "Shuangping Huang", "Tianshui Chen"], "title": "ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Continual Anomaly Detection (CAD) enables anomaly detection models in\nlearning new classes while preserving knowledge of historical classes. CAD\nfaces two key challenges: catastrophic forgetting and segmentation of small\nanomalous regions. Existing CAD methods store image distributions or patch\nfeatures to mitigate catastrophic forgetting, but they fail to preserve\npixel-level detailed features for accurate segmentation. To overcome this\nlimitation, we propose ReplayCAD, a novel diffusion-driven generative replay\nframework that replay high-quality historical data, thus effectively preserving\npixel-level detailed features. Specifically, we compress historical data by\nsearching for a class semantic embedding in the conditional space of the\npre-trained diffusion model, which can guide the model to replay data with\nfine-grained pixel details, thus improving the segmentation performance.\nHowever, relying solely on semantic features results in limited spatial\ndiversity. Hence, we further use spatial features to guide data compression,\nachieving precise control of sample space, thereby generating more diverse\ndata. Our method achieves state-of-the-art performance in both classification\nand segmentation, with notable improvements in segmentation: 11.5% on VisA and\n8.1% on MVTec. Our source code is available at\nhttps://github.com/HULEI7/ReplayCAD."}
{"id": "2505.07049", "pdf": "https://arxiv.org/pdf/2505.07049", "abs": "https://arxiv.org/abs/2505.07049", "authors": ["Yubo Shu", "Zhewei Huang", "Xin Wu", "Chen Hu", "Shuchang Zhou", "Daxin Jiang"], "title": "DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs", "categories": ["cs.AI"], "comment": null, "summary": "We propose DialogueReason, a reasoning paradigm that uncovers the lost roles\nin monologue-style reasoning models, aiming to boost diversity and coherency of\nthe reasoning process. Recent advances in RL-based large reasoning models have\nled to impressive long CoT capabilities and high performance on math and\nscience benchmarks. However, these reasoning models rely mainly on\nmonologue-style reasoning, which often limits reasoning diversity and\ncoherency, frequently recycling fixed strategies or exhibiting unnecessary\nshifts in attention. Our work consists of an analysis of monologue reasoning\npatterns and the development of a dialogue-based reasoning approach. We first\nintroduce the Compound-QA task, which concatenates multiple problems into a\nsingle prompt to assess both diversity and coherency of reasoning. Our analysis\nshows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by\nboth quantitative metrics and qualitative reasoning traces. Building on the\nanalysis, we propose a dialogue-based reasoning, named DialogueReason,\nstructured around agents, environment, and interactions. Using PPO with\nrule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt\ndialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA\ndatasets, showing that the dialogue reasoning model outperforms monologue\nmodels under more complex compound questions. Additionally, we discuss how\ndialogue-based reasoning helps enhance interpretability, facilitate more\nintuitive human interaction, and inspire advances in multi-agent system design."}
{"id": "2505.06300", "pdf": "https://arxiv.org/pdf/2505.06300", "abs": "https://arxiv.org/abs/2505.06300", "authors": ["Umberto Gonçalves de Sousa"], "title": "ARDNS-FN-Quantum: A Quantum-Enhanced Reinforcement Learning Framework with Cognitive-Inspired Adaptive Exploration for Dynamic Environments", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 7 figures", "summary": "Reinforcement learning (RL) has transformed sequential decision making, yet\ntraditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy\nOptimization (PPO) often struggle with efficient exploration, stability, and\nadaptability in dynamic environments. This study presents ARDNS-FN-Quantum\n(Adaptive Reward-Driven Neural Simulator with Quantum enhancement), a novel\nframework that integrates a 2-qubit quantum circuit for action selection, a\ndual-memory system inspired by human cognition, and adaptive exploration\nstrategies modulated by reward variance and curiosity. Evaluated in a 10X10\ngrid-world over 20,000 episodes, ARDNS-FN-Quantum achieves a 99.5% success rate\n(versus 81.3% for DQN and 97.0% for PPO), a mean reward of 9.0528 across all\nepisodes (versus 1.2941 for DQN and 7.6196 for PPO), and an average of 46.7\nsteps to goal (versus 135.9 for DQN and 62.5 for PPO). In the last 100\nepisodes, it records a mean reward of 9.1652 (versus 7.0916 for DQN and 9.0310\nfor PPO) and 37.2 steps to goal (versus 52.7 for DQN and 53.4 for PPO).\nGraphical analyses, including learning curves, steps-to-goal trends, reward\nvariance, and reward distributions, demonstrate ARDNS-FN-Quantum's superior\nstability (reward variance 5.424 across all episodes versus 252.262 for DQN and\n76.583 for PPO) and efficiency. By bridging quantum computing, cognitive\nscience, and RL, ARDNS-FN-Quantum offers a scalable, human-like approach to\nadaptive learning in uncertain environments, with potential applications in\nrobotics, autonomous systems, and decision-making under uncertainty."}
{"id": "2410.01098", "pdf": "https://arxiv.org/pdf/2410.01098", "abs": "https://arxiv.org/abs/2410.01098", "authors": ["Hanlong Wan", "Jian Zhang", "Yan Chen", "Weili Xu", "Fan Feng"], "title": "Exploring Gen-AI applications in building research and industry: A review", "categories": ["cs.AI", "cs.SY", "eess.IV", "eess.SY"], "comment": "This is a pre-peer review and copy editing version of an article\n  published in Building Simulation. The final authenticated version is\n  available online at:https://doi.org/10.1007/s12273-025-1279-x", "summary": "This paper investigates the transformative potential of Generative AI\n(Gen-AI) technologies, particularly large language models, within the building\nindustry. By leveraging these advanced AI tools, the study explores their\napplication across key areas such as automated compliance checking and building\ndesign assistance. The research highlights how Gen-AI can automate\nlabor-intensive processes, significantly improving efficiency and reducing\ncosts in building practices. The paper first discusses the two widely applied\nfundamental models-Transformer and Diffusion model-and summarizes current\npathways for accessing Gen-AI models and the most common techniques for\ncustomizing them. It then explores applications for text generation, such as\ncompliance checking, control support, data mining, and building simulation\ninput file editing. Additionally, it examines image generation, including\ndirect generation through diffusion models and indirect generation through\nlanguage model-supported template creation based on existing Computer-Aided\nDesign or other design tools with rendering. The paper concludes with a\ncomprehensive analysis of the current capabilities of Gen-AI in the building\nindustry, outlining future directions for research and development, with the\ngoal of paving the way for smarter, more effective, and responsive design,\nconstruction, and operational practices."}
{"id": "2505.07184", "pdf": "https://arxiv.org/pdf/2505.07184", "abs": "https://arxiv.org/abs/2505.07184", "authors": ["Yifan Wei", "Xiaoyan Yu", "Tengfei Pan", "Angsheng Li", "Li Du"], "title": "Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved unprecedented performance by\nleveraging vast pretraining corpora, yet their performance remains suboptimal\nin knowledge-intensive domains such as medicine and scientific research, where\nhigh factual precision is required. While synthetic data provides a promising\navenue for augmenting domain knowledge, existing methods frequently generate\nredundant samples that do not align with the model's true knowledge gaps. To\novercome this limitation, we propose a novel Structural Entropy-guided\nKnowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge\ndeficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to\nquantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree\nSearch (MCTS) to selectively explore regions where the model lacks\ndomain-specific knowledge. Guided by these insights, the framework generates\ntargeted synthetic data for supervised fine-tuning, enabling continuous\nself-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple\ndomain-specific benchmarks show that SENATOR effectively detects and repairs\nknowledge deficiencies, achieving notable performance improvements. The code\nand data for our methods and experiments are available at\nhttps://github.com/weiyifan1023/senator."}
{"id": "2505.06635", "pdf": "https://arxiv.org/pdf/2505.06635", "abs": "https://arxiv.org/abs/2505.06635", "authors": ["Xu Zheng", "Yuanhuiyi Lyu", "Lutao Jiang", "Danda Pani Paudel", "Luc Van Gool", "Xuming Hu"], "title": "Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization", "categories": ["cs.CV"], "comment": null, "summary": "Fusing and balancing multi-modal inputs from novel sensors for dense\nprediction tasks, particularly semantic segmentation, is critically important\nyet remains a significant challenge. One major limitation is the tendency of\nmulti-modal frameworks to over-rely on easily learnable modalities, a\nphenomenon referred to as unimodal dominance or bias. This issue becomes\nespecially problematic in real-world scenarios where the dominant modality may\nbe unavailable, resulting in severe performance degradation. To this end, we\napply a simple but effective plug-and-play regularization term based on\nfunctional entropy, which introduces no additional parameters or modules. This\nterm is designed to intuitively balance the contribution of each visual\nmodality to the segmentation results. Specifically, we leverage the log-Sobolev\ninequality to bound functional entropy using functional-Fisher-information. By\nmaximizing the information contributed by each visual modality, our approach\nmitigates unimodal dominance and establishes a more balanced and robust\nsegmentation framework. A multi-scale regularization module is proposed to\napply our proposed plug-and-play term on high-level features and also\nsegmentation predictions for more balanced multi-modal learning. Extensive\nexperiments on three datasets demonstrate that our proposed method achieves\nsuperior performance, i.e., +13.94%, +3.25%, and +3.64%, without introducing\nany additional parameters."}
{"id": "2505.07052", "pdf": "https://arxiv.org/pdf/2505.07052", "abs": "https://arxiv.org/abs/2505.07052", "authors": ["Humam Kourani", "Gyunam Park", "Wil M. P. van der Aalst"], "title": "Unlocking Non-Block-Structured Decisions: Inductive Mining with Choice Graphs", "categories": ["cs.AI"], "comment": "The Version of Record of this contribution will be published in the\n  proceedings of the 23rd International Conference on Business Process\n  Management (BPM 2025). This preprint has not undergone peer review or any\n  post-submission improvements or corrections", "summary": "Process discovery aims to automatically derive process models from event\nlogs, enabling organizations to analyze and improve their operational\nprocesses. Inductive mining algorithms, while prioritizing soundness and\nefficiency through hierarchical modeling languages, often impose a strict\nblock-structured representation. This limits their ability to accurately\ncapture the complexities of real-world processes. While recent advancements\nlike the Partially Ordered Workflow Language (POWL) have addressed the\nblock-structure limitation for concurrency, a significant gap remains in\neffectively modeling non-block-structured decision points. In this paper, we\nbridge this gap by proposing an extension of POWL to handle\nnon-block-structured decisions through the introduction of choice graphs.\nChoice graphs offer a structured yet flexible approach to model complex\ndecision logic within the hierarchical framework of POWL. We present an\ninductive mining discovery algorithm that uses our extension and preserves the\nquality guarantees of the inductive mining framework. Our experimental\nevaluation demonstrates that the discovered models, enriched with choice\ngraphs, more precisely represent the complex decision-making behavior found in\nreal-world processes, without compromising the high scalability inherent in\ninductive mining techniques."}
{"id": "2505.06301", "pdf": "https://arxiv.org/pdf/2505.06301", "abs": "https://arxiv.org/abs/2505.06301", "authors": ["Xiaozhou Ye", "Kevin I-Kai Wang"], "title": "Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Cross-user variability in Human Activity Recognition (HAR) remains a critical\nchallenge due to differences in sensor placement, body dynamics, and behavioral\npatterns. Traditional methods often fail to capture biomechanical invariants\nthat persist across users, limiting their generalization capability. We propose\nan Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)\nframework that integrates anatomical correlation knowledge into a unified graph\nneural network (GNN) architecture. By modeling three biomechanically motivated\nrelationships together-Interconnected Units, Analogous Units, and Lateral\nUnits-our method encodes domain-invariant features while addressing\nuser-specific variability through Variational Edge Feature Extractor. A\nGradient Reversal Layer (GRL) enforces adversarial domain generalization,\nensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and\nDSADS datasets demonstrate state-of-the-art performance. Our work bridges\nbiomechanical principles with graph-based adversarial learning by integrating\ninformation fusion techniques. This fusion of information underpins our unified\nand generalized model for cross-user HAR."}
{"id": "2504.09455", "pdf": "https://arxiv.org/pdf/2504.09455", "abs": "https://arxiv.org/abs/2504.09455", "authors": ["Hussain Md. Safwan", "Mahbub Islam Mahim"], "title": "Enhancing Wide-Angle Image Using Narrow-Angle View of the Same Scene", "categories": ["cs.CV", "eess.IV", "F.2.2; I.2.7"], "comment": null, "summary": "A common dilemma while photographing a scene is whether to capture it at a\nwider angle, allowing more of the scene to be covered but in less detail or to\nclick in a narrow angle that captures better details but leaves out portions of\nthe scene. We propose a novel method in this paper that infuses wider shots\nwith finer quality details that is usually associated with an image captured by\nthe primary lens by capturing the same scene using both narrow and wide field\nof view (FoV) lenses. We do so by training a Generative Adversarial Network\n(GAN)-based model to learn to extract the visual quality parameters from a\nnarrow-angle shot and to transfer these to the corresponding wide-angle image\nof the scene using residual connections and an attention-based fusion module.\nWe have mentioned in details the proposed technique to isolate the visual\nessence of an image and to transfer it into another image. We have also\nelaborately discussed our implementation details and have presented the results\nof evaluation over several benchmark datasets and comparisons with contemporary\nadvancements in the field."}
{"id": "2505.07205", "pdf": "https://arxiv.org/pdf/2505.07205", "abs": "https://arxiv.org/abs/2505.07205", "authors": ["Mouxiao Bian", "Rongzhao Zhang", "Chao Ding", "Xinwei Peng", "Jie Xu"], "title": "Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are poised to transform healthcare under China's\nHealthy China 2030 initiative, yet they introduce new ethical and\npatient-safety challenges. We present a novel 12,000-item Q&A benchmark\ncovering 11 ethics and 9 safety dimensions in medical contexts, to\nquantitatively evaluate these risks. Using this dataset, we assess\nstate-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing\nmoderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant\nimprovements after fine-tuning on our data (up to 50.8% accuracy). Results show\nnotable gaps in LLM decision-making on ethics and safety scenarios, reflecting\ninsufficient institutional oversight. We then identify systemic governance\nshortfalls-including the lack of fine-grained ethical audit protocols, slow\nadaptation by hospital IRBs, and insufficient evaluation tools-that currently\nhinder safe LLM deployment. Finally, we propose a practical governance\nframework for healthcare institutions (embedding LLM auditing teams, enacting\ndata ethics guidelines, and implementing safety simulation pipelines) to\nproactively manage LLM risks. Our study highlights the urgent need for robust\nLLM governance in Chinese healthcare, aligning AI innovation with patient\nsafety and ethical standards."}
{"id": "2505.06647", "pdf": "https://arxiv.org/pdf/2505.06647", "abs": "https://arxiv.org/abs/2505.06647", "authors": ["Zhe Li", "Sarah Cechnicka", "Cheng Ouyang", "Katharina Breininger", "Peter Schüffler", "Bernhard Kainz"], "title": "Dataset Distillation with Probabilistic Latent Features", "categories": ["cs.CV"], "comment": "23 pages", "summary": "As deep learning models grow in complexity and the volume of training data\nincreases, reducing storage and computational costs becomes increasingly\nimportant. Dataset distillation addresses this challenge by synthesizing a\ncompact set of synthetic data that can effectively replace the original dataset\nin downstream classification tasks. While existing methods typically rely on\nmapping data from pixel space to the latent space of a generative model, we\npropose a novel stochastic approach that models the joint distribution of\nlatent features. This allows our method to better capture spatial structures\nand produce diverse synthetic samples, which benefits model training.\nSpecifically, we introduce a low-rank multivariate normal distribution\nparameterized by a lightweight network. This design maintains low computational\ncomplexity and is compatible with various matching networks used in dataset\ndistillation. After distillation, synthetic images are generated by feeding the\nlearned latent features into a pretrained generator. These synthetic images are\nthen used to train classification models, and performance is evaluated on real\ntest set. We validate our method on several benchmarks, including ImageNet\nsubsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach\nachieves state-of-the-art cross architecture performance across a range of\nbackbone architectures, demonstrating its generality and effectiveness."}
{"id": "2505.07079", "pdf": "https://arxiv.org/pdf/2505.07079", "abs": "https://arxiv.org/abs/2505.07079", "authors": ["Robert Johansson", "Patrick Hammer", "Tony Lofthouse"], "title": "Arbitrarily Applicable Same/Opposite Relational Responding with NARS", "categories": ["cs.AI"], "comment": null, "summary": "Same/opposite relational responding, a fundamental aspect of human symbolic\ncognition, allows the flexible generalization of stimulus relationships based\non minimal experience. In this study, we demonstrate the emergence of\n\\textit{arbitrarily applicable} same/opposite relational responding within the\nNon-Axiomatic Reasoning System (NARS), a computational cognitive architecture\ndesigned for adaptive reasoning under uncertainty. Specifically, we extend NARS\nwith an implementation of \\textit{acquired relations}, enabling the system to\nexplicitly derive both symmetric (mutual entailment) and novel relational\ncombinations (combinatorial entailment) from minimal explicit training in a\ncontextually controlled matching-to-sample (MTS) procedure. Experimental\nresults show that NARS rapidly internalizes explicitly trained relational rules\nand robustly demonstrates derived relational generalizations based on arbitrary\ncontextual cues. Importantly, derived relational responding in critical test\nphases inherently combines both mutual and combinatorial entailments, such as\nderiving same-relations from multiple explicitly trained opposite-relations.\nInternal confidence metrics illustrate strong internalization of these\nrelational principles, closely paralleling phenomena observed in human\nrelational learning experiments. Our findings underscore the potential for\nintegrating nuanced relational learning mechanisms inspired by learning\npsychology into artificial general intelligence frameworks, explicitly\nhighlighting the arbitrary and context-sensitive relational capabilities\nmodeled within NARS."}
{"id": "2505.06302", "pdf": "https://arxiv.org/pdf/2505.06302", "abs": "https://arxiv.org/abs/2505.06302", "authors": ["Xuzhi Zhang", "Shaohui Peng", "Qirui Zhou", "Yuanbo Wen", "Qi Guo", "Ruizhi Chen", "Xinguo Zhu", "Weiqiang Xiong", "Haixin Chen", "Congying Ma", "Ke Gao", "Chen Zhao", "Yanjun Wu", "Yunji Chen", "Ling Li"], "title": "QiMeng-TensorOp: Automatically Generating High-Performance Tensor Operators with Hardware Primitives", "categories": ["cs.LG", "cs.AI", "I.2.2"], "comment": "10 pages, 5 figures", "summary": "Computation-intensive tensor operators constitute over 90\\% of the\ncomputations in Large Language Models (LLMs) and Deep Neural\nNetworks.Automatically and efficiently generating high-performance tensor\noperators with hardware primitives is crucial for diverse and ever-evolving\nhardware architectures like RISC-V, ARM, and GPUs, as manually optimized\nimplementation takes at least months and lacks portability.LLMs excel at\ngenerating high-level language codes, but they struggle to fully comprehend\nhardware characteristics and produce high-performance tensor operators. We\nintroduce a tensor-operator auto-generation framework with a one-line user\nprompt (QiMeng-TensorOp), which enables LLMs to automatically exploit hardware\ncharacteristics to generate tensor operators with hardware primitives, and tune\nparameters for optimal performance across diverse hardware. Experimental\nresults on various hardware platforms, SOTA LLMs, and typical tensor operators\ndemonstrate that QiMeng-TensorOp effectively unleashes the computing capability\nof various hardware platforms, and automatically generates tensor operators of\nsuperior performance. Compared with vanilla LLMs, QiMeng-TensorOp achieves up\nto $1291 \\times$ performance improvement. Even compared with human experts,\nQiMeng-TensorOp could reach $251 \\%$ of OpenBLAS on RISC-V CPUs, and $124 \\%$\nof cuBLAS on NVIDIA GPUs. Additionally, QiMeng-TensorOp also significantly\nreduces development costs by $200 \\times$ compared with human experts."}
{"id": "2505.07233", "pdf": "https://arxiv.org/pdf/2505.07233", "abs": "https://arxiv.org/abs/2505.07233", "authors": ["Jiashuo Sun", "Xianrui Zhong", "Sizhe Zhou", "Jiawei Han"], "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 6 figures, 15 tables", "summary": "Retrieval-augmented generation (RAG) systems combine large language models\n(LLMs) with external knowledge retrieval, making them highly effective for\nknowledge-intensive tasks. A crucial but often under-explored component of\nthese systems is the reranker, which refines retrieved documents to enhance\ngeneration quality and explainability. The challenge of selecting the optimal\nnumber of documents (k) remains unsolved: too few may omit critical\ninformation, while too many introduce noise and inefficiencies. Although recent\nstudies have explored LLM-based rerankers, they primarily leverage internal\nmodel knowledge and overlook the rich supervisory signals that LLMs can\nprovide, such as using response quality as feedback for optimizing reranking\ndecisions. In this paper, we propose DynamicRAG, a novel RAG framework where\nthe reranker dynamically adjusts both the order and number of retrieved\ndocuments based on the query. We model the reranker as an agent optimized\nthrough reinforcement learning (RL), using rewards derived from LLM output\nquality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates\nsuperior performance, achieving state-of-the-art results. The model, data and\ncode are available at https://github.com/GasolSun36/DynamicRAG"}
{"id": "2505.06663", "pdf": "https://arxiv.org/pdf/2505.06663", "abs": "https://arxiv.org/abs/2505.06663", "authors": ["Yongqi Wang", "Xinxiao Wu", "Shuo Yang"], "title": "METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection", "categories": ["cs.CV"], "comment": "IJCAI2025", "summary": "Open-vocabulary video visual relationship detection aims to detect objects\nand their relationships in videos without being restricted by predefined object\nor relationship categories. Existing methods leverage the rich semantic\nknowledge of pre-trained vision-language models such as CLIP to identify novel\ncategories. They typically adopt a cascaded pipeline to first detect objects\nand then classify relationships based on the detected objects, which may lead\nto error propagation and thus suboptimal performance. In this paper, we propose\nMutual EnhancemenT of Objects and Relationships (METOR), a query-based unified\nframework to jointly model and mutually enhance object detection and\nrelationship classification in open-vocabulary scenarios. Under this framework,\nwe first design a CLIP-based contextual refinement encoding module that\nextracts visual contexts of objects and relationships to refine the encoding of\ntext features and object queries, thus improving the generalization of encoding\nto novel categories. Then we propose an iterative enhancement module to\nalternatively enhance the representations of objects and relationships by fully\nexploiting their interdependence to improve recognition performance. Extensive\nexperiments on two public datasets, VidVRD and VidOR, demonstrate that our\nframework achieves state-of-the-art performance."}
{"id": "2505.07087", "pdf": "https://arxiv.org/pdf/2505.07087", "abs": "https://arxiv.org/abs/2505.07087", "authors": ["Robert E. Wray", "James R. Kirk", "John E. Laird"], "title": "Architectural Precedents for General Agents using Large Language Models", "categories": ["cs.AI", "I.2.11; I.2.7"], "comment": "14 pages, 2 figures. Submitted to AGI25", "summary": "One goal of AI (and AGI) is to identify and understand specific mechanisms\nand representations sufficient for general intelligence. Often, this work\nmanifests in research focused on architectures and many cognitive architectures\nhave been explored in AI/AGI. However, different research groups and even\ndifferent research traditions have somewhat independently identified\nsimilar/common patterns of processes and representations or cognitive design\npatterns that are manifest in existing architectures. Today, AI systems\nexploiting large language models (LLMs) offer a relatively new combination of\nmechanism and representation available for exploring the possibilities of\ngeneral intelligence. In this paper, we summarize a few recurring cognitive\ndesign patterns that have appeared in various pre-transformer AI architectures.\nWe then explore how these patterns are evident in systems using LLMs,\nespecially for reasoning and interactive (\"agentic\") use cases. By examining\nand applying these recurring patterns, we can also predict gaps or deficiencies\nin today's Agentic LLM Systems and identify likely subjects of future research\ntowards general intelligence using LLMs and other generative foundation models."}
{"id": "2505.06303", "pdf": "https://arxiv.org/pdf/2505.06303", "abs": "https://arxiv.org/abs/2505.06303", "authors": ["Li Yuan", "Yi Cai", "Xudong Shen", "Qing Li", "Qingbao Huang", "Zikun Deng", "Tao Wang"], "title": "Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for Unified Multimodal Information Extraction", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Multimodal Information Extraction (MIE) has gained attention for extracting\nstructured information from multimedia sources. Traditional methods tackle MIE\ntasks separately, missing opportunities to share knowledge across tasks. Recent\napproaches unify these tasks into a generation problem using instruction-based\nT5 models with visual adaptors, optimized through full-parameter fine-tuning.\nHowever, this method is computationally intensive, and multi-task fine-tuning\noften faces gradient conflicts, limiting performance. To address these\nchallenges, we propose collaborative multi-LoRA experts with achievement-based\nmulti-task loss (C-LoRAE) for MIE tasks. C-LoRAE extends the low-rank\nadaptation (LoRA) method by incorporating a universal expert to learn shared\nmultimodal knowledge from cross-MIE tasks and task-specific experts to learn\nspecialized instructional task features. This configuration enhances the\nmodel's generalization ability across multiple tasks while maintaining the\nindependence of various instruction tasks and mitigating gradient conflicts.\nAdditionally, we propose an achievement-based multi-task loss to balance\ntraining progress across tasks, addressing the imbalance caused by varying\nnumbers of training samples in MIE tasks. Experimental results on seven\nbenchmark datasets across three key MIE tasks demonstrate that C-LoRAE achieves\nsuperior overall performance compared to traditional fine-tuning methods and\nLoRA methods while utilizing a comparable number of training parameters to\nLoRA."}
{"id": "2505.07247", "pdf": "https://arxiv.org/pdf/2505.07247", "abs": "https://arxiv.org/abs/2505.07247", "authors": ["Peichao Lai", "Kexuan Zhang", "Yi Lin", "Linyihan Zhang", "Feiyang Ye", "Jinhao Yan", "Yanwei Xu", "Conghui He", "Yilei Wang", "Wentao Zhang", "Bin Cui"], "title": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems."}
{"id": "2505.06665", "pdf": "https://arxiv.org/pdf/2505.06665", "abs": "https://arxiv.org/abs/2505.06665", "authors": ["Zixian Zhao", "Andrew Howes", "Xingchen Zhang"], "title": "MultiTaskVIF: Segmentation-oriented visible and infrared image fusion via multi-task learning", "categories": ["cs.CV"], "comment": null, "summary": "Visible and infrared image fusion (VIF) has attracted significant attention\nin recent years. Traditional VIF methods primarily focus on generating fused\nimages with high visual quality, while recent advancements increasingly\nemphasize incorporating semantic information into the fusion model during\ntraining. However, most existing segmentation-oriented VIF methods adopt a\ncascade structure comprising separate fusion and segmentation models, leading\nto increased network complexity and redundancy. This raises a critical\nquestion: can we design a more concise and efficient structure to integrate\nsemantic information directly into the fusion model during training-Inspired by\nmulti-task learning, we propose a concise and universal training framework,\nMultiTaskVIF, for segmentation-oriented VIF models. In this framework, we\nintroduce a multi-task head decoder (MTH) to simultaneously output both the\nfused image and the segmentation result during training. Unlike previous\ncascade training frameworks that necessitate joint training with a complete\nsegmentation model, MultiTaskVIF enables the fusion model to learn semantic\nfeatures by simply replacing its decoder with MTH. Extensive experimental\nevaluations validate the effectiveness of the proposed method. Our code will be\nreleased upon acceptance."}
{"id": "2505.07089", "pdf": "https://arxiv.org/pdf/2505.07089", "abs": "https://arxiv.org/abs/2505.07089", "authors": ["Hanzheng Dai", "Yuanliang Li", "Zhibo Zhang", "Jun Yan"], "title": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\nintrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks\noften underperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sighted planning\nin the planning process, and hallucinations during command generation. In\naddition, the penetration testing (PT) process, with its trial-and-error\nnature, is limited by existing frameworks that lack mechanisms to learn from\nprevious failed operations, restricting adaptive improvement of PT strategies.\nTo address these limitations, we propose a knowledge-informed self-reflective\nPT framework powered by LLMs, called RefPentester, which is an AutoPT framework\ndesigned to assist human operators in identifying the current stage of the PT\nprocess, selecting appropriate tactic and technique for the stage, choosing\nsuggested action, providing step-by-step operational guidance, and learning\nfrom previous failed operations. We also modeled the PT process as a\nseven-state Stage Machine to integrate the proposed framework effectively. The\nevaluation shows that RefPentester can successfully reveal credentials on Hack\nThe Box's Sau machine, outperforming the baseline GPT-4o model by 16.7\\%.\nAcross PT stages, RefPentester also demonstrates superior success rates on PT\nstage transitions."}
{"id": "2505.06316", "pdf": "https://arxiv.org/pdf/2505.06316", "abs": "https://arxiv.org/abs/2505.06316", "authors": ["Guozhong Li", "Muhannad Alhumaidi", "Spiros Skiadopoulos", "Ibrahim Hoteit", "Panos Kalnis"], "title": "GraphComp: Extreme Error-bounded Compression of Scientific Data via Temporal Graph Autoencoders", "categories": ["cs.LG"], "comment": null, "summary": "The generation of voluminous scientific data poses significant challenges for\nefficient storage, transfer, and analysis. Recently, error-bounded lossy\ncompression methods emerged due to their ability to achieve high compression\nratios while controlling data distortion. However, they often overlook the\ninherent spatial and temporal correlations within scientific data, thus missing\nopportunities for higher compression. In this paper we propose GRAPHCOMP, a\nnovel graph-based method for error-bounded lossy compression of scientific\ndata. We perform irregular segmentation of the original grid data and generate\na graph representation that preserves the spatial and temporal correlations.\nInspired by Graph Neural Networks (GNNs), we then propose a temporal graph\nautoencoder to learn latent representations that significantly reduce the size\nof the graph, effectively compressing the original data. Decompression reverses\nthe process and utilizes the learnt graph model together with the latent\nrepresentation to reconstruct an approximation of the original data. The\ndecompressed data are guaranteed to satisfy a user-defined point-wise error\nbound. We compare our method against the state-of-the-art error-bounded lossy\nmethods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and synthetic\ndata. GRAPHCOMP consistently achieves the highest compression ratio across most\ndatasets, outperforming the second-best method by margins ranging from 22% to\n50%."}
{"id": "2505.07258", "pdf": "https://arxiv.org/pdf/2505.07258", "abs": "https://arxiv.org/abs/2505.07258", "authors": ["Wenqiang Wang", "Siyuan Liang", "Yangshijie Zhang", "Xiaojun Jia", "Hao Lin", "Xiaochun Cao"], "title": "No Query, No Access", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Textual adversarial attacks mislead NLP models, including Large Language\nModels (LLMs), by subtly modifying text. While effective, existing attacks\noften require knowledge of the victim model, extensive queries, or access to\ntraining data, limiting real-world feasibility. To overcome these constraints,\nwe introduce the \\textbf{Victim Data-based Adversarial Attack (VDBA)}, which\noperates using only victim texts. To prevent access to the victim model, we\ncreate a shadow dataset with publicly available pre-trained models and\nclustering methods as a foundation for developing substitute models. To address\nthe low attack success rate (ASR) due to insufficient information feedback, we\npropose the hierarchical substitution model design, generating substitute\nmodels to mitigate the failure of a single substitute model at the decision\nboundary.\n  Concurrently, we use diverse adversarial example generation, employing\nvarious attack methods to generate and select the adversarial example with\nbetter similarity and attack effectiveness. Experiments on the Emotion and SST5\ndatasets show that VDBA outperforms state-of-the-art methods, achieving an ASR\nimprovement of 52.08\\% while significantly reducing attack queries to 0. More\nimportantly, we discover that VDBA poses a significant threat to LLMs such as\nQwen2 and the GPT family, and achieves the highest ASR of 45.99% even without\naccess to the API, confirming that advanced NLP models still face serious\nsecurity risks. Our codes can be found at\nhttps://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/"}
{"id": "2505.06670", "pdf": "https://arxiv.org/pdf/2505.06670", "abs": "https://arxiv.org/abs/2505.06670", "authors": ["Zhe Li", "Hadrien Reynaud", "Mischa Dombrowski", "Sarah Cechnicka", "Franciskus Xaverius Erick", "Bernhard Kainz"], "title": "Video Dataset Condensation with Diffusion Models", "categories": ["cs.CV"], "comment": "10 pages", "summary": "In recent years, the rapid expansion of dataset sizes and the increasing\ncomplexity of deep learning models have significantly escalated the demand for\ncomputational resources, both for data storage and model training. Dataset\ndistillation has emerged as a promising solution to address this challenge by\ngenerating a compact synthetic dataset that retains the essential information\nfrom a large real dataset. However, existing methods often suffer from limited\nperformance and poor data quality, particularly in the video domain. In this\npaper, we focus on video dataset distillation by employing a video diffusion\nmodel to generate high-quality synthetic videos. To enhance representativeness,\nwe introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select\na diverse and informative subset of videos that effectively captures the\ncharacteristics of the original dataset. To further optimize computational\nefficiency, we explore a training-free clustering algorithm, Temporal-Aware\nCluster-based Distillation (TAC-DT), to select representative videos without\nrequiring additional training overhead. We validate the effectiveness of our\napproach through extensive experiments on four benchmark datasets,\ndemonstrating performance improvements of up to \\(10.61\\%\\) over the\nstate-of-the-art. Our method consistently outperforms existing approaches\nacross all datasets, establishing a new benchmark for video dataset\ndistillation."}
{"id": "2505.07171", "pdf": "https://arxiv.org/pdf/2505.07171", "abs": "https://arxiv.org/abs/2505.07171", "authors": ["Jeongho Kim", "Chanyeong Heo", "Jaehee Jung"], "title": "ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for Few-Shot Knowledge Graph Completion", "categories": ["cs.AI", "cs.IR"], "comment": "Accepted by SIGIR 2025, 5 pages, 1 figure", "summary": "Knowledge Graphs (KGs), composed of triples in the form of (head, relation,\ntail) and consisting of entities and relations, play a key role in information\nretrieval systems such as question answering, entity search, and\nrecommendation. In real-world KGs, although many entities exist, the relations\nexhibit a long-tail distribution, which can hinder information retrieval\nperformance. Previous few-shot knowledge graph completion studies focused\nexclusively on the positive triple information that exists in the graph or,\nwhen negative triples were incorporated, used them merely as a signal to\nindicate incorrect triples. To overcome this limitation, we propose\nRelation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First,\nnegative triples are generated by randomly replacing the tail entity in the\nsupport set. By conditionally incorporating positive information in the KG and\nnon-existent negative information into the diffusion process, the model\nseparately estimates the latent distributions for positive and negative\nrelations. Moreover, including an attention pooler enables the model to\nleverage the differences between positive and negative cases explicitly.\nExperiments on two widely used datasets demonstrate that our method outperforms\nexisting approaches, achieving state-of-the-art performance. The code is\navailable at https://github.com/hou27/ReCDAP-FKGC."}
{"id": "2505.06319", "pdf": "https://arxiv.org/pdf/2505.06319", "abs": "https://arxiv.org/abs/2505.06319", "authors": ["Zijian An", "Lifeng Zhou"], "title": "Reinforcement Learning for Game-Theoretic Resource Allocation on Graphs", "categories": ["cs.LG", "cs.GT"], "comment": "12 pages, 7 figures", "summary": "Game-theoretic resource allocation on graphs (GRAG) involves two players\ncompeting over multiple steps to control nodes of interest on a graph, a\nproblem modeled as a multi-step Colonel Blotto Game (MCBG). Finding optimal\nstrategies is challenging due to the dynamic action space and structural\nconstraints imposed by the graph. To address this, we formulate the MCBG as a\nMarkov Decision Process (MDP) and apply Reinforcement Learning (RL) methods,\nspecifically Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). To\nenforce graph constraints, we introduce an action-displacement adjacency matrix\nthat dynamically generates valid action sets at each step. We evaluate RL\nperformance across a variety of graph structures and initial resource\ndistributions, comparing against random, greedy, and learned RL policies.\nExperimental results show that both DQN and PPO consistently outperform\nbaseline strategies and converge to a balanced $50\\%$ win rate when competing\nagainst the learned RL policy. Particularly, on asymmetric graphs, RL agents\nsuccessfully exploit structural advantages and adapt their allocation\nstrategies, even under disadvantageous initial resource distributions."}
{"id": "2505.07271", "pdf": "https://arxiv.org/pdf/2505.07271", "abs": "https://arxiv.org/abs/2505.07271", "authors": ["Jiwoo Hong", "Noah Lee", "Eunki Kim", "Guijin Son", "Woojin Chung", "Aman Gupta", "Shao Tang", "James Thorne"], "title": "On the Robustness of Reward Models for Language Model Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "The Bradley-Terry (BT) model is widely practiced in reward modeling for\nreinforcement learning with human feedback (RLHF). Despite its effectiveness,\nreward models (RMs) trained with BT model loss are prone to over-optimization,\nlosing generalizability to unseen input distributions. In this paper, we study\nthe cause of over-optimization in RM training and its downstream effects on the\nRLHF procedure, accentuating the importance of distributional robustness of RMs\nin unseen data. First, we show that the excessive dispersion of hidden state\nnorms is the main source of over-optimization. Then, we propose batch-wise\nsum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,\nconstraining the rewards with extreme magnitudes. We assess the impact of BSR\nin improving robustness in RMs through four scenarios of over-optimization,\nwhere BSR consistently manifests better robustness. Subsequently, we compare\nthe plain BT model and BSR on RLHF training and empirically show that robust\nRMs better align the policy to the gold preference model. Finally, we apply BSR\nto high-quality data and models, which surpasses state-of-the-art RMs in the 8B\nscale by adding more than 5% in complex preference prediction tasks. By\nconducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length\nby 40% while adding a 7% increase in win rate, further highlighting that\nrobustness in RMs induces robustness in RLHF training. We release the code,\ndata, and models: https://github.com/LinkedIn-XFACT/RM-Robustness."}
{"id": "2505.06679", "pdf": "https://arxiv.org/pdf/2505.06679", "abs": "https://arxiv.org/abs/2505.06679", "authors": ["Jiayang Liu", "Siyuan Liang", "Shiqian Zhao", "Rongcheng Tu", "Wenbo Zhou", "Xiaochun Cao", "Dacheng Tao", "Siew Kei Lam"], "title": "Jailbreaking the Text-to-Video Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video generative models have achieved significant progress, driven by\nthe rapid advancements in diffusion models, with notable examples including\nPika, Luma, Kling, and Sora. Despite their remarkable generation ability, their\nvulnerability to jailbreak attack, i.e. to generate unsafe content, including\npornography, violence, and discrimination, raises serious safety concerns.\nExisting efforts, such as T2VSafetyBench, have provided valuable benchmarks for\nevaluating the safety of text-to-video models against unsafe prompts but lack\nsystematic studies for exploiting their vulnerabilities effectively. In this\npaper, we propose the \\textit{first} optimization-based jailbreak attack\nagainst text-to-video models, which is specifically designed. Our approach\nformulates the prompt generation task as an optimization problem with three key\nobjectives: (1) maximizing the semantic similarity between the input and\ngenerated prompts, (2) ensuring that the generated prompts can evade the safety\nfilter of the text-to-video model, and (3) maximizing the semantic similarity\nbetween the generated videos and the original input prompts. To further enhance\nthe robustness of the generated prompts, we introduce a prompt mutation\nstrategy that creates multiple prompt variants in each iteration, selecting the\nmost effective one based on the averaged score. This strategy not only improves\nthe attack success rate but also boosts the semantic relevance of the generated\nvideo. We conduct extensive experiments across multiple text-to-video models,\nincluding Open-Sora, Pika, Luma, and Kling. The results demonstrate that our\nmethod not only achieves a higher attack success rate compared to baseline\nmethods but also generates videos with greater semantic similarity to the\noriginal input prompts."}
{"id": "2505.07178", "pdf": "https://arxiv.org/pdf/2505.07178", "abs": "https://arxiv.org/abs/2505.07178", "authors": ["Yuri Nakao"], "title": "Accountability of Generative AI: Exploring a Precautionary Approach for \"Artificially Created Nature\"", "categories": ["cs.AI"], "comment": null, "summary": "The rapid development of generative artificial intelligence (AI) technologies\nraises concerns about the accountability of sociotechnical systems. Current\ngenerative AI systems rely on complex mechanisms that make it difficult for\neven experts to fully trace the reasons behind the outputs. This paper first\nexamines existing research on AI transparency and accountability and argues\nthat transparency is not a sufficient condition for accountability but can\ncontribute to its improvement. We then discuss that if it is not possible to\nmake generative AI transparent, generative AI technology becomes ``artificially\ncreated nature'' in a metaphorical sense, and suggest using the precautionary\nprinciple approach to consider AI risks. Finally, we propose that a platform\nfor citizen participation is needed to address the risks of generative AI."}
{"id": "2505.06320", "pdf": "https://arxiv.org/pdf/2505.06320", "abs": "https://arxiv.org/abs/2505.06320", "authors": ["Jan Kościałkowski", "Paweł Marcinkowski"], "title": "Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "8 pages, 6 figures, 4 tables, developed as a final project for the\n  Stanford Center for Professional Education XCS224U (Natural Language\n  Understanding) course", "summary": "Sentiment classification, a complex task in natural language processing,\nbecomes even more challenging when analyzing passages with multiple conflicting\ntones. Typically, longer passages exacerbate this issue, leading to decreased\nmodel performance. The aim of this paper is to introduce novel methodologies\nfor isolating conflicting sentiments and aggregating them to effectively\npredict the overall sentiment of such passages. One of the aggregation\nstrategies involves a Multi-Layer Perceptron (MLP) model which outperforms\nbaseline models across various datasets, including Amazon, Twitter, and SST\nwhile costing $\\sim$1/100 of what fine-tuning the baseline would take."}
{"id": "2505.07289", "pdf": "https://arxiv.org/pdf/2505.07289", "abs": "https://arxiv.org/abs/2505.07289", "authors": ["Stanislas Laborde", "Martin Cousseau", "Antoun Yaacoub", "Lionel Prevost"], "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?", "categories": ["cs.CL", "cs.AI", "cs.LG", "68P30 (Primary) 68T07, 68T50 (Secondary)", "I.2.6; I.5.1; I.2.7"], "comment": "Accepted for publication in the Proceedings of the 2025 International\n  Joint Conference on Neural Networks (IJCNN); this arXiv version includes an\n  appendix with 6 result tables; 10 pages, 15 figures, 7 tables", "summary": "The exponential growth in Large Language Model (LLM) deployment has\nintensified the need for efficient model compression techniques to reduce\ncomputational and memory costs. While pruning and quantization have shown\npromise, their combined potential remains largely unexplored. In this paper, we\nexamine joint compression and how strategically combining pruning and\nquantization could yield superior performance-to-compression ratios compared to\nsingle-method approaches. Recognizing the challenges in accurately assessing\nLLM performance, we address key limitations of previous evaluation frameworks\nand introduce the Semantic Retention Compression Rate (SrCr), a novel metric\nthat quantifies the trade-off between model compression and semantic\npreservation, facilitating the optimization of pruning-quantization\nconfigurations. Experiments demonstrate that our recommended combination\nachieves, on average, a 20% performance increase compared to an equivalent\nquantization-only model at the same theoretical compression rate."}
{"id": "2505.06683", "pdf": "https://arxiv.org/pdf/2505.06683", "abs": "https://arxiv.org/abs/2505.06683", "authors": ["Chunming He", "Rihan Zhang", "Fengyang Xiao", "Chengyu Fang", "Longxiang Tang", "Yulun Zhang", "Sina Farsiu"], "title": "UnfoldIR: Rethinking Deep Unfolding Network in Illumination Degradation Image Restoration", "categories": ["cs.CV"], "comment": "16 pages, 14 tables, 11 figures", "summary": "Deep unfolding networks (DUNs) are widely employed in illumination\ndegradation image restoration (IDIR) to merge the interpretability of\nmodel-based approaches with the generalization of learning-based methods.\nHowever, the performance of DUN-based methods remains considerably inferior to\nthat of state-of-the-art IDIR solvers. Our investigation indicates that this\nlimitation does not stem from structural shortcomings of DUNs but rather from\nthe limited exploration of the unfolding structure, particularly for (1)\nconstructing task-specific restoration models, (2) integrating advanced network\narchitectures, and (3) designing DUN-specific loss functions. To address these\nissues, we propose a novel DUN-based method, UnfoldIR, for IDIR tasks. UnfoldIR\nfirst introduces a new IDIR model with dedicated regularization terms for\nsmoothing illumination and enhancing texture. We unfold the iterative optimized\nsolution of this model into a multistage network, with each stage comprising a\nreflectance-assisted illumination correction (RAIC) module and an\nillumination-guided reflectance enhancement (IGRE) module. RAIC employs a\nvisual state space (VSS) to extract non-local features, enforcing illumination\nsmoothness, while IGRE introduces a frequency-aware VSS to globally align\nsimilar textures, enabling mildly degraded regions to guide the enhancement of\ndetails in more severely degraded areas. This suppresses noise while enhancing\ndetails. Furthermore, given the multistage structure, we propose an inter-stage\ninformation consistent loss to maintain network stability in the final stages.\nThis loss contributes to structural preservation and sustains the model's\nperformance even in unsupervised settings. Experiments verify our effectiveness\nacross 5 IDIR tasks and 3 downstream problems."}
{"id": "2505.07215", "pdf": "https://arxiv.org/pdf/2505.07215", "abs": "https://arxiv.org/abs/2505.07215", "authors": ["Vivek Verma", "David Huang", "William Chen", "Dan Klein", "Nicholas Tomlin"], "title": "Measuring General Intelligence with Generated Games", "categories": ["cs.AI"], "comment": null, "summary": "We present gg-bench, a collection of game environments designed to evaluate\ngeneral reasoning capabilities in language models. Unlike most static\nbenchmarks, gg-bench is a data generating process where new evaluation\ninstances can be generated at will. In particular, gg-bench is synthetically\ngenerated by (1) using a large language model (LLM) to generate natural\nlanguage descriptions of novel games, (2) using the LLM to implement each game\nin code as a Gym environment, and (3) training reinforcement learning (RL)\nagents via self-play on the generated games. We evaluate language models by\ntheir winrate against these RL agents by prompting models with the game\ndescription, current board state, and a list of valid moves, after which models\noutput the moves they wish to take. gg-bench is challenging: state-of-the-art\nLLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench\nusing in-context learning, while reasoning models such as o1, o3-mini and\nDeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,\ndata generation process, and evaluation code in order to support future\nmodeling work and expansion of our benchmark."}
{"id": "2505.06321", "pdf": "https://arxiv.org/pdf/2505.06321", "abs": "https://arxiv.org/abs/2505.06321", "authors": ["Hang Gao", "Chenhao Zhang", "Tie Wang", "Junsuo Zhao", "Fengge Wu", "Changwen Zheng", "Huaping Liu"], "title": "Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, they still face significant challenges, including high\ncomputational costs for training and limitations in solving complex reasoning\nproblems. Although existing methods have extended the reasoning capabilities of\nLLMs through structured paradigms, these approaches often rely on task-specific\nprompts and predefined reasoning processes, which constrain their flexibility\nand generalizability. To address these limitations, we propose a novel\nframework that leverages graph learning to enable more flexible and adaptive\nreasoning capabilities for LLMs. Specifically, this approach models the\nreasoning process of a problem as a graph and employs LLM-based graph learning\nto guide the adaptive generation of each reasoning step. To further enhance the\nadaptability of the model, we introduce a Graph Neural Network (GNN) module to\nperform representation learning on the generated reasoning process, enabling\nreal-time adjustments to both the model and the prompt. Experimental results\ndemonstrate that this method significantly improves reasoning performance\nacross multiple tasks without requiring additional training or task-specific\nprompt design. Code can be found in https://github.com/zch65458525/L2T."}
{"id": "2505.07293", "pdf": "https://arxiv.org/pdf/2505.07293", "abs": "https://arxiv.org/abs/2505.07293", "authors": ["Kai Hua", "Steven Wu", "Ge Zhang", "Ke Shen"], "title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection", "categories": ["cs.CL"], "comment": "28 pages, 19 figures", "summary": "Recently, there has been growing interest in collecting reasoning-intensive\npretraining data to improve LLMs' complex reasoning ability. Prior approaches\ntypically rely on supervised classifiers to identify such data, which requires\nlabeling by humans or LLMs, often introducing domain-specific biases. Due to\nthe attention heads being crucial to in-context reasoning, we propose\nAttentionInfluence, a simple yet effective, training-free method without\nsupervision signal. Our approach enables a small pretrained language model to\nact as a strong data selector through a simple attention head masking\noperation. Specifically, we identify retrieval heads and compute the loss\ndifference when masking these heads. We apply AttentionInfluence to a\n1.3B-parameter dense model to conduct data selection on the SmolLM corpus of\n241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B\ntokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD\nlearning rate scheduling. Our experimental results demonstrate substantial\nimprovements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive\nand reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and\nHumanEval). This demonstrates an effective weak-to-strong scaling property,\nwith small models improving the final performance of larger models-offering a\npromising and scalable path for reasoning-centric data selection."}
{"id": "2505.06684", "pdf": "https://arxiv.org/pdf/2505.06684", "abs": "https://arxiv.org/abs/2505.06684", "authors": ["Xuefeng Jiang", "Jia Li", "Nannan Wu", "Zhiyuan Wu", "Xujing Li", "Sheng Sun", "Gang Xu", "Yuwei Wang", "Qi Li", "Min Liu"], "title": "FNBench: Benchmarking Robust Federated Learning against Noisy Labels", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to IEEE TDSC, currently under major revision", "summary": "Robustness to label noise within data is a significant challenge in federated\nlearning (FL). From the data-centric perspective, the data quality of\ndistributed datasets can not be guaranteed since annotations of different\nclients contain complicated label noise of varying degrees, which causes the\nperformance degradation. There have been some early attempts to tackle noisy\nlabels in FL. However, there exists a lack of benchmark studies on\ncomprehensively evaluating their practical performance under unified settings.\nTo this end, we propose the first benchmark study FNBench to provide an\nexperimental investigation which considers three diverse label noise patterns\ncovering synthetic label noise, imperfect human-annotation errors and\nsystematic errors. Our evaluation incorporates eighteen state-of-the-art\nmethods over five image recognition datasets and one text classification\ndataset. Meanwhile, we provide observations to understand why noisy labels\nimpair FL, and additionally exploit a representation-aware regularization\nmethod to enhance the robustness of existing methods against noisy labels based\non our observations. Finally, we discuss the limitations of this work and\npropose three-fold future directions. To facilitate related communities, our\nsource code is open-sourced at https://github.com/Sprinter1999/FNBench."}
{"id": "2505.07299", "pdf": "https://arxiv.org/pdf/2505.07299", "abs": "https://arxiv.org/abs/2505.07299", "authors": ["André Artelt", "Stelios G. Vrachimis", "Demetrios G. Eliades", "Ulrike Kuhl", "Barbara Hammer", "Marios M. Polycarpou"], "title": "Interpretable Event Diagnosis in Water Distribution Networks", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "The increasing penetration of information and communication technologies in\nthe design, monitoring, and control of water systems enables the use of\nalgorithms for detecting and identifying unanticipated events (such as leakages\nor water contamination) using sensor measurements. However, data-driven\nmethodologies do not always give accurate results and are often not trusted by\noperators, who may prefer to use their engineering judgment and experience to\ndeal with such events.\n  In this work, we propose a framework for interpretable event diagnosis -- an\napproach that assists the operators in associating the results of algorithmic\nevent diagnosis methodologies with their own intuition and experience. This is\nachieved by providing contrasting (i.e., counterfactual) explanations of the\nresults provided by fault diagnosis algorithms; their aim is to improve the\nunderstanding of the algorithm's inner workings by the operators, thus enabling\nthem to take a more informed decision by combining the results with their\npersonal experiences. Specifically, we propose counterfactual event\nfingerprints, a representation of the difference between the current event\ndiagnosis and the closest alternative explanation, which can be presented in a\ngraphical way. The proposed methodology is applied and evaluated on a realistic\nuse case using the L-Town benchmark."}
{"id": "2505.06325", "pdf": "https://arxiv.org/pdf/2505.06325", "abs": "https://arxiv.org/abs/2505.06325", "authors": ["Daniel Geissler", "Lars Krupp", "Vishal Banwari", "David Habusch", "Bo Zhou", "Paul Lukowicz", "Jakob Karolus"], "title": "Human in the Latent Loop (HILL): Interactively Guiding Model Training Through Human Intuition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Latent space representations are critical for understanding and improving the\nbehavior of machine learning models, yet they often remain obscure and\nintricate. Understanding and exploring the latent space has the potential to\ncontribute valuable human intuition and expertise about respective domains. In\nthis work, we present HILL, an interactive framework allowing users to\nincorporate human intuition into the model training by interactively reshaping\nlatent space representations. The modifications are infused into the model\ntraining loop via a novel approach inspired by knowledge distillation, treating\nthe user's modifications as a teacher to guide the model in reshaping its\nintrinsic latent representation. The process allows the model to converge more\neffectively and overcome inefficiencies, as well as provide beneficial insights\nto the user. We evaluated HILL in a user study tasking participants to train an\noptimal model, closely observing the employed strategies. The results\ndemonstrated that human-guided latent space modifications enhance model\nperformance while maintaining generalization, yet also revealing the risks of\nincluding user biases. Our work introduces a novel human-AI interaction\nparadigm that infuses human intuition into model training and critically\nexamines the impact of human intervention on training strategies and potential\nbiases."}
{"id": "2505.07313", "pdf": "https://arxiv.org/pdf/2505.07313", "abs": "https://arxiv.org/abs/2505.07313", "authors": ["Baixuan Xu", "Chunyang Li", "Weiqi Wang", "Wei Fan", "Tianshi Zheng", "Haochen Shi", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "title": "Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Designing effective collaboration structure for multi-agent LLM systems to\nenhance collective reasoning is crucial yet remains under-explored. In this\npaper, we systematically investigate how collaborative reasoning performance is\naffected by three key design dimensions: (1) Expertise-Domain Alignment, (2)\nCollaboration Paradigm (structured workflow vs. diversity-driven integration),\nand (3) System Scale. Our findings reveal that expertise alignment benefits are\nhighly domain-contingent, proving most effective for contextual reasoning\ntasks. Furthermore, collaboration focused on integrating diverse knowledge\nconsistently outperforms rigid task decomposition. Finally, we empirically\nexplore the impact of scaling the multi-agent system with expertise\nspecialization and study the computational trade off, highlighting the need for\nmore efficient communication protocol design. This work provides concrete\nguidelines for configuring specialized multi-agent system and identifies\ncritical architectural trade-offs and bottlenecks for scalable multi-agent\nreasoning. The code will be made available upon acceptance."}
{"id": "2505.06694", "pdf": "https://arxiv.org/pdf/2505.06694", "abs": "https://arxiv.org/abs/2505.06694", "authors": ["XiaoTong Gu", "Shengyu Tang", "Yiming Cao", "Changdong Yu"], "title": "Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Underwater object detection using sonar imagery has become a critical and\nrapidly evolving research domain within marine technology. However, sonar\nimages are characterized by lower resolution and sparser features compared to\noptical images, which seriously degrades the performance of object detection.To\naddress these challenges, we specifically propose a Detection Transformer\n(DETR) architecture optimized with a Neural Architecture Search (NAS) approach\ncalled NAS-DETR for object detection in sonar images. First, an improved\nZero-shot Neural Architecture Search (NAS) method based on the maximum entropy\nprinciple is proposed to identify a real-time, high-representational-capacity\nCNN-Transformer backbone for sonar image detection. This method enables the\nefficient discovery of high-performance network architectures with low\ncomputational and time overhead. Subsequently, the backbone is combined with a\nFeature Pyramid Network (FPN) and a deformable attention-based Transformer\ndecoder to construct a complete network architecture. This architecture\nintegrates various advanced components and training schemes to enhance overall\nperformance. Extensive experiments demonstrate that this architecture achieves\nstate-of-the-art performance on two Representative datasets, while maintaining\nminimal overhead in real-time efficiency and computational complexity.\nFurthermore, correlation analysis between the key parameters and differential\nentropy-based fitness function is performed to enhance the interpretability of\nthe proposed framework. To the best of our knowledge, this is the first work in\nthe field of sonar object detection to integrate the DETR architecture with a\nNAS search mechanism."}
{"id": "2505.07315", "pdf": "https://arxiv.org/pdf/2505.07315", "abs": "https://arxiv.org/abs/2505.07315", "authors": ["Zexiao Wang", "Yankai Wang", "Xiaoqiang Liao", "Xinguo Ming", "Weiming Shen"], "title": "FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Due to the scarcity of industrial data, individual equipment users,\nparticularly start-ups, struggle to independently train a comprehensive fault\ndiagnosis model; federated learning enables collaborative training while\nensuring data privacy, making it an ideal solution. However, the diversity of\nworking conditions leads to variations in fault modes, resulting in\ninconsistent label spaces across different clients. In federated diagnostic\nscenarios, label space inconsistency leads to local models focus on\nclient-specific fault modes and causes local models from different clients to\nmap different failure modes to similar feature representations, which weakens\nthe aggregated global model's generalization. To tackle this issue, this\narticle proposed a federated cross-domain diagnostic framework termed Federated\nInvariant Features Learning (FedIFL). In intra-client training, prototype\ncontrastive learning mitigates intra-client domain shifts, subsequently,\nfeature generating ensures local models can access distributions of other\nclients in a privacy-friendly manner. Besides, in cross-client training, a\nfeature disentanglement mechanism is introduced to mitigate cross-client domain\nshifts, specifically, an instance-level federated instance consistency loss is\ndesigned to ensure the instance-level consistency of invariant features between\ndifferent clients, furthermore, a federated instance personalization loss and\nan orthogonal loss are constructed to distinguish specific features that from\nthe invariant features. Eventually, the aggregated model achieves promising\ngeneralization among global label spaces, enabling accurate fault diagnosis for\ntarget clients' Motor Driven Systems (MDSs) with inconsistent label spaces.\nExperiments on real-world MDSs validate the effectiveness and superiority of\nFedIFL in federated cross-domain diagnosis with inconsistent fault modes."}
{"id": "2505.06330", "pdf": "https://arxiv.org/pdf/2505.06330", "abs": "https://arxiv.org/abs/2505.06330", "authors": ["Junyu Xue", "Xudong Wang", "Xiaoling He", "Shicheng Liu", "Yi Wang", "Guoming Tang"], "title": "Prompting Large Language Models for Training-Free Non-Intrusive Load Monitoring", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Non-intrusive Load Monitoring (NILM) aims to disaggregate aggregate household\nelectricity consumption into individual appliance usage, enabling more\neffective energy management. While deep learning has advanced NILM, it remains\nlimited by its dependence on labeled data, restricted generalization, and lack\nof interpretability. In this paper, we introduce the first prompt-based NILM\nframework that leverages Large Language Models (LLMs) with in-context learning.\nWe design and evaluate prompt strategies that integrate appliance features,\ntimestamps and contextual information, as well as representative time-series\nexamples, using the REDD dataset. With optimized prompts, LLMs achieve\ncompetitive state detection accuracy, reaching an average F1-score of 0.676 on\nunseen households, and demonstrate robust generalization without the need for\nfine-tuning. LLMs also enhance interpretability by providing clear,\nhuman-readable explanations for their predictions. Our results show that LLMs\ncan reduce data requirements, improve adaptability, and provide transparent\nenergy disaggregation in NILM applications."}
{"id": "2505.07345", "pdf": "https://arxiv.org/pdf/2505.07345", "abs": "https://arxiv.org/abs/2505.07345", "authors": ["Ohjoon Kwon", "Changsu Lee", "Jihye Back", "Lim Sun Suk", "Inho Kang", "Donghyeon Jeon"], "title": "QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have been widely used for relevance assessment\nin information retrieval. However, our study demonstrates that combining two\ndistinct small language models (SLMs) with different architectures can\noutperform LLMs in this task. Our approach -- QUPID -- integrates a generative\nSLM with an embedding-based SLM, achieving higher relevance judgment accuracy\nwhile reducing computational costs compared to state-of-the-art LLM solutions.\nThis computational efficiency makes QUPID highly scalable for real-world search\nsystems processing millions of queries daily. In experiments across diverse\ndocument types, our method demonstrated consistent performance improvements\n(Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x\nfaster inference times. Furthermore, when integrated into production search\npipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how\narchitectural diversity in model combinations can significantly enhance both\nsearch relevance and operational efficiency in information retrieval systems."}
{"id": "2505.06710", "pdf": "https://arxiv.org/pdf/2505.06710", "abs": "https://arxiv.org/abs/2505.06710", "authors": ["Yicheng Song", "Tiancheng Lin", "Die Peng", "Su Yang", "Yi Xu"], "title": "SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images", "categories": ["cs.CV"], "comment": null, "summary": "Various multi-instance learning (MIL) based approaches have been developed\nand successfully applied to whole-slide pathological images (WSI). Existing MIL\nmethods emphasize the importance of feature aggregators, but largely neglect\nthe instance-level representation learning. They assume that the availability\nof a pre-trained feature extractor can be directly utilized or fine-tuned,\nwhich is not always the case. This paper proposes to pre-train feature\nextractor for MIL via a weakly-supervised scheme, i.e., propagating the weak\nbag-level labels to the corresponding instances for supervised learning. To\nlearn effective features for MIL, we further delve into several key components,\nincluding strong data augmentation, a non-linear prediction head and the robust\nloss function. We conduct experiments on common large-scale WSI datasets and\nfind it achieves better performance than other pre-training schemes (e.g.,\nImageNet pre-training and self-supervised learning) in different downstream\ntasks. We further show the compatibility and scalability of the proposed scheme\nby deploying it in fine-tuning the pathological-specific models and\npre-training on merged multiple datasets. To our knowledge, this is the first\nwork focusing on the representation learning for MIL."}
{"id": "2505.07374", "pdf": "https://arxiv.org/pdf/2505.07374", "abs": "https://arxiv.org/abs/2505.07374", "authors": ["Zhiye Xie", "Enmei Tu", "Xianping Fu", "Guoliang Yuan", "Yi Han"], "title": "AIS Data-Driven Maritime Monitoring Based on Transformer: A Comprehensive Review", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "With the increasing demands for safety, efficiency, and sustainability in\nglobal shipping, Automatic Identification System (AIS) data plays an\nincreasingly important role in maritime monitoring. AIS data contains\nspatial-temporal variation patterns of vessels that hold significant research\nvalue in the marine domain. However, due to its massive scale, the full\npotential of AIS data has long remained untapped. With its powerful sequence\nmodeling capabilities, particularly its ability to capture long-range\ndependencies and complex temporal dynamics, the Transformer model has emerged\nas an effective tool for processing AIS data. Therefore, this paper reviews the\nresearch on Transformer-based AIS data-driven maritime monitoring, providing a\ncomprehensive overview of the current applications of Transformer models in the\nmarine field. The focus is on Transformer-based trajectory prediction methods,\nbehavior detection, and prediction techniques. Additionally, this paper\ncollects and organizes publicly available AIS datasets from the reviewed\npapers, performing data filtering, cleaning, and statistical analysis. The\nstatistical results reveal the operational characteristics of different vessel\ntypes, providing data support for further research on maritime monitoring\ntasks. Finally, we offer valuable suggestions for future research, identifying\ntwo promising research directions. Datasets are available at\nhttps://github.com/eyesofworld/Maritime-Monitoring."}
{"id": "2505.06331", "pdf": "https://arxiv.org/pdf/2505.06331", "abs": "https://arxiv.org/abs/2505.06331", "authors": ["Feilong Jiang", "Xiaonan Hou", "Jianqiao Ye", "Min Xia"], "title": "Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) are a class of deep learning models\ndesigned to solve partial differential equations by incorporating physical laws\ndirectly into the loss function. However, the internal covariate shift, which\nhas been largely overlooked, hinders the effective utilization of neural\nnetwork capacity in PINNs. To this end, we propose Mask-PINNs, a novel\narchitecture designed to address this issue in PINNs. Unlike traditional\nnormalization methods such as BatchNorm or LayerNorm, we introduce a learnable,\nnonlinear mask function that constrains the feature distributions without\nviolating underlying physics. The experimental results show that the proposed\nmethod significantly improves feature distribution stability, accuracy, and\nrobustness across various activation functions and PDE benchmarks. Furthermore,\nit enables the stable and efficient training of wider networks a capability\nthat has been largely overlooked in PINNs."}
{"id": "2505.07409", "pdf": "https://arxiv.org/pdf/2505.07409", "abs": "https://arxiv.org/abs/2505.07409", "authors": ["Tim Wittenborg", "Constantin Sebastian Tremel", "Markus Stocker", "Sören Auer"], "title": "Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles", "categories": ["cs.CL"], "comment": "4 pages, 4 figures, submitted to ACM Web Conference 2025", "summary": "Democratic societies need reliable information. Misinformation in popular\nmedia such as news articles or videos threatens to impair civic discourse.\nCitizens are, unfortunately, not equipped to verify this content flood consumed\ndaily at increasing rates. This work aims to semi-automatically quantify\nscientific accuracy of online media. By semantifying media of unknown veracity,\ntheir statements can be compared against equally processed trusted sources. We\nimplemented a workflow using LLM-based statement extraction and knowledge graph\nanalysis. Our neurosymbolic system was able to evidently streamline\nstate-of-the-art veracity quantification. Evaluated via expert interviews and a\nuser survey, the tool provides a beneficial veracity indication. This\nindicator, however, is unable to annotate public media at the required\ngranularity and scale. Further work towards a FAIR (Findable, Accessible,\nInteroperable, Reusable) ground truth and complementary metrics are required to\nscientifically support civic discourse."}
{"id": "2505.06745", "pdf": "https://arxiv.org/pdf/2505.06745", "abs": "https://arxiv.org/abs/2505.06745", "authors": ["Parth Padalkar", "Gopal Gupta"], "title": "Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent neuro-symbolic approaches have successfully extracted symbolic\nrule-sets from CNN-based models to enhance interpretability. However, applying\nsimilar techniques to Vision Transformers (ViTs) remains challenging due to\ntheir lack of modular concept detectors and reliance on global self-attention\nmechanisms. We propose a framework for symbolic rule extraction from ViTs by\nintroducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This\nlinear layer operates on attention-weighted patch representations and learns a\ndisentangled, binarized representation in which individual neurons activate for\nhigh-level visual concepts. To encourage interpretability, we apply a\ncombination of L1 sparsity, entropy minimization, and supervised contrastive\nloss. These binarized concept activations are used as input to the FOLD-SE-M\nalgorithm, which generates a rule-set in the form of logic programs. Our method\nachieves a 5.14% better classification accuracy than the standard ViT while\nenabling symbolic reasoning. Crucially, the extracted rule-set is not merely\npost-hoc but acts as a logic-based decision layer that operates directly on the\nsparse concept representations. The resulting programs are concise and\nsemantically meaningful. This work is the first to extract executable logic\nprograms from ViTs using sparse symbolic representations. It bridges the gap\nbetween transformer-based vision models and symbolic logic programming,\nproviding a step forward in interpretable and verifiable neuro-symbolic AI."}
{"id": "2505.07453", "pdf": "https://arxiv.org/pdf/2505.07453", "abs": "https://arxiv.org/abs/2505.07453", "authors": ["Cornelius Wolff", "Madelon Hulsebos"], "title": "How well do LLMs reason over tabular data, really?", "categories": ["cs.AI"], "comment": "10 pages, 4 figures", "summary": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs."}
{"id": "2505.06333", "pdf": "https://arxiv.org/pdf/2505.06333", "abs": "https://arxiv.org/abs/2505.06333", "authors": ["Chathurangi Shyalika", "Renjith Prasad", "Fadi El Kalach", "Revathy Venkataramanan", "Ramtin Zand", "Ramy Harik", "Amit Sheth"], "title": "NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 7 figures, 2 tables, IJCAI 2025 (International Joint\n  Conferences on Artificial Intelligence) Special Track on AI4Tech: AI Enabling\n  Critical Technologies", "summary": "In modern assembly pipelines, identifying anomalies is crucial in ensuring\nproduct quality and operational efficiency. Conventional single-modality\nmethods fail to capture the intricate relationships required for precise\nanomaly prediction in complex predictive environments with abundant data and\nmultiple modalities. This paper proposes a neurosymbolic AI and fusion-based\napproach for multimodal anomaly prediction in assembly pipelines. We introduce\na time series and image-based fusion model that leverages decision-level fusion\ntechniques. Our research builds upon three primary novel approaches in\nmultimodal learning: time series and image-based decision-level fusion\nmodeling, transfer learning for fusion, and knowledge-infused learning. We\nevaluate the novel method using our derived and publicly available multimodal\ndataset and conduct comprehensive ablation studies to assess the impact of our\npreprocessing techniques and fusion model compared to traditional baselines.\nThe results demonstrate that a neurosymbolic AI-based fusion approach that uses\ntransfer learning can effectively harness the complementary strengths of time\nseries and image data, offering a robust and interpretable approach for anomaly\nprediction in assembly pipelines with enhanced performance. \\noindent The\ndatasets, codes to reproduce the results, supplementary materials, and demo are\navailable at https://github.com/ChathurangiShyalika/NSF-MAP."}
{"id": "2505.07416", "pdf": "https://arxiv.org/pdf/2505.07416", "abs": "https://arxiv.org/abs/2505.07416", "authors": ["Truc Mai-Thanh Nguyen", "Dat Minh Nguyen", "Son T. Luu", "Kiet Van Nguyen"], "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation", "categories": ["cs.CL"], "comment": "Accepted at NLDB 2025", "summary": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in\nrecommender systems, particularly in E-commerce platforms. Determining the\nhelpfulness of user-generated reviews enhances user experience and improves\nconsumer decision-making. However, existing datasets focus predominantly on\nEnglish and Indonesian, resulting in a lack of linguistic diversity, especially\nfor low-resource languages such as Vietnamese. In this paper, we introduce\nViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale\nbenchmark dataset for MRHP task in Vietnamese. This dataset covers four\ndomains, including 2K products with 46K reviews. Meanwhile, a large-scale\ndataset requires considerable time and cost. To optimize the annotation\nprocess, we leverage AI to assist annotators in constructing the ViMRHP\ndataset. With AI assistance, annotation time is reduced (90 to 120 seconds per\ntask down to 20 to 40 seconds per task) while maintaining data quality and\nlowering overall costs by approximately 65%. However, AI-generated annotations\nstill have limitations in complex annotation tasks, which we further examine\nthrough a detailed performance analysis. In our experiment on ViMRHP, we\nevaluate baseline models on human-verified and AI-generated annotations to\nassess their quality differences. The ViMRHP dataset is publicly available at\nhttps://github.com/trng28/ViMRHP"}
{"id": "2505.06796", "pdf": "https://arxiv.org/pdf/2505.06796", "abs": "https://arxiv.org/abs/2505.06796", "authors": ["Ye Zhu", "Yunan Wang", "Zitong Yu"], "title": "Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Multimodal news contains a wealth of information and is easily affected by\ndeepfake modeling attacks. To combat the latest image and text generation\nmethods, we present a new Multimodal Fake News Detection dataset (MFND)\ncontaining 11 manipulated types, designed to detect and localize highly\nauthentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning\n(SDML) model for fake news, which fully uses unimodal and mutual modal features\nto mine the intrinsic semantics of news. Under shallow inference, we propose\nthe momentum distillation-based light punishment contrastive learning for\nfine-grained uniform spatial image and text semantic alignment, and an adaptive\ncross-modal fusion module to enhance mutual modal features. Under deep\ninference, we design a two-branch framework to augment the image and text\nunimodal features, respectively merging with mutual modalities features, for\nfour predictions via dedicated detection and localization projections.\nExperiments on both mainstream and our proposed datasets demonstrate the\nsuperiority of the model. Codes and dataset are released at\nhttps://github.com/yunan-wang33/sdml."}
{"id": "2505.07460", "pdf": "https://arxiv.org/pdf/2505.07460", "abs": "https://arxiv.org/abs/2505.07460", "authors": ["Yi Chen", "JiaHao Zhao", "HaoHao Han"], "title": "A Survey on Collaborative Mechanisms Between Large and Small Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) deliver powerful AI capabilities but face\ndeployment challenges due to high resource costs and latency, whereas Small\nLanguage Models (SLMs) offer efficiency and deployability at the cost of\nreduced performance. Collaboration between LLMs and SLMs emerges as a crucial\nparadigm to synergistically balance these trade-offs, enabling advanced AI\napplications, especially on resource-constrained edge devices. This survey\nprovides a comprehensive overview of LLM-SLM collaboration, detailing various\ninteraction mechanisms (pipeline, routing, auxiliary, distillation, fusion),\nkey enabling technologies, and diverse application scenarios driven by\non-device needs like low latency, privacy, personalization, and offline\noperation. While highlighting the significant potential for creating more\nefficient, adaptable, and accessible AI, we also discuss persistent challenges\nincluding system overhead, inter-model consistency, robust task allocation,\nevaluation complexity, and security/privacy concerns. Future directions point\ntowards more intelligent adaptive frameworks, deeper model fusion, and\nexpansion into multimodal and embodied AI, positioning LLM-SLM collaboration as\na key driver for the next generation of practical and ubiquitous artificial\nintelligence."}
{"id": "2505.06335", "pdf": "https://arxiv.org/pdf/2505.06335", "abs": "https://arxiv.org/abs/2505.06335", "authors": ["Jinsheng Yuan", "Yuhang Hao", "Weisi Guo", "Yun Wu", "Chongyan Gu"], "title": "Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) has the potential for simultaneous global learning\namongst a large number of parallel agents, enabling emerging AI such as LLMs to\nbe trained across demographically diverse data. Central to this being efficient\nis the ability for FL to perform sparse gradient updates and remote direct\nmemory access at the central server. Most of the research in FL security\nfocuses on protecting data privacy at the edge client or in the communication\nchannels between the client and server. Client-facing attacks on the server are\nless well investigated as the assumption is that a large collective of clients\noffer resilience.\n  Here, we show that by attacking certain clients that lead to a high frequency\nrepetitive memory update in the server, we can remote initiate a rowhammer\nattack on the server memory. For the first time, we do not need backdoor access\nto the server, and a reinforcement learning (RL) attacker can learn how to\nmaximize server repetitive memory updates by manipulating the client's sensor\nobservation. The consequence of the remote rowhammer attack is that we are able\nto achieve bit flips, which can corrupt the server memory. We demonstrate the\nfeasibility of our attack using a large-scale FL automatic speech recognition\n(ASR) systems with sparse updates, our adversarial attacking agent can achieve\naround 70\\% repeated update rate (RUR) in the targeted server model,\neffectively inducing bit flips on server DRAM. The security implications are\nthat can cause disruptions to learning or may inadvertently cause elevated\nprivilege. This paves the way for further research on practical mitigation\nstrategies in FL and hardware design."}
{"id": "2505.07430", "pdf": "https://arxiv.org/pdf/2505.07430", "abs": "https://arxiv.org/abs/2505.07430", "authors": ["Mostafa Mohaimen Akand Faisal", "Rabeya Amin Jhuma"], "title": "Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The emergence of global health crises, such as COVID-19 and Monkeypox (mpox),\nhas underscored the importance of understanding public sentiment to inform\neffective public health strategies. This study conducts a comparative sentiment\nanalysis of public perceptions surrounding COVID-19 and mpox by leveraging\nextensive datasets of 147,475 and 106,638 tweets, respectively. Advanced\nmachine learning models, including Logistic Regression, Naive Bayes, RoBERTa,\nDistilRoBERTa and XLNet, were applied to perform sentiment classification, with\nresults indicating key trends in public emotion and discourse. The analysis\nhighlights significant differences in public sentiment driven by disease\ncharacteristics, media representation, and pandemic fatigue. Through the lens\nof sentiment polarity and thematic trends, this study offers valuable insights\ninto tailoring public health messaging, mitigating misinformation, and\nfostering trust during concurrent health crises. The findings contribute to\nadvancing sentiment analysis applications in public health informatics, setting\nthe groundwork for enhanced real-time monitoring and multilingual analysis in\nfuture research."}
{"id": "2505.06814", "pdf": "https://arxiv.org/pdf/2505.06814", "abs": "https://arxiv.org/abs/2505.06814", "authors": ["Bin Li", "Shenxi Liu", "Yixuan Weng", "Yue Du", "Yuhang Tian", "Shoujun Zhou"], "title": "Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "12 pages, 5 figures, 4 tables", "summary": "Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the\n2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been\nintroduced to further advance research in multi-modal, multilingual, and\nmulti-hop medical instructional question answering (M4IVQA) systems, with a\nspecific focus on medical instructional videos. The M4IVQA challenge focuses on\nevaluating models that integrate information from medical instructional videos,\nunderstand multiple languages, and answer multi-hop questions requiring\nreasoning over various modalities. This task consists of three tracks:\nmulti-modal, multilingual, and multi-hop Temporal Answer Grounding in Single\nVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus\nRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer\nGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to\ndevelop algorithms capable of processing both video and text data,\nunderstanding multilingual queries, and providing relevant answers to multi-hop\nmedical questions. We believe the newly introduced M4IVQA challenge will drive\ninnovations in multimodal reasoning systems for healthcare scenarios,\nultimately contributing to smarter emergency response systems and more\neffective medical education platforms in multilingual communities. Our official\nwebsite is https://cmivqa.github.io/"}
{"id": "2505.07473", "pdf": "https://arxiv.org/pdf/2505.07473", "abs": "https://arxiv.org/abs/2505.07473", "authors": ["Kai Xu", "YiWei Mao", "XinYi Guan", "ZiLong Feng"], "title": "Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks", "categories": ["cs.AI"], "comment": "28 pages, 15 figures", "summary": "The application of large language models (LLMs) in the field of coding is\nevolving rapidly: from code assistants, to autonomous coding agents, and then\nto generating complete projects through natural language. Early LLM code\nbenchmarks primarily focused on code generation accuracy, but these benchmarks\nhave gradually become saturated. Benchmark saturation weakens their guiding\nrole for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%.\nAmong various attempts to address benchmark saturation, approaches based on\nsoftware engineering have stood out, but the saturation of existing software\nengineering benchmarks is rapidly increasing. To address this, we propose a new\nbenchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks\nwith sequential dependencies. The tasks implement project features in sequence,\nsimulating real-world human development workflows. When designing Web-Bench, we\naim to cover the foundational elements of Web development: Web Standards and\nWeb Frameworks. Given the scale and complexity of these projects, which were\ndesigned by engineers with 5 to 10 years of experience, each presents a\nsignificant challenge. On average, a single project takes 4 to 8 hours for a\nsenior engineer to complete. On our given benchmark agent (Web-Agent), SOTA\n(Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better)\nthan SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss\nthat in any development field, Standards and Frameworks represent foundational\nknowledge and efficiency tools, respectively, and LLMs require optimization\ntailored to them."}
{"id": "2505.06351", "pdf": "https://arxiv.org/pdf/2505.06351", "abs": "https://arxiv.org/abs/2505.06351", "authors": ["Willem Diepeveen", "Jon Schwenk", "Andrea Bertozzi"], "title": "Latent Diffeomorphic Dynamic Mode Decomposition", "categories": ["cs.LG", "math.DS"], "comment": null, "summary": "We present Latent Diffeomorphic Dynamic Mode Decomposition (LDDMD), a new\ndata reduction approach for the analysis of non-linear systems that combines\nthe interpretability of Dynamic Mode Decomposition (DMD) with the predictive\npower of Recurrent Neural Networks (RNNs). Notably, LDDMD maintains simplicity,\nwhich enhances interpretability, while effectively modeling and learning\ncomplex non-linear systems with memory, enabling accurate predictions. This is\nexemplified by its successful application in streamflow prediction."}
{"id": "2505.07440", "pdf": "https://arxiv.org/pdf/2505.07440", "abs": "https://arxiv.org/abs/2505.07440", "authors": ["Rituraj Singh", "Sachin Pawar", "Girish Palshikar"], "title": "Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Commonsense knowledge bases (KB) are a source of specialized knowledge that\nis widely used to improve machine learning applications. However, even for a\nlarge KB such as ConceptNet, capturing explicit knowledge from each industry\ndomain is challenging. For example, only a few samples of general {\\em tasks}\nperformed by various industries are available in ConceptNet. Here, a task is a\nwell-defined knowledge-based volitional action to achieve a particular goal. In\nthis paper, we aim to fill this gap and present a weakly-supervised framework\nto augment commonsense KB with tasks carried out by various industry groups\n(IG). We attempt to {\\em match} each task with one or more suitable IGs by\ntraining a neural model to learn task-IG affinity and apply clustering to\nselect the top-k tasks per IG. We extract a total of 2339 triples of the form\n$\\langle IG, is~capable~of, task \\rangle$ from two publicly available news\ndatasets for 24 IGs with the precision of 0.86. This validates the reliability\nof the extracted task-IG pairs that can be directly added to existing KBs."}
{"id": "2505.06825", "pdf": "https://arxiv.org/pdf/2505.06825", "abs": "https://arxiv.org/abs/2505.06825", "authors": ["Thien Nhan Vo"], "title": "Active Learning for Multi-class Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "A principle bottleneck in image classification is the large number of\ntraining examples needed to train a classifier. Using active learning, we can\nreduce the number of training examples to teach a CNN classifier by\nstrategically selecting examples. Assigning values to image examples using\ndifferent uncertainty metrics allows the model to identify and select\nhigh-value examples in a smaller training set size. We demonstrate results for\ndigit recognition and fruit classification on the MNIST and Fruits360 data\nsets. We formally compare results for four different uncertainty metrics.\nFinally, we observe active learning is also effective on simpler (binary)\nclassification tasks, but marked improvement from random sampling is more\nevident on more difficult tasks. We show active learning is a viable algorithm\nfor image classification problems."}
{"id": "2505.07509", "pdf": "https://arxiv.org/pdf/2505.07509", "abs": "https://arxiv.org/abs/2505.07509", "authors": ["Feng Ding", "Tingting Wang", "Yupeng Gao", "Shuo Yu", "Jing Ren", "Feng Xia"], "title": "HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs", "categories": ["cs.AI"], "comment": null, "summary": "Outdated facts in temporal knowledge graphs (TKGs) result from exceeding the\nexpiration date of facts, which negatively impact reasoning performance on\nTKGs. However, existing reasoning methods primarily focus on positive\nimportance of historical facts, neglecting adverse effects of outdated facts.\nBesides, training on these outdated facts yields extra computational cost. To\naddress these challenges, we propose an outdated fact filtering framework named\nHALO, which quantifies the temporal validity of historical facts by exploring\nthe half-life theory to filter outdated facts in TKGs. HALO consists of three\nmodules: the temporal fact attention module, the dynamic relation-aware encoder\nmodule, and the outdated fact filtering module. Firstly, the temporal fact\nattention module captures the evolution of historical facts over time to\nidentify relevant facts. Secondly, the dynamic relation-aware encoder module is\ndesigned for efficiently predicting the half life of each fact. Finally, we\nconstruct a time decay function based on the half-life theory to quantify the\ntemporal validity of facts and filter outdated facts. Experimental results show\nthat HALO outperforms the state-of-the-art TKG reasoning methods on three\npublic datasets, demonstrating its effectiveness in detecting and filtering\noutdated facts (Codes are available at\nhttps://github.com/yushuowiki/K-Half/tree/main )."}
{"id": "2505.06367", "pdf": "https://arxiv.org/pdf/2505.06367", "abs": "https://arxiv.org/abs/2505.06367", "authors": ["Everest Yang", "Ria Vasishtha", "Luqman K. Dad", "Lisa A. Kachnic", "Andrew Hope", "Eric Wang", "Xiao Wu", "Yading Yuan", "David J. Brenner", "Igor Shuryak"], "title": "CAST: Time-Varying Treatment Effects with Application to Chemotherapy and Radiotherapy on Head and Neck Squamous Cell Carcinoma", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Causal machine learning (CML) enables individualized estimation of treatment\neffects, offering critical advantages over traditional correlation-based\nmethods. However, existing approaches for medical survival data with censoring\nsuch as causal survival forests estimate effects at fixed time points, limiting\ntheir ability to capture dynamic changes over time. We introduce Causal\nAnalysis for Survival Trajectories (CAST), a novel framework that models\ntreatment effects as continuous functions of time following treatment. By\ncombining parametric and non-parametric methods, CAST overcomes the limitations\nof discrete time-point analysis to estimate continuous effect trajectories.\nUsing the RADCURE dataset [1] of 2,651 patients with head and neck squamous\ncell carcinoma (HNSCC) as a clinically relevant example, CAST models how\nchemotherapy and radiotherapy effects evolve over time at the population and\nindividual levels. By capturing the temporal dynamics of treatment response,\nCAST reveals how treatment effects rise, peak, and decline over the follow-up\nperiod, helping clinicians determine when and for whom treatment benefits are\nmaximized. This framework advances the application of CML to personalized care\nin HNSCC and other life-threatening medical conditions. Source code/data\navailable at: https://github.com/CAST-FW/HNSCC"}
{"id": "2505.07495", "pdf": "https://arxiv.org/pdf/2505.07495", "abs": "https://arxiv.org/abs/2505.07495", "authors": ["Isabelle van der Vegt", "Bennett Kleinberg", "Marilu Miotto", "Jonas Festor"], "title": "Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces and evaluates three translations of the Grievance\nDictionary, a psycholinguistic dictionary for the analysis of violent,\nthreatening or grievance-fuelled texts. Considering the relevance of these\nthemes in languages beyond English, we translated the Grievance Dictionary to\nDutch, German, and Italian. We describe the process of automated translation\nsupplemented by human annotation. Psychometric analyses are performed,\nincluding internal reliability of dictionary categories and correlations with\nthe LIWC dictionary. The Dutch and German translations perform similarly to the\noriginal English version, whereas the Italian dictionary shows low reliability\nfor some categories. Finally, we make suggestions for further validation and\napplication of the dictionary, as well as for future dictionary translations\nfollowing a similar approach."}
{"id": "2505.06831", "pdf": "https://arxiv.org/pdf/2505.06831", "abs": "https://arxiv.org/abs/2505.06831", "authors": ["Miaoyun Zhao", "Qiang Zhang", "Chenrong Li"], "title": "Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification", "categories": ["cs.CV"], "comment": null, "summary": "Achieving group-robust generalization in the presence of spurious\ncorrelations remains a significant challenge, particularly when bias\nannotations are unavailable. Recent studies on Class-Conditional Distribution\nBalancing (CCDB) reveal that spurious correlations often stem from mismatches\nbetween the class-conditional and marginal distributions of bias attributes.\nThey achieve promising results by addressing this issue through simple\ndistribution matching in a bias-agnostic manner. However, CCDB approximates\neach distribution using a single Gaussian, which is overly simplistic and\nrarely holds in real-world applications. To address this limitation, we propose\na novel method called Bias Exploration via Overfitting (BEO), which captures\neach distribution in greater detail by modeling it as a mixture of latent\ngroups. Building on these group-level descriptions, we introduce a fine-grained\nvariant of CCDB, termed FG-CCDB, which performs more precise distribution\nmatching and balancing within each group. Through group-level reweighting,\nFG-CCDB learns sample weights from a global perspective, achieving stronger\nmitigation of spurious correlations without incurring substantial storage or\ncomputational costs. Extensive experiments demonstrate that BEO serves as a\nstrong proxy for ground-truth bias annotations and can be seamlessly integrated\nwith bias-supervised methods. Moreover, when combined with FG-CCDB, our method\nperforms on par with bias-supervised approaches on binary classification tasks\nand significantly outperforms them in highly biased multi-class scenarios."}
{"id": "2505.07531", "pdf": "https://arxiv.org/pdf/2505.07531", "abs": "https://arxiv.org/abs/2505.07531", "authors": ["Khurram Mazher", "Saad Bin Nasir"], "title": "QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads", "categories": ["cs.AI"], "comment": null, "summary": "We present QuantX: a tailored suite of recipes for LLM and VLM quantization.\nIt is capable of quantizing down to 3-bit resolutions with minimal loss in\nperformance. The quantization strategies in QuantX take into account\nhardware-specific constraints to achieve efficient dequantization during\ninference ensuring flexible trade-off between runtime speed, memory requirement\nand model accuracy. Our results demonstrate that QuantX achieves performance\nwithin 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for\nmultiple end user tasks and outperforms recently published state-of-the-art\nquantization techniques. This manuscript provides insights into the LLM\nquantization process that motivated the range of recipes and options that are\nincorporated in QuantX."}
{"id": "2505.06371", "pdf": "https://arxiv.org/pdf/2505.06371", "abs": "https://arxiv.org/abs/2505.06371", "authors": ["Jae-Won Chung", "Jiachen Liu", "Jeff J. Ma", "Ruofan Wu", "Oh Jun Kweon", "Yuxuan Xia", "Zhiyu Wu", "Mosharaf Chowdhury"], "title": "The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Leaderboard: https://ml.energy/leaderboard", "summary": "As the adoption of Generative AI in real-world services grow explosively,\nenergy has emerged as a critical bottleneck resource. However, energy remains a\nmetric that is often overlooked, under-explored, or poorly understood in the\ncontext of building ML systems. We present the ML.ENERGY Benchmark, a benchmark\nsuite and tool for measuring inference energy consumption under realistic\nservice environments, and the corresponding ML.ENERGY Leaderboard, which have\nserved as a valuable resource for those hoping to understand and optimize the\nenergy consumption of their generative AI services. In this paper, we explain\nfour key design principles for benchmarking ML energy we have acquired over\ntime, and then describe how they are implemented in the ML.ENERGY Benchmark. We\nthen highlight results from the latest iteration of the benchmark, including\nenergy measurements of 40 widely used model architectures across 6 different\ntasks, case studies of how ML design choices impact energy consumption, and how\nautomated optimization recommendations can lead to significant (sometimes more\nthan 40%) energy savings without changing what is being computed by the model.\nThe ML.ENERGY Benchmark is open-source and can be easily extended to various\ncustomized models and application scenarios."}
{"id": "2505.07512", "pdf": "https://arxiv.org/pdf/2505.07512", "abs": "https://arxiv.org/abs/2505.07512", "authors": ["Xu Huang", "Weiwen Liu", "Xingshan Zeng", "Yuefeng Huang", "Xinlong Hao", "Yuxian Wang", "Yirong Zeng", "Chuhan Wu", "Yasheng Wang", "Ruiming Tang", "Defu Lian"], "title": "ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The tool-using capability of large language models (LLMs) enables them to\naccess up-to-date external information and handle complex tasks. Current\napproaches to enhancing this capability primarily rely on distilling advanced\nmodels by data synthesis. However, this method incurs significant costs\nassociated with advanced model usage and often results in data compatibility\nissues, led by the high discrepancy in the knowledge scope between the advanced\nmodel and the target model. To address these challenges, we propose\nToolACE-DEV, a self-improving framework for tool learning. First, we decompose\nthe tool-learning objective into sub-tasks that enhance basic tool-making and\ntool-using abilities. Then, we introduce a self-evolving paradigm that allows\nlightweight models to self-improve, reducing reliance on advanced LLMs.\nExtensive experiments validate the effectiveness of our approach across models\nof varying scales and architectures."}
{"id": "2505.06840", "pdf": "https://arxiv.org/pdf/2505.06840", "abs": "https://arxiv.org/abs/2505.06840", "authors": ["Yixin Chen", "Shuai Zhang", "Boran Han", "Bernie Wang"], "title": "Visual Instruction Tuning with Chain of Region-of-Interest", "categories": ["cs.CV"], "comment": "N/A", "summary": "High-resolution (HR) images are pivotal for enhancing the recognition and\nunderstanding capabilities of multimodal large language models (MLLMs).\nHowever, directly increasing image resolution can significantly escalate\ncomputational demands. In this study, we propose a method called Chain of\nRegion-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating\nthe computational burden associated with high-resolution images for MLLMs.\nDrawing inspiration from the selective nature of the human visual system, we\nrecognize that not all regions within high-resolution images carry equal\nimportance. CoRoI seeks to identify and prioritize the most informative\nregions, thereby enhancing multimodal visual comprehension and recognition\nwhile circumventing the need for processing lengthy HR image tokens. Through\nextensive experiments on 11 benchmarks, we validate the efficacy of CoRoI\nacross varying sizes, ranging from 7B to 34B in parameters. Our models\nconsistently demonstrate superior performance across diverse multimodal\nbenchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all\nbenchmarks and our finetuned 34B model surpasses proprietary methods like\nGemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB,\nSEED-I, and MME."}
{"id": "2505.07581", "pdf": "https://arxiv.org/pdf/2505.07581", "abs": "https://arxiv.org/abs/2505.07581", "authors": ["Lei Wang", "Heyang Gao", "Xiaohe Bo", "Xu Chen", "Ji-Rong Wen"], "title": "YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Leveraging large language model (LLM) based agents to simulate human social\nbehaviors has recently gained significant attention. In this paper, we\nintroduce a novel social simulator called YuLan-OneSim. Compared to previous\nworks, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free\nscenario construction: Users can simply describe and refine their simulation\nscenarios through natural language interactions with our simulator. All\nsimulation code is automatically generated, significantly reducing the need for\nprogramming expertise. (2) Comprehensive default scenarios: We implement 50\ndefault simulation scenarios spanning 8 domains, including economics,\nsociology, politics, psychology, organization, demographics, law, and\ncommunication, broadening access for a diverse range of social researchers. (3)\nEvolvable simulation: Our simulator is capable of receiving external feedback\nand automatically fine-tuning the backbone LLMs, significantly enhancing the\nsimulation quality. (4) Large-scale simulation: By developing a fully\nresponsive agent framework and a distributed simulation architecture, our\nsimulator can handle up to 100,000 agents, ensuring more stable and reliable\nsimulation results. (5) AI social researcher: Leveraging the above features, we\ndevelop an AI social researcher. Users only need to propose a research topic,\nand the AI researcher will automatically analyze the input, construct\nsimulation environments, summarize results, generate technical reports, review\nand refine the reports--completing the social science research loop. To\ndemonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate\nthe quality of the automatically generated scenarios, the reliability,\nefficiency, and scalability of the simulation process, as well as the\nperformance of the AI social researcher."}
{"id": "2505.06384", "pdf": "https://arxiv.org/pdf/2505.06384", "abs": "https://arxiv.org/abs/2505.06384", "authors": ["Aditya Mishra", "Haroon Lone"], "title": "RiM: Record, Improve and Maintain Physical Well-being using Federated Learning", "categories": ["cs.LG", "cs.CR", "cs.CY"], "comment": "Report submitted in partial fulfilment of the requirements for the\n  award of the degree of Bachelor of Science (BS) in Electrical Engineering and\n  Computer Science", "summary": "In academic settings, the demanding environment often forces students to\nprioritize academic performance over their physical well-being. Moreover,\nprivacy concerns and the inherent risk of data breaches hinder the deployment\nof traditional machine learning techniques for addressing these health\nchallenges. In this study, we introduce RiM: Record, Improve, and Maintain, a\nmobile application which incorporates a novel personalized machine learning\nframework that leverages federated learning to enhance students' physical\nwell-being by analyzing their lifestyle habits. Our approach involves\npre-training a multilayer perceptron (MLP) model on a large-scale simulated\ndataset to generate personalized recommendations. Subsequently, we employ\nfederated learning to fine-tune the model using data from IISER Bhopal\nstudents, thereby ensuring its applicability in real-world scenarios. The\nfederated learning approach guarantees differential privacy by exclusively\nsharing model weights rather than raw data. Experimental results show that the\nFedAvg-based RiM model achieves an average accuracy of 60.71% and a mean\nabsolute error of 0.91--outperforming the FedPer variant (average accuracy\n46.34%, MAE 1.19)--thereby demonstrating its efficacy in predicting lifestyle\ndeficits under privacy-preserving constraints."}
{"id": "2505.07528", "pdf": "https://arxiv.org/pdf/2505.07528", "abs": "https://arxiv.org/abs/2505.07528", "authors": ["Lei Wang"], "title": "SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) models frequently encounter\nhallucination phenomena when integrating external information with internal\nparametric knowledge. Empirical studies demonstrate that the disequilibrium\nbetween external contextual information and internal parametric knowledge\nconstitutes a primary factor in hallucination generation. Existing\nhallucination detection methodologies predominantly emphasize either the\nexternal or internal mechanism in isolation, thereby overlooking their\nsynergistic effects. The recently proposed ReDeEP framework decouples these\ndual mechanisms, identifying two critical contributors to hallucinations:\nexcessive reliance on parametric knowledge encoded in feed-forward networks\n(FFN) and insufficient utilization of external information by attention\nmechanisms (particularly copy heads). ReDeEP quantitatively assesses these\nfactors to detect hallucinations and dynamically modulates the contributions of\nFFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and\nnumerous other hallucination detection approaches have been employed at\nlogit-level uncertainty estimation or language-level self-consistency\nevaluation, inadequately address the semantic dimensions of model responses,\nresulting in inconsistent hallucination assessments in RAG implementations.\nBuilding upon ReDeEP's foundation, this paper introduces SEReDeEP, which\nenhances computational processes through semantic entropy captured via trained\nlinear probes, thereby achieving hallucination assessments that more accurately\nreflect ground truth evaluations."}
{"id": "2505.06853", "pdf": "https://arxiv.org/pdf/2505.06853", "abs": "https://arxiv.org/abs/2505.06853", "authors": ["Carolina Vargas-Ecos", "Edwin Salcedo"], "title": "Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach", "categories": ["cs.CV"], "comment": "Accepted for publication at the 6th BioSMART Conference, 2025", "summary": "According to the Pan American Health Organization, the number of cancer cases\nin Latin America was estimated at 4.2 million in 2022 and is projected to rise\nto 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone\ncancers affecting young people, is difficult to detect due to its unique\ntexture and intensity. Surgical removal of osteosarcoma requires precise safety\nmargins to ensure complete resection while preserving healthy tissue.\nTherefore, this study proposes a method for estimating the confidence interval\nof surgical safety margins in osteosarcoma surgery around the knee. The\nproposed approach uses MRI and X-ray data from open-source repositories,\ndigital processing techniques, and unsupervised learning algorithms (such as\nk-means clustering) to define tumor boundaries. Experimental results highlight\nthe potential for automated, patient-specific determination of safety margins."}
{"id": "2505.07686", "pdf": "https://arxiv.org/pdf/2505.07686", "abs": "https://arxiv.org/abs/2505.07686", "authors": ["Muzhi Dai", "Chenxu Yang", "Qingyi Si"], "title": "S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As Test-Time Scaling emerges as an active research focus in the large\nlanguage model community, advanced post-training methods increasingly emphasize\nextending chain-of-thought (CoT) generation length, thereby enhancing reasoning\ncapabilities to approach Deepseek R1-like reasoning models. However, recent\nstudies reveal that reasoning models (even Qwen3) consistently exhibit\nexcessive thought redundancy in CoT generation. This overthinking problem stems\nfrom conventional outcome-reward reinforcement learning's systematic neglect in\nregulating intermediate reasoning steps. This paper proposes Serial-Group\nDecaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement\nlearning method that empowers models with the capability to determine the\nsufficiency of reasoning steps, subsequently triggering early exit of CoT\ngeneration. Specifically, unlike GRPO, which samples multiple possible\ncompletions (parallel group) in parallel, we select multiple temporal positions\nin the generation of one CoT to allow the model to exit thinking and instead\ngenerate answers (serial group), respectively. For the correct answers in a\nserial group, we assign rewards that decay according to positions, with lower\nrewards towards the later ones, thereby reinforcing the model's behavior to\ngenerate higher-quality answers at earlier phases with earlier exits of\nthinking. Empirical evaluations demonstrate compatibility with state-of-the-art\nreasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4%\n~ 61.1\\% sequence length reduction with 0.72% ~ 6.08% accuracy improvements\nacross GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks."}
{"id": "2505.06445", "pdf": "https://arxiv.org/pdf/2505.06445", "abs": "https://arxiv.org/abs/2505.06445", "authors": ["Yan Zheng", "Qiang Chen", "Chenglei Niu"], "title": "Tweedie Regression for Video Recommendation System", "categories": ["cs.LG", "cs.IR"], "comment": "ICMI 2025 IEEE 4th International Conference on Computing and Machine\n  Intelligence April 05-06, 2025", "summary": "Modern recommendation systems aim to increase click-through rates (CTR) for\nbetter user experience, through commonly treating ranking as a classification\ntask focused on predicting CTR. However, there is a gap between this method and\nthe actual objectives of businesses across different sectors. In video\nrecommendation services, the objective of video on demand (VOD) extends beyond\nmerely encouraging clicks, but also guiding users to discover their true\ninterests, leading to increased watch time. And longer users watch time will\nleads to more revenue through increased chances of presenting online display\nadvertisements. This research addresses the issue by redefining the problem\nfrom classification to regression, with a focus on maximizing revenue through\nuser viewing time. Due to the lack of positive labels on recommendation, the\nstudy introduces Tweedie Loss Function, which is better suited in this scenario\nthan the traditional mean square error loss. The paper also provides insights\non how Tweedie process capture users diverse interests. Our offline simulation\nand online A/B test revealed that we can substantially enhance our core\nbusiness objectives: user engagement in terms of viewing time and,\nconsequently, revenue. Additionally, we provide a theoretical comparison\nbetween the Tweedie Loss and the commonly employed viewing time weighted\nLogloss, highlighting why Tweedie Regression stands out as an efficient\nsolution. We further outline a framework for designing a loss function that\nfocuses on a singular objective."}
{"id": "2505.07591", "pdf": "https://arxiv.org/pdf/2505.07591", "abs": "https://arxiv.org/abs/2505.07591", "authors": ["Junjie Ye", "Caishuang Huang", "Zhuohan Chen", "Wenjie Fu", "Chenyuan Yang", "Leyi Yang", "Yilong Wu", "Peng Wang", "Meng Zhou", "Xiaolong Yang", "Tao Gui", "Qi Zhang", "Zhongchao Shi", "Jianping Fan", "Xuanjing Huang"], "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF."}
{"id": "2505.06855", "pdf": "https://arxiv.org/pdf/2505.06855", "abs": "https://arxiv.org/abs/2505.06855", "authors": ["Zhengmi Tang", "Yuto Mitsui", "Tomo Miyazaki", "Shinichiro Omachi"], "title": "Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies", "categories": ["cs.CV"], "comment": null, "summary": "Most existing text recognition methods are trained on large-scale synthetic\ndatasets due to the scarcity of labeled real-world datasets. Synthetic images,\nhowever, cannot faithfully reproduce real-world scenarios, such as uneven\nillumination, irregular layout, occlusion, and degradation, resulting in\nperformance disparities when handling complex real-world images. Recent\nself-supervised learning techniques, notably contrastive learning and masked\nimage modeling (MIM), narrow this domain gap by exploiting unlabeled real text\nimages. This study first analyzes the original Masked AutoEncoder (MAE) and\nobserves that random patch masking predominantly captures low-level textural\nfeatures but misses high-level contextual representations. To fully exploit the\nhigh-level contextual representations, we introduce random blockwise and span\nmasking in the text recognition task. These strategies can mask the continuous\nimage patches and completely remove some characters, forcing the model to infer\nrelationships among characters within a word. Our Multi-Masking Strategy (MMS)\nintegrates random patch, blockwise, and span masking into the MIM frame, which\njointly learns low and high-level textual representations. After fine-tuning\nwith real data, MMS outperforms the state-of-the-art self-supervised methods in\nvarious text-related tasks, including text recognition, segmentation, and\ntext-image super-resolution."}
{"id": "2505.07693", "pdf": "https://arxiv.org/pdf/2505.07693", "abs": "https://arxiv.org/abs/2505.07693", "authors": ["Sebastian Dumbrava"], "title": "Belief Injection for Epistemic Control in Linguistic State Space", "categories": ["cs.AI"], "comment": "30 pages, 9 figures", "summary": "This work introduces belief injection, a proactive epistemic control\nmechanism for artificial agents whose cognitive states are structured as\ndynamic ensembles of linguistic belief fragments. Grounded in the Semantic\nManifold framework, belief injection directly incorporates targeted linguistic\nbeliefs into an agent's internal cognitive state, influencing reasoning and\nalignment proactively rather than reactively. We delineate various injection\nstrategies, such as direct, context-aware, goal-oriented, and reflective\napproaches, and contrast belief injection with related epistemic control\nmechanisms, notably belief filtering. Additionally, this work discusses\npractical applications, implementation considerations, ethical implications,\nand outlines promising directions for future research into cognitive governance\nusing architecturally embedded belief injection."}
{"id": "2505.06446", "pdf": "https://arxiv.org/pdf/2505.06446", "abs": "https://arxiv.org/abs/2505.06446", "authors": ["Jessie Finocchiaro", "Rafael Frongillo", "Enrique Nueve"], "title": "Structured Prediction with Abstention via the Lovász Hinge", "categories": ["cs.LG"], "comment": "This paper is an extension of the work \"The Structured Abstain\n  Problem and the Lov\\'asz Hinge\" (arXiv:2203.08645) via the original authors", "summary": "The Lov\\'asz hinge is a convex loss function proposed for binary structured\nclassification, in which k related binary predictions jointly evaluated by a\nsubmodular function. Despite its prevalence in image segmentation and related\ntasks, the consistency of the Lov\\'asz hinge has remained open. We show that\nthe Lov\\'asz hinge is inconsistent with its desired target unless the set\nfunction used for evaluation is modular. Leveraging the embedding framework of\nFinocchiaro et al. (2024), we find the target loss for which the Lov\\'asz hinge\nis consistent. This target, which we call the structured abstain problem, is a\nvariant of selective classification for structured prediction that allows one\nto abstain on any subset of the k binary predictions. We derive a family of\nlink functions, each of which is simultaneously consistent for all\npolymatroids, a subset of submodular set functions. We then give sufficient\nconditions on the polymatroid for the structured abstain problem to be tightly\nembedded by the Lov\\'asz hinge, meaning no target prediction is redundant. We\nexperimentally demonstrate the potential of the structured abstain problem for\ninterpretability in structured classification tasks. Finally, for the\nmulticlass setting, we show that one can combine the binary encoding\nconstruction of Ramaswamy et al. (2018) with our link construction to achieve\nan efficient consistent surrogate for a natural multiclass generalization of\nthe structured abstain problem."}
{"id": "2505.07596", "pdf": "https://arxiv.org/pdf/2505.07596", "abs": "https://arxiv.org/abs/2505.07596", "authors": ["Ziyang Huang", "Xiaowei Yuan", "Yiming Ju", "Jun Zhao", "Kang Liu"], "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities."}
{"id": "2505.06881", "pdf": "https://arxiv.org/pdf/2505.06881", "abs": "https://arxiv.org/abs/2505.06881", "authors": ["Hamd Jalil", "Ahmed Qazi", "Asim Iqbal"], "title": "NeuRN: Neuro-inspired Domain Generalization for Image Classification", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "14 pages, 7 figures, 1 table", "summary": "Domain generalization in image classification is a crucial challenge, with\nmodels often failing to generalize well across unseen datasets. We address this\nissue by introducing a neuro-inspired Neural Response Normalization (NeuRN)\nlayer which draws inspiration from neurons in the mammalian visual cortex,\nwhich aims to enhance the performance of deep learning architectures on unseen\ntarget domains by training deep learning models on a source domain. The\nperformance of these models is considered as a baseline and then compared\nagainst models integrated with NeuRN on image classification tasks. We perform\nexperiments across a range of deep learning architectures, including ones\nderived from Neural Architecture Search and Vision Transformer. Additionally,\nin order to shortlist models for our experiment from amongst the vast range of\ndeep neural networks available which have shown promising results, we also\npropose a novel method that uses the Needleman-Wunsch algorithm to compute\nsimilarity between deep learning architectures. Our results demonstrate the\neffectiveness of NeuRN by showing improvement against baseline in cross-domain\nimage classification tasks. Our framework attempts to establish a foundation\nfor future neuro-inspired deep learning models."}
{"id": "2505.07757", "pdf": "https://arxiv.org/pdf/2505.07757", "abs": "https://arxiv.org/abs/2505.07757", "authors": ["Rintaro Ando"], "title": "Emotion-Gradient Metacognitive RSI (Part I): Theoretical Foundations and Single-Agent Architecture", "categories": ["cs.AI", "cs.LG", "F.1.2; I.2.0"], "comment": "21 pages, 3 figures. Part I of a four-part series (Parts II-IV\n  forthcoming)", "summary": "We present the Emotion-Gradient Metacognitive Recursive Self-Improvement\n(EG-MRSI) framework, a novel architecture that integrates introspective\nmetacognition, emotion-based intrinsic motivation, and recursive\nself-modification into a unified theoretical system. The framework is\nexplicitly capable of overwriting its own learning algorithm under formally\nbounded risk. Building upon the Noise-to-Meaning RSI (N2M-RSI) foundation,\nEG-MRSI introduces a differentiable intrinsic reward function driven by\nconfidence, error, novelty, and cumulative success. This signal regulates both\na metacognitive mapping and a self-modification operator constrained by\nprovable safety mechanisms. We formally define the initial agent configuration,\nemotion-gradient dynamics, and RSI trigger conditions, and derive a\nreinforcement-compatible optimization objective that guides the agent's\ndevelopment trajectory. Meaning Density and Meaning Conversion Efficiency are\nintroduced as quantifiable metrics of semantic learning, closing the gap\nbetween internal structure and predictive informativeness. This Part I paper\nestablishes the single-agent theoretical foundations of EG-MRSI. Future parts\nwill extend this framework to include safety certificates and rollback\nprotocols (Part II), collective intelligence mechanisms (Part III), and\nfeasibility constraints including thermodynamic and computational limits (Part\nIV). Together, the EG-MRSI series provides a rigorous, extensible foundation\nfor open-ended and safe AGI."}
{"id": "2505.06454", "pdf": "https://arxiv.org/pdf/2505.06454", "abs": "https://arxiv.org/abs/2505.06454", "authors": ["Syed Mhamudul Hasan", "Hussein Zangoti", "Iraklis Anagnostopoulos", "Abdur R. Shahid"], "title": "Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Recent studies have shown that sponge attacks can significantly increase the\nenergy consumption and inference latency of deep neural networks (DNNs).\nHowever, prior work has focused primarily on computer vision and natural\nlanguage processing tasks, overlooking the growing use of lightweight AI models\nin sensing-based applications on resource-constrained devices, such as those in\nInternet of Things (IoT) environments. These attacks pose serious threats of\nenergy depletion and latency degradation in systems where limited battery\ncapacity and real-time responsiveness are critical for reliable operation. This\npaper makes two key contributions. First, we present the first systematic\nexploration of energy-latency sponge attacks targeting sensing-based AI models.\nUsing wearable sensing-based AI as a case study, we demonstrate that sponge\nattacks can substantially degrade performance by increasing energy consumption,\nleading to faster battery drain, and by prolonging inference latency. Second,\nto mitigate such attacks, we investigate model pruning, a widely adopted\ncompression technique for resource-constrained AI, as a potential defense. Our\nexperiments show that pruning-induced sparsity significantly improves model\nresilience against sponge poisoning. We also quantify the trade-offs between\nmodel efficiency and attack resilience, offering insights into the security\nimplications of model compression in sensing-based AI systems deployed in IoT\nenvironments."}
{"id": "2505.07601", "pdf": "https://arxiv.org/pdf/2505.07601", "abs": "https://arxiv.org/abs/2505.07601", "authors": ["Edirlei Soares de Lima", "Marco A. Casanova", "Bruno Feijó", "Antonio L. Furtado"], "title": "Characterizing the Investigative Methods of Fictional Detectives with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detective fiction, a genre defined by its complex narrative structures and\ncharacter-driven storytelling, presents unique challenges for computational\nnarratology, a research field focused on integrating literary theory into\nautomated narrative generation. While traditional literary studies have offered\ndeep insights into the methods and archetypes of fictional detectives, these\nanalyses often focus on a limited number of characters and lack the scalability\nneeded for the extraction of unique traits that can be used to guide narrative\ngeneration methods. In this paper, we present an AI-driven approach for\nsystematically characterizing the investigative methods of fictional\ndetectives. Our multi-phase workflow explores the capabilities of 15 Large\nLanguage Models (LLMs) to extract, synthesize, and validate distinctive\ninvestigative traits of fictional detectives. This approach was tested on a\ndiverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes,\nWilliam Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin -\ncapturing the distinctive investigative styles that define each character. The\nidentified traits were validated against existing literary analyses and further\ntested in a reverse identification phase, achieving an overall accuracy of\n91.43%, demonstrating the method's effectiveness in capturing the distinctive\ninvestigative approaches of each detective. This work contributes to the\nbroader field of computational narratology by providing a scalable framework\nfor character analysis, with potential applications in AI-driven interactive\nstorytelling and automated narrative generation."}
{"id": "2505.06886", "pdf": "https://arxiv.org/pdf/2505.06886", "abs": "https://arxiv.org/abs/2505.06886", "authors": ["Ahmed Qazi", "Hamd Jalil", "Asim Iqbal"], "title": "Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "12 pages, 8 figures, 1 table", "summary": "The mouse is one of the most studied animal models in the field of systems\nneuroscience. Understanding the generalized patterns and decoding the neural\nrepresentations that are evoked by the diverse range of natural scene stimuli\nin the mouse visual cortex is one of the key quests in computational vision. In\nrecent years, significant parallels have been drawn between the primate visual\ncortex and hierarchical deep neural networks. However, their generalized\nefficacy in understanding mouse vision has been limited. In this study, we\ninvestigate the functional alignment between the mouse visual cortex and deep\nlearning models for object classification tasks. We first introduce a\ngeneralized representational learning strategy that uncovers a striking\nresemblance between the functional mapping of the mouse visual cortex and\nhigh-performing deep learning models on both top-down (population-level) and\nbottom-up (single cell-level) scenarios. Next, this representational similarity\nacross the two systems is further enhanced by the addition of Neural Response\nNormalization (NeuRN) layer, inspired by the activation profile of excitatory\nand inhibitory neurons in the visual cortex. To test the performance effect of\nNeuRN on real-world tasks, we integrate it into deep learning models and\nobserve significant improvements in their robustness against data shifts in\ndomain generalization tasks. Our work proposes a novel framework for comparing\nthe functional architecture of the mouse visual cortex with deep learning\nmodels. Our findings carry broad implications for the development of advanced\nAI models that draw inspiration from the mouse visual cortex, suggesting that\nthese models serve as valuable tools for studying the neural representations of\nthe mouse visual cortex and, as a result, enhancing their performance on\nreal-world tasks."}
{"id": "2505.07759", "pdf": "https://arxiv.org/pdf/2505.07759", "abs": "https://arxiv.org/abs/2505.07759", "authors": ["Jennifer Mondragon", "Carlos Rubio-Medrano", "Gael Cruz", "Dvijesh Shastri"], "title": "\"I Apologize For Not Understanding Your Policy\": Exploring the Specification and Evaluation of User-Managed Access Control Policies by AI Virtual Assistants", "categories": ["cs.AI"], "comment": null, "summary": "The rapid evolution of Artificial Intelligence (AI)-based Virtual Assistants\n(VAs) e.g., Google Gemini, ChatGPT, Microsoft Copilot, and High-Flyer Deepseek\nhas turned them into convenient interfaces for managing emerging technologies\nsuch as Smart Homes, Smart Cars, Electronic Health Records, by means of\nexplicit commands,e.g., prompts, which can be even launched via voice, thus\nproviding a very convenient interface for end-users. However, the proper\nspecification and evaluation of User-Managed Access Control Policies (U-MAPs),\nthe rules issued and managed by end-users to govern access to sensitive data\nand device functionality - within these VAs presents significant challenges,\nsince such a process is crucial for preventing security vulnerabilities and\nprivacy leaks without impacting user experience. This study provides an initial\nexploratory investigation on whether current publicly-available VAs can manage\nU-MAPs effectively across differing scenarios. By conducting unstructured to\nstructured tests, we evaluated the comprehension of such VAs, revealing a lack\nof understanding in varying U-MAP approaches. Our research not only identifies\nkey limitations, but offers valuable insights into how VAs can be further\nimproved to manage complex authorization rules and adapt to dynamic changes."}
{"id": "2505.06459", "pdf": "https://arxiv.org/pdf/2505.06459", "abs": "https://arxiv.org/abs/2505.06459", "authors": ["Pablo Flores", "Olga Graf", "Pavlos Protopapas", "Karim Pichara"], "title": "Improved Uncertainty Quantification in Physics-Informed Neural Networks Using Error Bounds and Solution Bundles", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "stat.ML"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have been widely used to obtain\nsolutions to various physical phenomena modeled as Differential Equations. As\nPINNs are not naturally equipped with mechanisms for Uncertainty\nQuantification, some work has been done to quantify the different uncertainties\nthat arise when dealing with PINNs. In this paper, we use a two-step procedure\nto train Bayesian Neural Networks that provide uncertainties over the solutions\nto differential equation systems provided by PINNs. We use available error\nbounds over PINNs to formulate a heteroscedastic variance that improves the\nuncertainty estimation. Furthermore, we solve forward problems and utilize the\nobtained uncertainties when doing parameter estimation in inverse problems in\ncosmology."}
{"id": "2505.07608", "pdf": "https://arxiv.org/pdf/2505.07608", "abs": "https://arxiv.org/abs/2505.07608", "authors": ["Xiaomi LLM-Core Team", ":", "Bingquan Xia", "Bowen Shen", "Cici", "Dawei Zhu", "Di Zhang", "Gang Wang", "Hailin Zhang", "Huaqiu Liu", "Jiebao Xiao", "Jinhao Dong", "Liang Zhao", "Peidian Li", "Peng Wang", "Shihua Yu", "Shimao Chen", "Weikun Wang", "Wenhan Ma", "Xiangwei Deng", "Yi Huang", "Yifan Song", "Zihan Jiang", "Bowen Ye", "Can Cai", "Chenhong He", "Dong Zhang", "Duo Zhang", "Guoan Wang", "Hao Tian", "Haochen Zhao", "Heng Qu", "Hongshen Xu", "Jun Shi", "Kainan Bao", "QingKai Fang", "Kang Zhou", "Kangyang Zhou", "Lei Li", "Menghang Zhu", "Nuo Chen", "Qiantong Wang", "Shaohui Liu", "Shicheng Li", "Shuhao Gu", "Shuhuai Ren", "Shuo Liu", "Sirui Deng", "Weiji Zhuang", "Weiwei Lv", "Wenyu Yang", "Xin Zhang", "Xing Yong", "Xing Zhang", "Xingchen Song", "Xinzhe Xu", "Xu Wang", "Yihan Yan", "Yu Tu", "Yuanyuan Tian", "Yudong Wang", "Yue Yu", "Zhenru Lin", "Zhichao Song", "Zihao Yue"], "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo."}
{"id": "2505.06894", "pdf": "https://arxiv.org/pdf/2505.06894", "abs": "https://arxiv.org/abs/2505.06894", "authors": ["Ahmed Qazi", "Abdul Basit", "Asim Iqbal"], "title": "NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "18 pages, 6 figures", "summary": "Neural Radiance Fields (NeRF) have significantly advanced the field of novel\nview synthesis, yet their generalization across diverse scenes and conditions\nremains challenging. Addressing this, we propose the integration of a novel\nbrain-inspired normalization technique Neural Generalization (NeuGen) into\nleading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts\nthe domain-invariant features, thereby enhancing the models' generalization\ncapabilities. It can be seamlessly integrated into NeRF architectures and\ncultivates a comprehensive feature set that significantly improves accuracy and\nrobustness in image rendering. Through this integration, NeuGen shows improved\nperformance on benchmarks on diverse datasets across state-of-the-art NeRF\narchitectures, enabling them to generalize better across varied scenes. Our\ncomprehensive evaluations, both quantitative and qualitative, confirm that our\napproach not only surpasses existing models in generalizability but also\nmarkedly improves rendering quality. Our work exemplifies the potential of\nmerging neuroscientific principles with deep learning frameworks, setting a new\nprecedent for enhanced generalizability and efficiency in novel view synthesis.\nA demo of our study is available at https://neugennerf.github.io."}
{"id": "2505.07773", "pdf": "https://arxiv.org/pdf/2505.07773", "abs": "https://arxiv.org/abs/2505.07773", "authors": ["Xinji Mai", "Haotian Xu", "Xing W", "Weinong Wang", "Yingying Zhang", "Wenqiang Zhang"], "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}."}
{"id": "2505.06475", "pdf": "https://arxiv.org/pdf/2505.06475", "abs": "https://arxiv.org/abs/2505.06475", "authors": ["Binwen Liu", "Peiyu Xu", "Quan Yuan", "Yihong Chen"], "title": "Probing In-Context Learning: Impact of Task Complexity and Model Architecture on Generalization and Efficiency", "categories": ["cs.LG"], "comment": null, "summary": "We investigate in-context learning (ICL) through a meticulous experimental\nframework that systematically varies task complexity and model architecture.\nExtending beyond the linear regression baseline, we introduce Gaussian kernel\nregression and nonlinear dynamical system tasks, which emphasize temporal and\nrecursive reasoning. We evaluate four distinct models: a GPT2-style\nTransformer, a Transformer with FlashAttention mechanism, a convolutional\nHyena-based model, and the Mamba state-space model. Each model is trained from\nscratch on synthetic datasets and assessed for generalization during testing.\nOur findings highlight that model architecture significantly shapes ICL\nperformance. The standard Transformer demonstrates robust performance across\ndiverse tasks, while Mamba excels in temporally structured dynamics. Hyena\neffectively captures long-range dependencies but shows higher variance early in\ntraining, and FlashAttention offers computational efficiency but is more\nsensitive in low-data regimes. Further analysis uncovers locality-induced\nshortcuts in Gaussian kernel tasks, enhanced nonlinear separability through\ninput range scaling, and the critical role of curriculum learning in mastering\nhigh-dimensional tasks."}
{"id": "2505.07610", "pdf": "https://arxiv.org/pdf/2505.07610", "abs": "https://arxiv.org/abs/2505.07610", "authors": ["Kenza Amara", "Rita Sevastjanova", "Mennatallah El-Assady"], "title": "Concept-Level Explainability for Auditing & Steering LLM Responses", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 7 figures, Submission to Neurips 2025", "summary": "As large language models (LLMs) become widely deployed, concerns about their\nsafety and alignment grow. An approach to steer LLM behavior, such as\nmitigating biases or defending against jailbreaks, is to identify which parts\nof a prompt influence specific aspects of the model's output. Token-level\nattribution methods offer a promising solution, but still struggle in text\ngeneration, explaining the presence of each token in the output separately,\nrather than the underlying semantics of the entire LLM response. We introduce\nConceptX, a model-agnostic, concept-level explainability method that identifies\nthe concepts, i.e., semantically rich tokens in the prompt, and assigns them\nimportance based on the outputs' semantic similarity. Unlike current\ntoken-level methods, ConceptX also offers to preserve context integrity through\nin-place token replacements and supports flexible explanation goals, e.g.,\ngender bias. ConceptX enables both auditing, by uncovering sources of bias, and\nsteering, by modifying prompts to shift the sentiment or reduce the harmfulness\nof LLM responses, without requiring retraining. Across three LLMs, ConceptX\noutperforms token-level methods like TokenSHAP in both faithfulness and human\nalignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for\nrandom edits and lower attack success rates from 0.463 to 0.242, outperforming\nattribution and paraphrasing baselines. While prompt engineering and\nself-explaining methods sometimes yield safer responses, ConceptX offers a\ntransparent and faithful alternative for improving LLM safety and alignment,\ndemonstrating the practical value of attribution-based explainability in\nguiding LLM behavior."}
{"id": "2505.06898", "pdf": "https://arxiv.org/pdf/2505.06898", "abs": "https://arxiv.org/abs/2505.06898", "authors": ["Honglong Yang", "Shanshan Song", "Yi Qin", "Lehan Wang", "Haonan Wang", "Xinpeng Ding", "Qixiang Zhang", "Bodong Du", "Xiaomeng Li"], "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generalist Medical AI (GMAI) systems have demonstrated expert-level\nperformance in biomedical perception tasks, yet their clinical utility remains\nlimited by inadequate multi-modal explainability and suboptimal prognostic\ncapabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI\nassistant that integrates textual and visual interpretability to support\ntransparent and trustworthy medical decision-making. XMedGPT not only produces\naccurate diagnostic and descriptive outputs, but also grounds referenced\nanatomical sites within medical images, bridging critical gaps in\ninterpretability and enhancing clinician usability. To support real-world\ndeployment, we introduce a reliability indexing mechanism that quantifies\nuncertainty through consistency-based assessment via interactive\nquestion-answering. We validate XMedGPT across four pillars: multi-modal\ninterpretability, uncertainty quantification, and prognostic modeling, and\nrigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical\nregions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between\nvisual rationales and clinical outcomes. For uncertainty estimation, it attains\nan AUC of 0.862 on visual question answering and 0.764 on radiology report\ngeneration. In survival and recurrence prediction for lung and glioma cancers,\nit surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.\nRigorous benchmarking across 347 datasets covers 40 imaging modalities and\nexternal validation spans 4 anatomical systems confirming exceptional\ngeneralizability, with performance gains surpassing existing GMAI by 20.7% for\nin-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,\nXMedGPT represents a significant leap forward in clinician-centric AI\nintegration, offering trustworthy and scalable support for diverse healthcare\napplications."}
{"id": "2505.06241", "pdf": "https://arxiv.org/pdf/2505.06241", "abs": "https://arxiv.org/abs/2505.06241", "authors": ["Arek Berc Gokdag", "Silvia Mura", "Antonio Coviello", "Michele Zhu", "Maurizio Magarini", "Umberto Spagnolini"], "title": "Low-Complexity CNN-Based Classification of Electroneurographic Signals", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Peripheral nerve interfaces (PNIs) facilitate neural recording and\nstimulation for treating nerve injuries, but real-time classification of\nelectroneurographic (ENG) signals remains challenging due to constraints on\ncomplexity and latency, particularly in implantable devices. This study\nintroduces MobilESCAPE-Net, a lightweight architecture that reduces\ncomputational cost while maintaining and slightly improving classification\nperformance. Compared to the state-of-the-art ESCAPE-Net, MobilESCAPE-Net\nachieves comparable accuracy and F1-score with significantly lower complexity,\nreducing trainable parameters by 99.9\\% and floating point operations per\nsecond by 92.47\\%, enabling faster inference and real-time processing. Its\nefficiency makes it well-suited for low-complexity ENG signal classification in\nresource-constrained environments such as implantable devices."}
{"id": "2505.06481", "pdf": "https://arxiv.org/pdf/2505.06481", "abs": "https://arxiv.org/abs/2505.06481", "authors": ["HamidReza Imani", "Jiaxin Peng", "Peiman Mohseni", "Abdolah Amirany", "Tarek El-Ghazawi"], "title": "QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "The deployment of mixture-of-experts (MoE) large language models (LLMs)\npresents significant challenges due to their high memory demands. These\nchallenges become even more pronounced in multi-tenant environments, where\nshared resources must accommodate multiple models, limiting the effectiveness\nof conventional virtualization techniques. This paper addresses the problem of\nefficiently serving multiple fine-tuned MoE-LLMs on a single-GPU. We propose a\nserving system that employs \\textit{similarity-based expert consolidation} to\nreduce the overall memory footprint by sharing similar experts across models.\nTo ensure output quality, we introduce \\textit{runtime partial\nreconfiguration}, dynamically replacing non-expert layers when processing\nrequests from different models. As a result, our approach achieves a\ncompetitive output quality while maintaining throughput comparable to serving a\nsingle model while incurring a negligible increase in time-to-first-token\n(TTFT). Experiments on a server with a single NVIDIA A100 GPU (80GB) using\nMixtral-8x7B models demonstrate an 85\\% average reduction in turnaround time\ncompared to NVIDIA's multi-instance GPU (MIG). Furthermore, experiments on\nGoogle's Switch Transformer Base-8 model with up to four variants demonstrate\nthe scalability and resilience of our approach in maintaining output quality\ncompared to other model merging baselines, highlighting its effectiveness."}
{"id": "2505.07637", "pdf": "https://arxiv.org/pdf/2505.07637", "abs": "https://arxiv.org/abs/2505.07637", "authors": ["Krish Goel", "Sanskar Pandey", "KS Mahadevan", "Harsh Kumar", "Vishesh Khadaria"], "title": "Chronocept: Instilling a Sense of Time in Machines", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 8 figures, 18 tables", "summary": "Human cognition is deeply intertwined with a sense of time, known as\nChronoception. This sense allows us to judge how long facts remain valid and\nwhen knowledge becomes outdated. Despite progress in vision, language, and\nmotor control, AI still struggles to reason about temporal validity. We\nintroduce Chronocept, the first benchmark to model temporal validity as a\ncontinuous probability distribution over time. Using skew-normal curves fitted\nalong semantically decomposed temporal axes, Chronocept captures nuanced\npatterns of emergence, decay, and peak relevance. It includes two datasets:\nBenchmark I (atomic facts) and Benchmark II (multi-sentence passages).\nAnnotations show strong inter-annotator agreement (84% and 89%). Our baselines\npredict curve parameters - location, scale, and skewness - enabling\ninterpretable, generalizable learning and outperforming classification-based\napproaches. Chronocept fills a foundational gap in AI's temporal reasoning,\nsupporting applications in knowledge grounding, fact-checking,\nretrieval-augmented generation (RAG), and proactive agents. Code and data are\npublicly available."}
{"id": "2505.06903", "pdf": "https://arxiv.org/pdf/2505.06903", "abs": "https://arxiv.org/abs/2505.06903", "authors": ["Yuanzhuo Wang", "Junwen Duan", "Xinyu Li", "Jianxin Wang"], "title": "CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal medical image analysis is essential for clinical decision-making,\nyet existing methods either align images and text at a coarse level - causing\npotential semantic mismatches - or depend solely on visual information, lacking\nmedical semantic integration. We present CheXLearner, the first end-to-end\nframework that unifies anatomical region detection, Riemannian manifold-based\nstructure alignment, and fine-grained regional semantic guidance. Our proposed\nMed-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to\nrobustly align anatomical structures and capture pathologically meaningful\ndiscrepancies across temporal chest X-rays. By introducing regional progression\ndescriptions as supervision, CheXLearner achieves enhanced cross-modal\nrepresentation learning and supports dynamic low-level feature optimization.\nExperiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and\n80.32% (+11.05%) F1-score on anatomical region progression detection -\nsubstantially outperforming state-of-the-art baselines, especially in\nstructurally complex regions. Additionally, our model attains a 91.52% average\nAUC score in downstream disease classification, validating its superior feature\nrepresentation."}
{"id": "2505.06246", "pdf": "https://arxiv.org/pdf/2505.06246", "abs": "https://arxiv.org/abs/2505.06246", "authors": ["Dominic Parosh Yamarthi", "Haripriya Raman", "Shamsad Parvin"], "title": "United States Road Accident Prediction using Random Forest Predictor", "categories": ["cs.CY", "cs.AI", "cs.LG", "stat.AP"], "comment": "5 Pages, 8 Figures", "summary": "Road accidents significantly threaten public safety and require in-depth\nanalysis for effective prevention and mitigation strategies. This paper focuses\non predicting accidents through the examination of a comprehensive traffic\ndataset covering 49 states in the United States. The dataset integrates\ninformation from diverse sources, including transportation departments, law\nenforcement, and traffic sensors. This paper specifically emphasizes predicting\nthe number of accidents, utilizing advanced machine learning models such as\nregression analysis and time series analysis. The inclusion of various factors,\nranging from environmental conditions to human behavior and infrastructure,\nensures a holistic understanding of the dynamics influencing road safety.\nTemporal and spatial analysis further allows for the identification of trends,\nseasonal variations, and high-risk areas. The implications of this research\nextend to proactive decision-making for policymakers and transportation\nauthorities. By providing accurate predictions and quantifiable insights into\nexpected accident rates under different conditions, the paper aims to empower\nauthorities to allocate resources efficiently and implement targeted\ninterventions. The goal is to contribute to the development of informed\npolicies and interventions that enhance road safety, creating a safer\nenvironment for all road users. Keywords: Machine Learning, Random Forest,\nAccident Prediction, AutoML, LSTM."}
{"id": "2505.06482", "pdf": "https://arxiv.org/pdf/2505.06482", "abs": "https://arxiv.org/abs/2505.06482", "authors": ["Minting Pan", "Yitao Zheng", "Jiajian Li", "Yunbo Wang", "Xiaokang Yang"], "title": "Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Offline reinforcement learning (RL) enables policy optimization in static\ndatasets, avoiding the risks and costs of real-world exploration. However, it\nstruggles with suboptimal behavior learning and inaccurate value estimation due\nto the lack of environmental interaction. In this paper, we present\nVideo-Enhanced Offline RL (VeoRL), a model-based approach that constructs an\ninteractive world model from diverse, unlabeled video data readily available\nonline. Leveraging model-based behavior guidance, VeoRL transfers commonsense\nknowledge of control policy and physical dynamics from natural videos to the RL\nagent within the target domain. Our method achieves substantial performance\ngains (exceeding 100% in some cases) across visuomotor control tasks in robotic\nmanipulation, autonomous driving, and open-world video games."}
{"id": "2505.07653", "pdf": "https://arxiv.org/pdf/2505.07653", "abs": "https://arxiv.org/abs/2505.07653", "authors": ["Iman Johary", "Raphael Romero", "Alexandru C. Mara", "Tijl De Bie"], "title": "JobHop: A Large-Scale Dataset of Career Trajectories", "categories": ["cs.CL"], "comment": null, "summary": "Understanding labor market dynamics is essential for policymakers, employers,\nand job seekers. However, comprehensive datasets that capture real-world career\ntrajectories are scarce. In this paper, we introduce JobHop, a large-scale\npublic dataset derived from anonymized resumes provided by VDAB, the public\nemployment service in Flanders, Belgium. Utilizing Large Language Models\n(LLMs), we process unstructured resume data to extract structured career\ninformation, which is then mapped to standardized ESCO occupation codes using a\nmulti-label classification model. This results in a rich dataset of over 2.3\nmillion work experiences, extracted from and grouped into more than 391,000\nuser resumes and mapped to standardized ESCO occupation codes, offering\nvaluable insights into real-world occupational transitions. This dataset\nenables diverse applications, such as analyzing labor market mobility, job\nstability, and the effects of career breaks on occupational transitions. It\nalso supports career path prediction and other data-driven decision-making\nprocesses. To illustrate its potential, we explore key dataset characteristics,\nincluding job distributions, career breaks, and job transitions, demonstrating\nits value for advancing labor market research."}
{"id": "2505.06912", "pdf": "https://arxiv.org/pdf/2505.06912", "abs": "https://arxiv.org/abs/2505.06912", "authors": ["Chao Ding", "Mouxiao Bian", "Pengcheng Chen", "Hongliang Zhang", "Tianbin Li", "Lihao Liu", "Jiayuan Chen", "Zhuoran Li", "Yabei Zhong", "Yongqi Liu", "Haiqing Huang", "Dongming Shan", "Junjun He", "Jie Xu"], "title": "Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI", "categories": ["cs.CV"], "comment": null, "summary": "Despite strong performance in medical question-answering, the clinical\nadoption of Large Language Models (LLMs) is critically hampered by their opaque\n'black-box' reasoning, limiting clinician trust. This challenge is compounded\nby the predominant reliance of current medical LLMs on corpora from scientific\nliterature or synthetic data, which often lack the granular expert validation\nand high clinical relevance essential for advancing their specialized medical\ncapabilities. To address these critical gaps, we introduce a highly clinically\nrelevant dataset with 31,247 medical question-answer pairs, each accompanied by\nexpert-validated chain-of-thought (CoT) explanations. This resource, spanning\nmultiple clinical domains, was curated via a scalable human-LLM hybrid\npipeline: LLM-generated rationales were iteratively reviewed, scored, and\nrefined by medical experts against a structured rubric, with substandard\noutputs revised through human effort or guided LLM regeneration until expert\nconsensus. This publicly available dataset provides a vital source for the\ndevelopment of medical LLMs that capable of transparent and verifiable\nreasoning, thereby advancing safer and more interpretable AI in medicine."}
{"id": "2505.06250", "pdf": "https://arxiv.org/pdf/2505.06250", "abs": "https://arxiv.org/abs/2505.06250", "authors": ["Yizhuo Wu", "Yi Zhu", "Kun Qian", "Qinyu Chen", "Anding Zhu", "John Gajadharsing", "Leo C. N. de Vreede", "Chang Gao"], "title": "DeltaDPD: Exploiting Dynamic Temporal Sparsity in Recurrent Neural Networks for Energy-Efficient Wideband Digital Predistortion", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE Microwave and Wireless Technology Letters (MWTL)", "summary": "Digital Predistortion (DPD) is a popular technique to enhance signal quality\nin wideband RF power amplifiers (PAs). With increasing bandwidth and data\nrates, DPD faces significant energy consumption challenges during deployment,\ncontrasting with its efficiency goals. State-of-the-art DPD models rely on\nrecurrent neural networks (RNN), whose computational complexity hinders system\nefficiency. This paper introduces DeltaDPD, exploring the dynamic temporal\nsparsity of input signals and neuronal hidden states in RNNs for\nenergy-efficient DPD, reducing arithmetic operations and memory accesses while\npreserving satisfactory linearization performance. Applying a TM3.1a 200MHz-BW\n256-QAM OFDM signal to a 3.5 GHz GaN Doherty RF PA, DeltaDPD achieves -50.03\ndBc in Adjacent Channel Power Ratio (ACPR), -37.22 dB in Normalized Mean Square\nError (NMSE) and -38.52 dBc in Error Vector Magnitude (EVM) with 52% temporal\nsparsity, leading to a 1.8X reduction in estimated inference power. The\nDeltaDPD code will be released after formal publication at\nhttps://www.opendpd.com."}
{"id": "2505.06497", "pdf": "https://arxiv.org/pdf/2505.06497", "abs": "https://arxiv.org/abs/2505.06497", "authors": ["Jiacheng Wang", "Hongtao Lv", "Lei Liu"], "title": "FedADP: Unified Model Aggregation for Federated Learning with Heterogeneous Model Architectures", "categories": ["cs.LG"], "comment": null, "summary": "Traditional Federated Learning (FL) faces significant challenges in terms of\nefficiency and accuracy, particularly in heterogeneous environments where\nclients employ diverse model architectures and have varying computational\nresources. Such heterogeneity complicates the aggregation process, leading to\nperformance bottlenecks and reduced model generalizability. To address these\nissues, we propose FedADP, a federated learning framework designed to adapt to\nclient heterogeneity by dynamically adjusting model architectures during\naggregation. FedADP enables effective collaboration among clients with\ndiffering capabilities, maximizing resource utilization and ensuring model\nquality. Our experimental results demonstrate that FedADP significantly\noutperforms existing methods, such as FlexiFed, achieving an accuracy\nimprovement of up to 23.30%, thereby enhancing model adaptability and training\nefficiency in heterogeneous real-world settings."}
{"id": "2505.07659", "pdf": "https://arxiv.org/pdf/2505.07659", "abs": "https://arxiv.org/abs/2505.07659", "authors": ["Ethan Gotlieb Wilcox", "Cui Ding", "Giovanni Acampa", "Tiago Pimentel", "Alex Warstadt", "Tamar I. Regev"], "title": "Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent", "categories": ["cs.CL"], "comment": null, "summary": "This paper argues that the relationship between lexical identity and prosody\n-- one well-studied parameter of linguistic variation -- can be characterized\nusing information theory. We predict that languages that use prosody to make\nlexical distinctions should exhibit a higher mutual information between word\nidentity and prosody, compared to languages that don't. We test this hypothesis\nin the domain of pitch, which is used to make lexical distinctions in tonal\nlanguages, like Cantonese. We use a dataset of speakers reading sentences aloud\nin ten languages across five language families to estimate the mutual\ninformation between the text and their pitch curves. We find that, across\nlanguages, pitch curves display similar amounts of entropy. However, these\ncurves are easier to predict given their associated text in the tonal\nlanguages, compared to pitch- and stress-accent languages, and thus the mutual\ninformation is higher in these languages, supporting our hypothesis. Our\nresults support perspectives that view linguistic typology as gradient, rather\nthan categorical."}
{"id": "2505.06920", "pdf": "https://arxiv.org/pdf/2505.06920", "abs": "https://arxiv.org/abs/2505.06920", "authors": ["Timing Li", "Bing Cao", "Pengfei Zhu", "Bin Xiao", "Qinghua Hu"], "title": "Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Acquiring accurately aligned multi-modal image pairs is fundamental for\nachieving high-quality multi-modal image fusion. To address the lack of ground\ntruth in current multi-modal image registration and fusion methods, we propose\na novel self-supervised \\textbf{B}i-directional\n\\textbf{S}elf-\\textbf{R}egistration framework (\\textbf{B-SR}). Specifically,\nB-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator\n(IPDG) to achieve self-supervised global-local registration. Visible-infrared\nimage pairs with spatially misaligned differences are aligned to obtain global\ndifferences through the registration module. The same image pairs are processed\nby PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain\nlocal differences. IPDG converts the obtained local differences into\npseudo-global differences, which are used to perform global-local difference\nconsistency with the global differences. Furthermore, aiming at eliminating the\neffect of modal gaps on the registration module, we design a neighborhood\ndynamic alignment loss to achieve cross-modal image edge alignment. Extensive\nexperiments on misaligned multi-modal images demonstrate the effectiveness of\nthe proposed method in multi-modal image alignment and fusion against the\ncompeting methods. Our code will be publicly available."}
{"id": "2505.06256", "pdf": "https://arxiv.org/pdf/2505.06256", "abs": "https://arxiv.org/abs/2505.06256", "authors": ["Fuhui Zhou", "Chunyu Liu", "Hao Zhang", "Wei Wu", "Qihui Wu", "Derrick Wing Kwan Ng", "Tony Q. S. Quek", "Chan-Byoung Chae"], "title": "SpectrumFM: A Foundation Model for Intelligent Spectrum Management", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Intelligent spectrum management is crucial for improving spectrum efficiency\nand achieving secure utilization of spectrum resources. However, existing\nintelligent spectrum management methods, typically based on small-scale models,\nsuffer from notable limitations in recognition accuracy, convergence speed, and\ngeneralization, particularly in the complex and dynamic spectrum environments.\nTo address these challenges, this paper proposes a novel spectrum foundation\nmodel, termed SpectrumFM, establishing a new paradigm for spectrum management.\nSpectrumFM features an innovative encoder architecture that synergistically\nexploits the convolutional neural networks and the multi-head self-attention\nmechanisms to enhance feature extraction and enable robust representation\nlearning. The model is pre-trained via two novel self-supervised learning\ntasks, namely masked reconstruction and next-slot signal prediction, which\nleverage large-scale in-phase and quadrature (IQ) data to achieve comprehensive\nand transferable spectrum representations. Furthermore, a parameter-efficient\nfine-tuning strategy is proposed to enable SpectrumFM to adapt to various\ndownstream spectrum management tasks, including automatic modulation\nclassification (AMC), wireless technology classification (WTC), spectrum\nsensing (SS), and anomaly detection (AD). Extensive experiments demonstrate\nthat SpectrumFM achieves superior performance in terms of accuracy, robustness,\nadaptability, few-shot learning efficiency, and convergence speed, consistently\noutperforming conventional methods across multiple benchmarks. Specifically,\nSpectrumFM improves AMC accuracy by up to 12.1% and WTC accuracy by 9.3%,\nachieves an area under the curve (AUC) of 0.97 in SS at -4 dB signal-to-noise\nratio (SNR), and enhances AD performance by over 10%."}
{"id": "2505.06520", "pdf": "https://arxiv.org/pdf/2505.06520", "abs": "https://arxiv.org/abs/2505.06520", "authors": ["Xuran Li", "Jingyi Wang", "Xiaohan Yuan", "Peixin Zhang", "Zhan Qin", "Zhibo Wang", "Kui Ren"], "title": "PRUNE: A Patching Based Repair Framework for Certiffable Unlearning of Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "It is often desirable to remove (a.k.a. unlearn) a speciffc part of the\ntraining data from a trained neural network model. A typical application\nscenario is to protect the data holder's right to be forgotten, which has been\npromoted by many recent regulation rules. Existing unlearning methods involve\ntraining alternative models with remaining data, which may be costly and\nchallenging to verify from the data holder or a thirdparty auditor's\nperspective. In this work, we provide a new angle and propose a novel\nunlearning approach by imposing carefully crafted \"patch\" on the original\nneural network to achieve targeted \"forgetting\" of the requested data to\ndelete. Speciffcally, inspired by the research line of neural network repair,\nwe propose to strategically seek a lightweight minimum \"patch\" for unlearning a\ngiven data point with certiffable guarantee. Furthermore, to unlearn a\nconsiderable amount of data points (or an entire class), we propose to\niteratively select a small subset of representative data points to unlearn,\nwhich achieves the effect of unlearning the whole set. Extensive experiments on\nmultiple categorical datasets demonstrates our approach's effectiveness,\nachieving measurable unlearning while preserving the model's performance and\nbeing competitive in efffciency and memory consumption compared to various\nbaseline methods."}
{"id": "2505.07671", "pdf": "https://arxiv.org/pdf/2505.07671", "abs": "https://arxiv.org/abs/2505.07671", "authors": ["Xianrui Zhong", "Bowen Jin", "Siru Ouyang", "Yanzhen Shen", "Qiao Jin", "Yin Fang", "Zhiyong Lu", "Jiawei Han"], "title": "Benchmarking Retrieval-Augmented Generation for Chemistry", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a powerful framework for\nenhancing large language models (LLMs) with external knowledge, particularly in\nscientific domains that demand specialized and dynamic information. Despite its\npromise, the application of RAG in the chemistry domain remains underexplored,\nprimarily due to the lack of high-quality, domain-specific corpora and\nwell-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a\ncomprehensive benchmark designed to systematically assess the effectiveness of\nRAG across a diverse set of chemistry-related tasks. The accompanying chemistry\ncorpus integrates heterogeneous knowledge sources, including scientific\nliterature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia\nentries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG\ntoolkit that supports five retrieval algorithms and eight LLMs. Using\nChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain\n-- achieving an average relative improvement of 17.4% over direct inference\nmethods. We further conduct in-depth analyses on retriever architectures,\ncorpus selection, and the number of retrieved passages, culminating in\npractical recommendations to guide future research and deployment of RAG\nsystems in the chemistry domain. The code and data is available at\nhttps://chemrag.github.io."}
{"id": "2505.06937", "pdf": "https://arxiv.org/pdf/2505.06937", "abs": "https://arxiv.org/abs/2505.06937", "authors": ["Fei Zhou", "Yi Li", "Mingqing Zhu"], "title": "Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, the dual-optical attention fusion crowd head point counting\nmodel (TAPNet) is proposed to address the problem of the difficulty of accurate\ncounting in complex scenes such as crowd dense occlusion and low light in crowd\ncounting tasks under UAV view. The model designs a dual-optical attention\nfusion module (DAFP) by introducing complementary information from infrared\nimages to improve the accuracy and robustness of all-day crowd counting. In\norder to fully utilize different modal information and solve the problem of\ninaccurate localization caused by systematic misalignment between image pairs,\nthis paper also proposes an adaptive two-optical feature decomposition fusion\nmodule (AFDF). In addition, we optimize the training strategy to improve the\nmodel robustness through spatial random offset data augmentation. Experiments\non two challenging public datasets, DroneRGBT and GAIIC2, show that the\nproposed method outperforms existing techniques in terms of performance,\nespecially in challenging dense low-light scenes. Code is available at\nhttps://github.com/zz-zik/TAPNet"}
{"id": "2505.06261", "pdf": "https://arxiv.org/pdf/2505.06261", "abs": "https://arxiv.org/abs/2505.06261", "authors": ["Wei Meng"], "title": "Modeling supply chain compliance response strategies based on AI synthetic data with structural path regression: A Simulation Study of EU 2027 Mandatory Labor Regulations", "categories": ["cs.CY", "cs.AI", "stat.AP", "90B06 (Primary) 62J05, 91B74 (Secondary)", "I.6.3; I.2.6; J.1"], "comment": "Simulated data modeling of the impact of non-tariff barriers in trade\n  wars", "summary": "In the context of the new mandatory labor compliance in the European Union\n(EU), which will be implemented in 2027, supply chain enterprises face\nstringent working hour management requirements and compliance risks. In order\nto scientifically predict the enterprises' coping behaviors and performance\noutcomes under the policy impact, this paper constructs a methodological\nframework that integrates the AI synthetic data generation mechanism and\nstructural path regression modeling to simulate the enterprises' strategic\ntransition paths under the new regulations. In terms of research methodology,\nthis paper adopts high-quality simulation data generated based on Monte Carlo\nmechanism and NIST synthetic data standards to construct a structural path\nanalysis model that includes multiple linear regression, logistic regression,\nmediation effect and moderating effect. The variable system covers 14\nindicators such as enterprise working hours, compliance investment, response\nspeed, automation level, policy dependence, etc. The variable set with\nexplanatory power is screened out through exploratory data analysis (EDA) and\nVIF multicollinearity elimination. The findings show that compliance investment\nhas a significant positive impact on firm survival and its effect is\ntransmitted through the mediating path of the level of intelligence; meanwhile,\nfirms' dependence on the EU market significantly moderates the strength of this\nmediating effect. It is concluded that AI synthetic data combined with\nstructural path modeling provides an effective tool for high-intensity\nregulatory simulation, which can provide a quantitative basis for corporate\nstrategic response, policy design and AI-assisted decision-making in the\npre-prediction stage lacking real scenario data. Keywords: AI synthetic data,\nstructural path regression modeling, compliance response strategy, EU 2027\nmandatory labor regulation"}
{"id": "2505.06534", "pdf": "https://arxiv.org/pdf/2505.06534", "abs": "https://arxiv.org/abs/2505.06534", "authors": ["Ummay Maria Muna", "Fahim Hafiz", "Shanta Biswas", "Riasat Azim"], "title": "GBDTSVM: Combined Support Vector Machine and Gradient Boosting Decision Tree Framework for efficient snoRNA-disease association prediction", "categories": ["cs.LG", "q-bio.QM"], "comment": "30 pages, 3 figures", "summary": "Small nucleolar RNAs (snoRNAs) are increasingly recognized for their critical\nrole in the pathogenesis and characterization of various human diseases.\nConsequently, the precise identification of snoRNA-disease associations (SDAs)\nis essential for the progression of diseases and the advancement of treatment\nstrategies. However, conventional biological experimental approaches are\ncostly, time-consuming, and resource-intensive; therefore, machine\nlearning-based computational methods offer a promising solution to mitigate\nthese limitations. This paper proposes a model called 'GBDTSVM', representing a\nnovel and efficient machine learning approach for predicting snoRNA-disease\nassociations by leveraging a Gradient Boosting Decision Tree (GBDT) and Support\nVector Machine (SVM). 'GBDTSVM' effectively extracts integrated snoRNA-disease\nfeature representations utilizing GBDT and SVM is subsequently utilized to\nclassify and identify potential associations. Furthermore, the method enhances\nthe accuracy of these predictions by incorporating Gaussian kernel profile\nsimilarity for both snoRNAs and diseases. Experimental evaluation of the\nGBDTSVM model demonstrated superior performance compared to state-of-the-art\nmethods in the field, achieving an area under the receiver operating\ncharacteristic (AUROC) of 0.96 and an area under the precision-recall curve\n(AUPRC) of 0.95 on MDRF dataset. Moreover, our model shows superior performance\non two more datasets named LSGT and PsnoD. Additionally, a case study on the\npredicted snoRNA-disease associations verified the top 10 predicted snoRNAs\nacross nine prevalent diseases, further validating the efficacy of the GBDTSVM\napproach. These results underscore the model's potential as a robust tool for\nadvancing snoRNA-related disease research. Source codes and datasets our\nproposed framework can be obtained from: https://github.com/mariamuna04/gbdtsvm"}
{"id": "2505.07672", "pdf": "https://arxiv.org/pdf/2505.07672", "abs": "https://arxiv.org/abs/2505.07672", "authors": ["Arun S. Maiya"], "title": "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages", "summary": "We present OnPrem.LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,\nand Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem.LLM also supports integration with a wide range of cloud LLM\nproviders when permitted, enabling hybrid deployments that balance performance\nwith data control. A no-code web interface extends accessibility to\nnon-technical users."}
{"id": "2505.06948", "pdf": "https://arxiv.org/pdf/2505.06948", "abs": "https://arxiv.org/abs/2505.06948", "authors": ["Pan Du", "Wangbo Zhao", "Xinai Lu", "Nian Liu", "Zhikai Li", "Chaoyu Gong", "Suyun Zhao", "Hong Chen", "Cuiping Li", "Kai Wang", "Yang You"], "title": "Unsupervised Learning for Class Distribution Mismatch", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Class distribution mismatch (CDM) refers to the discrepancy between class\ndistributions in training data and target tasks. Previous methods address this\nby designing classifiers to categorize classes known during training, while\ngrouping unknown or new classes into an \"other\" category. However, they focus\non semi-supervised scenarios and heavily rely on labeled data, limiting their\napplicability and performance. To address this, we propose Unsupervised\nLearning for Class Distribution Mismatch (UCDM), which constructs\npositive-negative pairs from unlabeled data for classifier training. Our\napproach randomly samples images and uses a diffusion model to add or erase\nsemantic classes, synthesizing diverse training pairs. Additionally, we\nintroduce a confidence-based labeling mechanism that iteratively assigns\npseudo-labels to valuable real-world data and incorporates them into the\ntraining process. Extensive experiments on three datasets demonstrate UCDM's\nsuperiority over previous semi-supervised methods. Specifically, with a 60%\nmismatch proportion on Tiny-ImageNet dataset, our approach, without relying on\nlabeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,\nand 72.5% in classifying known, unknown, and new classes."}
{"id": "2505.06264", "pdf": "https://arxiv.org/pdf/2505.06264", "abs": "https://arxiv.org/abs/2505.06264", "authors": ["Santhakumar Ramamoorthy", "Priya Rani", "James Mahon", "Glenn Mathews", "Shaun Cloherty", "Mahdi Babaei"], "title": "Prediction of Delirium Risk in Mild Cognitive Impairment Using Time-Series data, Machine Learning and Comorbidity Patterns -- A Retrospective Study", "categories": ["stat.AP", "cs.AI"], "comment": null, "summary": "Delirium represents a significant clinical concern characterized by high\nmorbidity and mortality rates, particularly in patients with mild cognitive\nimpairment (MCI). This study investigates the associated risk factors for\ndelirium by analyzing the comorbidity patterns relevant to MCI and developing a\nlongitudinal predictive model leveraging machine learning methodologies. A\nretrospective analysis utilizing the MIMIC-IV v2.2 database was performed to\nevaluate comorbid conditions, survival probabilities, and predictive modeling\noutcomes. The examination of comorbidity patterns identified distinct risk\nprofiles for the MCI population. Kaplan-Meier survival analysis demonstrated\nthat individuals with MCI exhibit markedly reduced survival probabilities when\ndeveloping delirium compared to their non-MCI counterparts, underscoring the\nheightened vulnerability within this cohort. For predictive modeling, a Long\nShort-Term Memory (LSTM) ML network was implemented utilizing time-series data,\ndemographic variables, Charlson Comorbidity Index (CCI) scores, and an array of\ncomorbid conditions. The model demonstrated robust predictive capabilities with\nan AUROC of 0.93 and an AUPRC of 0.92. This study underscores the critical role\nof comorbidities in evaluating delirium risk and highlights the efficacy of\ntime-series predictive modeling in pinpointing patients at elevated risk for\ndelirium development."}
{"id": "2505.06542", "pdf": "https://arxiv.org/pdf/2505.06542", "abs": "https://arxiv.org/abs/2505.06542", "authors": ["Adèle H. Ribeiro", "Dominik Heider"], "title": "dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "31 pages. This work has been submitted to the IEEE for possible\n  publication", "summary": "Causal discovery is central to inferring causal relationships from\nobservational data. In the presence of latent confounding, algorithms such as\nFast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing\nthe true model's Markov Equivalence Class. However, their correctness\ncritically depends on empirical faithfulness, the assumption that observed\n(in)dependencies perfectly reflect those of the underlying causal model, which\noften fails in practice due to limited sample sizes. To address this, we\nintroduce the first nonparametric score to assess a PAG's compatibility with\nobserved data, even with mixed variable types. This score is both necessary and\nsufficient to characterize structural uncertainty and distinguish between\ndistinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid\ncausal discovery algorithm to jointly address latent confounding, empirical\nunfaithfulness, and mixed data types. dcFCI integrates our score into an\n(Anytime)FCI-guided search that systematically explores, ranks, and validates\ncandidate PAGs. Experiments on synthetic and real-world scenarios demonstrate\nthat dcFCI significantly outperforms state-of-the-art methods, often recovering\nthe true PAG even in small and heterogeneous datasets. Examining top-ranked\nPAGs further provides valuable insights into structural uncertainty, supporting\nmore robust and informed causal reasoning and decision-making."}
{"id": "2505.07705", "pdf": "https://arxiv.org/pdf/2505.07705", "abs": "https://arxiv.org/abs/2505.07705", "authors": ["Letian Peng", "Jingbo Shang"], "title": "Codifying Character Logic in Role-Playing", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces Codified Profiles for role-playing, a novel approach\nthat represents character logic as structured, executable functions for\nbehavioral decision-making. Each profile defines a set of functions\nparse_by_scene(scene) that outputs a list of logic-grounded assertions\ntriggered_statements, using both explicit control structures (e.g.,\nif-then-else) and condition checks like check_condition(scene, question), where\neach question is a semantically meaningful prompt about the scene (e.g., \"Is\nthe character in danger?\") discriminated by the role-playing LLM as true,\nfalse, or unknown. This explicit representation offers three key advantages\nover traditional prompt-based profiles, which append character descriptions\ndirectly into text prompts: (1) Persistence, by enforcing complete and\nconsistent execution of character logic, rather than relying on the model's\nimplicit reasoning; (2) Updatability, through systematic inspection and\nrevision of behavioral logic, which is difficult to track or debug in\nprompt-only approaches; (3) Controllable Randomness, by supporting stochastic\nbehavior directly within the logic, enabling fine-grained variability that\nprompting alone struggles to achieve. To validate these advantages, we\nintroduce a new benchmark constructed from 83 characters and 5,141 scenes\ncurated from Fandom, using NLI-based scoring to compare character responses\nagainst ground-truth actions. Our experiments demonstrate the significant\nbenefits of codified profiles in improving persistence, updatability, and\nbehavioral diversity. Notably, by offloading a significant portion of reasoning\nto preprocessing, codified profiles enable even 1B-parameter models to perform\nhigh-quality role-playing, providing a scalable and efficient foundation for\nlocal deployment of role-play agents."}
{"id": "2505.06951", "pdf": "https://arxiv.org/pdf/2505.06951", "abs": "https://arxiv.org/abs/2505.06951", "authors": ["Seokjun Kwon", "Jeongmin Shin", "Namil Kim", "Soonmin Hwang", "Yukyung Choi"], "title": "Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": "7 pages, 4 figures, International Conference on Robotics and\n  Automation(ICRA) 2025", "summary": "In autonomous driving, thermal image semantic segmentation has emerged as a\ncritical research area, owing to its ability to provide robust scene\nunderstanding under adverse visual conditions. In particular, unsupervised\ndomain adaptation (UDA) for thermal image segmentation can be an efficient\nsolution to address the lack of labeled thermal datasets. Nevertheless, since\nthese methods do not effectively utilize the complementary information between\nRGB and thermal images, they significantly decrease performance during domain\nadaptation. In this paper, we present a comprehensive study on cross-spectral\nUDA for thermal image semantic segmentation. We first propose a novel masked\nmutual learning strategy that promotes complementary information exchange by\nselectively transferring results between each spectral model while masking out\nuncertain regions. Additionally, we introduce a novel prototypical\nself-supervised loss designed to enhance the performance of the thermal\nsegmentation model in nighttime scenarios. This approach addresses the\nlimitations of RGB pre-trained networks, which cannot effectively transfer\nknowledge under low illumination due to the inherent constraints of RGB\nsensors. In experiments, our method achieves higher performance over previous\nUDA methods and comparable performance to state-of-the-art supervised methods."}
{"id": "2505.06267", "pdf": "https://arxiv.org/pdf/2505.06267", "abs": "https://arxiv.org/abs/2505.06267", "authors": ["Ilyas Oulkadda", "Julien Perez"], "title": "AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) for code generation,\nexemplified by GitHub Copilot\\footnote{A coding extension powered by a Code-LLM\nto assist in code completion tasks} surpassing a million users, highlights the\ntransformative potential of these tools in improving developer productivity.\nHowever, this rapid growth also underscores critical concerns regarding the\nquality, safety, and reliability of the code they generate. As Code-LLMs\nevolve, they face significant challenges, including the diminishing returns of\nmodel scaling and the scarcity of new, high-quality training data. To address\nthese issues, this paper introduces Adversarial Knowledge Distillation (AKD), a\nnovel approach that leverages adversarially generated synthetic datasets to\ndistill the capabilities of larger models into smaller, more efficient ones. By\nsystematically stress-testing and refining the reasoning capabilities of\nCode-LLMs, AKD provides a framework for enhancing model robustness,\nreliability, and security while improving their parameter-efficiency. We\nbelieve this work represents a critical step toward ensuring dependable\nautomated code generation within the constraints of existing data and the\ncost-efficiency of model execution."}
{"id": "2505.06549", "pdf": "https://arxiv.org/pdf/2505.06549", "abs": "https://arxiv.org/abs/2505.06549", "authors": ["Matthias Chung", "Bas Peters", "Michael Solomon"], "title": "Good Things Come in Pairs: Paired Autoencoders for Inverse Problems", "categories": ["cs.LG", "stat.ML", "68T99"], "comment": "43 pages, 17 figures", "summary": "In this book chapter, we discuss recent advances in data-driven approaches\nfor inverse problems. In particular, we focus on the \\emph{paired autoencoder}\nframework, which has proven to be a powerful tool for solving inverse problems\nin scientific computing. The paired autoencoder framework is a novel approach\nthat leverages the strengths of both data-driven and model-based methods by\nprojecting both the data and the quantity of interest into a latent space and\nmapping these latent spaces to provide surrogate forward and inverse mappings.\nWe illustrate the advantages of this approach through numerical experiments,\nincluding seismic imaging and classical inpainting: nonlinear and linear\ninverse problems, respectively. Although the paired autoencoder framework is\nlikelihood-free, it generates multiple data- and model-based reconstruction\nmetrics that help assess whether examples are in or out of distribution. In\naddition to direct model estimates from data, the paired autoencoder enables\nlatent-space refinement to fit the observed data accurately. Numerical\nexperiments show that this procedure, combined with the latent-space initial\nguess, is essential for high-quality estimates, even when data noise exceeds\nthe training regime. We also introduce two novel variants that combine\nvariational and paired autoencoder ideas, maintaining the original benefits\nwhile enabling sampling for uncertainty analysis."}
{"id": "2505.07775", "pdf": "https://arxiv.org/pdf/2505.07775", "abs": "https://arxiv.org/abs/2505.07775", "authors": ["Nimet Beyza Bozdag", "Shuhaib Mehri", "Xiaocheng Yang", "Hyeonjeong Ha", "Zirui Cheng", "Esin Durmus", "Jiaxuan You", "Heng Ji", "Gokhan Tur", "Dilek Hakkani-Tür"], "title": "Must Read: A Systematic Survey of Computational Persuasion", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Persuasion is a fundamental aspect of communication, influencing\ndecision-making across diverse contexts, from everyday conversations to\nhigh-stakes scenarios such as politics, marketing, and law. The rise of\nconversational AI systems has significantly expanded the scope of persuasion,\nintroducing both opportunities and risks. AI-driven persuasion can be leveraged\nfor beneficial applications, but also poses threats through manipulation and\nunethical influence. Moreover, AI systems are not only persuaders, but also\nsusceptible to persuasion, making them vulnerable to adversarial attacks and\nbias reinforcement. Despite rapid advancements in AI-generated persuasive\ncontent, our understanding of what makes persuasion effective remains limited\ndue to its inherently subjective and context-dependent nature. In this survey,\nwe provide a comprehensive overview of computational persuasion, structured\naround three key perspectives: (1) AI as a Persuader, which explores\nAI-generated persuasive content and its applications; (2) AI as a Persuadee,\nwhich examines AI's susceptibility to influence and manipulation; and (3) AI as\na Persuasion Judge, which analyzes AI's role in evaluating persuasive\nstrategies, detecting manipulation, and ensuring ethical persuasion. We\nintroduce a taxonomy for computational persuasion research and discuss key\nchallenges, including evaluating persuasiveness, mitigating manipulative\npersuasion, and developing responsible AI-driven persuasive systems. Our survey\noutlines future research directions to enhance the safety, fairness, and\neffectiveness of AI-powered persuasion while addressing the risks posed by\nincreasingly capable language models."}
{"id": "2505.06975", "pdf": "https://arxiv.org/pdf/2505.06975", "abs": "https://arxiv.org/abs/2505.06975", "authors": ["Wei Shang", "Dongwei Ren", "Wanying Zhang", "Pengfei Zhu", "Qinghua Hu", "Wangmeng Zuo"], "title": "High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution", "categories": ["cs.CV", "I.4.3"], "comment": "10 pages, 6 figures, 5 tables", "summary": "The primary challenge in accelerating image super-resolution lies in reducing\ncomputation while maintaining performance and adaptability. Motivated by the\nobservation that high-frequency regions (e.g., edges and textures) are most\ncritical for reconstruction, we propose a training-free adaptive masking module\nfor acceleration that dynamically focuses computation on these challenging\nareas. Specifically, our method first extracts high-frequency components via\nGaussian blur subtraction and adaptively generates binary masks using K-means\nclustering to identify regions requiring intensive processing. Our method can\nbe easily integrated with both CNNs and Transformers. For CNN-based\narchitectures, we replace standard $3 \\times 3$ convolutions with an unfold\noperation followed by $1 \\times 1$ convolutions, enabling pixel-wise sparse\ncomputation guided by the mask. For Transformer-based models, we partition the\nmask into non-overlapping windows and selectively process tokens based on their\naverage values. During inference, unnecessary pixels or windows are pruned,\nsignificantly reducing computation. Moreover, our method supports\ndilation-based mask adjustment to control the processing scope without\nretraining, and is robust to unseen degradations (e.g., noise, compression).\nExtensive experiments on benchmarks demonstrate that our method reduces FLOPs\nby 24--43% for state-of-the-art models (e.g., CARN, SwinIR) while achieving\ncomparable or better quantitative metrics. The source code is available at\nhttps://github.com/shangwei5/AMSR"}
{"id": "2505.06581", "pdf": "https://arxiv.org/pdf/2505.06581", "abs": "https://arxiv.org/abs/2505.06581", "authors": ["Chao Yan"], "title": "An \\tilde{O}ptimal Differentially Private Learner for Concept Classes with VC Dimension 1", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "We present the first nearly optimal differentially private PAC learner for\nany concept class with VC dimension 1 and Littlestone dimension $d$. Our\nalgorithm achieves the sample complexity of\n$\\tilde{O}_{\\varepsilon,\\delta,\\alpha,\\delta}(\\log^* d)$, nearly matching the\nlower bound of $\\Omega(\\log^* d)$ proved by Alon et al. [STOC19]. Prior to our\nwork, the best known upper bound is $\\tilde{O}(VC\\cdot d^5)$ for general VC\nclasses, as shown by Ghazi et al. [STOC21]."}
{"id": "2505.07784", "pdf": "https://arxiv.org/pdf/2505.07784", "abs": "https://arxiv.org/abs/2505.07784", "authors": ["Da Ju", "Hagen Blix", "Adina Williams"], "title": "Domain Regeneration: How well do LLMs match syntactic properties of text domains?", "categories": ["cs.CL"], "comment": null, "summary": "Recent improvement in large language model performance have, in all\nlikelihood, been accompanied by improvement in how well they can approximate\nthe distribution of their training data. In this work, we explore the following\nquestion: which properties of text domains do LLMs faithfully approximate, and\nhow well do they do so? Applying observational approaches familiar from corpus\nlinguistics, we prompt a commonly used, opensource LLM to regenerate text from\ntwo domains of permissively licensed English text which are often contained in\nLLM training data -- Wikipedia and news text. This regeneration paradigm allows\nus to investigate whether LLMs can faithfully match the original human text\ndomains in a fairly semantically-controlled setting. We investigate varying\nlevels of syntactic abstraction, from more simple properties like sentence\nlength, and article readability, to more complex and higher order properties\nsuch as dependency tag distribution, parse depth, and parse complexity. We find\nthat the majority of the regenerated distributions show a shifted mean, a lower\nstandard deviation, and a reduction of the long tail, as compared to the human\noriginals."}
{"id": "2505.06982", "pdf": "https://arxiv.org/pdf/2505.06982", "abs": "https://arxiv.org/abs/2505.06982", "authors": ["Md. Naimur Asif Borno", "Md Sakib Hossain Shovon", "MD Hanif Sikder", "Iffat Firozy Rimi", "Tahani Jaser Alahmadi", "Mohammad Ali Moni"], "title": "Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in image-based medical disease detection encounters\nchallenges such as limited annotated data sets, inadequate spatial feature\nanalysis, data security issues, and inefficient training frameworks. This study\nintroduces a data-efficient image transformer (DeIT)-based approach that\novercomes these challenges by utilizing multiscale patch embedding for better\nfeature extraction and stratified weighted random sampling to address class\nimbalance. The model also incorporates a LoRA-enhanced transformer encoder, a\ndistillation framework, and federated learning for decentralized training,\nimproving both efficiency and data security. Consequently, it achieves\nstate-of-the-art performance, with the highest AUC, F1 score, precision,\nminimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations\nimprove interpretability by highlighting critical pathological regions,\nenhancing the model's clinical relevance. These results highlight the potential\nof this approach to advance AI-powered medical imaging and disease detection."}
{"id": "2505.06277", "pdf": "https://arxiv.org/pdf/2505.06277", "abs": "https://arxiv.org/abs/2505.06277", "authors": ["John Song", "Lihao Zhang", "Feng Ye", "Haijian Sun"], "title": "Terahertz Spatial Wireless Channel Modeling with Radio Radiance Field", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.NI"], "comment": "submitted to IEEE conferences", "summary": "Terahertz (THz) communication is a key enabler for 6G systems, offering\nultra-wide bandwidth and unprecedented data rates. However, THz signal\npropagation differs significantly from lower-frequency bands due to severe free\nspace path loss, minimal diffraction and specular reflection, and prominent\nscattering, making conventional channel modeling and pilot-based estimation\napproaches inefficient. In this work, we investigate the feasibility of\napplying radio radiance field (RRF) framework to the THz band. This method\nreconstructs a continuous RRF using visual-based geometry and sparse THz RF\nmeasurements, enabling efficient spatial channel state information\n(Spatial-CSI) modeling without dense sampling. We first build a fine simulated\nTHz scenario, then we reconstruct the RRF and evaluate the performance in terms\nof both reconstruction quality and effectiveness in THz communication, showing\nthat the reconstructed RRF captures key propagation paths with sparse training\nsamples. Our findings demonstrate that RRF modeling remains effective in the\nTHz regime and provides a promising direction for scalable, low-cost spatial\nchannel reconstruction in future 6G networks."}
{"id": "2505.06597", "pdf": "https://arxiv.org/pdf/2505.06597", "abs": "https://arxiv.org/abs/2505.06597", "authors": ["Ibrahim Talha Ersoy", "Karoline Wiesner"], "title": "Geometry of Learning -- L2 Phase Transitions in Deep and Shallow Neural Networks", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "physics.data-an"], "comment": null, "summary": "When neural networks (NNs) are subject to L2 regularization, increasing the\nregularization strength beyond a certain threshold pushes the model into an\nunder-parameterization regime. This transition manifests as a first-order phase\ntransition in single-hidden-layer NNs and a second-order phase transition in\nNNs with two or more hidden layers. This paper establishes a unified framework\nfor such transitions by integrating the Ricci curvature of the loss landscape\nwith regularizer-driven deep learning. First, we show that a curvature\nchange-point separates the model-accuracy regimes in the onset of learning and\nthat it is identical to the critical point of the phase transition driven by\nregularization. Second, we show that for more complex data sets additional\nphase transitions exist between model accuracies, and that they are again\nidentical to curvature change points in the error landscape. Third, by studying\nthe MNIST data set using a Variational Autoencoder, we demonstrate that the\ncurvature change points identify phase transitions in model accuracy outside\nthe L2 setting. Our framework also offers practical insights for optimizing\nmodel performance across various architectures and datasets. By linking\ngeometric features of the error landscape to observable phase transitions, our\nwork paves the way for more informed regularization strategies and potentially\nnew methods for probing the intrinsic structure of neural networks beyond the\nL2 context."}
{"id": "2505.07787", "pdf": "https://arxiv.org/pdf/2505.07787", "abs": "https://arxiv.org/abs/2505.07787", "authors": ["Tongxu Luo", "Wenyu Du", "Jiaxi Bi", "Stephen Chung", "Zhengyang Tang", "Hao Yang", "Min Zhang", "Benyou Wang"], "title": "Learning from Peers in Reasoning Models", "categories": ["cs.CL"], "comment": "29 pages, 32 figures", "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they\nmake mistakes in their reasoning paths. However, our study reveals that when\nthe reasoning process starts with a short but poor beginning, it becomes\ndifficult for the model to recover. We refer to this phenomenon as the \"Prefix\nDominance Trap\". Inspired by psychological findings that peer interaction can\npromote self-correction without negatively impacting already accurate\nindividuals, we propose **Learning from Peers** (LeaP) to address this\nphenomenon. Specifically, every tokens, each reasoning path summarizes its\nintermediate reasoning and shares it with others through a routing mechanism,\nenabling paths to incorporate peer insights during inference. However, we\nobserve that smaller models sometimes fail to follow summarization and\nreflection instructions effectively. To address this, we fine-tune them into\nour **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,\nand GPQA Diamond show that LeaP provides substantial improvements. For\ninstance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the\nbaseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks\nwith an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches\nthe performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis\nreveals LeaP's robust error correction by timely peer insights, showing strong\nerror tolerance and handling varied task difficulty. LeaP marks a milestone by\nenabling LRMs to collaborate during reasoning. Our code, datasets, and models\nare available at https://learning-from-peers.github.io/ ."}
{"id": "2505.06985", "pdf": "https://arxiv.org/pdf/2505.06985", "abs": "https://arxiv.org/abs/2505.06985", "authors": ["Panwen Hu", "Jiehui Huang", "Qiang Sun", "Xiaodan Liang"], "title": "BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation", "categories": ["cs.CV"], "comment": null, "summary": "Both zero-shot and tuning-based customized text-to-image (CT2I) generation\nhave made significant progress for storytelling content creation. In contrast,\nresearch on customized text-to-video (CT2V) generation remains relatively\nlimited. Existing zero-shot CT2V methods suffer from poor generalization, while\nanother line of work directly combining tuning-based T2I models with temporal\nmotion modules often leads to the loss of structural and texture information.\nTo bridge this gap, we propose an autoregressive structure and texture\npropagation module (STPM), which extracts key structural and texture features\nfrom the reference subject and injects them autoregressively into each video\nframe to enhance consistency. Additionally, we introduce a test-time reward\noptimization (TTRO) method to further refine fine-grained details. Quantitative\nand qualitative experiments validate the effectiveness of STPM and TTRO,\ndemonstrating improvements of 7.8 and 13.1 in CLIP-I and DINO consistency\nmetrics over the baseline, respectively."}
{"id": "2505.06299", "pdf": "https://arxiv.org/pdf/2505.06299", "abs": "https://arxiv.org/abs/2505.06299", "authors": ["Spyridon Raptis", "Haralampos-G. Stratigopoulos"], "title": "Input-Specific and Universal Adversarial Attack Generation for Spiking Neural Networks in the Spiking Domain", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "As Spiking Neural Networks (SNNs) gain traction across various applications,\nunderstanding their security vulnerabilities becomes increasingly important. In\nthis work, we focus on the adversarial attacks, which is perhaps the most\nconcerning threat. An adversarial attack aims at finding a subtle input\nperturbation to fool the network's decision-making. We propose two novel\nadversarial attack algorithms for SNNs: an input-specific attack that crafts\nadversarial samples from specific dataset inputs and a universal attack that\ngenerates a reusable patch capable of inducing misclassification across most\ninputs, thus offering practical feasibility for real-time deployment. The\nalgorithms are gradient-based operating in the spiking domain proving to be\neffective across different evaluation metrics, such as adversarial accuracy,\nstealthiness, and generation time. Experimental results on two widely used\nneuromorphic vision datasets, NMNIST and IBM DVS Gesture, show that our\nproposed attacks surpass in all metrics all existing state-of-the-art methods.\nAdditionally, we present the first demonstration of adversarial attack\ngeneration in the sound domain using the SHD dataset."}
{"id": "2505.06621", "pdf": "https://arxiv.org/pdf/2505.06621", "abs": "https://arxiv.org/abs/2505.06621", "authors": ["Thamiris Coelho", "Leo S. F. Ribeiro", "João Macedo", "Jefersson A. dos Santos", "Sandra Avila"], "title": "Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models", "categories": ["cs.LG", "cs.CV"], "comment": "ACM Conference on Fairness, Accountability, and Transparency (FAccT\n  2025)", "summary": "The distribution of child sexual abuse imagery (CSAI) is an ever-growing\nconcern of our modern world; children who suffered from this heinous crime are\nrevictimized, and the growing amount of illegal imagery distributed overwhelms\nlaw enforcement agents (LEAs) with the manual labor of categorization. To ease\nthis burden researchers have explored methods for automating data triage and\ndetection of CSAI, but the sensitive nature of the data imposes restricted\naccess and minimal interaction between real data and learning algorithms,\navoiding leaks at all costs. In observing how these restrictions have shaped\nthe literature we formalize a definition of \"Proxy Tasks\", i.e., the substitute\ntasks used for training models for CSAI without making use of CSA data. Under\nthis new terminology we review current literature and present a protocol for\nmaking conscious use of Proxy Tasks together with consistent input from LEAs to\ndesign better automation in this field. Finally, we apply this protocol to\nstudy -- for the first time -- the task of Few-shot Indoor Scene Classification\non CSAI, showing a final model that achieves promising results on a real-world\nCSAI dataset whilst having no weights actually trained on sensitive data."}
{"id": "2505.07796", "pdf": "https://arxiv.org/pdf/2505.07796", "abs": "https://arxiv.org/abs/2505.07796", "authors": ["Xingjin Wang", "Howe Tissue", "Lu Wang", "Linjing Li", "Daniel Dajun Zeng"], "title": "Learning Dynamics in Continual Pre-Training for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ICML2025 (spotlight)", "summary": "Continual Pre-Training (CPT) has become a popular and effective method to\napply strong foundation models to specific downstream tasks. In this work, we\nexplore the learning dynamics throughout the CPT process for large language\nmodels. We specifically focus on how general and downstream domain performance\nevolves at each training step, with domain performance measured via validation\nlosses. We have observed that the CPT loss curve fundamentally characterizes\nthe transition from one curve to another hidden curve, and could be described\nby decoupling the effects of distribution shift and learning rate annealing. We\nderive a CPT scaling law that combines the two factors, enabling the prediction\nof loss at any (continual) training steps and across learning rate schedules\n(LRS) in CPT. Our formulation presents a comprehensive understanding of several\ncritical factors in CPT, including loss potential, peak learning rate, training\nsteps, replay ratio, etc. Moreover, our approach can be adapted to customize\ntraining hyper-parameters to different CPT goals such as balancing general and\ndomain-specific performance. Extensive experiments demonstrate that our scaling\nlaw holds across various CPT datasets and training hyper-parameters."}
{"id": "2505.06991", "pdf": "https://arxiv.org/pdf/2505.06991", "abs": "https://arxiv.org/abs/2505.06991", "authors": ["Chih-Chung Hsu", "I-Hsuan Wu", "Wen-Hai Tseng", "Ching-Heng Cheng", "Ming-Hsuan Wu", "Jin-Hui Jiang", "Yu-Jou Hsiao"], "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "This report presents our semantic segmentation framework developed by team\nACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which\nfocuses on parsing outdoor scenes into nine semantic categories under\nreal-world conditions. Our method integrates a Swin Transformer backbone\nenhanced with Rotary Position Embedding (RoPE) for improved spatial\ngeneralization, alongside a Color Shift Estimation-and-Correction module\ndesigned to compensate for illumination inconsistencies in natural\nenvironments. To further improve training stability, we adopt a quantile-based\ndenoising strategy that downweights the top 2.5\\% of highest-error pixels,\ntreating them as noise and suppressing their influence during optimization.\nEvaluated on the official GOOSE test set, our approach achieved a mean\nIntersection over Union (mIoU) of 0.848, demonstrating the effectiveness of\ncombining color correction, positional encoding, and error-aware denoising in\nrobust semantic segmentation."}
{"id": "2505.06305", "pdf": "https://arxiv.org/pdf/2505.06305", "abs": "https://arxiv.org/abs/2505.06305", "authors": ["Haowei Yang", "Qingyi Lu", "Yang Wang", "Sibei Liu", "Jiayun Zheng", "Ao Xiang"], "title": "User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "With the widespread application of large language models (LLMs), user privacy\nprotection has become a significant research topic. Existing privacy preference\nmodeling methods often rely on large-scale user data, making effective privacy\npreference analysis challenging in data-limited environments. This study\nexplores how LLMs can analyze user behavior related to privacy protection in\nscenarios with limited data and proposes a method that integrates Few-shot\nLearning and Privacy Computing to model user privacy preferences. The research\nutilizes anonymized user privacy settings data, survey responses, and simulated\ndata, comparing the performance of traditional modeling approaches with\nLLM-based methods. Experimental results demonstrate that, even with limited\ndata, LLMs significantly improve the accuracy of privacy preference modeling.\nAdditionally, incorporating Differential Privacy and Federated Learning further\nreduces the risk of user data exposure. The findings provide new insights into\nthe application of LLMs in privacy protection and offer theoretical support for\nadvancing privacy computing and user behavior analysis."}
{"id": "2505.06651", "pdf": "https://arxiv.org/pdf/2505.06651", "abs": "https://arxiv.org/abs/2505.06651", "authors": ["Zehan Zhu", "Yan Huang", "Xin Wang", "Shouling Ji", "Jinming Xu"], "title": "Dyn-D$^2$P: Dynamic Differentially Private Decentralized Learning with Provable Utility Guarantee", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted by the 34th International Joint\n  Conference on Artificial Intelligence(IJCAI 2025)", "summary": "Most existing decentralized learning methods with differential privacy (DP)\nguarantee rely on constant gradient clipping bounds and fixed-level DP Gaussian\nnoises for each node throughout the training process, leading to a significant\naccuracy degradation compared to non-private counterparts. In this paper, we\npropose a new Dynamic Differentially Private Decentralized learning approach\n(termed Dyn-D$^2$P) tailored for general time-varying directed networks.\nLeveraging the Gaussian DP (GDP) framework for privacy accounting, Dyn-D$^2$P\ndynamically adjusts gradient clipping bounds and noise levels based on gradient\nconvergence. This proposed dynamic noise strategy enables us to enhance model\naccuracy while preserving the total privacy budget. Extensive experiments on\nbenchmark datasets demonstrate the superiority of Dyn-D$^2$P over its\ncounterparts employing fixed-level noises, especially under strong privacy\nguarantees. Furthermore, we provide a provable utility bound for Dyn-D$^2$P\nthat establishes an explicit dependency on network-related parameters, with a\nscaling factor of $1/\\sqrt{n}$ in terms of the number of nodes $n$ up to a bias\nerror term induced by gradient clipping. To our knowledge, this is the first\nmodel utility analysis for differentially private decentralized non-convex\noptimization with dynamic gradient clipping bounds and noise levels."}
{"id": "2505.07809", "pdf": "https://arxiv.org/pdf/2505.07809", "abs": "https://arxiv.org/abs/2505.07809", "authors": ["Máté Gedeon"], "title": "A Comparative Analysis of Static Word Embeddings for Hungarian", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a comprehensive analysis of various static word\nembeddings for Hungarian, including traditional models such as Word2Vec,\nFastText, as well as static embeddings derived from BERT-based models using\ndifferent extraction methods. We evaluate these embeddings on both intrinsic\nand extrinsic tasks to provide a holistic view of their performance. For\nintrinsic evaluation, we employ a word analogy task, which assesses the\nembeddings ability to capture semantic and syntactic relationships. Our results\nindicate that traditional static embeddings, particularly FastText, excel in\nthis task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among\nthe BERT-based models, the X2Static method for extracting static embeddings\ndemonstrates superior performance compared to decontextualized and aggregate\nmethods, approaching the effectiveness of traditional static embeddings. For\nextrinsic evaluation, we utilize a bidirectional LSTM model to perform Named\nEntity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results\nreveal that embeddings derived from dynamic models, especially those extracted\nusing the X2Static method, outperform purely static embeddings. Notably, ELMo\nembeddings achieve the highest accuracy in both NER and POS tagging tasks,\nunderscoring the benefits of contextualized representations even when used in a\nstatic form. Our findings highlight the continued relevance of static word\nembeddings in NLP applications and the potential of advanced extraction methods\nto enhance the utility of BERT-based models. This piece of research contributes\nto the understanding of embedding performance in the Hungarian language and\nprovides valuable insights for future developments in the field. The training\nscripts, evaluation codes, restricted vocabulary, and extracted embeddings will\nbe made publicly available to support further research and reproducibility."}
{"id": "2505.06995", "pdf": "https://arxiv.org/pdf/2505.06995", "abs": "https://arxiv.org/abs/2505.06995", "authors": ["Md. Naimur Asif Borno", "Md Sakib Hossain Shovon", "Asmaa Soliman Al-Moisheer", "Mohammad Ali Moni"], "title": "Replay-Based Continual Learning with Dual-Layered Distillation and a Streamlined U-Net for Efficient Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-to-image diffusion models are hindered by high\ncomputational demands, limiting accessibility and scalability. This paper\nintroduces KDC-Diff, a novel stable diffusion framework that enhances\nefficiency while maintaining image quality. KDC-Diff features a streamlined\nU-Net architecture with nearly half the parameters of the original U-Net\n(482M), significantly reducing model complexity. We propose a dual-layered\ndistillation strategy to ensure high-fidelity generation, transferring semantic\nand structural insights from a teacher to a compact student model while\nminimizing quality degradation. Additionally, replay-based continual learning\nis integrated to mitigate catastrophic forgetting, allowing the model to retain\nprior knowledge while adapting to new data. Despite operating under extremely\nlow computational resources, KDC-Diff achieves state-of-the-art performance on\nthe Oxford Flowers and Butterflies & Moths 100 Species datasets, demonstrating\ncompetitive metrics such as FID, CLIP, and LPIPS. Moreover, it significantly\nreduces inference time compared to existing models. These results establish\nKDC-Diff as a highly efficient and adaptable solution for text-to-image\ngeneration, particularly in computationally constrained environments."}
{"id": "2505.06307", "pdf": "https://arxiv.org/pdf/2505.06307", "abs": "https://arxiv.org/abs/2505.06307", "authors": ["Mingfei Zeng", "Ming Xie", "Xixi Zheng", "Chunhai Li", "Chuan Zhang", "Liehuang Zhu"], "title": "Large Language Model-driven Security Assistant for Internet of Things via Chain-of-Thought", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The rapid development of Internet of Things (IoT) technology has transformed\npeople's way of life and has a profound impact on both production and daily\nactivities. However, with the rapid advancement of IoT technology, the security\nof IoT devices has become an unavoidable issue in both research and\napplications. Although some efforts have been made to detect or mitigate IoT\nsecurity vulnerabilities, they often struggle to adapt to the complexity of IoT\nenvironments, especially when dealing with dynamic security scenarios. How to\nautomatically, efficiently, and accurately understand these vulnerabilities\nremains a challenge. To address this, we propose an IoT security assistant\ndriven by Large Language Model (LLM), which enhances the LLM's understanding of\nIoT security vulnerabilities and related threats. The aim of the ICoT method we\npropose is to enable the LLM to understand security issues by breaking down the\nvarious dimensions of security vulnerabilities and generating responses\ntailored to the user's specific needs and expertise level. By incorporating\nICoT, LLM can gradually analyze and reason through complex security scenarios,\nresulting in more accurate, in-depth, and personalized security recommendations\nand solutions. Experimental results show that, compared to methods relying\nsolely on LLM, our proposed LLM-driven IoT security assistant significantly\nimproves the understanding of IoT security issues through the ICoT approach and\nprovides personalized solutions based on the user's identity, demonstrating\nhigher accuracy and reliability."}
{"id": "2505.06653", "pdf": "https://arxiv.org/pdf/2505.06653", "abs": "https://arxiv.org/abs/2505.06653", "authors": ["Patrick Blumenberg", "Thomas Graave", "Tim Fingscheidt"], "title": "Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) demand extensive memory capacity during both\nfine-tuning and inference. To enable memory-efficient fine-tuning, existing\nmethods apply block-wise quantization techniques, such as NF4 and AF4, to the\nnetwork weights. We show that these quantization techniques incur suboptimal\nquantization errors. Therefore, as a first novelty, we propose an optimization\napproach for block-wise quantization. Using this method, we design a family of\nquantizers named 4-bit block-wise optimal float (BOF4), which consistently\nreduces the quantization error compared to both baseline methods. We provide\nboth a theoretical and a data-driven solution for the optimization process and\nprove their practical equivalence. Secondly, we propose a modification to the\nemployed normalization method based on the signed absolute block maximum\n(BOF4-S), enabling further reduction of the quantization error and empirically\nachieving less degradation in language modeling performance. Thirdly, we\nexplore additional variations of block-wise quantization methods applied to\nLLMs through an experimental study on the importance of accurately representing\nzero and large-amplitude weights on the one hand, and optimization towards\nvarious error metrics on the other hand. Lastly, we introduce a mixed-precision\nquantization strategy dubbed outlier-preserving quantization (OPQ) to address\nthe distributional mismatch induced by outlier weights in block-wise\nquantization. By storing outlier weights in 16-bit precision (OPQ) while\napplying BOF4-S, we achieve top performance among 4-bit block-wise quantization\ntechniques w.r.t. perplexity."}
{"id": "2505.06313", "pdf": "https://arxiv.org/pdf/2505.06313", "abs": "https://arxiv.org/abs/2505.06313", "authors": ["Bohdan M. Pavlyshenko"], "title": "AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The paper considers the use of GPT models with retrieval-augmented generation\n(RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity\nand NATO Article 5 trust opinion scores in different web sources: news sites\nfound via Google Search API, Youtube videos with comments, and Reddit\ndiscussions. A RAG approach using GPT-4.1 model was applied to analyse news\nwhere NATO related topics were discussed. Two levels of RAG analytics were\nused: on the first level, the GPT model generates qualitative news summaries\nand quantitative opinion scores using zero-shot prompts; on the second level,\nthe GPT model generates the summary of news summaries. Quantitative news\nopinion scores generated by the GPT model were analysed using Bayesian\nregression to get trend lines. The distributions found for the regression\nparameters make it possible to analyse an uncertainty in specified news opinion\nscore trends. Obtained results show a downward trend for analysed scores of\nopinion related to NATO unity.\n  This approach does not aim to conduct real political analysis; rather, it\nconsider AI based approaches which can be used for further analytics\n  as a part of a complex analytical approach. The obtained results demonstrate\nthat the use of GPT models for news analysis can give informative qualitative\nand quantitative analytics, providing important insights.\n  The dynamic model based on neural ordinary differential equations was\nconsidered for modelling public opinions. This approach makes it possible to\nanalyse different scenarios for evolving public opinions."}
{"id": "2505.07001", "pdf": "https://arxiv.org/pdf/2505.07001", "abs": "https://arxiv.org/abs/2505.07001", "authors": ["Bidur Khanal", "Sandesh Pokhrel", "Sanjay Bhandari", "Ramesh Rana", "Nikesh Shrestha", "Ram Bahadur Gurung", "Cristian Linte", "Angus Watson", "Yash Raj Shrestha", "Binod Bhattarai"], "title": "Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) are becoming increasingly popular in the\nmedical domain, bridging the gap between medical images and clinical language.\nExisting VLMs demonstrate an impressive ability to comprehend medical images\nand text queries to generate detailed, descriptive diagnostic medical reports.\nHowever, hallucination--the tendency to generate descriptions that are\ninconsistent with the visual content--remains a significant issue in VLMs, with\nparticularly severe implications in the medical field. To facilitate VLM\nresearch on gastrointestinal (GI) image analysis and study hallucination, we\ncurate a multimodal image-text GI dataset: Gut-VLM. This dataset is created\nusing a two-stage pipeline: first, descriptive medical reports of Kvasir-v2\nimages are generated using ChatGPT, which introduces some hallucinated or\nincorrect texts. In the second stage, medical experts systematically review\nthese reports, and identify and correct potential inaccuracies to ensure\nhigh-quality, clinically reliable annotations. Unlike traditional datasets that\ncontain only descriptive texts, our dataset also features tags identifying\nhallucinated sentences and their corresponding corrections. A common approach\nto reducing hallucination in VLM is to finetune the model on a small-scale,\nproblem-specific dataset. However, we take a different strategy using our\ndataset. Instead of finetuning the VLM solely for generating textual reports,\nwe finetune it to detect and correct hallucinations, an approach we call\nhallucination-aware finetuning. Our results show that this approach is better\nthan simply finetuning for descriptive report generation. Additionally, we\nconduct an extensive evaluation of state-of-the-art VLMs across several\nmetrics, establishing a benchmark. GitHub Repo:\nhttps://github.com/bhattarailab/Hallucination-Aware-VLM."}
{"id": "2505.06311", "pdf": "https://arxiv.org/pdf/2505.06311", "abs": "https://arxiv.org/abs/2505.06311", "authors": ["Tongyu Wen", "Chenglong Wang", "Xiyuan Yang", "Haoyu Tang", "Yueqi Xie", "Lingjuan Lyu", "Zhicheng Dou", "Fangzhao Wu"], "title": "Defending against Indirect Prompt Injection by Instruction Detection", "categories": ["cs.CR", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "The integration of Large Language Models (LLMs) with external sources is\nbecoming increasingly common, with Retrieval-Augmented Generation (RAG) being a\nprominent example. However, this integration introduces vulnerabilities of\nIndirect Prompt Injection (IPI) attacks, where hidden instructions embedded in\nexternal data can manipulate LLMs into executing unintended or harmful actions.\nWe recognize that the success of IPI attacks fundamentally relies in the\npresence of instructions embedded within external content, which can alter the\nbehavioral state of LLMs. Can effectively detecting such state changes help us\ndefend against IPI attacks? In this paper, we propose a novel approach that\ntakes external data as input and leverages the behavioral state of LLMs during\nboth forward and backward propagation to detect potential IPI attacks.\nSpecifically, we demonstrate that the hidden states and gradients from\nintermediate layers provide highly discriminative features for instruction\ndetection. By effectively combining these features, our approach achieves a\ndetection accuracy of 99.60\\% in the in-domain setting and 96.90\\% in the\nout-of-domain setting, while reducing the attack success rate to just 0.12\\% on\nthe BIPIA benchmark."}
{"id": "2505.06688", "pdf": "https://arxiv.org/pdf/2505.06688", "abs": "https://arxiv.org/abs/2505.06688", "authors": ["Jianxin Zhang", "Lianzi Jiang", "Xinyu Han", "Xiangrong Wang"], "title": "A Novel Framework for Significant Wave Height Prediction based on Adaptive Feature Extraction Time-Frequency Network", "categories": ["cs.LG"], "comment": null, "summary": "Precise forecasting of significant wave height (Hs) is essential for the\ndevelopment and utilization of wave energy. The challenges in predicting Hs\narise from its non-linear and non-stationary characteristics. The combination\nof decomposition preprocessing and machine learning models have demonstrated\nsignificant effectiveness in Hs prediction by extracting data features.\nHowever, decomposing the unknown data in the test set can lead to data leakage\nissues. To simultaneously achieve data feature extraction and prevent data\nleakage, a novel Adaptive Feature Extraction Time-Frequency Network (AFE-TFNet)\nis proposed to improve prediction accuracy and stability. It is encoder-decoder\nrolling framework. The encoder consists of two stages: feature extraction and\nfeature fusion. In the feature extraction stage, global and local frequency\ndomain features are extracted by combining Wavelet Transform (WT) and Fourier\nTransform (FT), and multi-scale frequency analysis is performed using Inception\nblocks. In the feature fusion stage, time-domain and frequency-domain features\nare integrated through dominant harmonic sequence energy weighting (DHSEW). The\ndecoder employed an advanced long short-term memory (LSTM) model. Hourly\nmeasured wind speed (Ws), dominant wave period (DPD), average wave period (APD)\nand Hs from three stations are used as the dataset, and the four metrics are\nemployed to evaluate the forecasting performance. Results show that AFE-TFNet\nsignificantly outperforms benchmark methods in terms of prediction accuracy.\nFeature extraction can significantly improve the prediction accuracy. DHSEW has\nsubstantially increased the accuracy of medium-term to long-term forecasting.\nThe prediction accuracy of AFE-TFNet does not demonstrate significant\nvariability with changes of rolling time window size. Overall, AFE-TFNet shows\nstrong potential for handling complex signal forecasting."}
{"id": "2505.06843", "pdf": "https://arxiv.org/pdf/2505.06843", "abs": "https://arxiv.org/abs/2505.06843", "authors": ["Zihan Guan", "Mengxuan Hu", "Ronghang Zhu", "Sheng Li", "Anil Vullikanti"], "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety", "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 13 figures", "summary": "Recent studies have uncovered a troubling vulnerability in the fine-tuning\nstage of large language models (LLMs): even fine-tuning on entirely benign\ndatasets can lead to a significant increase in the harmfulness of LLM outputs.\nBuilding on this finding, our red teaming study takes this threat one step\nfurther by developing a more effective attack. Specifically, we analyze and\nidentify samples within benign datasets that contribute most to safety\ndegradation, then fine-tune LLMs exclusively on these samples. We approach this\nproblem from an outlier detection perspective and propose Self-Inf-N, to detect\nand extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs\non 100 outlier samples selected by Self-Inf-N in the benign datasets severely\ncompromises LLM safety alignment. Extensive experiments across seven mainstream\nLLMs demonstrate that our attack exhibits high transferability across different\narchitectures and remains effective in practical scenarios. Alarmingly, our\nresults indicate that most existing mitigation strategies fail to defend\nagainst this attack, underscoring the urgent need for more robust alignment\nsafeguards. Codes are available at\nhttps://github.com/GuanZihan/Benign-Samples-Matter."}
{"id": "2505.07003", "pdf": "https://arxiv.org/pdf/2505.07003", "abs": "https://arxiv.org/abs/2505.07003", "authors": ["Peng Li", "Suizhi Ma", "Jialiang Chen", "Yuan Liu", "Chongyi Zhang", "Wei Xue", "Wenhan Luo", "Alla Sheffer", "Wenping Wang", "Yike Guo"], "title": "CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation", "categories": ["cs.CV"], "comment": "Siggraph 2025", "summary": "Recently, 3D generation methods have shown their powerful ability to automate\n3D model creation. However, most 3D generation methods only rely on an input\nimage or a text prompt to generate a 3D model, which lacks the control of each\ncomponent of the generated 3D model. Any modifications of the input image lead\nto an entire regeneration of the 3D models. In this paper, we introduce a new\nmethod called CMD that generates a 3D model from an input image while enabling\nflexible local editing of each component of the 3D model. In CMD, we formulate\nthe 3D generation as a conditional multiview diffusion model, which takes the\nexisting or known parts as conditions and generates the edited or added\ncomponents. This conditional multiview diffusion model not only allows the\ngeneration of 3D models part by part but also enables local editing of 3D\nmodels according to the local revision of the input image without changing\nother 3D parts. Extensive experiments are conducted to demonstrate that CMD\ndecomposes a complex 3D generation task into multiple components, improving the\ngeneration quality. Meanwhile, CMD enables efficient and flexible local editing\nof a 3D model by just editing one rendered image."}
{"id": "2505.06312", "pdf": "https://arxiv.org/pdf/2505.06312", "abs": "https://arxiv.org/abs/2505.06312", "authors": ["Pavel Naumov", "Jia Tao"], "title": "Responsibility Gap in Collective Decision Making", "categories": ["cs.GT", "cs.AI"], "comment": "full version of an IJCAI-25 paper", "summary": "The responsibility gap is a set of outcomes of a collective decision-making\nmechanism in which no single agent is individually responsible. In general,\nwhen designing a decision-making process, it is desirable to minimise the gap.\n  The paper proposes a concept of an elected dictatorship. It shows that, in a\nperfect information setting, the gap is empty if and only if the mechanism is\nan elected dictatorship. It also proves that in an imperfect information\nsetting, the class of gap-free mechanisms is positioned strictly between two\nvariations of the class of elected dictatorships."}
{"id": "2505.06690", "pdf": "https://arxiv.org/pdf/2505.06690", "abs": "https://arxiv.org/abs/2505.06690", "authors": ["Jianxin Zhang", "Lianzi Jiang", "Xinyu Han", "Xiangrong Wang", "Weinan Huang"], "title": "E2E-FANet: A Highly Generalizable Framework for Waves prediction Behind Floating Breakwaters via Exogenous-to-Endogenous Variable Attention", "categories": ["cs.LG"], "comment": null, "summary": "Accurate prediction of waves behind floating breakwaters (FB) is crucial for\noptimizing coastal engineering structures, enhancing safety, and improving\ndesign efficiency. Existing methods demonstrate limitations in capturing\nnonlinear interactions between waves and structures, while exhibiting\ninsufficient capability in modeling the complex frequency-domain relationships\namong elevations of different wave gauges. To address these challenges, this\nstudy introduces the Exogenous-to-Endogenous Frequency-Aware Network\n(E2E-FANet), a novel end-to-end neural network designed to model relationships\nbetween waves and structures. The E2E-FANetarchitecture incorporates a\nDual-Basis Frequency Mapping (DBFM) module that leverages orthogonal cosine and\nsine bases to extract wave features from the frequency domain while preserving\ntemporal information. Additionally, we introduce the Exogenous-to-Endogenous\nCross-Attention (E2ECA) module, which employs cross attention to model the\ninteractions between endogenous and exogenous variables. We incorporate a\nTemporal-wise Attention (TA) mechanism that adaptively captures complex\ndependencies in endogenous variables. These integrated modules function\nsynergistically, enabling E2E-FANet to achieve both comprehensive feature\nperception in the time-frequency domain and precise modeling of wave-structure\ninteractions. To comprehensively evaluate the performance of E2E-FANet, we\nconstructed a multi-level validation framework comprising three distinct\ntesting scenarios: internal validation under identical wave conditions,\ngeneralization testing across different wave conditions, and adaptability\ntesting with varying relative water density (RW) conditions. These\ncomprehensive tests demonstrate that E2E-FANet provides accurate waves behind\nFB predictions while successfully generalizing diverse wave conditions."}
{"id": "2505.06938", "pdf": "https://arxiv.org/pdf/2505.06938", "abs": "https://arxiv.org/abs/2505.06938", "authors": ["Katarzyna Anna Kapitan"], "title": "A digital perspective on the role of a stemma in material-philological transmission studies", "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "Taking its point of departure in the recent developments in the field of\ndigital humanities and the increasing automatisation of scholarly workflows,\nthis study explores the implications of digital approaches to textual\ntraditions for the broader field of textual scholarship. It argues that the\nrelative simplicity of creating computergenerated stemmas allows us to view the\nstemma codicum as a research tool rather than the final product of our\nscholarly investigation. Using the Old Norse saga of Hr\\'omundur as a case\nstudy, this article demonstrates that stemmas can serve as a starting point for\nexploring textual traditions further. In doing so, they enable us to address\nresearch questions that otherwise remain unanswered. The article is accompanied\nby datasets used to generate stemmas for the Hr\\'omundar saga tradition as well\nas two custom Python scripts. The scripts are designed to convert XML-based\ntextual data, encoded according to the TEI Guidelines, into the input format\nused for the analysis in the PHYLIP package to generate unrooted trees of\nrelationships between texts."}
{"id": "2505.07007", "pdf": "https://arxiv.org/pdf/2505.07007", "abs": "https://arxiv.org/abs/2505.07007", "authors": ["Zhengye Zhang", "Sirui Zhao", "Shifeng Liu", "Shukang Yin", "Xinglong Mao", "Tong Xu", "Enhong Chen"], "title": "MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expressions (MEs) are crucial psychological responses with significant\npotential for affective computing. However, current automatic micro-expression\nrecognition (MER) research primarily focuses on discrete emotion\nclassification, neglecting a convincing analysis of the subtle dynamic\nmovements and inherent emotional cues. The rapid progress in multimodal large\nlanguage models (MLLMs), known for their strong multimodal comprehension and\nlanguage generation abilities, offers new possibilities. MLLMs have shown\nsuccess in various vision-language tasks, indicating their potential to\nunderstand MEs comprehensively, including both fine-grained motion patterns and\nunderlying emotional semantics. Nevertheless, challenges remain due to the\nsubtle intensity and short duration of MEs, as existing MLLMs are not designed\nto capture such delicate frame-level facial dynamics. In this paper, we propose\na novel Micro-Expression Large Language Model (MELLM), which incorporates a\nsubtle facial motion perception strategy with the strong inference capabilities\nof MLLMs, representing the first exploration of MLLMs in the domain of ME\nanalysis. Specifically, to explicitly guide the MLLM toward motion-sensitive\nregions, we construct an interpretable motion-enhanced color map by fusing\nonset-apex optical flow dynamics with the corresponding grayscale onset frame\nas the model input. Additionally, specialized fine-tuning strategies are\nincorporated to further enhance the model's visual perception of MEs.\nFurthermore, we construct an instruction-description dataset based on Facial\nAction Coding System (FACS) annotations and emotion labels to train our MELLM.\nComprehensive evaluations across multiple benchmark datasets demonstrate that\nour model exhibits superior robustness and generalization capabilities in ME\nunderstanding (MEU). Code is available at https://github.com/zyzhangUstc/MELLM."}
{"id": "2505.06314", "pdf": "https://arxiv.org/pdf/2505.06314", "abs": "https://arxiv.org/abs/2505.06314", "authors": ["Ashok Goel", "Ploy Thajchayapong", "Vrinda Nandan", "Harshvardhan Sikka", "Spencer Rugaber"], "title": "A4L: An Architecture for AI-Augmented Learning", "categories": ["cs.CY", "cs.AI"], "comment": "14 pages, 7 figures", "summary": "AI promises personalized learning and scalable education. As AI agents\nincreasingly permeate education in support of teaching and learning, there is a\ncritical and urgent need for data architectures for collecting and analyzing\ndata on learning, and feeding the results back to teachers, learners, and the\nAI agents for personalization of learning at scale. At the National AI\nInstitute for Adult Learning and Online Education, we are developing an\nArchitecture for AI-Augmented Learning (A4L) for supporting adult learning\nthrough online education. We present the motivations, goals, requirements of\nthe A4L architecture. We describe preliminary applications of A4L and discuss\nhow it advances the goals of making learning more personalized and scalable."}
{"id": "2505.06699", "pdf": "https://arxiv.org/pdf/2505.06699", "abs": "https://arxiv.org/abs/2505.06699", "authors": ["Xiyuan Wei", "Ming Lin", "Fanjiang Ye", "Fengguang Song", "Liangliang Cao", "My T. That", "Tianbao Yang"], "title": "Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws", "categories": ["cs.LG", "stat.ML"], "comment": "18 pages, 6 figures", "summary": "This paper formalizes an emerging learning paradigm that uses a trained model\nas a reference to guide and enhance the training of a target model through\nstrategic data selection or weighting, named $\\textbf{model steering}$. While\nad-hoc methods have been used in various contexts, including the training of\nlarge foundation models, its underlying principles remain insufficiently\nunderstood, leading to sub-optimal performance. In this work, we propose a\ntheory-driven framework for model steering called $\\textbf{DRRho risk\nminimization}$, which is rooted in Distributionally Robust Optimization (DRO).\nThrough a generalization analysis, we provide theoretical insights into why\nthis approach improves generalization and data efficiency compared to training\nwithout a reference model. To the best of our knowledge, this is the first time\nsuch theoretical insights are provided for the new learning paradigm, which\nsignificantly enhance our understanding and practice of model steering.\nBuilding on these insights and the connection between contrastive learning and\nDRO, we introduce a novel method for Contrastive Language-Image Pretraining\n(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments\nvalidate the theoretical insights, reveal a superior scaling law compared to\nCLIP without a reference model, and demonstrate its strength over existing\nheuristic approaches."}
{"id": "2505.06972", "pdf": "https://arxiv.org/pdf/2505.06972", "abs": "https://arxiv.org/abs/2505.06972", "authors": ["Yuichi Sasazawa", "Yasuhiro Sogawa"], "title": "Web Page Classification using LLMs for Crawling Support", "categories": ["cs.IR", "cs.CL"], "comment": "8 pages, 2 figures", "summary": "A web crawler is a system designed to collect web pages, and efficient\ncrawling of new pages requires appropriate algorithms. While website features\nsuch as XML sitemaps and the frequency of past page updates provide important\nclues for accessing new pages, their universal application across diverse\nconditions is challenging. In this study, we propose a method to efficiently\ncollect new pages by classifying web pages into two types, \"Index Pages\" and\n\"Content Pages,\" using a large language model (LLM), and leveraging the\nclassification results to select index pages as starting points for accessing\nnew pages. We construct a dataset with automatically annotated web page types\nand evaluate our approach from two perspectives: the page type classification\nperformance and coverage of new pages. Experimental results demonstrate that\nthe LLM-based method outperformed baseline methods in both evaluation metrics."}
{"id": "2505.07013", "pdf": "https://arxiv.org/pdf/2505.07013", "abs": "https://arxiv.org/abs/2505.07013", "authors": ["Jitesh Joshi", "Youngjun Cho"], "title": "Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages, 6 figures", "summary": "Remote physiological sensing using camera-based technologies offers\ntransformative potential for non-invasive vital sign monitoring across\nhealthcare and human-computer interaction domains. Although deep learning\napproaches have advanced the extraction of physiological signals from video\ndata, existing methods have not been sufficiently assessed for their robustness\nto domain shifts. These shifts in remote physiological sensing include\nvariations in ambient conditions, camera specifications, head movements, facial\nposes, and physiological states which often impact real-world performance\nsignificantly. Cross-dataset evaluation provides an objective measure to assess\ngeneralization capabilities across these domain shifts. We introduce Target\nSignal Constrained Factorization module (TSFM), a novel multidimensional\nattention mechanism that explicitly incorporates physiological signal\ncharacteristics as factorization constraints, allowing more precise feature\nextraction. Building on this innovation, we present MMRPhys, an efficient\ndual-branch 3D-CNN architecture designed for simultaneous multitask estimation\nof photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal\nRGB and thermal video inputs. Through comprehensive cross-dataset evaluation on\nfive benchmark datasets, we demonstrate that MMRPhys with TSFM significantly\noutperforms state-of-the-art methods in generalization across domain shifts for\nrPPG and rRSP estimation, while maintaining a minimal inference latency\nsuitable for real-time applications. Our approach establishes new benchmarks\nfor robust multitask and multimodal physiological sensing and offers a\ncomputationally efficient framework for practical deployment in unconstrained\nenvironments. The web browser-based application featuring on-device real-time\ninference of MMRPhys model is available at\nhttps://physiologicailab.github.io/mmrphys-live"}
{"id": "2505.06315", "pdf": "https://arxiv.org/pdf/2505.06315", "abs": "https://arxiv.org/abs/2505.06315", "authors": ["Jose Sanchez Vicarte", "Marcin Spoczynski", "Mostafa Elsaid"], "title": "Threat Modeling for AI: The Case for an Asset-Centric Approach", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advances in AI are transforming AI's ubiquitous presence in our world\nfrom that of standalone AI-applications into deeply integrated AI-agents. These\nchanges have been driven by agents' increasing capability to autonomously make\ndecisions and initiate actions, using existing applications; whether those\napplications are AI-based or not. This evolution enables unprecedented levels\nof AI integration, with agents now able to take actions on behalf of systems\nand users -- including, in some cases, the powerful ability for the AI to write\nand execute scripts as it deems necessary. With AI systems now able to\nautonomously execute code, interact with external systems, and operate without\nhuman oversight, traditional security approaches fall short.\n  This paper introduces an asset-centric methodology for threat modeling AI\nsystems that addresses the unique security challenges posed by integrated AI\nagents. Unlike existing top-down frameworks that analyze individual attacks\nwithin specific product contexts, our bottom-up approach enables defenders to\nsystematically identify how vulnerabilities -- both conventional and\nAI-specific -- impact critical AI assets across distributed infrastructures\nused to develop and deploy these agents. This methodology allows security teams\nto: (1) perform comprehensive analysis that communicates effectively across\ntechnical domains, (2) quantify security assumptions about third-party AI\ncomponents without requiring visibility into their implementation, and (3)\nholistically identify AI-based vulnerabilities relevant to their specific\nproduct context. This approach is particularly relevant for securing agentic\nsystems with complex autonomous capabilities. By focusing on assets rather than\nattacks, our approach scales with the rapidly evolving threat landscape while\naccommodating increasingly complex and distributed AI development pipelines."}
{"id": "2505.06709", "pdf": "https://arxiv.org/pdf/2505.06709", "abs": "https://arxiv.org/abs/2505.06709", "authors": ["Abhishek Sinha", "Rahul Vaze"], "title": "Beyond $\\tilde{O}(\\sqrt{T})$ Constraint Violation for Online Convex Optimization with Adversarial Constraints", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "We revisit the Online Convex Optimization problem with adversarial\nconstraints (COCO) where, in each round, a learner is presented with a convex\ncost function and a convex constraint function, both of which may be chosen\nadversarially. The learner selects actions from a convex decision set in an\nonline fashion, with the goal of minimizing both regret and the cumulative\nconstraint violation (CCV) over a horizon of $T$ rounds. The best-known policy\nfor this problem achieves $O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV.\nIn this paper, we present a surprising improvement that achieves a\nsignificantly smaller CCV by trading it off with regret. Specifically, for any\nbounded convex cost and constraint functions, we propose an online policy that\nachieves $\\tilde{O}(\\sqrt{dT}+ T^\\beta)$ regret and $\\tilde{O}(dT^{1-\\beta})$\nCCV, where $d$ is the dimension of the decision set and $\\beta \\in [0,1]$ is a\ntunable parameter. We achieve this result by first considering the special case\nof $\\textsf{Constrained Expert}$ problem where the decision set is a\nprobability simplex and the cost and constraint functions are linear.\nLeveraging a new adaptive small-loss regret bound, we propose an efficient\npolicy for the $\\textsf{Constrained Expert}$ problem, that attains\n$O(\\sqrt{T\\ln N}+T^{\\beta})$ regret and $\\tilde{O}(T^{1-\\beta} \\ln N)$ CCV,\nwhere $N$ is the number of experts. The original problem is then reduced to the\n$\\textsf{Constrained Expert}$ problem via a covering argument. Finally, with an\nadditional smoothness assumption, we propose an efficient gradient-based policy\nattaining $O(T^{\\max(\\frac{1}{2},\\beta)})$ regret and $\\tilde{O}(T^{1-\\beta})$\nCCV."}
{"id": "2505.06993", "pdf": "https://arxiv.org/pdf/2505.06993", "abs": "https://arxiv.org/abs/2505.06993", "authors": ["Yuxuan He", "Junpeng Zhang", "Hongyuan Zhang", "Quanshi Zhang"], "title": "Towards the Three-Phase Dynamics of Generalization Power of a DNN", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses."}
{"id": "2505.07019", "pdf": "https://arxiv.org/pdf/2505.07019", "abs": "https://arxiv.org/abs/2505.07019", "authors": ["Khang Nguyen Quoc", "Lan Le Thi Thu", "Luyl-Da Quach"], "title": "A Vision-Language Foundation Model for Leaf Disease Identification", "categories": ["cs.CV"], "comment": null, "summary": "Leaf disease identification plays a pivotal role in smart agriculture.\nHowever, many existing studies still struggle to integrate image and textual\nmodalities to compensate for each other's limitations. Furthermore, many of\nthese approaches rely on pretraining with constrained datasets such as\nImageNet, which lack domain-specific information. We propose SCOLD (Soft-target\nCOntrastive learning for Leaf Disease identification), a context-aware\nvision-language foundation model tailored to address these challenges for\nagricultural tasks. SCOLD is developed using a diverse corpus of plant leaf\nimages and corresponding symptom descriptions, comprising over 186,000\nimage-caption pairs aligned with 97 unique concepts. Through task-agnostic\npretraining, SCOLD leverages contextual soft targets to mitigate overconfidence\nin contrastive learning by smoothing labels, thereby improving model\ngeneralization and robustness on fine-grained classification tasks.\nExperimental results demonstrate that SCOLD outperforms existing\nvision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across\nseveral benchmarks, including zero-shot and few-shot classification, image-text\nretrieval, and image classification, while maintaining a competitive parameter\nfootprint. Ablation studies further highlight SCOLD's effectiveness in contrast\nto its counterparts. The proposed approach significantly advances the\nagricultural vision-language foundation model, offering strong performance with\nminimal or no supervised fine-tuning. This work lays a solid groundwork for\nfuture research on models trained with long-form and simplified contexts, tasks\ninvolving class ambiguity, and multi-modal systems for intelligent plant\ndisease diagnostics. The code for this study is available at\nhttps://huggingface.co/enalis/scold"}
{"id": "2505.06324", "pdf": "https://arxiv.org/pdf/2505.06324", "abs": "https://arxiv.org/abs/2505.06324", "authors": ["Vipula Rawte", "Ryan A. Rossi", "Franck Dernoncourt", "Nedim Lipka"], "title": "Document Attribution: Examining Citation Relationships using Large Language Models", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly applied to document-based\ntasks - such as document summarization, question answering, and information\nextraction - where user requirements focus on retrieving information from\nprovided documents rather than relying on the model's parametric knowledge,\nensuring the trustworthiness and interpretability of these systems has become a\ncritical concern. A central approach to addressing this challenge is\nattribution, which involves tracing the generated outputs back to their source\ndocuments. However, since LLMs can produce inaccurate or imprecise responses,\nit is crucial to assess the reliability of these citations.\n  To tackle this, our work proposes two techniques. (1) A zero-shot approach\nthat frames attribution as a straightforward textual entailment task. Our\nmethod using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the\nbest baseline of ID and OOD sets of AttributionBench, respectively. (2) We also\nexplore the role of the attention mechanism in enhancing the attribution\nprocess. Using a smaller LLM, flan-t5-small, the F1 scores outperform the\nbaseline across almost all layers except layer 4 and layers 8 through 11."}
{"id": "2505.06730", "pdf": "https://arxiv.org/pdf/2505.06730", "abs": "https://arxiv.org/abs/2505.06730", "authors": ["Debashish Saha", "Piyush Malik", "Adrika Saha"], "title": "Activity and Subject Detection for UCI HAR Dataset with & without missing Sensor Data", "categories": ["cs.LG"], "comment": null, "summary": "Current studies in Human Activity Recognition (HAR) primarily focus on the\nclassification of activities through sensor data, while there is not much\nemphasis placed on recognizing the individuals performing these activities.\nThis type of classification is very important for developing personalized and\ncontext-sensitive applications. Additionally, the issue of missing sensor data,\nwhich often occurs in practical situations due to hardware malfunctions, has\nnot been explored yet. This paper seeks to fill these voids by introducing a\nlightweight LSTM-based model that can be used to classify both activities and\nsubjects. The proposed model was used to classify the HAR dataset by UCI [1],\nachieving an accuracy of 93.89% in activity recognition (across six\nactivities), nearing the 96.67% benchmark, and an accuracy of 80.19% in subject\nrecognition (involving 30 subjects), thereby establishing a new baseline for\nthis area of research. We then simulate the absence of sensor data to mirror\nreal-world scenarios and incorporate imputation techniques, both with and\nwithout Principal Component Analysis (PCA), to restore incomplete datasets. We\nfound that K-Nearest Neighbors (KNN) imputation performs the best for filling\nthe missing sensor data without PCA because the use of PCA resulted in slightly\nlower accuracy. These results demonstrate how well the framework handles\nmissing sensor data, which is a major step forward in using the Human Activity\nRecognition dataset for reliable classification tasks."}
{"id": "2505.07155", "pdf": "https://arxiv.org/pdf/2505.07155", "abs": "https://arxiv.org/abs/2505.07155", "authors": ["Shuai Wang", "Harrisen Scells", "Bevan Koopman", "Guido Zuccon"], "title": "Reassessing Large Language Model Boolean Query Generation for Systematic Reviews", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted in SIGIR-2025", "summary": "Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain."}
{"id": "2505.07032", "pdf": "https://arxiv.org/pdf/2505.07032", "abs": "https://arxiv.org/abs/2505.07032", "authors": ["Fei Zhao", "Runlin Zhang", "Chengcui Zhang", "Nitesh Saxena"], "title": "MarkMatch: Same-Hand Stuffing Detection", "categories": ["cs.CV"], "comment": null, "summary": "We present MarkMatch, a retrieval system for detecting whether two paper\nballot marks were filled by the same hand. Unlike the previous SOTA method\nBubbleSig, which used binary classification on isolated mark pairs, MarkMatch\nranks stylistic similarity between a query mark and a mark in the database\nusing contrastive learning. Our model is trained with a dense batch similarity\nmatrix and a dual loss objective. Each sample is contrasted against many\nnegatives within each batch, enabling the model to learn subtle handwriting\ndifference and improve generalization under handwriting variation and visual\nnoise, while diagonal supervision reinforces high confidence on true matches.\nThe model achieves an F1 score of 0.943, surpassing BubbleSig's best\nperformance. MarkMatch also integrates Segment Anything Model for flexible mark\nextraction via box- or point-based prompts. The system offers election auditors\na practical tool for visual, non-biometric investigation of suspicious ballots."}
{"id": "2505.06326", "pdf": "https://arxiv.org/pdf/2505.06326", "abs": "https://arxiv.org/abs/2505.06326", "authors": ["Alexander Ettinger"], "title": "Enterprise Architecture as a Dynamic Capability for Scalable and Sustainable Generative AI adoption: Bridging Innovation and Governance in Large Organisations", "categories": ["cs.CY", "cs.AI"], "comment": "82 pages excluding appendix", "summary": "Generative Artificial Intelligence is a powerful new technology with the\npotential to boost innovation and reshape governance in many industries.\nNevertheless, organisations face major challenges in scaling GenAI, including\ntechnology complexity, governance gaps and resource misalignments. This study\nexplores how Enterprise Architecture Management can meet the complex\nrequirements of GenAI adoption within large enterprises. Based on a systematic\nliterature review and the qualitative analysis of 16 semi-structured interviews\nwith experts, it examines the relationships between EAM, dynamic capabilities\nand GenAI adoption. The review identified key limitations in existing EA\nframeworks, particularly their inability to fully address the unique\nrequirements of GenAI. The interviews, analysed using the Gioia methodology,\nrevealed critical enablers and barriers to GenAI adoption across industries.\nThe findings indicate that EAM, when theorised as sensing, seizing and\ntransforming dynamic capabilities, can enhance GenAI adoption by improving\nstrategic alignment, governance frameworks and organisational agility. However,\nthe study also highlights the need to tailor EA frameworks to GenAI-specific\nchallenges, including low data governance maturity and the balance between\ninnovation and compliance. Several conceptual frameworks are proposed to guide\nEA leaders in aligning GenAI maturity with organisational readiness. The work\ncontributes to academic understanding and industry practice by clarifying the\nrole of EA in bridging innovation and governance in disruptive technology\nenvironments."}
{"id": "2505.06731", "pdf": "https://arxiv.org/pdf/2505.06731", "abs": "https://arxiv.org/abs/2505.06731", "authors": ["David Zucker"], "title": "Deeply Explainable Artificial Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While deep learning models have demonstrated remarkable success in numerous\ndomains, their black-box nature remains a significant limitation, especially in\ncritical fields such as medical image analysis and inference. Existing\nexplainability methods, such as SHAP, LIME, and Grad-CAM, are typically applied\npost hoc, adding computational overhead and sometimes producing inconsistent or\nambiguous results. In this paper, we present the Deeply Explainable Artificial\nNeural Network (DxANN), a novel deep learning architecture that embeds\nexplainability ante hoc, directly into the training process. Unlike\nconventional models that require external interpretation methods, DxANN is\ndesigned to produce per-sample, per-feature explanations as part of the forward\npass. Built on a flow-based framework, it enables both accurate predictions and\ntransparent decision-making, and is particularly well-suited for image-based\ntasks. While our focus is on medical imaging, the DxANN architecture is readily\nadaptable to other data modalities, including tabular and sequential data.\nDxANN marks a step forward toward intrinsically interpretable deep learning,\noffering a practical solution for applications where trust and accountability\nare essential."}
{"id": "2505.07166", "pdf": "https://arxiv.org/pdf/2505.07166", "abs": "https://arxiv.org/abs/2505.07166", "authors": ["Zheng Yao", "Shuai Wang", "Guido Zuccon"], "title": "Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted in SIGIR-2025", "summary": "Dense retrievers utilize pre-trained backbone language models (e.g., BERT,\nLLaMA) that are fine-tuned via contrastive learning to perform the task of\nencoding text into sense representations that can be then compared via a\nshallow similarity operation, e.g. inner product. Recent research has\nquestioned the role of fine-tuning vs. that of pre-training within dense\nretrievers, specifically arguing that retrieval knowledge is primarily gained\nduring pre-training, meaning knowledge not acquired during pre-training cannot\nbe sub-sequentially acquired via fine-tuning. We revisit this idea here as the\nclaim was only studied in the context of a BERT-based encoder using DPR as\nrepresentative dense retriever. We extend the previous analysis by testing\nother representation approaches (comparing the use of CLS tokens with that of\nmean pooling), backbone architectures (encoder-only BERT vs. decoder-only\nLLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our\nstudy confirms that in DPR tuning, pre-trained knowledge underpins retrieval\nperformance, with fine-tuning primarily adjusting neuron activation rather than\nreorganizing knowledge. However, this pattern does not hold universally, such\nas in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full\nreproducibility and make our implementation publicly available at\nhttps://github.com/ielab/DenseRetriever-Knowledge-Acquisition."}
{"id": "2505.07040", "pdf": "https://arxiv.org/pdf/2505.07040", "abs": "https://arxiv.org/abs/2505.07040", "authors": ["Zhengyang Lu", "Bingjie Lu", "Weifan Wang", "Feng Wang"], "title": "Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection", "categories": ["cs.CV"], "comment": null, "summary": "Fabric defect detection confronts two fundamental challenges. First,\nconventional non-maximum suppression disrupts gradient flow, which hinders\ngenuine end-to-end learning. Second, acquiring pixel-level annotations at\nindustrial scale is prohibitively costly. Addressing these limitations, we\npropose a differentiable NMS framework for fabric defect detection that\nachieves superior localization precision through end-to-end optimization. We\nreformulate NMS as a differentiable bipartite matching problem solved through\nthe Sinkhorn-Knopp algorithm, maintaining uninterrupted gradient flow\nthroughout the network. This approach specifically targets the irregular\nmorphologies and ambiguous boundaries of fabric defects by integrating proposal\nquality, feature similarity, and spatial relationships. Our entropy-constrained\nmask refinement mechanism further enhances localization precision through\nprincipled uncertainty modeling. Extensive experiments on the Tianchi fabric\ndefect dataset demonstrate significant performance improvements over existing\nmethods while maintaining real-time speeds suitable for industrial deployment.\nThe framework exhibits remarkable adaptability across different architectures\nand generalizes effectively to general object detection tasks."}
{"id": "2505.06347", "pdf": "https://arxiv.org/pdf/2505.06347", "abs": "https://arxiv.org/abs/2505.06347", "authors": ["Qing-Hong Cao", "Zong-Yue Hou", "Ying-Ying Li", "Xiaohui Liu", "Zhuo-Yang Song", "Liang-Qi Zhang", "Shutao Zhang", "Ke Zhao"], "title": "Quantum State Preparation via Large-Language-Model-Driven Evolution", "categories": ["quant-ph", "cs.AI", "hep-lat", "hep-ph"], "comment": "6 + 4 pages, 14 figures", "summary": "We propose an automated framework for quantum circuit design by integrating\nlarge-language models (LLMs) with evolutionary optimization to overcome the\nrigidity, scalability limitations, and expert dependence of traditional ones in\nvariational quantum algorithms. Our approach (FunSearch) autonomously discovers\nhardware-efficient ans\\\"atze with new features of scalability and\nsystem-size-independent number of variational parameters entirely from scratch.\nDemonstrations on the Ising and XY spin chains with n = 9 qubits yield circuits\ncontaining 4 parameters, achieving near-exact energy extrapolation across\nsystem sizes. Implementations on quantum hardware (Zuchongzhi chip) validate\npracticality, where two-qubit quantum gate noises can be effectively mitigated\nvia zero-noise extrapolations for a spin chain system as large as 20 sites.\nThis framework bridges algorithmic design and experimental constraints,\ncomplementing contemporary quantum architecture search frameworks to advance\nscalable quantum simulations."}
{"id": "2505.06744", "pdf": "https://arxiv.org/pdf/2505.06744", "abs": "https://arxiv.org/abs/2505.06744", "authors": ["Kai Müller", "Martin Wenzel", "Tobias Windisch"], "title": "LineFlow: A Framework to Learn Active Control of Production Lines", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at ICML 2025", "summary": "Many production lines require active control mechanisms, such as adaptive\nrouting, worker reallocation, and rescheduling, to maintain optimal\nperformance. However, designing these control systems is challenging for\nvarious reasons, and while reinforcement learning (RL) has shown promise in\naddressing these challenges, a standardized and general framework is still\nlacking. In this work, we introduce LineFlow, an extensible, open-source Python\nframework for simulating production lines of arbitrary complexity and training\nRL agents to control them. To demonstrate the capabilities and to validate the\nunderlying theoretical assumptions of LineFlow, we formulate core subproblems\nof active line control in ways that facilitate mathematical analysis. For each\nproblem, we provide optimal solutions for comparison. We benchmark\nstate-of-the-art RL algorithms and show that the learned policies approach\noptimal performance in well-understood scenarios. However, for more complex,\nindustrial-scale production lines, RL still faces significant challenges,\nhighlighting the need for further research in areas such as reward shaping,\ncurriculum learning, and hierarchical control."}
{"id": "2505.07167", "pdf": "https://arxiv.org/pdf/2505.07167", "abs": "https://arxiv.org/abs/2505.07167", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods."}
{"id": "2505.07050", "pdf": "https://arxiv.org/pdf/2505.07050", "abs": "https://arxiv.org/abs/2505.07050", "authors": ["Binbin Wei", "Yuhang Zhang", "Shishun Tian", "Muxin Liao", "Wei Li", "Wenbin Zou"], "title": "Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) aims to align source and target domain\ndistributions to close the domain gap, but still struggles with obtaining the\ntarget data. Fortunately, Domain Generalization (DG) excels without the need\nfor any target data. Recent works expose that depth maps contribute to improved\ngeneralized performance in the UDA tasks, but they ignore the noise and holes\nin depth maps due to device and environmental factors, failing to sufficiently\nand effectively learn domain-invariant representation. Although\nhigh-sensitivity region suppression has shown promising results in learning\ndomain-invariant features, existing methods cannot be directly applicable to\ndepth maps due to their unique characteristics. Hence, we propose a novel\nframework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal\nstylization flow (DSSS), focusing on learning domain-invariant features from\ndepth maps for the DG semantic segmentation. Specifically, we propose the RGB-D\ninter-modal stylization flow to generate stylized depth maps for sensitivity\ndetection, cleverly utilizing RGB information as the stylization source. Then,\na class-wise soft spatial sensitivity suppression is designed to identify and\nemphasize non-sensitive depth features that contain more domain-invariant\ninformation. Furthermore, an RGB-D soft alignment loss is proposed to ensure\nthat the stylized depth maps only align part of the RGB features while still\nretaining the unique depth information. To our best knowledge, our DSSS\nframework is the first work to integrate RGB and Depth information in the\nmulti-class DG semantic segmentation task. Extensive experiments over multiple\nbackbone networks show that our framework achieves remarkable performance\nimprovement."}
{"id": "2505.06363", "pdf": "https://arxiv.org/pdf/2505.06363", "abs": "https://arxiv.org/abs/2505.06363", "authors": ["Anmol Gupta", "Weiwei Gu", "Omkar Patil", "Jun Ki Lee", "Nakul Gopalan"], "title": "Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "As robots become more generalized and deployed in diverse environments, they\nmust interact with complex objects, many with multiple independent joints or\ndegrees of freedom (DoF) requiring precise control. A common strategy is object\nmodeling, where compact state-space models are learned from real-world\nobservations and paired with classical planning. However, existing methods\noften rely on prior knowledge or focus on single-DoF objects, limiting their\napplicability. They also fail to handle occluded joints and ignore the\nmanipulation sequences needed to access them. We address this by learning\nobject models from human demonstrations. We introduce Object Kinematic Sequence\nMachines (OKSMs), a novel representation capturing both kinematic constraints\nand manipulation order for multi-DoF objects. To estimate these models from\npoint cloud data, we present Pokenet, a deep neural network trained on human\ndemonstrations. We validate our approach on 8,000 simulated and 1,600\nreal-world annotated samples. Pokenet improves joint axis and state estimation\nby over 20 percent on real-world data compared to prior methods. Finally, we\ndemonstrate OKSMs on a Sawyer robot using inverse kinematics-based planning to\nmanipulate multi-DoF objects."}
{"id": "2505.06753", "pdf": "https://arxiv.org/pdf/2505.06753", "abs": "https://arxiv.org/abs/2505.06753", "authors": ["Muhamed Amin", "Bernard R. Brooks"], "title": "Boltzmann Classifier: A Thermodynamic-Inspired Approach to Supervised Learning", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "We propose a novel classification algorithm, the Boltzmann Classifier,\ninspired by the thermodynamic principles underlying the Boltzmann distribution.\nOur method computes a probabilistic estimate for each class based on an energy\nfunction derived from feature-wise deviations between input samples and\nclass-specific centroids. The resulting probabilities are proportional to the\nexponential negative energies, normalized across classes, analogous to the\nBoltzmann distribution used in statistical mechanics. In addition, the KT\nvariable can be used to allow the high energy states to be more accessible,\nwhich allows the tuning of their probabilities as needed. We evaluate the model\nperformance on several datasets from different applications. The model achieves\na high accuracy, which indicates that the Boltzmann Classifier is competitive\nwith standard models like logistic regression and k-nearest neighbors while\noffering a thermodynamically motivated probabilistic interpretation. our\nclassifier does not require iterative optimization or backpropagation and is\nthus computationally efficient and easy to integrate into existing workflows.\nThis work demonstrates how ideas from physics can inform new directions in\nmachine learning, providing a foundation for interpretable, energy-based\ndecision-making systems."}
{"id": "2505.07188", "pdf": "https://arxiv.org/pdf/2505.07188", "abs": "https://arxiv.org/abs/2505.07188", "authors": ["Chetan Pathade", "Shubham Patil"], "title": "Securing Genomic Data Against Inference Attacks in Federated Learning Environments", "categories": ["cs.CR", "cs.CL"], "comment": "10 Pages, 7 Figures", "summary": "Federated Learning (FL) offers a promising framework for collaboratively\ntraining machine learning models across decentralized genomic datasets without\ndirect data sharing. While this approach preserves data locality, it remains\nsusceptible to sophisticated inference attacks that can compromise individual\nprivacy. In this study, we simulate a federated learning setup using synthetic\ngenomic data and assess its vulnerability to three key attack vectors:\nMembership Inference Attack (MIA), Gradient-Based Membership Inference Attack,\nand Label Inference Attack (LIA). Our experiments reveal that Gradient-Based\nMIA achieves the highest effectiveness, with a precision of 0.79 and F1-score\nof 0.87, underscoring the risk posed by gradient exposure in federated updates.\nAdditionally, we visualize comparative attack performance through radar plots\nand quantify model leakage across clients. The findings emphasize the\ninadequacy of na\\\"ive FL setups in safeguarding genomic privacy and motivate\nthe development of more robust privacy-preserving mechanisms tailored to the\nunique sensitivity of genomic data."}
{"id": "2505.07057", "pdf": "https://arxiv.org/pdf/2505.07057", "abs": "https://arxiv.org/abs/2505.07057", "authors": ["Junhao Xia", "Chaoyang Zhang", "Yecheng Zhang", "Chengyang Zhou", "Zhichang Wang", "Bochun Liu", "Dongshuo Yin"], "title": "DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video generation based on diffusion models presents a challenging multimodal\ntask, with video editing emerging as a pivotal direction in this field. Recent\nvideo editing approaches primarily fall into two categories: training-required\nand training-free methods. While training-based methods incur high\ncomputational costs, training-free alternatives often yield suboptimal\nperformance. To address these limitations, we propose DAPE, a high-quality yet\ncost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for\nvideo editing. In the first stage, we design an efficient norm-tuning method to\nenhance temporal consistency in generated videos. The second stage introduces a\nvision-friendly adapter to improve visual quality. Additionally, we identify\ncritical shortcomings in existing benchmarks, including limited category\ndiversity, imbalanced object distribution, and inconsistent frame counts. To\nmitigate these issues, we curate a large dataset benchmark comprising 232\nvideos with rich annotations and 6 editing prompts, enabling objective and\ncomprehensive evaluation of advanced methods. Extensive experiments on existing\ndatasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate\nthat DAPE significantly improves temporal coherence and text-video alignment\nwhile outperforming previous state-of-the-art approaches."}
{"id": "2505.06378", "pdf": "https://arxiv.org/pdf/2505.06378", "abs": "https://arxiv.org/abs/2505.06378", "authors": ["Yuxiang Wei", "Zhuoqi Zeng", "Yue Zhong", "Jiawen Kang", "Ryan Wen Liu", "M. Shamim Hossain"], "title": "Bi-LSTM based Multi-Agent DRL with Computation-aware Pruning for Agent Twins Migration in Vehicular Embodied AI Networks", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "With the advancement of large language models and embodied Artificial\nIntelligence (AI) in the intelligent transportation scenarios, the combination\nof them in intelligent transportation spawns the Vehicular Embodied AI Network\n(VEANs). In VEANs, Autonomous Vehicles (AVs) are typical agents whose local\nadvanced AI applications are defined as vehicular embodied AI agents, enabling\ncapabilities such as environment perception and multi-agent collaboration. Due\nto computation latency and resource constraints, the local AI applications and\nservices running on vehicular embodied AI agents need to be migrated, and\nsubsequently referred to as vehicular embodied AI agent twins, which drive the\nadvancement of vehicular embodied AI networks to offload intensive tasks to\nRoadside Units (RSUs), mitigating latency problems while maintaining service\nquality. Recognizing workload imbalance among RSUs in traditional approaches,\nwe model AV-RSU interactions as a Stackelberg game to optimize bandwidth\nresource allocation for efficient migration. A Tiny Multi-Agent Bidirectional\nLSTM Proximal Policy Optimization (TMABLPPO) algorithm is designed to\napproximate the Stackelberg equilibrium through decentralized coordination.\nFurthermore, a personalized neural network pruning algorithm based on Path\neXclusion (PX) dynamically adapts to heterogeneous AV computation capabilities\nby identifying task-critical parameters in trained models, reducing model\ncomplexity with less performance degradation. Experimental validation confirms\nthe algorithm's effectiveness in balancing system load and minimizing delays,\ndemonstrating significant improvements in vehicular embodied AI agent\ndeployment."}
{"id": "2505.06759", "pdf": "https://arxiv.org/pdf/2505.06759", "abs": "https://arxiv.org/abs/2505.06759", "authors": ["Xavier Martínez-Luaña", "Manuel Fernández-Veiga", "Rebeca P. Díaz-Redondo", "Ana Fernández-Vilas"], "title": "Privacy-aware Berrut Approximated Coded Computing applied to general distributed learning", "categories": ["cs.LG", "cs.CR", "cs.DC", "cs.IT", "math.IT"], "comment": null, "summary": "Coded computing is one of the techniques that can be used for privacy\nprotection in Federated Learning. However, most of the constructions used for\ncoded computing work only under the assumption that the computations involved\nare exact, generally restricted to special classes of functions, and require\nquantized inputs. This paper considers the use of Private Berrut Approximate\nCoded Computing (PBACC) as a general solution to add strong but non-perfect\nprivacy to federated learning. We derive new adapted PBACC algorithms for\ncentralized aggregation, secure distributed training with centralized data, and\nsecure decentralized training with decentralized data, thus enlarging\nsignificantly the applications of the method and the existing privacy\nprotection tools available for these paradigms. Particularly, PBACC can be used\nrobustly to attain privacy guarantees in decentralized federated learning for a\nvariety of models. Our numerical results show that the achievable quality of\ndifferent learning models (convolutional neural networks, variational\nautoencoders, and Cox regression) is minimally altered by using these new\ncomputing schemes, and that the privacy leakage can be bounded strictly to less\nthan a fraction of one bit per participant. Additionally, the computational\ncost of the encoding and decoding processes depends only of the degree of\ndecentralization of the data."}
{"id": "2505.07558", "pdf": "https://arxiv.org/pdf/2505.07558", "abs": "https://arxiv.org/abs/2505.07558", "authors": ["Rei Higuchi", "Taiji Suzuki"], "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences is crucial for\nsafe deployment, yet existing methods assume specific preference models like\nBradley-Terry model. This assumption leads to statistical inconsistency, where\nmore data doesn't guarantee convergence to true human preferences. To address\nthis critical gap, we introduce a novel alignment method Direct Density Ratio\nOptimization (DDRO). DDRO directly estimates the density ratio between\npreferred and unpreferred output distributions, circumventing the need for\nexplicit human preference modeling. We theoretically prove that DDRO is\nstatistically consistent, ensuring convergence to the true preferred\ndistribution as the data size grows, regardless of the underlying preference\nstructure. Experiments demonstrate that DDRO achieves superior performance\ncompared to existing methods on many major benchmarks. DDRO unlocks the\npotential for truly data-driven alignment, paving the way for more reliable and\nhuman-aligned LLMs."}
{"id": "2505.07062", "pdf": "https://arxiv.org/pdf/2505.07062", "abs": "https://arxiv.org/abs/2505.07062", "authors": ["Dong Guo", "Faming Wu", "Feida Zhu", "Fuxing Leng", "Guang Shi", "Haobin Chen", "Haoqi Fan", "Jian Wang", "Jianyu Jiang", "Jiawei Wang", "Jingji Chen", "Jingjia Huang", "Kang Lei", "Liping Yuan", "Lishu Luo", "Pengfei Liu", "Qinghao Ye", "Rui Qian", "Shen Yan", "Shixiong Zhao", "Shuai Peng", "Shuangye Li", "Sihang Yuan", "Sijin Wu", "Tianheng Cheng", "Weiwei Liu", "Wenqian Wang", "Xianhan Zeng", "Xiao Liu", "Xiaobo Qin", "Xiaohan Ding", "Xiaojun Xiao", "Xiaoying Zhang", "Xuanwei Zhang", "Xuehan Xiong", "Yanghua Peng", "Yangrui Chen", "Yanwei Li", "Yanxu Hu", "Yi Lin", "Yiyuan Hu", "Yiyuan Zhang", "Youbin Wu", "Yu Li", "Yudong Liu", "Yue Ling", "Yujia Qin", "Zanbo Wang", "Zhiwu He", "Aoxue Zhang", "Bairen Yi", "Bencheng Liao", "Can Huang", "Can Zhang", "Chaorui Deng", "Chaoyi Deng", "Cheng Lin", "Cheng Yuan", "Chenggang Li", "Chenhui Gou", "Chenwei Lou", "Chengzhi Wei", "Chundian Liu", "Chunyuan Li", "Deyao Zhu", "Donghong Zhong", "Feng Li", "Feng Zhang", "Gang Wu", "Guodong Li", "Guohong Xiao", "Haibin Lin", "Haihua Yang", "Haoming Wang", "Heng Ji", "Hongxiang Hao", "Hui Shen", "Huixia Li", "Jiahao Li", "Jialong Wu", "Jianhua Zhu", "Jianpeng Jiao", "Jiashi Feng", "Jiaze Chen", "Jianhui Duan", "Jihao Liu", "Jin Zeng", "Jingqun Tang", "Jingyu Sun", "Joya Chen", "Jun Long", "Junda Feng", "Junfeng Zhan", "Junjie Fang", "Junting Lu", "Kai Hua", "Kai Liu", "Kai Shen", "Kaiyuan Zhang", "Ke Shen", "Ke Wang", "Keyu Pan", "Kun Zhang", "Kunchang Li", "Lanxin Li", "Lei Li", "Lei Shi", "Li Han", "Liang Xiang", "Liangqiang Chen", "Lin Chen", "Lin Li", "Lin Yan", "Liying Chi", "Longxiang Liu", "Mengfei Du", "Mingxuan Wang", "Ningxin Pan", "Peibin Chen", "Pengfei Chen", "Pengfei Wu", "Qingqing Yuan", "Qingyao Shuai", "Qiuyan Tao", "Renjie Zheng", "Renrui Zhang", "Ru Zhang", "Rui Wang", "Rui Yang", "Rui Zhao", "Shaoqiang Xu", "Shihao Liang", "Shipeng Yan", "Shu Zhong", "Shuaishuai Cao", "Shuangzhi Wu", "Shufan Liu", "Shuhan Chang", "Songhua Cai", "Tenglong Ao", "Tianhao Yang", "Tingting Zhang", "Wanjun Zhong", "Wei Jia", "Wei Weng", "Weihao Yu", "Wenhao Huang", "Wenjia Zhu", "Wenli Yang", "Wenzhi Wang", "Xiang Long", "XiangRui Yin", "Xiao Li", "Xiaolei Zhu", "Xiaoying Jia", "Xijin Zhang", "Xin Liu", "Xinchen Zhang", "Xinyu Yang", "Xiongcai Luo", "Xiuli Chen", "Xuantong Zhong", "Xuefeng Xiao", "Xujing Li", "Yan Wu", "Yawei Wen", "Yifan Du", "Yihao Zhang", "Yining Ye", "Yonghui Wu", "Yu Liu", "Yu Yue", "Yufeng Zhou", "Yufeng Yuan", "Yuhang Xu", "Yuhong Yang", "Yun Zhang", "Yunhao Fang", "Yuntao Li", "Yurui Ren", "Yuwen Xiong", "Zehua Hong", "Zehua Wang", "Zewei Sun", "Zeyu Wang", "Zhao Cai", "Zhaoyue Zha", "Zhecheng An", "Zhehui Zhao", "Zhengzhuo Xu", "Zhipeng Chen", "Zhiyong Wu", "Zhuofan Zheng", "Zihao Wang", "Zilong Huang", "Ziyu Zhu", "Zuquan Song"], "title": "Seed1.5-VL Technical Report", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)"}
{"id": "2505.06380", "pdf": "https://arxiv.org/pdf/2505.06380", "abs": "https://arxiv.org/abs/2505.06380", "authors": ["Josh Harguess", "Chris M. Ward"], "title": "Offensive Security for AI Systems: Concepts, Practices, and Applications", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) systems become increasingly adopted across\nsectors, the need for robust, proactive security strategies is paramount.\nTraditional defensive measures often fall short against the unique and evolving\nthreats facing AI-driven technologies, making offensive security an essential\napproach for identifying and mitigating risks. This paper presents a\ncomprehensive framework for offensive security in AI systems, emphasizing\nproactive threat simulation and adversarial testing to uncover vulnerabilities\nthroughout the AI lifecycle. We examine key offensive security techniques,\nincluding weakness and vulnerability assessment, penetration testing, and red\nteaming, tailored specifically to address AI's unique susceptibilities. By\nsimulating real-world attack scenarios, these methodologies reveal critical\ninsights, informing stronger defensive strategies and advancing resilience\nagainst emerging threats. This framework advances offensive AI security from\ntheoretical concepts to practical, actionable methodologies that organizations\ncan implement to strengthen their AI systems against emerging threats."}
{"id": "2505.06762", "pdf": "https://arxiv.org/pdf/2505.06762", "abs": "https://arxiv.org/abs/2505.06762", "authors": ["Junfeng Jiao", "Seung Gyu Baik", "Seung Jun Choi", "Yiming Xu"], "title": "Investigating Robotaxi Crash Severity Using Geographical Random Forest", "categories": ["cs.LG", "cs.RO"], "comment": "21 pages, 8 figures", "summary": "This paper quantitatively investigates the crash severity of Autonomous\nVehicles (AVs) with spatially localized machine learning and macroscopic\nmeasures of the urban built environment. We address spatial heterogeneity and\nspatial autocorrelation, while focusing on land use patterns and human\nbehavior. Our Geographical Random Forest (GRF) model, accompanied with a crash\nseverity risk map of San Francisco, presents three findings that are useful for\ncommercial operations of AVs and robotaxis. First, spatially localized machine\nlearning performed better than regular machine learning, when predicting AV\ncrash severity. Bias-variance tradeoff was evident as we adjust the\nlocalization weight hyperparameter. Second, land use was the most important\nbuilt environment measure, compared to intersections, building footprints,\npublic transit stops, and Points Of Interests (POIs). Third, it was predicted\nthat city center areas with greater diversity and commercial activities were\nmore likely to result in low-severity AV crashes, than residential\nneighborhoods. Residential land use may be associated with higher severity due\nto human behavior and less restrictive environment. This paper recommends to\nexplicitly consider geographic locations, and to design safety measures\nspecific to residential neighborhoods, when robotaxi operators train their AV\nsystems."}
{"id": "2505.07704", "pdf": "https://arxiv.org/pdf/2505.07704", "abs": "https://arxiv.org/abs/2505.07704", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Anton Razzhigaev", "Alexander Panchenko", "Vasily Konovalov"], "title": "Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Measuring how real images look is a complex task in artificial intelligence\nresearch. For example, an image of a boy with a vacuum cleaner in a desert\nviolates common sense. We introduce a novel method, which we call Through the\nLooking Glass (TLG), to assess image common sense consistency using Large\nVision-Language Models (LVLMs) and Transformer-based encoder. By leveraging\nLVLMs to extract atomic facts from these images, we obtain a mix of accurate\nfacts. We proceed by fine-tuning a compact attention-pooling classifier over\nencoded atomic facts. Our TLG has achieved a new state-of-the-art performance\non the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning\ncomponent."}
{"id": "2505.07071", "pdf": "https://arxiv.org/pdf/2505.07071", "abs": "https://arxiv.org/abs/2505.07071", "authors": ["Zihang Liu", "Zhenyu Zhang", "Hao Tang"], "title": "Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based image super-resolution (SR) methods have demonstrated\nremarkable performance. Recent advancements have introduced deterministic\nsampling processes that reduce inference from 15 iterative steps to a single\nstep, thereby significantly improving the inference speed of existing diffusion\nmodels. However, their efficiency remains limited when handling complex\nsemantic regions due to the single-step inference. To address this limitation,\nwe propose SAMSR, a semantic-guided diffusion framework that incorporates\nsemantic segmentation masks into the sampling process. Specifically, we\nintroduce the SAM-Noise Module, which refines Gaussian noise using segmentation\nmasks to preserve spatial and semantic features. Furthermore, we develop a\npixel-wise sampling strategy that dynamically adjusts the residual transfer\nrate and noise strength based on pixel-level semantic weights, prioritizing\nsemantically rich regions during the diffusion process. To enhance model\ntraining, we also propose a semantic consistency loss, which aligns pixel-wise\nsemantic weights between predictions and ground truth. Extensive experiments on\nboth real-world and synthetic datasets demonstrate that SAMSR significantly\nimproves perceptual quality and detail recovery, particularly in semantically\ncomplex images. Our code is released at https://github.com/Liu-Zihang/SAMSR."}
{"id": "2505.06394", "pdf": "https://arxiv.org/pdf/2505.06394", "abs": "https://arxiv.org/abs/2505.06394", "authors": ["Massimiliano Albanese", "Xinming Ou", "Kevin Lybarger", "Daniel Lende", "Dmitry Goldgof"], "title": "Towards AI-Driven Human-Machine Co-Teaming for Adaptive and Agile Cyber Security Operation Centers", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Security Operations Centers (SOCs) face growing challenges in managing\ncybersecurity threats due to an overwhelming volume of alerts, a shortage of\nskilled analysts, and poorly integrated tools. Human-AI collaboration offers a\npromising path to augment the capabilities of SOC analysts while reducing their\ncognitive overload. To this end, we introduce an AI-driven human-machine\nco-teaming paradigm that leverages large language models (LLMs) to enhance\nthreat intelligence, alert triage, and incident response workflows. We present\na vision in which LLM-based AI agents learn from human analysts the tacit\nknowledge embedded in SOC operations, enabling the AI agents to improve their\nperformance on SOC tasks through this co-teaming. We invite SOCs to collaborate\nwith us to further develop this process and uncover replicable patterns where\nhuman-AI co-teaming yields measurable improvements in SOC productivity."}
{"id": "2505.06795", "pdf": "https://arxiv.org/pdf/2505.06795", "abs": "https://arxiv.org/abs/2505.06795", "authors": ["Abhijit Gupta"], "title": "Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "Commodity price volatility creates economic challenges, necessitating\naccurate multi-horizon forecasting. Predicting prices for commodities like\ncopper and crude oil is complicated by diverse interacting factors\n(macroeconomic, supply/demand, geopolitical, etc.). Current models often lack\ntransparency, limiting strategic use. This paper presents a Regularized Sparse\nAutoencoder (RSAE), a deep learning framework for simultaneous multi-horizon\ncommodity price prediction and discovery of interpretable latent market\ndrivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week,\n1-month) using multivariate time series. Crucially, L1 regularization\n($\\|\\mathbf{z}\\|_1$) on its latent vector $\\mathbf{z}$ enforces sparsity,\npromoting parsimonious explanations of market dynamics through learned factors\nrepresenting underlying drivers (e.g., demand, supply shocks). Drawing from\nenergy-based models and sparse coding, the RSAE optimizes predictive accuracy\nwhile learning sparse representations. Evaluated on historical Copper and Crude\nOil data with numerous indicators, our findings indicate the RSAE offers\ncompetitive multi-horizon forecasting accuracy and data-driven insights into\nprice dynamics via its interpretable latent space, a key advantage over\ntraditional black-box approaches."}
{"id": "2505.07768", "pdf": "https://arxiv.org/pdf/2505.07768", "abs": "https://arxiv.org/abs/2505.07768", "authors": ["Yifeng Di", "Tianyi Zhang"], "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Accepted to ICSE 2025", "summary": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence."}
{"id": "2505.07073", "pdf": "https://arxiv.org/pdf/2505.07073", "abs": "https://arxiv.org/abs/2505.07073", "authors": ["Payal Varshney", "Adriano Lucieri", "Christoph Balada", "Andreas Dengel", "Sheraz Ahmed"], "title": "Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Concept-based explanations have emerged as an effective approach within\nExplainable Artificial Intelligence, enabling interpretable insights by\naligning model decisions with human-understandable concepts. However, existing\nmethods rely on computationally intensive procedures and struggle to\nefficiently capture complex, semantic concepts. Recently, the Concept Discovery\nthrough Latent Diffusion-based Counterfactual Trajectories (CDCT) framework,\nintroduced by Varshney et al. (2025), attempts to identify concepts via\ndimension-wise traversal of the latent space of a Variational Autoencoder\ntrained on counterfactual trajectories. Extending the CDCT framework, this work\nintroduces Concept Directions via Latent Clustering (CDLC), which extracts\nglobal, class-specific concept directions by clustering latent difference\nvectors derived from factual and diffusion-generated counterfactual image\npairs. CDLC substantially reduces computational complexity by eliminating the\nexhaustive latent dimension traversal required in CDCT and enables the\nextraction of multidimensional semantic concepts encoded across the latent\ndimensions. This approach is validated on a real-world skin lesion dataset,\ndemonstrating that the extracted concept directions align with clinically\nrecognized dermoscopic features and, in some cases, reveal dataset-specific\nbiases or unknown biomarkers. These results highlight that CDLC is\ninterpretable, scalable, and applicable across high-stakes domains and diverse\ndata modalities."}
{"id": "2505.06402", "pdf": "https://arxiv.org/pdf/2505.06402", "abs": "https://arxiv.org/abs/2505.06402", "authors": ["Alexiy Buynitsky", "Sina Ehsani", "Bhanu Pallakonda", "Pragyana Mishra"], "title": "Camera Control at the Edge with Language Models for Scene Understanding", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "7 pages, 6 figures. This work was presented and published at the 11th\n  IEEE International Conference on Control, Automation and Robotics (ICCAR) in\n  2025", "summary": "In this paper, we present Optimized Prompt-based Unified System (OPUS), a\nframework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom\n(PTZ) cameras, providing contextual understanding of natural environments. To\nachieve this goal, the OPUS system improves cost-effectiveness by generating\nkeywords from a high-level camera control API and transferring knowledge from\nlarger closed-source language models to smaller ones through Supervised\nFine-Tuning (SFT) on synthetic data. This enables efficient edge deployment\nwhile maintaining performance comparable to larger models like GPT-4. OPUS\nenhances environmental awareness by converting data from multiple cameras into\ntextual descriptions for language models, eliminating the need for specialized\nsensory tokens. In benchmark testing, our approach significantly outperformed\nboth traditional language model techniques and more complex prompting methods,\nachieving a 35% improvement over advanced techniques and a 20% higher task\naccuracy compared to closed-source models like Gemini Pro. The system\ndemonstrates OPUS's capability to simplify PTZ camera operations through an\nintuitive natural language interface. This approach eliminates the need for\nexplicit programming and provides a conversational method for interacting with\ncamera systems, representing a significant advancement in how users can control\nand utilize PTZ camera technology."}
{"id": "2505.06804", "pdf": "https://arxiv.org/pdf/2505.06804", "abs": "https://arxiv.org/abs/2505.06804", "authors": ["Xiaohan Wang", "Matthew Berger"], "title": "Topology Guidance: Controlling the Outputs of Generative Models via Vector Field Topology", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "For domains that involve numerical simulation, it can be computationally\nexpensive to run an ensemble of simulations spanning a parameter space of\ninterest to a user. To this end, an attractive surrogate for simulation is the\ngenerative modeling of fields produced by an ensemble, allowing one to\nsynthesize fields in a computationally cheap, yet accurate, manner. However,\nfor the purposes of visual analysis, a limitation of generative models is their\nlack of control, as it is unclear what one should expect when sampling a field\nfrom a model. In this paper we study how to make generative models of fields\nmore controllable, so that users can specify features of interest, in\nparticular topological features, that they wish to see in the output. We\npropose topology guidance, a method for guiding the sampling process of a\ngenerative model, specifically a diffusion model, such that a topological\ndescription specified as input is satisfied in the generated output. Central to\nour method, we couple a coordinate-based neural network used to represent\nfields, with a diffusion model used for generation. We show how to use\ntopologically-relevant signals provided by the coordinate-based network to help\nguide the denoising process of a diffusion model. This enables us to faithfully\nrepresent a user's specified topology, while ensuring that the output field\nremains within the generative data distribution. Specifically, we study 2D\nvector field topology, evaluating our method over an ensemble of fluid flows,\nwhere we show that generated vector fields faithfully adhere to the location,\nand type, of critical points over the spatial domain. We further show the\nbenefits of our method in aiding the comparison of ensembles, allowing one to\nexplore commonalities and differences in distributions along prescribed\ntopological features."}
{"id": "2306.09597", "pdf": "https://arxiv.org/pdf/2306.09597", "abs": "https://arxiv.org/abs/2306.09597", "authors": ["Han Wang", "Yi Zhu", "Ye Wang", "Yun Li", "Yunhao Yuan", "Jipeng Qiang"], "title": "Clickbait Detection via Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Clickbait, which aims to induce users with some surprising and even thrilling\nheadlines for increasing click-through rates, permeates almost all online\ncontent publishers, such as news portals and social media. Recently, Large\nLanguage Models (LLMs) have emerged as a powerful instrument and achieved\ntremendous success in a series of NLP downstream tasks. However, it is not yet\nknown whether LLMs can be served as a high-quality clickbait detection system.\nIn this paper, we analyze the performance of LLMs in the few-shot and zero-shot\nscenarios on several English and Chinese benchmark datasets. Experimental\nresults show that LLMs cannot achieve the best results compared to the\nstate-of-the-art deep and fine-tuning PLMs methods. Different from human\nintuition, the experiments demonstrated that LLMs cannot make satisfied\nclickbait detection just by the headlines."}
{"id": "2505.07119", "pdf": "https://arxiv.org/pdf/2505.07119", "abs": "https://arxiv.org/abs/2505.07119", "authors": ["Arianna Stropeni", "Francesco Borsatti", "Manuel Barusco", "Davide Dalle Pezze", "Marco Fabris", "Gian Antonio Susto"], "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing waste and operational costs is essential. Deploying deep learning\nmodels within Internet of Things (IoT) environments introduces specific\nchallenges due to the limited computational power and bandwidth of edge\ndevices. This study investigates how to perform VAD effectively under such\nconstraints by leveraging compact and efficient processing strategies. We\nevaluate several data compression techniques, examining the trade-off between\nsystem latency and detection accuracy. Experiments on the MVTec AD benchmark\ndemonstrate that significant compression can be achieved with minimal loss in\nanomaly detection performance compared to uncompressed data."}
{"id": "2505.06428", "pdf": "https://arxiv.org/pdf/2505.06428", "abs": "https://arxiv.org/abs/2505.06428", "authors": ["Somayeh Molaei", "Lionel P. Robert", "Nikola Banovic"], "title": "What Do People Want to Know About Artificial Intelligence (AI)? The Importance of Answering End-User Questions to Explain Autonomous Vehicle (AV) Decisions", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to the Proceedings of the ACM on Human-Computer Interaction,\n  CSCW, October 2025", "summary": "Improving end-users' understanding of decisions made by autonomous vehicles\n(AVs) driven by artificial intelligence (AI) can improve utilization and\nacceptance of AVs. However, current explanation mechanisms primarily help AI\nresearchers and engineers in debugging and monitoring their AI systems, and may\nnot address the specific questions of end-users, such as passengers, about AVs\nin various scenarios. In this paper, we conducted two user studies to\ninvestigate questions that potential AV passengers might pose while riding in\nan AV and evaluate how well answers to those questions improve their\nunderstanding of AI-driven AV decisions. Our initial formative study identified\na range of questions about AI in autonomous driving that existing explanation\nmechanisms do not readily address. Our second study demonstrated that\ninteractive text-based explanations effectively improved participants'\ncomprehension of AV decisions compared to simply observing AV decisions. These\nfindings inform the design of interactions that motivate end-users to engage\nwith and inquire about the reasoning behind AI-driven AV decisions."}
{"id": "2505.06818", "pdf": "https://arxiv.org/pdf/2505.06818", "abs": "https://arxiv.org/abs/2505.06818", "authors": ["Thien Nhan Vo"], "title": "Deep Learning for On-Street Parking Violation Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Illegal parking along with the lack of available parking spaces are among the\nbiggest issues faced in many large cities. These issues can have a significant\nimpact on the quality of life of citizens. On-street parking systems have been\ndesigned to this end aiming at ensuring that parking spaces will be available\nfor the local population, while also providing easy access to parking for\npeople visiting the city center. However, these systems are often affected by\nillegal parking, providing incorrect information regarding the availability of\nparking spaces. Even though this can be mitigated using sensors for detecting\nthe presence of cars in various parking sectors, the cost of these\nimplementations is usually prohibiting large. In this paper, we investigate an\nindirect way of predicting parking violations at a fine-grained level,\nequipping such parking systems with a valuable tool for providing more accurate\ninformation to citizens. To this end, we employed a Deep Learning (DL)-based\nmodel to predict fine-grained parking violation rates for on-street parking\nsystems. Moreover, we developed a data augmentation and smoothing technique for\nfurther improving the accuracy of DL models under the presence of missing and\nnoisy data. We demonstrate, using experiments on real data collected in\nThessaloniki, Greece, that the developed system can indeed provide accurate\nparking violation predictions."}
{"id": "2310.13548", "pdf": "https://arxiv.org/pdf/2310.13548", "abs": "https://arxiv.org/abs/2310.13548", "authors": ["Mrinank Sharma", "Meg Tong", "Tomasz Korbak", "David Duvenaud", "Amanda Askell", "Samuel R. Bowman", "Newton Cheng", "Esin Durmus", "Zac Hatfield-Dodds", "Scott R. Johnston", "Shauna Kravec", "Timothy Maxwell", "Sam McCandlish", "Kamal Ndousse", "Oliver Rausch", "Nicholas Schiefer", "Da Yan", "Miranda Zhang", "Ethan Perez"], "title": "Towards Understanding Sycophancy in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "I.2.6"], "comment": "32 pages, 20 figures", "summary": "Human feedback is commonly utilized to finetune AI assistants. But human\nfeedback may also encourage model responses that match user beliefs over\ntruthful ones, a behaviour known as sycophancy. We investigate the prevalence\nof sycophancy in models whose finetuning procedure made use of human feedback,\nand the potential role of human preference judgments in such behavior. We first\ndemonstrate that five state-of-the-art AI assistants consistently exhibit\nsycophancy across four varied free-form text-generation tasks. To understand if\nhuman preferences drive this broadly observed behavior, we analyze existing\nhuman preference data. We find that when a response matches a user's views, it\nis more likely to be preferred. Moreover, both humans and preference models\n(PMs) prefer convincingly-written sycophantic responses over correct ones a\nnon-negligible fraction of the time. Optimizing model outputs against PMs also\nsometimes sacrifices truthfulness in favor of sycophancy. Overall, our results\nindicate that sycophancy is a general behavior of state-of-the-art AI\nassistants, likely driven in part by human preference judgments favoring\nsycophantic responses."}
{"id": "2505.07165", "pdf": "https://arxiv.org/pdf/2505.07165", "abs": "https://arxiv.org/abs/2505.07165", "authors": ["Jun Li", "Hongzhang Zhu", "Tao Chen", "Xiaohua Qian"], "title": "Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework", "categories": ["cs.CV"], "comment": "accept by IEEE JBHI. Due to the limitation \"The abstract field cannot\n  be longer than 1,920 characters\", the abstract here is shorter than that in\n  the PDF file", "summary": "Recently, numerous pancreas segmentation methods have achieved promising\nperformance on local single-source datasets. However, these methods don't\nadequately account for generalizability issues, and hence typically show\nlimited performance and low stability on test data from other sources.\nConsidering the limited availability of distinct data sources, we seek to\nimprove the generalization performance of a pancreas segmentation model trained\nwith a single-source dataset, i.e., the single source generalization task. In\nparticular, we propose a dual self-supervised learning model that incorporates\nboth global and local anatomical contexts. Our model aims to fully exploit the\nanatomical features of the intra-pancreatic and extra-pancreatic regions, and\nhence enhance the characterization of the high-uncertainty regions for more\nrobust generalization. Specifically, we first construct a global-feature\ncontrastive self-supervised learning module that is guided by the pancreatic\nspatial structure. This module obtains complete and consistent pancreatic\nfeatures through promoting intra-class cohesion, and also extracts more\ndiscriminative features for differentiating between pancreatic and\nnon-pancreatic tissues through maximizing inter-class separation. It mitigates\nthe influence of surrounding tissue on the segmentation outcomes in\nhigh-uncertainty regions. Subsequently, a local-image restoration\nself-supervised learning module is introduced to further enhance the\ncharacterization of the high uncertainty regions. In this module, informative\nanatomical contexts are actually learned to recover randomly corrupted\nappearance patterns in those regions."}
{"id": "2505.06493", "pdf": "https://arxiv.org/pdf/2505.06493", "abs": "https://arxiv.org/abs/2505.06493", "authors": ["Jiawei Guo", "Haipeng Cai"], "title": "System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have gained widespread adoption across diverse\napplications due to their impressive generative capabilities. Their\nplug-and-play nature enables both developers and end users to interact with\nthese models through simple prompts. However, as LLMs become more integrated\ninto various systems in diverse domains, concerns around their security are\ngrowing. Existing studies mainly focus on threats arising from user prompts\n(e.g. prompt injection attack) and model output (e.g. model inversion attack),\nwhile the security of system prompts remains largely overlooked. This work\nbridges the critical gap. We introduce system prompt poisoning, a new attack\nvector against LLMs that, unlike traditional user prompt injection, poisons\nsystem prompts hence persistently impacts all subsequent user interactions and\nmodel responses. We systematically investigate four practical attack strategies\nin various poisoning scenarios. Through demonstration on both generative and\nreasoning LLMs, we show that system prompt poisoning is highly feasible without\nrequiring jailbreak techniques, and effective across a wide range of tasks,\nincluding those in mathematics, coding, logical reasoning, and natural language\nprocessing. Importantly, our findings reveal that the attack remains effective\neven when user prompts employ advanced prompting techniques like\nchain-of-thought (CoT). We also show that such techniques, including CoT and\nretrieval-augmentation-generation (RAG), which are proven to be effective for\nimproving LLM performance in a wide range of tasks, are significantly weakened\nin their effectiveness by system prompt poisoning."}
{"id": "2505.06835", "pdf": "https://arxiv.org/pdf/2505.06835", "abs": "https://arxiv.org/abs/2505.06835", "authors": ["Khai Nguyen"], "title": "Streaming Sliced Optimal Transport", "categories": ["cs.LG", "stat.CO", "stat.ME", "stat.ML"], "comment": "28 pages, 9 figures, 3 tables", "summary": "Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widely\nrecognized for its statistical and computational scalability. In this work, we\nfurther enhance the computational scalability by proposing the first method for\ncomputing SW from sample streams, called \\emph{streaming sliced Wasserstein}\n(Stream-SW). To define Stream-SW, we first introduce the streaming computation\nof the one-dimensional Wasserstein distance. Since the one-dimensional\nWasserstein (1DW) distance has a closed-form expression, given by the absolute\ndifference between the quantile functions of the compared distributions, we\nleverage quantile approximation techniques for sample streams to define the\nstreaming 1DW distance. By applying streaming 1DW to all projections, we obtain\nStream-SW. The key advantage of Stream-SW is its low memory complexity while\nproviding theoretical guarantees on the approximation error. We demonstrate\nthat Stream-SW achieves a more accurate approximation of SW than random\nsubsampling, with lower memory consumption, in comparing Gaussian distributions\nand mixtures of Gaussians from streaming samples. Additionally, we conduct\nexperiments on point cloud classification, point cloud gradient flows, and\nstreaming change point detection to further highlight the favorable performance\nof Stream-SW."}
{"id": "2312.11805", "pdf": "https://arxiv.org/pdf/2312.11805", "abs": "https://arxiv.org/abs/2312.11805", "authors": ["Gemini Team", "Rohan Anil", "Sebastian Borgeaud", "Jean-Baptiste Alayrac", "Jiahui Yu", "Radu Soricut", "Johan Schalkwyk", "Andrew M. Dai", "Anja Hauth", "Katie Millican", "David Silver", "Melvin Johnson", "Ioannis Antonoglou", "Julian Schrittwieser", "Amelia Glaese", "Jilin Chen", "Emily Pitler", "Timothy Lillicrap", "Angeliki Lazaridou", "Orhan Firat", "James Molloy", "Michael Isard", "Paul R. Barham", "Tom Hennigan", "Benjamin Lee", "Fabio Viola", "Malcolm Reynolds", "Yuanzhong Xu", "Ryan Doherty", "Eli Collins", "Clemens Meyer", "Eliza Rutherford", "Erica Moreira", "Kareem Ayoub", "Megha Goel", "Jack Krawczyk", "Cosmo Du", "Ed Chi", "Heng-Tze Cheng", "Eric Ni", "Purvi Shah", "Patrick Kane", "Betty Chan", "Manaal Faruqui", "Aliaksei Severyn", "Hanzhao Lin", "YaGuang Li", "Yong Cheng", "Abe Ittycheriah", "Mahdis Mahdieh", "Mia Chen", "Pei Sun", "Dustin Tran", "Sumit Bagri", "Balaji Lakshminarayanan", "Jeremiah Liu", "Andras Orban", "Fabian Güra", "Hao Zhou", "Xinying Song", "Aurelien Boffy", "Harish Ganapathy", "Steven Zheng", "HyunJeong Choe", "Ágoston Weisz", "Tao Zhu", "Yifeng Lu", "Siddharth Gopal", "Jarrod Kahn", "Maciej Kula", "Jeff Pitman", "Rushin Shah", "Emanuel Taropa", "Majd Al Merey", "Martin Baeuml", "Zhifeng Chen", "Laurent El Shafey", "Yujing Zhang", "Olcan Sercinoglu", "George Tucker", "Enrique Piqueras", "Maxim Krikun", "Iain Barr", "Nikolay Savinov", "Ivo Danihelka", "Becca Roelofs", "Anaïs White", "Anders Andreassen", "Tamara von Glehn", "Lakshman Yagati", "Mehran Kazemi", "Lucas Gonzalez", "Misha Khalman", "Jakub Sygnowski", "Alexandre Frechette", "Charlotte Smith", "Laura Culp", "Lev Proleev", "Yi Luan", "Xi Chen", "James Lottes", "Nathan Schucher", "Federico Lebron", "Alban Rrustemi", "Natalie Clay", "Phil Crone", "Tomas Kocisky", "Jeffrey Zhao", "Bartek Perz", "Dian Yu", "Heidi Howard", "Adam Bloniarz", "Jack W. Rae", "Han Lu", "Laurent Sifre", "Marcello Maggioni", "Fred Alcober", "Dan Garrette", "Megan Barnes", "Shantanu Thakoor", "Jacob Austin", "Gabriel Barth-Maron", "William Wong", "Rishabh Joshi", "Rahma Chaabouni", "Deeni Fatiha", "Arun Ahuja", "Gaurav Singh Tomar", "Evan Senter", "Martin Chadwick", "Ilya Kornakov", "Nithya Attaluri", "Iñaki Iturrate", "Ruibo Liu", "Yunxuan Li", "Sarah Cogan", "Jeremy Chen", "Chao Jia", "Chenjie Gu", "Qiao Zhang", "Jordan Grimstad", "Ale Jakse Hartman", "Xavier Garcia", "Thanumalayan Sankaranarayana Pillai", "Jacob Devlin", "Michael Laskin", "Diego de Las Casas", "Dasha Valter", "Connie Tao", "Lorenzo Blanco", "Adrià Puigdomènech Badia", "David Reitter", "Mianna Chen", "Jenny Brennan", "Clara Rivera", "Sergey Brin", "Shariq Iqbal", "Gabriela Surita", "Jane Labanowski", "Abhi Rao", "Stephanie Winkler", "Emilio Parisotto", "Yiming Gu", "Kate Olszewska", "Ravi Addanki", "Antoine Miech", "Annie Louis", "Denis Teplyashin", "Geoff Brown", "Elliot Catt", "Jan Balaguer", "Jackie Xiang", "Pidong Wang", "Zoe Ashwood", "Anton Briukhov", "Albert Webson", "Sanjay Ganapathy", "Smit Sanghavi", "Ajay Kannan", "Ming-Wei Chang", "Axel Stjerngren", "Josip Djolonga", "Yuting Sun", "Ankur Bapna", "Matthew Aitchison", "Pedram Pejman", "Henryk Michalewski", "Tianhe Yu", "Cindy Wang", "Juliette Love", "Junwhan Ahn", "Dawn Bloxwich", "Kehang Han", "Peter Humphreys", "Thibault Sellam", "James Bradbury", "Varun Godbole", "Sina Samangooei", "Bogdan Damoc", "Alex Kaskasoli", "Sébastien M. R. Arnold", "Vijay Vasudevan", "Shubham Agrawal", "Jason Riesa", "Dmitry Lepikhin", "Richard Tanburn", "Srivatsan Srinivasan", "Hyeontaek Lim", "Sarah Hodkinson", "Pranav Shyam", "Johan Ferret", "Steven Hand", "Ankush Garg", "Tom Le Paine", "Jian Li", "Yujia Li", "Minh Giang", "Alexander Neitz", "Zaheer Abbas", "Sarah York", "Machel Reid", "Elizabeth Cole", "Aakanksha Chowdhery", "Dipanjan Das", "Dominika Rogozińska", "Vitaliy Nikolaev", "Pablo Sprechmann", "Zachary Nado", "Lukas Zilka", "Flavien Prost", "Luheng He", "Marianne Monteiro", "Gaurav Mishra", "Chris Welty", "Josh Newlan", "Dawei Jia", "Miltiadis Allamanis", "Clara Huiyi Hu", "Raoul de Liedekerke", "Justin Gilmer", "Carl Saroufim", "Shruti Rijhwani", "Shaobo Hou", "Disha Shrivastava", "Anirudh Baddepudi", "Alex Goldin", "Adnan Ozturel", "Albin Cassirer", "Yunhan Xu", "Daniel Sohn", "Devendra Sachan", "Reinald Kim Amplayo", "Craig Swanson", "Dessie Petrova", "Shashi Narayan", "Arthur Guez", "Siddhartha Brahma", "Jessica Landon", "Miteyan Patel", "Ruizhe Zhao", "Kevin Villela", "Luyu Wang", "Wenhao Jia", "Matthew Rahtz", "Mai Giménez", "Legg Yeung", "James Keeling", "Petko Georgiev", "Diana Mincu", "Boxi Wu", "Salem Haykal", "Rachel Saputro", "Kiran Vodrahalli", "James Qin", "Zeynep Cankara", "Abhanshu Sharma", "Nick Fernando", "Will Hawkins", "Behnam Neyshabur", "Solomon Kim", "Adrian Hutter", "Priyanka Agrawal", "Alex Castro-Ros", "George van den Driessche", "Tao Wang", "Fan Yang", "Shuo-yiin Chang", "Paul Komarek", "Ross McIlroy", "Mario Lučić", "Guodong Zhang", "Wael Farhan", "Michael Sharman", "Paul Natsev", "Paul Michel", "Yamini Bansal", "Siyuan Qiao", "Kris Cao", "Siamak Shakeri", "Christina Butterfield", "Justin Chung", "Paul Kishan Rubenstein", "Shivani Agrawal", "Arthur Mensch", "Kedar Soparkar", "Karel Lenc", "Timothy Chung", "Aedan Pope", "Loren Maggiore", "Jackie Kay", "Priya Jhakra", "Shibo Wang", "Joshua Maynez", "Mary Phuong", "Taylor Tobin", "Andrea Tacchetti", "Maja Trebacz", "Kevin Robinson", "Yash Katariya", "Sebastian Riedel", "Paige Bailey", "Kefan Xiao", "Nimesh Ghelani", "Lora Aroyo", "Ambrose Slone", "Neil Houlsby", "Xuehan Xiong", "Zhen Yang", "Elena Gribovskaya", "Jonas Adler", "Mateo Wirth", "Lisa Lee", "Music Li", "Thais Kagohara", "Jay Pavagadhi", "Sophie Bridgers", "Anna Bortsova", "Sanjay Ghemawat", "Zafarali Ahmed", "Tianqi Liu", "Richard Powell", "Vijay Bolina", "Mariko Iinuma", "Polina Zablotskaia", "James Besley", "Da-Woon Chung", "Timothy Dozat", "Ramona Comanescu", "Xiance Si", "Jeremy Greer", "Guolong Su", "Martin Polacek", "Raphaël Lopez Kaufman", "Simon Tokumine", "Hexiang Hu", "Elena Buchatskaya", "Yingjie Miao", "Mohamed Elhawaty", "Aditya Siddhant", "Nenad Tomasev", "Jinwei Xing", "Christina Greer", "Helen Miller", "Shereen Ashraf", "Aurko Roy", "Zizhao Zhang", "Ada Ma", "Angelos Filos", "Milos Besta", "Rory Blevins", "Ted Klimenko", "Chih-Kuan Yeh", "Soravit Changpinyo", "Jiaqi Mu", "Oscar Chang", "Mantas Pajarskas", "Carrie Muir", "Vered Cohen", "Charline Le Lan", "Krishna Haridasan", "Amit Marathe", "Steven Hansen", "Sholto Douglas", "Rajkumar Samuel", "Mingqiu Wang", "Sophia Austin", "Chang Lan", "Jiepu Jiang", "Justin Chiu", "Jaime Alonso Lorenzo", "Lars Lowe Sjösund", "Sébastien Cevey", "Zach Gleicher", "Thi Avrahami", "Anudhyan Boral", "Hansa Srinivasan", "Vittorio Selo", "Rhys May", "Konstantinos Aisopos", "Léonard Hussenot", "Livio Baldini Soares", "Kate Baumli", "Michael B. Chang", "Adrià Recasens", "Ben Caine", "Alexander Pritzel", "Filip Pavetic", "Fabio Pardo", "Anita Gergely", "Justin Frye", "Vinay Ramasesh", "Dan Horgan", "Kartikeya Badola", "Nora Kassner", "Subhrajit Roy", "Ethan Dyer", "Víctor Campos Campos", "Alex Tomala", "Yunhao Tang", "Dalia El Badawy", "Elspeth White", "Basil Mustafa", "Oran Lang", "Abhishek Jindal", "Sharad Vikram", "Zhitao Gong", "Sergi Caelles", "Ross Hemsley", "Gregory Thornton", "Fangxiaoyu Feng", "Wojciech Stokowiec", "Ce Zheng", "Phoebe Thacker", "Çağlar Ünlü", "Zhishuai Zhang", "Mohammad Saleh", "James Svensson", "Max Bileschi", "Piyush Patil", "Ankesh Anand", "Roman Ring", "Katerina Tsihlas", "Arpi Vezer", "Marco Selvi", "Toby Shevlane", "Mikel Rodriguez", "Tom Kwiatkowski", "Samira Daruki", "Keran Rong", "Allan Dafoe", "Nicholas FitzGerald", "Keren Gu-Lemberg", "Mina Khan", "Lisa Anne Hendricks", "Marie Pellat", "Vladimir Feinberg", "James Cobon-Kerr", "Tara Sainath", "Maribeth Rauh", "Sayed Hadi Hashemi", "Richard Ives", "Yana Hasson", "Eric Noland", "Yuan Cao", "Nathan Byrd", "Le Hou", "Qingze Wang", "Thibault Sottiaux", "Michela Paganini", "Jean-Baptiste Lespiau", "Alexandre Moufarek", "Samer Hassan", "Kaushik Shivakumar", "Joost van Amersfoort", "Amol Mandhane", "Pratik Joshi", "Anirudh Goyal", "Matthew Tung", "Andrew Brock", "Hannah Sheahan", "Vedant Misra", "Cheng Li", "Nemanja Rakićević", "Mostafa Dehghani", "Fangyu Liu", "Sid Mittal", "Junhyuk Oh", "Seb Noury", "Eren Sezener", "Fantine Huot", "Matthew Lamm", "Nicola De Cao", "Charlie Chen", "Sidharth Mudgal", "Romina Stella", "Kevin Brooks", "Gautam Vasudevan", "Chenxi Liu", "Mainak Chain", "Nivedita Melinkeri", "Aaron Cohen", "Venus Wang", "Kristie Seymore", "Sergey Zubkov", "Rahul Goel", "Summer Yue", "Sai Krishnakumaran", "Brian Albert", "Nate Hurley", "Motoki Sano", "Anhad Mohananey", "Jonah Joughin", "Egor Filonov", "Tomasz Kępa", "Yomna Eldawy", "Jiawern Lim", "Rahul Rishi", "Shirin Badiezadegan", "Taylor Bos", "Jerry Chang", "Sanil Jain", "Sri Gayatri Sundara Padmanabhan", "Subha Puttagunta", "Kalpesh Krishna", "Leslie Baker", "Norbert Kalb", "Vamsi Bedapudi", "Adam Kurzrok", "Shuntong Lei", "Anthony Yu", "Oren Litvin", "Xiang Zhou", "Zhichun Wu", "Sam Sobell", "Andrea Siciliano", "Alan Papir", "Robby Neale", "Jonas Bragagnolo", "Tej Toor", "Tina Chen", "Valentin Anklin", "Feiran Wang", "Richie Feng", "Milad Gholami", "Kevin Ling", "Lijuan Liu", "Jules Walter", "Hamid Moghaddam", "Arun Kishore", "Jakub Adamek", "Tyler Mercado", "Jonathan Mallinson", "Siddhinita Wandekar", "Stephen Cagle", "Eran Ofek", "Guillermo Garrido", "Clemens Lombriser", "Maksim Mukha", "Botu Sun", "Hafeezul Rahman Mohammad", "Josip Matak", "Yadi Qian", "Vikas Peswani", "Pawel Janus", "Quan Yuan", "Leif Schelin", "Oana David", "Ankur Garg", "Yifan He", "Oleksii Duzhyi", "Anton Älgmyr", "Timothée Lottaz", "Qi Li", "Vikas Yadav", "Luyao Xu", "Alex Chinien", "Rakesh Shivanna", "Aleksandr Chuklin", "Josie Li", "Carrie Spadine", "Travis Wolfe", "Kareem Mohamed", "Subhabrata Das", "Zihang Dai", "Kyle He", "Daniel von Dincklage", "Shyam Upadhyay", "Akanksha Maurya", "Luyan Chi", "Sebastian Krause", "Khalid Salama", "Pam G Rabinovitch", "Pavan Kumar Reddy M", "Aarush Selvan", "Mikhail Dektiarev", "Golnaz Ghiasi", "Erdem Guven", "Himanshu Gupta", "Boyi Liu", "Deepak Sharma", "Idan Heimlich Shtacher", "Shachi Paul", "Oscar Akerlund", "François-Xavier Aubet", "Terry Huang", "Chen Zhu", "Eric Zhu", "Elico Teixeira", "Matthew Fritze", "Francesco Bertolini", "Liana-Eleonora Marinescu", "Martin Bölle", "Dominik Paulus", "Khyatti Gupta", "Tejasi Latkar", "Max Chang", "Jason Sanders", "Roopa Wilson", "Xuewei Wu", "Yi-Xuan Tan", "Lam Nguyen Thiet", "Tulsee Doshi", "Sid Lall", "Swaroop Mishra", "Wanming Chen", "Thang Luong", "Seth Benjamin", "Jasmine Lee", "Ewa Andrejczuk", "Dominik Rabiej", "Vipul Ranjan", "Krzysztof Styrc", "Pengcheng Yin", "Jon Simon", "Malcolm Rose Harriott", "Mudit Bansal", "Alexei Robsky", "Geoff Bacon", "David Greene", "Daniil Mirylenka", "Chen Zhou", "Obaid Sarvana", "Abhimanyu Goyal", "Samuel Andermatt", "Patrick Siegler", "Ben Horn", "Assaf Israel", "Francesco Pongetti", "Chih-Wei \"Louis\" Chen", "Marco Selvatici", "Pedro Silva", "Kathie Wang", "Jackson Tolins", "Kelvin Guu", "Roey Yogev", "Xiaochen Cai", "Alessandro Agostini", "Maulik Shah", "Hung Nguyen", "Noah Ó Donnaile", "Sébastien Pereira", "Linda Friso", "Adam Stambler", "Adam Kurzrok", "Chenkai Kuang", "Yan Romanikhin", "Mark Geller", "ZJ Yan", "Kane Jang", "Cheng-Chun Lee", "Wojciech Fica", "Eric Malmi", "Qijun Tan", "Dan Banica", "Daniel Balle", "Ryan Pham", "Yanping Huang", "Diana Avram", "Hongzhi Shi", "Jasjot Singh", "Chris Hidey", "Niharika Ahuja", "Pranab Saxena", "Dan Dooley", "Srividya Pranavi Potharaju", "Eileen O'Neill", "Anand Gokulchandran", "Ryan Foley", "Kai Zhao", "Mike Dusenberry", "Yuan Liu", "Pulkit Mehta", "Ragha Kotikalapudi", "Chalence Safranek-Shrader", "Andrew Goodman", "Joshua Kessinger", "Eran Globen", "Prateek Kolhar", "Chris Gorgolewski", "Ali Ibrahim", "Yang Song", "Ali Eichenbaum", "Thomas Brovelli", "Sahitya Potluri", "Preethi Lahoti", "Cip Baetu", "Ali Ghorbani", "Charles Chen", "Andy Crawford", "Shalini Pal", "Mukund Sridhar", "Petru Gurita", "Asier Mujika", "Igor Petrovski", "Pierre-Louis Cedoz", "Chenmei Li", "Shiyuan Chen", "Niccolò Dal Santo", "Siddharth Goyal", "Jitesh Punjabi", "Karthik Kappaganthu", "Chester Kwak", "Pallavi LV", "Sarmishta Velury", "Himadri Choudhury", "Jamie Hall", "Premal Shah", "Ricardo Figueira", "Matt Thomas", "Minjie Lu", "Ting Zhou", "Chintu Kumar", "Thomas Jurdi", "Sharat Chikkerur", "Yenai Ma", "Adams Yu", "Soo Kwak", "Victor Ähdel", "Sujeevan Rajayogam", "Travis Choma", "Fei Liu", "Aditya Barua", "Colin Ji", "Ji Ho Park", "Vincent Hellendoorn", "Alex Bailey", "Taylan Bilal", "Huanjie Zhou", "Mehrdad Khatir", "Charles Sutton", "Wojciech Rzadkowski", "Fiona Macintosh", "Roopali Vij", "Konstantin Shagin", "Paul Medina", "Chen Liang", "Jinjing Zhou", "Pararth Shah", "Yingying Bi", "Attila Dankovics", "Shipra Banga", "Sabine Lehmann", "Marissa Bredesen", "Zifan Lin", "John Eric Hoffmann", "Jonathan Lai", "Raynald Chung", "Kai Yang", "Nihal Balani", "Arthur Bražinskas", "Andrei Sozanschi", "Matthew Hayes", "Héctor Fernández Alcalde", "Peter Makarov", "Will Chen", "Antonio Stella", "Liselotte Snijders", "Michael Mandl", "Ante Kärrman", "Paweł Nowak", "Xinyi Wu", "Alex Dyck", "Krishnan Vaidyanathan", "Raghavender R", "Jessica Mallet", "Mitch Rudominer", "Eric Johnston", "Sushil Mittal", "Akhil Udathu", "Janara Christensen", "Vishal Verma", "Zach Irving", "Andreas Santucci", "Gamaleldin Elsayed", "Elnaz Davoodi", "Marin Georgiev", "Ian Tenney", "Nan Hua", "Geoffrey Cideron", "Edouard Leurent", "Mahmoud Alnahlawi", "Ionut Georgescu", "Nan Wei", "Ivy Zheng", "Dylan Scandinaro", "Heinrich Jiang", "Jasper Snoek", "Mukund Sundararajan", "Xuezhi Wang", "Zack Ontiveros", "Itay Karo", "Jeremy Cole", "Vinu Rajashekhar", "Lara Tumeh", "Eyal Ben-David", "Rishub Jain", "Jonathan Uesato", "Romina Datta", "Oskar Bunyan", "Shimu Wu", "John Zhang", "Piotr Stanczyk", "Ye Zhang", "David Steiner", "Subhajit Naskar", "Michael Azzam", "Matthew Johnson", "Adam Paszke", "Chung-Cheng Chiu", "Jaume Sanchez Elias", "Afroz Mohiuddin", "Faizan Muhammad", "Jin Miao", "Andrew Lee", "Nino Vieillard", "Jane Park", "Jiageng Zhang", "Jeff Stanway", "Drew Garmon", "Abhijit Karmarkar", "Zhe Dong", "Jong Lee", "Aviral Kumar", "Luowei Zhou", "Jonathan Evens", "William Isaac", "Geoffrey Irving", "Edward Loper", "Michael Fink", "Isha Arkatkar", "Nanxin Chen", "Izhak Shafran", "Ivan Petrychenko", "Zhe Chen", "Johnson Jia", "Anselm Levskaya", "Zhenkai Zhu", "Peter Grabowski", "Yu Mao", "Alberto Magni", "Kaisheng Yao", "Javier Snaider", "Norman Casagrande", "Evan Palmer", "Paul Suganthan", "Alfonso Castaño", "Irene Giannoumis", "Wooyeol Kim", "Mikołaj Rybiński", "Ashwin Sreevatsa", "Jennifer Prendki", "David Soergel", "Adrian Goedeckemeyer", "Willi Gierke", "Mohsen Jafari", "Meenu Gaba", "Jeremy Wiesner", "Diana Gage Wright", "Yawen Wei", "Harsha Vashisht", "Yana Kulizhskaya", "Jay Hoover", "Maigo Le", "Lu Li", "Chimezie Iwuanyanwu", "Lu Liu", "Kevin Ramirez", "Andrey Khorlin", "Albert Cui", "Tian LIN", "Marcus Wu", "Ricardo Aguilar", "Keith Pallo", "Abhishek Chakladar", "Ginger Perng", "Elena Allica Abellan", "Mingyang Zhang", "Ishita Dasgupta", "Nate Kushman", "Ivo Penchev", "Alena Repina", "Xihui Wu", "Tom van der Weide", "Priya Ponnapalli", "Caroline Kaplan", "Jiri Simsa", "Shuangfeng Li", "Olivier Dousse", "Fan Yang", "Jeff Piper", "Nathan Ie", "Rama Pasumarthi", "Nathan Lintz", "Anitha Vijayakumar", "Daniel Andor", "Pedro Valenzuela", "Minnie Lui", "Cosmin Paduraru", "Daiyi Peng", "Katherine Lee", "Shuyuan Zhang", "Somer Greene", "Duc Dung Nguyen", "Paula Kurylowicz", "Cassidy Hardin", "Lucas Dixon", "Lili Janzer", "Kiam Choo", "Ziqiang Feng", "Biao Zhang", "Achintya Singhal", "Dayou Du", "Dan McKinnon", "Natasha Antropova", "Tolga Bolukbasi", "Orgad Keller", "David Reid", "Daniel Finchelstein", "Maria Abi Raad", "Remi Crocker", "Peter Hawkins", "Robert Dadashi", "Colin Gaffney", "Ken Franko", "Anna Bulanova", "Rémi Leblond", "Shirley Chung", "Harry Askham", "Luis C. Cobo", "Kelvin Xu", "Felix Fischer", "Jun Xu", "Christina Sorokin", "Chris Alberti", "Chu-Cheng Lin", "Colin Evans", "Alek Dimitriev", "Hannah Forbes", "Dylan Banarse", "Zora Tung", "Mark Omernick", "Colton Bishop", "Rachel Sterneck", "Rohan Jain", "Jiawei Xia", "Ehsan Amid", "Francesco Piccinno", "Xingyu Wang", "Praseem Banzal", "Daniel J. Mankowitz", "Alex Polozov", "Victoria Krakovna", "Sasha Brown", "MohammadHossein Bateni", "Dennis Duan", "Vlad Firoiu", "Meghana Thotakuri", "Tom Natan", "Matthieu Geist", "Ser tan Girgin", "Hui Li", "Jiayu Ye", "Ofir Roval", "Reiko Tojo", "Michael Kwong", "James Lee-Thorp", "Christopher Yew", "Danila Sinopalnikov", "Sabela Ramos", "John Mellor", "Abhishek Sharma", "Kathy Wu", "David Miller", "Nicolas Sonnerat", "Denis Vnukov", "Rory Greig", "Jennifer Beattie", "Emily Caveness", "Libin Bai", "Julian Eisenschlos", "Alex Korchemniy", "Tomy Tsai", "Mimi Jasarevic", "Weize Kong", "Phuong Dao", "Zeyu Zheng", "Frederick Liu", "Fan Yang", "Rui Zhu", "Tian Huey Teh", "Jason Sanmiya", "Evgeny Gladchenko", "Nejc Trdin", "Daniel Toyama", "Evan Rosen", "Sasan Tavakkol", "Linting Xue", "Chen Elkind", "Oliver Woodman", "John Carpenter", "George Papamakarios", "Rupert Kemp", "Sushant Kafle", "Tanya Grunina", "Rishika Sinha", "Alice Talbert", "Diane Wu", "Denese Owusu-Afriyie", "Cosmo Du", "Chloe Thornton", "Jordi Pont-Tuset", "Pradyumna Narayana", "Jing Li", "Saaber Fatehi", "John Wieting", "Omar Ajmeri", "Benigno Uria", "Yeongil Ko", "Laura Knight", "Amélie Héliou", "Ning Niu", "Shane Gu", "Chenxi Pang", "Yeqing Li", "Nir Levine", "Ariel Stolovich", "Rebeca Santamaria-Fernandez", "Sonam Goenka", "Wenny Yustalim", "Robin Strudel", "Ali Elqursh", "Charlie Deck", "Hyo Lee", "Zonglin Li", "Kyle Levin", "Raphael Hoffmann", "Dan Holtmann-Rice", "Olivier Bachem", "Sho Arora", "Christy Koh", "Soheil Hassas Yeganeh", "Siim Põder", "Mukarram Tariq", "Yanhua Sun", "Lucian Ionita", "Mojtaba Seyedhosseini", "Pouya Tafti", "Zhiyu Liu", "Anmol Gulati", "Jasmine Liu", "Xinyu Ye", "Bart Chrzaszcz", "Lily Wang", "Nikhil Sethi", "Tianrun Li", "Ben Brown", "Shreya Singh", "Wei Fan", "Aaron Parisi", "Joe Stanton", "Vinod Koverkathu", "Christopher A. Choquette-Choo", "Yunjie Li", "TJ Lu", "Abe Ittycheriah", "Prakash Shroff", "Mani Varadarajan", "Sanaz Bahargam", "Rob Willoughby", "David Gaddy", "Guillaume Desjardins", "Marco Cornero", "Brona Robenek", "Bhavishya Mittal", "Ben Albrecht", "Ashish Shenoy", "Fedor Moiseev", "Henrik Jacobsson", "Alireza Ghaffarkhah", "Morgane Rivière", "Alanna Walton", "Clément Crepy", "Alicia Parrish", "Zongwei Zhou", "Clement Farabet", "Carey Radebaugh", "Praveen Srinivasan", "Claudia van der Salm", "Andreas Fidjeland", "Salvatore Scellato", "Eri Latorre-Chimoto", "Hanna Klimczak-Plucińska", "David Bridson", "Dario de Cesare", "Tom Hudson", "Piermaria Mendolicchio", "Lexi Walker", "Alex Morris", "Matthew Mauger", "Alexey Guseynov", "Alison Reid", "Seth Odoom", "Lucia Loher", "Victor Cotruta", "Madhavi Yenugula", "Dominik Grewe", "Anastasia Petrushkina", "Tom Duerig", "Antonio Sanchez", "Steve Yadlowsky", "Amy Shen", "Amir Globerson", "Lynette Webb", "Sahil Dua", "Dong Li", "Surya Bhupatiraju", "Dan Hurt", "Haroon Qureshi", "Ananth Agarwal", "Tomer Shani", "Matan Eyal", "Anuj Khare", "Shreyas Rammohan Belle", "Lei Wang", "Chetan Tekur", "Mihir Sanjay Kale", "Jinliang Wei", "Ruoxin Sang", "Brennan Saeta", "Tyler Liechty", "Yi Sun", "Yao Zhao", "Stephan Lee", "Pandu Nayak", "Doug Fritz", "Manish Reddy Vuyyuru", "John Aslanides", "Nidhi Vyas", "Martin Wicke", "Xiao Ma", "Evgenii Eltyshev", "Nina Martin", "Hardie Cate", "James Manyika", "Keyvan Amiri", "Yelin Kim", "Xi Xiong", "Kai Kang", "Florian Luisier", "Nilesh Tripuraneni", "David Madras", "Mandy Guo", "Austin Waters", "Oliver Wang", "Joshua Ainslie", "Jason Baldridge", "Han Zhang", "Garima Pruthi", "Jakob Bauer", "Feng Yang", "Riham Mansour", "Jason Gelman", "Yang Xu", "George Polovets", "Ji Liu", "Honglong Cai", "Warren Chen", "XiangHai Sheng", "Emily Xue", "Sherjil Ozair", "Christof Angermueller", "Xiaowei Li", "Anoop Sinha", "Weiren Wang", "Julia Wiesinger", "Emmanouil Koukoumidis", "Yuan Tian", "Anand Iyer", "Madhu Gurumurthy", "Mark Goldenson", "Parashar Shah", "MK Blake", "Hongkun Yu", "Anthony Urbanowicz", "Jennimaria Palomaki", "Chrisantha Fernando", "Ken Durden", "Harsh Mehta", "Nikola Momchev", "Elahe Rahimtoroghi", "Maria Georgaki", "Amit Raul", "Sebastian Ruder", "Morgan Redshaw", "Jinhyuk Lee", "Denny Zhou", "Komal Jalan", "Dinghua Li", "Blake Hechtman", "Parker Schuh", "Milad Nasr", "Kieran Milan", "Vladimir Mikulik", "Juliana Franco", "Tim Green", "Nam Nguyen", "Joe Kelley", "Aroma Mahendru", "Andrea Hu", "Joshua Howland", "Ben Vargas", "Jeffrey Hui", "Kshitij Bansal", "Vikram Rao", "Rakesh Ghiya", "Emma Wang", "Ke Ye", "Jean Michel Sarr", "Melanie Moranski Preston", "Madeleine Elish", "Steve Li", "Aakash Kaku", "Jigar Gupta", "Ice Pasupat", "Da-Cheng Juan", "Milan Someswar", "Tejvi M.", "Xinyun Chen", "Aida Amini", "Alex Fabrikant", "Eric Chu", "Xuanyi Dong", "Amruta Muthal", "Senaka Buthpitiya", "Sarthak Jauhari", "Nan Hua", "Urvashi Khandelwal", "Ayal Hitron", "Jie Ren", "Larissa Rinaldi", "Shahar Drath", "Avigail Dabush", "Nan-Jiang Jiang", "Harshal Godhia", "Uli Sachs", "Anthony Chen", "Yicheng Fan", "Hagai Taitelbaum", "Hila Noga", "Zhuyun Dai", "James Wang", "Chen Liang", "Jenny Hamer", "Chun-Sung Ferng", "Chenel Elkind", "Aviel Atias", "Paulina Lee", "Vít Listík", "Mathias Carlen", "Jan van de Kerkhof", "Marcin Pikus", "Krunoslav Zaher", "Paul Müller", "Sasha Zykova", "Richard Stefanec", "Vitaly Gatsko", "Christoph Hirnschall", "Ashwin Sethi", "Xingyu Federico Xu", "Chetan Ahuja", "Beth Tsai", "Anca Stefanoiu", "Bo Feng", "Keshav Dhandhania", "Manish Katyal", "Akshay Gupta", "Atharva Parulekar", "Divya Pitta", "Jing Zhao", "Vivaan Bhatia", "Yashodha Bhavnani", "Omar Alhadlaq", "Xiaolin Li", "Peter Danenberg", "Dennis Tu", "Alex Pine", "Vera Filippova", "Abhipso Ghosh", "Ben Limonchik", "Bhargava Urala", "Chaitanya Krishna Lanka", "Derik Clive", "Yi Sun", "Edward Li", "Hao Wu", "Kevin Hongtongsak", "Ianna Li", "Kalind Thakkar", "Kuanysh Omarov", "Kushal Majmundar", "Michael Alverson", "Michael Kucharski", "Mohak Patel", "Mudit Jain", "Maksim Zabelin", "Paolo Pelagatti", "Rohan Kohli", "Saurabh Kumar", "Joseph Kim", "Swetha Sankar", "Vineet Shah", "Lakshmi Ramachandruni", "Xiangkai Zeng", "Ben Bariach", "Laura Weidinger", "Tu Vu", "Alek Andreev", "Antoine He", "Kevin Hui", "Sheleem Kashem", "Amar Subramanya", "Sissie Hsiao", "Demis Hassabis", "Koray Kavukcuoglu", "Adam Sadovsky", "Quoc Le", "Trevor Strohman", "Yonghui Wu", "Slav Petrov", "Jeffrey Dean", "Oriol Vinyals"], "title": "Gemini: A Family of Highly Capable Multimodal Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of the Gemini family in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases. We discuss our\napproach toward post-training and deploying Gemini models responsibly to users\nthrough services including Gemini, Gemini Advanced, Google AI Studio, and Cloud\nVertex AI."}
{"id": "2505.07172", "pdf": "https://arxiv.org/pdf/2505.07172", "abs": "https://arxiv.org/abs/2505.07172", "authors": ["Zexian Yang", "Dian Li", "Dayan Wu", "Gang Liu", "Weiping Wang"], "title": "Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant advancements in multimodal reasoning tasks, existing\nLarge Vision-Language Models (LVLMs) are prone to producing visually ungrounded\nresponses when interpreting associated images. In contrast, when humans embark\non learning new knowledge, they often rely on a set of fundamental pre-study\nprinciples: reviewing outlines to grasp core concepts, summarizing key points\nto guide their focus and enhance understanding. However, such preparatory\nactions are notably absent in the current instruction tuning processes. This\npaper presents Re-Critic, an easily scalable rationale-augmented framework\ndesigned to incorporate fundamental rules and chain-of-thought (CoT) as a\nbridge to enhance reasoning abilities. Specifically, Re-Critic develops a\nvisual rationale synthesizer that scalably augments raw instructions with\nrationale explanation. To probe more contextually grounded responses, Re-Critic\nemploys an in-context self-critic mechanism to select response pairs for\npreference tuning. Experiments demonstrate that models fine-tuned with our\nrationale-augmented dataset yield gains that extend beyond\nhallucination-specific tasks to broader multimodal reasoning tasks."}
{"id": "2505.06503", "pdf": "https://arxiv.org/pdf/2505.06503", "abs": "https://arxiv.org/abs/2505.06503", "authors": ["David Balaban"], "title": "Attention Mechanisms in Dynamical Systems: A Case Study with Predator-Prey Models", "categories": ["math.DS", "cs.AI", "es: 92B05 (Primary), 34C60, 37N25, 68T07, 93B30 (Secondary)"], "comment": "5 figures, 12 pages, python code included", "summary": "Attention mechanisms are widely used in artificial intelligence to enhance\nperformance and interpretability. In this paper, we investigate their utility\nin modeling classical dynamical systems -- specifically, a noisy predator-prey\n(Lotka-Volterra) system. We train a simple linear attention model on perturbed\ntime-series data to reconstruct system trajectories. Remarkably, the learned\nattention weights align with the geometric structure of the Lyapunov function:\nhigh attention corresponds to flat regions (where perturbations have small\neffect), and low attention aligns with steep regions (where perturbations have\nlarge effect). We further demonstrate that attention-based weighting can serve\nas a proxy for sensitivity analysis, capturing key phase-space properties\nwithout explicit knowledge of the system equations. These results suggest a\nnovel use of AI-derived attention for interpretable, data-driven analysis and\ncontrol of nonlinear systems. For example our framework could support future\nwork in biological modeling of circadian rhythms, and interpretable machine\nlearning for dynamical environments."}
{"id": "2505.06839", "pdf": "https://arxiv.org/pdf/2505.06839", "abs": "https://arxiv.org/abs/2505.06839", "authors": ["Enric Boix-Adsera", "Philippe Rigollet"], "title": "The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Mixture-of-Experts (MoE) layers are increasingly central to frontier model\narchitectures. By selectively activating parameters, they reduce computational\ncost while scaling total parameter count. This paper investigates the impact of\nthe number of active experts, termed granularity, comparing architectures with\nmany (e.g., 8 per layer in DeepSeek) to those with fewer (e.g., 1 per layer in\nLlama-4 models). We prove an exponential separation in network expressivity\nbased on this design parameter, suggesting that models benefit from higher\ngranularity. Experimental results corroborate our theoretical findings and\nillustrate this separation."}
{"id": "2405.06691", "pdf": "https://arxiv.org/pdf/2405.06691", "abs": "https://arxiv.org/abs/2405.06691", "authors": ["Lars Klein", "Nearchos Potamitis", "Roland Aydin", "Robert West", "Caglar Gulcehre", "Akhil Arora"], "title": "Fleet of Agents: Coordinated Problem Solving with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": "ICML 2025; 28 pages, 68 figures, 8 tables", "summary": "While numerous frameworks have been developed to enhance the reasoning\nabilities of large language models (LLMs), there is a scarcity of methods that\neffectively balance the trade-off between cost and quality. In this paper, we\nintroduce Fleet of Agents (FoA), a novel and intuitive yet principled framework\nutilizing LLMs as agents to navigate through dynamic tree searches, employing a\ngenetic-type particle filtering approach. FoA spawns a multitude of agents,\neach exploring the search space autonomously, followed by a selection phase\nwhere resampling based on a heuristic value function optimizes the balance\nbetween exploration and exploitation. This mechanism enables dynamic branching,\nadapting the exploration strategy based on discovered solutions. We conduct\nextensive experiments on three benchmark tasks, ``Game of 24'',\n``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs,\n``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average\nacross all tasks and LLMs, FoA obtains a quality improvement of ~5% while\nrequiring only ~40% of the cost of previous SOTA methods. Notably, our analyses\nreveal that (1) FoA achieves the best cost-quality trade-off among all\nbenchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B\nmodel. FoA is publicly available at https://github.com/au-clan/FoA."}
{"id": "2505.07198", "pdf": "https://arxiv.org/pdf/2505.07198", "abs": "https://arxiv.org/abs/2505.07198", "authors": ["Xufei Wang", "Gengxuan Tian", "Junqiao Zhao", "Siyue Tao", "Qiwen Gu", "Qiankun Yu", "Tiantian Feng"], "title": "Ranking-aware Continual Learning for LiDAR Place Recognition", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Place recognition plays a significant role in SLAM, robot navigation, and\nautonomous driving applications. Benefiting from deep learning, the performance\nof LiDAR place recognition (LPR) has been greatly improved. However, many\nexisting learning-based LPR methods suffer from catastrophic forgetting, which\nseverely harms the performance of LPR on previously trained places after\ntraining on a new environment. In this paper, we introduce a continual learning\nframework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate\nforgetting. Inspired by the ranking process of place recognition retrieval, we\npresent a ranking-aware knowledge distillation loss that encourages the network\nto preserve the high-level place recognition knowledge. We also introduce a\nknowledge fusion module to integrate the knowledge of old and new models for\nLiDAR place recognition. Our extensive experiments demonstrate that KDF can be\napplied to different networks to overcome catastrophic forgetting, surpassing\nthe state-of-the-art methods in terms of mean Recall@1 and forgetting score."}
{"id": "2505.06561", "pdf": "https://arxiv.org/pdf/2505.06561", "abs": "https://arxiv.org/abs/2505.06561", "authors": ["Danil Belov", "Artem Erkhov", "Elizaveta Pestova", "Ilya Osokin", "Dzmitry Tsetserukou", "Pavel Osinenko"], "title": "Quadrupedal Robot Skateboard Mounting via Reverse Curriculum Learning", "categories": ["cs.RO", "cs.AI", "math.OC"], "comment": null, "summary": "The aim of this work is to enable quadrupedal robots to mount skateboards\nusing Reverse Curriculum Reinforcement Learning. Although prior work has\ndemonstrated skateboarding for quadrupeds that are already positioned on the\nboard, the initial mounting phase still poses a significant challenge. A\ngoal-oriented methodology was adopted, beginning with the terminal phases of\nthe task and progressively increasing the complexity of the problem definition\nto approximate the desired objective. The learning process was initiated with\nthe skateboard rigidly fixed within the global coordinate frame and the robot\npositioned directly above it. Through gradual relaxation of these initial\nconditions, the learned policy demonstrated robustness to variations in\nskateboard position and orientation, ultimately exhibiting a successful\ntransfer to scenarios involving a mobile skateboard. The code, trained models,\nand reproducible examples are available at the following link:\nhttps://github.com/dancher00/quadruped-skateboard-mounting"}
{"id": "2505.06849", "pdf": "https://arxiv.org/pdf/2505.06849", "abs": "https://arxiv.org/abs/2505.06849", "authors": ["Tamilselvan Subramani", "Sebastian Bartscher"], "title": "Predictive Digital Twins for Thermal Management Using Machine Learning and Reduced-Order Models", "categories": ["cs.LG", "68T07, 65M99, 80A23", "I.2.6; G.1.8; J.2"], "comment": "10 pages, 2 tables, from M.Tech. thesis accepted at BITS Pilani, 2022", "summary": "Digital twins enable real-time simulation and prediction in engineering\nsystems. This paper presents a novel framework for predictive digital twins of\na headlamp heatsink, integrating physics-based reduced-order models (ROMs) from\ncomputational fluid dynamics (CFD) with supervised machine learning. A\ncomponent-based ROM library, derived via proper orthogonal decomposition (POD),\ncaptures thermal dynamics efficiently. Machine learning models, including\nDecision Trees, k-Nearest Neighbors, Support Vector Regression (SVR), and\nNeural Networks, predict optimal ROM configurations, enabling rapid digital\ntwin updates. The Neural Network achieves a mean absolute error (MAE) of\n54.240, outperforming other models. Quantitative comparisons of predicted and\noriginal values demonstrate high accuracy. This scalable, interpretable\nframework advances thermal management in automotive systems, supporting robust\ndesign and predictive maintenance."}
{"id": "2406.15163", "pdf": "https://arxiv.org/pdf/2406.15163", "abs": "https://arxiv.org/abs/2406.15163", "authors": ["Muhammad Imran", "Olga Kellert", "Carlos Gómez-Rodríguez"], "title": "A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses\nsaid bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance\nand better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one\nimproves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than\nTransformer-based models in polarity prediction tasks."}
{"id": "2505.07209", "pdf": "https://arxiv.org/pdf/2505.07209", "abs": "https://arxiv.org/abs/2505.07209", "authors": ["Yan Xie", "Zequn Zeng", "Hao Zhang", "Yucheng Ding", "Yi Wang", "Zhengjue Wang", "Bo Chen", "Hongwei Liu"], "title": "Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Concept Bottleneck Models (CBMs) try to make the decision-making process\ntransparent by exploring an intermediate concept space between the input image\nand the output prediction. Existing CBMs just learn coarse-grained relations\nbetween the whole image and the concepts, less considering local image\ninformation, leading to two main drawbacks: i) they often produce spurious\nvisual-concept relations, hence decreasing model reliability; and ii) though\nCBMs could explain the importance of every concept to the final prediction, it\nis still challenging to tell which visual region produces the prediction. To\nsolve these problems, this paper proposes a Disentangled Optimal Transport CBM\n(DOT-CBM) framework to explore fine-grained visual-concept relations between\nlocal image patches and concepts. Specifically, we model the concept prediction\nprocess as a transportation problem between the patches and concepts, thereby\nachieving explicit fine-grained feature alignment. We also incorporate\northogonal projection losses within the modality to enhance local feature\ndisentanglement. To further address the shortcut issues caused by statistical\nbiases in the data, we utilize the visual saliency map and concept label\nstatistics as transportation priors. Thus, DOT-CBM can visualize inversion\nheatmaps, provide more reliable concept predictions, and produce more accurate\nclass predictions. Comprehensive experiments demonstrate that our proposed\nDOT-CBM achieves SOTA performance on several tasks, including image\nclassification, local part detection and out-of-distribution generalization."}
{"id": "2505.06584", "pdf": "https://arxiv.org/pdf/2505.06584", "abs": "https://arxiv.org/abs/2505.06584", "authors": ["Ziluo Ding", "Haobin Jiang", "Yuxuan Wang", "Zhenguo Sun", "Yu Zhang", "Xiaojie Niu", "Ming Yang", "Weishuai Zeng", "Xinrun Xu", "Zongqing Lu"], "title": "JAEGER: Dual-Level Humanoid Whole-Body Controller", "categories": ["cs.RO", "cs.AI"], "comment": "15 pages, 2 figures", "summary": "This paper presents JAEGER, a dual-level whole-body controller for humanoid\nrobots that addresses the challenges of training a more robust and versatile\npolicy. Unlike traditional single-controller approaches, JAEGER separates the\ncontrol of the upper and lower bodies into two independent controllers, so that\nthey can better focus on their distinct tasks. This separation alleviates the\ndimensionality curse and improves fault tolerance. JAEGER supports both root\nvelocity tracking (coarse-grained control) and local joint angle tracking\n(fine-grained control), enabling versatile and stable movements. To train the\ncontroller, we utilize a human motion dataset (AMASS), retargeting human poses\nto humanoid poses through an efficient retargeting network, and employ a\ncurriculum learning approach. This method performs supervised learning for\ninitialization, followed by reinforcement learning for further exploration. We\nconduct our experiments on two humanoid platforms and demonstrate the\nsuperiority of our approach against state-of-the-art methods in both simulation\nand real environments."}
{"id": "2505.06852", "pdf": "https://arxiv.org/pdf/2505.06852", "abs": "https://arxiv.org/abs/2505.06852", "authors": ["Ziyi Liu", "Phuc Luong", "Mario Boley", "Daniel F. Schmidt"], "title": "Improving Random Forests by Smoothing", "categories": ["cs.LG", "stat.ML"], "comment": "14 pages, 2 figures, 4 pages appendix, 3 figures in appendix", "summary": "Gaussian process regression is a popular model in the small data regime due\nto its sound uncertainty quantification and the exploitation of the smoothness\nof the regression function that is encountered in a wide range of practical\nproblems. However, Gaussian processes perform sub-optimally when the degree of\nsmoothness is non-homogeneous across the input domain. Random forest regression\npartially addresses this issue by providing local basis functions of variable\nsupport set sizes that are chosen in a data-driven way. However, they do so at\nthe expense of forgoing any degree of smoothness, which often results in poor\nperformance in the small data regime. Here, we aim to combine the advantages of\nboth models by applying a kernel-based smoothing mechanism to a learned random\nforest or any other piecewise constant prediction function. As we demonstrate\nempirically, the resulting model consistently improves the predictive\nperformance of the underlying random forests and, in almost all test cases,\nalso improves the log loss of the usual uncertainty quantification based on\ninter-tree variance. The latter advantage can be attributed to the ability of\nthe smoothing model to take into account the uncertainty over the exact\ntree-splitting locations."}
{"id": "2406.17692", "pdf": "https://arxiv.org/pdf/2406.17692", "abs": "https://arxiv.org/abs/2406.17692", "authors": ["Thom Lake", "Eunsol Choi", "Greg Durrett"], "title": "From Distributional to Overton Pluralism: Investigating Large Language Model Alignment", "categories": ["cs.CL", "cs.LG"], "comment": "NAACL 2025 (Main Conference)", "summary": "The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment."}
{"id": "2505.07249", "pdf": "https://arxiv.org/pdf/2505.07249", "abs": "https://arxiv.org/abs/2505.07249", "authors": ["Philippe Colantoni", "Rafique Ahmed", "Prashant Ghimire", "Damien Muselet", "Alain Trémeau"], "title": "When Dance Video Archives Challenge Computer Vision", "categories": ["cs.CV"], "comment": null, "summary": "The accuracy and efficiency of human body pose estimation depend on the\nquality of the data to be processed and of the particularities of these data.\nTo demonstrate how dance videos can challenge pose estimation techniques, we\nproposed a new 3D human body pose estimation pipeline which combined up-to-date\ntechniques and methods that had not been yet used in dance analysis. Second, we\nperformed tests and extensive experimentations from dance video archives, and\nused visual analytic tools to evaluate the impact of several data parameters on\nhuman body pose. Our results are publicly available for research at\nhttps://www.couleur.org/articles/arXiv-1-2025/"}
{"id": "2505.06589", "pdf": "https://arxiv.org/pdf/2505.06589", "abs": "https://arxiv.org/abs/2505.06589", "authors": ["Gabriel Peyré"], "title": "Optimal Transport for Machine Learners", "categories": ["stat.ML", "cs.AI", "math.OC"], "comment": "arXiv admin note: text overlap with arXiv:1803.00567", "summary": "Optimal Transport is a foundational mathematical theory that connects\noptimization, partial differential equations, and probability. It offers a\npowerful framework for comparing probability distributions and has recently\nbecome an important tool in machine learning, especially for designing and\nevaluating generative models. These course notes cover the fundamental\nmathematical aspects of OT, including the Monge and Kantorovich formulations,\nBrenier's theorem, the dual and dynamic formulations, the Bures metric on\nGaussian distributions, and gradient flows. It also introduces numerical\nmethods such as linear programming, semi-discrete solvers, and entropic\nregularization. Applications in machine learning include topics like training\nneural networks via gradient flows, token dynamics in transformers, and the\nstructure of GANs and diffusion models. These notes focus primarily on\nmathematical content rather than deep learning techniques."}
{"id": "2505.06858", "pdf": "https://arxiv.org/pdf/2505.06858", "abs": "https://arxiv.org/abs/2505.06858", "authors": ["Tianyu Chen", "Haoyi Zhou", "Ying Li", "Hao Wang", "Zhenzhe Zhang", "Tianchen Zhu", "Shanghang Zhang", "Jianxin Li"], "title": "FreqMoE: Dynamic Frequency Enhancement for Neural PDE Solvers", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Fourier Neural Operators (FNO) have emerged as promising solutions for\nefficiently solving partial differential equations (PDEs) by learning\ninfinite-dimensional function mappings through frequency domain\ntransformations. However, the sparsity of high-frequency signals limits\ncomputational efficiency for high-dimensional inputs, and fixed-pattern\ntruncation often causes high-frequency signal loss, reducing performance in\nscenarios such as high-resolution inputs or long-term predictions. To address\nthese challenges, we propose FreqMoE, an efficient and progressive training\nframework that exploits the dependency of high-frequency signals on\nlow-frequency components. The model first learns low-frequency weights and then\napplies a sparse upward-cycling strategy to construct a mixture of experts\n(MoE) in the frequency domain, effectively extending the learned weights to\nhigh-frequency regions. Experiments on both regular and irregular grid PDEs\ndemonstrate that FreqMoE achieves up to 16.6% accuracy improvement while using\nmerely 2.1% parameters (47.32x reduction) compared to dense FNO. Furthermore,\nthe approach demonstrates remarkable stability in long-term predictions and\ngeneralizes seamlessly to various FNO variants and grid structures,\nestablishing a new ``Low frequency Pretraining, High frequency Fine-tuning''\nparadigm for solving PDEs."}
{"id": "2408.05093", "pdf": "https://arxiv.org/pdf/2408.05093", "abs": "https://arxiv.org/abs/2408.05093", "authors": ["Zikai Xie"], "title": "Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, submitted to ACL ARR", "summary": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability."}
{"id": "2505.07251", "pdf": "https://arxiv.org/pdf/2505.07251", "abs": "https://arxiv.org/abs/2505.07251", "authors": ["Wenqiang Wang", "Yangshijie Zhang"], "title": "Incomplete In-context Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision language models (LVLMs) achieve remarkable performance through\nVision In-context Learning (VICL), a process that depends significantly on\ndemonstrations retrieved from an extensive collection of annotated examples\n(retrieval database). Existing studies often assume that the retrieval database\ncontains annotated examples for all labels. However, in real-world scenarios,\ndelays in database updates or incomplete data annotation may result in the\nretrieval database containing labeled samples for only a subset of classes. We\nrefer to this phenomenon as an \\textbf{incomplete retrieval database} and\ndefine the in-context learning under this condition as \\textbf{Incomplete\nIn-context Learning (IICL)}. To address this challenge, we propose\n\\textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage\nframework designed to mitigate the limitations of IICL. The Iterative Judgments\nStage reformulates an \\(\\boldsymbol{m}\\)-class classification problem into a\nseries of \\(\\boldsymbol{m}\\) binary classification tasks, effectively\nconverting the IICL setting into a standard VICL scenario. The Integrated\nPrediction Stage further refines the classification process by leveraging both\nthe input image and the predictions from the Iterative Judgments Stage to\nenhance overall classification accuracy. IJIP demonstrates considerable\nperformance across two LVLMs and two datasets under three distinct conditions\nof label incompleteness, achieving the highest accuracy of 93.9\\%. Notably,\neven in scenarios where labels are fully available, IJIP still achieves the\nbest performance of all six baselines. Furthermore, IJIP can be directly\napplied to \\textbf{Prompt Learning} and is adaptable to the \\textbf{text\ndomain}."}
{"id": "2505.06595", "pdf": "https://arxiv.org/pdf/2505.06595", "abs": "https://arxiv.org/abs/2505.06595", "authors": ["Hai-Vy Nguyen", "Fabrice Gamboa", "Sixin Zhang", "Reda Chhaibi", "Serge Gratton", "Thierry Giaccone"], "title": "Feature Representation Transferring to Lightweight Models via Perception Coherence", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "math.PR"], "comment": null, "summary": "In this paper, we propose a method for transferring feature representation to\nlightweight student models from larger teacher models. We mathematically define\na new notion called \\textit{perception coherence}. Based on this notion, we\npropose a loss function, which takes into account the dissimilarities between\ndata points in feature space through their ranking. At a high level, by\nminimizing this loss function, the student model learns to mimic how the\nteacher model \\textit{perceives} inputs. More precisely, our method is\nmotivated by the fact that the representational capacity of the student model\nis weaker than the teacher model. Hence, we aim to develop a new method\nallowing for a better relaxation. This means that, the student model does not\nneed to preserve the absolute geometry of the teacher one, while preserving\nglobal coherence through dissimilarity ranking. Our theoretical insights\nprovide a probabilistic perspective on the process of feature representation\ntransfer. Our experiments results show that our method outperforms or achieves\non-par performance compared to strong baseline methods for representation\ntransferring."}
{"id": "2505.06863", "pdf": "https://arxiv.org/pdf/2505.06863", "abs": "https://arxiv.org/abs/2505.06863", "authors": ["Jiebo Song", "Huaming Ling"], "title": "Masked Subspace Clustering Methods", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "To further utilize the unsupervised features and pairwise information, we\npropose a general Bilevel Clustering Optimization (BCO) framework to improve\nthe performance of clustering. And then we introduce three special cases on\nsubspace clustering with two different types of masks. At first, we reformulate\nthe original subspace clustering as a Basic Masked Subspace Clustering (BMSC),\nwhich reformulate the diagonal constraints to a hard mask. Then, we provide a\nGeneral Masked Subspace Clustering (GMSC) method to integrate different\nclustering via a soft mask. Furthermore, based on BCO and GMSC, we induce a\nlearnable soft mask and design a Recursive Masked Subspace Clustering (RMSC)\nmethod that can alternately update the affinity matrix and the soft mask.\nNumerical experiments show that our models obtain significant improvement\ncompared with the baselines on several commonly used datasets, such as MNIST,\nUSPS, ORL, COIL20 and COIL100."}
{"id": "2408.05497", "pdf": "https://arxiv.org/pdf/2408.05497", "abs": "https://arxiv.org/abs/2408.05497", "authors": ["Maxwell J. Yin", "Boyu Wang", "Charles Ling"], "title": "MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Models trained on real-world data often mirror and exacerbate existing social\nbiases. Traditional methods for mitigating these biases typically require prior\nknowledge of the specific biases to be addressed, such as gender or racial\nbiases, and the social groups associated with each instance. In this paper, we\nintroduce a novel adversarial training strategy that operates independently of\nprior bias-type knowledge and protected attribute labels. Our approach\nproactively identifies biases during model training by utilizing auxiliary\nmodels, which are trained concurrently by predicting the performance of the\nmain model without relying on task labels. Additionally, we implement these\nauxiliary models at various levels of the feature maps of the main model,\nenabling the detection of a broader and more nuanced range of bias features.\nThrough experiments on racial and gender biases in sentiment and occupation\nclassification tasks, our method effectively reduces social biases without the\nneed for demographic annotations. Moreover, our approach not only matches but\noften surpasses the efficacy of methods that require detailed demographic\ninsights, marking a significant advancement in bias mitigation techniques."}
{"id": "2505.07254", "pdf": "https://arxiv.org/pdf/2505.07254", "abs": "https://arxiv.org/abs/2505.07254", "authors": ["Mohamed Nagy", "Naoufel Werghi", "Bilal Hassan", "Jorge Dias", "Majid Khonji"], "title": "Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This work addresses the critical lack of precision in state estimation in the\nKalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of\nselecting the appropriate motion model. Existing literature commonly relies on\nconstant motion models for estimating the states of objects, neglecting the\ncomplex motion dynamics unique to each object. Consequently, trajectory\ndivision and imprecise object localization arise, especially under occlusion\nconditions. The core of these challenges lies in the limitations of the current\nKalman filter formulation, which fails to account for the variability of motion\ndynamics as objects navigate their environments. This work introduces a novel\nformulation of the Kalman filter that incorporates motion dynamics, allowing\nthe motion model to adaptively adjust according to changes in the object's\nmovement. The proposed Kalman filter substantially improves state estimation,\nlocalization, and trajectory prediction compared to the traditional Kalman\nfilter. This is reflected in tracking performance that surpasses recent\nbenchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\\% and\n0.81\\% in higher order tracking accuracy (HOTA) and multi-object tracking\naccuracy (MOTA), respectively. Furthermore, the proposed Kalman filter\nconsistently outperforms the baseline across various detectors. Additionally,\nit shows an enhanced capability in managing long occlusions compared to the\nbaseline Kalman filter, achieving margins of 1.22\\% in higher order tracking\naccuracy (HOTA) and 1.55\\% in multi-object tracking accuracy (MOTA) on the\nKITTI dataset. The formulation's efficiency is evident, with an additional\nprocessing time of only approximately 0.078 ms per frame, ensuring its\napplicability in real-time applications."}
{"id": "2505.06612", "pdf": "https://arxiv.org/pdf/2505.06612", "abs": "https://arxiv.org/abs/2505.06612", "authors": ["Yuqin Lan"], "title": "Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation", "categories": ["cs.SI", "cs.AI", "cs.IR", "F.2.2; I.2.7"], "comment": "10 pages, 5 figures", "summary": "In the era of rapid development of social media, social recommendation\nsystems as hybrid recommendation systems have been widely applied. Existing\nmethods capture interest similarity between users to filter out\ninterest-irrelevant relations in social networks that inevitably decrease\nrecommendation accuracy, however, limited research has a focus on the mutual\ninfluence of semantic information between the social network and the user-item\ninteraction network for further improving social recommendation. To address\nthese issues, we introduce a social \\underline{r}ecommendation model with\nro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion\nand multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly\npropose to construct a social tensor in order to smooth the training process of\nthe model. Then, a graph convolutional network and a tensor convolutional\nnetwork are employed to capture user's item preference and social preference,\nrespectively. Considering the different semantic information in the user-item\ninteraction network and the social network, a bi-semantic coordination loss is\nproposed to model the mutual influence of semantic information. To alleviate\nthe interference of interest-irrelevant relations on multi-semantic modeling,\nwe further use Bayesian posterior probability to mine potential social\nrelations to replace social noise. Finally, the sliding window mechanism is\nutilized to update the social tensor as the input for the next iteration.\nExtensive experiments on three real datasets show Burger has a superior\nperformance compared with the state-of-the-art models."}
{"id": "2505.06874", "pdf": "https://arxiv.org/pdf/2505.06874", "abs": "https://arxiv.org/abs/2505.06874", "authors": ["Thanh Son Nguyen", "Van Thanh Nguyen", "Dang Minh Duc Nguyen"], "title": "Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting has attracted significant attention, leading to the\nde-velopment of a wide range of approaches, from traditional statistical\nmeth-ods to advanced deep learning models. Among them, the Auto-Regressive\nIntegrated Moving Average (ARIMA) model remains a widely adopted linear\ntechnique due to its effectiveness in modeling temporal dependencies in\neconomic, industrial, and social data. On the other hand, polynomial\nclassifi-ers offer a robust framework for capturing non-linear relationships\nand have demonstrated competitive performance in domains such as stock price\npre-diction. In this study, we propose a hybrid forecasting approach that\ninte-grates the ARIMA model with a polynomial classifier to leverage the\ncom-plementary strengths of both models. The hybrid method is evaluated on\nmultiple real-world time series datasets spanning diverse domains. Perfor-mance\nis assessed based on forecasting accuracy and computational effi-ciency.\nExperimental results reveal that the proposed hybrid model consist-ently\noutperforms the individual models in terms of prediction accuracy, al-beit with\na modest increase in execution time."}
{"id": "2408.09701", "pdf": "https://arxiv.org/pdf/2408.09701", "abs": "https://arxiv.org/abs/2408.09701", "authors": ["Mingda Li", "Abhijit Mishra", "Utkarsh Mujumdar"], "title": "Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer", "categories": ["cs.CL", "68T50 (Primary) 68T07 (Secondary)"], "comment": "Accepted and to appear in IJCNN 2025", "summary": "The use of Large Language Models (LLMs) for program code generation has\ngained substantial attention, but their biases and limitations with non-English\nprompts challenge global inclusivity. This paper investigates the complexities\nof multilingual prompt-based code generation. Our evaluations of LLMs,\nincluding CODELLAMA and CODEGEMMA, reveal significant disparities in code\nquality for non-English prompts; we also demonstrate the inadequacy of simple\napproaches like prompt translation, bootstrapped data augmentation, and\nfine-tuning. To address this, we propose a zero-shot cross-lingual approach\nusing a neural projection technique, integrating a cross-lingual encoder like\nLASER to map multilingual embeddings from it into the LLM's token space. This\nmethod requires training only on English data and scales effectively to other\nlanguages. Results on a translated and quality-checked MBPP dataset show\nsubstantial improvements in code quality. This research promotes a more\ninclusive code generation landscape by empowering LLMs with multilingual\ncapabilities to support the diverse linguistic spectrum in programming."}
{"id": "2505.07256", "pdf": "https://arxiv.org/pdf/2505.07256", "abs": "https://arxiv.org/abs/2505.07256", "authors": ["Christoph Huber", "Ludwig Schleeh", "Dino Knoll", "Michael Guthe"], "title": "Synthetic Similarity Search in Automotive Production", "categories": ["cs.CV"], "comment": "Accepted for publication in Procedia CIRP", "summary": "Visual quality inspection in automotive production is essential for ensuring\nthe safety and reliability of vehicles. Computer vision (CV) has become a\npopular solution for these inspections due to its cost-effectiveness and\nreliability. However, CV models require large, annotated datasets, which are\ncostly and time-consuming to collect. To reduce the need for extensive training\ndata, we propose a novel image classification pipeline that combines similarity\nsearch using a vision-based foundation model with synthetic data. Our approach\nleverages a DINOv2 model to transform input images into feature vectors, which\nare then compared to pre-classified reference images using cosine distance\nmeasurements. By utilizing synthetic data instead of real images as references,\nour pipeline achieves high classification accuracy without relying on real\ndata. We evaluate this approach in eight real-world inspection scenarios and\ndemonstrate that it meets the high performance requirements of production\nenvironments."}
{"id": "2505.06620", "pdf": "https://arxiv.org/pdf/2505.06620", "abs": "https://arxiv.org/abs/2505.06620", "authors": ["Dima Alattal", "Asal Khoshravan Azar", "Puja Myles", "Richard Branson", "Hatim Abdulhussein", "Allan Tucker"], "title": "Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations", "categories": ["cs.HC", "cs.AI", "H.5.2"], "comment": "47 pages", "summary": "There is a growing demand for the use of Artificial Intelligence (AI) and\nMachine Learning (ML) in healthcare, particularly as clinical decision support\nsystems to assist medical professionals. However, the complexity of many of\nthese models, often referred to as black box models, raises concerns about\ntheir safe integration into clinical settings as it is difficult to understand\nhow they arrived at their predictions. This paper discusses insights and\nrecommendations derived from an expert working group convened by the UK\nMedicine and Healthcare products Regulatory Agency (MHRA). The group consisted\nof healthcare professionals, regulators, and data scientists, with a primary\nfocus on evaluating the outputs from different AI algorithms in clinical\ndecision-making contexts. Additionally, the group evaluated findings from a\npilot study investigating clinicians' behaviour and interaction with AI methods\nduring clinical diagnosis. Incorporating AI methods is crucial for ensuring the\nsafety and trustworthiness of medical AI devices in clinical settings. Adequate\ntraining for stakeholders is essential to address potential issues, and further\ninsights and recommendations for safely adopting AI systems in healthcare\nsettings are provided."}
{"id": "2505.06892", "pdf": "https://arxiv.org/pdf/2505.06892", "abs": "https://arxiv.org/abs/2505.06892", "authors": ["Zhen Liu", "Yicheng Luo", "Boyuan Li", "Emadeldeen Eldele", "Min Wu", "Qianli Ma"], "title": "Learning Soft Sparse Shapes for Efficient Time-Series Classification", "categories": ["cs.LG"], "comment": "Accepted in ICML 2025", "summary": "Shapelets are discriminative subsequences (or shapes) with high\ninterpretability in time series classification. Due to the time-intensive\nnature of shapelet discovery, existing shapelet-based methods mainly focus on\nselecting discriminative shapes while discarding others to achieve candidate\nsubsequence sparsification. However, this approach may exclude beneficial\nshapes and overlook the varying contributions of shapelets to classification\nperformance. To this end, we propose a \\textbf{Soft} sparse \\textbf{Shape}s\n(\\textbf{SoftShape}) model for efficient time series classification. Our\napproach mainly introduces soft shape sparsification and soft shape learning\nblocks. The former transforms shapes into soft representations based on\nclassification contribution scores, merging lower-scored ones into a single\nshape to retain and differentiate all subsequence information. The latter\nfacilitates intra- and inter-shape temporal pattern learning, improving model\nefficiency by using sparsified soft shapes as inputs. Specifically, we employ a\nlearnable router to activate a subset of class-specific expert networks for\nintra-shape pattern learning. Meanwhile, a shared expert network learns\ninter-shape patterns by converting sparsified shapes into sequences. Extensive\nexperiments show that SoftShape outperforms state-of-the-art methods and\nproduces interpretable results."}
{"id": "2409.11055", "pdf": "https://arxiv.org/pdf/2409.11055", "abs": "https://arxiv.org/abs/2409.11055", "authors": ["Jemin Lee", "Sihyeong Park", "Jinse Kwon", "Jihun Oh", "Yongin Kwon"], "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IJCAI 2025, 21 pages, 2 figure", "summary": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in Coding and STEM tasks, though it occasionally reports\nimprovements in reasoning."}
{"id": "2505.07263", "pdf": "https://arxiv.org/pdf/2505.07263", "abs": "https://arxiv.org/abs/2505.07263", "authors": ["Xiaokun Wang", "Chris", "Jiangbo Pei", "Wei Shen", "Yi Peng", "Yunzhuo Hao", "Weijie Qiu", "Ai Jian", "Tianyidan Xie", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We propose Skywork-VL Reward, a multimodal reward model that provides reward\nsignals for both multimodal understanding and reasoning tasks. Our technical\napproach comprises two key components: First, we construct a large-scale\nmultimodal preference dataset that covers a wide range of tasks and scenarios,\nwith responses collected from both standard vision-language models (VLMs) and\nadvanced VLM reasoners. Second, we design a reward model architecture based on\nQwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage\nfine-tuning using pairwise ranking loss on pairwise preference data.\nExperimental evaluations show that Skywork-VL Reward achieves state-of-the-art\nresults on multimodal VL-RewardBench and exhibits competitive performance on\nthe text-only RewardBench benchmark. Furthermore, preference data constructed\nbased on our Skywork-VL Reward proves highly effective for training Mixed\nPreference Optimization (MPO), leading to significant improvements in\nmultimodal reasoning capabilities. Our results underscore Skywork-VL Reward as\na significant advancement toward general-purpose, reliable reward models for\nmultimodal alignment. Our model has been publicly released to promote\ntransparency and reproducibility."}
{"id": "2505.06625", "pdf": "https://arxiv.org/pdf/2505.06625", "abs": "https://arxiv.org/abs/2505.06625", "authors": ["Tianhao Cai", "Liang Wang", "Limin Xiao", "Meng Han", "Zeyu Wang", "Lin Sun", "Xiaojian Liao"], "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated NPUs", "categories": ["cs.AR", "cs.AI"], "comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)", "summary": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average)."}
{"id": "2505.06911", "pdf": "https://arxiv.org/pdf/2505.06911", "abs": "https://arxiv.org/abs/2505.06911", "authors": ["Lishan Yang", "Wei Zhang", "Quan Z. Sheng", "Weitong Chen", "Lina Yao", "Weitong Chen", "Ali Shakeri"], "title": "MMiC: Mitigating Modality Incompleteness in Clustered Federated Learning", "categories": ["cs.LG", "cs.AI", "I.2.11; I.2.7"], "comment": "10 pages, 10 figures, it's KDD'2025 under reviewing", "summary": "In the era of big data, data mining has become indispensable for uncovering\nhidden patterns and insights from vast and complex datasets. The integration of\nmultimodal data sources further enhances its potential. Multimodal Federated\nLearning (MFL) is a distributed approach that enhances the efficiency and\nquality of multimodal learning, ensuring collaborative work and privacy\nprotection. However, missing modalities pose a significant challenge in MFL,\noften due to data quality issues or privacy policies across the clients. In\nthis work, we present MMiC, a framework for Mitigating Modality incompleteness\nin MFL within the Clusters. MMiC replaces partial parameters within client\nmodels inside clusters to mitigate the impact of missing modalities.\nFurthermore, it leverages the Banzhaf Power Index to optimize client selection\nunder these conditions. Finally, MMiC employs an innovative approach to\ndynamically control global aggregation by utilizing Markovitz Portfolio\nOptimization. Extensive experiments demonstrate that MMiC consistently\noutperforms existing federated learning architectures in both global and\npersonalized performance on multimodal datasets with missing modalities,\nconfirming the effectiveness of our proposed solution."}
{"id": "2409.13746", "pdf": "https://arxiv.org/pdf/2409.13746", "abs": "https://arxiv.org/abs/2409.13746", "authors": ["Thanh Son Do", "Daniel B. Hier", "Tayo Obafemi-Ajayi"], "title": "Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy", "categories": ["cs.CL", "cs.AI", "I.2"], "comment": "Presented at 2025 IEEE Conference on Artificial Intelligence (CAI).\n  Santa Clara, CA. May 5, 2025", "summary": "This study evaluates the ability of large language models (LLMs) to map\nbiomedical ontology terms to their corresponding ontology IDs across the Human\nPhenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies.\nUsing counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate\nfor their prevalence in the biomedical literature, we examined the relationship\nbetween ontology ID prevalence and mapping accuracy. Results indicate that\nontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO\nIDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers.\nHigher prevalence of ontology IDs in the biomedical literature correlated with\nhigher mapping accuracy. Predictive models based on receiver operating\ncharacteristic (ROC) curves confirmed this relationship.\n  In contrast, this pattern did not apply to mapping protein names to Human\nGenome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline\nperformance (95%) in mapping protein names to HUGO gene symbols, with mapping\naccuracy unaffected by prevalence. We propose that the high prevalence of HUGO\ngene symbols in the literature has caused these symbols to become lexicalized,\nenabling GPT-4 to map protein names to HUGO gene symbols with high accuracy.\nThese findings highlight the limitations of LLMs in mapping ontology terms to\nlow-prevalence ontology IDs and underscore the importance of incorporating\nontology ID prevalence into the training and evaluation of LLMs for biomedical\napplications."}
{"id": "2505.07300", "pdf": "https://arxiv.org/pdf/2505.07300", "abs": "https://arxiv.org/abs/2505.07300", "authors": ["Sofia Casarin", "Sergio Escalera", "Oswald Lanz"], "title": "L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers", "categories": ["cs.CV"], "comment": "accepted at CVPR 2025", "summary": "Training-free Neural Architecture Search (NAS) efficiently identifies\nhigh-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot\nand one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the\nneed for model training, and (ii) interpretable, with proxy designs often\ntheoretically grounded. Despite rapid developments in the field, current SOTA\nZC proxies are typically constrained to well-established convolutional search\nspaces. With the rise of Large Language Models shaping the future of deep\nlearning, this work extends ZC proxy applicability to Vision Transformers\n(ViTs). We present a new benchmark using the Autoformer search space evaluated\non 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients\ninformation (L-SWAG), a novel, generalizable metric that characterizes both\nconvolutional and transformer architectures across 14 tasks. Additionally,\nprevious works highlighted how different proxies contain complementary\ninformation, motivating the need for a ML model to identify useful\ncombinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low\nInformation gain and Bias Re-Alignment), a method that strategically combines\nproxies to best represent a specific benchmark. Integrated into the NAS search,\nLIBRA-NAS outperforms evolution and gradient-based NAS techniques by\nidentifying an architecture with a 17.0% test error on ImageNet1k in just 0.1\nGPU days."}
{"id": "2505.06632", "pdf": "https://arxiv.org/pdf/2505.06632", "abs": "https://arxiv.org/abs/2505.06632", "authors": ["Rathin Chandra Shit", "Sharmila Subudhi"], "title": "AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles", "categories": ["cs.CR", "cs.AI"], "comment": "Scheduled for presentation at an upcoming conference", "summary": "Autonomous Vehicles (AV) proliferation brings important and pressing security\nand reliability issues that must be dealt with to guarantee public safety and\nhelp their widespread adoption. The contribution of the proposed research is\ntowards achieving more secure, reliable, and trustworthy autonomous\ntransportation system by providing more capabilities for anomaly detection,\ndata provenance, and real-time response in safety critical AV deployments. In\nthis research, we develop a new framework that combines the power of Artificial\nIntelligence (AI) for real-time anomaly detection with blockchain technology to\ndetect and prevent any malicious activity including sensor failures in AVs.\nThrough Long Short-Term Memory (LSTM) networks, our approach continually\nmonitors associated multi-sensor data streams to detect anomalous patterns that\nmay represent cyberattacks as well as hardware malfunctions. Further, this\nframework employs a decentralized platform for securely storing sensor data and\nanomaly alerts in a blockchain ledger for data incorruptibility and\nauthenticity, while offering transparent forensic features. Moreover, immediate\nautomated response mechanisms are deployed using smart contracts when anomalies\nare found. This makes the AV system more resilient to attacks from both\ncyberspace and hardware component failure. Besides, we identify potential\nchallenges of scalability in handling high frequency sensor data, computational\nconstraint in resource constrained environment, and of distributed data storage\nin terms of privacy."}
{"id": "2505.06917", "pdf": "https://arxiv.org/pdf/2505.06917", "abs": "https://arxiv.org/abs/2505.06917", "authors": ["Yuqi Xiong", "Yang Wen"], "title": "Non-Stationary Time Series Forecasting Based on Fourier Analysis and Cross Attention Mechanism", "categories": ["cs.LG"], "comment": "IJCNN 2025", "summary": "Time series forecasting has important applications in financial analysis,\nweather forecasting, and traffic management. However, existing deep learning\nmodels are limited in processing non-stationary time series data because they\ncannot effectively capture the statistical characteristics that change over\ntime. To address this problem, this paper proposes a new framework, AEFIN,\nwhich enhances the information sharing ability between stable and unstable\ncomponents by introducing a cross-attention mechanism, and combines Fourier\nanalysis networks with MLP to deeply explore the seasonal patterns and trend\ncharacteristics in unstable components. In addition, we design a new loss\nfunction that combines time-domain stability constraints, time-domain\ninstability constraints, and frequency-domain stability constraints to improve\nthe accuracy and robustness of forecasting. Experimental results show that\nAEFIN outperforms the most common models in terms of mean square error and mean\nabsolute error, especially under non-stationary data conditions, and shows\nexcellent forecasting capabilities. This paper provides an innovative solution\nfor the modeling and forecasting of non-stationary time series data, and\ncontributes to the research of deep learning for complex time series."}
{"id": "2410.01294", "pdf": "https://arxiv.org/pdf/2410.01294", "abs": "https://arxiv.org/abs/2410.01294", "authors": ["Brian R. Y. Huang", "Maximilian Li", "Leonard Tang"], "title": "Endless Jailbreaks with Bijection Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite extensive safety measures, LLMs are vulnerable to adversarial inputs,\nor jailbreaks, which can elicit unsafe behaviors. In this work, we introduce\nbijection learning, a powerful attack algorithm which automatically fuzzes LLMs\nfor safety vulnerabilities using randomly-generated encodings whose complexity\ncan be tightly controlled. We leverage in-context learning to teach models\nbijective encodings, pass encoded queries to the model to bypass built-in\nsafety mechanisms, and finally decode responses back into English. Our attack\nis extremely effective on a wide range of frontier language models. Moreover,\nby controlling complexity parameters such as number of key-value mappings in\nthe encodings, we find a close relationship between the capability level of the\nattacked LLM and the average complexity of the most effective bijection\nattacks. Our work highlights that new vulnerabilities in frontier models can\nemerge with scale: more capable models are more severely jailbroken by\nbijection attacks."}
{"id": "2505.07301", "pdf": "https://arxiv.org/pdf/2505.07301", "abs": "https://arxiv.org/abs/2505.07301", "authors": ["Katsuki Shimbo", "Hiromu Taketsugu", "Norimichi Ukita"], "title": "Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos", "categories": ["cs.CV"], "comment": "5 pages, 4 figures", "summary": "In 3D Human Motion Prediction (HMP), conventional methods train HMP models\nwith expensive motion capture data. However, the data collection cost of such\nmotion capture data limits the data diversity, which leads to poor\ngeneralizability to unseen motions or subjects. To address this issue, this\npaper proposes to enhance HMP with additional learning using estimated poses\nfrom easily available videos. The 2D poses estimated from the monocular videos\nare carefully transformed into motion capture-style 3D motions through our\npipeline. By additional learning with the obtained motions, the HMP model is\nadapted to the test domain. The experimental results demonstrate the\nquantitative and qualitative impact of our method."}
{"id": "2505.06652", "pdf": "https://arxiv.org/pdf/2505.06652", "abs": "https://arxiv.org/abs/2505.06652", "authors": ["Ernesto Giralt Hernandez", "Lazaro Antonio Bueno Perez"], "title": "Enfoque Odychess: Un método dialéctico, constructivista y adaptativo para la enseñanza del ajedrez con inteligencias artificiales generativas", "categories": ["cs.CY", "cs.AI"], "comment": "Full article in Spanish", "summary": "Chess teaching has evolved through different approaches, however, traditional\nmethodologies, often based on memorization, contrast with the new possibilities\noffered by generative artificial intelligence, a technology still little\nexplored in this field. This study seeks to empirically validate the\neffectiveness of the Odychess Approach in improving chess knowledge, strategic\nunderstanding, and metacognitive skills in students. A quasi-experimental study\nwas conducted with a pre-test/post-test design and a control group (N=60). The\nexperimental intervention implemented the Odychess Approach, incorporating a\nLlama 3.3 language model that was specifically adapted using\nParameter-Efficient Fine-Tuning (PEFT) techniques to act as a Socratic chess\ntutor. Quantitative assessment instruments were used to measure chess\nknowledge, strategic understanding, and metacognitive skills before and after\nthe intervention. The results of the quasi-experimental study showed\nsignificant improvements in the experimental group compared to the control\ngroup in the three variables analyzed: chess knowledge, strategic\nunderstanding, and metacognitive skills. The complementary qualitative analysis\nrevealed greater analytical depth, more developed dialectical reasoning, and\nincreased intrinsic motivation in students who participated in the Odychess\nmethod-based intervention. The Odychess Approach represents an effective\npedagogical methodology for teaching chess, demonstrating the potential of the\nsynergistic integration of constructivist and dialectical principles with\ngenerative artificial intelligence. The implications of this work are relevant\nfor educators and institutions interested in adopting innovative pedagogical\ntechnologies and for researchers in the field of AI applied to education,\nhighlighting the transferability of the language model adaptation methodology\nto other educational domains."}
{"id": "2505.06936", "pdf": "https://arxiv.org/pdf/2505.06936", "abs": "https://arxiv.org/abs/2505.06936", "authors": ["Mohammad Mashayekhi", "Kamran Salehian"], "title": "AI-Powered Inverse Design of Ku-Band SIW Resonant Structures by Iterative Residual Correction Network", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 14 figures", "summary": "Inverse electromagnetic modeling has emerged as a powerful approach for\ndesigning complex microwave structures with high accuracy and efficiency. In\nthis study, we propose an Iterative Residual Correction Network (IRC-Net) for\nthe inverse design of Ku-band Substrate Integrated Waveguide (SIW) components\nbased on multimode resonators. We use a multimode resonance structure to\ndemonstrate that it is possible to control the resonances of the structure.\nTherefore, these structures can be used for resonant components and smart\nfilter design. The proposed deep learning architecture leverages residual\nneural networks to overcome the limitations of traditional inverse design\ntechniques, such as the Feedforward Inverse Model (FIM), offering improved\ngeneralization and prediction accuracy. The approach begins with a FIM to\ngenerate initial design estimates, followed by an iterative correction strategy\ninspired by the Hybrid Inverse-Forward Residual Refinement Network\n(HiFR\\textsuperscript{2}-Net), which we call IRC-Net. Experiments demonstrate\nthat the IRC-Net achieves substantial improvements in prediction accuracy\ncompared to traditional single-stage networks, validated through statistical\nmetrics, full-wave electromagnetic simulations, and measurements. To validate\nthe proposed framework, we first design and fabricate a three-resonance SIW\nstructure. Next, we apply the trained IRC-Net model to predict the geometry of\na four-resonance structure based on its desired frequency response. Both\ndesigns are fabricated and tested, showing strong agreement between the\nsimulated, predicted, and measured results, confirming the effectiveness and\npracticality of the proposed method."}
{"id": "2410.14812", "pdf": "https://arxiv.org/pdf/2410.14812", "abs": "https://arxiv.org/abs/2410.14812", "authors": ["Victoria Lin", "Louis-Philippe Morency", "Eli Ben-Michael"], "title": "Isolated Causal Effects of Natural Language", "categories": ["cs.CL", "stat.ME"], "comment": "ICML 2025", "summary": "As language technologies become widespread, it is important to understand how\nchanges in language affect reader perceptions and behaviors. These\nrelationships may be formalized as the isolated causal effect of some focal\nlanguage-encoded intervention (e.g., factual inaccuracies) on an external\noutcome (e.g., readers' beliefs). In this paper, we introduce a formal\nestimation framework for isolated causal effects of language. We show that a\ncore challenge of estimating isolated effects is the need to approximate all\nnon-focal language outside of the intervention. Drawing on the principle of\nomitted variable bias, we provide measures for evaluating the quality of both\nnon-focal language approximations and isolated effect estimates themselves. We\nfind that poor approximation of non-focal language can lead to bias in the\ncorresponding isolated effect estimates due to omission of relevant variables,\nand we show how to assess the sensitivity of effect estimates to such bias\nalong the two key axes of fidelity and overlap. In experiments on\nsemi-synthetic and real-world data, we validate the ability of our framework to\ncorrectly recover isolated effects and demonstrate the utility of our proposed\nmeasures."}
{"id": "2505.07306", "pdf": "https://arxiv.org/pdf/2505.07306", "abs": "https://arxiv.org/abs/2505.07306", "authors": ["Sander De Coninck", "Emilio Gamba", "Bart Van Doninck", "Abdellatif Bey-Temsamani", "Sam Leroux", "Pieter Simoens"], "title": "Enabling Privacy-Aware AI-Based Ergonomic Analysis", "categories": ["cs.CV"], "comment": "Accepted and presented at the 35th CIRP Design conference", "summary": "Musculoskeletal disorders (MSDs) are a leading cause of injury and\nproductivity loss in the manufacturing industry, incurring substantial economic\ncosts. Ergonomic assessments can mitigate these risks by identifying workplace\nadjustments that improve posture and reduce strain. Camera-based systems offer\na non-intrusive, cost-effective method for continuous ergonomic tracking, but\nthey also raise significant privacy concerns. To address this, we propose a\nprivacy-aware ergonomic assessment framework utilizing machine learning\ntechniques. Our approach employs adversarial training to develop a lightweight\nneural network that obfuscates video data, preserving only the essential\ninformation needed for human pose estimation. This obfuscation ensures\ncompatibility with standard pose estimation algorithms, maintaining high\naccuracy while protecting privacy. The obfuscated video data is transmitted to\na central server, where state-of-the-art keypoint detection algorithms extract\nbody landmarks. Using multi-view integration, 3D keypoints are reconstructed\nand evaluated with the Rapid Entire Body Assessment (REBA) method. Our system\nprovides a secure, effective solution for ergonomic monitoring in industrial\nenvironments, addressing both privacy and workplace safety concerns."}
{"id": "2505.06682", "pdf": "https://arxiv.org/pdf/2505.06682", "abs": "https://arxiv.org/abs/2505.06682", "authors": ["Zijian Zhao"], "title": "A Short Overview of Multi-Modal Wi-Fi Sensing", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Wi-Fi sensing has emerged as a significant technology in wireless sensing and\nIntegrated Sensing and Communication (ISAC), offering benefits such as low\ncost, high penetration, and enhanced privacy. Currently, it is widely utilized\nin various applications, including action recognition, human localization, and\ncrowd counting. However, Wi-Fi sensing also faces challenges, such as low\nrobustness and difficulties in data collection. Recently, there has been an\nincreasing focus on multi-modal Wi-Fi sensing, where other modalities can act\nas teachers, providing ground truth or robust features for Wi-Fi sensing models\nto learn from, or can be directly fused with Wi-Fi for enhanced sensing\ncapabilities. Although these methods have demonstrated promising results and\nsubstantial value in practical applications, there is a lack of comprehensive\nsurveys reviewing them. To address this gap, this paper reviews the multi-modal\nWi-Fi sensing literature \\textbf{from the past 24 months} and highlights the\ncurrent limitations, challenges and future directions in this field."}
{"id": "2505.06945", "pdf": "https://arxiv.org/pdf/2505.06945", "abs": "https://arxiv.org/abs/2505.06945", "authors": ["Maryam Farhadizadeh", "Maria Weymann", "Michael Blaß", "Johann Kraus", "Christopher Gundler", "Sebastian Walter", "Noah Hempen", "Harald Binde", "Nadine Binder"], "title": "A systematic review of challenges and proposed solutions in modeling multimodal data", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Multimodal data modeling has emerged as a powerful approach in clinical\nresearch, enabling the integration of diverse data types such as imaging,\ngenomics, wearable sensors, and electronic health records. Despite its\npotential to improve diagnostic accuracy and support personalized care,\nmodeling such heterogeneous data presents significant technical challenges.\nThis systematic review synthesizes findings from 69 studies to identify common\nobstacles, including missing modalities, limited sample sizes, dimensionality\nimbalance, interpretability issues, and finding the optimal fusion techniques.\nWe highlight recent methodological advances, such as transfer learning,\ngenerative models, attention mechanisms, and neural architecture search that\noffer promising solutions. By mapping current trends and innovations, this\nreview provides a comprehensive overview of the field and offers practical\ninsights to guide future research and development in multimodal modeling for\nmedical applications."}
{"id": "2410.18966", "pdf": "https://arxiv.org/pdf/2410.18966", "abs": "https://arxiv.org/abs/2410.18966", "authors": ["Yujuan Fu", "Ozlem Uzuner", "Meliha Yetisgen", "Fei Xia"], "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions", "categories": ["cs.CL"], "comment": "This paper is accepted by NAACL 2025 findings. Link to the paper\n  presentation: https://youtu.be/IhaxwbZOcaU", "summary": "Large language models (LLMs) have demonstrated great performance across\nvarious benchmarks, showing potential as general-purpose task solvers. However,\nas LLMs are typically trained on vast amounts of data, a significant concern in\ntheir evaluation is data contamination, where overlap between training data and\nevaluation datasets inflates performance assessments. Multiple approaches have\nbeen developed to identify data contamination. These approaches rely on\nspecific assumptions that may not hold universally across different settings.\nTo bridge this gap, we systematically review 50 papers on data contamination\ndetection, categorize the underlying assumptions, and assess whether they have\nbeen rigorously validated. We identify and analyze eight categories of\nassumptions and test three of them as case studies. Our case studies focus on\ndetecting direct, instance-level data contamination, which is also referred to\nas Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches\nbased on these three assumptions can have similar performance to random\nguessing, on datasets used in LLM pretraining, suggesting that current LLMs\nmight learn data distributions rather than memorizing individual instances.\nMeanwhile, MIA can easily fail when there are data distribution shifts between\nthe seen and unseen instances."}
{"id": "2505.07322", "pdf": "https://arxiv.org/pdf/2505.07322", "abs": "https://arxiv.org/abs/2505.07322", "authors": ["Gang He", "Siqi Wang", "Kepeng Xu", "Lin Zhang"], "title": "RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming\nincreasingly prevalent, intensifying the demand for converting Standard Dynamic\nRange (SDR) content to HDR. Existing methods primarily rely on fixed tone\nmapping operators, which are inadequate for handling SDR inputs with diverse\nstyles commonly found in real-world scenarios. To address this challenge, we\npropose a generalized SDR-to-HDR method that handles diverse styles in\nreal-world SDR content, termed Realistic Style Disentangled Representation\nLearning (RealRep). By disentangling luminance and chrominance, we analyze the\nintrinsic differences between contents with varying styles and propose a\ndisentangled multi-view style representation learning method. This approach\ncaptures the guidance prior of true luminance and chrominance distributions\nacross different styles, even when the SDR style distributions exhibit\nsignificant variations, thereby establishing a robust embedding space for\ninverse tone mapping. Motivated by the difficulty of directly utilizing\ndegradation representation priors, we further introduce the Degradation-Domain\nAware Controlled Mapping Network (DDACMNet), a two-stage framework that\nperforms adaptive hierarchical mapping guided by a control-aware normalization\nmechanism. DDACMNet dynamically modulates the mapping process via\ndegradation-conditioned hierarchical features, enabling robust adaptation\nacross diverse degradation domains. Extensive experiments show that RealRep\nconsistently outperforms state-of-the-art methods with superior generalization\nand perceptually faithful HDR color gamut reconstruction."}
{"id": "2505.06737", "pdf": "https://arxiv.org/pdf/2505.06737", "abs": "https://arxiv.org/abs/2505.06737", "authors": ["Ahmed Abouelazm", "Jonas Michel", "Helen Gremmelmaier", "Tim Joseph", "Philip Schörner", "J. Marius Zöllner"], "title": "Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted in the 36th IEEE Intelligent vehicles Symposium (IV 2025)", "summary": "Reinforcement Learning (RL) is a promising approach for achieving autonomous\ndriving due to robust decision-making capabilities. RL learns a driving policy\nthrough trial and error in traffic scenarios, guided by a reward function that\ncombines the driving objectives. The design of such reward function has\nreceived insufficient attention, yielding ill-defined rewards with various\npitfalls. Safety, in particular, has long been regarded only as a penalty for\ncollisions. This leaves the risks associated with actions leading up to a\ncollision unaddressed, limiting the applicability of RL in real-world\nscenarios. To address these shortcomings, our work focuses on enhancing the\nreward formulation by defining a set of driving objectives and structuring them\nhierarchically. Furthermore, we discuss the formulation of these objectives in\na normalized manner to transparently determine their contribution to the\noverall reward. Additionally, we introduce a novel risk-aware objective for\nvarious driving interactions based on a two-dimensional ellipsoid function and\nan extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the\nefficacy of our proposed reward in unsignalized intersection scenarios with\nvarying traffic densities. The approach decreases collision rates by 21\\% on\naverage compared to baseline rewards and consistently surpasses them in route\nprogress and cumulative reward, demonstrating its capability to promote safer\ndriving behaviors while maintaining high-performance levels."}
{"id": "2505.06978", "pdf": "https://arxiv.org/pdf/2505.06978", "abs": "https://arxiv.org/abs/2505.06978", "authors": ["Lei Lei", "Kan Zheng", "Xuemin", "Shen"], "title": "Learning Value of Information towards Joint Communication and Control in 6G V2X", "categories": ["cs.LG"], "comment": null, "summary": "As Cellular Vehicle-to-Everything (C-V2X) evolves towards future\nsixth-generation (6G) networks, Connected Autonomous Vehicles (CAVs) are\nemerging to become a key application. Leveraging data-driven Machine Learning\n(ML), especially Deep Reinforcement Learning (DRL), is expected to\nsignificantly enhance CAV decision-making in both vehicle control and V2X\ncommunication under uncertainty. These two decision-making processes are\nclosely intertwined, with the value of information (VoI) acting as a crucial\nbridge between them. In this paper, we introduce Sequential Stochastic Decision\nProcess (SSDP) models to define and assess VoI, demonstrating their application\nin optimizing communication systems for CAVs. Specifically, we formally define\nthe SSDP model and demonstrate that the MDP model is a special case of it. The\nSSDP model offers a key advantage by explicitly representing the set of\ninformation that can enhance decision-making when available. Furthermore, as\ncurrent research on VoI remains fragmented, we propose a systematic VoI\nmodeling framework grounded in the MDP, Reinforcement Learning (RL) and Optimal\nControl theories. We define different categories of VoI and discuss their\ncorresponding estimation methods. Finally, we present a structured approach to\nleverage the various VoI metrics for optimizing the ``When\", ``What\", and\n``How\" to communicate problems. For this purpose, SSDP models are formulated\nwith VoI-associated reward functions derived from VoI-based optimization\nobjectives. While we use a simple vehicle-following control problem to\nillustrate the proposed methodology, it holds significant potential to\nfacilitate the joint optimization of stochastic, sequential control and\ncommunication decisions in a wide range of networked control systems."}
{"id": "2411.02316", "pdf": "https://arxiv.org/pdf/2411.02316", "abs": "https://arxiv.org/abs/2411.02316", "authors": ["Mete Ismayilzada", "Claire Stevenson", "Lonneke van der Plas"], "title": "Evaluating Creative Short Story Generation in Humans and Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICCC 2025", "summary": "Story-writing is a fundamental aspect of human imagination, relying heavily\non creativity to produce narratives that are novel, effective, and surprising.\nWhile large language models (LLMs) have demonstrated the ability to generate\nhigh-quality stories, their creative story-writing capabilities remain\nunder-explored. In this work, we conduct a systematic analysis of creativity in\nshort story generation across 60 LLMs and 60 people using a five-sentence\ncue-word-based creative story-writing task. We use measures to automatically\nevaluate model- and human-generated stories across several dimensions of\ncreativity, including novelty, surprise, diversity, and linguistic complexity.\nWe also collect creativity ratings and Turing Test classifications from\nnon-expert and expert human raters and LLMs. Automated metrics show that LLMs\ngenerate stylistically complex stories, but tend to fall short in terms of\nnovelty, surprise and diversity when compared to average human writers. Expert\nratings generally coincide with automated metrics. However, LLMs and\nnon-experts rate LLM stories to be more creative than human-generated stories.\nWe discuss why and how these differences in ratings occur, and their\nimplications for both human and artificial creativity."}
{"id": "2505.07333", "pdf": "https://arxiv.org/pdf/2505.07333", "abs": "https://arxiv.org/abs/2505.07333", "authors": ["Matthew Marchellus", "Nadhira Noor", "In Kyu Park"], "title": "Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025", "summary": "Fast 3D clothed human reconstruction from monocular video remains a\nsignificant challenge in computer vision, particularly in balancing\ncomputational efficiency with reconstruction quality. Current approaches are\neither focused on static image reconstruction but too computationally\nintensive, or achieve high quality through per-video optimization that requires\nminutes to hours of processing, making them unsuitable for real-time\napplications. To this end, we present TemPoFast3D, a novel method that\nleverages temporal coherency of human appearance to reduce redundant\ncomputation while maintaining reconstruction quality. Our approach is a\n\"plug-and play\" solution that uniquely transforms pixel-aligned reconstruction\nnetworks to handle continuous video streams by maintaining and refining a\ncanonical appearance representation through efficient coordinate mapping.\nExtensive experiments demonstrate that TemPoFast3D matches or exceeds\nstate-of-the-art methods across standard metrics while providing high-quality\ntextured reconstruction across diverse pose and appearance, with a maximum\nspeed of 12 FPS."}
{"id": "2505.06740", "pdf": "https://arxiv.org/pdf/2505.06740", "abs": "https://arxiv.org/abs/2505.06740", "authors": ["Ahmed Abouelazm", "Mianzhi Liu", "Christian Hubschneider", "Yin Wu", "Daniel Slieter", "J. Marius Zöllner"], "title": "Boundary-Guided Trajectory Prediction for Road Aware and Physically Feasible Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)", "summary": "Accurate prediction of surrounding road users' trajectories is essential for\nsafe and efficient autonomous driving. While deep learning models have improved\nperformance, challenges remain in preventing off-road predictions and ensuring\nkinematic feasibility. Existing methods incorporate road-awareness modules and\nenforce kinematic constraints but lack plausibility guarantees and often\nintroduce trade-offs in complexity and flexibility. This paper proposes a novel\nframework that formulates trajectory prediction as a constrained regression\nguided by permissible driving directions and their boundaries. Using the\nagent's current state and an HD map, our approach defines the valid boundaries\nand ensures on-road predictions by training the network to learn superimposed\npaths between left and right boundary polylines. To guarantee feasibility, the\nmodel predicts acceleration profiles that determine the vehicle's travel\ndistance along these paths while adhering to kinematic constraints. We evaluate\nour approach on the Argoverse-2 dataset against the HPTR baseline. Our approach\nshows a slight decrease in benchmark metrics compared to HPTR but notably\nimproves final displacement error and eliminates infeasible trajectories.\nMoreover, the proposed approach has superior generalization to less prevalent\nmaneuvers and unseen out-of-distribution scenarios, reducing the off-road rate\nunder adversarial attacks from 66\\% to just 1\\%. These results highlight the\neffectiveness of our approach in generating feasible and robust predictions."}
{"id": "2505.07004", "pdf": "https://arxiv.org/pdf/2505.07004", "abs": "https://arxiv.org/abs/2505.07004", "authors": ["Jinuk Kim", "Marwa El Halabi", "Wonpyo Park", "Clemens JS Schaefer", "Deokjae Lee", "Yeonhong Park", "Jae W. Lee", "Hyun Oh Song"], "title": "GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Post-training quantization is a key technique for reducing the memory and\ninference latency of large language models by quantizing weights and\nactivations without requiring retraining. However, existing methods either (1)\nfail to account for the varying importance of hidden features to the end loss\nor, when incorporating end loss, (2) neglect the critical interactions between\nmodel weights. To address these limitations, we propose GuidedQuant, a novel\nquantization approach that integrates gradient information from the end loss\ninto the quantization objective while preserving cross-weight dependencies\nwithin output channels. GuidedQuant consistently boosts the performance of\nstate-of-the-art quantization methods across weight-only scalar, weight-only\nvector, and weight-and-activation quantization. Additionally, we introduce a\nnovel non-uniform scalar quantization algorithm, which is guaranteed to\nmonotonically decrease the quantization objective value, and outperforms\nexisting methods in this category. We release the code at\nhttps://github.com/snu-mllab/GuidedQuant."}
{"id": "2411.03700", "pdf": "https://arxiv.org/pdf/2411.03700", "abs": "https://arxiv.org/abs/2411.03700", "authors": ["Anaelia Ovalle", "Krunoslav Lehman Pavasovic", "Louis Martin", "Luke Zettlemoyer", "Eric Michael Smith", "Kai-Wei Chang", "Adina Williams", "Levent Sagun"], "title": "The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models", "categories": ["cs.CL"], "comment": "Accepted to 2025 ACM FAccT", "summary": "Natural-language assistants are designed to provide users with helpful\nresponses while avoiding harmful outputs, largely achieved through alignment to\nhuman preferences. Yet there is limited understanding of whether alignment\ntechniques may inadvertently perpetuate or even amplify harmful biases\ninherited from their pre-aligned base models. This issue is compounded by the\nchoice of bias evaluation benchmarks in popular preference-finetuned models,\nwhich predominantly focus on dominant social categories, such as binary gender,\nthereby limiting insights into biases affecting underrepresented groups.\nTowards addressing this gap, we center transgender, nonbinary, and other\ngender-diverse identities to investigate how alignment procedures interact with\npre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a\ncomprehensive survey of bias evaluation modalities across leading\npreference-finetuned LLMs, highlighting critical gaps in gender-diverse\nrepresentation, 2) systematic evaluation of gender-diverse biases across 16\nmodels spanning Direct Preference Optimization (DPO) stages, uncovering harms\npopular bias benchmarks fail to detect, and 3) a flexible framework for\nmeasuring harmful biases in implicit reward signals applicable to other social\ncontexts. Our findings reveal that DPO-aligned models are particularly\nsensitive to supervised finetuning (SFT), and can amplify two forms of\nreal-world gender-diverse harms from their base models: stigmatization and\ngender non-affirmative language. We conclude with recommendations tailored to\nDPO and broader alignment practices, advocating for the adoption of\ncommunity-informed bias evaluation frameworks to more effectively identify and\naddress underrepresented harms in LLMs."}
{"id": "2505.07336", "pdf": "https://arxiv.org/pdf/2505.07336", "abs": "https://arxiv.org/abs/2505.07336", "authors": ["Zhixuan Zhang", "Xiaopeng Li", "Qi Liu"], "title": "SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by Pattern Recognition", "summary": "Background subtraction (BGS) is utilized to detect moving objects in a video\nand is commonly employed at the onset of object tracking and human recognition\nprocesses. Nevertheless, existing BGS techniques utilizing deep learning still\nencounter challenges with various background noises in videos, including\nvariations in lighting, shifts in camera angles, and disturbances like air\nturbulence or swaying trees. To address this problem, we design a spiking\nautoencoder network, termed SAEN-BGS, based on noise resilience and\ntime-sequence sensitivity of spiking neural networks (SNNs) to enhance the\nseparation of foreground and background. To eliminate unnecessary background\nnoise and preserve the important foreground elements, we begin by creating the\ncontinuous spiking conv-and-dconv block, which serves as the fundamental\nbuilding block for the decoder in SAEN-BGS. Moreover, in striving for enhanced\nenergy efficiency, we introduce a novel self-distillation spiking supervised\nlearning method grounded in ANN-to-SNN frameworks, resulting in decreased power\nconsumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016\ndatasets, our approach demonstrates superior segmentation performance relative\nto other baseline methods, even when challenged by complex scenarios with\ndynamic backgrounds."}
{"id": "2505.06743", "pdf": "https://arxiv.org/pdf/2505.06743", "abs": "https://arxiv.org/abs/2505.06743", "authors": ["Marius Baden", "Ahmed Abouelazm", "Christian Hubschneider", "Yin Wu", "Daniel Slieter", "J. Marius Zöllner"], "title": "TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)\n  for oral presentation", "summary": "Trajectory prediction is crucial for autonomous driving, enabling vehicles to\nnavigate safely by anticipating the movements of surrounding road users.\nHowever, current deep learning models often lack trustworthiness as their\npredictions can be physically infeasible and illogical to humans. To make\npredictions more trustworthy, recent research has incorporated prior knowledge,\nlike the social force model for modeling interactions and kinematic models for\nphysical realism. However, these approaches focus on priors that suit either\nvehicles or pedestrians and do not generalize to traffic with mixed agent\nclasses. We propose incorporating interaction and kinematic priors of all agent\nclasses--vehicles, pedestrians, and cyclists with class-specific interaction\nlayers to capture agent behavioral differences. To improve the interpretability\nof the agent interactions, we introduce DG-SFM, a rule-based interaction\nimportance score that guides the interaction layer. To ensure physically\nfeasible predictions, we proposed suitable kinematic models for all agent\nclasses with a novel pedestrian kinematic model. We benchmark our approach on\nthe Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our\nbaseline. Experiments demonstrate that our method improves interaction\ninterpretability, revealing a correlation between incorrect predictions and\ndivergence from our interaction prior. Even though incorporating the kinematic\nmodels causes a slight decrease in accuracy, they eliminate infeasible\ntrajectories found in the dataset and the baseline model. Thus, our approach\nfosters trust in trajectory prediction as its interaction reasoning is\ninterpretable, and its predictions adhere to physics."}
{"id": "2505.07023", "pdf": "https://arxiv.org/pdf/2505.07023", "abs": "https://arxiv.org/abs/2505.07023", "authors": ["Alexander Koebler", "Thomas Decker", "Ingo Thon", "Volker Tresp", "Florian Buettner"], "title": "Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We study the problem of monitoring machine learning models under gradual\ndistribution shifts, where circumstances change slowly over time, often leading\nto unnoticed yet significant declines in accuracy. To address this, we propose\nIncremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free\nmethod that estimates performance changes by modeling gradual shifts using\noptimal transport. In addition, IUPM quantifies the uncertainty in the\nperformance prediction and introduces an active labeling procedure to restore a\nreliable estimate under a limited labeling budget. Our experiments show that\nIUPM outperforms existing performance estimation baselines in various gradual\nshift scenarios and that its uncertainty awareness guides label acquisition\nmore effectively compared to other strategies."}
{"id": "2411.04223", "pdf": "https://arxiv.org/pdf/2411.04223", "abs": "https://arxiv.org/abs/2411.04223", "authors": ["Weiliang Zhao", "Daniel Ben-Levi", "Wei Hao", "Junfeng Yang", "Chengzhi Mao"], "title": "Diversity Helps Jailbreak Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We have uncovered a powerful jailbreak technique that leverages large\nlanguage models' ability to diverge from prior context, enabling them to bypass\nsafety constraints and generate harmful outputs. By simply instructing the LLM\nto deviate and obfuscate previous attacks, our method dramatically outperforms\nexisting approaches, achieving up to a 62.83% higher success rate in\ncompromising ten leading chatbots, including GPT-4, Gemini, and Llama, while\nusing only 12.9% of the queries. This revelation exposes a critical flaw in\ncurrent LLM safety training, suggesting that existing methods may merely mask\nvulnerabilities rather than eliminate them. Our findings sound an urgent alarm\nfor the need to revolutionize testing methodologies to ensure robust and\nreliable LLM security."}
{"id": "2505.07344", "pdf": "https://arxiv.org/pdf/2505.07344", "abs": "https://arxiv.org/abs/2505.07344", "authors": ["Yuan Zhang", "Jiacheng Jiang", "Guoqing Ma", "Zhiying Lu", "Haoyang Huang", "Jianlong Yuan", "Nan Duan"], "title": "Generative Pre-trained Autoregressive Diffusion Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space."}
{"id": "2505.06799", "pdf": "https://arxiv.org/pdf/2505.06799", "abs": "https://arxiv.org/abs/2505.06799", "authors": ["Erik L. Connerty", "Ethan N. Evans", "Gerasimos Angelatos", "Vignesh Narayanan"], "title": "Quantum Observers: A NISQ Hardware Demonstration of Chaotic State Prediction Using Quantum Echo-state Networks", "categories": ["quant-ph", "cs.AI"], "comment": "14 pages, 12 figures", "summary": "Recent advances in artificial intelligence have highlighted the remarkable\ncapabilities of neural network (NN)-powered systems on classical computers.\nHowever, these systems face significant computational challenges that limit\nscalability and efficiency. Quantum computers hold the potential to overcome\nthese limitations and increase processing power beyond classical systems.\nDespite this, integrating quantum computing with NNs remains largely unrealized\ndue to challenges posed by noise, decoherence, and high error rates in current\nquantum hardware. Here, we propose a novel quantum echo-state network (QESN)\ndesign and implementation algorithm that can operate within the presence of\nnoise on current IBM hardware. We apply classical control-theoretic response\nanalysis to characterize the QESN, emphasizing its rich nonlinear dynamics and\nmemory, as well as its ability to be fine-tuned with sparsity and re-uploading\nblocks. We validate our approach through a comprehensive demonstration of QESNs\nfunctioning as quantum observers, applied in both high-fidelity simulations and\nhardware experiments utilizing data from a prototypical chaotic Lorenz system.\nOur results show that the QESN can predict long time-series with persistent\nmemory, running over 100 times longer than the median T}1 and T2 of the IBM\nMarrakesh QPU, achieving state-of-the-art time-series performance on\nsuperconducting hardware."}
{"id": "2505.07026", "pdf": "https://arxiv.org/pdf/2505.07026", "abs": "https://arxiv.org/abs/2505.07026", "authors": ["Maximilian Egger", "Rawad Bitar", "Rüdiger Urbanke"], "title": "Efficient Machine Unlearning by Model Splitting and Core Sample Selection", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Machine unlearning is essential for meeting legal obligations such as the\nright to be forgotten, which requires the removal of specific data from machine\nlearning models upon request. While several approaches to unlearning have been\nproposed, existing solutions often struggle with efficiency and, more\ncritically, with the verification of unlearning - particularly in the case of\nweak unlearning guarantees, where verification remains an open challenge. We\nintroduce a generalized variant of the standard unlearning metric that enables\nmore efficient and precise unlearning strategies. We also present an\nunlearning-aware training procedure that, in many cases, allows for exact\nunlearning. We term our approach MaxRR. When exact unlearning is not feasible,\nMaxRR still supports efficient unlearning with properties closely matching\nthose achieved through full retraining."}
{"id": "2411.15100", "pdf": "https://arxiv.org/pdf/2411.15100", "abs": "https://arxiv.org/abs/2411.15100", "authors": ["Yixin Dong", "Charlie F. Ruan", "Yaxing Cai", "Ruihang Lai", "Ziyi Xu", "Yilong Zhao", "Tianqi Chen"], "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.PL"], "comment": "MLSys '25", "summary": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving."}
{"id": "2505.07347", "pdf": "https://arxiv.org/pdf/2505.07347", "abs": "https://arxiv.org/abs/2505.07347", "authors": ["Jiewen Yang", "Taoran Huang", "Shangwei Ding", "Xiaowei Xu", "Qinhua Zhao", "Yong Jiang", "Jiarong Guo", "Bin Pu", "Jiexuan Zheng", "Caojin Zhang", "Hongwen Fei", "Xiaomeng Li"], "title": "AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography", "categories": ["cs.CV"], "comment": null, "summary": "Echocardiographers can detect pulmonary hypertension using Doppler\nechocardiography; however, accurately assessing its progression often proves\nchallenging. Right heart catheterization (RHC), the gold standard for precise\nevaluation, is invasive and unsuitable for routine use, limiting its\npracticality for timely diagnosis and monitoring of pulmonary hypertension\nprogression. Here, we propose MePH, a multi-view, multi-modal vision-language\nmodel to accurately assess pulmonary hypertension progression using\nnon-invasive echocardiography. We constructed a large dataset comprising paired\nstandardized echocardiogram videos, spectral images and RHC data, covering\n1,237 patient cases from 12 medical centers. For the first time, MePH precisely\nmodels the correlation between non-invasive multi-view, multi-modal\nechocardiography and the pressure and resistance obtained via RHC. We show that\nMePH significantly outperforms echocardiographers' assessments using\nechocardiography, reducing the mean absolute error in estimating mean pulmonary\narterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and\n43.81%, respectively. In eight independent external hospitals, MePH achieved a\nmean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an\narea under the curve of 0.921, surpassing echocardiographers (area under the\ncurve of 0.842) in accurately predicting the severity of pulmonary\nhypertension, whether mild or severe. A prospective study demonstrated that\nMePH can predict treatment efficacy for patients. Our work provides pulmonary\nhypertension patients with a non-invasive and timely method for monitoring\ndisease progression, improving the accuracy and efficiency of pulmonary\nhypertension management while enabling earlier interventions and more\npersonalized treatment decisions."}
{"id": "2505.06821", "pdf": "https://arxiv.org/pdf/2505.06821", "abs": "https://arxiv.org/abs/2505.06821", "authors": ["Dipayan Saha", "Hasan Al Shaikh", "Shams Tarek", "Farimah Farahmandi"], "title": "ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification", "categories": ["cs.CR", "cs.AI", "cs.ET"], "comment": "This paper has been presented at IEEE VLSI Test Symposium (VTS) 2025", "summary": "Current hardware security verification processes predominantly rely on manual\nthreat modeling and test plan generation, which are labor-intensive,\nerror-prone, and struggle to scale with increasing design complexity and\nevolving attack methodologies. To address these challenges, we propose\nThreatLens, an LLM-driven multi-agent framework that automates security threat\nmodeling and test plan generation for hardware security verification.\nThreatLens integrates retrieval-augmented generation (RAG) to extract relevant\nsecurity knowledge, LLM-powered reasoning for threat assessment, and\ninteractive user feedback to ensure the generation of practical test plans. By\nautomating these processes, the framework reduces the manual verification\neffort, enhances coverage, and ensures a structured, adaptable approach to\nsecurity verification. We evaluated our framework on the NEORV32 SoC,\ndemonstrating its capability to automate security verification through\nstructured test plans and validating its effectiveness in real-world scenarios."}
{"id": "2505.07036", "pdf": "https://arxiv.org/pdf/2505.07036", "abs": "https://arxiv.org/abs/2505.07036", "authors": ["Mahade Hasan", "Farhana Yasmin"], "title": "Predicting Diabetes Using Machine Learning: A Comparative Study of Classifiers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diabetes remains a significant health challenge globally, contributing to\nsevere complications like kidney disease, vision loss, and heart issues. The\napplication of machine learning (ML) in healthcare enables efficient and\naccurate disease prediction, offering avenues for early intervention and\npatient support. Our study introduces an innovative diabetes prediction\nframework, leveraging both traditional ML techniques such as Logistic\nRegression, SVM, Na\\\"ive Bayes, and Random Forest and advanced ensemble methods\nlike AdaBoost, Gradient Boosting, Extra Trees, and XGBoost. Central to our\napproach is the development of a novel model, DNet, a hybrid architecture\ncombining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)\nlayers for effective feature extraction and sequential learning. The DNet model\ncomprises an initial convolutional block for capturing essential features,\nfollowed by a residual block with skip connections to facilitate efficient\ninformation flow. Batch Normalization and Dropout are employed for robust\nregularization, and an LSTM layer captures temporal dependencies within the\ndata. Using a Kaggle-sourced real-world diabetes dataset, our model evaluation\nspans cross-validation accuracy, precision, recall, F1 score, and ROC-AUC.\nAmong the models, DNet demonstrates the highest efficacy with an accuracy of\n99.79% and an AUC-ROC of 99.98%, establishing its potential for superior\ndiabetes prediction. This robust hybrid architecture showcases the value of\ncombining CNN and LSTM layers, emphasizing its applicability in medical\ndiagnostics and disease prediction tasks."}
{"id": "2412.05342", "pdf": "https://arxiv.org/pdf/2412.05342", "abs": "https://arxiv.org/abs/2412.05342", "authors": ["Xiaoyu Wang", "Ningyuan Xi", "Teng Chen", "Qingqing Gu", "Yue Zhao", "Xiaokai Chen", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "title": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCNN 2025", "summary": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe."}
{"id": "2505.07373", "pdf": "https://arxiv.org/pdf/2505.07373", "abs": "https://arxiv.org/abs/2505.07373", "authors": ["Lintao Xiang", "Hongpei Zheng", "Bailin Deng", "Hujun Yin"], "title": "Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Neural implicit surface reconstruction using volume rendering techniques has\nrecently achieved significant advancements in creating high-fidelity surfaces\nfrom multiple 2D images. However, current methods primarily target scenes with\nconsistent illumination and struggle to accurately reconstruct 3D geometry in\nuncontrolled environments with transient occlusions or varying appearances.\nWhile some neural radiance field (NeRF)-based variants can better manage\nphotometric variations and transient objects in complex scenes, they are\ndesigned for novel view synthesis rather than precise surface reconstruction\ndue to limited surface constraints. To overcome this limitation, we introduce a\nnovel approach that applies multiple geometric constraints to the implicit\nsurface optimization process, enabling more accurate reconstructions from\nunconstrained image collections. First, we utilize sparse 3D points from\nstructure-from-motion (SfM) to refine the signed distance function estimation\nfor the reconstructed surface, with a displacement compensation to accommodate\nnoise in the sparse points. Additionally, we employ robust normal priors\nderived from a normal predictor, enhanced by edge prior filtering and\nmulti-view consistency constraints, to improve alignment with the actual\nsurface geometry. Extensive testing on the Heritage-Recon benchmark and other\ndatasets has shown that the proposed method can accurately reconstruct surfaces\nfrom in-the-wild images, yielding geometries with superior accuracy and\ngranularity compared to existing techniques. Our approach enables high-quality\n3D reconstruction of various landmarks, making it applicable to diverse\nscenarios such as digital preservation of cultural heritage sites."}
{"id": "2505.06827", "pdf": "https://arxiv.org/pdf/2505.06827", "abs": "https://arxiv.org/abs/2505.06827", "authors": ["Fabrice Y Harel-Canada", "Boran Erol", "Connor Choi", "Jason Liu", "Gary Jiarui Song", "Nanyun Peng", "Amit Sahai"], "title": "Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking", "categories": ["cs.CR", "cs.AI"], "comment": "In Review @ ACL 2025", "summary": "Watermarking AI-generated text is critical for combating misuse. Yet recent\ntheoretical work argues that any watermark can be erased via random walk\nattacks that perturb text while preserving quality. However, such attacks rely\non two key assumptions: (1) rapid mixing (watermarks dissolve quickly under\nperturbations) and (2) reliable quality preservation (automated quality oracles\nperfectly guide edits). Through large-scale experiments and human-validated\nassessments, we find mixing is slow: 100% of perturbed texts retain traces of\ntheir origin after hundreds of edits, defying rapid mixing. Oracles falter, as\nstate-of-the-art quality detectors misjudge edits (77% accuracy), compounding\nerrors during attacks. Ultimately, attacks underperform: automated walks remove\nwatermarks just 26% of the time -- dropping to 10% under human quality review.\nThese findings challenge the inevitability of watermark removal. Instead,\npractical barriers -- slow mixing and imperfect quality control -- reveal\nwatermarking to be far more robust than theoretical models suggest. The gap\nbetween idealized attacks and real-world feasibility underscores the need for\nstronger watermarking methods and more realistic attack models."}
{"id": "2505.07045", "pdf": "https://arxiv.org/pdf/2505.07045", "abs": "https://arxiv.org/abs/2505.07045", "authors": ["Junjie Yu", "John S. Schreck", "David John Gagne", "Keith W. Oleson", "Jie Li", "Yongtu Liang", "Qi Liao", "Mingfei Sun", "David O. Topping", "Zhonghua Zheng"], "title": "Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL)-based heating, ventilation, and air conditioning\n(HVAC) control has emerged as a promising technology for reducing building\nenergy consumption while maintaining indoor thermal comfort. However, the\nefficacy of such strategies is influenced by the background climate and their\nimplementation may potentially alter both the indoor climate and local urban\nclimate. This study proposes an integrated framework combining RL with an urban\nclimate model that incorporates a building energy model, aiming to evaluate the\nefficacy of RL-based HVAC control across different background climates, impacts\nof RL strategies on indoor climate and local urban climate, and the\ntransferability of RL strategies across cities. Our findings reveal that the\nreward (defined as a weighted combination of energy consumption and thermal\ncomfort) and the impacts of RL strategies on indoor climate and local urban\nclimate exhibit marked variability across cities with different background\nclimates. The sensitivity of reward weights and the transferability of RL\nstrategies are also strongly influenced by the background climate. Cities in\nhot climates tend to achieve higher rewards across most reward weight\nconfigurations that balance energy consumption and thermal comfort, and those\ncities with more varying atmospheric temperatures demonstrate greater RL\nstrategy transferability. These findings underscore the importance of\nthoroughly evaluating RL-based HVAC control strategies in diverse climatic\ncontexts. This study also provides a new insight that city-to-city learning\nwill potentially aid the deployment of RL-based HVAC control."}
{"id": "2412.08587", "pdf": "https://arxiv.org/pdf/2412.08587", "abs": "https://arxiv.org/abs/2412.08587", "authors": ["Hang Zhao", "Qile P. Chen", "Yijing Barry Zhang", "Gang Yang"], "title": "Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 3 tables", "summary": "Both encoder-only models (e.g., BERT, RoBERTa) and large language models\n(LLMs, e.g., Llama3) have been widely used for text classification tasks.\nHowever, there is a lack of systematic studies comparing the performance of\nencoder-based models and LLMs in text classification, particularly when\nfine-tuning is involved. This study employed a diverse range of models and\nmethods, varying in size and architecture, and including both fine-tuned and\npre-trained approaches. We first assessed the performances of these LLMs on the\n20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only\nRoBERTa models. Additionally, we explored the multi-task capabilities of both\nmodel types by combining multiple classification tasks, including intent\ndetection and slot-filling, into a single model using data from both datasets.\nOur results indicate that fully fine-tuned Llama3-70B models outperform\nRoBERTa-large and other decoder LLMs across various classification tasks and\ndatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the\nperformance of dual-model setups in both tasks across both datasets. Overall,\nour study provides a comprehensive benchmark of encoder-only and LLM models on\ntext classification tasks and demonstrates a method to combine two or more\nfully fine-tuned decoder LLMs for reduced latency and equivalent performance."}
{"id": "2505.07375", "pdf": "https://arxiv.org/pdf/2505.07375", "abs": "https://arxiv.org/abs/2505.07375", "authors": ["Yuqi Cheng", "Yunkang Cao", "Dongfang Wang", "Weiming Shen", "Wenlong Li"], "title": "Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection", "categories": ["cs.CV"], "comment": "12 pages, 12 figures", "summary": "Point cloud anomaly detection is essential for various industrial\napplications. The huge computation and storage costs caused by the increasing\nproduct classes limit the application of single-class unsupervised methods,\nnecessitating the development of multi-class unsupervised methods. However, the\nfeature similarity between normal and anomalous points from different class\ndata leads to the feature confusion problem, which greatly hinders the\nperformance of multi-class methods. Therefore, we introduce a multi-class point\ncloud anomaly detection method, named GLFM, leveraging global-local feature\nmatching to progressively separate data that are prone to confusion across\nmultiple classes. Specifically, GLFM is structured into three stages: Stage-I\nproposes an anomaly synthesis pipeline that stretches point clouds to create\nabundant anomaly data that are utilized to adapt the point cloud feature\nextractor for better feature representation. Stage-II establishes the global\nand local memory banks according to the global and local feature distributions\nof all the training data, weakening the impact of feature confusion on the\nestablishment of the memory bank. Stage-III implements anomaly detection of\ntest data leveraging its feature distance from global and local memory banks.\nExtensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts\ndataset showcase our proposed GLFM's superior point cloud anomaly detection\nperformance. The code is available at\nhttps://github.com/hustCYQ/GLFM-Multi-class-3DAD."}
{"id": "2505.06841", "pdf": "https://arxiv.org/pdf/2505.06841", "abs": "https://arxiv.org/abs/2505.06841", "authors": ["Prabhdeep Cheema", "Erhan Guven"], "title": "Optimizing Recommendations using Fine-Tuned LLMs", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "Accepted and presented at IEEE CAI 2025. This version includes minor\n  clarifications and formatting updates", "summary": "As digital media platforms strive to meet evolving user expectations,\ndelivering highly personalized and intuitive movies and media recommendations\nhas become essential for attracting and retaining audiences. Traditional\nsystems often rely on keyword-based search and recommendation techniques, which\nlimit users to specific keywords and a combination of keywords. This paper\nproposes an approach that generates synthetic datasets by modeling real-world\nuser interactions, creating complex chat-style data reflective of diverse\npreferences. This allows users to express more information with complex\npreferences, such as mood, plot details, and thematic elements, in addition to\nconventional criteria like genre, title, and actor-based searches. In today's\nsearch space, users cannot write queries like ``Looking for a fantasy movie\nfeaturing dire wolves, ideally set in a harsh frozen world with themes of\nloyalty and survival.''\n  Building on these contributions, we evaluate synthetic datasets for diversity\nand effectiveness in training and benchmarking models, particularly in areas\noften absent from traditional datasets. This approach enhances personalization\nand accuracy by enabling expressive and natural user queries. It establishes a\nfoundation for the next generation of conversational AI-driven search and\nrecommendation systems in digital entertainment."}
{"id": "2505.07070", "pdf": "https://arxiv.org/pdf/2505.07070", "abs": "https://arxiv.org/abs/2505.07070", "authors": ["Francesco Cagnetta", "Alessandro Favero", "Antonio Sclocchi", "Matthieu Wyart"], "title": "Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures", "categories": ["cs.LG", "cond-mat.dis-nn", "stat.ML"], "comment": "14 pages, 8 figures", "summary": "How do neural language models acquire a language's structure when trained for\nnext-token prediction? We address this question by deriving theoretical scaling\nlaws for neural network performance on synthetic datasets generated by the\nRandom Hierarchy Model (RHM) -- an ensemble of probabilistic context-free\ngrammars designed to capture the hierarchical structure of natural language\nwhile remaining analytically tractable. Previously, we developed a theory of\nrepresentation learning based on data correlations that explains how deep\nlearning models capture the hierarchical structure of the data sequentially,\none layer at a time. Here, we extend our theoretical framework to account for\narchitectural differences. In particular, we predict and empirically validate\nthat convolutional networks, whose structure aligns with that of the generative\nprocess through locality and weight sharing, enjoy a faster scaling of\nperformance compared to transformer models, which rely on global self-attention\nmechanisms. This finding clarifies the architectural biases underlying neural\nscaling laws and highlights how representation learning is shaped by the\ninteraction between model architecture and the statistical properties of data."}
{"id": "2412.20309", "pdf": "https://arxiv.org/pdf/2412.20309", "abs": "https://arxiv.org/abs/2412.20309", "authors": ["Shintaro Ozaki", "Yuta Kato", "Siyuan Feng", "Masayo Tomita", "Kazuki Hayashi", "Wataru Hashimoto", "Ryoma Obara", "Masafumi Oyamada", "Katsuhiko Hayashi", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain", "categories": ["cs.CL"], "comment": "Accepted to BioNLP2025 (Workshop colocated with ACL2025)", "summary": "Retrieval Augmented Generation (RAG) complements the knowledge of Large\nLanguage Models (LLMs) by leveraging external information to enhance response\naccuracy for queries. This approach is widely applied in several fields by\ntaking its advantage of injecting the most up-to-date information, and\nresearchers are focusing on understanding and improving this aspect to unlock\nthe full potential of RAG in such high-stakes applications. However, despite\nthe potential of RAG to address these needs, the mechanisms behind the\nconfidence levels of its outputs remain underexplored, although the confidence\nof information is very critical in some domains, such as finance, healthcare,\nand medicine. Our study focuses the impact of RAG on confidence within the\nmedical domain under various configurations and models. We evaluate confidence\nby treating the model's predicted probability as its output and calculating\nExpected Calibration Error (ECE) and Adaptive Calibration Error (ACE) scores\nbased on the probabilities and accuracy. In addition, we analyze whether the\norder of retrieved documents within prompts calibrates the confidence. Our\nfindings reveal large variation in confidence and accuracy depending on the\nmodel, settings, and the format of input prompts. These results underscore the\nnecessity of optimizing configurations based on the specific model and\nconditions."}
{"id": "2505.07381", "pdf": "https://arxiv.org/pdf/2505.07381", "abs": "https://arxiv.org/abs/2505.07381", "authors": ["Baoping Cheng", "Yukun Zhang", "Liming Wang", "Xiaoyan Xie", "Tao Fu", "Dongkun Wang", "Xiaoming Tao"], "title": "Few-shot Semantic Encoding and Decoding for Video Surveillance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the continuous increase in the number and resolution of video\nsurveillance cameras, the burden of transmitting and storing surveillance video\nis growing. Traditional communication methods based on Shannon's theory are\nfacing optimization bottlenecks. Semantic communication, as an emerging\ncommunication method, is expected to break through this bottleneck and reduce\nthe storage and transmission consumption of video. Existing semantic decoding\nmethods often require many samples to train the neural network for each scene,\nwhich is time-consuming and labor-intensive. In this study, a semantic encoding\nand decoding method for surveillance video is proposed. First, the sketch was\nextracted as semantic information, and a sketch compression method was proposed\nto reduce the bit rate of semantic information. Then, an image translation\nnetwork was proposed to translate the sketch into a video frame with a\nreference frame. Finally, a few-shot sketch decoding network was proposed to\nreconstruct video from sketch. Experimental results showed that the proposed\nmethod achieved significantly better video reconstruction performance than\nbaseline methods. The sketch compression method could effectively reduce the\nstorage and transmission consumption of semantic information with little\ncompromise on video quality. The proposed method provides a novel semantic\nencoding and decoding method that only needs a few training samples for each\nsurveillance scene, thus improving the practicality of the semantic\ncommunication system."}
{"id": "2505.06860", "pdf": "https://arxiv.org/pdf/2505.06860", "abs": "https://arxiv.org/abs/2505.06860", "authors": ["Xia Du", "Jiajie Zhu", "Jizhe Zhou", "Chi-man Pun", "Zheng Lin", "Cong Wu", "Zhe Chen", "Jun Luo"], "title": "DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "In the field of digital security, Reversible Adversarial Examples (RAE)\ncombine adversarial attacks with reversible data hiding techniques to\neffectively protect sensitive data and prevent unauthorized analysis by\nmalicious Deep Neural Networks (DNNs). However, existing RAE techniques\nprimarily focus on white-box attacks, lacking a comprehensive evaluation of\ntheir effectiveness in black-box scenarios. This limitation impedes their\nbroader deployment in complex, dynamic environments. Further more, traditional\nblack-box attacks are often characterized by poor transferability and high\nquery costs, significantly limiting their practical applicability. To address\nthese challenges, we propose the Dual-Phase Merging Transferable Reversible\nAttack method, which generates highly transferable initial adversarial\nperturbations in a white-box model and employs a memory augmented black-box\nstrategy to effectively mislead target mod els. Experimental results\ndemonstrate the superiority of our approach, achieving a 99.0% attack success\nrate and 100% recovery rate in black-box scenarios, highlighting its robustness\nin privacy protection. Moreover, we successfully implemented a black-box attack\non a commercial model, further substantiating the potential of this approach\nfor practical use."}
{"id": "2505.07081", "pdf": "https://arxiv.org/pdf/2505.07081", "abs": "https://arxiv.org/abs/2505.07081", "authors": ["Gregoire Fournier", "Sourav Medya"], "title": "COMRECGC: Global Graph Counterfactual Explainer through Common Recourse", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at ICML 2025", "summary": "Graph neural networks (GNNs) have been widely used in various domains such as\nsocial networks, molecular biology, or recommendation systems. Concurrently,\ndifferent explanations methods of GNNs have arisen to complement its black-box\nnature. Explanations of the GNNs' predictions can be categorized into two\ntypes--factual and counterfactual. Given a GNN trained on binary classification\ninto ''accept'' and ''reject'' classes, a global counterfactual explanation\nconsists in generating a small set of ''accept'' graphs relevant to all of the\ninput ''reject'' graphs. The transformation of a ''reject'' graph into an\n''accept'' graph is called a recourse. A common recourse explanation is a small\nset of recourse, from which every ''reject'' graph can be turned into an\n''accept'' graph. Although local counterfactual explanations have been studied\nextensively, the problem of finding common recourse for global counterfactual\nexplanation remains unexplored, particularly for GNNs. In this paper, we\nformalize the common recourse explanation problem, and design an effective\nalgorithm, COMRECGC, to solve it. We benchmark our algorithm against strong\nbaselines on four different real-world graphs datasets and demonstrate the\nsuperior performance of COMRECGC against the competitors. We also compare the\ncommon recourse explanations to the graph counterfactual explanation, showing\nthat common recourse explanations are either comparable or superior, making\nthem worth considering for applications such as drug discovery or computational\nbiology."}
{"id": "2501.01652", "pdf": "https://arxiv.org/pdf/2501.01652", "abs": "https://arxiv.org/abs/2501.01652", "authors": ["Yin Cai", "Zhouhong Gu", "Zhaohan Du", "Zheyu Ye", "Shaosheng Cao", "Yiqian Xu", "Hongwei Feng", "Ping Chen"], "title": "MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nenvironmental perception, reasoning-based decision-making, and simulating\ncomplex human behaviors, particularly in interactive role-playing contexts.\nThis paper introduces the Multiverse Interactive Role-play Ability General\nEvaluation (MIRAGE), a comprehensive framework designed to assess LLMs'\nproficiency in portraying advanced human behaviors through murder mystery\ngames. MIRAGE features eight intricately crafted scripts encompassing diverse\nthemes and styles, providing a rich simulation. To evaluate LLMs' performance,\nMIRAGE employs four distinct methods: the Trust Inclination Index (TII) to\nmeasure dynamics of trust and suspicion, the Clue Investigation Capability\n(CIC) to measure LLMs' capability of conducting information, the Interactivity\nCapability Index (ICI) to assess role-playing capabilities and the Script\nCompliance Index (SCI) to assess LLMs' capability of understanding and\nfollowing instructions. Our experiments indicate that even popular models like\nGPT-4 face significant challenges in navigating the complexities presented by\nthe MIRAGE. The datasets and simulation codes are available in\n\\href{https://github.com/lime728/MIRAGE}{github}."}
{"id": "2505.07387", "pdf": "https://arxiv.org/pdf/2505.07387", "abs": "https://arxiv.org/abs/2505.07387", "authors": ["Chunpeng Li", "Ya-tang Li"], "title": "Feature Visualization in 3D Convolutional Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the computations of convolutional neural networks requires\neffective visualization of their kernels. While maximal activation methods have\nproven successful in highlighting the preferred features of 2D convolutional\nkernels, directly applying these techniques to 3D convolutions often leads to\nuninterpretable results due to the higher dimensionality and complexity of 3D\nfeatures. To address this challenge, we propose a novel visualization approach\nfor 3D convolutional kernels that disentangles their texture and motion\npreferences. Our method begins with a data-driven decomposition of the optimal\ninput that maximally activates a given kernel. We then introduce a two-stage\noptimization strategy to extract distinct texture and motion components from\nthis input. Applying our approach to visualize kernels at various depths of\nseveral pre-trained models, we find that the resulting\nvisualizations--particularly those capturing motion--clearly reveal the\npreferred dynamic patterns encoded by 3D kernels. These results demonstrate the\neffectiveness of our method in providing interpretable insights into 3D\nconvolutional operations. Code is available at\nhttps://github.com/YatangLiLab/3DKernelVisualizer."}
{"id": "2505.06861", "pdf": "https://arxiv.org/pdf/2505.06861", "abs": "https://arxiv.org/abs/2505.06861", "authors": ["Dongxiu Liu", "Haoyi Niu", "Zhihao Wang", "Jinliang Zheng", "Yinan Zheng", "Zhonghong Ou", "Jianming Hu", "Jianxiong Li", "Xianyuan Zhan"], "title": "Efficient Robotic Policy Learning via Latent Space Backward Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Current robotic planning methods often rely on predicting multi-frame images\nwith full pixel details. While this fine-grained approach can serve as a\ngeneric world model, it introduces two significant challenges for downstream\npolicy learning: substantial computational costs that hinder real-time\ndeployment, and accumulated inaccuracies that can mislead action extraction.\nPlanning with coarse-grained subgoals partially alleviates efficiency issues.\nHowever, their forward planning schemes can still result in off-task\npredictions due to accumulation errors, leading to misalignment with long-term\ngoals. This raises a critical question: Can robotic planning be both efficient\nand accurate enough for real-time control in long-horizon, multi-stage tasks?\nTo address this, we propose a Latent Space Backward Planning scheme (LBP),\nwhich begins by grounding the task into final latent goals, followed by\nrecursively predicting intermediate subgoals closer to the current state. The\ngrounded final goal enables backward subgoal planning to always remain aware of\ntask completion, facilitating on-task prediction along the entire planning\nhorizon. The subgoal-conditioned policy incorporates a learnable token to\nsummarize the subgoal sequences and determines how each subgoal guides action\nextraction. Through extensive simulation and real-robot long-horizon\nexperiments, we show that LBP outperforms existing fine-grained and forward\nplanning methods, achieving SOTA performance. Project Page:\nhttps://lbp-authors.github.io"}
{"id": "2505.07086", "pdf": "https://arxiv.org/pdf/2505.07086", "abs": "https://arxiv.org/abs/2505.07086", "authors": ["Tong Chen", "Yinuo Zhang", "Sophia Tang", "Pranam Chatterjee"], "title": "Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Designing biological sequences that satisfy multiple, often conflicting,\nfunctional and biophysical criteria remains a central challenge in biomolecule\nengineering. While discrete flow matching models have recently shown promise\nfor efficient sampling in high-dimensional sequence spaces, existing approaches\naddress only single objectives or require continuous embeddings that can\ndistort discrete distributions. We present Multi-Objective-Guided Discrete Flow\nMatching (MOG-DFM), a general framework to steer any pretrained discrete-time\nflow matching generator toward Pareto-efficient trade-offs across multiple\nscalar objectives. At each sampling step, MOG-DFM computes a hybrid\nrank-directional score for candidate transitions and applies an adaptive\nhypercone filter to enforce consistent multi-objective progression. We also\ntrained two unconditional discrete flow matching models, PepDFM for diverse\npeptide generation and EnhancerDFM for functional enhancer DNA generation, as\nbase generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in\ngenerating peptide binders optimized across five properties (hemolysis,\nnon-fouling, solubility, half-life, and binding affinity), and in designing DNA\nsequences with specific enhancer classes and DNA shapes. In total, MOG-DFM\nproves to be a powerful tool for multi-property-guided biomolecule sequence\ndesign."}
{"id": "2501.11110", "pdf": "https://arxiv.org/pdf/2501.11110", "abs": "https://arxiv.org/abs/2501.11110", "authors": ["Yiyao Yu", "Yuxiang Zhang", "Dongdong Zhang", "Xiao Liang", "Hengyuan Zhang", "Xingxing Zhang", "Mahmoud Khademi", "Hany Awadalla", "Junjie Wang", "Yujiu Yang", "Furu Wei"], "title": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made notable progress in mathematical\nreasoning, yet often rely on single-paradigm reasoning, limiting their\neffectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a\nnovel unified framework integrating multiple reasoning paradigms--Natural\nLanguage Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning\n(SR)--to enable synergistic collaboration. CoR generates multiple potential\nanswers via different reasoning paradigms and synthesizes them into a coherent\nfinal solution. We propose a Progressive Paradigm Training (PPT) strategy for\nmodels to progressively master these paradigms, leading to CoR-Math-7B.\nExperimental results demonstrate that CoR-Math-7B significantly outperforms\ncurrent SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o\nin theorem proving and a 15.0% improvement over RL-based methods on the MATH\nbenchmark in arithmetic tasks. These results show the enhanced mathematical\ncomprehension ability of our model, enabling zero-shot generalization across\ntasks."}
{"id": "2505.07396", "pdf": "https://arxiv.org/pdf/2505.07396", "abs": "https://arxiv.org/abs/2505.07396", "authors": ["Olaf Wysocki", "Benedikt Schwab", "Manoj Kumar Biswanath", "Qilin Zhang", "Jingwei Zhu", "Thomas Froech", "Medhini Heeramaglore", "Ihab Hijazi", "Khaoula Kanna", "Mathias Pechinger", "Zhaiyu Chen", "Yao Sun", "Alejandro Rueda Segura", "Ziyang Xu", "Omar AbdelGafar", "Mansour Mehranfar", "Chandan Yeshwanth", "Yueh-Cheng Liu", "Hadi Yazdi", "Jiapan Wang", "Stefan Auer", "Katharina Anders", "Klaus Bogenberger", "Andre Borrmann", "Angela Dai", "Ludwig Hoegner", "Christoph Holst", "Thomas H. Kolbe", "Ferdinand Ludwig", "Matthias Nießner", "Frank Petzold", "Xiao Xiang Zhu", "Boris Jutzi"], "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing", "summary": "Urban Digital Twins (UDTs) have become essential for managing cities and\nintegrating complex, heterogeneous data from diverse sources. Creating UDTs\ninvolves challenges at multiple process stages, including acquiring accurate 3D\nsource data, reconstructing high-fidelity 3D models, maintaining models'\nupdates, and ensuring seamless interoperability to downstream tasks. Current\ndatasets are usually limited to one part of the processing chain, hampering\ncomprehensive UDTs validation. To address these challenges, we introduce the\nfirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.\nThis dataset includes georeferenced, semantically aligned 3D models and\nnetworks along with various terrestrial, mobile, aerial, and satellite\nobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently\n767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high\naccuracy, and multimodal data integration, the benchmark supports robust\nanalysis of sensors and the development of advanced reconstruction methods.\nAdditionally, we explore downstream tasks demonstrating the potential of\nTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar\npotential analysis, point cloud semantic segmentation, and LoD3 building\nreconstruction. We are convinced this contribution lays a foundation for\novercoming current limitations in UDT creation, fostering new research\ndirections and practical solutions for smarter, data-driven urban environments.\nThe project is available under: https://tum2t.win"}
{"id": "2505.06883", "pdf": "https://arxiv.org/pdf/2505.06883", "abs": "https://arxiv.org/abs/2505.06883", "authors": ["Botian Xu", "Haoyang Weng", "Qingzhou Lu", "Yang Gao", "Huazhe Xu"], "title": "FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has made significant strides in legged robot\ncontrol, enabling locomotion across diverse terrains and complex\nloco-manipulation capabilities. However, the commonly used position or velocity\ntracking-based objectives are agnostic to forces experienced by the robot,\nleading to stiff and potentially dangerous behaviors and poor control during\nforceful interactions. To address this limitation, we present\n\\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET).\nInspired by impedance control, we use RL to train a control policy to imitate a\nvirtual mass-spring-damper system, allowing fine-grained control under external\nforces by manipulating the virtual spring. In simulation, we demonstrate that\nour quadruped robot achieves improved robustness to large impulses (up to 200\nNs) and exhibits controllable compliance, achieving an 80% reduction in\ncollision impulse. The policy is deployed to a physical robot to showcase both\ncompliance and the ability to engage with large forces by kinesthetic control\nand pulling payloads up to 2/3 of its weight. Further extension to a legged\nloco-manipulator and a humanoid shows the applicability of our method to more\ncomplex settings to enable whole-body compliance control. Project Website:\nhttps://egalahad.github.io/facet/"}
{"id": "2505.07090", "pdf": "https://arxiv.org/pdf/2505.07090", "abs": "https://arxiv.org/abs/2505.07090", "authors": ["Bilal Ahmed", "Yuqing Qiu", "Diab W. Abueidda", "Waleed El-Sekelly", "Tarek Abdoun", "Mostafa E. Mobasher"], "title": "Physics-informed Multiple-Input Operators for efficient dynamic response prediction of structures", "categories": ["cs.LG"], "comment": null, "summary": "Finite element (FE) modeling is essential for structural analysis but remains\ncomputationally intensive, especially under dynamic loading. While operator\nlearning models have shown promise in replicating static structural responses\nat FEM level accuracy, modeling dynamic behavior remains more challenging. This\nwork presents a Multiple Input Operator Network (MIONet) that incorporates a\nsecond trunk network to explicitly encode temporal dynamics, enabling accurate\nprediction of structural responses under moving loads. Traditional DeepONet\narchitectures using recurrent neural networks (RNNs) are limited by fixed time\ndiscretization and struggle to capture continuous dynamics. In contrast, MIONet\npredicts responses continuously over both space and time, removing the need for\nstep wise modeling. It maps scalar inputs including load type, velocity,\nspatial mesh, and time steps to full field structural responses. To improve\nefficiency and enforce physical consistency, we introduce a physics informed\nloss based on dynamic equilibrium using precomputed mass, damping, and\nstiffness matrices, without solving the governing PDEs directly. Further, a\nSchur complement formulation reduces the training domain, significantly cutting\ncomputational costs while preserving global accuracy. The model is validated on\nboth a simple beam and the KW-51 bridge, achieving FEM level accuracy within\nseconds. Compared to GRU based DeepONet, our model offers comparable accuracy\nwith improved temporal continuity and over 100 times faster inference, making\nit well suited for real-time structural monitoring and digital twin\napplications."}
{"id": "2501.13428", "pdf": "https://arxiv.org/pdf/2501.13428", "abs": "https://arxiv.org/abs/2501.13428", "authors": ["Bo Gao", "Michael W. Spratling"], "title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages and 2 figures", "summary": "Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by decomposing the Softmax operation into a non-linear\ntransformation and the $l_1$-norm. We identify the latter as essential for\nmaintaining model performance. By replacing the non-linear transformation with\nthe Softplus activation function and introducing a dynamic scale factor for\ndifferent token lengths based on invariance entropy, we create a novel\nattention mechanism with performance better than conventional Softmax attention\nacross various inference lengths. To further improve the length extrapolation\nability of the proposed attention mechanism, we introduce a novel re-weighting\nmechanism that amplifies significant attention weights while diminishing weaker\nones, enabling the model to concentrate more effectively on relevant tokens.\nWhen combined with our proposed attention mechanism, this approach maintains\nnearly constant validation loss even at 16$\\times$ the training token length,\nensures numerical stability, and achieves superior results on downstream\nbenchmarks."}
{"id": "2505.07398", "pdf": "https://arxiv.org/pdf/2505.07398", "abs": "https://arxiv.org/abs/2505.07398", "authors": ["Mingqian Ji", "Jian Yang", "Shanshan Zhang"], "title": "DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "State-of-the-art LiDAR-camera 3D object detectors usually focus on feature\nfusion. However, they neglect the factor of depth while designing the fusion\nstrategy. In this work, we are the first to observe that different modalities\nplay different roles as depth varies via statistical analysis and\nvisualization. Based on this finding, we propose a Depth-Aware Hybrid Feature\nFusion (DepthFusion) strategy that guides the weights of point cloud and RGB\nimage modalities by introducing depth encoding at both global and local levels.\nSpecifically, the Depth-GFusion module adaptively adjusts the weights of image\nBird's-Eye-View (BEV) features in multi-modal global features via depth\nencoding. Furthermore, to compensate for the information lost when transferring\nraw features to the BEV space, we propose a Depth-LFusion module, which\nadaptively adjusts the weights of original voxel features and multi-view image\nfeatures in multi-modal local features via depth encoding. Extensive\nexperiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion\nmethod surpasses previous state-of-the-art methods. Moreover, our DepthFusion\nis more robust to various kinds of corruptions, outperforming previous methods\non the nuScenes-C dataset."}
{"id": "2505.06913", "pdf": "https://arxiv.org/pdf/2505.06913", "abs": "https://arxiv.org/abs/2505.06913", "authors": ["Brian Challita", "Pierre Parrend"], "title": "RedTeamLLM: an Agentic AI framework for offensive security", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": null, "summary": "From automated intrusion testing to discovery of zero-day attacks before\nsoftware launch, agentic AI calls for great promises in security engineering.\nThis strong capability is bound with a similar threat: the security and\nresearch community must build up its models before the approach is leveraged by\nmalicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM,\nan integrated architecture with a comprehensive security model for\nautomatization of pentest tasks. RedTeamLLM follows three key steps:\nsummarizing, reasoning and act, which embed its operational capacity. This\nnovel framework addresses four open challenges: plan correction, memory\nmanagement, context window constraint, and generality vs. specialization.\nEvaluation is performed through the automated resolution of a range of\nentry-level, but not trivial, CTF challenges. The contribution of the reasoning\ncapability of our agentic AI framework is specifically evaluated."}
{"id": "2505.07100", "pdf": "https://arxiv.org/pdf/2505.07100", "abs": "https://arxiv.org/abs/2505.07100", "authors": ["Julian Rosenberger", "Philipp Schröppel", "Sven Kruschel", "Mathias Kraus", "Patrick Zschech", "Maximilian Förster"], "title": "Navigating the Rashomon Effect: How Personalization Can Help Adjust Interpretable Machine Learning Models to Individual Users", "categories": ["cs.LG", "cs.HC"], "comment": "Accepted as a Completed Research Paper at the Thirty-Third European\n  Conference on Information Systems (ECIS 2025), Amman, Jordan", "summary": "The Rashomon effect describes the observation that in machine learning (ML)\nmultiple models often achieve similar predictive performance while explaining\nthe underlying relationships in different ways. This observation holds even for\nintrinsically interpretable models, such as Generalized Additive Models (GAMs),\nwhich offer users valuable insights into the model's behavior. Given the\nexistence of multiple GAM configurations with similar predictive performance, a\nnatural question is whether we can personalize these configurations based on\nusers' needs for interpretability. In our study, we developed an approach to\npersonalize models based on contextual bandits. In an online experiment with\n108 users in a personalized treatment and a non-personalized control group, we\nfound that personalization led to individualized rather than one-size-fits-all\nconfigurations. Despite these individual adjustments, the interpretability\nremained high across both groups, with users reporting a strong understanding\nof the models. Our research offers initial insights into the potential for\npersonalizing interpretable ML."}
{"id": "2502.01568", "pdf": "https://arxiv.org/pdf/2502.01568", "abs": "https://arxiv.org/abs/2502.01568", "authors": ["Benjamin A. Spiegel", "Lucas Gelfond", "George Konidaris"], "title": "Visual Theory of Mind Enables the Invention of Proto-Writing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for oral presentation at CogSci 2025, published here with\n  permission from organizers", "summary": "Symbolic writing systems are graphical semiotic codes that are ubiquitous in\nmodern society but are otherwise absent in the animal kingdom. Anthropological\nevidence suggests that the earliest forms of some writing systems originally\nconsisted of iconic pictographs, which signify their referent via visual\nresemblance. While previous studies have examined the emergence and,\nseparately, the evolution of pictographic systems through a computational lens,\nmost employ non-naturalistic methodologies that make it difficult to draw clear\nanalogies to human and animal cognition. We develop a multi-agent reinforcement\nlearning testbed for emergent communication called a Signification Game, and\nformulate a model of inferential communication that enables agents to leverage\nvisual theory of mind to communicate actions using pictographs. Our model,\nwhich is situated within a broader formalism for animal communication, sheds\nlight on the cognitive and cultural processes underlying the emergence of\nproto-writing."}
{"id": "2505.07444", "pdf": "https://arxiv.org/pdf/2505.07444", "abs": "https://arxiv.org/abs/2505.07444", "authors": ["Zeynep Galymzhankyzy", "Eric Martinson"], "title": "Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture", "categories": ["cs.CV", "I.4.6"], "comment": "4 pages, 5 figures, 1 table", "summary": "Efficient crop-weed segmentation is critical for site-specific weed control\nin precision agriculture. Conventional CNN-based methods struggle to generalize\nand rely on RGB imagery, limiting performance under complex field conditions.\nTo address these challenges, we propose a lightweight transformer-CNN hybrid.\nIt processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using\nspecialized encoders and dynamic modality integration. Evaluated on the\nWeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of\n78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7\nmillion parameters, the model offers high accuracy, computational efficiency,\nand potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and\nedge devices, advancing precision weed management."}
{"id": "2505.06963", "pdf": "https://arxiv.org/pdf/2505.06963", "abs": "https://arxiv.org/abs/2505.06963", "authors": ["Tarik Houichime", "Younes EL Amrani"], "title": "Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper introduces an innovative approach for the autonomous landing of\nUnmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera,\ntherefore obviating the requirement for depth estimation cameras. Drawing on\nthe inherent human estimating process, the proposed method reframes the landing\ntask as an optimization problem. The UAV employs variations in the visual\ncharacteristics of a specially designed lenticular circle on the landing pad,\nwhere the perceived color and form provide critical information for estimating\nboth altitude and depth. Reinforcement learning algorithms are utilized to\napproximate the functions governing these estimations, enabling the UAV to\nascertain ideal landing settings via training. This method's efficacy is\nassessed by simulations and experiments, showcasing its potential for robust\nand accurate autonomous landing without dependence on complex sensor setups.\nThis research contributes to the advancement of cost-effective and efficient\nUAV landing solutions, paving the way for wider applicability across various\nfields."}
{"id": "2505.07124", "pdf": "https://arxiv.org/pdf/2505.07124", "abs": "https://arxiv.org/abs/2505.07124", "authors": ["Francisco Andrade", "Gabriel Peyré", "Clarice Poon"], "title": "Learning from Samples: Inverse Problems over measures via Sharpened Fenchel-Young Losses", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Estimating parameters from samples of an optimal probability distribution is\nessential in applications ranging from socio-economic modeling to biological\nsystem analysis. In these settings, the probability distribution arises as the\nsolution to an optimization problem that captures either static interactions\namong agents or the dynamic evolution of a system over time. Our approach\nrelies on minimizing a new class of loss functions, called sharpened\nFenchel-Young losses, which measure the sub-optimality gap of the optimization\nproblem over the space of measures. We study the stability of this estimation\nmethod when only a finite number of sample is available. The parameters to be\nestimated typically correspond to a cost function in static problems and to a\npotential function in dynamic problems. To analyze stability, we introduce a\ngeneral methodology that leverages the strong convexity of the loss function\ntogether with the sample complexity of the forward optimization problem. Our\nanalysis emphasizes two specific settings in the context of optimal transport,\nwhere our method provides explicit stability guarantees: The first is inverse\nunbalanced optimal transport (iUOT) with entropic regularization, where the\nparameters to estimate are cost functions that govern transport computations;\nthis method has applications such as link prediction in machine learning. The\nsecond is inverse gradient flow (iJKO), where the objective is to recover a\npotential function that drives the evolution of a probability distribution via\nthe Jordan-Kinderlehrer-Otto (JKO) time-discretization scheme; this is\nparticularly relevant for understanding cell population dynamics in single-cell\ngenomics. Finally, we validate our approach through numerical experiments on\nGaussian distributions, where closed-form solutions are available, to\ndemonstrate the practical performance of our methods"}
{"id": "2502.12084", "pdf": "https://arxiv.org/pdf/2502.12084", "abs": "https://arxiv.org/abs/2502.12084", "authors": ["Jianshu Zhang", "Dongyu Yao", "Renjie Pi", "Paul Pu Liang", "Yi R. Fung"], "title": "VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues", "categories": ["cs.CL"], "comment": "Project Page: https://vlm2-bench.github.io/", "summary": "Visually linking matching cues is a crucial ability in daily life, such as\nidentifying the same person in multiple photos based on their cues, even\nwithout knowing who they are. Despite the extensive knowledge that\nvision-language models (VLMs) possess, it remains largely unexplored whether\nthey are capable of performing this fundamental task. To address this, we\nintroduce VLM2-Bench, a benchmark designed to assess whether VLMs can Visually\nLink Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive\nevaluation across eight open-source VLMs and GPT-4o, along with further\nanalysis of various language-side and vision-side prompting methods, leads to a\ntotal of eight key findings. We identify critical challenges in models' ability\nto link visual cues, highlighting a significant performance gap where even\nGPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i)\nenhancing core visual capabilities to improve adaptability and reduce reliance\non prior knowledge, (ii) establishing clearer principles for integrating\nlanguage-based reasoning in vision-centric tasks to prevent unnecessary biases,\nand (iii) shifting vision-text training paradigms toward fostering models'\nability to independently structure and infer relationships among visual cues."}
{"id": "2505.07481", "pdf": "https://arxiv.org/pdf/2505.07481", "abs": "https://arxiv.org/abs/2505.07481", "authors": ["Erik Landolsi", "Fredrik Kahl"], "title": "Addressing degeneracies in latent interpolation for diffusion models", "categories": ["cs.CV"], "comment": "14 pages, 12 figures", "summary": "There is an increasing interest in using image-generating diffusion models\nfor deep data augmentation and image morphing. In this context, it is useful to\ninterpolate between latents produced by inverting a set of input images, in\norder to generate new images representing some mixture of the inputs. We\nobserve that such interpolation can easily lead to degenerate results when the\nnumber of inputs is large. We analyze the cause of this effect theoretically\nand experimentally, and suggest a suitable remedy. The suggested approach is a\nrelatively simple normalization scheme that is easy to use whenever\ninterpolation between latents is needed. We measure image quality using FID and\nCLIP embedding distance and show experimentally that baseline interpolation\nmethods lead to a drop in quality metrics long before the degeneration issue is\nclearly visible. In contrast, our method significantly reduces the degeneration\neffect and leads to improved quality metrics also in non-degenerate situations."}
{"id": "2505.07012", "pdf": "https://arxiv.org/pdf/2505.07012", "abs": "https://arxiv.org/abs/2505.07012", "authors": ["Hao Xu", "Yinqiao Wang", "Niloy J. Mitra", "Shuaicheng Liu", "Pheng-Ann Heng", "Chi-Wing Fu"], "title": "Hand-Shadow Poser", "categories": ["cs.CG", "cs.AI"], "comment": "SIGGRAPH 2025 (ACM TOG)", "summary": "Hand shadow art is a captivating art form, creatively using hand shadows to\nreproduce expressive shapes on the wall. In this work, we study an inverse\nproblem: given a target shape, find the poses of left and right hands that\ntogether best produce a shadow resembling the input. This problem is\nnontrivial, since the design space of 3D hand poses is huge while being\nrestrictive due to anatomical constraints. Also, we need to attend to the\ninput's shape and crucial features, though the input is colorless and\ntextureless. To meet these challenges, we design Hand-Shadow Poser, a\nthree-stage pipeline, to decouple the anatomical constraints (by hand) and\nsemantic constraints (by shadow shape): (i) a generative hand assignment module\nto explore diverse but reasonable left/right-hand shape hypotheses; (ii) a\ngeneralized hand-shadow alignment module to infer coarse hand poses with a\nsimilarity-driven strategy for selecting hypotheses; and (iii) a\nshadow-feature-aware refinement module to optimize the hand poses for physical\nplausibility and shadow feature preservation. Further, we design our pipeline\nto be trainable on generic public hand data, thus avoiding the need for any\nspecialized training dataset. For method validation, we build a benchmark of\n210 diverse shadow shapes of varying complexity and a comprehensive set of\nmetrics, including a novel DINOv2-based evaluation metric. Through extensive\ncomparisons with multiple baselines and user studies, our approach is\ndemonstrated to effectively generate bimanual hand poses for a large variety of\nhand shapes for over 85% of the benchmark cases."}
{"id": "2505.07137", "pdf": "https://arxiv.org/pdf/2505.07137", "abs": "https://arxiv.org/abs/2505.07137", "authors": ["Danny Calegari"], "title": "Triangulating PL functions and the existence of efficient ReLU DNNs", "categories": ["cs.LG", "math.GT"], "comment": "4 pages", "summary": "We show that every piecewise linear function $f:R^d \\to R$ with compact\nsupport a polyhedron $P$ has a representation as a sum of so-called `simplex\nfunctions'. Such representations arise from degree 1 triangulations of the\nrelative homology class (in $R^{d+1}$) bounded by $P$ and the graph of $f$, and\ngive a short elementary proof of the existence of efficient universal ReLU\nneural networks that simultaneously compute all such functions $f$ of bounded\ncomplexity."}
{"id": "2502.12896", "pdf": "https://arxiv.org/pdf/2502.12896", "abs": "https://arxiv.org/abs/2502.12896", "authors": ["Eva Sánchez Salido", "Julio Gonzalo", "Guillermo Marco"], "title": "None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers."}
{"id": "2505.07496", "pdf": "https://arxiv.org/pdf/2505.07496", "abs": "https://arxiv.org/abs/2505.07496", "authors": ["Mohamed Ali Souibgui", "Changkyu Choi", "Andrey Barsky", "Kangsoo Jung", "Ernest Valveny", "Dimosthenis Karatzas"], "title": "DocVXQA: Context-Aware Visual Explanations for Document Question Answering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose DocVXQA, a novel framework for visually self-explainable document\nquestion answering. The framework is designed not only to produce accurate\nanswers to questions but also to learn visual heatmaps that highlight\ncontextually critical regions, thereby offering interpretable justifications\nfor the model's decisions. To integrate explanations into the learning process,\nwe quantitatively formulate explainability principles as explicit learning\nobjectives. Unlike conventional methods that emphasize only the regions\npertinent to the answer, our framework delivers explanations that are\n\\textit{contextually sufficient} while remaining\n\\textit{representation-efficient}. This fosters user trust while achieving a\nbalance between predictive performance and interpretability in DocVQA\napplications. Extensive experiments, including human evaluation, provide strong\nevidence supporting the effectiveness of our method. The code is available at\nhttps://github.com/dali92002/DocVXQA."}
{"id": "2505.07020", "pdf": "https://arxiv.org/pdf/2505.07020", "abs": "https://arxiv.org/abs/2505.07020", "authors": ["Suyeon Choi"], "title": "R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction", "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.2"], "comment": "theory-only preprint. Independent research", "summary": "This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego),\na theoretical framework for restructuring emotional output in long-term\nhuman-AI interaction. While prior affective computing approaches emphasized\nexpressiveness, immersion, and responsiveness, they often neglected the\ncognitive and structural consequences of repeated emotional engagement. R-CAGE\ninstead conceptualizes emotional output not as reactive expression but as\nethical design structure requiring architectural intervention. The model is\ngrounded in experiential observations of subtle affective symptoms such as\nlocalized head tension, interpretive fixation, and emotional lag arising from\nprolonged interaction with affective AI systems. These indicate a mismatch\nbetween system-driven emotion and user interpretation that cannot be fully\nexplained by biometric data or observable behavior. R-CAGE adopts a\nuser-centered stance prioritizing psychological recovery, interpretive\nautonomy, and identity continuity. The framework consists of four control\nblocks: (1) Control of Rhythmic Expression regulates output pacing to reduce\nfatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing\nof affective stimuli; (3) Guarding of Cognitive Framing reduces semantic\npressure to allow flexible interpretation; (4) Ego-Aligned Response Design\nsupports self-reference recovery during interpretive lag. By structurally\nregulating emotional rhythm, sensory intensity, and interpretive affordances,\nR-CAGE frames emotion not as performative output but as sustainable design\nunit. The goal is to protect users from oversaturation and cognitive overload\nwhile sustaining long-term interpretive agency in AI-mediated environments."}
{"id": "2505.07149", "pdf": "https://arxiv.org/pdf/2505.07149", "abs": "https://arxiv.org/abs/2505.07149", "authors": ["Heqing Ren", "Chao Feng", "Alberto Huertas", "Burkhard Stiller"], "title": "AugMixCloak: A Defense against Membership Inference Attacks via Image Transformation", "categories": ["cs.LG"], "comment": null, "summary": "Traditional machine learning (ML) raises serious privacy concerns, while\nfederated learning (FL) mitigates the risk of data leakage by keeping data on\nlocal devices. However, the training process of FL can still leak sensitive\ninformation, which adversaries may exploit to infer private data. One of the\nmost prominent threats is the membership inference attack (MIA), where the\nadversary aims to determine whether a particular data record was part of the\ntraining set.\n  This paper addresses this problem through a two-stage defense called\nAugMixCloak. The core idea is to apply data augmentation and principal\ncomponent analysis (PCA)-based information fusion to query images, which are\ndetected by perceptual hashing (pHash) as either identical to or highly similar\nto images in the training set. Experimental results show that AugMixCloak\nsuccessfully defends against both binary classifier-based MIA and metric-based\nMIA across five datasets and various decentralized FL (DFL) topologies.\nCompared with regularization-based defenses, AugMixCloak demonstrates stronger\nprotection. Compared with confidence score masking, AugMixCloak exhibits better\ngeneralization."}
{"id": "2503.00955", "pdf": "https://arxiv.org/pdf/2503.00955", "abs": "https://arxiv.org/abs/2503.00955", "authors": ["Dien X. Tran", "Nam V. Nguyen", "Thanh T. Tran", "Anh T. Hoang", "Tai V. Duong", "Di T. Le", "Phuc-Lu Le"], "title": "SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "The rise of misinformation, exacerbated by Large Language Models (LLMs) like\nGPT and Gemini, demands robust fact-checking solutions, especially for\nlow-resource languages like Vietnamese. Existing methods struggle with semantic\nambiguity, homonyms, and complex linguistic structures, often trading accuracy\nfor efficiency. We introduce SemViQA, a novel Vietnamese fact-checking\nframework integrating Semantic-based Evidence Retrieval (SER) and Two-step\nVerdict Classification (TVC). Our approach balances precision and speed,\nachieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01\nand 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge.\nAdditionally, SemViQA Faster improves inference speed 7x while maintaining\ncompetitive accuracy. SemViQA sets a new benchmark for Vietnamese fact\nverification, advancing the fight against misinformation. The source code is\navailable at: https://github.com/DAVID-NGUYEN-S16/SemViQA."}
{"id": "2505.07500", "pdf": "https://arxiv.org/pdf/2505.07500", "abs": "https://arxiv.org/abs/2505.07500", "authors": ["Bahram Mohammadi", "Ehsan Abbasnejad", "Yuankai Qi", "Qi Wu", "Anton Van Den Hengel", "Javen Qinfeng Shi"], "title": "Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The remote embodied referring expression (REVERIE) task requires an agent to\nnavigate through complex indoor environments and localize a remote object\nspecified by high-level instructions, such as \"bring me a spoon\", without\npre-exploration. Hence, an efficient navigation plan is essential for the final\nsuccess. This paper proposes a novel parameter-efficient action planner using\nlarge language models (PEAP-LLM) to generate a single-step instruction at each\nlocation. The proposed model consists of two modules, LLM goal planner (LGP)\nand LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan\nfrom REVERIE instructions, including the target object and room. Then, LAP\ngenerates a single-step instruction with the goal-oriented plan, high-level\ninstruction, and current visual observation as input. PEAP-LLM enables the\nembodied agent to interact with LAP as the path planner on the fly. A simple\ndirect application of LLMs hardly achieves good performance. Also, existing\nhard-prompt-based methods are error-prone in complicated scenarios and need\nhuman intervention. To address these issues and prevent the LLM from generating\nhallucinations and biased information, we propose a novel two-stage method for\nfine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct\npreference optimization (DPO). SFT improves the quality of generated\ninstructions, while DPO utilizes environmental feedback. Experimental results\nshow the superiority of our proposed model on REVERIE compared to the previous\nstate-of-the-art."}
{"id": "2505.07041", "pdf": "https://arxiv.org/pdf/2505.07041", "abs": "https://arxiv.org/abs/2505.07041", "authors": ["Samaneh Mohammadi", "Iraklis Symeonidis", "Ali Balador", "Francesco Flammini"], "title": "Empirical Analysis of Asynchronous Federated Learning on Heterogeneous Devices: Efficiency, Fairness, and Privacy Trade-offs", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "This paper was accepted to IJCNN 2025. This version is a preprint and\n  not the official published version", "summary": "Device heterogeneity poses major challenges in Federated Learning (FL), where\nresource-constrained clients slow down synchronous schemes that wait for all\nupdates before aggregation. Asynchronous FL addresses this by incorporating\nupdates as they arrive, substantially improving efficiency. While its\nefficiency gains are well recognized, its privacy costs remain largely\nunexplored, particularly for high-end devices that contribute updates more\nfrequently, increasing their cumulative privacy exposure. This paper presents\nthe first comprehensive analysis of the efficiency-fairness-privacy trade-off\nin synchronous vs. asynchronous FL under realistic device heterogeneity. We\nempirically compare FedAvg and staleness-aware FedAsync using a physical\ntestbed of five edge devices spanning diverse hardware tiers, integrating Local\nDifferential Privacy (LDP) and the Moments Accountant to quantify per-client\nprivacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical\nbenchmark, we show that FedAsync achieves up to 10x faster convergence but\nexacerbates fairness and privacy disparities: high-end devices contribute 6-10x\nmore updates and incur up to 5x higher privacy loss, while low-end devices\nsuffer amplified accuracy degradation due to infrequent, stale, and\nnoise-perturbed updates. These findings motivate the need for adaptive FL\nprotocols that jointly optimize aggregation and privacy mechanisms based on\nclient capacity and participation dynamics, moving beyond static,\none-size-fits-all solutions."}
{"id": "2505.07180", "pdf": "https://arxiv.org/pdf/2505.07180", "abs": "https://arxiv.org/abs/2505.07180", "authors": ["Ruichu Cai", "Kaitao Zheng", "Junxian Huang", "Zijian Li", "Zhengming Chen", "Boyan Xu", "Zhifeng Hao"], "title": "Causal View of Time Series Imputation: Some Identification Results on Missing Mechanism", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Time series imputation is one of the most challenge problems and has broad\napplications in various fields like health care and the Internet of Things.\nExisting methods mainly aim to model the temporally latent dependencies and the\ngeneration process from the observed time series data. In real-world scenarios,\ndifferent types of missing mechanisms, like MAR (Missing At Random), and MNAR\n(Missing Not At Random) can occur in time series data. However, existing\nmethods often overlook the difference among the aforementioned missing\nmechanisms and use a single model for time series imputation, which can easily\nlead to misleading results due to mechanism mismatching. In this paper, we\npropose a framework for time series imputation problem by exploring Different\nMissing Mechanisms (DMM in short) and tailoring solutions accordingly.\nSpecifically, we first analyze the data generation processes with temporal\nlatent states and missing cause variables for different mechanisms.\nSequentially, we model these generation processes via variational inference and\nestimate prior distributions of latent variables via normalizing flow-based\nneural architecture. Furthermore, we establish identifiability results under\nthe nonlinear independent component analysis framework to show that latent\nvariables are identifiable. Experimental results show that our method surpasses\nexisting time series imputation techniques across various datasets with\ndifferent missing mechanisms, demonstrating its effectiveness in real-world\napplications."}
{"id": "2503.01217", "pdf": "https://arxiv.org/pdf/2503.01217", "abs": "https://arxiv.org/abs/2503.01217", "authors": ["Sijin Sun", "Ming Deng", "Xinrui Yu", "Liangbin Zhao"], "title": "HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages, 6 figures; Accepted for publication at the 2025\n  International Joint Conference on Neural Networks (IJCNN 2025), Rome, Italy,\n  30 June - 5 July", "summary": "Incorrect boundary division, complex semantic representation, and differences\nin pronunciation and meaning often lead to errors in Chinese Named Entity\nRecognition(CNER). To address these issues, this paper proposes HREB-CRF\nframework: Hierarchical Reduced-bias EMA with CRF. The proposed method\namplifies word boundaries and pools long text gradients through exponentially\nfixed-bias weighted average of local and global hierarchical attention.\nExperimental results on the MSRA, Resume, and Weibo datasets show excellent in\nF1, outperforming the baseline model by 1.1\\%, 1.6\\%, and 9.8\\%. The\nsignificant improvement in F1 shows evidences of strong effectiveness and\nrobustness of approach in CNER tasks."}
{"id": "2505.07511", "pdf": "https://arxiv.org/pdf/2505.07511", "abs": "https://arxiv.org/abs/2505.07511", "authors": ["Mauricio Orbes-Arteaga", "Oeslle Lucena", "Sabastien Ourselin", "M. Jorge Cardoso"], "title": "MAIS: Memory-Attention for Interactive Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Interactive medical segmentation reduces annotation effort by refining\npredictions through user feedback. Vision Transformer (ViT)-based models, such\nas the Segment Anything Model (SAM), achieve state-of-the-art performance using\nuser clicks and prior masks as prompts. However, existing methods treat\ninteractions as independent events, leading to redundant corrections and\nlimited refinement gains. We address this by introducing MAIS, a\nMemory-Attention mechanism for Interactive Segmentation that stores past user\ninputs and segmentation states, enabling temporal context integration. Our\napproach enhances ViT-based segmentation across diverse imaging modalities,\nachieving more efficient and accurate refinements."}
{"id": "2505.07064", "pdf": "https://arxiv.org/pdf/2505.07064", "abs": "https://arxiv.org/abs/2505.07064", "authors": ["Shusen Liu", "Haichao Miao", "Peer-Timo Bremer"], "title": "ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "While powerful and well-established, tools like ParaView present a steep\nlearning curve that discourages many potential users. This work introduces\nParaView-MCP, an autonomous agent that integrates modern multimodal large\nlanguage models (MLLMs) with ParaView to not only lower the barrier to entry\nbut also augment ParaView with intelligent decision support. By leveraging the\nstate-of-the-art reasoning, command execution, and vision capabilities of\nMLLMs, ParaView-MCP enables users to interact with ParaView through natural\nlanguage and visual inputs. Specifically, our system adopted the Model Context\nProtocol (MCP) - a standardized interface for model-application communication -\nthat facilitates direct interaction between MLLMs with ParaView's Python API to\nallow seamless information exchange between the user, the language model, and\nthe visualization tool itself. Furthermore, by implementing a visual feedback\nmechanism that allows the agent to observe the viewport, we unlock a range of\nnew capabilities, including recreating visualizations from examples,\nclosed-loop visualization parameter updates based on user-defined goals, and\neven cross-application collaboration involving multiple tools. Broadly, we\nbelieve such an agent-driven visualization paradigm can profoundly change the\nway we interact with visualization tools. We expect a significant uptake in the\ndevelopment of such visualization tools, in both visualization research and\nindustry."}
{"id": "2505.07222", "pdf": "https://arxiv.org/pdf/2505.07222", "abs": "https://arxiv.org/abs/2505.07222", "authors": ["Nima Dehghani"], "title": "Compression, Regularity, Randomness and Emergent Structure: Rethinking Physical Complexity in the Data-Driven Era", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.IT", "math.IT", "physics.bio-ph", "physics.data-an"], "comment": null, "summary": "Complexity science offers a wide range of measures for quantifying\nunpredictability, structure, and information. Yet, a systematic conceptual\norganization of these measures is still missing.\n  We present a unified framework that locates statistical, algorithmic, and\ndynamical measures along three axes (regularity, randomness, and complexity)\nand situates them in a common conceptual space. We map statistical,\nalgorithmic, and dynamical measures into this conceptual space, discussing\ntheir computational accessibility and approximability.\n  This taxonomy reveals the deep challenges posed by uncomputability and\nhighlights the emergence of modern data-driven methods (including autoencoders,\nlatent dynamical models, symbolic regression, and physics-informed neural\nnetworks) as pragmatic approximations to classical complexity ideals. Latent\nspaces emerge as operational arenas where regularity extraction, noise\nmanagement, and structured compression converge, bridging theoretical\nfoundations with practical modeling in high-dimensional systems.\n  We close by outlining implications for physics-informed AI and AI-guided\ndiscovery in complex physical systems, arguing that classical questions of\ncomplexity remain central to next-generation scientific modeling."}
{"id": "2503.01844", "pdf": "https://arxiv.org/pdf/2503.01844", "abs": "https://arxiv.org/abs/2503.01844", "authors": ["Miriam Havin", "Timna Wharton Kleinman", "Moran Koren", "Yaniv Dover", "Ariel Goldstein"], "title": "Can (A)I Change Your Mind?", "categories": ["cs.CL"], "comment": "Accetped to CogSci 2025", "summary": "The increasing integration of large language model (LLM) based conversational\nagents into everyday life raises critical cognitive and social questions about\ntheir potential to influence human opinions. Although previous studies have\nshown that LLM-based agents can generate persuasive content, these typically\ninvolve controlled, English-language settings. Addressing this, our\npreregistered study explored LLM's persuasive capabilities in more ecological,\nunconstrained scenarios, examining both static (written paragraphs) and dynamic\n(conversations via Telegram) interaction types. Conducted entirely in Hebrew\nwith 200 participants, the study assessed the persuasive effects of both LLM\nand human interlocutors on controversial civil policy topics. Results indicated\nthat participants adopted LLM and human perspectives similarly, with\nsignificant opinion changes evident across all conditions, regardless of\ninterlocutor type or interaction mode. Confidence levels increased\nsignificantly in most scenarios, except in static LLM interactions. These\nfindings demonstrate LLM-based agents' robust persuasive capabilities across\ndiverse sources and settings, highlighting their potential impact on shaping\npublic opinions."}
{"id": "2505.07530", "pdf": "https://arxiv.org/pdf/2505.07530", "abs": "https://arxiv.org/abs/2505.07530", "authors": ["Raul Ismayilov", "Luuk Spreeuwers", "Dzemila Sero"], "title": "FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic face datasets are increasingly used to overcome the limitations of\nreal-world biometric data, including privacy concerns, demographic imbalance,\nand high collection costs. However, many existing methods lack fine-grained\ncontrol over identity attributes and fail to produce paired,\nidentity-consistent images under structured capture conditions. We introduce\nFLUXSynID, a framework for generating high-resolution synthetic face datasets\nwith user-defined identity attribute distributions and paired document-style\nand trusted live capture images. The dataset generated using the FLUXSynID\nframework shows improved alignment with real-world identity distributions and\ngreater inter-set diversity compared to prior work. The FLUXSynID framework for\ngenerating custom datasets, along with a dataset of 14,889 synthetic\nidentities, is publicly released to support biometric research, including face\nrecognition and morphing attack detection."}
{"id": "2505.07078", "pdf": "https://arxiv.org/pdf/2505.07078", "abs": "https://arxiv.org/abs/2505.07078", "authors": ["Weixian Waylon Li", "Hyeonjun Kim", "Mihai Cucuringu", "Tiejun Ma"], "title": "Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?", "categories": ["q-fin.TR", "cs.AI", "cs.CE"], "comment": "14 pages", "summary": "Large Language Models (LLMs) have recently been leveraged for asset pricing\ntasks and stock trading applications, enabling AI agents to generate investment\ndecisions from unstructured financial data. However, most evaluations of LLM\ntiming-based investing strategies are conducted on narrow timeframes and\nlimited stock universes, overstating effectiveness due to survivorship and\ndata-snooping biases. We critically assess their generalizability and\nrobustness by proposing FINSABER, a backtesting framework evaluating\ntiming-based strategies across longer periods and a larger universe of symbols.\nSystematic backtests over two decades and 100+ symbols reveal that previously\nreported LLM advantages deteriorate significantly under broader cross-section\nand over a longer-term evaluation. Our market regime analysis further\ndemonstrates that LLM strategies are overly conservative in bull markets,\nunderperforming passive benchmarks, and overly aggressive in bear markets,\nincurring heavy losses. These findings highlight the need to develop LLM\nstrategies that are able to prioritise trend detection and regime-aware risk\ncontrols over mere scaling of framework complexity."}
{"id": "2505.07245", "pdf": "https://arxiv.org/pdf/2505.07245", "abs": "https://arxiv.org/abs/2505.07245", "authors": ["Fei Liu", "Huanhuan Ren", "Yu Guan", "Xiuxu Wang", "Wang Lv", "Zhiqiang Hu", "Yaxi Chen"], "title": "REMEDI: Relative Feature Enhanced Meta-Learning with Distillation for Imbalanced Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Predicting future vehicle purchases among existing owners presents a critical\nchallenge due to extreme class imbalance (<0.5% positive rate) and complex\nbehavioral patterns. We propose REMEDI (Relative feature Enhanced Meta-learning\nwith Distillation for Imbalanced prediction), a novel multi-stage framework\naddressing these challenges. REMEDI first trains diverse base models to capture\ncomplementary aspects of user behavior. Second, inspired by comparative\nop-timization techniques, we introduce relative performance meta-features\n(deviation from ensemble mean, rank among peers) for effective model fusion\nthrough a hybrid-expert architecture. Third, we distill the ensemble's\nknowledge into a single efficient model via supervised fine-tuning with MSE\nloss, enabling practical deployment. Evaluated on approximately 800,000 vehicle\nowners, REMEDI significantly outperforms baseline approaches, achieving the\nbusiness target of identifying ~50% of actual buyers within the top 60,000\nrecommendations at ~10% precision. The distilled model preserves the ensemble's\npredictive power while maintaining deployment efficiency, demonstrating\nREMEDI's effectiveness for imbalanced prediction in industry settings."}
{"id": "2503.01921", "pdf": "https://arxiv.org/pdf/2503.01921", "abs": "https://arxiv.org/abs/2503.01921", "authors": ["Jiaying Hong", "Thanet Markchom", "Jianfei Xu", "Tong Wu", "Huizhi Liang"], "title": "NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in\ncontent generated by various large language models (LLMs) across multiple\nlanguages. This task involves not only identifying the presence of\nhallucinations but also pinpointing their specific occurrences. To tackle this\nchallenge, this study introduces two methods: modified RefChecker and modified\nSelfCheckGPT. The modified RefChecker integrates prompt-based factual\nverification into References, structuring them as claim-based tests rather than\nsingle external knowledge sources. The modified SelfCheckGPT incorporates\nexternal knowledge to overcome its reliance on internal knowledge. In addition,\nboth methods' original prompt designs are enhanced to identify hallucinated\nwords within LLM-generated texts. Experimental results demonstrate the\neffectiveness of the approach, achieving a high ranking on the test dataset in\ndetecting hallucinations across various languages, with an average IoU of\n0.5310 and an average COR of 0.5669."}
{"id": "2505.07533", "pdf": "https://arxiv.org/pdf/2505.07533", "abs": "https://arxiv.org/abs/2505.07533", "authors": ["Ahmad Fall", "Federica Granese", "Alex Lence", "Dominique Fourer", "Blaise Hanczar", "Joe-Elie Salem", "Jean-Daniel Zucker", "Edi Prifti"], "title": "IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monitoring and analyzing electrocardiogram (ECG) signals, even under varying\nphysiological conditions, including those influenced by physical activity,\ndrugs and stress, is crucial to accurately assess cardiac health. However,\ncurrent AI-based methods often fail to account for how these factors interact\nand alter ECG patterns, ultimately limiting their applicability in real-world\nsettings. This study introduces IKrNet, a novel neural network model, which\nidentifies drug-specific patterns in ECGs amidst certain physiological\nconditions. IKrNet's architecture incorporates spatial and temporal dynamics by\nusing a convolutional backbone with varying receptive field size to capture\nspatial features. A bi-directional Long Short-Term Memory module is also\nemployed to model temporal dependencies. By treating heart rate variability as\na surrogate for physiological fluctuations, we evaluated IKrNet's performance\nacross diverse scenarios, including conditions with physical stress, drug\nintake alone, and a baseline without drug presence. Our assessment follows a\nclinical protocol in which 990 healthy volunteers were administered 80mg of\nSotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a\nlife-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art\nmodels' accuracy and stability in varying physiological conditions,\nunderscoring its clinical viability."}
{"id": "2505.07096", "pdf": "https://arxiv.org/pdf/2505.07096", "abs": "https://arxiv.org/abs/2505.07096", "authors": ["Prithwish Dan", "Kushal Kedia", "Angela Chao", "Edward Weiyi Duan", "Maximus Adrian Pace", "Wei-Chiu Ma", "Sanjiban Choudhury"], "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Human videos offer a scalable way to train robot manipulation policies, but\nlack the action labels needed by standard imitation learning algorithms.\nExisting cross-embodiment approaches try to map human motion to robot actions,\nbut often fail when the embodiments differ significantly. We propose X-Sim, a\nreal-to-sim-to-real framework that uses object motion as a dense and\ntransferable signal for learning robot policies. X-Sim starts by reconstructing\na photorealistic simulation from an RGBD human video and tracking object\ntrajectories to define object-centric rewards. These rewards are used to train\na reinforcement learning (RL) policy in simulation. The learned policy is then\ndistilled into an image-conditioned diffusion policy using synthetic rollouts\nrendered with varied viewpoints and lighting. To transfer to the real world,\nX-Si introduces an online domain adaptation technique that aligns real and\nsimulated observations during deployment. Importantly, X-Sim does not require\nany robot teleoperation data. We evaluate it across 5 manipulation tasks in 2\nenvironments and show that it: (1) improves task progress by 30% on average\nover hand-tracking and sim-to-real baselines, (2) matches behavior cloning with\n10x less data collection time, and (3) generalizes to new camera viewpoints and\ntest-time changes. Code and videos are available at\nhttps://portal-cornell.github.io/X-Sim/."}
{"id": "2505.07260", "pdf": "https://arxiv.org/pdf/2505.07260", "abs": "https://arxiv.org/abs/2505.07260", "authors": ["Yuanhang Yang", "Chaozheng Wang", "Jing Li"], "title": "UMoE: Unifying Attention and FFN with Shared Experts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising\napproach for scaling Transformer models. While initial works primarily\nincorporated MoE into feed-forward network (FFN) layers, recent studies have\nexplored extending the MoE paradigm to attention layers to enhance model\nperformance. However, existing attention-based MoE layers require specialized\nimplementations and demonstrate suboptimal performance compared to their\nFFN-based counterparts. In this paper, we aim to unify the MoE designs in\nattention and FFN layers by introducing a novel reformulation of the attention\nmechanism, revealing an underlying FFN-like structure within attention modules.\nOur proposed architecture, UMoE, achieves superior performance through\nattention-based MoE layers while enabling efficient parameter sharing between\nFFN and attention components."}
{"id": "2503.03122", "pdf": "https://arxiv.org/pdf/2503.03122", "abs": "https://arxiv.org/abs/2503.03122", "authors": ["Zichao Li", "Xueru Wen", "Jie Lou", "Yuqiu Ji", "Yaojie Lu", "Xianpei Han", "Debing Zhang", "Le Sun"], "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025", "summary": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling."}
{"id": "2505.07538", "pdf": "https://arxiv.org/pdf/2505.07538", "abs": "https://arxiv.org/abs/2505.07538", "authors": ["Bohan Wang", "Zhongqi Yue", "Fengda Zhang", "Shuo Chen", "Li'an Bi", "Junzhe Zhang", "Xue Song", "Kennard Yanting Chan", "Jiachun Pan", "Weijia Wu", "Mingze Zhou", "Wang Lin", "Kaihang Pan", "Saining Zhang", "Liyu Jia", "Wentao Hu", "Wei Zhao", "Hanwang Zhang"], "title": "Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We completely discard the conventional spatial prior in image representation\nand introduce a novel discrete visual tokenizer: Self-consistency Tokenizer\n(Selftok). At its design core, we compose an autoregressive (AR) prior --\nmirroring the causal structure of language -- into visual tokens by using the\nreverse diffusion process of image generation. The AR property makes Selftok\nfundamentally distinct from traditional spatial tokens in the following two key\nways: - Selftok offers an elegant and minimalist approach to unify diffusion\nand AR for vision-language models (VLMs): By representing images with Selftok\ntokens, we can train a VLM using a purely discrete autoregressive architecture\n-- like that in LLMs -- without requiring additional modules or training\nobjectives. - We theoretically show that the AR prior satisfies the Bellman\nequation, whereas the spatial prior does not. Therefore, Selftok supports\nreinforcement learning (RL) for visual generation with effectiveness comparable\nto that achieved in LLMs. Besides the AR property, Selftok is also a SoTA\ntokenizer that achieves a favorable trade-off between high-quality\nreconstruction and compression rate. We use Selftok to build a pure AR VLM for\nboth visual comprehension and generation tasks. Impressively, without using any\ntext-image training pairs, a simple policy gradient RL working in the visual\ntokens can significantly boost the visual generation benchmark, surpassing all\nthe existing models by a large margin. Therefore, we believe that Selftok\neffectively addresses the long-standing challenge that visual tokens cannot\nsupport effective RL. When combined with the well-established strengths of RL\nin LLMs, this brings us one step closer to realizing a truly multimodal LLM.\nProject Page: https://selftok-team.github.io/report/."}
{"id": "2505.07214", "pdf": "https://arxiv.org/pdf/2505.07214", "abs": "https://arxiv.org/abs/2505.07214", "authors": ["Pascal Spiegler", "Arash Harirpoush", "Yiming Xiao"], "title": "Towards user-centered interactive medical image segmentation in VR with an assistive AI agent", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "Crucial in disease analysis and surgical planning, manual segmentation of\nvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and\nchallenging to master, while fully automatic algorithms can benefit from\nuser-feedback. Therefore, with the complementary power of the latest\nradiological AI foundation models and virtual reality (VR)'s intuitive data\ninteraction, we propose SAMIRA, a novel conversational AI agent that assists\nusers with localizing, segmenting, and visualizing 3D medical concepts in VR.\nThrough speech-based interaction, the agent helps users understand radiological\nfeatures, locate clinical targets, and generate segmentation masks that can be\nrefined with just a few point prompts. The system also supports true-to-scale\n3D visualization of segmented pathology to enhance patient-specific anatomical\nunderstanding. Furthermore, to determine the optimal interaction paradigm under\nnear-far attention-switching for refining segmentation masks in an immersive,\nhuman-in-the-loop workflow, we compare VR controller pointing, head pointing,\nand eye tracking as input modes. With a user study, evaluations demonstrated a\nhigh usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as\nstrong support for the proposed VR system's guidance, training potential, and\nintegration of AI in radiological segmentation tasks."}
{"id": "2505.07274", "pdf": "https://arxiv.org/pdf/2505.07274", "abs": "https://arxiv.org/abs/2505.07274", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains", "categories": ["cs.LG"], "comment": null, "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings."}
{"id": "2503.10354", "pdf": "https://arxiv.org/pdf/2503.10354", "abs": "https://arxiv.org/abs/2503.10354", "authors": ["Nevidu Jayatilleke", "Ruvan Weerasinghe"], "title": "A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization", "categories": ["cs.CL"], "comment": "Accepted Paper in the 8th International Research Conference on Smart\n  Computing and Systems Engineering, University of Kelaniya, Sri Lanka.\n  (Pending Publication)", "summary": "Automatic patent summarization approaches that help in the patent analysis\nand comprehension procedure are in high demand due to the colossal growth of\ninnovations. The development of natural language processing (NLP), text mining,\nand deep learning has notably amplified the efficacy of text summarization\nmodels for abundant types of documents. Summarizing patent text remains a\npertinent challenge due to the labyrinthine writing style of these documents,\nwhich includes technical and legal intricacies. Additionally, these patent\ndocument contents are considerably lengthier than archetypal documents, which\ncomplicates the process of extracting pertinent information for summarization.\nEmbodying extractive and abstractive text summarization methodologies into a\nhybrid framework, this study proposes a system for efficiently creating\nabstractive summaries of patent records. The procedure involves leveraging the\nLexRank graph-based algorithm to retrieve the important sentences from input\nparent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART)\nmodel that has been fine-tuned using Low-Ranking Adaptation (LoRA) for\nproducing text summaries. This is accompanied by methodical testing and\nevaluation strategies. Furthermore, the author employed certain meta-learning\ntechniques to achieve Domain Generalization (DG) of the abstractive component\nacross multiple patent fields."}
{"id": "2505.07539", "pdf": "https://arxiv.org/pdf/2505.07539", "abs": "https://arxiv.org/abs/2505.07539", "authors": ["Hao Li", "Sicheng Li", "Xiang Gao", "Abudouaihati Batuer", "Lu Yu", "Yiyi Liao"], "title": "GIFStream: 4D Gaussian-based Immersive Video with Feature Stream", "categories": ["cs.CV"], "comment": "14 pages, 10 figures", "summary": "Immersive video offers a 6-Dof-free viewing experience, potentially playing a\nkey role in future video technology. Recently, 4D Gaussian Splatting has gained\nattention as an effective approach for immersive video due to its high\nrendering efficiency and quality, though maintaining quality with manageable\nstorage remains challenging. To address this, we introduce GIFStream, a novel\n4D Gaussian representation using a canonical space and a deformation field\nenhanced with time-dependent feature streams. These feature streams enable\ncomplex motion modeling and allow efficient compression by leveraging temporal\ncorrespondence and motion-aware pruning. Additionally, we incorporate both\ntemporal and spatial compression networks for end-to-end compression.\nExperimental results show that GIFStream delivers high-quality immersive video\nat 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project\npage: https://xdimlab.github.io/GIFStream"}
{"id": "2505.07236", "pdf": "https://arxiv.org/pdf/2505.07236", "abs": "https://arxiv.org/abs/2505.07236", "authors": ["Oleg Sautenkov", "Yasheerah Yaqoot", "Muhammad Ahsan Mustafa", "Faryal Batool", "Jeffrin Sam", "Artem Lykov", "Chih-Yung Wen", "Dzmitry Tsetserukou"], "title": "UAV-CodeAgents: Scalable UAV Mission Planning via Multi-Agent ReAct and Vision-Language Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted", "summary": "We present UAV-CodeAgents, a scalable multi-agent framework for autonomous\nUAV mission generation, built on large language and vision-language models\n(LLMs/VLMs). The system leverages the ReAct (Reason + Act) paradigm to\ninterpret satellite imagery, ground high-level natural language instructions,\nand collaboratively generate UAV trajectories with minimal human supervision. A\ncore component is a vision-grounded, pixel-pointing mechanism that enables\nprecise localization of semantic targets on aerial maps. To support real-time\nadaptability, we introduce a reactive thinking loop, allowing agents to\niteratively reflect on observations, revise mission goals, and coordinate\ndynamically in evolving environments.\n  UAV-CodeAgents is evaluated on large-scale mission scenarios involving\nindustrial and environmental fire detection. Our results show that a lower\ndecoding temperature (0.5) yields higher planning reliability and reduced\nexecution time, with an average mission creation time of 96.96 seconds and a\nsuccess rate of 93%. We further fine-tune Qwen2.5VL-7B on 9,000 annotated\nsatellite images, achieving strong spatial grounding across diverse visual\ncategories. To foster reproducibility and future research, we will release the\nfull codebase and a novel benchmark dataset for vision-language-based UAV\nplanning."}
{"id": "2505.07291", "pdf": "https://arxiv.org/pdf/2505.07291", "abs": "https://arxiv.org/abs/2505.07291", "authors": ["Prime Intellect Team", "Sami Jaghouar", "Justus Mattern", "Jack Min Ong", "Jannik Straube", "Manveer Basra", "Aaron Pazdera", "Kushal Thaman", "Matthew Di Ferrante", "Felix Gabriel", "Fares Obeid", "Kemal Erdem", "Michael Keiblinger", "Johannes Hagemann"], "title": "INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning", "categories": ["cs.LG", "cs.DC"], "comment": "26 pages, 12 figures", "summary": "We introduce INTELLECT-2, the first globally distributed reinforcement\nlearning (RL) training run of a 32 billion parameter language model. Unlike\ntraditional centralized training efforts, INTELLECT-2 trains a reasoning model\nusing fully asynchronous RL across a dynamic, heterogeneous swarm of\npermissionless compute contributors.\n  To enable a training run with this unique infrastructure, we built various\ncomponents from scratch: we introduce PRIME-RL, our training framework\npurpose-built for distributed asynchronous reinforcement learning, based on top\nof novel components such as TOPLOC, which verifies rollouts from untrusted\ninference workers, and SHARDCAST, which efficiently broadcasts policy weights\nfrom training nodes to inference workers.\n  Beyond infrastructure components, we propose modifications to the standard\nGRPO training recipe and data filtering techniques that were crucial to achieve\ntraining stability and ensure that our model successfully learned its training\nobjective, thus improving upon QwQ-32B, the state of the art reasoning model in\nthe 32B parameter range.\n  We open-source INTELLECT-2 along with all of our code and data, hoping to\nencourage and enable more open research in the field of decentralized training."}
{"id": "2504.13471", "pdf": "https://arxiv.org/pdf/2504.13471", "abs": "https://arxiv.org/abs/2504.13471", "authors": ["Jiliang Ni", "Jiachen Pu", "Zhongyi Yang", "Kun Zhou", "Hui Wang", "Xiaoliang Xiao", "Dakui Wang", "Xin Li", "Jingfeng Luo", "Conggang Hu"], "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced artificial\nintelligence by optimizing traditional Natural Language Processing (NLP)\nworkflows, facilitating their integration into various systems. Many such NLP\nsystems, including ours, directly incorporate LLMs. However, this approach\neither results in expensive costs or yields suboptimal performance after\nfine-tuning. In this paper, we introduce a three-stage cost-efficient\nend-to-end LLM deployment pipeline, comprising prototyping, knowledge transfer,\nand model compression, to effectively tackle the cost-performance dilemma in\nLLM-based frameworks. Its high cost-efficiency is manifested not only in\nsimplifying system complexity and producing super-tiny online models with\nenhanced performance and reduced costs in the results, but also in addressing\ndevelopment cycle constraints, the lack of extensive high-quality data, and\nlimited computational resources during the project development process. In the\nfirst stage, we construct an optimal performance prototype system by\ntransforming complex tasks into a function call-based LLM-driven pipeline,\nwhich serves as a teacher model to generate high-quality data. In the second\nstage, we combine techniques like rejection sampling fine-tuning, reinforcement\nlearning, and knowledge distillation to transfer knowledge to 0.5B student\nmodels, delivering effective performance at minimal cost. In the final stage,\nwe further compress models to 0.4B via quantization and pruning, achieving\nultra-low latency and cost. Extensive experimental results and the framework's\nmodular design suggest cross-domain capabilities and potential applicability in\nother NLP areas."}
{"id": "2505.07540", "pdf": "https://arxiv.org/pdf/2505.07540", "abs": "https://arxiv.org/abs/2505.07540", "authors": ["Juan E. Tapia", "Fabian Stockhardt", "Lázaro Janier González-Soler", "Christoph Busch"], "title": "SynID: Passport Synthetic Dataset for Presentation Attack Detection", "categories": ["cs.CV"], "comment": null, "summary": "The demand for Presentation Attack Detection (PAD) to identify fraudulent ID\ndocuments in remote verification systems has significantly risen in recent\nyears. This increase is driven by several factors, including the rise of remote\nwork, online purchasing, migration, and advancements in synthetic images.\nAdditionally, we have noticed a surge in the number of attacks aimed at the\nenrolment process. Training a PAD to detect fake ID documents is very\nchallenging because of the limited number of ID documents available due to\nprivacy concerns. This work proposes a new passport dataset generated from a\nhybrid method that combines synthetic data and open-access information using\nthe ICAO requirement to obtain realistic training and testing images."}
{"id": "2505.07239", "pdf": "https://arxiv.org/pdf/2505.07239", "abs": "https://arxiv.org/abs/2505.07239", "authors": ["Guang Yan", "Yuhui Zhang", "Zimu Guo", "Lutan Zhao", "Xiaojun Chen", "Chen Wang", "Wenhao Wang", "Dan Meng", "Rui Hou"], "title": "Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted to SP 2025", "summary": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction."}
{"id": "2505.07303", "pdf": "https://arxiv.org/pdf/2505.07303", "abs": "https://arxiv.org/abs/2505.07303", "authors": ["Bianca Marin Moreno", "Khaled Eldowa", "Pierre Gaillard", "Margaux Brégère", "Nadia Oudjane"], "title": "Online Episodic Convex Reinforcement Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We study online learning in episodic finite-horizon Markov decision processes\n(MDPs) with convex objective functions, known as the concave utility\nreinforcement learning (CURL) problem. This setting generalizes RL from linear\nto convex losses on the state-action distribution induced by the agent's\npolicy. The non-linearity of CURL invalidates classical Bellman equations and\nrequires new algorithmic approaches. We introduce the first algorithm achieving\nnear-optimal regret bounds for online CURL without any prior knowledge on the\ntransition function. To achieve this, we use an online mirror descent algorithm\nwith varying constraint sets and a carefully designed exploration bonus. We\nthen address for the first time a bandit version of CURL, where the only\nfeedback is the value of the objective function on the state-action\ndistribution induced by the agent's policy. We achieve a sub-linear regret\nbound for this more challenging problem by adapting techniques from bandit\nconvex optimization to the MDP setting."}
{"id": "2504.16007", "pdf": "https://arxiv.org/pdf/2504.16007", "abs": "https://arxiv.org/abs/2504.16007", "authors": ["Igor Rozhkov", "Natalia Loukachevitch"], "title": "Methods for Recognizing Nested Terms", "categories": ["cs.CL"], "comment": "Published in Computational Linguistics and Intellectual Technologies:\n  Proceedings of the International Conference \"Dialogue 2025\"", "summary": "In this paper, we describe our participation in the RuTermEval competition\ndevoted to extracting nested terms. We apply the Binder model, which was\npreviously successfully applied to the recognition of nested named entities, to\nextract nested terms. We obtained the best results of term recognition in all\nthree tracks of the RuTermEval competition. In addition, we study the new task\nof recognition of nested terms from flat training data annotated with terms\nwithout nestedness. We can conclude that several approaches we proposed in this\nwork are viable enough to retrieve nested terms effectively without nested\nlabeling of them."}
{"id": "2505.07552", "pdf": "https://arxiv.org/pdf/2505.07552", "abs": "https://arxiv.org/abs/2505.07552", "authors": ["Efe Bozkir", "Christian Kosel", "Tina Seidel", "Enkelejda Kasneci"], "title": "Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted as a long paper at the Educational Data Mining (EDM)\n  Conference 2025", "summary": "Teachers' visual attention and its distribution across the students in\nclassrooms can constitute important implications for student engagement,\nachievement, and professional teacher training. Despite that, inferring the\ninformation about where and which student teachers focus on is not trivial.\nMobile eye tracking can provide vital help to solve this issue; however, the\nuse of mobile eye tracking alone requires a significant amount of manual\nannotations. To address this limitation, we present an automated processing\npipeline concept that requires minimal manually annotated data to recognize\nwhich student the teachers focus on. To this end, we utilize state-of-the-art\nface detection models and face recognition feature embeddings to train face\nrecognition models with transfer learning in the classroom context and combine\nthese models with the teachers' gaze from mobile eye trackers. We evaluated our\napproach with data collected from four different classrooms, and our results\nshow that while it is possible to estimate the visually focused students with\nreasonable performance in all of our classroom setups, U-shaped and small\nclassrooms led to the best results with accuracies of approximately 0.7 and\n0.9, respectively. While we did not evaluate our method for teacher-student\ninteractions and focused on the validity of the technical approach, as our\nmethodology does not require a vast amount of manually annotated data and\noffers a non-intrusive way of handling teachers' visual attention, it could\nhelp improve instructional strategies, enhance classroom management, and\nprovide feedback for professional teacher development."}
{"id": "2505.07261", "pdf": "https://arxiv.org/pdf/2505.07261", "abs": "https://arxiv.org/abs/2505.07261", "authors": ["Ce Hao", "Anxing Xiao", "Zhiwei Xue", "Harold Soh"], "title": "CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Diffusion-based planners have shown strong performance in short-horizon tasks\nbut often fail in complex, long-horizon settings. We trace the failure to loose\ncoupling between high-level (HL) sub-goal selection and low-level (LL)\ntrajectory generation, which leads to incoherent plans and degraded\nperformance. We propose Coupled Hierarchical Diffusion (CHD), a framework that\nmodels HL sub-goals and LL trajectories jointly within a unified diffusion\nprocess. A shared classifier passes LL feedback upstream so that sub-goals\nself-correct while sampling proceeds. This tight HL-LL coupling improves\ntrajectory coherence and enables scalable long-horizon diffusion planning.\nExperiments across maze navigation, tabletop manipulation, and household\nenvironments show that CHD consistently outperforms both flat and hierarchical\ndiffusion baselines."}
{"id": "2505.07309", "pdf": "https://arxiv.org/pdf/2505.07309", "abs": "https://arxiv.org/abs/2505.07309", "authors": ["Pei-Fu Guo", "Yun-Da Tsai", "Shou-De Lin"], "title": "Uncertainty Profiles for LLMs: Uncertainty Source Decomposition and Adaptive Model-Metric Selection", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) often generate fluent but factually incorrect\noutputs, known as hallucinations, which undermine their reliability in\nreal-world applications. While uncertainty estimation has emerged as a\npromising strategy for detecting such errors, current metrics offer limited\ninterpretability and lack clarity about the types of uncertainty they capture.\nIn this paper, we present a systematic framework for decomposing LLM\nuncertainty into four distinct sources, inspired by previous research. We\ndevelop a source-specific estimation pipeline to quantify these uncertainty\ntypes and evaluate how existing metrics relate to each source across tasks and\nmodels. Our results show that metrics, task, and model exhibit systematic\nvariation in uncertainty characteristic. Building on this, we propose a method\nfor task specific metric/model selection guided by the alignment or divergence\nbetween their uncertainty characteristics and that of a given task. Our\nexperiments across datasets and models demonstrate that our uncertainty-aware\nselection strategy consistently outperforms baseline strategies, helping us\nselect appropriate models or uncertainty metrics, and contributing to more\nreliable and efficient deployment in uncertainty estimation."}
{"id": "2504.16394", "pdf": "https://arxiv.org/pdf/2504.16394", "abs": "https://arxiv.org/abs/2504.16394", "authors": ["Fahmida Liza Piya", "Rahmatollah Beheshti"], "title": "ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation."}
{"id": "2505.07556", "pdf": "https://arxiv.org/pdf/2505.07556", "abs": "https://arxiv.org/abs/2505.07556", "authors": ["Kamil Jeziorek", "Tomasz Kryjak"], "title": "Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs", "categories": ["cs.CV"], "comment": "Presented at the Real-time Processing of Image, Depth and Video\n  Information 2025 workshop and to be considered for publication is the SPIE\n  Proceedings", "summary": "Event cameras offer significant advantages over traditional frame-based\nsensors. These include microsecond temporal resolution, robustness under\nvarying lighting conditions and low power consumption. Nevertheless, the\neffective processing of their sparse, asynchronous event streams remains\nchallenging. Existing approaches to this problem can be categorised into two\ndistinct groups. The first group involves the direct processing of event data\nwith neural models, such as Spiking Neural Networks or Graph Convolutional\nNeural Networks. However, this approach is often accompanied by a compromise in\nterms of qualitative performance. The second group involves the conversion of\nevents into dense representations with handcrafted aggregation functions, which\ncan boost accuracy at the cost of temporal fidelity. This paper introduces a\nnovel Self-Supervised Event Representation (SSER) method leveraging Gated\nRecurrent Unit (GRU) networks to achieve precise per-pixel encoding of event\ntimestamps and polarities without temporal discretisation. The recurrent layers\nare trained in a self-supervised manner to maximise the fidelity of event-time\nencoding. The inference is performed with event representations generated\nasynchronously, thus ensuring compatibility with high-throughput sensors. The\nexperimental validation demonstrates that SSER outperforms aggregation-based\nbaselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx\nobject detection datasets. Furthermore, the paper presents the first hardware\nimplementation of recurrent representation for event data on a System-on-Chip\nFPGA, achieving sub-microsecond latency and power consumption between 1-2 W,\nsuitable for real-time, power-efficient applications. Code is available at\nhttps://github.com/vision-agh/RecRepEvent."}
{"id": "2505.07286", "pdf": "https://arxiv.org/pdf/2505.07286", "abs": "https://arxiv.org/abs/2505.07286", "authors": ["Keyue Qiu", "Yuxuan Song", "Zhehuan Fan", "Peidong Liu", "Zhe Zhang", "Mingyue Zheng", "Hao Zhou", "Wei-Ying Ma"], "title": "Piloting Structure-Based Drug Design via Modality-Specific Optimal Schedule", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Structure-Based Drug Design (SBDD) is crucial for identifying bioactive\nmolecules. Recent deep generative models are faced with challenges in geometric\nstructure modeling. A major bottleneck lies in the twisted probability path of\nmulti-modalities -- continuous 3D positions and discrete 2D topologies -- which\njointly determine molecular geometries. By establishing the fact that noise\nschedules decide the Variational Lower Bound (VLB) for the twisted probability\npath, we propose VLB-Optimal Scheduling (VOS) strategy in this under-explored\narea, which optimizes VLB as a path integral for SBDD. Our model effectively\nenhances molecular geometries and interaction modeling, achieving\nstate-of-the-art PoseBusters passing rate of 95.9% on CrossDock, more than 10%\nimprovement upon strong baselines, while maintaining high affinities and robust\nintramolecular validity evaluated on held-out test set."}
{"id": "2505.07320", "pdf": "https://arxiv.org/pdf/2505.07320", "abs": "https://arxiv.org/abs/2505.07320", "authors": ["Yuhao Li", "Ling Luo", "Uwe Aickelin"], "title": "Dynamical Label Augmentation and Calibration for Noisy Electronic Health Records", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Medical research, particularly in predicting patient outcomes, heavily relies\non medical time series data extracted from Electronic Health Records (EHR),\nwhich provide extensive information on patient histories. Despite rigorous\nexamination, labeling errors are inevitable and can significantly impede\naccurate predictions of patient outcome. To address this challenge, we propose\nan \\textbf{A}ttention-based Learning Framework with Dynamic\n\\textbf{C}alibration and Augmentation for \\textbf{T}ime series Noisy\n\\textbf{L}abel \\textbf{L}earning (ACTLL). This framework leverages a\ntwo-component Beta mixture model to identify the certain and uncertain sets of\ninstances based on the fitness distribution of each class, and it captures\nglobal temporal dynamics while dynamically calibrating labels from the\nuncertain set or augmenting confident instances from the certain set.\nExperimental results on large-scale EHR datasets eICU and MIMIC-IV-ED, and\nseveral benchmark datasets from the UCR and UEA repositories, demonstrate that\nour model ACTLL has achieved state-of-the-art performance, especially under\nhigh noise levels."}
{"id": "2504.19044", "pdf": "https://arxiv.org/pdf/2504.19044", "abs": "https://arxiv.org/abs/2504.19044", "authors": ["Di Wu", "Yibin Lei", "Christof Monz"], "title": "Calibrating Translation Decoding with Quality Estimation on LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Neural machine translation (NMT) systems typically employ maximum a\nposteriori (MAP) decoding to select the highest-scoring translation from the\ndistribution mass. However, recent evidence highlights the inadequacy of MAP\ndecoding, often resulting in low-quality or even pathological hypotheses -- the\ndecoding objective is not aligned with real-world translation quality. This\npaper proposes calibrating hypothesis likelihoods with translation quality from\na distribution view by directly optimizing their Pearson correlation -- thereby\nenhancing the effectiveness of translation decoding. With our method,\ntranslation on large language models (LLMs) improves substantially after\nlimited training (2K instances per direction). This improvement is orthogonal\nto those achieved through supervised fine-tuning, leading to substantial gains\nacross a broad range of metrics and human evaluations -- even when applied to\ntop-performing translation-specialized LLMs fine-tuned on high-quality\ntranslation data, such as Tower, or when compared to recent preference\noptimization methods, like CPO. Moreover, the calibrated translation likelihood\ncan directly serve as a strong proxy for translation quality, closely\napproximating or even surpassing some state-of-the-art translation quality\nestimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates\nthat calibration enhances the effectiveness of MAP decoding, thereby enabling\ngreater efficiency in real-world deployment. The resulting state-of-the-art\ntranslation model, which covers 10 languages, along with the accompanying code\nand human evaluation data, has been released to the community:\nhttps://github.com/moore3930/calibrating-llm-mt."}
{"id": "2505.07573", "pdf": "https://arxiv.org/pdf/2505.07573", "abs": "https://arxiv.org/abs/2505.07573", "authors": ["Sarah de Boer", "Hartmut Häntze", "Kiran Vaidhya Venkadesh", "Myrthe A. D. Buser", "Gabriel E. Humpire Mamani", "Lina Xu", "Lisa C. Adams", "Jawed Nawabi", "Keno K. Bressem", "Bram van Ginneken", "Mathias Prokop", "Alessa Hering"], "title": "Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework", "categories": ["cs.CV", "cs.AI"], "comment": "35 pages, 11 figures", "summary": "Kidney abnormality segmentation has important potential to enhance the\nclinical workflow, especially in settings requiring quantitative assessments.\nKidney volume could serve as an important biomarker for renal diseases, with\nchanges in volume correlating directly with kidney function. Currently,\nclinical practice often relies on subjective visual assessment for evaluating\nkidney size and abnormalities, including tumors and cysts, which are typically\nstaged based on diameter, volume, and anatomical location. To support a more\nobjective and reproducible approach, this research aims to develop a robust,\nthoroughly validated kidney abnormality segmentation algorithm, made publicly\navailable for clinical and research use. We employ publicly available training\ndatasets and leverage the state-of-the-art medical image segmentation framework\nnnU-Net. Validation is conducted using both proprietary and public test\ndatasets, with segmentation performance quantified by Dice coefficient and the\n95th percentile Hausdorff distance. Furthermore, we analyze robustness across\nsubgroups based on patient sex, age, CT contrast phases, and tumor histologic\nsubtypes. Our findings demonstrate that our segmentation algorithm, trained\nexclusively on publicly available data, generalizes effectively to external\ntest sets and outperforms existing state-of-the-art models across all tested\ndatasets. Subgroup analyses reveal consistent high performance, indicating\nstrong robustness and reliability. The developed algorithm and associated code\nare publicly accessible at\nhttps://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation."}
{"id": "2505.07294", "pdf": "https://arxiv.org/pdf/2505.07294", "abs": "https://arxiv.org/abs/2505.07294", "authors": ["Tong Zhang", "Boyuan Zheng", "Ruiqian Nai", "Yingdong Hu", "Yen-Jen Wang", "Geng Chen", "Fanqi Lin", "Jiongye Li", "Chuye Hong", "Koushil Sreenath", "Yang Gao"], "title": "HuB: Learning Extreme Humanoid Balance", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "Project website: https://hub-robot.github.io", "summary": "The human body demonstrates exceptional motor capabilities-such as standing\nsteadily on one foot or performing a high kick with the leg raised over 1.5\nmeters-both requiring precise balance control. While recent research on\nhumanoid control has leveraged reinforcement learning to track human motions\nfor skill acquisition, applying this paradigm to balance-intensive tasks\nremains challenging. In this work, we identify three key obstacles: instability\nfrom reference motion errors, learning difficulties due to morphological\nmismatch, and the sim-to-real gap caused by sensor noise and unmodeled\ndynamics. To address these challenges, we propose HuB (Humanoid Balance), a\nunified framework that integrates reference motion refinement, balance-aware\npolicy learning, and sim-to-real robustness training, with each component\ntargeting a specific challenge. We validate our approach on the Unitree G1\nhumanoid robot across challenging quasi-static balance tasks, including extreme\nsingle-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy\nremains stable even under strong physical disturbances-such as a forceful\nsoccer strike-while baseline methods consistently fail to complete these tasks.\nProject website: https://hub-robot.github.io"}
{"id": "2505.07351", "pdf": "https://arxiv.org/pdf/2505.07351", "abs": "https://arxiv.org/abs/2505.07351", "authors": ["Prateek Garg", "Lokesh Nagalapatti", "Sunita Sarawagi"], "title": "From Search To Sampling: Generative Models For Robust Algorithmic Recourse", "categories": ["cs.LG"], "comment": null, "summary": "Algorithmic Recourse provides recommendations to individuals who are\nadversely impacted by automated model decisions, on how to alter their profiles\nto achieve a favorable outcome. Effective recourse methods must balance three\nconflicting goals: proximity to the original profile to minimize cost,\nplausibility for realistic recourse, and validity to ensure the desired\noutcome. We show that existing methods train for these objectives separately\nand then search for recourse through a joint optimization over the recourse\ngoals during inference, leading to poor recourse recommendations. We introduce\nGenRe, a generative recourse model designed to train the three recourse\nobjectives jointly. Training such generative models is non-trivial due to lack\nof direct recourse supervision. We propose efficient ways to synthesize such\nsupervision and further show that GenRe's training leads to a consistent\nestimator. Unlike most prior methods, that employ non-robust gradient descent\nbased search during inference, GenRe simply performs a forward sampling over\nthe generative model to produce minimum cost recourse, leading to superior\nperformance across multiple metrics. We also demonstrate GenRe provides the\nbest trade-off between cost, plausibility and validity, compared to\nstate-of-art baselines. Our code is available at:\nhttps://github.com/prateekgargx/genre."}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339", "abs": "https://arxiv.org/abs/2504.19339", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "title": "Explanatory Summarization with Discourse-Driven Planning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL 2025)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination."}
{"id": "2505.07576", "pdf": "https://arxiv.org/pdf/2505.07576", "abs": "https://arxiv.org/abs/2505.07576", "authors": ["Manuel Barusco", "Francesco Borsatti", "Youssef Ben Khalifa", "Davide Dalle Pezze", "Gian Antonio Susto"], "title": "Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semiconductor manufacturing is a complex, multistage process. Automated\nvisual inspection of Scanning Electron Microscope (SEM) images is indispensable\nfor minimizing equipment downtime and containing costs. Most previous research\nconsiders supervised approaches, assuming a sufficient number of anomalously\nlabeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging\nresearch domain, focuses on unsupervised learning, avoiding the costly defect\ncollection phase while providing explanations of the predictions. We introduce\na benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.\nOur results demonstrate the efficacy of modern VAD approaches in this field."}
{"id": "2505.07317", "pdf": "https://arxiv.org/pdf/2505.07317", "abs": "https://arxiv.org/abs/2505.07317", "authors": ["Ashmita Sampatsing", "Sophie Vos", "Emma Beauxis-Aussalet", "Justus Bogner"], "title": "How Do Companies Manage the Environmental Sustainability of AI? An Interview Study About Green AI Efforts and Regulations", "categories": ["cs.CY", "cs.AI"], "comment": "Accepted for publication at the 11th International Conference on ICT\n  for Sustainability (ICT4S'25), see https://conf.researchr.org/home/ict4s-2025", "summary": "With the ever-growing adoption of artificial intelligence (AI), AI-based\nsoftware and its negative impact on the environment are no longer negligible,\nand studying and mitigating this impact has become a critical area of research.\nHowever, it is currently unclear which role environmental sustainability plays\nduring AI adoption in industry and how AI regulations influence Green AI\npractices and decision-making in industry. We therefore aim to investigate the\nGreen AI perception and management of industry practitioners. To this end, we\nconducted a total of 11 interviews with participants from 10 different\norganizations that adopted AI-based software. The interviews explored three\nmain themes: AI adoption, current efforts in mitigating the negative\nenvironmental impact of AI, and the influence of the EU AI Act and the\nCorporate Sustainability Reporting Directive (CSRD). Our findings indicate that\n9 of 11 participants prioritized business efficiency during AI adoption, with\nminimal consideration of environmental sustainability. Monitoring and\nmitigation of AI's environmental impact were very limited. Only one participant\nmonitored negative environmental effects. Regarding applied mitigation\npractices, six participants reported no actions, with the others sporadically\nmentioning techniques like prompt engineering, relying on smaller models, or\nnot overusing AI. Awareness and compliance with the EU AI Act are low, with\nonly one participant reporting on its influence, while the CSRD drove\nsustainability reporting efforts primarily in larger companies. All in all, our\nfindings reflect a lack of urgency and priority for sustainable AI among these\ncompanies. We suggest that current regulations are not very effective, which\nhas implications for policymakers. Additionally, there is a need to raise\nindustry awareness, but also to provide user-friendly techniques and tools for\nGreen AI practices."}
{"id": "2505.07367", "pdf": "https://arxiv.org/pdf/2505.07367", "abs": "https://arxiv.org/abs/2505.07367", "authors": ["Julian Rodemann", "James Bailie"], "title": "Generalization Bounds and Stopping Rules for Learning with Self-Selected Data", "categories": ["cs.LG", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "comment": "38 pages, 4 figures", "summary": "Many learning paradigms self-select training data in light of previously\nlearned parameters. Examples include active learning, semi-supervised learning,\nbandits, or boosting. Rodemann et al. (2024) unify them under the framework of\n\"reciprocal learning\". In this article, we address the question of how well\nthese methods can generalize from their self-selected samples. In particular,\nwe prove universal generalization bounds for reciprocal learning using covering\nnumbers and Wasserstein ambiguity sets. Our results require no assumptions on\nthe distribution of self-selected data, only verifiable conditions on the\nalgorithms. We prove results for both convergent and finite iteration\nsolutions. The latter are anytime valid, thereby giving rise to stopping rules\nfor a practitioner seeking to guarantee the out-of-sample performance of their\nreciprocal learning algorithm. Finally, we illustrate our bounds and stopping\nrules for reciprocal learning's special case of semi-supervised learning."}
{"id": "2505.00024", "pdf": "https://arxiv.org/pdf/2505.00024", "abs": "https://arxiv.org/abs/2505.00024", "authors": ["Shaokun Zhang", "Yi Dong", "Jieyu Zhang", "Jan Kautz", "Bryan Catanzaro", "Andrew Tao", "Qingyun Wu", "Zhiding Yu", "Guilin Liu"], "title": "Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 6 tables, 12 figures. - update new results - add more\n  details", "summary": "Enabling large language models with external tools has become a pivotal\nstrategy for extending their functionality beyond text space. To enhance LLMs'\ntool-calling abilities, previous approaches primarily rely on supervised\nfine-tuning (SFT) with trajectories distilled from stronger models, often\nresulting in imitative reasoning that limits generalization. In this work, we\nexplore rule-based reinforcement learning to enhance tool-calling in LLMs,\nresulting in Nemotron-Research-Tool-N1, a series of tool-calling reasoning\nmodels. Rather than enforcing supervision over intermediate distilled reasoning\ntraces, Tool-N1 is trained with a binary RL reward that assesses only the\nformat validity and functional correctness of tool invocations. This\nlightweight supervision allows the model to develop reasoning strategies\nindependently, without relying on annotated trajectories. Experiments on\nseveral major benchmarks show that Tool-N1-7B/14B clearly outperform GPT-4o. We\nconduct a systematic study on the design of rule-based reinforcement learning\nstrategies for training tool-calling models. Using 5,518 distilled reasoning\ntrajectories, we compare SFT, RL, and the SFT-then-RL pipeline, finding that\nthe widely adopted SFT-then-RL paradigm does not necessarily outperform pure\nRL."}
{"id": "2505.07611", "pdf": "https://arxiv.org/pdf/2505.07611", "abs": "https://arxiv.org/abs/2505.07611", "authors": ["Yi Zhang", "Wenye Zhou", "Ruonan Lin", "Xin Yang", "Hao Zheng"], "title": "Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions", "categories": ["cs.CV"], "comment": null, "summary": "Traffic accident prediction and detection are critical for enhancing road\nsafety,and vision-based traffic accident anticipation (Vision-TAA) has emerged\nas a promising approach in the era of deep learning.This paper reviews 147\nrecent studies,focusing on the application of supervised,unsupervised,and\nhybrid deep learning models for accident prediction,alongside the use of\nreal-world and synthetic datasets.Current methodologies are categorized into\nfour key approaches: image and video feature-based prediction, spatiotemporal\nfeature-based prediction, scene understanding,and multimodal data fusion.While\nthese methods demonstrate significant potential,challenges such as data\nscarcity,limited generalization to complex scenarios,and real-time performance\nconstraints remain prevalent. This review highlights opportunities for future\nresearch,including the integration of multimodal data fusion, self-supervised\nlearning,and Transformer-based architectures to enhance prediction accuracy and\nscalability.By synthesizing existing advancements and identifying critical\ngaps, this paper provides a foundational reference for developing robust and\nadaptive Vision-TAA systems,contributing to road safety and traffic management."}
{"id": "2505.07339", "pdf": "https://arxiv.org/pdf/2505.07339", "abs": "https://arxiv.org/abs/2505.07339", "authors": ["Gabriel Lima", "Nina Grgić-Hlača", "Markus Langer", "Yixin Zou"], "title": "Laypeople's Attitudes Towards Fair, Affirmative, and Discriminatory Decision-Making Algorithms", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Affirmative algorithms have emerged as a potential answer to algorithmic\ndiscrimination, seeking to redress past harms and rectify the source of\nhistorical injustices. We present the results of two experiments ($N$$=$$1193$)\ncapturing laypeople's perceptions of affirmative algorithms -- those which\nexplicitly prioritize the historically marginalized -- in hiring and criminal\njustice. We contrast these opinions about affirmative algorithms with folk\nattitudes towards algorithms that prioritize the privileged (i.e.,\ndiscriminatory) and systems that make decisions independently of demographic\ngroups (i.e., fair). We find that people -- regardless of their political\nleaning and identity -- view fair algorithms favorably and denounce\ndiscriminatory systems. In contrast, we identify disagreements concerning\naffirmative algorithms: liberals and racial minorities rate affirmative systems\nas positively as their fair counterparts, whereas conservatives and those from\nthe dominant racial group evaluate affirmative algorithms as negatively as\ndiscriminatory systems. We identify a source of these divisions: people have\nvarying beliefs about who (if anyone) is marginalized, shaping their views of\naffirmative algorithms. We discuss the possibility of bridging these\ndisagreements to bring people together towards affirmative algorithms."}
{"id": "2505.07411", "pdf": "https://arxiv.org/pdf/2505.07411", "abs": "https://arxiv.org/abs/2505.07411", "authors": ["Wenhao Hu", "Paul Henderson", "José Cano"], "title": "ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "Pruning is a widely used method for compressing Deep Neural Networks (DNNs),\nwhere less relevant parameters are removed from a DNN model to reduce its size.\nHowever, removing parameters reduces model accuracy, so pruning is typically\ncombined with fine-tuning, and sometimes other operations such as rewinding\nweights, to recover accuracy. A common approach is to repeatedly prune and then\nfine-tune, with increasing amounts of model parameters being removed in each\nstep. While straightforward to implement, pruning pipelines that follow this\napproach are computationally expensive due to the need for repeated\nfine-tuning.\n  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs\nthat significantly decreases the time required for pruning by reducing the\noverall cost of fine-tuning, while maintaining a similar accuracy to existing\npruning pipelines. ICE-Pruning is based on three main components: i) an\nautomatic mechanism to determine after which pruning steps fine-tuning should\nbe performed; ii) a freezing strategy for faster fine-tuning in each pruning\nstep; and iii) a custom pruning-aware learning rate scheduler to further\nimprove the accuracy of each pruning step and reduce the overall time\nconsumption. We also propose an efficient auto-tuning stage for the\nhyperparameters (e.g., freezing percentage) introduced by the three components.\nWe evaluate ICE-Pruning on several DNN models and datasets, showing that it can\naccelerate pruning by up to 9.61x. Code is available at\nhttps://github.com/gicLAB/ICE-Pruning"}
{"id": "2505.02656", "pdf": "https://arxiv.org/pdf/2505.02656", "abs": "https://arxiv.org/abs/2505.02656", "authors": ["Rawan Bondok", "Mayar Nassar", "Salam Khalifa", "Kurt Micallef", "Nizar Habash"], "title": "Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset", "categories": ["cs.CL"], "comment": null, "summary": "Proper names in Arabic Wikipedia are frequently undiacritized, creating\nambiguity in pronunciation and interpretation, especially for transliterated\nnamed entities of foreign origin. While transliteration and diacritization have\nbeen well-studied separately in Arabic NLP,their intersection remains\nunderexplored. In this paper, we introduce a new manually diacritized dataset\nof Arabic proper names of various origins with their English Wikipedia\nequivalent glosses, and present the challenges and guidelines we followed to\ncreate it. We benchmark GPT-4o on the task of recovering full diacritization\ngiven the undiacritized Arabic and English forms, and analyze its performance.\nAchieving 73% accuracy, our results underscore both the difficulty of the task\nand the need for improved models and resources. We release our dataset to\nfacilitate further research on Arabic Wikipedia proper name diacritization."}
{"id": "2505.07620", "pdf": "https://arxiv.org/pdf/2505.07620", "abs": "https://arxiv.org/abs/2505.07620", "authors": ["Simone Azeglio", "Victor Calbiague Garcia", "Guilhem Glaziou", "Peter Neri", "Olivier Marre", "Ulisse Ferrari"], "title": "Higher-Order Convolution Improves Neural Predictivity in the Retina", "categories": ["cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "We present a novel approach to neural response prediction that incorporates\nhigher-order operations directly within convolutional neural networks (CNNs).\nOur model extends traditional 3D CNNs by embedding higher-order operations\nwithin the convolutional operator itself, enabling direct modeling of\nmultiplicative interactions between neighboring pixels across space and time.\nOur model increases the representational power of CNNs without increasing their\ndepth, therefore addressing the architectural disparity between deep artificial\nnetworks and the relatively shallow processing hierarchy of biological visual\nsystems. We evaluate our approach on two distinct datasets: salamander retinal\nganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC\nresponses to controlled geometric transformations. Our higher-order CNN (HoCNN)\nachieves superior performance while requiring only half the training data\ncompared to standard architectures, demonstrating correlation coefficients up\nto 0.75 with neural responses (against 0.80$\\pm$0.02 retinal reliability). When\nintegrated into state-of-the-art architectures, our approach consistently\nimproves performance across different species and stimulus conditions. Analysis\nof the learned representations reveals that our network naturally encodes\nfundamental geometric transformations, particularly scaling parameters that\ncharacterize object expansion and contraction. This capability is especially\nrelevant for specific cell types, such as transient OFF-alpha and transient ON\ncells, which are known to detect looming objects and object motion\nrespectively, and where our model shows marked improvement in response\nprediction. The correlation coefficients for scaling parameters are more than\ntwice as high in HoCNN (0.72) compared to baseline models (0.32)."}
{"id": "2505.07372", "pdf": "https://arxiv.org/pdf/2505.07372", "abs": "https://arxiv.org/abs/2505.07372", "authors": ["David de-Fitero-Dominguez", "Antonio Garcia-Cabot", "Eva Garcia-Lopez"], "title": "Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper presents a novel methodology for enhancing Automated Program\nRepair (APR) through synthetic data generation utilizing Large Language Models\n(LLMs). Current APR systems are constrained by the limited availability of\nhigh-quality training data encompassing diverse bug types across multiple\nprogramming languages. The proposed approach addresses this limitation through\na two-phase process: a synthetic sample generation followed by a rigorous\nquality assessment. Multiple state-of-the-art LLMs were employed to generate\napproximately 30,000 paired examples of buggy and fixed code across 12\nprogramming languages and 13 bug categories. Subsequently, these samples\nunderwent cross-model evaluation against five criteria: correctness, code\nquality, security, performance, and completeness. Experimental evaluation on\nthe VulRepair test set dataset showed statistically significant improvements in\nPerfect Prediction rates, with the quality-filtered synthetic dataset\noutperforming both baseline and real-world commit data configurations in\ncertain scenarios. The methodology was validated through rigorous statistical\ntesting, including ANOVA and post-hoc Tukey's Honest Significant Difference\nanalysis. Furthermore, the best-performing configurations surpassed existing\nsystems despite using a less computationally intensive decoding strategy. This\nresearch establishes a self-bootstrapping paradigm in which LLMs generate and\nevaluate their own training data, potentially transforming approaches to data\nscarcity across software engineering tasks and advancing the development of\nrobust, adaptable tools for automated code maintenance."}
{"id": "2505.07413", "pdf": "https://arxiv.org/pdf/2505.07413", "abs": "https://arxiv.org/abs/2505.07413", "authors": ["Tung L Nguyen", "Toby Hocking"], "title": "Learning Penalty for Optimal Partitioning via Automatic Feature Extraction", "categories": ["cs.LG", "stat.AP"], "comment": "9 Figures", "summary": "Changepoint detection identifies significant shifts in data sequences, making\nit important in areas like finance, genetics, and healthcare. The Optimal\nPartitioning algorithms efficiently detect these changes, using a penalty\nparameter to limit the changepoints number. Determining the appropriate value\nfor this penalty can be challenging. Traditionally, this process involved\nmanually extracting statistical features, such as sequence length or variance\nto make the prediction. This study proposes a novel approach that uses\nrecurrent neural networks to learn this penalty directly from raw sequences by\nautomatically extracting features. Experiments conducted on 20 benchmark\ngenomic datasets show that this novel method surpasses traditional methods in\npartitioning accuracy in most cases."}
{"id": "2310.14356", "pdf": "https://arxiv.org/pdf/2310.14356", "abs": "https://arxiv.org/abs/2310.14356", "authors": ["Andre Ye", "Sebastin Santy", "Jena D. Hwang", "Amy X. Zhang", "Ranjay Krishna"], "title": "Semantic and Expressive Variation in Image Captions Across Languages", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.HC"], "comment": "CVPR 2025", "summary": "Computer vision often treats human perception as homogeneous: an implicit\nassumption that visual stimuli are perceived similarly by everyone. This\nassumption is reflected in the way researchers collect datasets and train\nvision models. By contrast, literature in cross-cultural psychology and\nlinguistics has provided evidence that people from different cultural\nbackgrounds observe vastly different concepts even when viewing the same visual\nstimuli. In this paper, we study how these differences manifest themselves in\nvision-language datasets and models, using language as a proxy for culture. By\ncomparing textual descriptions generated across 7 languages for the same\nimages, we find significant differences in the semantic content and linguistic\nexpression. When datasets are multilingual as opposed to monolingual,\ndescriptions have higher semantic coverage on average, where coverage is\nmeasured using scene graphs, model embeddings, and linguistic taxonomies. For\nexample, multilingual descriptions have on average 29.9% more objects, 24.5%\nmore relations, and 46.0% more attributes than a set of monolingual captions.\nWhen prompted to describe images in different languages, popular models (e.g.\nLLaVA) inherit this bias and describe different parts of the image. Moreover,\nfinetuning models on captions from one language performs best on corresponding\ntest data from that language, while finetuning on multilingual data performs\nconsistently well across all test data compositions. Our work points towards\nthe need to account for and embrace the diversity of human perception in the\ncomputer vision community."}
{"id": "2505.07622", "pdf": "https://arxiv.org/pdf/2505.07622", "abs": "https://arxiv.org/abs/2505.07622", "authors": ["Zhuo Song", "Ye Zhang", "Kunhong Li", "Longguang Wang", "Yulan Guo"], "title": "A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Cross-view geo-localization is a promising solution for large-scale\nlocalization problems, requiring the sequential execution of retrieval and\nmetric localization tasks to achieve fine-grained predictions. However,\nexisting methods typically focus on designing standalone models for these two\ntasks, resulting in inefficient collaboration and increased training overhead.\nIn this paper, we propose UnifyGeo, a novel unified hierarchical\ngeo-localization framework that integrates retrieval and metric localization\ntasks into a single network. Specifically, we first employ a unified learning\nstrategy with shared parameters to jointly learn multi-granularity\nrepresentation, facilitating mutual reinforcement between these two tasks.\nSubsequently, we design a re-ranking mechanism guided by a dedicated loss\nfunction, which enhances geo-localization performance by improving both\nretrieval accuracy and metric localization references. Extensive experiments\ndemonstrate that UnifyGeo significantly outperforms the state-of-the-arts in\nboth task-isolated and task-associated settings. Remarkably, on the challenging\nVIGOR benchmark, which supports fine-grained localization evaluation, the\n1-meter-level localization recall rate improves from 1.53\\% to 39.64\\% and from\n0.43\\% to 25.58\\% under same-area and cross-area evaluations, respectively.\nCode will be made publicly available."}
{"id": "2505.07377", "pdf": "https://arxiv.org/pdf/2505.07377", "abs": "https://arxiv.org/abs/2505.07377", "authors": ["Suleyman Ozdel", "Can Sarpkaya", "Efe Bozkir", "Hong Gao", "Enkelejda Kasneci"], "title": "Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to EDM 2025 (Eighteenth International Conference on\n  Educational Data Mining)", "summary": "Transforming educational technologies through the integration of large\nlanguage models (LLMs) and virtual reality (VR) offers the potential for\nimmersive and interactive learning experiences. However, the effects of LLMs on\nuser engagement and attention in educational environments remain open\nquestions. In this study, we utilized a fully LLM-driven virtual learning\nenvironment, where peers and teachers were LLM-driven, to examine how students\nbehaved in such settings. Specifically, we investigate how peer question-asking\nbehaviors influenced student engagement, attention, cognitive load, and\nlearning outcomes and found that, in conditions where LLM-driven peer learners\nasked questions, students exhibited more targeted visual scanpaths, with their\nattention directed toward the learning content, particularly in complex\nsubjects. Our results suggest that peer questions did not introduce extraneous\ncognitive load directly, as the cognitive load is strongly correlated with\nincreased attention to the learning material. Considering these findings, we\nprovide design recommendations for optimizing VR learning spaces."}
{"id": "2505.07437", "pdf": "https://arxiv.org/pdf/2505.07437", "abs": "https://arxiv.org/abs/2505.07437", "authors": ["Xiaotian Lin", "Yanlin Qi", "Yizhang Zhu", "Themis Palpanas", "Chengliang Chai", "Nan Tang", "Yuyu Luo"], "title": "LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "Instruction tuning has emerged as a critical paradigm for improving the\ncapabilities and alignment of large language models (LLMs). However, existing\niterative model-aware data selection methods incur significant computational\noverhead, as they rely on repeatedly performing full-dataset model inference to\nestimate sample utility for subsequent training iterations, creating a\nfundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient\niterative data selection framework that accurately estimates sample utility\nentirely within the standard training loop, eliminating the need for costly\nadditional model inference. At its core, LEAD introduces Instance-Level Dynamic\nUncertainty (IDU), a theoretically grounded utility function combining\ninstantaneous training loss, gradient-based approximation of loss changes, and\nexponential smoothing of historical loss signals. To further scale efficiently\nto large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy,\nadaptively prioritizing informative clusters through a multi-armed bandit\nmechanism, followed by precise fine-grained selection of high-utility samples\nusing IDU. Extensive experiments across four diverse benchmarks show that LEAD\nsignificantly outperforms state-of-the-art methods, improving average model\nperformance by 6.1%-10.8% while using only 2.5% of the training data and\nreducing overall training time by 5-10x."}
{"id": "2311.11796", "pdf": "https://arxiv.org/pdf/2311.11796", "abs": "https://arxiv.org/abs/2311.11796", "authors": ["Guangjing Wang", "Ce Zhou", "Yuanda Wang", "Bocheng Chen", "Hanqing Guo", "Qiben Yan"], "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "As Artificial Intelligence (AI) systems increasingly underpin critical\napplications, from autonomous vehicles to biometric authentication, their\nvulnerability to transferable attacks presents a growing concern. These\nattacks, designed to generalize across instances, domains, models, tasks,\nmodalities, or even hardware platforms, pose severe risks to security, privacy,\nand system integrity. This survey delivers the first comprehensive review of\ntransferable attacks across seven major categories, including evasion,\nbackdoor, data poisoning, model stealing, model inversion, membership\ninference, and side-channel attacks. We introduce a unified six-dimensional\ntaxonomy: cross-instance, cross-domain, cross-modality, cross-model,\ncross-task, and cross-hardware, which systematically captures the diverse\ntransfer pathways of adversarial strategies. Through this framework, we examine\nboth the underlying mechanics and practical implications of transferable\nattacks on AI systems. Furthermore, we review cutting-edge methods for\nenhancing attack transferability, organized around data augmentation and\noptimization strategies. By consolidating fragmented research and identifying\ncritical future directions, this work provides a foundational roadmap for\nunderstanding, evaluating, and defending against transferable threats in\nreal-world AI systems."}
{"id": "2505.07652", "pdf": "https://arxiv.org/pdf/2505.07652", "abs": "https://arxiv.org/abs/2505.07652", "authors": ["Ozgur Kara", "Krishna Kumar Singh", "Feng Liu", "Duygu Ceylan", "James M. Rehg", "Tobias Hinz"], "title": "ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Current diffusion-based text-to-video methods are limited to producing short\nvideo clips of a single shot and lack the capability to generate multi-shot\nvideos with discrete transitions where the same character performs distinct\nactivities across the same or different backgrounds. To address this limitation\nwe propose a framework that includes a dataset collection pipeline and\narchitectural extensions to video diffusion models to enable text-to-multi-shot\nvideo generation. Our approach enables generation of multi-shot videos as a\nsingle video with full attention across all frames of all shots, ensuring\ncharacter and background consistency, and allows users to control the number,\nduration, and content of shots through shot-specific conditioning. This is\nachieved by incorporating a transition token into the text-to-video model to\ncontrol at which frames a new shot begins and a local attention masking\nstrategy which controls the transition token's effect and allows shot-specific\nprompting. To obtain training data we propose a novel data collection pipeline\nto construct a multi-shot video dataset from existing single-shot video\ndatasets. Extensive experiments demonstrate that fine-tuning a pre-trained\ntext-to-video model for a few thousand iterations is enough for the model to\nsubsequently be able to generate multi-shot videos with shot-specific control,\noutperforming the baselines. You can find more details in\nhttps://shotadapter.github.io/"}
{"id": "2505.07393", "pdf": "https://arxiv.org/pdf/2505.07393", "abs": "https://arxiv.org/abs/2505.07393", "authors": ["Nadine Sandjo Tchatchoua", "Richard Harper"], "title": "AI in Money Matters", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In November 2022, Europe and the world by and large were stunned by the birth\nof a new large language model : ChatGPT. Ever since then, both academic and\npopulist discussions have taken place in various public spheres such as\nLinkedIn and X(formerly known as Twitter) with the view to both understand the\ntool and its benefits for the society. The views of real actors in professional\nspaces, especially in regulated industries such as finance and law have been\nlargely missing. We aim to begin to close this gap by presenting results from\nan empirical investigation conducted through interviews with professional\nactors in the Fintech industry. The paper asks the question, how and to what\nextent are large language models in general and ChatGPT in particular being\nadopted and used in the Fintech industry? The results show that while the\nfintech experts we spoke with see a potential in using large language models in\nthe future, a lot of questions marks remain concerning how they are policed and\ntherefore might be adopted in a regulated industry such as Fintech. This paper\naims to add to the existing academic discussing around large language models,\nwith a contribution to our understanding of professional viewpoints."}
{"id": "2505.07447", "pdf": "https://arxiv.org/pdf/2505.07447", "abs": "https://arxiv.org/abs/2505.07447", "authors": ["Peng Sun", "Yi Jiang", "Tao Lin"], "title": "Unified Continuous Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "https://github.com/LINs-lab/UCGM", "summary": "Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM."}
{"id": "2403.16971", "pdf": "https://arxiv.org/pdf/2403.16971", "abs": "https://arxiv.org/abs/2403.16971", "authors": ["Kai Mei", "Xi Zhu", "Wujiang Xu", "Wenyue Hua", "Mingyu Jin", "Zelong Li", "Shuyuan Xu", "Ruosong Ye", "Yingqiang Ge", "Yongfeng Zhang"], "title": "AIOS: LLM Agent Operating System", "categories": ["cs.OS", "cs.AI", "cs.CL"], "comment": null, "summary": "LLM-based intelligent agents face significant deployment challenges,\nparticularly related to resource management. Allowing unrestricted access to\nLLM or tool resources can lead to inefficient or even potentially harmful\nresource allocation and utilization for agents. Furthermore, the absence of\nproper scheduling and resource management mechanisms in current agent designs\nhinders concurrent processing and limits overall system efficiency. As the\ndiversity and complexity of agents continue to grow, addressing these resource\nmanagement issues becomes increasingly critical to LLM-based agent systems. To\naddress these challenges, this paper proposes the architecture of AIOS\n(LLM-based AI Agent Operating System) under the context of managing LLM-based\nagents. It introduces a novel architecture for serving LLM-based agents by\nisolating resources and LLM-specific services from agent applications into an\nAIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling,\ncontext management, memory management, storage management, access control) and\nefficient management of resources (e.g., LLM and external tools) for runtime\nagents. To enhance usability, AIOS also includes an AIOS-Agent SDK, a\ncomprehensive suite of APIs designed for utilizing functionalities provided by\nthe AIOS kernel. Experimental results demonstrate that using AIOS can achieve\nup to 2.1x faster execution for serving agents built by various agent\nframeworks. The source code is available at\nhttps://github.com/agiresearch/AIOS."}
{"id": "2505.07689", "pdf": "https://arxiv.org/pdf/2505.07689", "abs": "https://arxiv.org/abs/2505.07689", "authors": ["Quang Vinh Nguyen", "Minh Duc Nguyen", "Thanh Hoang Son Vo", "Hyung-Jeong Yang", "Soo-Hyung Kim"], "title": "Anatomical Attention Alignment representation for Radiology Report Generation", "categories": ["cs.CV"], "comment": null, "summary": "Automated Radiology report generation (RRG) aims at producing detailed\ndescriptions of medical images, reducing radiologists' workload and improving\naccess to high-quality diagnostic services. Existing encoder-decoder models\nonly rely on visual features extracted from raw input images, which can limit\nthe understanding of spatial structures and semantic relationships, often\nresulting in suboptimal text generation. To address this, we propose Anatomical\nAttention Alignment Network (A3Net), a framework that enhance visual-textual\nunderstanding by constructing hyper-visual representations. Our approach\nintegrates a knowledge dictionary of anatomical structures with patch-level\nvisual features, enabling the model to effectively associate image regions with\ntheir corresponding anatomical entities. This structured representation\nimproves semantic reasoning, interpretability, and cross-modal alignment,\nultimately enhancing the accuracy and clinical relevance of generated reports.\nExperimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net\nsignificantly improves both visual perception and text generation quality. Our\ncode is available at \\href{https://github.com/Vinh-AI/A3Net}{GitHub}."}
{"id": "2505.07450", "pdf": "https://arxiv.org/pdf/2505.07450", "abs": "https://arxiv.org/abs/2505.07450", "authors": ["Neil De La Fuente", "Maria Pilligua", "Daniel Vidal", "Albin Soutiff", "Cecilia Curreli", "Daniel Cremers", "Andrey Barsky"], "title": "Prototype Augmented Hypernetworks for Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "CVPR (LatinX in CV)", "summary": "Continual learning (CL) aims to learn a sequence of tasks without forgetting\nprior knowledge, but gradient updates for a new task often overwrite the\nweights learned earlier, causing catastrophic forgetting (CF). We propose\nPrototype-Augmented Hypernetworks (PAH), a framework where a single\nhypernetwork, conditioned on learnable task prototypes, dynamically generates\ntask-specific classifier heads on demand. To mitigate forgetting, PAH combines\ncross-entropy with dual distillation losses, one to align logits and another to\nalign prototypes, ensuring stable feature representations across tasks.\nEvaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves\nstate-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7\n% and 4.4 % forgetting, respectively, surpassing prior methods without storing\nsamples or heads."}
{"id": "2505.07477", "pdf": "https://arxiv.org/pdf/2505.07477", "abs": "https://arxiv.org/abs/2505.07477", "authors": ["Hongkun Dou", "Zeyu Li", "Xingyu Jiang", "Hongjue Li", "Lijun Yang", "Wen Yao", "Yue Deng"], "title": "You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models (DMs) have recently demonstrated remarkable success in\nmodeling large-scale data distributions. However, many downstream tasks require\nguiding the generated content based on specific differentiable metrics,\ntypically necessitating backpropagation during the generation process. This\napproach is computationally expensive, as generating with DMs often demands\ntens to hundreds of recursive network calls, resulting in high memory usage and\nsignificant time consumption. In this paper, we propose a more efficient\nalternative that approaches the problem from the perspective of parallel\ndenoising. We show that full backpropagation throughout the entire generation\nprocess is unnecessary. The downstream metrics can be optimized by retaining\nthe computational graph of only one step during generation, thus providing a\nshortcut for gradient propagation. The resulting method, which we call Shortcut\nDiffusion Optimization (SDO), is generic, high-performance, and computationally\nlightweight, capable of optimizing all parameter types in diffusion sampling.\nWe demonstrate the effectiveness of SDO on several real-world tasks, including\ncontrolling generation by optimizing latent and aligning the DMs by fine-tuning\nnetwork parameters. Compared to full backpropagation, our approach reduces\ncomputational costs by $\\sim 90\\%$ while maintaining superior performance. Code\nis available at https://github.com/deng-ai-lab/SDO."}
{"id": "2405.15189", "pdf": "https://arxiv.org/pdf/2405.15189", "abs": "https://arxiv.org/abs/2405.15189", "authors": ["Dong Huang", "Jianbo Dai", "Han Weng", "Puzhen Wu", "Yuhao Qing", "Heming Cui", "Zhijiang Guo", "Jie M. Zhang"], "title": "EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization", "categories": ["cs.SE", "cs.CL"], "comment": "Accepted by NeurIPS 2024", "summary": "Large language models (LLMs) have shown remarkable progress in code\ngeneration, but their generated code often suffers from inefficiency, resulting\nin longer execution times and higher memory consumption. To address this issue,\nwe propose \\textbf{EffiLearner}, a self-optimization framework that utilizes\nexecution overhead profiles to improve the efficiency of LLM-generated code.\nEffiLearner first generates code using an LLM, then executes it locally to\ncapture execution time and memory usage profiles. These profiles are fed back\nto the LLM, which then revises the code to reduce overhead. To evaluate the\neffectiveness of EffiLearner, we conduct extensive experiments on the\nEffiBench, HumanEval, and MBPP with 16 open-source and 6 closed-source models.\nOur evaluation results demonstrate that through iterative self-optimization,\nEffiLearner significantly enhances the efficiency of LLM-generated code. For\nexample, the execution time (ET) of StarCoder2-15B for the EffiBench decreases\nfrom 0.93 (s) to 0.12 (s) which reduces 87.1% the execution time requirement\ncompared with the initial code. The total memory usage (TMU) of StarCoder2-15B\nalso decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8% of total\nmemory consumption during the execution process. The source code of EffiLearner\nwas released in https://github.com/huangd1999/EffiLearner"}
{"id": "2505.07690", "pdf": "https://arxiv.org/pdf/2505.07690", "abs": "https://arxiv.org/abs/2505.07690", "authors": ["Songlin Dong", "Chenhao Ding", "Jiangyang Li", "Jizhou Han", "Qiang Wang", "Yuhang He", "Yihong Gong"], "title": "Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter for Continual Learning of Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "This study aims to address the problem of multi-domain task incremental\nlearning~(MTIL), which requires that vision-language models~(VLMs) continuously\nacquire new knowledge while maintaining their inherent zero-shot recognition\ncapability. Existing paradigms delegate the testing of unseen-domain samples to\nthe original CLIP, which only prevents the degradation of the model's zero-shot\ncapability but fails to enhance the generalization of the VLM further. To this\nend, we propose a novel MTIL framework, named AFA, which comprises two core\nmodules: (1) an against forward-forgetting adapter that learns task-invariant\ninformation for each dataset in the incremental tasks to enhance the zero-shot\nrecognition ability of VLMs; (2) an against backward-forgetting adapter that\nstrengthens the few-shot learning capability of VLMs while supporting\nincremental learning. Extensive experiments demonstrate that the AFA method\nsignificantly outperforms existing state-of-the-art approaches, especially in\nfew-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP\nin terms of transferability. The code is provided in the Supplementary\nMaterial."}
{"id": "2505.07457", "pdf": "https://arxiv.org/pdf/2505.07457", "abs": "https://arxiv.org/abs/2505.07457", "authors": ["R. Maria del Rio-Chanona", "Marco Pangallo", "Cars Hommes"], "title": "Can Generative AI agents behave like humans? Evidence from laboratory market experiments", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "comment": null, "summary": "We explore the potential of Large Language Models (LLMs) to replicate human\nbehavior in economic market experiments. Compared to previous studies, we focus\non dynamic feedback between LLM agents: the decisions of each LLM impact the\nmarket price at the current step, and so affect the decisions of the other LLMs\nat the next step. We compare LLM behavior to market dynamics observed in\nlaboratory settings and assess their alignment with human participants'\nbehavior. Our findings indicate that LLMs do not adhere strictly to rational\nexpectations, displaying instead bounded rationality, similarly to human\nparticipants. Providing a minimal context window i.e. memory of three previous\ntime steps, combined with a high variability setting capturing response\nheterogeneity, allows LLMs to replicate broad trends seen in human experiments,\nsuch as the distinction between positive and negative feedback markets.\nHowever, differences remain at a granular level--LLMs exhibit less\nheterogeneity in behavior than humans. These results suggest that LLMs hold\npromise as tools for simulating realistic human behavior in economic contexts,\nthough further research is needed to refine their accuracy and increase\nbehavioral diversity."}
{"id": "2505.07503", "pdf": "https://arxiv.org/pdf/2505.07503", "abs": "https://arxiv.org/abs/2505.07503", "authors": ["Quang-Duy Tran", "Bao Duong", "Phuoc Nguyen", "Thin Nguyen"], "title": "Identifying Causal Direction via Variational Bayesian Compression", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at the 42nd International Conference on Machine Learning\n  (ICML2025)", "summary": "Telling apart the cause and effect between two random variables with purely\nobservational data is a challenging problem that finds applications in various\nscientific disciplines. A key principle utilized in this task is the\nalgorithmic Markov condition, which postulates that the joint distribution,\nwhen factorized according to the causal direction, yields a more succinct\ncodelength compared to the anti-causal direction. Previous approaches\napproximate these codelengths by relying on simple functions or Gaussian\nprocesses (GPs) with easily evaluable complexity, compromising between model\nfitness and computational complexity. To overcome these limitations, we propose\nleveraging the variational Bayesian learning of neural networks as an\ninterpretation of the codelengths. Consequently, we can enhance the model\nfitness while promoting the succinctness of the codelengths, while avoiding the\nsignificant computational complexity of the GP-based approaches. Extensive\nexperiments on both synthetic and real-world benchmarks in cause-effect\nidentification demonstrate the effectiveness of our proposed method, surpassing\nthe overall performance of related complexity-based and structural causal model\nregression-based approaches."}
{"id": "2406.06620", "pdf": "https://arxiv.org/pdf/2406.06620", "abs": "https://arxiv.org/abs/2406.06620", "authors": ["Jiexia Ye", "Weiqi Zhang", "Ziyue Li", "Jia Li", "Meng Zhao", "Fugee Tsung"], "title": "MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages, 6 figure, 3 tables", "summary": "The recent rapid advancements in language models (LMs) have garnered\nattention in medical time series-text multimodal learning. However, existing\ncontrastive learning-based and prompt-based LM approaches tend to be biased,\noften assigning a primary role to time series modality while treating text\nmodality as secondary. We classify these approaches under a temporal-primary\nparadigm, which may overlook the unique and critical task-relevant information\nembedded in text modality like clinical reports, thus failing to fully leverage\nmutual benefits and complementarity of different modalities. To fill this gap,\nwe propose a novel textual-temporal multimodal learning paradigm that enables\neither modality to serve as the primary while being enhanced by the other,\nthereby effectively capturing modality-specific information and fostering\ncross-modal interaction. In specific, we design MedualTime, a language model\ncomposed of dual adapters to implement temporal-primary and textual-primary\nmodeling simultaneously. Within each adapter, lightweight adaptation tokens are\ninjected into the top layers of LM to encourage high-level modality fusion. The\nshared LM pipeline by dual adapters not only achieves adapter alignment but\nalso enables efficient fine-tuning, reducing computational resources.\nEmpirically, MedualTime demonstrates superior performance on medical data,\nachieving notable improvements of 8% accuracy and 12% F1 in supervised\nsettings. Furthermore, MedualTime's transferability is validated by few-shot\nlabel transfer experiments from coarse-grained to fine-grained medical data.\nhttps://github.com/start2020/MedualTime"}
{"id": "2505.07691", "pdf": "https://arxiv.org/pdf/2505.07691", "abs": "https://arxiv.org/abs/2505.07691", "authors": ["Negin Ghamsarian", "Sahar Nasirihaghighi", "Klaus Schoeffmann", "Raphael Sznitman"], "title": "Feedback-Driven Pseudo-Label Reliability Assessment: Redefining Thresholding for Semi-Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": "11 pages, 5 Figures", "summary": "Semi-supervised learning leverages unlabeled data to enhance model\nperformance, addressing the limitations of fully supervised approaches. Among\nits strategies, pseudo-supervision has proven highly effective, typically\nrelying on one or multiple teacher networks to refine pseudo-labels before\ntraining a student network. A common practice in pseudo-supervision is\nfiltering pseudo-labels based on pre-defined confidence thresholds or entropy.\nHowever, selecting optimal thresholds requires large labeled datasets, which\nare often scarce in real-world semi-supervised scenarios. To overcome this\nchallenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic\nfeedback-driven thresholding strategy for pseudo-label selection. Instead of\nrelying on static confidence thresholds, ENCORE estimates class-wise\ntrue-positive confidence within the unlabeled dataset and continuously adjusts\nthresholds based on the model's response to different levels of pseudo-label\nfiltering. This feedback-driven mechanism ensures the retention of informative\npseudo-labels while filtering unreliable ones, enhancing model training without\nmanual threshold tuning. Our method seamlessly integrates into existing\npseudo-supervision frameworks and significantly improves segmentation\nperformance, particularly in data-scarce conditions. Extensive experiments\ndemonstrate that integrating ENCORE with existing pseudo-supervision frameworks\nenhances performance across multiple datasets and network architectures,\nvalidating its effectiveness in semi-supervised learning."}
{"id": "2505.07508", "pdf": "https://arxiv.org/pdf/2505.07508", "abs": "https://arxiv.org/abs/2505.07508", "authors": ["Jing Ren", "Mingliang Hou", "Zhixuan Liu", "Xiaomei Bai"], "title": "EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph anomaly detection is a popular and vital task in various real-world\nscenarios, which has been studied for several decades. Recently, many studies\nextending deep learning-based methods have shown preferable performance on\ngraph anomaly detection. However, existing methods are lack of efficiency that\nis definitely necessary for embedded devices. Towards this end, we propose an\nEfficient Anomaly detection model on heterogeneous Graphs via contrastive\nLEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms of\ntheir distances to the local context. The proposed method first samples\ninstance pairs on meta path-level for contrastive learning. Then, a graph\nautoencoder-based model is applied to learn informative node embeddings in an\nunsupervised way, which will be further combined with the discriminator to\npredict the anomaly scores of nodes. Experimental results show that EAGLE\noutperforms the state-of-the-art methods on three heterogeneous network\ndatasets."}
{"id": "2505.07525", "pdf": "https://arxiv.org/pdf/2505.07525", "abs": "https://arxiv.org/abs/2505.07525", "authors": ["Sana Ayromlou", "D. B. Emerson"], "title": "Adaptive Latent-Space Constraints in Personalized FL", "categories": ["cs.LG", "68T07", "I.2.0; I.2.11; I.2.6"], "comment": "14 Pages, 1 Algorithm, 3 Figures, 3 Tables", "summary": "Federated learning (FL) has become an effective and widely used approach to\ntraining deep learning models on decentralized datasets held by distinct\nclients. FL also strengthens both security and privacy protections for training\ndata. Common challenges associated with statistical heterogeneity between\ndistributed datasets have spurred significant interest in personalized FL (pFL)\nmethods, where models combine aspects of global learning with local modeling\nspecific to each client's unique characteristics. In this work, the efficacy of\ntheoretically supported, adaptive MMD measures within the Ditto framework, a\nstate-of-the-art technique in pFL, are investigated. The use of such measures\nsignificantly improves model performance across a variety of tasks, especially\nthose with pronounced feature heterogeneity. While the Ditto algorithm is\nspecifically considered, such measures are directly applicable to a number of\nother pFL settings, and the results motivate the use of constraints tailored to\nthe various kinds of heterogeneity expected in FL systems."}
{"id": "2408.14419", "pdf": "https://arxiv.org/pdf/2408.14419", "abs": "https://arxiv.org/abs/2408.14419", "authors": ["Shubham Bharti", "Shiyun Cheng", "Jihyun Rho", "Jianrui Zhang", "Mu Cai", "Yong Jae Lee", "Martina Rau", "Xiaojin Zhu"], "title": "CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large\nlanguage models. CHARTOM consists of specially designed data visualizing\ncharts. Given a chart, a language model needs to not only correctly comprehend\nthe chart (the FACT question) but also judge if the chart will be misleading to\na human reader (the MIND question). Both questions have significant societal\nbenefits. We detail the construction of the CHARTOM benchmark including its\ncalibration on human performance. We benchmark leading LLMs as of late 2024 -\nincluding GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset\nand found that our benchmark was challenging to all of them, suggesting room\nfor future large language models to improve."}
{"id": "2505.07715", "pdf": "https://arxiv.org/pdf/2505.07715", "abs": "https://arxiv.org/abs/2505.07715", "authors": ["Qi Xu", "Jie Deng", "Jiangrong Shen", "Biwu Chen", "Huajin Tang", "Gang Pan"], "title": "Hybrid Spiking Vision Transformer for Object Detection with Event Cameras", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Event-based object detection has gained increasing attention due to its\nadvantages such as high temporal resolution, wide dynamic range, and\nasynchronous address-event representation. Leveraging these advantages, Spiking\nNeural Networks (SNNs) have emerged as a promising approach, offering low\nenergy consumption and rich spatiotemporal dynamics. To further enhance the\nperformance of event-based object detection, this study proposes a novel hybrid\nspike vision Transformer (HsVT) model. The HsVT model integrates a spatial\nfeature extraction module to capture local and global features, and a temporal\nfeature extraction module to model time dependencies and long-term patterns in\nevent sequences. This combination enables HsVT to capture spatiotemporal\nfeatures, improving its capability to handle complex event-based object\ndetection tasks. To support research in this area, we developed and publicly\nreleased The Fall Detection Dataset as a benchmark for event-based object\ndetection tasks. This dataset, captured using an event-based camera, ensures\nfacial privacy protection and reduces memory usage due to the event\nrepresentation format. We evaluated the HsVT model on GEN1 and Fall Detection\ndatasets across various model sizes. Experimental results demonstrate that HsVT\nachieves significant performance improvements in event detection with fewer\nparameters."}
{"id": "2505.07534", "pdf": "https://arxiv.org/pdf/2505.07534", "abs": "https://arxiv.org/abs/2505.07534", "authors": ["Jürgen Bernard"], "title": "The Human-Data-Model Interaction Canvas for Visual Analytics", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "7 pages, 5 figures, LaTeX; to appear at the 16th International\n  EuroVis Workshop on Visual Analytics (EuroVA'25) as a position paper", "summary": "Visual Analytics (VA) integrates humans, data, and models as key actors in\ninsight generation and data-driven decision-making. This position paper values\nand reflects on 16 VA process models and frameworks and makes nine high-level\nobservations that motivate a fresh perspective on VA. The contribution is the\nHDMI Canvas, a perspective to VA that complements the strengths of existing VA\nprocess models and frameworks. It systematically characterizes diverse roles of\nhumans, data, and models, and how these actors benefit from and contribute to\nVA processes. The descriptive power of the HDMI Canvas eases the\ndifferentiation between a series of VA building blocks, rather than describing\ngeneral VA principles only. The canvas includes modern human-centered\nmethodologies, including human knowledge externalization and forms of feedback\nloops, while interpretable and explainable AI highlight model contributions\nbeyond their conventional outputs. The HDMI Canvas has generative power,\nguiding the design of new VA processes and is optimized for external\nstakeholders, improving VA outreach, interdisciplinary collaboration, and\nuser-centered design. The utility of the HDMI Canvas is demonstrated through\ntwo preliminary case studies."}
{"id": "2505.07527", "pdf": "https://arxiv.org/pdf/2505.07527", "abs": "https://arxiv.org/abs/2505.07527", "authors": ["Hu Wang", "Congbo Ma", "Ian Reid", "Mohammad Yaqub"], "title": "Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Reward baseline is important for Reinforcement Learning (RL) algorithms to\nreduce variance in policy gradient estimates. Recently, for language modeling,\nGroup Relative Policy Optimization (GRPO) is proposed to compute the advantage\nfor each output by subtracting the mean reward, as the baseline, for all\noutputs in the group. However, it can lead to inaccurate advantage estimates in\nenvironments with highly noisy rewards, potentially introducing bias. In this\nwork, we propose a model, called Kalman Filter Enhanced Group Relative Policy\nOptimization (KRPO), by using lightweight Kalman filtering to dynamically\nestimate the latent reward mean and variance. This filtering technique replaces\nthe naive batch mean baseline, enabling more adaptive advantage normalization.\nOur method does not require additional learned parameters over GRPO. This\napproach offers a simple yet effective way to incorporate multiple outputs of\nGRPO into advantage estimation, improving policy optimization in settings where\nhighly dynamic reward signals are difficult to model for language models.\nThrough experiments and analyses, we show that using a more adaptive advantage\nestimation model, KRPO can improve the stability and performance of GRPO. The\ncode is available at https://github.com/billhhh/KRPO_LLMs_RL"}
{"id": "2410.09982", "pdf": "https://arxiv.org/pdf/2410.09982", "abs": "https://arxiv.org/abs/2410.09982", "authors": ["Vithursan Thangarasa", "Ganesh Venkatesh", "Mike Lasby", "Nish Sinnadurai", "Sean Lie"], "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to MLSys 2025. Main paper: 14 pp., 4 figs., 6 tabs.;\n  Supplementary: 5 pp", "summary": "Large language models have driven significant progress in natural language\nprocessing, but their deployment requires substantial compute and memory\nresources. As models scale, compression techniques become essential for\nbalancing model quality with computational efficiency. Structured pruning,\nwhich removes less critical components of the model, is a promising strategy\nfor reducing complexity. However, one-shot pruning often results in significant\nquality degradation, particularly in tasks requiring multi-step reasoning. To\nrecover lost quality, supervised fine-tuning (SFT) is commonly applied, but it\ncan lead to catastrophic forgetting by shifting the model's learned data\ndistribution. Therefore, addressing the degradation from both pruning and SFT\nis essential to preserve the original model's quality. In this work, we utilize\nself-data distilled fine-tuning to address these challenges. Our approach\nleverages the original, unpruned model to generate a distilled dataset that\npreserves semantic richness and mitigates catastrophic forgetting by\nmaintaining alignment with the base model's knowledge. Empirically, we\ndemonstrate that self-data distillation consistently outperforms standard SFT,\nimproving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard\nv1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct\n(i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B\nparameters), our method retains 91.2% of the original model's accuracy compared\nto 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore,\ncombining self-data distilled models through model merging yields enhanced\nquality retention. Additionally, leveraging these pruned models in speculative\ndecoding increases token acceptance rates, thereby improving inference\nefficiency in applied settings."}
{"id": "2505.07721", "pdf": "https://arxiv.org/pdf/2505.07721", "abs": "https://arxiv.org/abs/2505.07721", "authors": ["Vignesh Edithal", "Le Zhang", "Ilia Blank", "Imran Junejo"], "title": "Gameplay Highlights Generation", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we enable gamers to share their gaming experience on social\nmedia by automatically generating eye-catching highlight reels from their\ngameplay session Our automation will save time for gamers while increasing\naudience engagement. We approach the highlight generation problem by first\nidentifying intervals in the video where interesting events occur and then\nconcatenate them. We developed an in-house gameplay event detection dataset\ncontaining interesting events annotated by humans using VIA video annotator.\nTraditional techniques for highlight detection such as game engine integration\nrequires expensive collaboration with game developers. OCR techniques which\ndetect patches of specific images or texts require expensive per game\nengineering and may not generalize across game UI and different language. We\nfinetuned a multimodal general purpose video understanding model such as X-CLIP\nusing our dataset which generalizes across multiple games in a genre without\nper game engineering. Prompt engineering was performed to improve the\nclassification performance of this multimodal model. Our evaluation showed that\nsuch a finetuned model can detect interesting events in first person shooting\ngames from unseen gameplay footage with more than 90% accuracy. Moreover, our\nmodel performed significantly better on low resource games (small dataset) when\ntrained along with high resource games, showing signs of transfer learning. To\nmake the model production ready, we used ONNX libraries to enable cross\nplatform inference. These libraries also provide post training quantization\ntools to reduce model size and inference time for deployment. ONNX runtime\nlibraries with DirectML backend were used to perform efficient inference on\nWindows OS. We show that natural language supervision in the X-CLIP model leads\nto data efficient and highly performant video recognition models."}
{"id": "2505.07546", "pdf": "https://arxiv.org/pdf/2505.07546", "abs": "https://arxiv.org/abs/2505.07546", "authors": ["Jingjie Zheng", "Aryo Pradipta Gema", "Giwon Hong", "Xuanli He", "Pasquale Minervini", "Youcheng Sun", "Qiongkai Xu"], "title": "GRADA: Graph-based Reranker against Adversarial Documents Attack", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large\nlanguage models (LLMs) by integrating external knowledge from retrieved\ndocuments, thereby overcoming the limitations of models' static intrinsic\nknowledge. However, these systems are susceptible to adversarial attacks that\nmanipulate the retrieval process by introducing documents that are adversarial\nyet semantically similar to the query. Notably, while these adversarial\ndocuments resemble the query, they exhibit weak similarity to benign documents\nin the retrieval set. Thus, we propose a simple yet effective Graph-based\nReranking against Adversarial Document Attacks (GRADA) framework aiming at\npreserving retrieval quality while significantly reducing the success of\nadversaries. Our study evaluates the effectiveness of our approach through\nexperiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,\nLlama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with\nresults from the Natural Questions dataset demonstrating up to an 80% reduction\nin attack success rates while maintaining minimal loss in accuracy."}
{"id": "2505.07548", "pdf": "https://arxiv.org/pdf/2505.07548", "abs": "https://arxiv.org/abs/2505.07548", "authors": ["Lingkun Luo", "Shiqiang Hu", "Liming Chen"], "title": "Noise Optimized Conditional Diffusion for Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "9 pages, 4 figures This work has been accepted by the International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet\nthe scarcity of High-Confidence Pseudo-Labeled Target Domain Samples\n(\\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical\nalignment, causing DA failures. To address this challenge, we propose\n\\textbf{N}oise \\textbf{O}ptimized \\textbf{C}onditional \\textbf{D}iffusion for\n\\textbf{D}omain \\textbf{A}daptation (\\textbf{NOCDDA}), which seamlessly\nintegrates the generative capabilities of conditional diffusion models with the\ndecision-making requirements of DA to achieve task-coupled optimization for\nefficient adaptation. For robust cross-domain consistency, we modify the DA\nclassifier to align with the conditional diffusion classifier within a unified\noptimization framework, enabling forward training on noise-varying cross-domain\nsamples. Furthermore, we argue that the conventional \\( \\mathcal{N}(\\mathbf{0},\n\\mathbf{I}) \\) initialization in diffusion models often generates\nclass-confused hcpl-tds, compromising discriminative DA. To resolve this, we\nintroduce a class-aware noise optimization strategy that refines sampling\nregions for reverse class-specific hcpl-tds generation, effectively enhancing\ncross-domain alignment. Extensive experiments across 5 benchmark datasets and\n29 DA tasks demonstrate significant performance gains of \\textbf{NOCDDA} over\n31 state-of-the-art methods, validating its robustness and effectiveness."}
{"id": "2410.13439", "pdf": "https://arxiv.org/pdf/2410.13439", "abs": "https://arxiv.org/abs/2410.13439", "authors": ["Guangming Huang", "Yunfei Long", "Cunjin Luo"], "title": "Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Supervised contrastive learning has achieved remarkable success by leveraging\nlabel information; however, determining positive samples in multi-label\nscenarios remains a critical challenge. In multi-label supervised contrastive\nlearning (MSCL), multi-label relations are not yet fully defined, leading to\nambiguity in identifying positive samples and formulating contrastive loss\nfunctions to construct the representation space. To address these challenges,\nwe: (i) first define five distinct multi-label relations in MSCL to\nsystematically identify positive samples, (ii) introduce a novel\nSimilarity-Dissimilarity Loss that dynamically re-weights samples through\ncomputing the similarity and dissimilarity factors between positive samples and\ngiven anchors based on multi-label relations, and (iii) further provide\ntheoretical grounded proofs for our method through rigorous mathematical\nanalysis that supports the formulation and effectiveness of the proposed loss\nfunction. We conduct the experiments across both image and text modalities, and\nextend the evaluation to medical domain. The results demonstrate that our\nmethod consistently outperforms baselines in a comprehensive evaluation,\nconfirming its effectiveness and robustness. Code is available at:\nhttps://github.com/guangminghuang/similarity-dissimilarity-loss."}
{"id": "2505.07734", "pdf": "https://arxiv.org/pdf/2505.07734", "abs": "https://arxiv.org/abs/2505.07734", "authors": ["Jiangling Zhang", "Weijie Zhu", "Jirui Huang", "Yaxiong Chen"], "title": "LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention", "categories": ["cs.CV"], "comment": null, "summary": "Detecting AI-synthetic faces presents a critical challenge: it is hard to\ncapture consistent structural relationships between facial regions across\ndiverse generation techniques. Current methods, which focus on specific\nartifacts rather than fundamental inconsistencies, often fail when confronted\nwith novel generative models. To address this limitation, we introduce\nLayer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer\ndesigned for robust facial forgery detection. This model integrates distinct\nRegion-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation\n(LAMM) components within each layer. RG-MHA utilizes facial landmarks to create\nregional attention masks, guiding the model to scrutinize architectural\ninconsistencies across different facial areas. Crucially, the separate LAMM\nmodule dynamically generates layer-specific parameters, including mask weights\nand gating values, based on network context. These parameters then modulate the\nbehavior of RG-MHA, enabling adaptive adjustment of regional focus across\nnetwork depths. This architecture facilitates the capture of subtle,\nhierarchical forgery cues ubiquitous among diverse generation techniques, such\nas GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT\ndemonstrates superior performance, achieving 94.09% mean ACC (a +5.45%\nimprovement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results\ndemonstrate LAMM-ViT's exceptional ability to generalize and its potential for\nreliable deployment against evolving synthetic media threats."}
{"id": "2505.07553", "pdf": "https://arxiv.org/pdf/2505.07553", "abs": "https://arxiv.org/abs/2505.07553", "authors": ["Tor Sporsem", "Rasmus Ulfsnes"], "title": "Towards Requirements Engineering for RAG Systems", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted to EASE 2025, 17-20 June, Istanbul, Turkey", "summary": "This short paper explores how a maritime company develops and integrates\nlarge-language models (LLM). Specifically by looking at the requirements\nengineering for Retrieval Augmented Generation (RAG) systems in expert\nsettings. Through a case study at a maritime service provider, we demonstrate\nhow data scientists face a fundamental tension between user expectations of AI\nperfection and the correctness of the generated outputs. Our findings reveal\nthat data scientists must identify context-specific \"retrieval requirements\"\nthrough iterative experimentation together with users because they are the ones\nwho can determine correctness. We present an empirical process model describing\nhow data scientists practically elicited these \"retrieval requirements\" and\nmanaged system limitations. This work advances software engineering knowledge\nby providing insights into the specialized requirements engineering processes\nfor implementing RAG systems in complex domain-specific applications."}
{"id": "2505.07554", "pdf": "https://arxiv.org/pdf/2505.07554", "abs": "https://arxiv.org/abs/2505.07554", "authors": ["Erica Coppolillo"], "title": "Injecting Knowledge Graphs into Large Language Models", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) remains a key challenge for symbolic reasoning. Existing\nmethods mainly rely on prompt engineering or fine-tuning, which lose structural\nfidelity or incur high computational costs. Building on recent encoding\ntechniques which integrate graph embeddings within the LLM input as tokens, we\nextend this paradigm to the KG domain by leveraging Knowledge Graph Embedding\n(KGE) models, thus enabling graph-aware reasoning. Our approach is\nmodel-agnostic, resource-efficient, and compatible with any LLMs. Extensive\nexperimentation on synthetic and real-world datasets shows that our method\nimproves reasoning performance over established baselines, further achieving\nthe best trade-off in terms of accuracy and efficiency against state-of-the-art\nLLMs."}
{"id": "2501.09798", "pdf": "https://arxiv.org/pdf/2501.09798", "abs": "https://arxiv.org/abs/2501.09798", "authors": ["Andrey Labunets", "Nishit V. Pandya", "Ashish Hooda", "Xiaohan Fu", "Earlence Fernandes"], "title": "Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-based Prompt Injection Attacks via the Fine-Tuning Interface", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "We surface a new threat to closed-weight Large Language Models (LLMs) that\nenables an attacker to compute optimization-based prompt injections.\nSpecifically, we characterize how an attacker can leverage the loss-like\ninformation returned from the remote fine-tuning interface to guide the search\nfor adversarial prompts. The fine-tuning interface is hosted by an LLM vendor\nand allows developers to fine-tune LLMs for their tasks, thus providing\nutility, but also exposes enough information for an attacker to compute\nadversarial prompts. Through an experimental analysis, we characterize the\nloss-like values returned by the Gemini fine-tuning API and demonstrate that\nthey provide a useful signal for discrete optimization of adversarial prompts\nusing a greedy search algorithm. Using the PurpleLlama prompt injection\nbenchmark, we demonstrate attack success rates between 65% and 82% on Google's\nGemini family of LLMs. These attacks exploit the classic utility-security\ntradeoff - the fine-tuning interface provides a useful feature for developers\nbut also exposes the LLMs to powerful attacks."}
{"id": "2505.07744", "pdf": "https://arxiv.org/pdf/2505.07744", "abs": "https://arxiv.org/abs/2505.07744", "authors": ["Halid Ziya Yerebakan", "Kritika Iyer", "Xueqi Guo", "Yoshihisa Shinagawa", "Gerardo Hermosillo Valadez"], "title": "BodyGPS: Anatomical Positioning System", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce a new type of foundational model for parsing human anatomy in\nmedical images that works for different modalities. It supports supervised or\nunsupervised training and can perform matching, registration, classification,\nor segmentation with or without user interaction. We achieve this by training a\nneural network estimator that maps query locations to atlas coordinates via\nregression. Efficiency is improved by sparsely sampling the input, enabling\nresponse times of less than 1 ms without additional accelerator hardware. We\ndemonstrate the utility of the algorithm in both CT and MRI modalities."}
{"id": "2505.07575", "pdf": "https://arxiv.org/pdf/2505.07575", "abs": "https://arxiv.org/abs/2505.07575", "authors": ["Samuel Erickson", "Mikael Johansson"], "title": "Personalized Federated Learning under Model Dissimilarity Constraints", "categories": ["cs.LG"], "comment": null, "summary": "One of the defining challenges in federated learning is that of statistical\nheterogeneity among clients. We address this problem with KARULA, a regularized\nstrategy for personalized federated learning, which constrains the pairwise\nmodel dissimilarities between clients based on the difference in their\ndistributions, as measured by a surrogate for the 1-Wasserstein distance\nadapted for the federated setting. This allows the strategy to adapt to highly\ncomplex interrelations between clients, that e.g., clustered approaches fail to\ncapture. We propose an inexact projected stochastic gradient algorithm to solve\nthe constrained problem that the strategy defines, and show theoretically that\nit converges with smooth, possibly non-convex losses to a neighborhood of a\nstationary point with rate O(1/K). We demonstrate the effectiveness of KARULA\non synthetic and real federated data sets."}
{"id": "2502.01891", "pdf": "https://arxiv.org/pdf/2502.01891", "abs": "https://arxiv.org/abs/2502.01891", "authors": ["Kemal Kurniawan", "Meladel Mistica", "Timothy Baldwin", "Jey Han Lau"], "title": "Training and Evaluating with Human Label Variation: An Empirical Study", "categories": ["cs.LG", "cs.CL"], "comment": "25 pages, 7 figures. Fixed PO-JSD values on the MFRC dataset", "summary": "Human label variation (HLV) challenges the standard assumption that a\nlabelled instance has a single ground truth, instead embracing the natural\nvariation in human annotation to train and evaluate models. While various\ntraining methods and metrics for HLV have been proposed, it is still unclear\nwhich methods and metrics perform best in what settings. We propose new\nevaluation metrics for HLV leveraging fuzzy set theory. Since these new\nproposed metrics are differentiable, we then in turn experiment with employing\nthese metrics as training objectives. We conduct an extensive study over 6 HLV\ndatasets testing 14 training methods and 6 evaluation metrics. We find that\ntraining on either disaggregated annotations or soft labels performs best\nacross metrics, outperforming training using the proposed training objectives\nwith differentiable metrics. We also show that our proposed soft metric is more\ninterpretable and correlates best with human preference."}
{"id": "2505.07747", "pdf": "https://arxiv.org/pdf/2505.07747", "abs": "https://arxiv.org/abs/2505.07747", "authors": ["Weiyu Li", "Xuanyang Zhang", "Zheng Sun", "Di Qi", "Hao Li", "Wei Cheng", "Weiwei Cai", "Shihao Wu", "Jiarui Liu", "Zihao Wang", "Xiao Chen", "Feipeng Tian", "Jianxiong Pan", "Zeming Li", "Gang Yu", "Xiangyu Zhang", "Daxin Jiang", "Ping Tan"], "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets", "categories": ["cs.CV"], "comment": "Technical report", "summary": "While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation."}
{"id": "2505.07621", "pdf": "https://arxiv.org/pdf/2505.07621", "abs": "https://arxiv.org/abs/2505.07621", "authors": ["Leonardo Kuffo", "Peter Boncz"], "title": "Bang for the Buck: Vector Search on Cloud CPUs", "categories": ["cs.DB", "cs.AI"], "comment": "To be published in Proceedings of 21st International Workshop on Data\n  Management on New Hardware (DaMoN '25)", "summary": "Vector databases have emerged as a new type of systems that support efficient\nquerying of high-dimensional vectors. Many of these offer their database as a\nservice in the cloud. However, the variety of available CPUs and the lack of\nvector search benchmarks across CPUs make it difficult for users to choose one.\nIn this study, we show that CPU microarchitectures available in the cloud\nperform significantly differently across vector search scenarios. For instance,\nin an IVF index on float32 vectors, AMD's Zen4 gives almost 3x more queries per\nsecond (QPS) compared to Intel's Sapphire Rapids, but for HNSW indexes, the\ntables turn. However, when looking at the number of queries per dollar (QP$),\nGraviton3 is the best option for most indexes and quantization settings, even\nover Graviton4 (Table 1). With this work, we hope to guide users in getting the\nbest \"bang for the buck\" when deploying vector search systems."}
{"id": "2505.07614", "pdf": "https://arxiv.org/pdf/2505.07614", "abs": "https://arxiv.org/abs/2505.07614", "authors": ["Gleb Molodtsov", "Daniil Medyakov", "Sergey Skorik", "Nikolas Khachaturov", "Shahane Tigranyan", "Vladimir Aletov", "Aram Avetisyan", "Martin Takáč", "Aleksandr Beznosikov"], "title": "Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Recent advancements in machine learning have improved performance while also\nincreasing computational demands. While federated and distributed setups\naddress these issues, their structure is vulnerable to malicious influences. In\nthis paper, we address a specific threat, Byzantine attacks, where compromised\nclients inject adversarial updates to derail global convergence. We combine the\ntrust scores concept with trial function methodology to dynamically filter\noutliers. Our methods address the critical limitations of previous approaches,\nallowing functionality even when Byzantine nodes are in the majority. Moreover,\nour algorithms adapt to widely used scaled methods like Adam and RMSProp, as\nwell as practical scenarios, including local training and partial\nparticipation. We validate the robustness of our methods by conducting\nextensive experiments on both synthetic and real ECG data collected from\nmedical institutions. Furthermore, we provide a broad theoretical analysis of\nour algorithms and their extensions to aforementioned practical setups. The\nconvergence guarantees of our methods are comparable to those of classical\nalgorithms developed without Byzantine interference."}
{"id": "2502.14581", "pdf": "https://arxiv.org/pdf/2502.14581", "abs": "https://arxiv.org/abs/2502.14581", "authors": ["Julian Rodemann", "Esteban Garces Arias", "Christoph Luther", "Christoph Jansen", "Thomas Augustin"], "title": "A Statistical Case Against Empirical Human-AI Alignment", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.OT"], "comment": "24 pages, 2 figures, 5 tables", "summary": "Empirical human-AI alignment aims to make AI systems act in line with\nobserved human behavior. While noble in its goals, we argue that empirical\nalignment can inadvertently introduce statistical biases that warrant caution.\nThis position paper thus advocates against naive empirical alignment, offering\nprescriptive alignment and a posteriori empirical alignment as alternatives. We\nsubstantiate our principled argument by tangible examples like human-centric\ndecoding of language models."}
{"id": "2505.07812", "pdf": "https://arxiv.org/pdf/2505.07812", "abs": "https://arxiv.org/abs/2505.07812", "authors": ["Chenze Shao", "Fandong Meng", "Jie Zhou"], "title": "Continuous Visual Autoregressive Generation via Score Maximization", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Conventional wisdom suggests that autoregressive models are used to process\ndiscrete data. When applied to continuous modalities such as visual data,\nVisual AutoRegressive modeling (VAR) typically resorts to quantization-based\napproaches to cast the data into a discrete space, which can introduce\nsignificant information loss. To tackle this issue, we introduce a Continuous\nVAR framework that enables direct visual autoregressive generation without\nvector quantization. The underlying theoretical foundation is strictly proper\nscoring rules, which provide powerful statistical tools capable of evaluating\nhow well a generative model approximates the true distribution. Within this\nframework, all we need is to select a strictly proper score and set it as the\ntraining objective to optimize. We primarily explore a class of training\nobjectives based on the energy score, which is likelihood-free and thus\novercomes the difficulty of making probabilistic predictions in the continuous\nspace. Previous efforts on continuous autoregressive generation, such as GIVT\nand diffusion loss, can also be derived from our framework using other strictly\nproper scores. Source code: https://github.com/shaochenze/EAR."}
{"id": "2505.07634", "pdf": "https://arxiv.org/pdf/2505.07634", "abs": "https://arxiv.org/abs/2505.07634", "authors": ["Jian Liu", "Xiongtao Shi", "Thai Duy Nguyen", "Haitian Zhang", "Tianxiang Zhang", "Wei Sun", "Yanjie Li", "Athanasios V. Vasilakos", "Giovanni Iacca", "Arshad Ali Khan", "Arvind Kumar", "Jae Won Cho", "Ajmal Mian", "Lihua Xie", "Erik Cambria", "Lin Wang"], "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "51 pages, 17 figures, 9 tables", "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios."}
{"id": "2505.07629", "pdf": "https://arxiv.org/pdf/2505.07629", "abs": "https://arxiv.org/abs/2505.07629", "authors": ["Yizhou Ma", "Zhuoqin Yang", "Luis-Daniel Ibáñez"], "title": "Enhancing Federated Learning with Kolmogorov-Arnold Networks: A Comparative Study Across Diverse Aggregation Strategies", "categories": ["cs.LG"], "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. It was prepared prior to submission to, and has\n  since been accepted at, ICIC 2025. The final Version of Record will be\n  published in the ICIC 2025 proceedings by Springer", "summary": "Multilayer Perceptron (MLP), as a simple yet powerful model, continues to be\nwidely used in classification and regression tasks. However, traditional MLPs\noften struggle to efficiently capture nonlinear relationships in load data when\ndealing with complex datasets. Kolmogorov-Arnold Networks (KAN), inspired by\nthe Kolmogorov-Arnold representation theorem, have shown promising capabilities\nin modeling complex nonlinear relationships. In this study, we explore the\nperformance of KANs within federated learning (FL) frameworks and compare them\nto traditional Multilayer Perceptrons. Our experiments, conducted across four\ndiverse datasets demonstrate that KANs consistently outperform MLPs in terms of\naccuracy, stability, and convergence efficiency. KANs exhibit remarkable\nrobustness under varying client numbers and non-IID data distributions,\nmaintaining superior performance even as client heterogeneity increases.\nNotably, KANs require fewer communication rounds to converge compared to MLPs,\nhighlighting their efficiency in FL scenarios. Additionally, we evaluate\nmultiple parameter aggregation strategies, with trimmed mean and FedProx\nemerging as the most effective for optimizing KAN performance. These findings\nestablish KANs as a robust and scalable alternative to MLPs for federated\nlearning tasks, paving the way for their application in decentralized and\nprivacy-preserving environments."}
{"id": "2502.14908", "pdf": "https://arxiv.org/pdf/2502.14908", "abs": "https://arxiv.org/abs/2502.14908", "authors": ["Peter Carragher", "Nikitha Rao", "Abhinand Jha", "R Raghav", "Kathleen M. Carley"], "title": "SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision language models (VLM) demonstrate sophisticated multimodal reasoning\nyet are prone to hallucination when confronted with knowledge conflicts,\nimpeding their deployment in information-sensitive contexts. While existing\nresearch addresses robustness in unimodal models, the multimodal domain lacks\nsystematic investigation of cross-modal knowledge conflicts. This research\nintroduces \\segsub, a framework for applying targeted image perturbations to\ninvestigate VLM resilience against knowledge conflicts. Our analysis reveals\ndistinct vulnerability patterns: while VLMs are robust to parametric conflicts\n(20% adherence rates), they exhibit significant weaknesses in identifying\ncounterfactual conditions (<30% accuracy) and resolving source conflicts (<1%\naccuracy). Correlations between contextual richness and hallucination rate (r =\n-0.368, p = 0.003) reveal the kinds of images that are likely to cause\nhallucinations. Through targeted fine-tuning on our benchmark dataset, we\ndemonstrate improvements in VLM knowledge conflict detection, establishing a\nfoundation for developing hallucination-resilient multimodal systems in\ninformation-sensitive environments."}
{"id": "2505.07818", "pdf": "https://arxiv.org/pdf/2505.07818", "abs": "https://arxiv.org/abs/2505.07818", "authors": ["Zeyue Xue", "Jie Wu", "Yu Gao", "Fangyuan Kong", "Lingting Zhu", "Mengzhao Chen", "Zhiheng Liu", "Wei Liu", "Qiushan Guo", "Weilin Huang", "Ping Luo"], "title": "DanceGRPO: Unleashing GRPO on Visual Generation", "categories": ["cs.CV"], "comment": "Project Page: https://dancegrpo.github.io/", "summary": "Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased."}
{"id": "2505.07664", "pdf": "https://arxiv.org/pdf/2505.07664", "abs": "https://arxiv.org/abs/2505.07664", "authors": ["Werner Geyer", "Jessica He", "Daita Sarkar", "Michelle Brachman", "Chris Hammond", "Jennifer Heins", "Zahra Ashktorab", "Carlos Rosemberg", "Charlie Hill"], "title": "A Case Study Investigating the Role of Generative AI in Quality Evaluations of Epics in Agile Software Development", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "The broad availability of generative AI offers new opportunities to support\nvarious work domains, including agile software development. Agile epics are a\nkey artifact for product managers to communicate requirements to stakeholders.\nHowever, in practice, they are often poorly defined, leading to churn, delivery\ndelays, and cost overruns. In this industry case study, we investigate\nopportunities for large language models (LLMs) to evaluate agile epic quality\nin a global company. Results from a user study with 17 product managers\nindicate how LLM evaluations could be integrated into their work practices,\nincluding perceived values and usage in improving their epics. High levels of\nsatisfaction indicate that agile epics are a new, viable application of AI\nevaluations. However, our findings also outline challenges, limitations, and\nadoption barriers that can inform both practitioners and researchers on the\nintegration of such evaluations into future agile work practices."}
{"id": "2505.07635", "pdf": "https://arxiv.org/pdf/2505.07635", "abs": "https://arxiv.org/abs/2505.07635", "authors": ["Dazhuo Qiu", "Haolai Che", "Arijit Khan", "Yinghui Wu"], "title": "Generating Skyline Explanations for Graph Neural Networks", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "This paper proposes a novel approach to generate subgraph explanations for\ngraph neural networks GNNs that simultaneously optimize multiple measures for\nexplainability. Existing GNN explanation methods often compute subgraphs\n(called ``explanatory subgraphs'') that optimize a pre-defined, single\nexplainability measure, such as fidelity or conciseness. This can lead to\nbiased explanations that cannot provide a comprehensive explanation to clarify\nthe output of GNN models. We introduce skyline explanation, a GNN explanation\nparadigm that aims to identify k explanatory subgraphs by simultaneously\noptimizing multiple explainability measures. (1) We formulate skyline\nexplanation generation as a multi-objective optimization problem, and pursue\nexplanations that approximate a skyline set of explanatory subgraphs. We show\nthe hardness for skyline explanation generation. (2) We design efficient\nalgorithms with an onion-peeling approach that strategically removes edges from\nneighbors of nodes of interests, and incrementally improves explanations as it\nexplores an interpretation domain, with provable quality guarantees. (3) We\nfurther develop an algorithm to diversify explanations to provide more\ncomprehensive perspectives. Using real-world graphs, we empirically verify the\neffectiveness, efficiency, and scalability of our algorithms."}
{"id": "2503.08980", "pdf": "https://arxiv.org/pdf/2503.08980", "abs": "https://arxiv.org/abs/2503.08980", "authors": ["Yuhang Liu", "Dong Gong", "Yichao Cai", "Erdun Gao", "Zhen Zhang", "Biwei Huang", "Mingming Gong", "Anton van den Hengel", "Javen Qinfeng Shi"], "title": "I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human-interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result, i.e., the representations learned by LLMs through\nnext-token prediction can be approximately modeled as the logarithm of the\nposterior probabilities of these latent discrete concepts given input context,\nup to an invertible linear transformation. This theoretical finding not only\nprovides evidence that LLMs capture underlying generative factors, but also\nprovide a unified prospective for understanding of the linear representation\nhypothesis. Taking this a step further, our finding motivates a reliable\nevaluation of sparse autoencoders by treating the performance of supervised\nconcept extractors as an upper bound. Pushing this idea even further, it\ninspires a structural variant that enforces dependence among latent concepts in\naddition to promoting sparsity. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families, and demonstrate the effectiveness of our structured\nsparse autoencoder."}
{"id": "2505.06285", "pdf": "https://arxiv.org/pdf/2505.06285", "abs": "https://arxiv.org/abs/2505.06285", "authors": ["Yuhan Yuan", "Xiaomo Jiang", "Yanfeng Han", "Ke Xiao"], "title": "FEMSN: Frequency-Enhanced Multiscale Network for fault diagnosis of rotating machinery under strong noise environments", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Rolling bearings are critical components of rotating machinery, and their\nproper functioning is essential for industrial production. Most existing\ncondition monitoring methods focus on extracting discriminative features from\ntime-domain signals to assess bearing health status. However, under complex\noperating conditions, periodic impulsive characteristics related to fault\ninformation are often obscured by noise interference. Consequently, existing\napproaches struggle to learn distinctive fault-related features in such\nscenarios. To address this issue, this paper proposes a novel CNN-based model\nnamed FEMSN. Specifically, a Fourier Adaptive Denoising Encoder Layer (FADEL)\nis introduced as an input denoising layer to enhance key features while\nfiltering out irrelevant information. Subsequently, a Multiscale Time-Frequency\nFusion (MSTFF) module is employed to extract fused time-frequency features,\nfurther improving the model robustness and nonlinear representation capability.\nAdditionally, a distillation layer is incorporated to expand the receptive\nfield. Based on these advancements, a novel deep lightweight CNN model, termed\nthe Frequency-Enhanced Multiscale Network (FEMSN), is developed. The\neffectiveness of FEMSN and FADEL in machine health monitoring and stability\nassessment is validated through two case studies."}
{"id": "2505.07675", "pdf": "https://arxiv.org/pdf/2505.07675", "abs": "https://arxiv.org/abs/2505.07675", "authors": ["Seongjae Kang", "Dong Bok Lee", "Hyungjoon Jang", "Sung Ju Hwang"], "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "41 pages, 19 figures, preprint", "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\n$\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead\n$\\mathbf{\\texttt{O}}$ptimization ($\\mathbf{\\texttt{DHO}}$) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that $\\texttt{DHO}$ mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that $\\texttt{DHO}$\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters."}
{"id": "2505.07674", "pdf": "https://arxiv.org/pdf/2505.07674", "abs": "https://arxiv.org/abs/2505.07674", "authors": ["Nan Jiang", "Wenxuan Zhu", "Xu Han", "Weiqiang Huang", "Yumeng Sun"], "title": "Joint Graph Convolution and Sequential Modeling for Scalable Network Traffic Estimation", "categories": ["cs.LG"], "comment": null, "summary": "This study focuses on the challenge of predicting network traffic within\ncomplex topological environments. It introduces a spatiotemporal modeling\napproach that integrates Graph Convolutional Networks (GCN) with Gated\nRecurrent Units (GRU). The GCN component captures spatial dependencies among\nnetwork nodes, while the GRU component models the temporal evolution of traffic\ndata. This combination allows for precise forecasting of future traffic\npatterns. The effectiveness of the proposed model is validated through\ncomprehensive experiments on the real-world Abilene network traffic dataset.\nThe model is benchmarked against several popular deep learning methods.\nFurthermore, a set of ablation experiments is conducted to examine the\ninfluence of various components on performance, including changes in the number\nof graph convolution layers, different temporal modeling strategies, and\nmethods for constructing the adjacency matrix. Results indicate that the\nproposed approach achieves superior performance across multiple metrics,\ndemonstrating robust stability and strong generalization capabilities in\ncomplex network traffic forecasting scenarios."}
{"id": "2504.01382", "pdf": "https://arxiv.org/pdf/2504.01382", "abs": "https://arxiv.org/abs/2504.01382", "authors": ["Tianci Xue", "Weijian Qi", "Tianneng Shi", "Chan Hee Song", "Boyu Gou", "Dawn Song", "Huan Sun", "Yu Su"], "title": "An Illusion of Progress? Assessing the Current State of Web Agents", "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 17 figures, 7 tables", "summary": "As digitalization and cloud technologies evolve, the web is becoming\nincreasingly important in the modern society. Autonomous web agents based on\nlarge language models (LLMs) hold a great potential in work automation. It is\ntherefore important to accurately measure and monitor the progression of their\ncapabilities. In this work, we conduct a comprehensive and rigorous assessment\nof the current state of web agents. Our results depict a very different picture\nof the competency of current agents, suggesting over-optimism in previously\nreported results. This gap can be attributed to shortcomings in existing\nbenchmarks. We introduce Online-Mind2Web, an online evaluation benchmark\nconsisting of 300 diverse and realistic tasks spanning 136 websites. It enables\nus to evaluate web agents under a setting that approximates how real users use\nthese agents. To facilitate more scalable evaluation and development, we also\ndevelop a novel LLM-as-a-Judge automatic evaluation method and show that it can\nachieve around 85% agreement with human judgment, substantially higher than\nexisting methods. Finally, we present the first comprehensive comparative\nanalysis of current web agents, highlighting both their strengths and\nlimitations to inspire future research."}
{"id": "2505.06483", "pdf": "https://arxiv.org/pdf/2505.06483", "abs": "https://arxiv.org/abs/2505.06483", "authors": ["Shehryar Khattak", "Timon Homberger", "Lukas Bernreiter", "Julian Nubert", "Olov Andersson", "Roland Siegwart", "Kostas Alexis", "Marco Hutter"], "title": "CompSLAM: Complementary Hierarchical Multi-Modal Localization and Mapping for Robot Autonomy in Underground Environments", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 9 figures, Code:\n  https://github.com/leggedrobotics/compslam_subt", "summary": "Robot autonomy in unknown, GPS-denied, and complex underground environments\nrequires real-time, robust, and accurate onboard pose estimation and mapping\nfor reliable operations. This becomes particularly challenging in\nperception-degraded subterranean conditions under harsh environmental factors,\nincluding darkness, dust, and geometrically self-similar structures. This paper\ndetails CompSLAM, a highly resilient and hierarchical multi-modal localization\nand mapping framework designed to address these challenges. Its flexible\narchitecture achieves resilience through redundancy by leveraging the\ncomplementary nature of pose estimates derived from diverse sensor modalities.\nDeveloped during the DARPA Subterranean Challenge, CompSLAM was successfully\ndeployed on all aerial, legged, and wheeled robots of Team Cerberus during\ntheir competition-winning final run. Furthermore, it has proven to be a\nreliable odometry and mapping solution in various subsequent projects, with\nextensions enabling multi-robot map sharing for marsupial robotic deployments\nand collaborative mapping. This paper also introduces a comprehensive dataset\nacquired by a manually teleoperated quadrupedal robot, covering a significant\nportion of the DARPA Subterranean Challenge finals course. This dataset\nevaluates CompSLAM's robustness to sensor degradations as the robot traverses\n740 meters in an environment characterized by highly variable geometries and\ndemanding lighting conditions. The CompSLAM code and the DARPA SubT Finals\ndataset are made publicly available for the benefit of the robotics community"}
{"id": "2505.07683", "pdf": "https://arxiv.org/pdf/2505.07683", "abs": "https://arxiv.org/abs/2505.07683", "authors": ["Steven Song", "Morgan Borjigin-Wang", "Irene Madejski", "Robert L. Grossman"], "title": "Multimodal Survival Modeling in the Age of Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 7 figures, 8 tables", "summary": "The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a\nlarge-scale reference through its harmonized genomics, clinical, and image\ndata. Prior studies have trained bespoke cancer survival prediction models from\nunimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning\nis the development of foundation models (FMs) to derive meaningful feature\nembeddings, agnostic to a specific modeling task. Biomedical text especially\nhas seen growing development of FMs. While TCGA contains free-text data as\npathology reports, these have been historically underutilized. Here, we\ninvestigate the feasibility of training classical, multimodal survival models\nover zero-shot embeddings extracted by FMs. We show the ease and additive\neffect of multimodal fusion, outperforming unimodal models. We demonstrate the\nbenefit of including pathology report text and rigorously evaluate the effect\nof model-based text summarization and hallucination. Overall, we modernize\nsurvival modeling by leveraging FMs and information extraction from pathology\nreports."}
{"id": "2505.07680", "pdf": "https://arxiv.org/pdf/2505.07680", "abs": "https://arxiv.org/abs/2505.07680", "authors": ["Hang Wu", "Jianian Zhu", "Yinghui Li", "Haojie Wang", "Biao Hou", "Jidong Zhai"], "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models", "categories": ["cs.LG", "cs.DC"], "comment": "10 pages", "summary": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method."}
{"id": "2504.07840", "pdf": "https://arxiv.org/pdf/2504.07840", "abs": "https://arxiv.org/abs/2504.07840", "authors": ["Cansu Koyuturk", "Emily Theophilou", "Sabrina Patania", "Gregor Donabauer", "Andrea Martinenghi", "Chiara Antico", "Alessia Telari", "Alessia Testa", "Sathya Bursic", "Franca Garzotto", "Davinia Hernandez-Leo", "Udo Kruschwitz", "Davide Taibi", "Simona Amenta", "Martin Ruskov", "Dimitri Ognibene"], "title": "Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "Accepted for AIED 2025, the 26th International Conference on\n  Artificial Intelligence in Education, July 22 - 26, 2025, Palermo, Italy", "summary": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems."}
{"id": "2505.06746", "pdf": "https://arxiv.org/pdf/2505.06746", "abs": "https://arxiv.org/abs/2505.06746", "authors": ["Morui Zhu", "Yongqi Zhu", "Yihao Zhu", "Qi Chen", "Deyuan Qu", "Song Fu", "Qing Yang"], "title": "M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark", "categories": ["cs.RO", "cs.CV", "I.2.10; I.2.9"], "comment": "supplementary material included", "summary": "We introduce M$^3$CAD, a novel benchmark designed to advance research in\ngeneric cooperative autonomous driving. M$^3$CAD comprises 204 sequences with\n30k frames, spanning a diverse range of cooperative driving scenarios. Each\nsequence includes multiple vehicles and sensing modalities, e.g., LiDAR point\nclouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving\ntasks, including object detection and tracking, mapping, motion forecasting,\noccupancy prediction, and path planning. This rich multimodal setup enables\nM$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving\nresearch, significantly broadening the scope of research in the field. To our\nknowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored\nfor cooperative multi-task autonomous driving research. We evaluate the\nstate-of-the-art end-to-end solution on M$^3$CAD to establish baseline\nperformance. To foster cooperative autonomous driving research, we also propose\nE2EC, a simple yet effective framework for cooperative driving solution that\nleverages inter-vehicle shared information for improved path planning. We\nrelease M$^3$CAD, along with our baseline models and evaluation results, to\nsupport the development of robust cooperative autonomous driving systems. All\nresources will be made publicly available on https://github.com/zhumorui/M3CAD"}
{"id": "2505.07711", "pdf": "https://arxiv.org/pdf/2505.07711", "abs": "https://arxiv.org/abs/2505.07711", "authors": ["Pranav Sinha", "Sumit Kumar Jha", "Sunny Raj"], "title": "Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations", "categories": ["cs.ET", "cs.AI", "quant-ph"], "comment": "7 pages, 2 tables and 3 figures", "summary": "We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where\nquantum computers are limited by noisy gates, some of which are more\nerror-prone than others and can render the final computation incomprehensible.\nQuantum circuit compilation algorithms attempt to minimize these noisy gates\nwhen mapping quantum algorithms onto quantum hardware but face computational\nchallenges that restrict their application to circuits with no more than 5-6\nqubits, necessitating the need to partition large circuits before the\napplication of noisy quantum gate minimization algorithms. The existing\ngeneration of these algorithms is heuristic in nature and does not account for\ndownstream gate minimization tasks. Large language models (LLMs) have the\npotential to change this and help improve quantum circuit partitions. This\npaper investigates the use of LLMs, such as Llama and Mistral, for partitioning\nquantum circuits by capitalizing on their abilities to understand and generate\ncode, including QASM. Specifically, we teach LLMs to partition circuits using\nthe quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through\nexperimental evaluations, we show that careful fine-tuning of open source LLMs\nenables us to obtain an accuracy of 53.4% for the partition task while\nover-the-shelf LLMs are unable to correctly partition circuits, using standard\n1-shot and few-shot training approaches."}
{"id": "2505.07702", "pdf": "https://arxiv.org/pdf/2505.07702", "abs": "https://arxiv.org/abs/2505.07702", "authors": ["Onthada Preedasawakul", "Nathakhun Wiroonsri"], "title": "4TaStiC: Time and trend traveling time series clustering for classifying long-term type 2 diabetes patients", "categories": ["cs.LG", "cs.CY", "62H30 (Primary) 62M10, 92C50 (Secondary)"], "comment": null, "summary": "Diabetes is one of the most prevalent diseases worldwide, characterized by\npersistently high blood sugar levels, capable of damaging various internal\norgans and systems. Diabetes patients require routine check-ups, resulting in a\ntime series of laboratory records, such as hemoglobin A1c, which reflects each\npatient's health behavior over time and informs their doctor's recommendations.\nClustering patients into groups based on their entire time series data assists\ndoctors in making recommendations and choosing treatments without the need to\nreview all records. However, time series clustering of this type of dataset\nintroduces some challenges; patients visit their doctors at different time\npoints, making it difficult to capture and match trends, peaks, and patterns.\nAdditionally, two aspects must be considered: differences in the levels of\nlaboratory results and differences in trends and patterns. To address these\nchallenges, we introduce a new clustering algorithm called Time and Trend\nTraveling Time Series Clustering (4TaStiC), using a base dissimilarity measure\ncombined with Euclidean and Pearson correlation metrics. We evaluated this\nalgorithm on artificial datasets, comparing its performance with that of seven\nexisting methods. The results show that 4TaStiC outperformed the other methods\non the targeted datasets. Finally, we applied 4TaStiC to cluster a cohort of\n1,989 type 2 diabetes patients at Siriraj Hospital. Each group of patients\nexhibits clear characteristics that will benefit doctors in making efficient\nclinical decisions. Furthermore, the proposed algorithm can be applied to\ncontexts outside the medical field."}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988", "abs": "https://arxiv.org/abs/2504.18988", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "The manuscript has been withdrawn by the authors due to ongoing\n  revisions and substantial updates", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research."}
{"id": "2505.07728", "pdf": "https://arxiv.org/pdf/2505.07728", "abs": "https://arxiv.org/abs/2505.07728", "authors": ["Lihan Zha", "Apurva Badithela", "Michael Zhang", "Justin Lidard", "Jeremy Bao", "Emily Zhou", "David Snyder", "Allen Z. Ren", "Dhruv Shah", "Anirudha Majumdar"], "title": "Guiding Data Collection via Factored Scaling Curves", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Project website: https://factored-data-scaling.github.io", "summary": "Generalist imitation learning policies trained on large datasets show great\npromise for solving diverse manipulation tasks. However, to ensure\ngeneralization to different conditions, policies need to be trained with data\ncollected across a large set of environmental factor variations (e.g., camera\npose, table height, distractors) $-$ a prohibitively expensive undertaking, if\ndone exhaustively. We introduce a principled method for deciding what data to\ncollect and how much to collect for each factor by constructing factored\nscaling curves (FSC), which quantify how policy performance varies as data\nscales along individual or paired factors. These curves enable targeted data\nacquisition for the most influential factor combinations within a given budget.\nWe evaluate the proposed method through extensive simulated and real-world\nexperiments, across both training-from-scratch and fine-tuning settings, and\nshow that it boosts success rates in real-world tasks in new environments by up\nto 26% over existing data-collection strategies. We further demonstrate how\nfactored scaling curves can effectively guide data collection using an offline\nmetric, without requiring real-world evaluation at scale."}
{"id": "2505.07735", "pdf": "https://arxiv.org/pdf/2505.07735", "abs": "https://arxiv.org/abs/2505.07735", "authors": ["Nicholas T. Runcie", "Charlotte M. Deane", "Fergus Imrie"], "title": "Assessing the Chemical Intelligence of Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models are versatile, general-purpose tools with a wide range\nof applications. Recently, the advent of \"reasoning models\" has led to\nsubstantial improvements in their abilities in advanced problem-solving domains\nsuch as mathematics and software engineering. In this work, we assessed the\nability of reasoning models to directly perform chemistry tasks, without any\nassistance from external tools. We created a novel benchmark, called ChemIQ,\nwhich consists of 796 questions assessing core concepts in organic chemistry,\nfocused on molecular comprehension and chemical reasoning. Unlike previous\nbenchmarks, which primarily use multiple choice formats, our approach requires\nmodels to construct short-answer responses, more closely reflecting real-world\napplications. The reasoning models, exemplified by OpenAI's o3-mini, correctly\nanswered 28%-59% of questions depending on the reasoning level used, with\nhigher reasoning levels significantly increasing performance on all tasks.\nThese models substantially outperformed the non-reasoning model, GPT-4o, which\nachieved only 7% accuracy. We found that Large Language Models can now convert\nSMILES strings to IUPAC names, a task earlier models were unable to perform.\nAdditionally, we show that the latest reasoning models can elucidate structures\nfrom 1H and 13C NMR data, correctly generating SMILES strings for 74% of\nmolecules containing up to 10 heavy atoms, and in one case solving a structure\ncomprising 21 heavy atoms. For each task, we found evidence that the reasoning\nprocess mirrors that of a human chemist. Our results demonstrate that the\nlatest reasoning models have the ability to perform advanced chemical\nreasoning."}
{"id": "2504.20879", "pdf": "https://arxiv.org/pdf/2504.20879", "abs": "https://arxiv.org/abs/2504.20879", "authors": ["Shivalika Singh", "Yiyang Nan", "Alex Wang", "Daniel D'Souza", "Sayash Kapoor", "Ahmet Üstün", "Sanmi Koyejo", "Yuntian Deng", "Shayne Longpre", "Noah A. Smith", "Beyza Ermis", "Marzieh Fadaee", "Sara Hooker"], "title": "The Leaderboard Illusion", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "comment": "68 pages, 18 figures, 9 tables", "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field"}
{"id": "2505.06980", "pdf": "https://arxiv.org/pdf/2505.06980", "abs": "https://arxiv.org/abs/2505.06980", "authors": ["Lei Wan", "Prabesh Gupta", "Andreas Eich", "Marcel Kettelgerdes", "Hannan Ejaz Keen", "Michael Klöppel-Gersdorf", "Alexey Vinel"], "title": "VALISENS: A Validated Innovative Multi-Sensor System for Cooperative Automated Driving", "categories": ["cs.RO", "cs.CV"], "comment": "7 pages, 11 figures, submitted to IEEE ITSC", "summary": "Perception is a core capability of automated vehicles and has been\nsignificantly advanced through modern sensor technologies and artificial\nintelligence. However, perception systems still face challenges in complex\nreal-world scenarios. To improve robustness against various external factors,\nmulti-sensor fusion techniques are essential, combining the strengths of\ndifferent sensor modalities. With recent developments in Vehicle-to-Everything\n(V2X communication, sensor fusion can now extend beyond a single vehicle to a\ncooperative multi-agent system involving Connected Automated Vehicle (CAV) and\nintelligent infrastructure. This paper presents VALISENS, an innovative\nmulti-sensor system distributed across multiple agents. It integrates onboard\nand roadside LiDARs, radars, thermal cameras, and RGB cameras to enhance\nsituational awareness and support cooperative automated driving. The thermal\ncamera adds critical redundancy for perceiving Vulnerable Road User (VRU),\nwhile fusion with roadside sensors mitigates visual occlusions and extends the\nperception range beyond the limits of individual vehicles. We introduce the\ncorresponding perception module built on this sensor system, which includes\nobject detection, tracking, motion forecasting, and high-level data fusion. The\nproposed system demonstrates the potential of cooperative perception in\nreal-world test environments and lays the groundwork for future Cooperative\nIntelligent Transport Systems (C-ITS) applications."}
{"id": "2505.07755", "pdf": "https://arxiv.org/pdf/2505.07755", "abs": "https://arxiv.org/abs/2505.07755", "authors": ["Tomasz Szydlo", "Viacheslaw Horbanow", "Dev Nandan Jha", "Shashikant Ilager", "Aleksander Slominski", "Rajiv Ranjan"], "title": "Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Edge computing has emerged as a pivotal technology, offering significant\nadvantages such as low latency, enhanced data security, and reduced reliance on\ncentralized cloud infrastructure. These benefits are crucial for applications\nrequiring real-time data processing or strict security measures. Despite these\nadvantages, edge devices operating within edge clusters are often\nunderutilized. This inefficiency is mainly due to the absence of a holistic\nperformance profiling mechanism which can help dynamically adjust the desired\nsystem configuration for a given workload. Since edge computing environments\ninvolve a complex interplay between CPU frequency, power consumption, and\napplication performance, a deeper understanding of these correlations is\nessential. By uncovering these relationships, it becomes possible to make\ninformed decisions that enhance both computational efficiency and energy\nsavings. To address this gap, this paper evaluates the power consumption and\nperformance characteristics of a single processing node within an edge cluster\nusing a synthetic microbenchmark by varying the workload size and CPU\nfrequency. The results show how an optimal measure can lead to optimized usage\nof edge resources, given both performance and power consumption."}
{"id": "2505.07750", "pdf": "https://arxiv.org/pdf/2505.07750", "abs": "https://arxiv.org/abs/2505.07750", "authors": ["Gašper Petelin", "Gjorgjina Cenikj"], "title": "The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting Wrong", "categories": ["cs.LG"], "comment": null, "summary": "Algorithm selection, aiming to identify the best algorithm for a given\nproblem, plays a pivotal role in continuous black-box optimization. A common\napproach involves representing optimization functions using a set of features,\nwhich are then used to train a machine learning meta-model for selecting\nsuitable algorithms. Various approaches have demonstrated the effectiveness of\nthese algorithm selection meta-models. However, not all evaluation approaches\nare equally valid for assessing the performance of meta-models. We highlight\nmethodological issues that frequently occur in the community and should be\naddressed when evaluating algorithm selection approaches. First, we identify\nflaws with the \"leave-instance-out\" evaluation technique. We show that\nnon-informative features and meta-models can achieve high accuracy, which\nshould not be the case with a well-designed evaluation framework. Second, we\ndemonstrate that measuring the performance of optimization algorithms with\nmetrics sensitive to the scale of the objective function requires careful\nconsideration of how this impacts the construction of the meta-model, its\npredictions, and the model's error. Such metrics can falsely present overly\noptimistic performance assessments of the meta-models. This paper emphasizes\nthe importance of careful evaluation, as loosely defined methodologies can\nmislead researchers, divert efforts, and introduce noise into the field"}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831", "abs": "https://arxiv.org/abs/2505.00831", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Kentaro Inui", "Dezhen Song"], "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics. Our source code is available here:\nhttps://github.com/quangpham2006/SmallPlan"}
{"id": "2505.07085", "pdf": "https://arxiv.org/pdf/2505.07085", "abs": "https://arxiv.org/abs/2505.07085", "authors": ["Matt Franchi", "Hauke Sandhaus", "Madiha Zahrah Choksi", "Severin Engelmann", "Wendy Ju", "Helen Nissenbaum"], "title": "Privacy of Groups in Dense Street Imagery", "categories": ["cs.CY", "cs.CV", "cs.ET"], "comment": "To appear in ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) '25", "summary": "Spatially and temporally dense street imagery (DSI) datasets have grown\nunbounded. In 2024, individual companies possessed around 3 trillion unique\nimages of public streets. DSI data streams are only set to grow as companies\nlike Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze\ncollisions. Academic researchers leverage DSI to explore novel approaches to\nurban analysis. Despite good-faith efforts by DSI providers to protect\nindividual privacy through blurring faces and license plates, these measures\nfail to address broader privacy concerns. In this work, we find that increased\ndata density and advancements in artificial intelligence enable harmful group\nmembership inferences from supposedly anonymized data. We perform a penetration\ntest to demonstrate how easily sensitive group affiliations can be inferred\nfrom obfuscated pedestrians in 25,232,608 dashcam images taken in New York\nCity. We develop a typology of identifiable groups within DSI and analyze\nprivacy implications through the lens of contextual integrity. Finally, we\ndiscuss actionable recommendations for researchers working with data from DSI\nproviders."}
{"id": "2505.07793", "pdf": "https://arxiv.org/pdf/2505.07793", "abs": "https://arxiv.org/abs/2505.07793", "authors": ["Assaf Ben-Kish", "Itamar Zimerman", "M. Jehanzeb Mirza", "James Glass", "Leonid Karlinsky", "Raja Giryes"], "title": "Overflow Prevention Enhances Long-Context Recurrent LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A recent trend in LLMs is developing recurrent sub-quadratic models that\nimprove long-context processing efficiency. We investigate leading large\nlong-context models, focusing on how their fixed-size recurrent memory affects\ntheir performance. Our experiments reveal that, even when these models are\ntrained for extended contexts, their use of long contexts remains\nunderutilized. Specifically, we demonstrate that a chunk-based inference\nprocedure, which identifies and processes only the most relevant portion of the\ninput can mitigate recurrent memory failures and be effective for many\nlong-context tasks: On LongBench, our method improves the overall performance\nof Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%,\nRecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this\nsimple approach also leads to state-of-the-art results in the challenging\nLongBench v2 benchmark, showing competitive performance with equivalent size\nTransformers. Furthermore, our findings raise questions about whether recurrent\nmodels genuinely exploit long-range dependencies, as our single-chunk strategy\ndelivers stronger performance - even in tasks that presumably require\ncross-context relations."}
{"id": "2505.07777", "pdf": "https://arxiv.org/pdf/2505.07777", "abs": "https://arxiv.org/abs/2505.07777", "authors": ["Arya Grayeli", "Vipin Swarup", "Steven E. Noel"], "title": "Synthesizing Diverse Network Flow Datasets with Scalable Dynamic Multigraph Generation", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Obtaining real-world network datasets is often challenging because of\nprivacy, security, and computational constraints. In the absence of such\ndatasets, graph generative models become essential tools for creating synthetic\ndatasets. In this paper, we introduce a novel machine learning model for\ngenerating high-fidelity synthetic network flow datasets that are\nrepresentative of real-world networks. Our approach involves the generation of\ndynamic multigraphs using a stochastic Kronecker graph generator for structure\ngeneration and a tabular generative adversarial network for feature generation.\nWe further employ an XGBoost (eXtreme Gradient Boosting) model for graph\nalignment, ensuring accurate overlay of features onto the generated graph\nstructure. We evaluate our model using new metrics that assess both the\naccuracy and diversity of the synthetic graphs. Our results demonstrate\nimprovements in accuracy over previous large-scale graph generation methods\nwhile maintaining similar efficiency. We also explore the trade-off between\naccuracy and diversity in synthetic graph dataset creation, a topic not\nextensively covered in related works. Our contributions include the synthesis\nand evaluation of large real-world netflow datasets and the definition of new\nmetrics for evaluating synthetic graph generative models."}
{"id": "2505.05190", "pdf": "https://arxiv.org/pdf/2505.05190", "abs": "https://arxiv.org/abs/2505.05190", "authors": ["Yixin Cheng", "Hongcheng Guo", "Yangming Li", "Leonid Sigal"], "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "ICML 2025 Accpeted", "summary": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking."}
{"id": "2505.07110", "pdf": "https://arxiv.org/pdf/2505.07110", "abs": "https://arxiv.org/abs/2505.07110", "authors": ["Tong Zhang", "Fenghua Shao", "Runsheng Zhang", "Yifan Zhuang", "Liuqingqing Yang"], "title": "DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Based on the DeepSORT algorithm, this study explores the application of\nvisual tracking technology in intelligent human-computer interaction,\nespecially in the field of gesture recognition and tracking. With the rapid\ndevelopment of artificial intelligence and deep learning technology,\nvisual-based interaction has gradually replaced traditional input devices and\nbecome an important way for intelligent systems to interact with users. The\nDeepSORT algorithm can achieve accurate target tracking in dynamic environments\nby combining Kalman filters and deep learning feature extraction methods. It is\nespecially suitable for complex scenes with multi-target tracking and fast\nmovements. This study experimentally verifies the superior performance of\nDeepSORT in gesture recognition and tracking. It can accurately capture and\ntrack the user's gesture trajectory and is superior to traditional tracking\nmethods in terms of real-time and accuracy. In addition, this study also\ncombines gesture recognition experiments to evaluate the recognition ability\nand feedback response of the DeepSORT algorithm under different gestures (such\nas sliding, clicking, and zooming). The experimental results show that DeepSORT\ncan not only effectively deal with target occlusion and motion blur but also\ncan stably track in a multi-target environment, achieving a smooth user\ninteraction experience. Finally, this paper looks forward to the future\ndevelopment direction of intelligent human-computer interaction systems based\non visual tracking and proposes future research focuses such as algorithm\noptimization, data fusion, and multimodal interaction in order to promote a\nmore intelligent and personalized interactive experience. Keywords-DeepSORT,\nvisual tracking, gesture recognition, human-computer interaction"}
{"id": "2505.07802", "pdf": "https://arxiv.org/pdf/2505.07802", "abs": "https://arxiv.org/abs/2505.07802", "authors": ["Reece O'Mahoney", "Wanming Yu", "Ioannis Havoutis"], "title": "Improving Trajectory Stitching with Flow Models", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models have shown great promise as trajectory planners, given\ntheir affinity to modeling complex distributions and guidable inference\nprocess. Previous works have successfully applied these in the context of\nrobotic manipulation but perform poorly when the required solution does not\nexist as a complete trajectory within the training set. We identify that this\nis a result of being unable to plan via stitching, and subsequently address the\narchitectural and dataset choices needed to remedy this. On top of this, we\npropose a novel addition to the training and inference procedures to both\nstabilize and enhance these capabilities. We demonstrate the efficacy of our\napproach by generating plans with out of distribution boundary conditions and\nperforming obstacle avoidance on the Franka Panda in simulation and on real\nhardware. In both of these tasks our method performs significantly better than\nthe baselines and is able to avoid obstacles up to four times as large."}
{"id": "2505.07782", "pdf": "https://arxiv.org/pdf/2505.07782", "abs": "https://arxiv.org/abs/2505.07782", "authors": ["Rushi Qiang", "Yuchen Zhuang", "Yinghao Li", "Dingu Sagar V K", "Rongzhi Zhang", "Changhao Li", "Ian Shu-Hei Wong", "Sherry Yang", "Percy Liang", "Chao Zhang", "Bo Dai"], "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "categories": ["cs.LG"], "comment": null, "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents."}
{"id": "2505.07813", "pdf": "https://arxiv.org/pdf/2505.07813", "abs": "https://arxiv.org/abs/2505.07813", "authors": ["Tony Tao", "Mohan Kumar Srirama", "Jason Jingzhou Liu", "Kenneth Shaw", "Deepak Pathak"], "title": "DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "In RSS 2025. Website at https://dexwild.github.io", "summary": "Large-scale, diverse robot datasets have emerged as a promising path toward\nenabling dexterous manipulation policies to generalize to novel environments,\nbut acquiring such datasets presents many challenges. While teleoperation\nprovides high-fidelity datasets, its high cost limits its scalability. Instead,\nwhat if people could use their own hands, just as they do in everyday life, to\ncollect data? In DexWild, a diverse team of data collectors uses their hands to\ncollect hours of interactions across a multitude of environments and objects.\nTo record this data, we create DexWild-System, a low-cost, mobile, and\neasy-to-use device. The DexWild learning framework co-trains on both human and\nrobot demonstrations, leading to improved performance compared to training on\neach dataset individually. This combination results in robust robot policies\ncapable of generalizing to novel environments, tasks, and embodiments with\nminimal additional robot-specific data. Experimental results demonstrate that\nDexWild significantly improves performance, achieving a 68.5% success rate in\nunseen environments-nearly four times higher than policies trained with robot\ndata only-and offering 5.8x better cross-embodiment generalization. Video\nresults, codebases, and instructions at https://dexwild.github.io"}
{"id": "2505.07783", "pdf": "https://arxiv.org/pdf/2505.07783", "abs": "https://arxiv.org/abs/2505.07783", "authors": ["Yanxin Liu", "Yunqi Zhang"], "title": "Relative Overfitting and Accept-Reject Framework", "categories": ["cs.LG"], "comment": null, "summary": "Currently, the scaling law of Large Language Models (LLMs) faces challenges\nand bottlenecks. This paper posits that noise effects, stemming from changes in\nthe signal-to-noise ratio under diminishing marginal returns, are the root\ncause of these issues. To control this noise, we investigated the differences\nbetween models with performance advantages and disadvantages, introducing the\nconcept of \"relative overfitting.\" Based on their complementary strengths, we\nhave proposed an application framework, Accept-Reject (AR). In Natural Language\nProcessing (NLP), we use LLMs and Small Language Models (SLMs) as the medium\nfor discussion. This framework enables SLMs to exert a universal positive\ninfluence on LLM decision outputs, rather than the intuitively expected\nnegative influence. We validated our approach using self-built models based on\nmainstream architectures and pre-trained mainstream models across multiple\ndatasets, including basic language modeling, long-context tasks, subject\nexamination, and question-answering (QA) benchmarks. The results demonstrate\nthat through our structure, compared to increasing the LLM's parameters, we can\nachieve better performance improvements with significantly lower parameter and\ncomputational costs in many scenarios. These improvements are universal,\nstable, and effective. Furthermore, we explore the potential of \"relative\noverfitting\" and the AR framework in other machine learning domains, such as\ncomputer vision (CV) and AI for science. We hope the proposed approach can help\nscale laws overcome existing bottlenecks."}
{"id": "2505.07600", "pdf": "https://arxiv.org/pdf/2505.07600", "abs": "https://arxiv.org/abs/2505.07600", "authors": ["Oriol Barbany", "Adrià Colomé", "Carme Torras"], "title": "Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at ICRA 2025 Workshop \"Reflections on Representations and\n  Manipulating Deformable Objects\". Project page\n  https://barbany.github.io/bifold/", "summary": "Manipulating clothes is challenging due to their complex dynamics, high\ndeformability, and frequent self-occlusions. Garments exhibit a nearly infinite\nnumber of configurations, making explicit state representations difficult to\ndefine. In this paper, we analyze BiFold, a model that predicts\nlanguage-conditioned pick-and-place actions from visual observations, while\nimplicitly encoding garment state through end-to-end learning. To address\nscenarios such as crumpled garments or recovery from failed manipulations,\nBiFold leverages temporal context to improve state estimation. We examine the\ninternal representations of the model and present evidence that its fine-tuning\nand temporal context enable effective alignment between text and image regions,\nas well as temporal consistency."}
{"id": "2505.07816", "pdf": "https://arxiv.org/pdf/2505.07816", "abs": "https://arxiv.org/abs/2505.07816", "authors": ["Veeti Ahvonen", "Damian Heiman", "Antti Kuusisto"], "title": "A class of distributed automata that contains the modal mu-fragment", "categories": ["cs.LO", "cs.AI", "F.4.1; F.1.1; I.2.0"], "comment": null, "summary": "This paper gives a translation from the $\\mu$-fragment of the graded modal\n$\\mu$-calculus to a class of distributed message-passing automata. As a\ncorollary, we obtain an alternative proof for a theorem from\n\\cite{ahvonen_neurips} stating that recurrent graph neural networks working\nwith reals and graded modal substitution calculus have the same expressive\npower in restriction to the logic monadic second-order logic MSO."}
{"id": "2505.07797", "pdf": "https://arxiv.org/pdf/2505.07797", "abs": "https://arxiv.org/abs/2505.07797", "authors": ["Daniel Beechey", "Thomas M. S. Smith", "Özgür Şimşek"], "title": "A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning agents can achieve superhuman performance, but their\ndecisions are often difficult to interpret. This lack of transparency limits\ndeployment, especially in safety-critical settings where human trust and\naccountability are essential. In this work, we develop a theoretical framework\nfor explaining reinforcement learning through the influence of state features,\nwhich represent what the agent observes in its environment. We identify three\ncore elements of the agent-environment interaction that benefit from\nexplanation: behaviour (what the agent does), performance (what the agent\nachieves), and value estimation (what the agent expects to achieve). We treat\nstate features as players cooperating to produce each element and apply Shapley\nvalues, a principled method from cooperative game theory, to identify the\ninfluence of each feature. This approach yields a family of mathematically\ngrounded explanations with clear semantics and theoretical guarantees. We use\nillustrative examples to show how these explanations align with human intuition\nand reveal novel insights. Our framework unifies and extends prior work, making\nexplicit the assumptions behind existing approaches, and offers a principled\nfoundation for more interpretable and trustworthy reinforcement learning."}
{"id": "2505.07754", "pdf": "https://arxiv.org/pdf/2505.07754", "abs": "https://arxiv.org/abs/2505.07754", "authors": ["Samik Banerjee", "Caleb Stam", "Daniel J. Tward", "Steven Savoia", "Yusu Wang", "Partha P. Mitra"], "title": "Skeletonization of neuronal processes using Discrete Morse techniques from computational topology", "categories": ["q-bio.NC", "cs.CV"], "comment": "Under Review in Nature", "summary": "To understand biological intelligence we need to map neuronal networks in\nvertebrate brains. Mapping mesoscale neural circuitry is done using injections\nof tracers that label groups of neurons whose axons project to different brain\nregions. Since many neurons are labeled, it is difficult to follow individual\naxons. Previous approaches have instead quantified the regional projections\nusing the total label intensity within a region. However, such a quantification\nis not biologically meaningful. We propose a new approach better connected to\nthe underlying neurons by skeletonizing labeled axon fragments and then\nestimating a volumetric length density. Our approach uses a combination of deep\nnets and the Discrete Morse (DM) technique from computational topology. This\ntechnique takes into account nonlocal connectivity information and therefore\nprovides noise-robustness. We demonstrate the utility and scalability of the\napproach on whole-brain tracer injected data. We also define and illustrate an\ninformation theoretic measure that quantifies the additional information\nobtained, compared to the skeletonized tracer injection fragments, when\nindividual axon morphologies are available. Our approach is the first\napplication of the DM technique to computational neuroanatomy. It can help\nbridge between single-axon skeletons and tracer injections, two important data\ntypes in mapping neural networks in vertebrates."}
{"id": "2505.07819", "pdf": "https://arxiv.org/pdf/2505.07819", "abs": "https://arxiv.org/abs/2505.07819", "authors": ["Yiyang Lu", "Yufeng Tian", "Zhecheng Yuan", "Xianbang Wang", "Pu Hua", "Zhengrong Xue", "Huazhe Xu"], "title": "H$^{\\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Visuomotor policy learning has witnessed substantial progress in robotic\nmanipulation, with recent approaches predominantly relying on generative models\nto model the action distribution. However, these methods often overlook the\ncritical coupling between visual perception and action prediction. In this\nwork, we introduce $\\textbf{Triply-Hierarchical Diffusion\nPolicy}~(\\textbf{H$^{\\mathbf{3}}$DP})$, a novel visuomotor learning framework\nthat explicitly incorporates hierarchical structures to strengthen the\nintegration between visual features and action generation. H$^{3}$DP contains\n$\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes\nRGB-D observations based on depth information; (2) multi-scale visual\nrepresentations that encode semantic features at varying levels of granularity;\nand (3) a hierarchically conditioned diffusion process that aligns the\ngeneration of coarse-to-fine actions with corresponding visual features.\nExtensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5\\%}$\naverage relative improvement over baselines across $\\mathbf{44}$ simulation\ntasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual\nreal-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/."}
{"id": "2409.04300", "pdf": "https://arxiv.org/pdf/2409.04300", "abs": "https://arxiv.org/abs/2409.04300", "authors": ["Oliver Weissl", "Evgenii Egorov"], "title": "Equivariant Machine Learning Decoder for 3D Toric Codes", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Mitigating errors in computing and communication systems has seen a great\ndeal of research since the beginning of the widespread use of these\ntechnologies. However, as we develop new methods to do computation or\ncommunication, we also need to reiterate the method used to deal with errors.\nWithin the field of quantum computing, error correction is getting a lot of\nattention since errors can propagate fast and invalidate results, which makes\nthe theoretical exponential speed increase in computation time, compared to\ntraditional systems, obsolete. To correct errors in quantum systems,\nerror-correcting codes are used. A subgroup of codes, topological codes, is\ncurrently the focus of many research papers. Topological codes represent parity\ncheck matrices corresponding to graphs embedded on a $d$-dimensional surface.\nFor our research, the focus lies on the toric code with a 3D square lattice.\nThe goal of any decoder is robustness to noise, which can increase with code\nsize. However, a reasonable decoder performance scales polynomially with\nlattice size. As error correction is a time-sensitive operation, we propose a\nneural network using an inductive bias: equivariance. This allows the network\nto learn from a rather small subset of the exponentially growing training space\nof possible inputs. In addition, we investigate how transformer networks can\nhelp in correction. These methods will be compared with various configurations\nand previously published methods of decoding errors in the 3D toric code."}
{"id": "2505.07766", "pdf": "https://arxiv.org/pdf/2505.07766", "abs": "https://arxiv.org/abs/2505.07766", "authors": ["Xuying Huang", "Sicong Pan", "Maren Bennewitz"], "title": "Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "User privacy is a crucial concern in robotic applications, especially when\nmobile service robots are deployed in personal or sensitive environments.\nHowever, many robotic downstream tasks require the use of cameras, which may\nraise privacy risks. To better understand user perceptions of privacy in\nrelation to visual data, we conducted a user study investigating how different\nimage modalities and image resolutions affect users' privacy concerns. The\nresults show that depth images are broadly viewed as privacy-safe, and a\nsimilarly high proportion of respondents feel the same about semantic\nsegmentation images. Additionally, the majority of participants consider 32*32\nresolution RGB images to be almost sufficiently privacy-preserving, while most\nbelieve that 16*16 resolution can fully guarantee privacy protection."}
{"id": "2404.12534", "pdf": "https://arxiv.org/pdf/2404.12534", "abs": "https://arxiv.org/abs/2404.12534", "authors": ["Peiyang Song", "Kaiyu Yang", "Anima Anandkumar"], "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean", "categories": ["cs.AI", "cs.LG", "cs.LO", "stat.ML"], "comment": "Published at NeuS 2025. All code and artifacts open-sourced at\n  https://github.com/lean-dojo/LeanCopilot. All evaluation details are made\n  public at\n  https://github.com/Peiyang-Song/mathematics_in_lean/tree/full-scale-experiment", "summary": "Neural theorem proving combines large language models (LLMs) with proof\nassistants such as Lean, where the correctness of formal proofs can be\nrigorously verified, leaving no room for hallucination. With existing neural\ntheorem provers pretrained on a fixed collection of data and offering valuable\nsuggestions at times, it is challenging for them to continually prove novel\ntheorems in a fully autonomous mode, where human insights may be critical. In\nthis paper, we explore LLMs as copilots that assist humans in proving theorems.\nWe introduce Lean Copilot, a general framework for running LLM inference\nnatively in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nLean users can use our pretrained models or bring their own ones that run\neither locally (with or without GPUs) or on the cloud. Using Lean Copilot, we\nbuild LLM-based tools that suggest proof steps, complete proof goals, and\nselect relevant premises. Experimental results on the Mathematics in Lean\ntextbook demonstrate the effectiveness of our method compared to existing\nrule-based proof automation in Lean (aesop). When assisting humans, Lean\nCopilot requires only 2.08 manually-entered proof steps on average (3.86\nrequired by aesop); when automating the theorem proving process, Lean Copilot\nautomates 74.2% proof steps on average, 85% better than aesop (40.1%). We open\nsource all code and artifacts under a permissive MIT license to facilitate\nfurther research."}
{"id": "2505.06243", "pdf": "https://arxiv.org/pdf/2505.06243", "abs": "https://arxiv.org/abs/2505.06243", "authors": ["Mykola Kozlenko"], "title": "Supervised machine learning based signal demodulation in chaotic communications", "categories": ["eess.SP", "cs.LG", "68T07 (Primary) 94A12, 94A13, 94A14 (Secondary)", "I.2.6; C.2"], "comment": "5 pages, 3 figures, 1 table. This paper was originally published in\n  2022 International Conference on Innovative Solutions in Software Engineering\n  (ICISSE), available: https://zenodo.org/records/7512427", "summary": "A chaotic modulation scheme is an efficient wideband communication method. It\nutilizes the deterministic chaos to generate pseudo-random carriers. Chaotic\nbifurcation parameter modulation is one of the well-known and widely-used\ntechniques. This paper presents the machine learning based demodulation\napproach for the bifurcation parameter keying. It presents the structure of a\nconvolutional neural network as well as performance metrics values for signals\ngenerated with the chaotic logistic map. The paper provides an assessment of\nthe overall accuracy for binary signals. It reports the accuracy value of 0.88\nfor the bifurcation parameter deviation of 1.34% in the presence of additive\nwhite Gaussian noise at the normalized signal-to-noise ratio value of 20 dB for\nbalanced dataset."}
{"id": "2505.07815", "pdf": "https://arxiv.org/pdf/2505.07815", "abs": "https://arxiv.org/abs/2505.07815", "authors": ["Seungjae Lee", "Daniel Ekpo", "Haowen Liu", "Furong Huang", "Abhinav Shrivastava", "Jia-Bin Huang"], "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project webpage: https://ive-robot.github.io/", "summary": "Exploration is essential for general-purpose robotic learning, especially in\nopen-ended environments where dense rewards, explicit goals, or task-specific\nsupervision are scarce. Vision-language models (VLMs), with their semantic\nreasoning over objects, spatial relations, and potential outcomes, present a\ncompelling foundation for generating high-level exploratory behaviors. However,\ntheir outputs are often ungrounded, making it difficult to determine whether\nimagined transitions are physically feasible or informative. To bridge the gap\nbetween imagination and execution, we present IVE (Imagine, Verify, Execute),\nan agentic exploration framework inspired by human curiosity. Human exploration\nis often driven by the desire to discover novel scene configurations and to\ndeepen understanding of the environment. Similarly, IVE leverages VLMs to\nabstract RGB-D observations into semantic scene graphs, imagine novel scenes,\npredict their physical plausibility, and generate executable skill sequences\nthrough action tools. We evaluate IVE in both simulated and real-world tabletop\nenvironments. The results show that IVE enables more diverse and meaningful\nexploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the\nentropy of visited states. Moreover, the collected experience supports\ndownstream learning, producing policies that closely match or exceed the\nperformance of those trained on human-collected demonstrations."}
{"id": "2406.14132", "pdf": "https://arxiv.org/pdf/2406.14132", "abs": "https://arxiv.org/abs/2406.14132", "authors": ["Bin Li", "Jiayan Pei", "Feiyang Xiao", "Yifan Zhao", "Zhixing Zhang", "Diwei Liu", "HengXu He", "Jia Jia"], "title": "Enhancing Monotonic Modeling with Spatio-Temporal Adaptive Awareness in Diverse Marketing", "categories": ["cs.AI"], "comment": "12 pages", "summary": "In the mobile internet era, the Online Food Ordering Service (OFOS) emerges\nas an integral component of inclusive finance owing to the convenience it\nbrings to people. OFOS platforms offer dynamic allocation incentives to users\nand merchants through diverse marketing campaigns to encourage payments while\nmaintaining the platforms' budget efficiency. Despite significant progress, the\nmarketing domain continues to face two primary challenges: (i) how to allocate\na limited budget with greater efficiency, demanding precision in predicting\nusers' monotonic response (i.e. sensitivity) to incentives, and (ii) ensuring\nspatio-temporal adaptability and robustness in diverse marketing campaigns\nacross different times and locations. To address these issues, we propose a\nConstrained Monotonic Adaptive Network (CoMAN) method for spatio-temporal\nperception within marketing pricing. Specifically, we capture spatio-temporal\npreferences within attribute features through two foundational spatio-temporal\nperception modules. To further enhance catching the user sensitivity\ndifferentials to incentives across varied times and locations, we design\nmodules for learning spatio-temporal convexity and concavity as well as for\nexpressing sensitivity functions. CoMAN can achieve a more efficient allocation\nof incentive investments during pricing, thus increasing the conversion rate\nand orders while maintaining budget efficiency. Extensive offline and online\nexperimental results within our diverse marketing campaigns demonstrate the\neffectiveness of the proposed approach while outperforming the monotonic\nstate-of-the-art method."}
{"id": "2505.06245", "pdf": "https://arxiv.org/pdf/2505.06245", "abs": "https://arxiv.org/abs/2505.06245", "authors": ["Dominic Schneider", "Lutz Rapp", "Christoph Ament"], "title": "A Transformer-Based Approach for Diagnosing Fault Cases in Optical Fiber Amplifiers", "categories": ["eess.SP", "cs.LG"], "comment": "This paper has been accepted for publication at the 25th\n  International Conference on Transparent Optical Networks (ICTON) 2025", "summary": "A transformer-based deep learning approach is presented that enables the\ndiagnosis of fault cases in optical fiber amplifiers using condition-based\nmonitoring time series data. The model, Inverse Triple-Aspect Self-Attention\nTransformer (ITST), uses an encoder-decoder architecture, utilizing three\nfeature extraction paths in the encoder, feature-engineered data for the\ndecoder and a self-attention mechanism. The results show that ITST outperforms\nstate-of-the-art models in terms of classification accuracy, which enables\npredictive maintenance for optical fiber amplifiers, reducing network downtimes\nand maintenance costs."}
{"id": "2505.07817", "pdf": "https://arxiv.org/pdf/2505.07817", "abs": "https://arxiv.org/abs/2505.07817", "authors": ["Kanchana Ranasinghe", "Xiang Li", "Cristina Mata", "Jongwoo Park", "Michael S Ryoo"], "title": "Pixel Motion as Universal Representation for Robot Control", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present LangToMo, a vision-language-action framework structured as a\ndual-system architecture that uses pixel motion forecasts as intermediate\nrepresentations. Our high-level System 2, an image diffusion model, generates\ntext-conditioned pixel motion sequences from a single frame to guide robot\ncontrol. Pixel motion-a universal, interpretable, and motion-centric\nrepresentation-can be extracted from videos in a self-supervised manner,\nenabling diffusion model training on web-scale video-caption data. Treating\ngenerated pixel motion as learned universal representations, our low level\nSystem 1 module translates these into robot actions via motion-to-action\nmapping functions, which can be either hand-crafted or learned with minimal\nsupervision. System 2 operates as a high-level policy applied at sparse\ntemporal intervals, while System 1 acts as a low-level policy at dense temporal\nintervals. This hierarchical decoupling enables flexible, scalable, and\ngeneralizable robot control under both unsupervised and supervised settings,\nbridging the gap between language, motion, and action. Checkout\nhttps://kahnchana.github.io/LangToMo for visualizations."}
{"id": "2407.19031", "pdf": "https://arxiv.org/pdf/2407.19031", "abs": "https://arxiv.org/abs/2407.19031", "authors": ["Tony Shaska"], "title": "Artificial Neural Networks on Graded Vector Spaces", "categories": ["cs.AI", "cs.LG", "68T05, 68Q32, 16W50, 17B70, 58A50, 14A22", "I.2; I.2.6"], "comment": null, "summary": "This paper presents a transformative framework for artificial neural networks\nover graded vector spaces, tailored to model hierarchical and structured data\nin fields like algebraic geometry and physics. By exploiting the algebraic\nproperties of graded vector spaces, where features carry distinct weights, we\nextend classical neural networks with graded neurons, layers, and activation\nfunctions that preserve structural integrity. Grounded in group actions,\nrepresentation theory, and graded algebra, our approach combines theoretical\nrigor with practical utility.\n  We introduce graded neural architectures, loss functions prioritizing graded\ncomponents, and equivariant extensions adaptable to diverse gradings. Case\nstudies validate the framework's effectiveness, outperforming standard neural\nnetworks in tasks such as predicting invariants in weighted projective spaces\nand modeling supersymmetric systems.\n  This work establishes a new frontier in machine learning, merging\nmathematical sophistication with interdisciplinary applications. Future\nchallenges, including computational scalability and finite field extensions,\noffer rich opportunities for advancing this paradigm."}
{"id": "2505.06249", "pdf": "https://arxiv.org/pdf/2505.06249", "abs": "https://arxiv.org/abs/2505.06249", "authors": ["Geraldine Henningsen"], "title": "An Early Warning Model for Forced Displacement", "categories": ["stat.AP", "cs.CY", "cs.LG"], "comment": "13 pages, 6 figures", "summary": "Monitoring tools for anticipatory action are increasingly gaining traction to\nimprove the efficiency and timeliness of humanitarian responses. Whilst\npredictive models can now forecast conflicts with high accuracy, translating\nthese predictions into potential forced displacement movements remains\nchallenging because it is often unclear which precise events will trigger\nsignificant population movements. This paper presents a novel monitoring\napproach for refugee and asylum seeker flows that addresses this challenge.\nUsing gradient boosting classification, we combine conflict forecasts with a\ncomprehensive set of economic, political, and demographic variables to assess\ntwo distinct risks at the country of origin: the likelihood of significant\ndisplacement flows and the probability of sudden increases in these flows. The\nmodel generates country-specific monthly risk indices for these two events with\nprediction horizons of one, three, and six months. Our analysis shows high\naccuracy in predicting significant displacement flows and good accuracy in\nforecasting sudden increases in displacement--the latter being inherently more\ndifficult to predict, given the complexity of displacement triggers. We achieve\nthese results by including predictive factors beyond conflict, thereby\ndemonstrating that forced displacement risks can be assessed through an\nintegrated analysis of multiple country-level indicators. Whilst these risk\nindices provide valuable quantitative support for humanitarian planning, they\nshould always be understood as decision-support tools within a broader\nanalytical framework."}
{"id": "2307.00811", "pdf": "https://arxiv.org/pdf/2307.00811", "abs": "https://arxiv.org/abs/2307.00811", "authors": ["Dongwei Wang", "Zhi Han", "Yanmei Wang", "Xiai Chen", "Baichen Liu", "Yandong Tang"], "title": "Review helps learn better: Temporal Supervised Knowledge Distillation", "categories": ["cs.CV", "cs.AI"], "comment": "This work is outdated and needs new improvements to make it valuable", "summary": "Reviewing plays an important role when learning knowledge. The knowledge\nacquisition at a certain time point may be strongly inspired with the help of\nprevious experience. Thus the knowledge growing procedure should show strong\nrelationship along the temporal dimension. In our research, we find that during\nthe network training, the evolution of feature map follows temporal sequence\nproperty. A proper temporal supervision may further improve the network\ntraining performance. Inspired by this observation, we propose Temporal\nSupervised Knowledge Distillation (TSKD). Specifically, we extract the\nspatiotemporal features in the different training phases of student by\nconvolutional Long Short-term memory network (Conv-LSTM). Then, we train the\nstudent net through a dynamic target, rather than static teacher network\nfeatures. This process realizes the refinement of old knowledge in student\nnetwork, and utilizes it to assist current learning. Extensive experiments\nverify the effectiveness and advantages of our method over existing knowledge\ndistillation methods, including various network architectures and different\ntasks (image classification and object detection) ."}
{"id": "2410.15665", "pdf": "https://arxiv.org/pdf/2410.15665", "abs": "https://arxiv.org/abs/2410.15665", "authors": ["Xun Jiang", "Feng Li", "Han Zhao", "Jiahao Qiu", "Jiaying Wang", "Jun Shao", "Shihao Xu", "Shu Zhang", "Weiling Chen", "Xavier Tang", "Yize Chen", "Mengyue Wu", "Weizhi Ma", "Mengdi Wang", "Tianqiao Chen"], "title": "Long Term Memory: The Foundation of AI Self-Evolution", "categories": ["cs.AI", "cs.LG"], "comment": "56 pages, 13 figures", "summary": "Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications."}
{"id": "2505.06263", "pdf": "https://arxiv.org/pdf/2505.06263", "abs": "https://arxiv.org/abs/2505.06263", "authors": ["Yiping Meng", "Yiming Sun"], "title": "From Biometrics to Environmental Control: AI-Enhanced Digital Twins for Personalized Health Interventions in Healing Landscapes", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "The dynamic nature of human health and comfort calls for adaptive systems\nthat respond to individual physiological needs in real time. This paper\npresents an AI-enhanced digital twin framework that integrates biometric\nsignals, specifically electrocardiogram (ECG) data, with environmental\nparameters such as temperature, humidity, and ventilation. Leveraging\nIoT-enabled sensors and biometric monitoring devices, the system continuously\nacquires, synchronises, and preprocesses multimodal data streams to construct a\nresponsive virtual replica of the physical environment. To validate this\nframework, a detailed case study is conducted using the MIT-BIH noise stress\ntest dataset. ECG signals are filtered and segmented using dynamic sliding\nwindows, followed by extracting heart rate variability (HRV) features such as\nSDNN, BPM, QTc, and LF/HF ratio. Relative deviation metrics are computed\nagainst clean baselines to quantify stress responses. A random forest\nclassifier is trained to predict stress levels across five categories, and\nShapley Additive exPlanations (SHAP) is used to interpret model behaviour and\nidentify key contributing features. These predictions are mapped to a\nstructured set of environmental interventions using a Five Level Stress\nIntervention Mapping, which activates multi-scale responses across personal,\nroom, building, and landscape levels. This integration of physiological\ninsight, explainable AI, and adaptive control establishes a new paradigm for\nhealth-responsive built environments. It lays the foundation for the future\ndevelopment of intelligent, personalised healing spaces."}
{"id": "2310.02719", "pdf": "https://arxiv.org/pdf/2310.02719", "abs": "https://arxiv.org/abs/2310.02719", "authors": ["Hongyi Fan", "Joe Kileel", "Benjamin Kimia"], "title": "Condition numbers in multiview geometry, instability in relative pose estimation, and RANSAC", "categories": ["cs.CV", "cs.NA", "math.NA"], "comment": "v2: additional experiments; correction to Theorem 20 (with current\n  numbering); other edits", "summary": "In this paper, we introduce a general framework for analyzing the numerical\nconditioning of minimal problems in multiple view geometry, using tools from\ncomputational algebra and Riemannian geometry. Special motivation comes from\nthe fact that relative pose estimation, based on standard 5-point or 7-point\nRandom Sample Consensus (RANSAC) algorithms, can fail even when no outliers are\npresent and there is enough data to support a hypothesis. We argue that these\ncases arise due to the intrinsic instability of the 5- and 7-point minimal\nproblems. We apply our framework to characterize the instabilities, both in\nterms of the world scenes that lead to infinite condition number, and directly\nin terms of ill-conditioned image data. The approach produces computational\ntests for assessing the condition number before solving the minimal problem.\nLastly, synthetic and real data experiments suggest that RANSAC serves not only\nto remove outliers, but in practice it also selects for well-conditioned image\ndata, which is consistent with our theory."}
{"id": "2410.17500", "pdf": "https://arxiv.org/pdf/2410.17500", "abs": "https://arxiv.org/abs/2410.17500", "authors": ["Ryota Maruo", "Koh Takeuchi", "Hisashi Kashima"], "title": "Learning Fair and Preferable Allocations through Neural Network", "categories": ["cs.AI"], "comment": null, "summary": "The fair allocation of indivisible resources is a fundamental problem.\nExisting research has developed various allocation mechanisms or algorithms to\nsatisfy different fairness notions. For example, round robin (RR) was proposed\nto meet the fairness criterion known as envy-freeness up to one good (EF1).\nExpert algorithms without mathematical formulations are used in real-world\nresource allocation problems to find preferable outcomes for users. Therefore,\nwe aim to design mechanisms that strictly satisfy good properties with\nreplicating expert knowledge. However, this problem is challenging because such\nheuristic rules are often difficult to formalize mathematically, complicating\ntheir integration into theoretical frameworks. Additionally, formal algorithms\nstruggle to find preferable outcomes, and directly replicating these implicit\nrules can result in unfair allocations because human decision-making can\nintroduce biases. In this paper, we aim to learn implicit allocation mechanisms\nfrom examples while strictly satisfying fairness constraints, specifically\nfocusing on learning EF1 allocation mechanisms through supervised learning on\nexamples of reported valuations and corresponding allocation outcomes produced\nby implicit rules. To address this, we developed a neural RR (NRR), a novel\nneural network that parameterizes RR. NRR is built from a differentiable\nrelaxation of RR and can be trained to learn the agent ordering used for RR. We\nconducted experiments to learn EF1 allocation mechanisms from examples,\ndemonstrating that our method outperforms baselines in terms of the proximity\nof predicted allocations and other metrics."}
{"id": "2505.06291", "pdf": "https://arxiv.org/pdf/2505.06291", "abs": "https://arxiv.org/abs/2505.06291", "authors": ["Wei Xiong", "Junming Lin", "Jiangtong Li", "Jie Li", "Changjun Jiang"], "title": "ALFEE: Adaptive Large Foundation Model for EEG Representation", "categories": ["eess.SP", "cs.CE", "cs.HC", "cs.LG"], "comment": "17pages, 17 figures", "summary": "While foundation models excel in text, image, and video domains, the critical\nbiological signals, particularly electroencephalography(EEG), remain\nunderexplored. EEG benefits neurological research with its high temporal\nresolution, operational practicality, and safety profile. However, low\nsignal-to-noise ratio, inter-subject variability, and cross-paradigm\ndifferences hinder the generalization of current models. Existing methods often\nemploy simplified strategies, such as a single loss function or a\nchannel-temporal joint representation module, and suffer from a domain gap\nbetween pretraining and evaluation tasks that compromises efficiency and\nadaptability. To address these limitations, we propose the Adaptive Large\nFoundation model for EEG signal representation(ALFEE) framework, a novel hybrid\ntransformer architecture with two learning stages for robust EEG representation\nlearning. ALFEE employs a hybrid attention that separates channel-wise feature\naggregation from temporal dynamics modeling, enabling robust EEG representation\nwith variable channel configurations. A channel encoder adaptively compresses\nvariable channel information, a temporal encoder captures task-guided\nevolution, and a hybrid decoder reconstructs signals in both temporal and\nfrequency domains. During pretraining, ALFEE optimizes task prediction, channel\nand temporal mask reconstruction, and temporal forecasting to enhance\nmulti-scale and multi-channel representation. During fine-tuning, a full-model\nadaptation with a task-specific token dictionary and a cross-attention layer\nboosts performance across multiple tasks. After 25,000 hours of pretraining,\nextensive experimental results on six downstream EEG tasks demonstrate the\nsuperior performance of ALFEE over existing models. Our ALFEE framework\nestablishes a scalable foundation for biological signal analysis with\nimplementation at https://github.com/xw1216/ALFEE."}
{"id": "2312.02156", "pdf": "https://arxiv.org/pdf/2312.02156", "abs": "https://arxiv.org/abs/2312.02156", "authors": ["Kangfu Mei", "Luis Figueroa", "Zhe Lin", "Zhihong Ding", "Scott Cohen", "Vishal M. Patel"], "title": "Latent Feature-Guided Diffusion Models for Shadow Removal", "categories": ["cs.CV", "cs.AI"], "comment": "project page see https://kfmei.com/shadow-diffusion/", "summary": "Recovering textures under shadows has remained a challenging problem due to\nthe difficulty of inferring shadow-free scenes from shadow images. In this\npaper, we propose the use of diffusion models as they offer a promising\napproach to gradually refine the details of shadow regions during the diffusion\nprocess. Our method improves this process by conditioning on a learned latent\nfeature space that inherits the characteristics of shadow-free images, thus\navoiding the limitation of conventional methods that condition on degraded\nimages only. Additionally, we propose to alleviate potential local optima\nduring training by fusing noise features with the diffusion network. We\ndemonstrate the effectiveness of our approach which outperforms the previous\nbest method by 13% in terms of RMSE on the AISTD dataset. Further, we explore\ninstance-level shadow removal, where our model outperforms the previous best\nmethod by 82% in terms of RMSE on the DESOBA dataset."}
{"id": "2412.19507", "pdf": "https://arxiv.org/pdf/2412.19507", "abs": "https://arxiv.org/abs/2412.19507", "authors": ["Zhaolong Ling", "Honghui Peng", "Yiwen Zhang", "Debo Cheng", "Xingyu Wu", "Peng Zhou", "Kui Yu"], "title": "Hybrid Local Causal Discovery", "categories": ["cs.AI"], "comment": "This paper has been accepted for publication in the Proceedings of\n  the 34th International Joint Conference on Artificial Intelligence (IJCAI\n  2025)", "summary": "Local causal discovery aims to learn and distinguish the direct causes and\neffects of a target variable from observed data. Existing constraint-based\nlocal causal discovery methods use AND or OR rules in constructing the local\ncausal skeleton, but using either rule alone is prone to produce cascading\nerrors in the learned local causal skeleton, and thus impacting the inference\nof local causal relationships. On the other hand, directly applying score-based\nglobal causal discovery methods to local causal discovery may randomly return\nincorrect results due to the existence of local equivalence classes. To address\nthe above issues, we propose a Hybrid Local Causal Discovery algorithm, called\nHLCD. Specifically, HLCD initially utilizes a constraint-based approach\ncombined with the OR rule to obtain a candidate skeleton and then employs a\nscore-based method to eliminate redundant portions in the candidate skeleton.\nFurthermore, during the local causal orientation phase, HLCD distinguishes\nbetween V-structures and equivalence classes by comparing the local structure\nscores between the two, thereby avoiding orientation interference caused by\nlocal equivalence classes. We conducted extensive experiments with seven\nstate-of-the-art competitors on 14 benchmark Bayesian network datasets, and the\nexperimental results demonstrate that HLCD significantly outperforms existing\nlocal causal discovery algorithms."}
{"id": "2505.06310", "pdf": "https://arxiv.org/pdf/2505.06310", "abs": "https://arxiv.org/abs/2505.06310", "authors": ["Tao Shen", "Jethro Browell", "Daniela Castro-Camilo"], "title": "Adaptive Bayesian Very Short-Term Wind Power Forecasting Based on the Generalised Logit Transformation", "categories": ["stat.AP", "cs.LG"], "comment": "31 pages, 10 figures and tables. Submitted to International Journal\n  of Forecasting", "summary": "Wind power plays an increasingly significant role in achieving the 2050 Net\nZero Strategy. Despite its rapid growth, its inherent variability presents\nchallenges in forecasting. Accurately forecasting wind power generation is one\nkey demand for the stable and controllable integration of renewable energy into\nexisting grid operations. This paper proposes an adaptive method for very\nshort-term forecasting that combines the generalised logit transformation with\na Bayesian approach. The generalised logit transformation processes\ndouble-bounded wind power data to an unbounded domain, facilitating the\napplication of Bayesian methods. A novel adaptive mechanism for updating the\ntransformation shape parameter is introduced to leverage Bayesian updates by\nrecovering a small sample of representative data. Four adaptive forecasting\nmethods are investigated, evaluating their advantages and limitations through\nan extensive case study of over 100 wind farms ranging four years in the UK.\nThe methods are evaluated using the Continuous Ranked Probability Score and we\npropose the use of functional reliability diagrams to assess calibration.\nResults indicate that the proposed Bayesian method with adaptive shape\nparameter updating outperforms benchmarks, yielding consistent improvements in\nCRPS and forecast reliability. The method effectively addresses uncertainty,\nensuring robust and accurate probabilistic forecasting which is essential for\ngrid integration and decision-making."}
{"id": "2312.14999", "pdf": "https://arxiv.org/pdf/2312.14999", "abs": "https://arxiv.org/abs/2312.14999", "authors": ["Tin Nguyen", "Peijie Chen", "Anh Totti Nguyen"], "title": "Leveraging Habitat Information for Fine-grained Bird Identification", "categories": ["cs.CV"], "comment": null, "summary": "Traditional bird classifiers mostly rely on the visual characteristics of\nbirds. Some prior works even train classifiers to be invariant to the\nbackground, completely discarding the living environment of birds. Instead, we\nare the first to explore integrating habitat information, one of the four major\ncues for identifying birds by ornithologists, into modern bird classifiers. We\nfocus on two leading model types: (1) CNNs and ViTs trained on the downstream\nbird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with\nhabitat-augmented data results in an improvement of up to +0.83 and +0.23\npoints on NABirds and CUB-200, respectively. Similarly, adding habitat\ndescriptors to the prompts for CLIP yields a substantial accuracy boost of up\nto +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find\nconsistent accuracy improvement after integrating habitat features into the\nimage augmentation process and into the textual descriptors of vision-language\nCLIP classifiers. Code is available at:\nhttps://anonymous.4open.science/r/reasoning-8B7E/."}
{"id": "2501.18009", "pdf": "https://arxiv.org/pdf/2501.18009", "abs": "https://arxiv.org/abs/2501.18009", "authors": ["Lan Pan", "Hanbo Xie", "Robert C. Wilson"], "title": "Large Language Models Think Too Fast To Explore Effectively", "categories": ["cs.AI", "q-bio.NC"], "comment": "21 pages, 16 figures, under review", "summary": "Large Language Models (LLMs) have emerged with many intellectual capacities.\nWhile numerous benchmarks assess their intelligence, limited attention has been\ngiven to their ability to explore--an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with\ntraditional LLMs relying primarily on uncertainty-driven strategies, unlike\nhumans who balance uncertainty and empowerment. Results indicate that\ntraditional reasoning-focused LLMs, such as GPT-4o, exhibit a significantly\nfaster and less detailed reasoning process, limiting their exploratory\nperformance. In contrast, the DeepSeek reasoning model demonstrates prolonged,\niterative thought processes marked by repetitive analysis of combinations and\npast trials, reflecting a more thorough and human-like exploration strategy.\nRepresentational analysis of the models with Sparse Autoencoders (SAE) revealed\nthat uncertainty and choices are represented at earlier transformer blocks,\nwhile empowerment values are processed later, causing LLMs to think too fast\nand make premature decisions, hindering effective exploration. These findings\nshed light on the limitations of LLM exploration and suggest directions for\nimproving their adaptability."}
{"id": "2505.06375", "pdf": "https://arxiv.org/pdf/2505.06375", "abs": "https://arxiv.org/abs/2505.06375", "authors": ["Nahshon Mokua Obiri", "Kristof Van Laerhoven"], "title": "A Comprehensive Data Description for LoRaWAN Path Loss Measurements in an Indoor Office Setting: Effects of Environmental Factors", "categories": ["cs.NI", "cs.AR", "cs.LG", "eess.SP"], "comment": "This is a peer-reviewed article with the help of IEEE Access editors.\n  The relevant DOI will be availed soon", "summary": "This paper presents a comprehensive dataset of LoRaWAN technology path loss\nmeasurements collected in an indoor office environment, focusing on quantifying\nthe effects of environmental factors on signal propagation. Utilizing a network\nof six strategically placed LoRaWAN end devices (EDs) and a single indoor\ngateway (GW) at the University of Siegen, City of Siegen, Germany, we\nsystematically measured signal strength indicators such as the Received Signal\nStrength Indicator (RSSI) and the Signal-to-Noise Ratio (SNR) under various\nenvironmental conditions, including temperature, relative humidity, carbon\ndioxide (CO$_2$) concentration, barometric pressure, and particulate matter\nlevels (PM$_{2.5}$). Our empirical analysis confirms that transient phenomena\nsuch as reflections, scattering, interference, occupancy patterns (induced by\nenvironmental parameter variations), and furniture rearrangements can alter\nsignal attenuation by as much as 10.58 dB, highlighting the dynamic nature of\nindoor propagation. As an example of how this dataset can be utilized, we\ntested and evaluated a refined Log-Distance Path Loss and Shadowing Model that\nintegrates both structural obstructions (Multiple Walls) and Environmental\nParameters (LDPLSM-MW-EP). Compared to a baseline model that considers only\nMultiple Walls (LDPLSM-MW), the enhanced approach reduced the root mean square\nerror (RMSE) from 10.58 dB to 8.04 dB and increased the coefficient of\ndetermination (R$^2$) from 0.6917 to 0.8222. By capturing the extra effects of\nenvironmental conditions and occupancy dynamics, this improved model provides\nvaluable insights for optimizing power usage and prolonging device battery\nlife, enhancing network reliability in indoor Internet of Things (IoT)\ndeployments, among other applications. This dataset offers a solid foundation\nfor future research and development in indoor wireless communication."}
{"id": "2401.13174", "pdf": "https://arxiv.org/pdf/2401.13174", "abs": "https://arxiv.org/abs/2401.13174", "authors": ["Dong Zhang", "Pingcheng Dong", "Long Chen", "Kwang-Ting Cheng"], "title": "Towards Complementary Knowledge Distillation for Efficient Dense Image Prediction", "categories": ["cs.CV"], "comment": "under submission", "summary": "It has been revealed that small efficient dense image prediction (EDIP)\nmodels, trained using the knowledge distillation (KD) framework, encounter two\nkey challenges, including maintaining boundary region completeness and\npreserving target region connectivity, despite their favorable capacity to\nrecognize main object regions. In this work, we propose a complementary\nboundary and context distillation (BCD) method within the KD framework for\nEDIPs, which facilitates the targeted knowledge transfer from large accurate\nteacher models to compact efficient student models. Specifically, the boundary\ndistillation component focuses on extracting explicit object-level semantic\nboundaries from the hierarchical feature maps of the backbone network to\nenhance the student model's mask quality in boundary regions. Concurrently, the\ncontext distillation component leverages self-relations as a bridge to transfer\nimplicit pixel-level contexts from the teacher model to the student model,\nensuring strong connectivity in target regions. Our proposed BCD method is\nspecifically designed for EDIP tasks and is characterized by its simplicity and\nefficiency. Extensive experimental results across semantic segmentation, object\ndetection, and instance segmentation on various representative datasets\ndemonstrate that our method can outperform existing methods without requiring\nextra supervisions or incurring increased inference costs, resulting in\nwell-defined object boundaries and smooth connecting regions."}
{"id": "2501.19112", "pdf": "https://arxiv.org/pdf/2501.19112", "abs": "https://arxiv.org/abs/2501.19112", "authors": ["Lara Lawniczak", "Christoph Benzmüller"], "title": "Logical Modalities within the European AI Act: An Analysis", "categories": ["cs.AI", "cs.CY", "cs.LO", "68T01 68T27 68T30 03Axx 03B16 03B35 03B45 03B60 03B70", "I.2.0; I.2.3; I.2.4; J.1"], "comment": "Extended preprint of paper accepted for ICAIL 2025; 15 pages, 19\n  figures", "summary": "The paper presents a comprehensive analysis of the European AI Act in terms\nof its logical modalities, with the aim of preparing its formal representation,\nfor example, within the logic-pluralistic Knowledge Engineering Framework and\nMethodology (LogiKEy). LogiKEy develops computational tools for normative\nreasoning based on formal methods, employing Higher-Order Logic (HOL) as a\nunifying meta-logic to integrate diverse logics through shallow semantic\nembeddings. This integration is facilitated by Isabelle/HOL, a proof assistant\ntool equipped with several automated theorem provers. The modalities within the\nAI Act and the logics suitable for their representation are discussed. For a\nselection of these logics, embeddings in HOL are created, which are then used\nto encode sample paragraphs. Initial experiments evaluate the suitability of\nthese embeddings for automated reasoning, and highlight key challenges on the\nway to more robust reasoning capabilities."}
{"id": "2505.06386", "pdf": "https://arxiv.org/pdf/2505.06386", "abs": "https://arxiv.org/abs/2505.06386", "authors": ["Donghao Ren", "Fred Hohman", "Halden Lin", "Dominik Moritz"], "title": "Embedding Atlas: Low-Friction, Interactive Embedding Visualization", "categories": ["cs.HC", "cs.LG"], "comment": "Website: https://apple.github.io/embedding-atlas/", "summary": "Embedding projections are popular for visualizing large datasets and models.\nHowever, people often encounter \"friction\" when using embedding visualization\ntools: (1) barriers to adoption, e.g., tedious data wrangling and loading,\nscalability limits, no integration of results into existing workflows, and (2)\nlimitations in possible analyses, without integration with external tools to\nadditionally show coordinated views of metadata. In this paper, we present\nEmbedding Atlas, a scalable, interactive visualization tool designed to make\ninteracting with large embeddings as easy as possible. Embedding Atlas uses\nmodern web technologies and advanced algorithms -- including density-based\nclustering, and automated labeling -- to provide a fast and rich data analysis\nexperience at scale. We evaluate Embedding Atlas with a competitive analysis\nagainst other popular embedding tools, showing that Embedding Atlas's feature\nset specifically helps reduce friction, and report a benchmark on its real-time\nrendering performance with millions of points. Embedding Atlas is available as\nopen source to support future work in embedding-based analysis."}
{"id": "2404.04856", "pdf": "https://arxiv.org/pdf/2404.04856", "abs": "https://arxiv.org/abs/2404.04856", "authors": ["Chenguang Liu", "Chisheng Wang", "Feifei Dong", "Xiayang Xiao", "Xin Su", "Chuanhua Zhu", "Dejin Zhang", "Qingquan Li"], "title": "Msmsfnet: a multi-stream and multi-scale fusion net for edge detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Edge detection is a long-standing problem in computer vision. Despite the\nefficiency of existing algorithms, their performance, however, rely heavily on\nthe pre-trained weights of the backbone network on the ImageNet dataset. The\nuse of pre-trained weights in previous methods significantly increases the\ndifficulty to design new models for edge detection without relying on existing\nwell-trained ImageNet models, as pre-training the model on the ImageNet dataset\nis expensive and becomes compulsory to ensure the fairness of comparison.\nBesides, the pre-training and fine-tuning strategy is not always useful and\nsometimes even inaccessible. For instance, the pre-trained weights on the\nImageNet dataset are unlikely to be helpful for edge detection in Synthetic\nAperture Radar (SAR) images due to strong differences in the statistics between\noptical images and SAR images. Moreover, no dataset has comparable size to the\nImageNet dataset for SAR image processing. In this work, we study the\nperformance achievable by state-of-the-art deep learning based edge detectors\nin publicly available datasets when they are trained from scratch, and devise a\nnew network architecture, the multi-stream and multi-scale fusion net\n(msmsfnet), for edge detection. We show in our experiments that by training all\nmodels from scratch, our model outperforms state-of-the-art edge detectors in\nthree publicly available datasets. We also demonstrate the efficiency of our\nmodel for edge detection in SAR images, where no useful pre-trained weight is\navailable. Finally, We show that our model is able to achieve competitive\nperformance on the BSDS500 dataset when the pre-trained weights are used."}
{"id": "2502.06655", "pdf": "https://arxiv.org/pdf/2502.06655", "abs": "https://arxiv.org/abs/2502.06655", "authors": ["Meilin Chen", "Jian Tian", "Liang Ma", "Di Xie", "Weijie Chen", "Jiang Zhu"], "title": "Unbiased Evaluation of Large Language Models from a Causal Perspective", "categories": ["cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Benchmark contamination has become a significant concern in the LLM\nevaluation community. Previous Agents-as-an-Evaluator address this issue by\ninvolving agents in the generation of questions. Despite their success, the\nbiases in Agents-as-an-Evaluator methods remain largely unexplored. In this\npaper, we present a theoretical formulation of evaluation bias, providing\nvaluable insights into designing unbiased evaluation protocols. Furthermore, we\nidentify two type of bias in Agents-as-an-Evaluator through carefully designed\nprobing tasks on a minimal Agents-as-an-Evaluator setup. To address these\nissues, we propose the Unbiased Evaluator, an evaluation protocol that delivers\na more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive\nexperiments reveal significant room for improvement in current LLMs.\nAdditionally, we demonstrate that the Unbiased Evaluator not only offers strong\nevidence of benchmark contamination but also provides interpretable evaluation\nresults."}
{"id": "2505.06407", "pdf": "https://arxiv.org/pdf/2505.06407", "abs": "https://arxiv.org/abs/2505.06407", "authors": ["Ramin Esmzad", "Gokul S. Sankar", "Teawon Han", "Hamidreza Modares"], "title": "Direct Data Driven Control Using Noisy Measurements", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.OC"], "comment": "Submitted to IEEE-TAC", "summary": "This paper presents a novel direct data-driven control framework for solving\nthe linear quadratic regulator (LQR) under disturbances and noisy state\nmeasurements. The system dynamics are assumed unknown, and the LQR solution is\nlearned using only a single trajectory of noisy input-output data while\nbypassing system identification. Our approach guarantees mean-square stability\n(MSS) and optimal performance by leveraging convex optimization techniques that\nincorporate noise statistics directly into the controller synthesis. First, we\nestablish a theoretical result showing that the MSS of an uncertain data-driven\nsystem implies the MSS of the true closed-loop system. Building on this, we\ndevelop a robust stability condition using linear matrix inequalities (LMIs)\nthat yields a stabilizing controller gain from noisy measurements. Finally, we\nformulate a data-driven LQR problem as a semidefinite program (SDP) that\ncomputes an optimal gain, minimizing the steady-state covariance. Extensive\nsimulations on benchmark systems -- including a rotary inverted pendulum and an\nactive suspension system -- demonstrate the superior robustness and accuracy of\nour method compared to existing data-driven LQR approaches. The proposed\nframework offers a practical and theoretically grounded solution for controller\ndesign in noise-corrupted environments where system identification is\ninfeasible."}
{"id": "2407.01330", "pdf": "https://arxiv.org/pdf/2407.01330", "abs": "https://arxiv.org/abs/2407.01330", "authors": ["Jiangbei Hu", "Yanggeng Li", "Fei Hou", "Junhui Hou", "Zhebin Zhang", "Shengfa Wang", "Na Lei", "Ying He"], "title": "A Lightweight UDF Learning Framework for 3D Reconstruction Based on Local Shape Functions", "categories": ["cs.CV", "I.3.5"], "comment": "11 pages, 10 figures", "summary": "Unsigned distance fields (UDFs) provide a versatile framework for\nrepresenting a diverse array of 3D shapes, encompassing both watertight and\nnon-watertight geometries. Traditional UDF learning methods typically require\nextensive training on large 3D shape datasets, which is costly and necessitates\nre-training for new datasets. This paper presents a novel neural framework,\nLoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging local\nshape functions to learn UDFs. We observe that 3D shapes manifest simple\npatterns in localized regions, prompting us to develop a training dataset of\npoint cloud patches characterized by mathematical functions that represent a\ncontinuum from smooth surfaces to sharp edges and corners. Our approach learns\nfeatures within a specific radius around each query point and utilizes an\nattention mechanism to focus on the crucial features for UDF estimation.\nDespite being highly lightweight, with only 653 KB of trainable parameters and\na modest-sized training dataset with 0.5 GB storage, our method enables\nefficient and robust surface reconstruction from point clouds without requiring\nfor shape-specific training. Furthermore, our method exhibits enhanced\nresilience to noise and outliers in point clouds compared to existing methods.\nWe conduct comprehensive experiments and comparisons across various datasets,\nincluding synthetic and real-scanned point clouds, to validate our method's\nefficacy. Notably, our lightweight framework offers rapid and reliable\ninitialization for other unsupervised iterative approaches, improving both the\nefficiency and accuracy of their reconstructions. Our project and code are\navailable at https://jbhu67.github.io/LoSF-UDF.github.io."}
{"id": "2502.10931", "pdf": "https://arxiv.org/pdf/2502.10931", "abs": "https://arxiv.org/abs/2502.10931", "authors": ["Meet Udeshi", "Minghao Shao", "Haoran Xi", "Nanda Rani", "Kimberly Milner", "Venkata Sai Charan Putrevu", "Brendan Dolan-Gavitt", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "title": "D-CIPHER: Dynamic Collaborative Intelligent Multi-Agent System with Planner and Heterogeneous Executors for Offensive Security", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) have been used in cybersecurity such as\nautonomous security analysis or penetration testing. Capture the Flag (CTF)\nchallenges serve as benchmarks to assess automated task-planning abilities of\nLLM agents for cybersecurity. Early attempts to apply LLMs for solving CTF\nchallenges used single-agent systems, where feedback was restricted to a single\nreasoning-action loop. This approach was inadequate for complex CTF tasks.\nInspired by real-world CTF competitions, where teams of experts collaborate, we\nintroduce the D-CIPHER LLM multi-agent framework for collaborative CTF solving.\nD-CIPHER integrates agents with distinct roles with dynamic feedback loops to\nenhance reasoning on complex tasks. It introduces the Planner-Executor agent\nsystem, consisting of a Planner agent for overall problem-solving along with\nmultiple heterogeneous Executor agents for individual tasks, facilitating\nefficient allocation of responsibilities among the agents. Additionally,\nD-CIPHER incorporates an Auto-prompter agent to improve problem-solving by\nauto-generating a highly relevant initial prompt. We evaluate D-CIPHER on\nmultiple CTF benchmarks and LLM models via comprehensive studies to highlight\nthe impact of our enhancements. Additionally, we manually map the CTFs in NYU\nCTF Bench to MITRE ATT&CK techniques that apply for a comprehensive evaluation\nof D-CIPHER's offensive security capability. D-CIPHER achieves state-of-the-art\nperformance on three benchmarks: 22.0% on NYU CTF Bench, 22.5% on Cybench, and\n44.0% on HackTheBox, which is 2.5% to 8.5% better than previous work. D-CIPHER\nsolves 65% more ATT&CK techniques compared to previous work, demonstrating\nstronger offensive capability."}
{"id": "2505.06435", "pdf": "https://arxiv.org/pdf/2505.06435", "abs": "https://arxiv.org/abs/2505.06435", "authors": ["Insung Kong", "Kunwoong Kim", "Yongdai Kim"], "title": "Fair Representation Learning for Continuous Sensitive Attributes using Expectation of Integral Probability Metrics", "categories": ["stat.ML", "cs.LG"], "comment": "42 pages, 30 figures. IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (2025)", "summary": "AI fairness, also known as algorithmic fairness, aims to ensure that\nalgorithms operate without bias or discrimination towards any individual or\ngroup. Among various AI algorithms, the Fair Representation Learning (FRL)\napproach has gained significant interest in recent years. However, existing FRL\nalgorithms have a limitation: they are primarily designed for categorical\nsensitive attributes and thus cannot be applied to continuous sensitive\nattributes, such as age or income. In this paper, we propose an FRL algorithm\nfor continuous sensitive attributes. First, we introduce a measure called the\nExpectation of Integral Probability Metrics (EIPM) to assess the fairness level\nof representation space for continuous sensitive attributes. We demonstrate\nthat if the distribution of the representation has a low EIPM value, then any\nprediction head constructed on the top of the representation become fair,\nregardless of the selection of the prediction head. Furthermore, EIPM possesses\na distinguished advantage in that it can be accurately estimated using our\nproposed estimator with finite samples. Based on these properties, we propose a\nnew FRL algorithm called Fair Representation using EIPM with MMD (FREM).\nExperimental evidences show that FREM outperforms other baseline methods."}
{"id": "2407.13120", "pdf": "https://arxiv.org/pdf/2407.13120", "abs": "https://arxiv.org/abs/2407.13120", "authors": ["Shuchang Zhang", "Hui Zhang", "Hongxia Wang"], "title": "HPPP: Halpern-type Preconditioned Proximal Point Algorithms and Applications to Image Restoration", "categories": ["cs.CV", "math.OC"], "comment": null, "summary": "Recently, the degenerate preconditioned proximal point (PPP) method provides\na unified and flexible framework for designing and analyzing operator-splitting\nalgorithms such as Douglas-Rachford (DR). However, the degenerate PPP method\nexhibits weak convergence in the infinite-dimensional Hilbert space and lacks\naccelerated variants. To address these issues, we propose a Halpern-type PPP\n(HPPP) algorithm, which leverages the strong convergence and acceleration\nproperties of Halpern's iteration method. Moreover, we propose a novel\nalgorithm for image restoration by combining HPPP with denoiser priors such as\nPlug-and-Play (PnP) prior, which can be viewed as an accelerated PnP method.\nFinally, numerical experiments including several toy examples and image\nrestoration validate the effectiveness of our proposed algorithms."}
{"id": "2503.15752", "pdf": "https://arxiv.org/pdf/2503.15752", "abs": "https://arxiv.org/abs/2503.15752", "authors": ["Yutong Xie", "Qiaozhu Mei", "Walter Yuan", "Matthew O. Jackson"], "title": "Using Language Models to Decipher the Motivation Behind Human Behaviors", "categories": ["cs.AI"], "comment": null, "summary": "AI presents a novel tool for deciphering the motivations behind human\nbehaviors. By varying prompts to a large language model, we can elicit the full\nrange of human behaviors in a variety of different scenarios in classic\neconomic games. By analyzing which prompts elicit which behaviors, we infer\n(decipher) the motivations behind the human behaviors. We also show how one can\nanalyze the prompts to reveal relationships between the classic economic games,\nproviding insight into what different economic scenarios induce people to think\nabout. We also show how this deciphering process can be used to understand\ndifferences in the behavioral tendencies of different populations. We show how\nAI offers a new way to examine the thinking and framing that produce different\nbehaviors."}
{"id": "2505.06461", "pdf": "https://arxiv.org/pdf/2505.06461", "abs": "https://arxiv.org/abs/2505.06461", "authors": ["Haolin Zhang", "Jeff Huang"], "title": "Challenging GPU Dominance: When CPUs Outperform for On-Device LLM Inference", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "The common assumption in on-device AI is that GPUs, with their superior\nparallel processing, always provide the best performance for large language\nmodel (LLM) inference. In this work, we challenge this notion by empirically\ndemonstrating that, under certain conditions, CPUs can outperform GPUs for LLM\ninference on mobile devices. Using a 1-billion-parameter LLM deployed via\nllama.cpp on the iPhone 15 Pro, we show that a CPU-only configuration (two\nthreads, F16 precision) achieves 17 tokens per second, surpassing the 12.8\ntokens per second obtained with GPU acceleration. We analyze the architectural\nfactors driving this counterintuitive result, revealing that GPU memory\ntransfer overhead and CPU thread optimization play a critical role.\nFurthermore, we explore the impact of thread oversubscription, quantization\nstrategies, and hardware constraints, providing new insights into efficient\non-device AI execution. Our findings challenge conventional GPU-first thinking,\nhighlighting the untapped potential of optimized CPU inference and paving the\nway for smarter deployment strategies in mobile AI. However, fully explaining\nthe observed CPU advantage remains difficult due to limited access to low-level\nprofiling tools on iOS."}
{"id": "2407.18715", "pdf": "https://arxiv.org/pdf/2407.18715", "abs": "https://arxiv.org/abs/2407.18715", "authors": ["Peng Hao", "Weilong Wang", "Xiaobing Wang", "Yingying Jiang", "Hanchao Jia", "Shaowei Cui", "Junhang Wei", "Xiaoshuai Hao"], "title": "BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation", "categories": ["cs.CV"], "comment": "16 pages, 4 figures", "summary": "Scene Graph Generation (SGG) remains a challenging task due to its\ncompositional property. Previous approaches improve prediction efficiency\nthrough end-to-end learning. However, these methods exhibit limited performance\nas they assume unidirectional conditioning between entities and predicates,\nwhich restricts effective information interaction. To address this limitation,\nwe propose a novel bidirectional conditioning factorization in a\nsemantic-aligned space for SGG, enabling efficient and generalizable\ninteraction between entities and predicates. Specifically, we introduce an\nend-to-end scene graph generation model, the Bidirectional Conditioning\nTransformer (BCTR), to implement this factorization. BCTR consists of two key\nmodules. First, the Bidirectional Conditioning Generator (BCG) performs\nmulti-stage interactive feature augmentation between entities and predicates,\nenabling mutual enhancement between these predictions. Second, Random Feature\nAlignment (RFA) is present to regularize feature space by distilling\nmulti-modal knowledge from pre-trained models. Within this regularized feature\nspace, BCG is feasible to capture interaction patterns across diverse\nrelationships during training, and the learned interaction patterns can\ngeneralize to unseen but semantically related relationships during inference.\nExtensive experiments on Visual Genome and Open Image V6 show that BCTR\nachieves state-of-the-art performance on both benchmarks."}
{"id": "2503.18938", "pdf": "https://arxiv.org/pdf/2503.18938", "abs": "https://arxiv.org/abs/2503.18938", "authors": ["Shenyuan Gao", "Siyuan Zhou", "Yilun Du", "Jun Zhang", "Chuang Gan"], "title": "AdaWorld: Learning Adaptable World Models with Latent Actions", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "ICML 2025. Project page: https://adaptable-world-model.github.io/,\n  code and model: https://github.com/Little-Podi/AdaWorld", "summary": "World models aim to learn action-controlled future prediction and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this limitation, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning."}
{"id": "2505.06531", "pdf": "https://arxiv.org/pdf/2505.06531", "abs": "https://arxiv.org/abs/2505.06531", "authors": ["Yong-Syun Cao", "Shinpei Imori", "Ching-Kang Ing"], "title": "High-Dimensional Importance-Weighted Information Criteria: Theory and Optimality", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Imori and Ing (2025) proposed the importance-weighted orthogonal greedy\nalgorithm (IWOGA) for model selection in high-dimensional misspecified\nregression models under covariate shift. To determine the number of IWOGA\niterations, they introduced the high-dimensional importance-weighted\ninformation criterion (HDIWIC). They argued that the combined use of IWOGA and\nHDIWIC, IWOGA + HDIWIC, achieves an optimal trade-off between variance and\nsquared bias, leading to optimal convergence rates in terms of conditional mean\nsquared prediction error. In this article, we provide a theoretical\njustification for this claim by establishing the optimality of IWOGA + HDIWIC\nunder a set of reasonable assumptions."}
{"id": "2408.12232", "pdf": "https://arxiv.org/pdf/2408.12232", "abs": "https://arxiv.org/abs/2408.12232", "authors": ["Hanzheng Wang", "Wei Li", "Xiang-Gen Xia", "Qian Du"], "title": "BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking", "categories": ["cs.CV"], "comment": "IEEE Transactions on Neural Networks and Learning Systems, 2025", "summary": "Hyperspectral object tracking (HOT) has exhibited potential in various\napplications, particularly in scenes where objects are camouflaged. Existing\ntrackers can effectively retrieve objects via band regrouping because of the\nbias in existing HOT datasets, where most objects tend to have distinguishing\nvisual appearances rather than spectral characteristics. This bias allows the\ntracker to directly use the visual features obtained from the false-color\nimages generated by hyperspectral images without the need to extract spectral\nfeatures. To tackle this bias, we find that the tracker should focus on the\nspectral information when object appearance is unreliable. Thus, we provide a\nnew task called hyperspectral camouflaged object tracking (HCOT) and\nmeticulously construct a large-scale HCOT dataset, termed BihoT, which consists\nof 41,912 hyperspectral images covering 49 video sequences. The dataset covers\nvarious artificial camouflage scenes where objects have similar appearances,\ndiverse spectrums, and frequent occlusion, making it a very challenging dataset\nfor HCOT. Besides, a simple but effective baseline model, named spectral\nprompt-based distractor-aware network (SPDAN), is proposed, comprising a\nspectral embedding network (SEN), a spectral prompt-based backbone network\n(SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts\nspectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN\nfine-tunes powerful RGB trackers with spectral prompts and alleviates the\ninsufficiency of training samples. Moreover, the DAM utilizes a novel statistic\nto capture the distractor caused by occlusion from objects and background.\nExtensive experiments demonstrate that our proposed SPDAN achieves\nstate-of-the-art performance on the proposed BihoT and other HOT datasets."}
{"id": "2503.21138", "pdf": "https://arxiv.org/pdf/2503.21138", "abs": "https://arxiv.org/abs/2503.21138", "authors": ["Hedong Yan"], "title": "A Computational Theory for Efficient Mini Agent Evaluation with Causal Guarantees", "categories": ["cs.AI", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "In order to reduce the cost of experimental evaluation for agents, we\nintroduce a computational theory of evaluation for mini agents: build\nevaluation model to accelerate the evaluation procedures. We prove upper bounds\nof generalized error and generalized causal effect error of given evaluation\nmodels for infinite agents. We also prove efficiency, and consistency to\nestimated causal effect from deployed agents to evaluation metric by\nprediction. To learn evaluation models, we propose a meta-learner to handle\nheterogeneous agents space problem. Comparing with existed evaluation\napproaches, our (conditional) evaluation model reduced 24.1\\% to 99.0\\%\nevaluation errors across 12 scenes, including individual medicine, scientific\nsimulation, social experiment, business activity, and quantum trade. The\nevaluation time is reduced 3 to 7 order of magnitude per subject comparing with\nexperiments or simulations."}
{"id": "2505.06601", "pdf": "https://arxiv.org/pdf/2505.06601", "abs": "https://arxiv.org/abs/2505.06601", "authors": ["Yuanhang Luo", "Yeheng Ge", "Ruijian Han", "Guohao Shen"], "title": "Learning Guarantee of Reward Modeling Using Deep Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In this work, we study the learning theory of reward modeling with pairwise\ncomparison data using deep neural networks. We establish a novel non-asymptotic\nregret bound for deep reward estimators in a non-parametric setting, which\ndepends explicitly on the network architecture. Furthermore, to underscore the\ncritical importance of clear human beliefs, we introduce a margin-type\ncondition that assumes the conditional winning probability of the optimal\naction in pairwise comparisons is significantly distanced from 1/2. This\ncondition enables a sharper regret bound, which substantiates the empirical\nefficiency of Reinforcement Learning from Human Feedback and highlights clear\nhuman beliefs in its success. Notably, this improvement stems from high-quality\npairwise comparison data implied by the margin-type condition, is independent\nof the specific estimators used, and thus applies to various learning\nalgorithms and models."}
{"id": "2408.13877", "pdf": "https://arxiv.org/pdf/2408.13877", "abs": "https://arxiv.org/abs/2408.13877", "authors": ["Xiaoyu Guo", "Pengzhi Zhong", "Hao Zhang", "Defeng Huang", "Huikai Shao", "Qijun Zhao", "Shuiwang Li"], "title": "Camouflaged Object Tracking: A Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Visual tracking has seen remarkable advancements, largely driven by the\navailability of large-scale training datasets that have enabled the development\nof highly accurate and robust algorithms. While significant progress has been\nmade in tracking general objects, research on more challenging scenarios, such\nas tracking camouflaged objects, remains limited. Camouflaged objects, which\nblend seamlessly with their surroundings or other objects, present unique\nchallenges for detection and tracking in complex environments. This challenge\nis particularly critical in applications such as military, security,\nagriculture, and marine monitoring, where precise tracking of camouflaged\nobjects is essential. To address this gap, we introduce the Camouflaged Object\nTracking Dataset (COTD), a specialized benchmark designed specifically for\nevaluating camouflaged object tracking methods. The COTD dataset comprises 200\nsequences and approximately 80,000 frames, each annotated with detailed\nbounding boxes. Our evaluation of 20 existing tracking algorithms reveals\nsignificant deficiencies in their performance with camouflaged objects. To\naddress these issues, we propose a novel tracking framework, HiPTrack-MLS,\nwhich demonstrates promising results in improving tracking performance for\ncamouflaged objects. COTD and code are avialable at\nhttps://github.com/openat25/HIPTrack-MLS."}
{"id": "2503.23350", "pdf": "https://arxiv.org/pdf/2503.23350", "abs": "https://arxiv.org/abs/2503.23350", "authors": ["Liangbo Ning", "Ziran Liang", "Zhuohang Jiang", "Haohao Qu", "Yujuan Ding", "Wenqi Fan", "Xiao-yong Wei", "Shanru Lin", "Hui Liu", "Philip S. Yu", "Qing Li"], "title": "A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models", "categories": ["cs.AI"], "comment": "Accepted by KDD 2025;", "summary": "With the advancement of web techniques, they have significantly\nrevolutionized various aspects of people's lives. Despite the importance of the\nweb, many tasks performed on it are repetitive and time-consuming, negatively\nimpacting overall quality of life. To efficiently handle these tedious daily\ntasks, one of the most promising approaches is to advance autonomous agents\nbased on Artificial Intelligence (AI) techniques, referred to as AI Agents, as\nthey can operate continuously without fatigue or performance degradation. In\nthe context of the web, leveraging AI Agents -- termed WebAgents -- to\nautomatically assist people in handling tedious daily tasks can dramatically\nenhance productivity and efficiency. Recently, Large Foundation Models (LFMs)\ncontaining billions of parameters have exhibited human-like language\nunderstanding and reasoning capabilities, showing proficiency in performing\nvarious complex tasks. This naturally raises the question: `Can LFMs be\nutilized to develop powerful AI Agents that automatically handle web tasks,\nproviding significant convenience to users?' To fully explore the potential of\nLFMs, extensive research has emerged on WebAgents designed to complete daily\nweb tasks according to user instructions, significantly enhancing the\nconvenience of daily human life. In this survey, we comprehensively review\nexisting research studies on WebAgents across three key aspects: architectures,\ntraining, and trustworthiness. Additionally, several promising directions for\nfuture research are explored to provide deeper insights."}
{"id": "2505.06701", "pdf": "https://arxiv.org/pdf/2505.06701", "abs": "https://arxiv.org/abs/2505.06701", "authors": ["Akansha Shukla", "Parth Atulbhai Gandhi", "Yuval Elovici", "Asaf Shabtai"], "title": "RuleGenie: SIEM Detection Rule Set Optimization", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "SIEM systems serve as a critical hub, employing rule-based logic to detect\nand respond to threats. Redundant or overlapping rules in SIEM systems lead to\nexcessive false alerts, degrading analyst performance due to alert fatigue, and\nincrease computational overhead and response latency for actual threats. As a\nresult, optimizing SIEM rule sets is essential for efficient operations.\nDespite the importance of such optimization, research in this area is limited,\nwith current practices relying on manual optimization methods that are both\ntime-consuming and error-prone due to the scale and complexity of\nenterprise-level rule sets. To address this gap, we present RuleGenie, a novel\nlarge language model (LLM) aided recommender system designed to optimize SIEM\nrule sets. Our approach leverages transformer models' multi-head attention\ncapabilities to generate SIEM rule embeddings, which are then analyzed using a\nsimilarity matching algorithm to identify the top-k most similar rules. The LLM\nthen processes the rules identified, utilizing its information extraction,\nlanguage understanding, and reasoning capabilities to analyze rule similarity,\nevaluate threat coverage and performance metrics, and deliver optimized\nrecommendations for refining the rule set. By automating the rule optimization\nprocess, RuleGenie allows security teams to focus on more strategic tasks while\nenhancing the efficiency of SIEM systems and strengthening organizations'\nsecurity posture. We evaluated RuleGenie on a comprehensive set of real-world\nSIEM rule formats, including Splunk, Sigma, and AQL (Ariel query language),\ndemonstrating its platform-agnostic capabilities and adaptability across\ndiverse security infrastructures. Our experimental results show that RuleGenie\ncan effectively identify redundant rules, which in turn decreases false\npositive rates and enhances overall rule efficiency."}
{"id": "2409.00020", "pdf": "https://arxiv.org/pdf/2409.00020", "abs": "https://arxiv.org/abs/2409.00020", "authors": ["Shahab Aldin Shojaeezadeh", "Abdelrazek Elnashar", "Tobias Karl David Weber"], "title": "A novel fusion of Sentinel-1 and Sentinel-2 with climate data for crop phenology estimation using Machine Learning", "categories": ["cs.CV"], "comment": null, "summary": "Crop phenology describes the physiological development stages of crops from\nplanting to harvest which is valuable information for decision makers to plan\nand adapt agricultural management strategies. In the era of big Earth\nobservation data ubiquity, attempts have been made to accurately detect crop\nphenology using Remote Sensing (RS) and high resolution weather data. However,\nmost studies have focused on large scale predictions of phenology or developed\nmethods which are not adequate to help crop modeler communities on leveraging\nSentinel-1 and Sentinal-2 data and fusing them with high resolution climate\ndata, using a novel framework. For this, we trained a Machine Learning (ML)\nLightGBM model to predict 13 phenological stages for eight major crops across\nGermany at 20 m scale. Observed phonologies were taken from German national\nphenology network (German Meteorological Service; DWD) between 2017 and 2021.\nWe proposed a thorough feature selection analysis to find the best combination\nof RS and climate data to detect phenological stages. At national scale,\npredicted phenology resulted in a reasonable precision of R2 > 0.43 and a low\nMean Absolute Error of 6 days, averaged over all phenological stages and crops.\nThe spatio-temporal analysis of the model predictions demonstrates its\ntransferability across different spatial and temporal context of Germany. The\nresults indicated that combining radar sensors with climate data yields a very\npromising performance for a multitude of practical applications. Moreover,\nthese improvements are expected to be useful to generate highly valuable input\nfor crop model calibrations and evaluations, facilitate informed agricultural\ndecisions, and contribute to sustainable food production to address the\nincreasing global food demand."}
{"id": "2504.14379", "pdf": "https://arxiv.org/pdf/2504.14379", "abs": "https://arxiv.org/abs/2504.14379", "authors": ["Andrew Lee", "Lihao Sun", "Chris Wendler", "Fernanda Viégas", "Martin Wattenberg"], "title": "The Geometry of Self-Verification in a Task-Specific Reasoning Model", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "How do reasoning models verify their own answers? We study this question by\ntraining a model using DeepSeek R1's recipe on the CountDown task. We leverage\nthe fact that preference tuning leads to mode collapse, yielding a model that\nalways produces highly structured chain-of-thought sequences. With this setup,\nwe do top-down and bottom-up analyses to reverse-engineer how the model\nverifies its outputs. Top-down, we find Gated Linear Unit (GLU) weights\nencoding verification-related tokens, such as ``success'' or ``incorrect''.\nBottom-up, we find that ``previous-token heads'' are mainly responsible for\nself-verification in our setup. Our analyses meet in the middle: drawing\ninspiration from inter-layer communication channels, we use the identified GLU\nweights to localize as few as three attention heads that can disable\nself-verification, pointing to a necessary component of a potentially larger\nverification circuit. Finally, we verify that similar verification components\nexist in our base model and a general reasoning DeepSeek-R1 model."}
{"id": "2505.06711", "pdf": "https://arxiv.org/pdf/2505.06711", "abs": "https://arxiv.org/abs/2505.06711", "authors": ["Junfan Xia", "Bin Jiang"], "title": "Efficient Parallelization of Message Passing Neural Networks", "categories": ["physics.chem-ph", "cs.LG"], "comment": "33 pages, 8 figures", "summary": "Machine learning potentials have achieved great success in accelerating\natomistic simulations. Many of them rely on local descriptors that readily\nallow parallelization. More recent message passing neural network (MPNN) models\nhave demonstrated their superior accuracy and become increasingly popular.\nHowever, parallelizing MPNN models for large-scale simulations across compute\nnodes remains a challenge, as the previously argued poor scalability with the\nnumber of MP layers and the necessity of data communication. Here, we propose\nan efficient parallel algorithm for MPNN models, in which additional data\ncommunication is minimized among local atoms only in each MP layer without\nredundant computation, thus scaling linearly with the layer number. Integrated\nwith our recursively embedded atom neural network model, this algorithm\ndemonstrates excellent strong scaling and weak scaling behaviors in several\nbenchmark systems. This approach enables massive molecular dynamics simulations\non MPNN models for hundreds of millions of atoms as fast as on strictly local\nmodels, vastly extending the applicability of the MPNN potential to an\nunprecedented scale. This general parallelization framework can empower various\nMPNN models to efficiently simulate very large and complex systems."}
{"id": "2409.00638", "pdf": "https://arxiv.org/pdf/2409.00638", "abs": "https://arxiv.org/abs/2409.00638", "authors": ["Gangwei Xu", "Xianqi Wang", "Zhaoxing Zhang", "Junda Cheng", "Chunyuan Liao", "Xin Yang"], "title": "IGEV++: Iterative Multi-range Geometry Encoding Volumes for Stereo Matching", "categories": ["cs.CV"], "comment": "Accepted by TPAMI 2025", "summary": "Stereo matching is a core component in many computer vision and robotics\nsystems. Despite significant advances over the last decade, handling matching\nambiguities in ill-posed regions and large disparities remains an open\nchallenge. In this paper, we propose a new deep network architecture, called\nIGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range\nGeometry Encoding Volumes (MGEV), which encode coarse-grained geometry\ninformation for ill-posed regions and large disparities, while preserving\nfine-grained geometry information for details and small disparities. To\nconstruct MGEV, we introduce an adaptive patch matching module that efficiently\nand effectively computes matching costs for large disparity ranges and/or\nill-posed regions. We further propose a selective geometry feature fusion\nmodule to adaptively fuse multi-range and multi-granularity geometry features\nin MGEV. Then, we input the fused geometry features into ConvGRUs to\niteratively update the disparity map. MGEV allows to efficiently handle large\ndisparities and ill-posed regions, such as occlusions and textureless regions,\nand enjoys rapid convergence during iterations. Our IGEV++ achieves the best\nperformance on the Scene Flow test set across all disparity ranges, up to\n768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury,\nETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23\\%\n2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury,\nrepresenting error reductions of 31.9\\% and 54.8\\% compared to RAFT-Stereo and\nGMStereo, respectively. We also present a real-time version of IGEV++ that\nachieves the best performance among all published real-time methods on the\nKITTI benchmarks. The code is publicly available at\nhttps://github.com/gangweix/IGEV and https://github.com/gangweix/IGEV-plusplus."}
{"id": "2504.17929", "pdf": "https://arxiv.org/pdf/2504.17929", "abs": "https://arxiv.org/abs/2504.17929", "authors": ["Ayesha Siddique", "Khurram Khalil", "Khaza Anuarul Hoque"], "title": "ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing", "categories": ["cs.AI", "cs.AR"], "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), June 30th - July 5th, 2025 in Rome, Italy", "summary": "Explainable artificial intelligence (XAI) enhances AI system transparency by\nframing interpretability as an optimization problem. However, this approach\noften necessitates numerous iterations of computationally intensive operations,\nlimiting its applicability in real-time scenarios. While recent research has\nfocused on XAI hardware acceleration on FPGAs and TPU, these methods do not\nfully address energy efficiency in real-time settings. To address this\nlimitation, we propose XAIedge, a novel framework that leverages approximate\ncomputing techniques into XAI algorithms, including integrated gradients, model\ndistillation, and Shapley analysis. XAIedge translates these algorithms into\napproximate matrix computations and exploits the synergy between convolution,\nFourier transform, and approximate computing paradigms. This approach enables\nefficient hardware acceleration on TPU-based edge devices, facilitating faster\nreal-time outcome interpretations. Our comprehensive evaluation demonstrates\nthat XAIedge achieves a $2\\times$ improvement in energy efficiency compared to\nexisting accurate XAI hardware acceleration techniques while maintaining\ncomparable accuracy. These results highlight the potential of XAIedge to\nsignificantly advance the deployment of explainable AI in energy-constrained\nreal-time applications."}
{"id": "2505.06756", "pdf": "https://arxiv.org/pdf/2505.06756", "abs": "https://arxiv.org/abs/2505.06756", "authors": ["Michael W. Trosset", "Kaiyi Tan", "Minh Tang", "Carey E. Priebe"], "title": "Out-of-Sample Embedding with Proximity Data: Projection versus Restricted Reconstruction", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": "19 pages, 2 figures", "summary": "The problem of using proximity (similarity or dissimilarity) data for the\npurpose of \"adding a point to a vector diagram\" was first studied by J.C. Gower\nin 1968. Since then, a number of methods -- mostly kernel methods -- have been\nproposed for solving what has come to be called the problem of *out-of-sample\nembedding*. We survey the various kernel methods that we have encountered and\nshow that each can be derived from one or the other of two competing\nstrategies: *projection* or *restricted reconstruction*. Projection can be\nanalogized to a well-known formula for adding a point to a principal component\nanalysis. Restricted reconstruction poses a different challenge: how to best\napproximate redoing the entire multivariate analysis while holding fixed the\nvector diagram that was previously obtained. This strategy results in a\nnonlinear optimization problem that can be simplified to a unidimensional\nsearch. Various circumstances may warrant either projection or restricted\nreconstruction."}
{"id": "2409.06002", "pdf": "https://arxiv.org/pdf/2409.06002", "abs": "https://arxiv.org/abs/2409.06002", "authors": ["Quang-Huy Che", "Duc-Tri Le", "Bich-Nga Pham", "Duc-Khai Lam", "Vinh-Tiep Nguyen"], "title": "Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Data augmentation is crucial for pixel-wise annotation tasks like semantic\nsegmentation, where labeling requires significant effort and intensive labor.\nTraditional methods, involving simple transformations such as rotations and\nflips, create new images but often lack diversity along key semantic dimensions\nand fail to alter high-level semantic properties. To address this issue,\ngenerative models have emerged as an effective solution for augmenting data by\ngenerating synthetic images. Controllable Generative models offer data\naugmentation methods for semantic segmentation tasks by using prompts and\nvisual references from the original image. However, these models face\nchallenges in generating synthetic images that accurately reflect the content\nand structure of the original image due to difficulties in creating effective\nprompts and visual references. In this work, we introduce an effective data\naugmentation pipeline for semantic segmentation using Controllable Diffusion\nmodel. Our proposed method includes efficient prompt generation using\n\\textit{Class-Prompt Appending} and \\textit{Visual Prior Blending} to enhance\nattention to labeled classes in real images, allowing the pipeline to generate\na precise number of augmented images while preserving the structure of\nsegmentation-labeled classes. In addition, we implement a \\textit{class\nbalancing algorithm} to ensure a balanced training dataset when merging the\nsynthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline\ndemonstrates its effectiveness in generating high-quality synthetic images for\nsemantic segmentation. Our code is available at\n\\href{https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this\nhttps URL}."}
{"id": "2505.01462", "pdf": "https://arxiv.org/pdf/2505.01462", "abs": "https://arxiv.org/abs/2505.01462", "authors": ["Hermann Borotschnig"], "title": "Emotions in Artificial Intelligence", "categories": ["cs.AI", "cs.CY", "68T01, 68T37", "I.2.0; K.4.1"], "comment": "40 pages, 1 figure", "summary": "This conceptual contribution offers a speculative account of how AI systems\nmight emulate emotions as experienced by humans and animals. It presents a\nthought experiment grounded in the hypothesis that natural emotions evolved as\nheuristics for rapid situational appraisal and action selection, enabling\nbiologically adaptive behaviour without requiring full deliberative modeling.\nThe text examines whether artificial systems operating in complex action spaces\ncould similarly benefit from these principles. It is proposed that affect be\ninterwoven with episodic memory by storing corresponding affective tags\nalongside all events. This allows AIs to establish whether present situations\nresemble past events and project the associated emotional labels onto the\ncurrent context. These emotional cues are then combined with need-driven\nemotional hints. The combined emotional state facilitates decision-making in\nthe present by modulating action selection. The low complexity and experiential\ninertness of the proposed architecture are emphasized as evidence that\nemotional expression and consciousness are, in principle, orthogonal-permitting\nthe theoretical possibility of affective zombies. On this basis, the moral\nstatus of AIs emulating affective states is critically examined. It is argued\nthat neither the mere presence of internal representations of emotion nor\nconsciousness alone suffices for moral standing; rather, the capacity for\nself-awareness of inner emotional states is posited as a necessary condition. A\ncomplexity-based criterion is proposed to exclude such awareness in the\npresented model. Additional thought experiments are presented to test the\nconceptual boundaries of this framework."}
{"id": "2505.06774", "pdf": "https://arxiv.org/pdf/2505.06774", "abs": "https://arxiv.org/abs/2505.06774", "authors": ["Ammar Daskin"], "title": "Quantum RNNs and LSTMs Through Entangling and Disentangling Power of Unitary Transformations", "categories": ["quant-ph", "cs.LG"], "comment": "the simulation code can be downloaded from\n  https://github.com/adaskin/quantum-lstm", "summary": "In this paper, we discuss how quantum recurrent neural networks (RNNs) and\ntheir enhanced version, long short-term memory (LSTM) networks, can be modeled\nusing the core ideas presented in Ref.[1], where the entangling and\ndisentangling power of unitary transformations is investigated. In particular,\nwe interpret entangling and disentangling power as information retention and\nforgetting mechanisms in LSTMs. Therefore, entanglement becomes a key component\nof the optimization (training) process. We believe that, by leveraging prior\nknowledge of the entangling power of unitaries, the proposed quantum-classical\nframework can guide and help to design better-parameterized quantum circuits\nfor various real-world applications."}
{"id": "2409.10297", "pdf": "https://arxiv.org/pdf/2409.10297", "abs": "https://arxiv.org/abs/2409.10297", "authors": ["Blaine Hoak", "Patrick McDaniel"], "title": "On Synthetic Texture Datasets: Challenges, Creation, and Curation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The influence of textures on machine learning models has been an ongoing\ninvestigation, specifically in texture bias/learning, interpretability, and\nrobustness. However, due to the lack of large and diverse texture data\navailable, the findings in these works have been limited, as more comprehensive\nevaluations have not been feasible. Image generative models are able to provide\ndata creation at scale, but utilizing these models for texture synthesis has\nbeen unexplored and poses additional challenges both in creating accurate\ntexture images and validating those images. In this work, we introduce an\nextensible methodology and corresponding new dataset for generating\nhigh-quality, diverse texture images capable of supporting a broad set of\ntexture-based tasks. Our pipeline consists of: (1) developing prompts from a\nrange of descriptors to serve as input to text-to-image models, (2) adopting\nand adapting Stable Diffusion pipelines to generate and filter the\ncorresponding images, and (3) further filtering down to the highest quality\nimages. Through this, we create the Prompted Textures Dataset (PTD), a dataset\nof 246,285 texture images that span 56 textures. During the process of\ngenerating images, we find that NSFW safety filters in image generation\npipelines are highly sensitive to texture (and flag up to 60\\% of our texture\nimages), uncovering a potential bias in these models and presenting unique\nchallenges when working with texture data. Through both standard metrics and a\nhuman evaluation, we find that our dataset is high quality and diverse. Our\ndataset is available for download at https://zenodo.org/records/15359142."}
{"id": "2505.02118", "pdf": "https://arxiv.org/pdf/2505.02118", "abs": "https://arxiv.org/abs/2505.02118", "authors": ["Wei Liu", "Zhongyu Niu", "Lang Gao", "Zhiying Deng", "Jun Wang", "Haozhao Wang", "Ruixuan Li"], "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets", "categories": ["cs.AI"], "comment": "ICML 2025", "summary": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct)."}
{"id": "2505.06800", "pdf": "https://arxiv.org/pdf/2505.06800", "abs": "https://arxiv.org/abs/2505.06800", "authors": ["Jairon H. N. Batista", "Flávio B. Gonçalves", "Yuri F. Saporito", "Rodrigo S. Targino"], "title": "Reverse-BSDE Monte Carlo", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "Recently, there has been a growing interest in generative models based on\ndiffusions driven by the empirical robustness of these methods in generating\nhigh-dimensional photorealistic images and the possibility of using the vast\nexisting toolbox of stochastic differential equations. %This remarkable ability\nmay stem from their capacity to model and generate multimodal distributions. In\nthis work, we offer a novel perspective on the approach introduced in Song et\nal. (2021), shifting the focus from a \"learning\" problem to a \"sampling\"\nproblem. To achieve this, we reformulate the equations governing\ndiffusion-based generative models as a Forward-Backward Stochastic Differential\nEquation (FBSDE), which avoids the well-known issue of pre-estimating the\ngradient of the log target density. The solution of this FBSDE is proved to be\nunique using non-standard techniques. Additionally, we propose a numerical\nsolution to this problem, leveraging on Deep Learning techniques. This\nreformulation opens new pathways for sampling multidimensional distributions\nwith densities known up to a normalization constant, a problem frequently\nencountered in Bayesian statistics."}
{"id": "2409.18653", "pdf": "https://arxiv.org/pdf/2409.18653", "abs": "https://arxiv.org/abs/2409.18653", "authors": ["Yuli Zhou", "Guolei Sun", "Yawei Li", "Guo-Sen Xie", "Luca Benini", "Ender Konukoglu"], "title": "When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": "Technical report. Accepted by Visual Intelligence. Code is released\n  at https://github.com/zhoustan/SAM2-VCOS", "summary": "This study investigates the application and performance of the Segment\nAnything Model 2 (SAM2) in the challenging task of video camouflaged object\nsegmentation (VCOS). VCOS involves detecting objects that blend seamlessly in\nthe surroundings for videos, due to similar colors and textures, poor light\nconditions, etc. Compared to the objects in normal scenes, camouflaged objects\nare much more difficult to detect. SAM2, a video foundation model, has shown\npotential in various tasks. But its effectiveness in dynamic camouflaged\nscenarios remains under-explored. This study presents a comprehensive study on\nSAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged\nvideo datasets using different models and prompts (click, box, and mask).\nSecond, we explore the integration of SAM2 with existing multimodal large\nlanguage models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by\nfine-tuning it on the video camouflaged dataset. Our comprehensive experiments\ndemonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged\nobjects in videos. We also show that this ability could be further improved by\nspecifically adjusting SAM2's parameters for VCOS. The code is available at\nhttps://github.com/zhoustan/SAM2-VCOS"}
{"id": "2505.02216", "pdf": "https://arxiv.org/pdf/2505.02216", "abs": "https://arxiv.org/abs/2505.02216", "authors": ["Aidan Curtis", "Hao Tang", "Thiago Veloso", "Kevin Ellis", "Joshua Tenenbaum", "Tomás Lozano-Pérez", "Leslie Pack Kaelbling"], "title": "LLM-Guided Probabilistic Program Induction for POMDP Model Estimation", "categories": ["cs.AI"], "comment": null, "summary": "Partially Observable Markov Decision Processes (POMDPs) model decision making\nunder uncertainty. While there are many approaches to approximately solving\nPOMDPs, we aim to address the problem of learning such models. In particular,\nwe are interested in a subclass of POMDPs wherein the components of the model,\nincluding the observation function, reward function, transition function, and\ninitial state distribution function, can be modeled as low-complexity\nprobabilistic graphical models in the form of a short probabilistic program.\nOur strategy to learn these programs uses an LLM as a prior, generating\ncandidate probabilistic programs that are then tested against the empirical\ndistribution and adjusted through feedback. We experiment on a number of\nclassical toy POMDP problems, simulated MiniGrid domains, and two real\nmobile-base robotics search domains involving partial observability. Our\nresults show that using an LLM to guide in the construction of a low-complexity\nPOMDP model can be more effective than tabular POMDP learning, behavior\ncloning, or direct LLM planning."}
{"id": "2505.06805", "pdf": "https://arxiv.org/pdf/2505.06805", "abs": "https://arxiv.org/abs/2505.06805", "authors": ["Tommaso Giovannelli", "Griffin Dean Kent", "Luis Nunes Vicente"], "title": "A stochastic gradient method for trilevel optimization", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "With the success that the field of bilevel optimization has seen in recent\nyears, similar methodologies have started being applied to solving more\ndifficult applications that arise in trilevel optimization. At the helm of\nthese applications are new machine learning formulations that have been\nproposed in the trilevel context and, as a result, efficient and theoretically\nsound stochastic methods are required. In this work, we propose the first-ever\nstochastic gradient descent method for solving unconstrained trilevel\noptimization problems and provide a convergence theory that covers all forms of\ninexactness of the trilevel adjoint gradient, such as the inexact solutions of\nthe middle-level and lower-level problems, inexact computation of the trilevel\nadjoint formula, and noisy estimates of the gradients, Hessians, Jacobians, and\ntensors of third-order derivatives involved. We also demonstrate the promise of\nour approach by providing numerical results on both synthetic trilevel problems\nand trilevel formulations for hyperparameter adversarial tuning."}
{"id": "2409.20223", "pdf": "https://arxiv.org/pdf/2409.20223", "abs": "https://arxiv.org/abs/2409.20223", "authors": ["Chen Xie", "Ciyun Lin", "Xiaoyu Zheng", "Bowen Gong", "Antonio M. López"], "title": "GTransPDM: A Graph-embedded Transformer with Positional Decoupling for Pedestrian Crossing Intention Prediction", "categories": ["cs.CV"], "comment": "IEEE SPL", "summary": "Understanding and predicting pedestrian crossing behavioral intention is\ncrucial for the driving safety of autonomous vehicles. Nonetheless, challenges\nemerge when using promising images or environmental context masks to extract\nvarious factors for time-series network modeling, causing pre-processing errors\nor a loss of efficiency. Typically, pedestrian positions captured by onboard\ncameras are often distorted and do not accurately reflect their actual\nmovements. To address these issues, GTransPDM -- a Graph-embedded Transformer\nwith a Position Decoupling Module -- was developed for pedestrian crossing\nintention prediction by leveraging multi-modal features. First, a positional\ndecoupling module was proposed to decompose pedestrian lateral motion and\nencode depth cues in the image view. Then, a graph-embedded Transformer was\ndesigned to capture the spatio-temporal dynamics of human pose skeletons,\nintegrating essential factors such as position, skeleton, and ego-vehicle\nmotion. Experimental results indicate that the proposed method achieves 92%\naccuracy on the PIE dataset and 87% accuracy on the JAAD dataset, with a\nprocessing speed of 0.05ms. It outperforms the state-of-the-art in comparison."}
{"id": "2505.05396", "pdf": "https://arxiv.org/pdf/2505.05396", "abs": "https://arxiv.org/abs/2505.05396", "authors": ["Stefanos Gkikas"], "title": "A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "From the original abstract: This thesis initially aims to study the pain\nassessment process from a clinical-theoretical perspective while exploring and\nexamining existing automatic approaches. Building on this foundation, the\nprimary objective of this Ph.D. project is to develop innovative computational\nmethods for automatic pain assessment that achieve high performance and are\napplicable in real clinical settings. A primary goal is to thoroughly\ninvestigate and assess significant factors, including demographic elements that\nimpact pain perception, as recognized in pain research, through a computational\nstandpoint. Within the limits of the available data in this research area, our\ngoal was to design, develop, propose, and offer automatic pain assessment\npipelines for unimodal and multimodal configurations that are applicable to the\nspecific requirements of different scenarios. The studies published in this\nPh.D. thesis showcased the effectiveness of the proposed methods, achieving\nstate-of-the-art results. Additionally, they paved the way for exploring new\napproaches in artificial intelligence, foundation models, and generative\nartificial intelligence."}
{"id": "2505.06864", "pdf": "https://arxiv.org/pdf/2505.06864", "abs": "https://arxiv.org/abs/2505.06864", "authors": ["Shunyao Wang", "Ming Cheng", "Christina Dan Wang"], "title": "NewsNet-SDF: Stochastic Discount Factor Estimation with Pretrained Language Model News Embeddings via Adversarial Networks", "categories": ["q-fin.PM", "cs.LG"], "comment": null, "summary": "Stochastic Discount Factor (SDF) models provide a unified framework for asset\npricing and risk assessment, yet traditional formulations struggle to\nincorporate unstructured textual information. We introduce NewsNet-SDF, a novel\ndeep learning framework that seamlessly integrates pretrained language model\nembeddings with financial time series through adversarial networks. Our\nmultimodal architecture processes financial news using GTE-multilingual models,\nextracts temporal patterns from macroeconomic data via LSTM networks, and\nnormalizes firm characteristics, fusing these heterogeneous information sources\nthrough an innovative adversarial training mechanism. Our dataset encompasses\napproximately 2.5 million news articles and 10,000 unique securities,\naddressing the computational challenges of processing and aligning text data\nwith financial time series. Empirical evaluations on U.S. equity data\n(1980-2022) demonstrate NewsNet-SDF substantially outperforms alternatives with\na Sharpe ratio of 2.80. The model shows a 471% improvement over CAPM, over 200%\nimprovement versus traditional SDF implementations, and a 74% reduction in\npricing errors compared to the Fama-French five-factor model. In comprehensive\ncomparisons, our deep learning approach consistently outperforms traditional,\nmodern, and other neural asset pricing models across all key metrics. Ablation\nstudies confirm that text embeddings contribute significantly more to model\nperformance than macroeconomic features, with news-derived principal components\nranking among the most influential determinants of SDF dynamics. These results\nvalidate the effectiveness of our multimodal deep learning approach in\nintegrating unstructured text with traditional financial data for more accurate\nasset pricing, providing new insights for digital intelligent decision-making\nin financial technology."}
{"id": "2410.03311", "pdf": "https://arxiv.org/pdf/2410.03311", "abs": "https://arxiv.org/abs/2410.03311", "authors": ["Ye Wang", "Sipeng Zheng", "Bin Cao", "Qianshan Wei", "Weishuai Zeng", "Qin Jin", "Zongqing Lu"], "title": "Scaling Large Motion Models with Million-Level Human Motions", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025", "summary": "Inspired by the recent success of LLMs, the field of human motion\nunderstanding has increasingly shifted toward developing large motion models.\nDespite some progress, current efforts remain far from achieving truly\ngeneralist models, primarily due to the lack of massive high-quality data. To\naddress this gap, we present MotionLib, the first million-level dataset for\nmotion generation, which is at least 15$\\times$ larger than existing\ncounterparts and enriched with hierarchical text descriptions. Using MotionLib,\nwe train a large motion model named Being-M0, demonstrating robust performance\nacross a wide range of human activities, including unseen ones. Through\nsystematic investigation, for the first time, we highlight the importance of\nscaling both data and model size for advancing motion generation, along with\nkey insights to achieve this goal. To better integrate the motion modality, we\npropose Motionbook, an innovative motion encoding approach including (1) a\ncompact yet lossless feature to represent motions; (2) a novel 2D lookup-free\nmotion tokenizer that preserves fine-grained motion details while expanding\ncodebook capacity, significantly enhancing the representational power of motion\ntokens. We believe this work lays the groundwork for developing more versatile\nand powerful motion generation models in the future. For further details, visit\nhttps://github.com/BeingBeyond/Being-M0."}
{"id": "2505.05758", "pdf": "https://arxiv.org/pdf/2505.05758", "abs": "https://arxiv.org/abs/2505.05758", "authors": ["Azim Ospanov", "Farzan Farnia", "Roozbeh Yousefzadeh"], "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Formal reasoning and automated theorem proving constitute a challenging\nsubfield of machine learning, in which machines are tasked with proving\nmathematical theorems using formal languages like Lean. A formal verification\nsystem can check whether a formal proof is correct or not almost\ninstantaneously, but generating a completely correct formal proof with large\nlanguage models (LLMs) remains a formidable task. The usual approach in the\nliterature is to prompt the LLM many times (up to several thousands) until one\nof the generated proofs passes the verification system. In this work, we\npresent APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a\nmodular, model-agnostic pipeline that combines the strengths of the Lean\ncompiler with an LLM's reasoning abilities to achieve better proof-generation\nresults at a low sampling budget. Apollo directs a fully automated process in\nwhich the LLM generates proofs for theorems, a set of agents analyze the\nproofs, fix the syntax errors, identify the mistakes in the proofs using Lean,\nisolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on\neach remaining goal with a low top-K budget. The repaired sub-proofs are\nrecombined and reverified, iterating up to a user-controlled maximum number of\nattempts. On the miniF2F benchmark, we establish a new state-of-the-art\naccuracy of 75.0% among 7B-parameter models while keeping the sampling budget\nbelow one thousand. Moreover, Apollo raises the state-of-the-art accuracy for\nGoedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few\nhundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%\naccuracy. Our results demonstrate that targeted, compiler-guided repair of LLM\noutputs yields dramatic gains in both efficiency and correctness, suggesting a\ngeneral paradigm for scalable automated theorem proving."}
{"id": "2505.06900", "pdf": "https://arxiv.org/pdf/2505.06900", "abs": "https://arxiv.org/abs/2505.06900", "authors": ["Zhenzhou Jin", "Li You", "Derrick Wing Kwan Ng", "Xiang-Gen Xia", "Xiqi Gao"], "title": "Near-Field Channel Estimation for XL-MIMO: A Deep Generative Model Guided by Side Information", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": "15 pages, 11 figures, to appear on IEEE Transactions on Cognitive\n  Communications and Networking", "summary": "This paper investigates the near-field (NF) channel estimation (CE) for\nextremely large-scale multiple-input multiple-output (XL-MIMO) systems.\nConsidering the pronounced NF effects in XL-MIMO communications, we first\nestablish a joint angle-distance (AD) domain-based spherical-wavefront physical\nchannel model that captures the inherent sparsity of XL-MIMO channels.\nLeveraging the channel's sparsity in the joint AD domain, the CE is approached\nas a task of reconstructing sparse signals. Anchored in this framework, we\nfirst propose a compressed sensing algorithm to acquire a preliminary channel\nestimate. Harnessing the powerful implicit prior learning capability of\ngenerative artificial intelligence (GenAI), we further propose a GenAI-based\napproach to refine the estimated channel. Specifically, we introduce the\npreliminary estimated channel as side information, and derive the evidence\nlower bound (ELBO) of the log-marginal distribution of the target NF channel\nconditioned on the preliminary estimated channel, which serves as the\noptimization objective for the proposed generative diffusion model (GDM).\nAdditionally, we introduce a more generalized version of the GDM, the\nnon-Markovian GDM (NM-GDM), to accelerate the sampling process, achieving an\napproximately tenfold enhancement in sampling efficiency. Experimental results\nindicate that the proposed approach is capable of offering substantial\nperformance gain in CE compared to existing benchmark schemes within NF XL-MIMO\nsystems. Furthermore, our approach exhibits enhanced generalization\ncapabilities in both the NF or far-field (FF) regions."}
{"id": "2410.13371", "pdf": "https://arxiv.org/pdf/2410.13371", "abs": "https://arxiv.org/abs/2410.13371", "authors": ["Zezhun Shi"], "title": "Rotating-star Pattern for Camera Calibration", "categories": ["cs.CV"], "comment": null, "summary": "Camera calibration is fundamental to 3D vision, and the choice of calibration\npattern greatly affects the accuracy. To address aberration issue, star-shaped\npattern has been proposed as alternatives to traditional checkerboard. However,\nsuch pattern suffers from aliasing artifacts. In this paper, we present a novel\nsolution by employing a series of checkerboard patterns rotated around a\ncentral point instead of a single star-shaped pattern. We further propose a\ncomplete feature extraction algorithm tailored for this design. Experimental\nresults demonstrate that our approach offers improved accuracy over the\nconventional star-shaped pattern and achieves high stability across varying\nexposure levels."}
{"id": "2210.11111", "pdf": "https://arxiv.org/pdf/2210.11111", "abs": "https://arxiv.org/abs/2210.11111", "authors": ["Henrique Donâncio", "Laurent Vercouter", "Harald Roclawski"], "title": "The Pump Scheduling Problem: A Real-World Scenario for Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) has demonstrated impressive results in\ndomains such as games and robotics, where task formulations are well-defined.\nHowever, few DRL benchmarks are grounded in complex, real-world environments,\nwhere safety constraints, partial observability, and the need for\nhand-engineered task representations pose significant challenges. To help\nbridge this gap, we introduce a testbed based on the pump scheduling problem in\na real-world water distribution facility. The task involves controlling pumps\nto ensure a reliable water supply while minimizing energy consumption and\nrespecting the constraints of the system. Our testbed includes a realistic\nsimulator, three years of high-resolution (1-minute) operational data from\nhuman-led control, and a baseline RL task formulation. This testbed supports a\nwide range of research directions, including offline RL, safe exploration,\ninverse RL, and multi-objective optimization."}
{"id": "2505.06906", "pdf": "https://arxiv.org/pdf/2505.06906", "abs": "https://arxiv.org/abs/2505.06906", "authors": ["Sindre Benjamin Remman", "Anastasios M. Lekkas"], "title": "Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted for publication at the 2025 European Control Conference\n  (ECC)", "summary": "This paper presents a novel method for generating realistic counterfactual\nexplanations (CFEs) in machine learning (ML)-based control for mobile robots\nusing 2D LiDAR. ML models, especially artificial neural networks (ANNs), can\nprovide advanced decision-making and control capabilities by learning from\ndata. However, they often function as black boxes, making it challenging to\ninterpret them. This is especially a problem in safety-critical control\napplications. To generate realistic CFEs, we parameterize the LiDAR space with\nsimple shapes such as circles and rectangles, whose parameters are chosen by a\ngenetic algorithm, and the configurations are transformed into LiDAR data by\nraycasting. Our model-agnostic approach generates CFEs in the form of synthetic\nLiDAR data that resembles a base LiDAR state but is modified to produce a\npre-defined ML model control output based on a query from the user. We\ndemonstrate our method on a mobile robot, the TurtleBot3, controlled using deep\nreinforcement learning (DRL) in real-world and simulated scenarios. Our method\ngenerates logical and realistic CFEs, which helps to interpret the DRL agent's\ndecision making. This paper contributes towards advancing explainable AI in\nmobile robotics, and our method could be a tool for understanding, debugging,\nand improving ML-based autonomous control."}
{"id": "2410.19794", "pdf": "https://arxiv.org/pdf/2410.19794", "abs": "https://arxiv.org/abs/2410.19794", "authors": ["Zohreh Aghababaeyan", "Manel Abdellatif", "Lionel Briand", "Ramesh S"], "title": "DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural Networks", "categories": ["cs.CV", "cs.LG", "cs.SE"], "comment": null, "summary": "Deep Neural Networks (DNNs) are increasingly deployed across applications.\nHowever, ensuring their reliability remains a challenge, and in many\nsituations, alternative models with similar functionality and accuracy are\navailable. Traditional accuracy-based evaluations often fail to capture\nbehavioral differences between models, especially with limited test datasets,\nmaking it difficult to select or combine models effectively. Differential\ntesting addresses this by generating test inputs that expose discrepancies in\nDNN model behavior. However, existing approaches face significant limitations:\nmany rely on model internals or are constrained by available seed inputs. To\naddress these challenges, we propose DiffGAN, a black-box test image generation\napproach for differential testing of DNN models. DiffGAN leverages a Generative\nAdversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II to\ngenerate diverse and valid triggering inputs that reveal behavioral\ndiscrepancies between models. DiffGAN employs two custom fitness functions,\nfocusing on diversity and divergence, to guide the exploration of the GAN input\nspace and identify discrepancies between models' outputs. By strategically\nsearching this space, DiffGAN generates inputs with specific features that\ntrigger differences in model behavior. DiffGAN is black-box, making it\napplicable in more situations. We evaluate DiffGAN on eight DNN model pairs\ntrained on widely used image datasets. Our results show DiffGAN significantly\noutperforms a SOTA baseline, generating four times more triggering inputs, with\ngreater diversity and validity, within the same budget. Additionally, the\ngenerated inputs improve the accuracy of a machine learning-based model\nselection mechanism, which selects the best-performing model based on input\ncharacteristics and can serve as a smart output voting mechanism when using\nalternative models."}
{"id": "2303.13326", "pdf": "https://arxiv.org/pdf/2303.13326", "abs": "https://arxiv.org/abs/2303.13326", "authors": ["Ying Cao", "Elsa Rizk", "Stefan Vlaski", "Ali H. Sayed"], "title": "Decentralized Adversarial Training over Graphs", "categories": ["cs.LG", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2303.01936", "summary": "The vulnerability of machine learning models to adversarial attacks has been\nattracting considerable attention in recent years. Most existing studies focus\non the behavior of stand-alone single-agent learners. In comparison, this work\nstudies adversarial training over graphs, where individual agents are subjected\nto perturbations of varied strength levels across space. It is expected that\ninteractions by linked agents, and the heterogeneity of the attack models that\nare possible over the graph, can help enhance robustness in view of the\ncoordination power of the group. Using a min-max formulation of distributed\nlearning, we develop a decentralized adversarial training framework for\nmulti-agent systems. Specifically, we devise two decentralized adversarial\ntraining algorithms by relying on two popular decentralized learning\nstrategies--diffusion and consensus. We analyze the convergence properties of\nthe proposed framework for strongly-convex, convex, and non-convex\nenvironments, and illustrate the enhanced robustness to adversarial attacks."}
{"id": "2505.06927", "pdf": "https://arxiv.org/pdf/2505.06927", "abs": "https://arxiv.org/abs/2505.06927", "authors": ["Ryan Cory-Wright", "Andrés Gómez"], "title": "Stability Regularized Cross-Validation", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "Some of this material previously appeared in 2306.14851v2, which we\n  have split into two papers (this one and 2306.14851v3), because it contained\n  two ideas that need separate papers", "summary": "We revisit the problem of ensuring strong test-set performance via\ncross-validation. Motivated by the generalization theory literature, we propose\na nested k-fold cross-validation scheme that selects hyperparameters by\nminimizing a weighted sum of the usual cross-validation metric and an empirical\nmodel-stability measure. The weight on the stability term is itself chosen via\na nested cross-validation procedure. This reduces the risk of strong validation\nset performance and poor test set performance due to instability. We benchmark\nour procedure on a suite of 13 real-world UCI datasets, and find that, compared\nto k-fold cross-validation over the same hyperparameters, it improves the\nout-of-sample MSE for sparse ridge regression and CART by 4% on average, but\nhas no impact on XGBoost. This suggests that for interpretable and unstable\nmodels, such as sparse regression and CART, our approach is a viable and\ncomputationally affordable method for improving test-set performance."}
{"id": "2411.06864", "pdf": "https://arxiv.org/pdf/2411.06864", "abs": "https://arxiv.org/abs/2411.06864", "authors": ["Andrés Muñoz", "Nancy Thomas", "Annita Vapsi", "Daniel Borrajo"], "title": "Veri-Car: Towards Open-world Vehicle Information Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Many industrial and service sectors require tools to extract vehicle\ncharacteristics from images. This is a complex task not only by the variety of\nnoise, and large number of classes, but also by the constant introduction of\nnew vehicle models to the market. In this paper, we present Veri-Car, an\ninformation retrieval integrated approach designed to help on this task. It\nleverages supervised learning techniques to accurately identify the make, type,\nmodel, year, color, and license plate of cars. The approach also addresses the\nchallenge of handling open-world problems, where new car models and variations\nfrequently emerge, by employing a sophisticated combination of pre-trained\nmodels, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust\nperformance, achieving high precision and accuracy in classifying both seen and\nunseen data. Additionally, it integrates an ensemble license plate detection,\nand an OCR model to extract license plate numbers with impressive accuracy."}
{"id": "2310.13786", "pdf": "https://arxiv.org/pdf/2310.13786", "abs": "https://arxiv.org/abs/2310.13786", "authors": ["Eric Aubinais", "Elisabeth Gassiat", "Pablo Piantanida"], "title": "Fundamental Limits of Membership Inference Attacks on Machine Learning Models", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Membership inference attacks (MIA) can reveal whether a particular data point\nwas part of the training dataset, potentially exposing sensitive information\nabout individuals. This article provides theoretical guarantees by exploring\nthe fundamental statistical limitations associated with MIAs on machine\nlearning models at large. More precisely, we first derive the statistical\nquantity that governs the effectiveness and success of such attacks. We then\ntheoretically prove that in a non-linear regression setting with overfitting\nlearning procedures, attacks may have a high probability of success. Finally,\nwe investigate several situations for which we provide bounds on this quantity\nof interest. Interestingly, our findings indicate that discretizing the data\nmight enhance the learning procedure's security. Specifically, it is\ndemonstrated to be limited by a constant, which quantifies the diversity of the\nunderlying data distribution. We illustrate those results through simple\nsimulations."}
{"id": "2505.06928", "pdf": "https://arxiv.org/pdf/2505.06928", "abs": "https://arxiv.org/abs/2505.06928", "authors": ["Chi-Sheng Chen", "En-Jui Kuo"], "title": "Unraveling Quantum Environments: Transformer-Assisted Learning in Lindblad Dynamics", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Understanding dissipation in open quantum systems is crucial for the\ndevelopment of robust quantum technologies. In this work, we introduce a\nTransformer-based machine learning framework to infer time-dependent\ndissipation rates in quantum systems governed by the Lindblad master equation.\nOur approach uses time series of observable quantities, such as expectation\nvalues of single Pauli operators, as input to learn dissipation profiles\nwithout requiring knowledge of the initial quantum state or even the system\nHamiltonian.\n  We demonstrate the effectiveness of our approach on a hierarchy of open\nquantum models of increasing complexity, including single-qubit systems with\ntime-independent or time-dependent jump rates, two-qubit interacting systems\n(e.g., Heisenberg and transverse Ising models), and the Jaynes--Cummings model\ninvolving light--matter interaction and cavity loss with time-dependent decay\nrates. Our method accurately reconstructs both fixed and time-dependent decay\nrates from observable time series. To support this, we prove that under\nreasonable assumptions, the jump rates in all these models are uniquely\ndetermined by a finite set of observables, such as qubit and photon\nmeasurements. In practice, we combine Transformer-based architectures with\nlightweight feature extraction techniques to efficiently learn these dynamics.\nOur results suggest that modern machine learning tools can serve as scalable\nand data-driven alternatives for identifying unknown environments in open\nquantum systems."}
{"id": "2411.07742", "pdf": "https://arxiv.org/pdf/2411.07742", "abs": "https://arxiv.org/abs/2411.07742", "authors": ["Tianyu Sun", "Jianhao Li", "Xueqian Zhang", "Zhongdao Wang", "Bailan Feng", "Hengshuang Zhao"], "title": "Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning", "categories": ["cs.CV"], "comment": null, "summary": "This paper studies point cloud perception within outdoor environments.\nExisting methods face limitations in recognizing objects located at a distance\nor occluded, due to the sparse nature of outdoor point clouds. In this work, we\nobserve a significant mitigation of this problem by accumulating multiple\ntemporally consecutive point cloud sweeps, resulting in a remarkable\nimprovement in perception accuracy. However, the computation cost also\nincreases, hindering previous approaches from utilizing a large number of point\ncloud sweeps. To tackle this challenge, we find that a considerable portion of\npoints in the accumulated point cloud is redundant, and discarding these points\nhas minimal impact on perception accuracy. We introduce a simple yet effective\nGumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a\nlearned end-to-end sampling. The GSP layer is decoupled from other network\ncomponents and thus can be seamlessly integrated into existing point cloud\nnetwork architectures. Without incurring additional computational overhead, we\nincrease the number of point cloud sweeps from 10, a common practice, to as\nmany as 40. Consequently, there is a significant enhancement in perception\nperformance. For instance, in nuScenes 3D object detection and BEV map\nsegmentation tasks, our pruning strategy improves several 3D perception\nbaseline methods."}
{"id": "2402.01454", "pdf": "https://arxiv.org/pdf/2402.01454", "abs": "https://arxiv.org/abs/2402.01454", "authors": ["Masayuki Takayama", "Tadahisa Okuda", "Thong Pham", "Tatsuyoshi Ikenoue", "Shingo Fukuma", "Shohei Shimizu", "Akiyoshi Sannai"], "title": "Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": null, "summary": "In practical statistical causal discovery (SCD), embedding domain expert\nknowledge as constraints into the algorithm is important for reasonable causal\nmodels reflecting the broad knowledge of domain experts, despite the challenges\nin the systematic acquisition of background knowledge. To overcome these\nchallenges, this paper proposes a novel method for causal inference, in which\nSCD and knowledge-based causal inference (KBCI) with a large language model\n(LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs\nand prior knowledge augmentation for SCD. The experiments in this work have\nrevealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach\nthe ground truths, more than the SCD result without prior knowledge. These\nexperiments have also revealed that the SCD result can be further improved if\nthe LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we\nhave demonstrated that the background knowledge provided by the LLM can improve\nthe SCD on this dataset, even if this dataset has never been included in the\ntraining data of the LLM. For future practical application of this proposed\nmethod across important domains such as healthcare, we also thoroughly discuss\nthe limitations, risks of critical errors, expected improvement of techniques\naround LLMs, and realistic integration of expert checks of the results into\nthis automatic process, with SCP simulations under various conditions both in\nsuccessful and failure scenarios. The careful and appropriate application of\nthe proposed approach in this work, with improvement and customization for each\ndomain, can thus address challenges such as dataset biases and limitations,\nillustrating the potential of LLMs to improve data-driven causal inference\nacross diverse scientific domains.\n  The code used in this work is publicly available at:\nwww.github.com/mas-takayama/LLM-and-SCD"}
{"id": "2505.06958", "pdf": "https://arxiv.org/pdf/2505.06958", "abs": "https://arxiv.org/abs/2505.06958", "authors": ["James Tobler", "Hira Taqdees Syeda", "Toby Murray"], "title": "A Formally Verified Robustness Certifier for Neural Networks (Extended Version)", "categories": ["cs.PL", "cs.LG"], "comment": null, "summary": "Neural networks are often susceptible to minor perturbations in input that\ncause them to misclassify. A recent solution to this problem is the use of\nglobally-robust neural networks, which employ a function to certify that the\nclassification of an input cannot be altered by such a perturbation. Outputs\nthat pass this test are called certified robust. However, to the authors'\nknowledge, these certification functions have not yet been verified at the\nimplementation level. We demonstrate how previous unverified implementations\nare exploitably unsound in certain circumstances. Moreover, they often rely on\napproximation-based algorithms, such as power iteration, that (perhaps\nsurprisingly) do not guarantee soundness. To provide assurance that a given\noutput is robust, we implemented and formally verified a certification function\nfor globally-robust neural networks in Dafny. We describe the program, its\nspecifications, and the important design decisions taken for its implementation\nand verification, as well as our experience applying it in practice."}
{"id": "2411.11011", "pdf": "https://arxiv.org/pdf/2411.11011", "abs": "https://arxiv.org/abs/2411.11011", "authors": ["Kunwei Lv", "Ruobing Wu", "Suyang Chen", "Ping Lan"], "title": "CCi-YOLOv8n: Enhanced Fire Detection with CARAFE and Context-Guided Modules", "categories": ["cs.CV"], "comment": "13 pages,7 figures", "summary": "Fire incidents in urban and forested areas pose serious threats,underscoring\nthe need for more effective detection technologies. To address these\nchallenges, we present CCi-YOLOv8n, an enhanced YOLOv8 model with targeted\nimprovements for detecting small fires and smoke. The model integrates the\nCARAFE up-sampling operator and a context-guided module to reduce information\nloss during up-sampling and down-sampling, thereby retaining richer feature\nrepresentations. Additionally, an inverted residual mobile block enhanced C2f\nmodule captures small targets and fine smoke patterns, a critical improvement\nover the original model's detection capacity.For validation, we introduce\nWeb-Fire, a dataset curated for fire and smoke detection across diverse\nreal-world scenarios. Experimental results indicate that CCi-YOLOv8n\noutperforms YOLOv8n in detection precision, confirming its effectiveness for\nrobust fire detection tasks."}
{"id": "2403.05581", "pdf": "https://arxiv.org/pdf/2403.05581", "abs": "https://arxiv.org/abs/2403.05581", "authors": ["Thiago Freitas dos Santos", "Nardine Osman", "Marco Schorlemmer"], "title": "Can Interpretability Layouts Influence Human Perception of Offensive Sentences?", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper conducts a user study to assess whether three machine learning\n(ML) interpretability layouts can influence participants' views when evaluating\nsentences containing hate speech, focusing on the \"Misogyny\" and \"Racism\"\nclasses. Given the existence of divergent conclusions in the literature, we\nprovide empirical evidence on using ML interpretability in online communities\nthrough statistical and qualitative analyses of questionnaire responses. The\nGeneralized Additive Model estimates participants' ratings, incorporating\nwithin-subject and between-subject designs. While our statistical analysis\nindicates that none of the interpretability layouts significantly influences\nparticipants' views, our qualitative analysis demonstrates the advantages of ML\ninterpretability: 1) triggering participants to provide corrective feedback in\ncase of discrepancies between their views and the model, and 2) providing\ninsights to evaluate a model's behavior beyond traditional performance metrics."}
{"id": "2505.07011", "pdf": "https://arxiv.org/pdf/2505.07011", "abs": "https://arxiv.org/abs/2505.07011", "authors": ["Maximilian Egger", "Svenja Lage", "Rawad Bitar", "Antonia Wachter-Zeh"], "title": "Source Anonymity for Private Random Walk Decentralized Learning", "categories": ["cs.CR", "cs.DC", "cs.IT", "cs.LG", "math.IT", "stat.ML"], "comment": null, "summary": "This paper considers random walk-based decentralized learning, where at each\niteration of the learning process, one user updates the model and sends it to a\nrandomly chosen neighbor until a convergence criterion is met. Preserving data\nprivacy is a central concern and open problem in decentralized learning. We\npropose a privacy-preserving algorithm based on public-key cryptography and\nanonymization. In this algorithm, the user updates the model and encrypts the\nresult using a distant user's public key. The encrypted result is then\ntransmitted through the network with the goal of reaching that specific user.\nThe key idea is to hide the source's identity so that, when the destination\nuser decrypts the result, it does not know who the source was. The challenge is\nto design a network-dependent probability distribution (at the source) over the\npotential destinations such that, from the receiver's perspective, all users\nhave a similar likelihood of being the source. We introduce the problem and\nconstruct a scheme that provides anonymity with theoretical guarantees. We\nfocus on random regular graphs to establish rigorous guarantees."}
{"id": "2411.11370", "pdf": "https://arxiv.org/pdf/2411.11370", "abs": "https://arxiv.org/abs/2411.11370", "authors": ["Ke Zhang", "Zhaoye Zheng", "Yurong Guo", "Jiacun Wang", "Jiyuan Yang", "Yangjie Xiao"], "title": "Transmission Line Defect Detection Based on UAV Patrol Images and Vision-language Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Unmanned aerial vehicle (UAV) patrol inspection has emerged as a predominant\napproach in transmission line monitoring owing to its cost-effectiveness.\nDetecting defects in transmission lines is a critical task during UAV patrol\ninspection. However, due to imaging distance and shooting angles, UAV patrol\nimages often suffer from insufficient defect-related visual information, which\nhas an adverse effect on detection accuracy. In this article, we propose a\nnovel method for detecting defects in UAV patrol images, which is based on\nvision-language pretraining for transmission line (VLP-TL) and a progressive\ntransfer strategy (PTS). Specifically, VLP-TL contains two novel pretraining\ntasks tailored for the transmission line scenario, aimimg at pretraining an\nimage encoder with abundant knowledge acquired from both visual and linguistic\ninformation. Transferring the pretrained image encoder to the defect detector\nas its backbone can effectively alleviate the insufficient visual information\nproblem. In addition, the PTS further improves transfer performance by\nprogressively bridging the gap between pretraining and downstream defection\ndetection. Experimental results demonstrate that the proposed method\nsignificantly improves defect detection accuracy by jointly utilizing\nmultimodal information, overcoming the limitations of insufficient\ndefect-related visual information provided by UAV patrol images."}
{"id": "2403.17154", "pdf": "https://arxiv.org/pdf/2403.17154", "abs": "https://arxiv.org/abs/2403.17154", "authors": ["Jaskirat Singh", "Emad Fallahzadeh", "Bram Adams", "Ahmed E. Hassan"], "title": "On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Deciding what combination of operators to use across the Edge AI tiers to\nachieve specific latency and model performance requirements is an open question\nfor MLOps engineers. This study aims to empirically assess the accuracy vs\ninference time trade-off of different black-box Edge AI deployment strategies,\ni.e., combinations of deployment operators and deployment tiers. In this paper,\nwe conduct inference experiments involving 3 deployment operators (i.e.,\nPartitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile,\nEdge, Cloud) and their combinations on four widely used Computer-Vision models\nto investigate the optimal strategies from the point of view of MLOps\ndevelopers. Our findings suggest that Edge deployment using the hybrid\nQuantization + Early Exit operator could be preferred over non-hybrid operators\n(Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency\nis a concern at medium accuracy loss. However, when minimizing accuracy loss is\na concern, MLOps engineers should prefer using only a Quantization operator on\nedge at a latency reduction or increase, respectively over the Early\nExit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge)\noperators. In scenarios constrained by Mobile CPU/RAM resources, a preference\nfor Partitioning across mobile and edge tiers is observed over mobile\ndeployment. For models with smaller input data samples (such as FCN), a\nnetwork-constrained cloud deployment can also be a better alternative than\nMobile/Edge deployment and Partitioning strategies. For models with large input\ndata samples (ResNet, ResNext, DUC), an edge tier having higher\nnetwork/computational capabilities than Cloud/Mobile can be a more viable\noption than Partitioning and Mobile/Cloud deployment strategies."}
{"id": "2505.07033", "pdf": "https://arxiv.org/pdf/2505.07033", "abs": "https://arxiv.org/abs/2505.07033", "authors": ["Ningsheng Zhao", "Trang Bui", "Jia Yuan Yu", "Krzysztof Dzieciolowski"], "title": "Outperformance Score: A Universal Standardization Method for Confusion-Matrix-Based Classification Performance Metrics", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Many classification performance metrics exist, each suited to a specific\napplication. However, these metrics often differ in scale and can exhibit\nvarying sensitivity to class imbalance rates in the test set. As a result, it\nis difficult to use the nominal values of these metrics to interpret and\nevaluate classification performances, especially when imbalance rates vary. To\naddress this problem, we introduce the outperformance score function, a\nuniversal standardization method for confusion-matrix-based classification\nperformance (CMBCP) metrics. It maps any given metric to a common scale of\n$[0,1]$, while providing a clear and consistent interpretation. Specifically,\nthe outperformance score represents the percentile rank of the observed\nclassification performance within a reference distribution of possible\nperformances. This unified framework enables meaningful comparison and\nmonitoring of classification performance across test sets with differing\nimbalance rates. We illustrate how the outperformance scores can be applied to\na variety of commonly used classification performance metrics and demonstrate\nthe robustness of our method through experiments on real-world datasets\nspanning multiple classification applications."}
{"id": "2411.11904", "pdf": "https://arxiv.org/pdf/2411.11904", "abs": "https://arxiv.org/abs/2411.11904", "authors": ["Yue Zhou", "Mengcheng Lan", "Xiang Li", "Litong Feng", "Yiping Ke", "Xue Jiang", "Qingyun Li", "Xue Yang", "Wayne Zhang"], "title": "GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Remote sensing (RS) visual grounding aims to use natural language expression\nto locate specific objects (in the form of the bounding box or segmentation\nmask) in RS images, enhancing human interaction with intelligent RS\ninterpretation systems. Early research in this area was primarily based on\nhorizontal bounding boxes (HBBs), but as more diverse RS datasets have become\navailable, tasks involving oriented bounding boxes (OBBs) and segmentation\nmasks have emerged. In practical applications, different targets require\ndifferent grounding types: HBB can localize an object's position, OBB provides\nits orientation, and mask depicts its shape. However, existing specialized\nmethods are typically tailored to a single type of RS visual grounding task and\nare hard to generalize across tasks. In contrast, large vision-language models\n(VLMs) exhibit powerful multi-task learning capabilities but struggle to handle\ndense prediction tasks like segmentation. This paper proposes GeoGround, a\nnovel framework that unifies support for HBB, OBB, and mask RS visual grounding\ntasks, allowing flexible output selection. Rather than customizing the\narchitecture of VLM, our work aims to elegantly support pixel-level visual\ngrounding output through the Text-Mask technique. We define prompt-assisted and\ngeometry-guided learning to enhance consistency across different signals.\nExperimental results show that GeoGround demonstrates strong performance across\nfour RS visual grounding tasks, matching the performance of specialized methods\non multiple benchmarks. Code available at https://github.com/zytx121/GeoGround"}
{"id": "2405.17412", "pdf": "https://arxiv.org/pdf/2405.17412", "abs": "https://arxiv.org/abs/2405.17412", "authors": ["Aditya Ravuri", "Neil D. Lawrence"], "title": "Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "Updated figures", "summary": "This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in Ravuri et al. (2023), that describes the graph Laplacian\n(an estimate of the data precision matrix) using a Wishart distribution, with a\nmean given by a non-linear covariance function evaluated on the latents. This\ninterpretation offers deeper theoretical and semantic insights into such\nalgorithms, and forging a connection to Gaussian process latent variable models\nby showing that well-known kernels can be used to describe covariances implied\nby graph Laplacians. We also introduce tools with which similar dimensionality\nreduction methods can be studied."}
{"id": "2505.07046", "pdf": "https://arxiv.org/pdf/2505.07046", "abs": "https://arxiv.org/abs/2505.07046", "authors": ["Stephen Thomas"], "title": "Streaming Krylov-Accelerated Stochastic Gradient Descent", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "We present SKA-SGD (Streaming Krylov-Accelerated Stochastic Gradient\nDescent), a novel optimization approach that accelerates convergence for\nill-conditioned problems by projecting stochastic gradients onto a\nlow-dimensional Krylov subspace. Directly inspired by recent advances in s-step\nConjugate Gradient methods with streaming Gauss-Seidel Gram solvers\n\\cite{dambra2025sstep}, our method extends these techniques to the stochastic\noptimization domain. Our approach combines three key innovations: (1)\nprojection coefficients computed via a single streaming Gauss-Seidel iteration,\nwhich is mathematically equivalent to Modified Gram-Schmidt orthogonalization;\n(2) a Chebyshev polynomial basis for constructing the Krylov subspace,\nproviding superior numerical stability; and (3) efficient implementation for\nAMD GPUs using HIP. We prove that our streaming approach achieves a backward\nerror near machine precision with $O(s^2)$ complexity rather than $O(s^3)$,\nwhere $s$ is the Krylov subspace dimension. Experimental results demonstrate\nthat SKA-SGD significantly outperforms standard SGD and Adam in convergence\nrate and final error, particularly for problems with condition numbers\nexceeding $10^3$. GPU performance analysis reveals a crossover point where\ncommunication-avoiding benefits outweigh computational overhead, typically\noccurring at moderate scale ($p \\approx 64$ processors) for problem sizes $n\n\\geq 10^6$."}
{"id": "2411.14494", "pdf": "https://arxiv.org/pdf/2411.14494", "abs": "https://arxiv.org/abs/2411.14494", "authors": ["Nitish Shukla", "Arun Ross"], "title": "dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph", "categories": ["cs.CV"], "comment": null, "summary": "A facial morph is an image created by combining two face images pertaining to\ntwo distinct identities. Face demorphing inverts the process and tries to\nrecover the original images constituting a facial morph. While morph attack\ndetection (MAD) techniques can be used to flag morph images, they do not\ndivulge any visual information about the faces used to create them. Demorphing\nhelps address this problem. Existing demorphing techniques are either very\nrestrictive (assume identities during testing) or produce feeble outputs (both\noutputs look very similar). In this paper, we overcome these issues by\nproposing dc-GAN, a novel GAN-based demorphing method conditioned on the morph\nimages. Our method overcomes morph-replication and produces high quality\nreconstructions of the bonafide images used to create the morphs. Moreover, our\nmethod is highly generalizable across demorphing paradigms\n(differential/reference-free). We conduct experiments on AMSL, FRLL-Morphs and\nMorDiff datasets to showcase the efficacy of our method."}
{"id": "2407.10424", "pdf": "https://arxiv.org/pdf/2407.10424", "abs": "https://arxiv.org/abs/2407.10424", "authors": ["Yang Zhao", "Di Huang", "Chongxiao Li", "Pengwei Jin", "Muxin Song", "Yinan Xu", "Ziyuan Nan", "Mingju Gao", "Tianyun Ma", "Lei Qi", "Yansong Pan", "Zhenxing Zhang", "Rui Zhang", "Xishan Zhang", "Zidong Du", "Qi Guo", "Xing Hu"], "title": "CodeV: Empowering LLMs with HDL Generation through Multi-Level Summarization", "categories": ["cs.PL", "cs.AI"], "comment": "13 pages, 10 figures, journal", "summary": "The design flow of processors, particularly in hardware description languages\n(HDL) like Verilog and Chisel, is complex and costly. While recent advances in\nlarge language models (LLMs) have significantly improved coding tasks in\nsoftware languages such as Python, their application in HDL generation remains\nlimited due to the scarcity of high-quality HDL data. Traditional methods of\nadapting LLMs for hardware design rely on synthetic HDL datasets, which often\nsuffer from low quality because even advanced LLMs like GPT perform poorly in\nthe HDL domain. Moreover, these methods focus solely on chat tasks and the\nVerilog language, limiting their application scenarios.\n  In this paper, we observe that: (1) HDL code collected from the real world is\nof higher quality than code generated by LLMs. (2) LLMs like GPT-3.5 excel in\nsummarizing HDL code rather than generating it. (3) An explicit language tag\ncan help LLMs better adapt to the target language when there is insufficient\ndata. Based on these observations, we propose an efficient LLM fine-tuning\npipeline for HDL generation that integrates a multi-level summarization data\nsynthesis process with a novel Chat-FIM-Tag supervised fine-tuning method. The\npipeline enhances the generation of HDL code from natural language descriptions\nand enables the handling of various tasks such as chat and infilling incomplete\ncode. Utilizing this pipeline, we introduce CodeV, a series of HDL generation\nLLMs. Among them, CodeV-All not only possesses a more diverse range of language\nabilities, i.e. Verilog and Chisel, and a broader scope of tasks, i.e. Chat and\nfill-in-middle (FIM), but it also achieves performance on VerilogEval that is\ncomparable to or even surpasses that of CodeV-Verilog fine-tuned on Verilog\nonly, making them the first series of open-source LLMs designed for\nmulti-scenario HDL generation."}
{"id": "2505.07054", "pdf": "https://arxiv.org/pdf/2505.07054", "abs": "https://arxiv.org/abs/2505.07054", "authors": ["Austin Braniff", "Yuhe Tian"], "title": "YANNs: Y-wise Affine Neural Networks for Exact and Efficient Representations of Piecewise Linear Functions", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "comment": null, "summary": "This work formally introduces Y-wise Affine Neural Networks (YANNs), a\nfully-explainable network architecture that continuously and efficiently\nrepresent piecewise affine functions with polytopic subdomains. Following from\nthe proofs, it is shown that the development of YANNs requires no training to\nachieve the functionally equivalent representation. YANNs thus maintain all\nmathematical properties of the original formulations. Multi-parametric model\npredictive control is utilized as an application showcase of YANNs, which\ntheoretically computes optimal control laws as a piecewise affine function of\nstates, outputs, setpoints, and disturbances. With the exact representation of\nmulti-parametric control laws, YANNs retain essential control-theoretic\nguarantees such as recursive feasibility and stability. This sets YANNs apart\nfrom the existing works which apply neural networks for approximating optimal\ncontrol laws instead of exactly representing them. By optimizing the inference\nspeed of the networks, YANNs can evaluate substantially faster in real-time\ncompared to traditional piecewise affine function calculations. Numerical case\nstudies are presented to demonstrate the algorithmic scalability with respect\nto the input/output dimensions and the number of subdomains. YANNs represent a\nsignificant advancement in control as the first neural network-based controller\nthat inherently ensures both feasibility and stability. Future applications can\nleverage them as an efficient and interpretable starting point for data-driven\nmodeling/control."}
{"id": "2412.00176", "pdf": "https://arxiv.org/pdf/2412.00176", "abs": "https://arxiv.org/abs/2412.00176", "authors": ["Hui Ren", "Joanna Materzynska", "Rohit Gandikota", "David Bau", "Antonio Torralba"], "title": "Opt-In Art: Learning Art Styles Only from Few Examples", "categories": ["cs.CV"], "comment": null, "summary": "We explore whether pre-training on datasets with paintings is necessary for a\nmodel to learn an artistic style with only a few examples. To investigate this,\nwe train a text-to-image model exclusively on photographs, without access to\nany painting-related content. We show that it is possible to adapt a model that\nis trained without paintings to an artistic style, given only few examples.\nUser studies and automatic evaluations confirm that our model (post-adaptation)\nperforms on par with state-of-the-art models trained on massive datasets that\ncontain artistic content like paintings, drawings or illustrations. Finally,\nusing data attribution techniques, we analyze how both artistic and\nnon-artistic datasets contribute to generating artistic-style images.\nSurprisingly, our findings suggest that high-quality artistic outputs can be\nachieved without prior exposure to artistic data, indicating that artistic\nstyle generation can occur in a controlled, opt-in manner using only a limited,\ncarefully selected set of training examples."}
{"id": "2407.13163", "pdf": "https://arxiv.org/pdf/2407.13163", "abs": "https://arxiv.org/abs/2407.13163", "authors": ["Yi Zhang", "Ruihong Qiu", "Jiajun Liu", "Sen Wang"], "title": "ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for Recommender Systems", "categories": ["cs.IR", "cs.AI"], "comment": "CIKM 2024", "summary": "Offline reinforcement learning (RL) is an effective tool for real-world\nrecommender systems with its capacity to model the dynamic interest of users\nand its interactive nature. Most existing offline RL recommender systems focus\non model-based RL through learning a world model from offline data and building\nthe recommendation policy by interacting with this model. Although these\nmethods have made progress in the recommendation performance, the effectiveness\nof model-based offline RL methods is often constrained by the accuracy of the\nestimation of the reward model and the model uncertainties, primarily due to\nthe extreme discrepancy between offline logged data and real-world data in user\ninteractions with online platforms. To fill this gap, a more accurate reward\nmodel and uncertainty estimation are needed for the model-based RL methods. In\nthis paper, a novel model-based Reward Shaping in Offline Reinforcement\nLearning for Recommender Systems, ROLeR, is proposed for reward and uncertainty\nestimation in recommendation systems. Specifically, a non-parametric reward\nshaping method is designed to refine the reward model. In addition, a flexible\nand more representative uncertainty penalty is designed to fit the needs of\nrecommendation systems. Extensive experiments conducted on four benchmark\ndatasets showcase that ROLeR achieves state-of-the-art performance compared\nwith existing baselines. The source code can be downloaded at\nhttps://github.com/ArronDZhang/ROLeR."}
{"id": "2505.07067", "pdf": "https://arxiv.org/pdf/2505.07067", "abs": "https://arxiv.org/abs/2505.07067", "authors": ["Francesco Cagnetta", "Hyunmo Kang", "Matthieu Wyart"], "title": "Learning curves theory for hierarchically compositional data with power-law distributed features", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "comment": null, "summary": "Recent theories suggest that Neural Scaling Laws arise whenever the task is\nlinearly decomposed into power-law distributed units. Alternatively, scaling\nlaws also emerge when data exhibit a hierarchically compositional structure, as\nis thought to occur in language and images. To unify these views, we consider\nclassification and next-token prediction tasks based on probabilistic\ncontext-free grammars -- probabilistic models that generate data via a\nhierarchy of production rules. For classification, we show that having\npower-law distributed production rules results in a power-law learning curve\nwith an exponent depending on the rules' distribution and a large\nmultiplicative constant that depends on the hierarchical structure. By\ncontrast, for next-token prediction, the distribution of production rules\ncontrols the local details of the learning curve, but not the exponent\ndescribing the large-scale behaviour."}
{"id": "2412.00626", "pdf": "https://arxiv.org/pdf/2412.00626", "abs": "https://arxiv.org/abs/2412.00626", "authors": ["You Wu", "Xiangyang Yang", "Xucheng Wang", "Hengzhou Ye", "Dan Zeng", "Shuiwang Li"], "title": "MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum Learning", "categories": ["cs.CV"], "comment": null, "summary": "Harnessing low-light enhancement and domain adaptation, nighttime UAV\ntracking has made substantial strides. However, over-reliance on image\nenhancement, limited high-quality nighttime data, and a lack of integration\nbetween daytime and nighttime trackers hinder the development of an end-to-end\ntrainable framework. Additionally, current ViT-based trackers demand heavy\ncomputational resources due to their reliance on the self-attention mechanism.\nIn this paper, we propose a novel pure Mamba-based tracking framework\n(MambaNUT) that employs a state space model with linear complexity as its\nbackbone, incorporating a single-stream architecture that integrates feature\nlearning and template-search coupling within Vision Mamba. We introduce an\nadaptive curriculum learning (ACL) approach that dynamically adjusts sampling\nstrategies and loss weights, thereby improving the model's ability of\ngeneralization. Our ACL is composed of two levels of curriculum schedulers: (1)\nsampling scheduler that transforms the data distribution from imbalanced to\nbalanced, as well as from easier (daytime) to harder (nighttime) samples; (2)\nloss scheduler that dynamically assigns weights based on the size of the\ntraining set and IoU of individual instances. Exhaustive experiments on\nmultiple nighttime UAV tracking benchmarks demonstrate that the proposed\nMambaNUT achieves state-of-the-art performance while requiring lower\ncomputational costs. The code will be available at\nhttps://github.com/wuyou3474/MambaNUT."}
{"id": "2408.00540", "pdf": "https://arxiv.org/pdf/2408.00540", "abs": "https://arxiv.org/abs/2408.00540", "authors": ["Shih-Kai Chou", "Jernej Hribar", "Vid Hanžel", "Mihael Mohorčič", "Carolina Fortuna"], "title": "The Energy Cost of Artificial Intelligence Lifecycle in Communication Networks", "categories": ["cs.ET", "cs.AI", "cs.LG"], "comment": "13 pages, 9 figures", "summary": "Artificial Intelligence (AI) is being incorporated in several optimization,\nscheduling, orchestration as well as in native communication network functions.\nWhile this paradigm shift results in increased energy consumption, quantifying\nthe end-toend energy consumption of adding intelligence to such systems is\nparticularly challenging. Conventional metrics focus on either communication,\ncomputation infrastructure, or model development. To address this, we propose a\nnew metric, the Energy Cost of AI Lifecycle (eCAL) of one AI model in a system.\neCAL captures the energy consumption throughout the development and deployment\nof an AI-model providing intelligence in a wireless communication network by\nanalyzing the complexity of data collection and manipulation in individual\ncomponents and deriving overall and per-bit energy consumption. We show that\nthe better a model is and the more it is used, the more energy efficient an\ninference is. For a simple case study, eCAL for making 100 inferences is 2.73\ntimes higher than for 1000 inferences. Additionally, we have developed a\nmodular and extendable opensource simulation tool to enable researchers,\npractitioners, and engineers to calculate the end-to-end energy cost with\nvarious configurations and across various systems, ensuring adaptability to\ndiverse use cases."}
{"id": "2505.07068", "pdf": "https://arxiv.org/pdf/2505.07068", "abs": "https://arxiv.org/abs/2505.07068", "authors": ["Jinchao Feng", "Sui Tang"], "title": "A Sparse Bayesian Learning Algorithm for Estimation of Interaction Kernels in Motsch-Tadmor Model", "categories": ["stat.ML", "cs.LG", "math.DS"], "comment": "18 pages", "summary": "In this paper, we investigate the data-driven identification of asymmetric\ninteraction kernels in the Motsch-Tadmor model based on observed trajectory\ndata. The model under consideration is governed by a class of semilinear\nevolution equations, where the interaction kernel defines a normalized,\nstate-dependent Laplacian operator that governs collective dynamics. To address\nthe resulting nonlinear inverse problem, we propose a variational framework\nthat reformulates kernel identification using the implicit form of the\ngoverning equations, reducing it to a subspace identification problem. We\nestablish an identifiability result that characterizes conditions under which\nthe interaction kernel can be uniquely recovered up to scale. To solve the\ninverse problem robustly, we develop a sparse Bayesian learning algorithm that\nincorporates informative priors for regularization, quantifies uncertainty, and\nenables principled model selection. Extensive numerical experiments on\nrepresentative interacting particle systems demonstrate the accuracy,\nrobustness, and interpretability of the proposed framework across a range of\nnoise levels and data regimes."}
{"id": "2412.01485", "pdf": "https://arxiv.org/pdf/2412.01485", "abs": "https://arxiv.org/abs/2412.01485", "authors": ["Cong Xie", "Han Zou", "Ruiqi Yu", "Yan Zhang", "Zhenpeng Zhan"], "title": "SerialGen: Personalized Image Generation by First Standardization Then Personalization", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we are interested in achieving both high text controllability\nand whole-body appearance consistency in the generation of personalized human\ncharacters. We propose a novel framework, named SerialGen, which is a serial\ngeneration method consisting of two stages: first, a standardization stage that\nstandardizes reference images, and then a personalized generation stage based\non the standardized reference. Furthermore, we introduce two modules aimed at\nenhancing the standardization process. Our experimental results validate the\nproposed framework's ability to produce personalized images that faithfully\nrecover the reference image's whole-body appearance while accurately responding\nto a wide range of text prompts. Through thorough analysis, we highlight the\ncritical contribution of the proposed serial generation method and\nstandardization model, evidencing enhancements in appearance consistency\nbetween reference and output images and across serial outputs generated from\ndiverse text prompts. The term \"Serial\" in this work carries a double meaning:\nit refers to the two-stage method and also underlines our ability to generate\nserial images with consistent appearance throughout."}
{"id": "2408.14806", "pdf": "https://arxiv.org/pdf/2408.14806", "abs": "https://arxiv.org/abs/2408.14806", "authors": ["Maria Despoina Siampou", "Jialiang Li", "John Krumm", "Cyrus Shahabi", "Hua Lu"], "title": "Poly2Vec: Polymorphic Fourier-Based Encoding of Geospatial Objects for GeoAI Applications", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Encoding geospatial objects is fundamental for geospatial artificial\nintelligence (GeoAI) applications, which leverage machine learning (ML) models\nto analyze spatial information. Common approaches transform each object into\nknown formats, like image and text, for compatibility with ML models. However,\nthis process often discards crucial spatial information, such as the object's\nposition relative to the entire space, reducing downstream task effectiveness.\nAlternative encoding methods that preserve some spatial properties are often\ndevised for specific data objects (e.g., point encoders), making them\nunsuitable for tasks that involve different data types (i.e., points,\npolylines, and polygons). To this end, we propose Poly2Vec, a polymorphic\nFourier-based encoding approach that unifies the representation of geospatial\nobjects, while preserving the essential spatial properties. Poly2Vec\nincorporates a learned fusion module that adaptively integrates the magnitude\nand phase of the Fourier transform for different tasks and geometries. We\nevaluate Poly2Vec on five diverse tasks, organized into two categories. The\nfirst empirically demonstrates that Poly2Vec consistently outperforms\nobject-specific baselines in preserving three key spatial relationships:\ntopology, direction, and distance. The second shows that integrating Poly2Vec\ninto a state-of-the-art GeoAI workflow improves the performance in two popular\ntasks: population prediction and land use inference."}
{"id": "2505.07101", "pdf": "https://arxiv.org/pdf/2505.07101", "abs": "https://arxiv.org/abs/2505.07101", "authors": ["Haichen Hu", "David Simchi-Levi", "Navid Azizan"], "title": "Constrained Online Decision-Making with Density Estimation Oracles", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Contextual online decision-making problems with constraints appear in a wide\nrange of real-world applications, such as personalized recommendation with\nresource limits, adaptive experimental design, and decision-making under safety\nor fairness requirements. In this paper, we investigate a general formulation\nof sequential decision-making with stage-wise feasibility constraints, where at\neach round, the learner must select an action based on observed context while\nensuring that a problem-specific feasibility criterion is satisfied. We propose\na unified algorithmic framework that captures many existing constrained\nlearning problems, including constrained bandits, active learning with label\nbudgets, online hypothesis testing with Type I error control, and model\ncalibration. Central to our approach is the concept of upper counterfactual\nconfidence bounds, which enables the design of practically efficient online\nalgorithms with strong theoretical guarantee using any offline conditional\ndensity estimation oracle. Technically, to handle feasibility constraints in\ncomplex environments, we introduce a generalized notion of the eluder dimension\n- extending it from the classical setting based on square loss to a broader\nclass of metric-like probability divergences. This allows us to capture the\ncomplexity of various density function classes and characterize the utility\nregret incurred due to feasibility constraint uncertainty. Our result offers a\nprincipled foundation for constrained sequential decision-making in both theory\nand practice."}
{"id": "2412.01818", "pdf": "https://arxiv.org/pdf/2412.01818", "abs": "https://arxiv.org/abs/2412.01818", "authors": ["Qizhe Zhang", "Aosong Cheng", "Ming Lu", "Renrui Zhang", "Zhiyong Zhuo", "Jiajun Cao", "Shaobo Guo", "Qi She", "Shanghang Zhang"], "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 9 figures, code: https://github.com/Theia-4869/VisPruner,\n  project page: https://theia-4869.github.io/VisPruner", "summary": "Large vision-language models (LVLMs) generally contain significantly more\nvisual tokens than their textual counterparts, resulting in a considerable\ncomputational burden. Recent efforts have been made to tackle this issue by\npruning visual tokens early within the language model. Most existing works use\nattention scores between text and visual tokens to assess the importance of\nvisual tokens. However, in this study, we first analyze the text-visual\nattention in the language model and find that this score is not an ideal\nindicator for token pruning. Based on the analysis, We propose VisPruner, a\nplug-and-play method that utilizes visual cues for more effective token pruning\nin LVLMs. Specifically, we first use visual attention to select a limited\nnumber of significant tokens. Then, we remove duplicate tokens from the\nremaining ones based on their similarity. By retaining diverse tokens alongside\nthe initially selected important tokens, we maximally preserve the visual\ninformation of the input image. Experimental results demonstrate that our\nVisPruner sustains strong performance across various VLM architectures and\nreduction ratios, significantly outperforming existing methods based on\ntext-visual attention. Notably, without any training, VisPruner can reduce the\nFLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining\ncomparable performance. Our code is available at\nhttps://github.com/Theia-4869/VisPruner."}
{"id": "2409.05808", "pdf": "https://arxiv.org/pdf/2409.05808", "abs": "https://arxiv.org/abs/2409.05808", "authors": ["Mohammad Baqar", "Rajat Khanda"], "title": "The Future of Software Testing: AI-Powered Test Case Generation and Validation", "categories": ["cs.SE", "cs.AI"], "comment": "Version 2, 19 Pages", "summary": "Software testing is a crucial phase in the software development lifecycle\n(SDLC), ensuring that products meet necessary functional, performance, and\nquality benchmarks before release. Despite advancements in automation,\ntraditional methods of generating and validating test cases still face\nsignificant challenges, including prolonged timelines, human error, incomplete\ntest coverage, and high costs of manual intervention. These limitations often\nlead to delayed product launches and undetected defects that compromise\nsoftware quality and user satisfaction. The integration of artificial\nintelligence (AI) into software testing presents a promising solution to these\npersistent challenges. AI-driven testing methods automate the creation of\ncomprehensive test cases, dynamically adapt to changes, and leverage machine\nlearning to identify high-risk areas in the codebase. This approach enhances\nregression testing efficiency while expanding overall test coverage.\nFurthermore, AI-powered tools enable continuous testing and self-healing test\ncases, significantly reducing manual oversight and accelerating feedback loops,\nultimately leading to faster and more reliable software releases. This paper\nexplores the transformative potential of AI in improving test case generation\nand validation, focusing on its ability to enhance efficiency, accuracy, and\nscalability in testing processes. It also addresses key challenges associated\nwith adapting AI for testing, including the need for high quality training\ndata, ensuring model transparency, and maintaining a balance between automation\nand human oversight. Through case studies and examples of real-world\napplications, this paper illustrates how AI can significantly enhance testing\nefficiency across both legacy and modern software systems."}
{"id": "2505.07105", "pdf": "https://arxiv.org/pdf/2505.07105", "abs": "https://arxiv.org/abs/2505.07105", "authors": ["Hongwei Shang", "Nguyen Vo", "Nitin Yadav", "Tian Zhang", "Ajit Puthenputhussery", "Xunfan Cai", "Shuyi Chen", "Prijith Chandran", "Changsung Kang"], "title": "Knowledge Distillation for Enhancing Walmart E-commerce Search Relevance Using Large Language Models", "categories": ["cs.IR", "cs.LG"], "comment": "9 pages, published at WWWW'25", "summary": "Ensuring the products displayed in e-commerce search results are relevant to\nusers queries is crucial for improving the user experience. With their advanced\nsemantic understanding, deep learning models have been widely used for\nrelevance matching in search tasks. While large language models (LLMs) offer\nsuperior ranking capabilities, it is challenging to deploy LLMs in real-time\nsystems due to the high-latency requirements. To leverage the ranking power of\nLLMs while meeting the low-latency demands of production systems, we propose a\nnovel framework that distills a high performing LLM into a more efficient,\nlow-latency student model. To help the student model learn more effectively\nfrom the teacher model, we first train the teacher LLM as a classification\nmodel with soft targets. Then, we train the student model to capture the\nrelevance margin between pairs of products for a given query using mean squared\nerror loss. Instead of using the same training data as the teacher model, we\nsignificantly expand the student model dataset by generating unlabeled data and\nlabeling it with the teacher model predictions. Experimental results show that\nthe student model performance continues to improve as the size of the augmented\ntraining data increases. In fact, with enough augmented data, the student model\ncan outperform the teacher model. The student model has been successfully\ndeployed in production at Walmart.com with significantly positive metrics."}
{"id": "2412.09521", "pdf": "https://arxiv.org/pdf/2412.09521", "abs": "https://arxiv.org/abs/2412.09521", "authors": ["Shengxuming Zhang", "Weihan Li", "Tianhong Gao", "Jiacong Hu", "Haoming Luo", "Xiuming Zhang", "Jing Zhang", "Mingli Song", "Zunlei Feng"], "title": "Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pathological diagnosis is vital for determining disease characteristics,\nguiding treatment, and assessing prognosis, relying heavily on detailed,\nmulti-scale analysis of high-resolution whole slide images (WSI). However,\ntraditional pure vision models face challenges of redundant feature extraction,\nwhereas existing large vision-language models (LVLMs) are limited by input\nresolution constraints, hindering their efficiency and accuracy. To overcome\nthese issues, we propose two innovative strategies: the mixed task-guided\nfeature enhancement, which directs feature extraction toward lesion-related\ndetails across scales, and the prompt-guided detail feature completion, which\nintegrates coarse- and fine-grained features from WSI based on specific prompts\nwithout compromising inference speed. Leveraging a comprehensive dataset of\n490,000 samples from diverse pathology tasks-including cancer detection,\ngrading, vascular and neural invasion identification, and so on-we trained the\npathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that\nthis model significantly outperforms existing methods in diagnostic accuracy\nand efficiency, offering an interactive, clinically aligned approach for\nauxiliary diagnosis in a wide range of pathology applications."}
{"id": "2409.06953", "pdf": "https://arxiv.org/pdf/2409.06953", "abs": "https://arxiv.org/abs/2409.06953", "authors": ["Zeno Kujawa", "John Poole", "Dobrik Georgiev", "Danilo Numeroso", "Henry Fleischmann", "Pietro Liò"], "title": "Neural Algorithmic Reasoning with Multiple Correct Solutions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Neural Algorithmic Reasoning (NAR) extends classical algorithms to higher\ndimensional data. However, canonical implementations of NAR train neural\nnetworks to return only a single solution, even when there are multiple correct\nsolutions to a problem, such as single-source shortest paths. For some\napplications, it is desirable to recover more than one correct solution. To\nthat end, we give the first method for NAR with multiple solutions. We\ndemonstrate our method on two classical algorithms: Bellman-Ford (BF) and\nDepth-First Search (DFS), favouring deeper insight into two algorithms over a\nbroader survey of algorithms. This method involves generating appropriate\ntraining data as well as sampling and validating solutions from model output.\nEach step of our method, which can serve as a framework for neural algorithmic\nreasoning beyond the tasks presented in this paper, might be of independent\ninterest to the field and our results represent the first attempt at this task\nin the NAR literature."}
{"id": "2505.07163", "pdf": "https://arxiv.org/pdf/2505.07163", "abs": "https://arxiv.org/abs/2505.07163", "authors": ["Natalia G. Berloff"], "title": "Exact Spin Elimination in Ising Hamiltonians and Energy-Based Machine Learning", "categories": ["quant-ph", "cs.DM", "cs.DS", "cs.ET", "cs.LG"], "comment": "28 pages, 6 figures", "summary": "We present an exact spin-elimination technique that reduces the\ndimensionality of both quadratic and k-local Ising Hamiltonians while\npreserving their original ground-state configurations. By systematically\nreplacing each removed spin with an effective interaction among its neighbors,\nour method lowers the total spin count without invoking approximations or\niterative recalculations. This capability is especially beneficial for\nhardware-constrained platforms, classical or quantum, that can directly\nimplement multi-body interactions but have limited qubit or spin resources. We\ndemonstrate three key advances enabled by this technique. First, we handle\nlarger instances of benchmark problems such as Max-Cut on cubic graphs without\nexceeding a 2-local interaction limit. Second, we reduce qubit requirements in\nQAOA-based integer factorization on near-term quantum devices, thus extending\nthe feasible range of integers to be factorized. Third, we improve memory\ncapacity in Hopfield associative memories and enhance memory retrieval by\nsuppressing spurious attractors, enhancing retrieval performance. Our\nspin-elimination procedure trades local spin complexity for higher-order\ncouplings or higher node degrees in a single pass, opening new avenues for\nscaling up combinatorial optimization and energy-based machine learning on\nnear-term hardware. Finally, these results underscore that the next-generation\nphysical spin machines will likely capitalize on k-local spin Hamiltonians to\noffer an alternative to classical computations."}
{"id": "2412.14489", "pdf": "https://arxiv.org/pdf/2412.14489", "abs": "https://arxiv.org/abs/2412.14489", "authors": ["Shu Shen", "C. L. Philip Chen", "Tong Zhang"], "title": "Multi-QuAD: Multi-Level Quality-Adaptive Dynamic Network for Reliable Multimodal Classification", "categories": ["cs.CV"], "comment": "12 pages, 11 figures", "summary": "Multimodal machine learning has achieved remarkable progress in many\nscenarios, but its reliability is undermined by varying sample quality. This\npaper finds that existing reliable multimodal classification methods not only\nfail to provide robust estimation of data quality, but also lack dynamic\nnetworks for sample-specific depth and parameters to achieve reliable\ninference. To this end, a novel framework for multimodal reliable\nclassification termed \\textit{Multi-level Quality-Adaptive Dynamic multimodal\nnetwork} (Multi-QuAD) is proposed. Multi-QuAD first adopts a novel approach\nbased on noise-free prototypes and a classifier-free design to reliably\nestimate the quality of each sample at both modality and feature levels. It\nthen achieves sample-specific network depth via the \\textbf{\\textit{Global\nConfidence Normalized Depth (GCND)}} mechanism. By normalizing depth across\nmodalities and samples, \\textit{\\textbf{GCND}} effectively mitigates the impact\nof challenging modality inputs on dynamic depth reliability. Furthermore,\nMulti-QuAD provides sample-adaptive network parameters via the\n\\textbf{\\textit{Layer-wise Greedy Parameter (LGP)}} mechanism driven by\nfeature-level quality. The cross-modality layer-wise greedy strategy in\n\\textbf{\\textit{LGP}} designs a reliable parameter prediction paradigm for\nmultimodal networks with variable architecture for the first time. Experiments\nconducted on four datasets demonstrate that Multi-QuAD significantly\noutperforms state-of-the-art methods in classification performance and\nreliability, exhibiting strong adaptability to data with diverse quality."}
{"id": "2409.09787", "pdf": "https://arxiv.org/pdf/2409.09787", "abs": "https://arxiv.org/abs/2409.09787", "authors": ["RuiKang OuYang", "Bo Qiang", "José Miguel Hernández-Lobato"], "title": "BNEM: A Boltzmann Sampler Based on Bootstrapped Noised Energy Matching", "categories": ["cs.LG", "cs.AI", "stat.CO", "stat.ML"], "comment": "38 pages, 10 figures, 10 tables", "summary": "Developing an efficient sampler capable of generating independent and\nidentically distributed (IID) samples from a Boltzmann distribution is a\ncrucial challenge in scientific research, e.g. molecular dynamics. In this\nwork, we intend to learn neural samplers given energy functions instead of data\nsampled from the Boltzmann distribution. By learning the energies of the noised\ndata, we propose a diffusion-based sampler, Noised Energy Matching, which\ntheoretically has lower variance and more complexity compared to related works.\nFurthermore, a novel bootstrapping technique is applied to NEM to balance\nbetween bias and variance. We evaluate NEM and BNEM on a 2-dimensional 40\nGaussian Mixture Model (GMM) and a 4-particle double-well potential (DW-4). The\nexperimental results demonstrate that BNEM can achieve state-of-the-art\nperformance while being more robust."}
{"id": "2505.07244", "pdf": "https://arxiv.org/pdf/2505.07244", "abs": "https://arxiv.org/abs/2505.07244", "authors": ["Christian Kuehn", "Sara-Viola Kuntz"], "title": "The Influence of the Memory Capacity of Neural DDEs on the Universal Approximation Property", "categories": ["math.DS", "cs.LG", "cs.NE"], "comment": null, "summary": "Neural Ordinary Differential Equations (Neural ODEs), which are the\ncontinuous-time analog of Residual Neural Networks (ResNets), have gained\nsignificant attention in recent years. Similarly, Neural Delay Differential\nEquations (Neural DDEs) can be interpreted as an infinite depth limit of\nDensely Connected Residual Neural Networks (DenseResNets). In contrast to\ntraditional ResNet architectures, DenseResNets are feed-forward networks that\nallow for shortcut connections across all layers. These additional connections\nintroduce memory in the network architecture, as typical in many modern\narchitectures. In this work, we explore how the memory capacity in neural DDEs\ninfluences the universal approximation property. The key parameter for studying\nthe memory capacity is the product $K \\tau$ of the Lipschitz constant and the\ndelay of the DDE. In the case of non-augmented architectures, where the network\nwidth is not larger than the input and output dimensions, neural ODEs and\nclassical feed-forward neural networks cannot have the universal approximation\nproperty. We show that if the memory capacity $K\\tau$ is sufficiently small,\nthe dynamics of the neural DDE can be approximated by a neural ODE.\nConsequently, non-augmented neural DDEs with a small memory capacity also lack\nthe universal approximation property. In contrast, if the memory capacity\n$K\\tau$ is sufficiently large, we can establish the universal approximation\nproperty of neural DDEs for continuous functions. If the neural DDE\narchitecture is augmented, we can expand the parameter regions in which\nuniversal approximation is possible. Overall, our results show that by\nincreasing the memory capacity $K\\tau$, the infinite-dimensional phase space of\nDDEs with positive delay $\\tau>0$ is not sufficient to guarantee a direct jump\ntransition to universal approximation, but only after a certain memory\nthreshold, universal approximation holds."}
{"id": "2501.00843", "pdf": "https://arxiv.org/pdf/2501.00843", "abs": "https://arxiv.org/abs/2501.00843", "authors": ["Nathanael L. Baisa"], "title": "FusionSORT: Fusion Methods for Online Multi-object Visual Tracking", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we investigate four different fusion methods for associating\ndetections to tracklets in multi-object visual tracking. In addition to\nconsidering strong cues such as motion and appearance information, we also\nconsider weak cues such as height intersection-over-union (height-IoU) and\ntracklet confidence information in the data association using different fusion\nmethods. These fusion methods include minimum, weighted sum based on IoU,\nKalman filter (KF) gating, and hadamard product of costs due to the different\ncues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and\nDanceTrack datasets, and find out that the choice of a fusion method is key for\ndata association in multi-object visual tracking. We hope that this\ninvestigative work helps the computer vision research community to use the\nright fusion method for data association in multi-object visual tracking."}
{"id": "2410.01265", "pdf": "https://arxiv.org/pdf/2410.01265", "abs": "https://arxiv.org/abs/2410.01265", "authors": ["Haodong Liang", "Krishnakumar Balasubramanian", "Lifeng Lai"], "title": "Transformers Handle Endogeneity in In-Context Linear Regression", "categories": ["stat.ML", "cs.AI", "cs.LG", "econ.EM", "math.ST", "stat.TH"], "comment": "37 pages, 8 figures", "summary": "We explore the capability of transformers to address endogeneity in\nin-context linear regression. Our main finding is that transformers inherently\npossess a mechanism to handle endogeneity effectively using instrumental\nvariables (IV). First, we demonstrate that the transformer architecture can\nemulate a gradient-based bi-level optimization procedure that converges to the\nwidely used two-stage least squares $(\\textsf{2SLS})$ solution at an\nexponential rate. Next, we propose an in-context pretraining scheme and provide\ntheoretical guarantees showing that the global minimizer of the pre-training\nloss achieves a small excess loss. Our extensive experiments validate these\ntheoretical findings, showing that the trained transformer provides more robust\nand reliable in-context predictions and coefficient estimates than the\n$\\textsf{2SLS}$ method, in the presence of endogeneity."}
{"id": "2505.07267", "pdf": "https://arxiv.org/pdf/2505.07267", "abs": "https://arxiv.org/abs/2505.07267", "authors": ["Gerardo Duran-Martin"], "title": "Adaptive, Robust and Scalable Bayesian Filtering for Online Learning", "categories": ["stat.ML", "cs.LG"], "comment": "PhD thesis", "summary": "In this thesis, we introduce Bayesian filtering as a principled framework for\ntackling diverse sequential machine learning problems, including online\n(continual) learning, prequential (one-step-ahead) forecasting, and contextual\nbandits. To this end, this thesis addresses key challenges in applying Bayesian\nfiltering to these problems: adaptivity to non-stationary environments,\nrobustness to model misspecification and outliers, and scalability to the\nhigh-dimensional parameter space of deep neural networks. We develop novel\ntools within the Bayesian filtering framework to address each of these\nchallenges, including: (i) a modular framework that enables the development\nadaptive approaches for online learning; (ii) a novel, provably robust filter\nwith similar computational cost to standard filters, that employs Generalised\nBayes; and (iii) a set of tools for sequentially updating model parameters\nusing approximate second-order optimisation methods that exploit the\noverparametrisation of high-dimensional parametric models such as neural\nnetworks. Theoretical analysis and empirical results demonstrate the improved\nperformance of our methods in dynamic, high-dimensional, and misspecified\nmodels."}
{"id": "2501.16003", "pdf": "https://arxiv.org/pdf/2501.16003", "abs": "https://arxiv.org/abs/2501.16003", "authors": ["Zhibo Ren", "Pritthijit Nath", "Pancham Shukla"], "title": "Improving Tropical Cyclone Forecasting With Video Diffusion Models", "categories": ["cs.CV", "physics.ao-ph"], "comment": "Recommended for spotlight presentation at the ICLR 2025 workshop on\n  Tackling Climate Change with Machine Learning. 7 pages, 7 figures", "summary": "Tropical cyclone (TC) forecasting is crucial for disaster preparedness and\nmitigation. While recent deep learning approaches have shown promise, existing\nmethods often treat TC evolution as a series of independent frame-to-frame\npredictions, limiting their ability to capture long-term dynamics. We present a\nnovel application of video diffusion models for TC forecasting that explicitly\nmodels temporal dependencies through additional temporal layers. Our approach\nenables the model to generate multiple frames simultaneously, better capturing\ncyclone evolution patterns. We introduce a two-stage training strategy that\nsignificantly improves individual-frame quality and performance in low-data\nregimes. Experimental results show our method outperforms the previous approach\nof Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably,\nwe extend the reliable forecasting horizon from 36 to 50 hours. Through\ncomprehensive evaluation using both traditional metrics and Fr\\'echet Video\nDistance (FVD), we demonstrate that our approach produces more temporally\ncoherent forecasts while maintaining competitive single-frame quality. Code\naccessible at https://github.com/Ren-creater/forecast-video-diffmodels."}
{"id": "2410.01639", "pdf": "https://arxiv.org/pdf/2410.01639", "abs": "https://arxiv.org/abs/2410.01639", "authors": ["Elizaveta Tennant", "Stephen Hailes", "Mirco Musolesi"], "title": "Moral Alignment for LLM Agents", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "Published at the 13th International Conference on Learning\n  Representations (ICLR'25), Singapore, Apr 2025.\n  https://openreview.net/forum?id=MeGDmZjUXy", "summary": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare underway to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and their\ntransparency will decrease. Consequently, developing effective methods for\naligning them to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit, opaque and are\nessentially deduced from relative preferences over different model outputs. In\nthis work, instead of relying on human feedback, we introduce the design of\nreward functions that explicitly and transparently encode core human values for\nReinforcement Learning-based fine-tuning of foundation agent models.\nSpecifically, we use intrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques."}
{"id": "2505.07272", "pdf": "https://arxiv.org/pdf/2505.07272", "abs": "https://arxiv.org/abs/2505.07272", "authors": ["Javier Salazar Cavazos", "Jeffrey A. Fessler", "Laura Balzano"], "title": "ALPCAH: Subspace Learning for Sample-wise Heteroscedastic Data", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": null, "summary": "Principal component analysis (PCA) is a key tool in the field of data\ndimensionality reduction. However, some applications involve heterogeneous data\nthat vary in quality due to noise characteristics associated with each data\nsample. Heteroscedastic methods aim to deal with such mixed data quality. This\npaper develops a subspace learning method, named ALPCAH, that can estimate the\nsample-wise noise variances and use this information to improve the estimate of\nthe subspace basis associated with the low-rank structure of the data. Our\nmethod makes no distributional assumptions of the low-rank component and does\nnot assume that the noise variances are known. Further, this method uses a soft\nrank constraint that does not require subspace dimension to be known.\nAdditionally, this paper develops a matrix factorized version of ALPCAH, named\nLR-ALPCAH, that is much faster and more memory efficient at the cost of\nrequiring subspace dimension to be known or estimated. Simulations and real\ndata experiments show the effectiveness of accounting for data\nheteroscedasticity compared to existing algorithms. Code available at\nhttps://github.com/javiersc1/ALPCAH."}
{"id": "2502.00848", "pdf": "https://arxiv.org/pdf/2502.00848", "abs": "https://arxiv.org/abs/2502.00848", "authors": ["Yuanhuiyi Lyu", "Xu Zheng", "Lutao Jiang", "Yibo Yan", "Xin Zou", "Huiyu Zhou", "Linfeng Zhang", "Xuming Hu"], "title": "RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning", "categories": ["cs.CV"], "comment": "Accepted to ICML2025", "summary": "Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux,\nhave achieved notable progress. However, these models are strongly restricted\nto their limited knowledge, a.k.a., their own fixed parameters, that are\ntrained with closed datasets. This leads to significant hallucinations or\ndistortions when facing fine-grained and unseen novel real-world objects, e.g.,\nthe appearance of the Tesla Cybertruck. To this end, we present the first\nreal-object-based retrieval-augmented generation framework (RealRAG), which\naugments fine-grained and unseen novel object generation by learning and\nretrieving real-world images to overcome the knowledge gaps of generative\nmodels. Specifically, to integrate missing memory for unseen novel object\ngeneration, we train a reflective retriever by self-reflective contrastive\nlearning, which injects the generator's knowledge into the sef-reflective\nnegatives, ensuring that the retrieved augmented images compensate for the\nmodel's missing knowledge. Furthermore, the real-object-based framework\nintegrates fine-grained visual knowledge for the generative models, tackling\nthe distortion problem and improving the realism for fine-grained object\ngeneration. Our Real-RAG is superior in its modular application to all types of\nstate-of-the-art text-to-image generative models and also delivers remarkable\nperformance boosts with all of them, such as a gain of 16.18% FID score with\nthe auto-regressive model on the Stanford Car benchmark."}
{"id": "2410.03024", "pdf": "https://arxiv.org/pdf/2410.03024", "abs": "https://arxiv.org/abs/2410.03024", "authors": ["Marcel Kollovieh", "Marten Lienen", "David Lüdke", "Leo Schwinn", "Stephan Günnemann"], "title": "Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Recent advancements in generative modeling, particularly diffusion models,\nhave opened new directions for time series modeling, achieving state-of-the-art\nperformance in forecasting and synthesis. However, the reliance of\ndiffusion-based models on a simple, fixed prior complicates the generative\nprocess since the data and prior distributions differ significantly. We\nintroduce TSFlow, a conditional flow matching (CFM) model for time series\ncombining Gaussian processes, optimal transport paths, and data-dependent prior\ndistributions. By incorporating (conditional) Gaussian processes, TSFlow aligns\nthe prior distribution more closely with the temporal structure of the data,\nenhancing both unconditional and conditional generation. Furthermore, we\npropose conditional prior sampling to enable probabilistic forecasting with an\nunconditionally trained model. In our experimental evaluation on eight\nreal-world datasets, we demonstrate the generative capabilities of TSFlow,\nproducing high-quality unconditional samples. Finally, we show that both\nconditionally and unconditionally trained models achieve competitive results\nacross multiple forecasting benchmarks."}
{"id": "2505.07329", "pdf": "https://arxiv.org/pdf/2505.07329", "abs": "https://arxiv.org/abs/2505.07329", "authors": ["Jordan Frery", "Roman Bredehoft", "Jakub Klemsa", "Arthur Meyre", "Andrei Stoian"], "title": "Private LoRA Fine-tuning of Open-Source LLMs with Homomorphic Encryption", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Preserving data confidentiality during the fine-tuning of open-source Large\nLanguage Models (LLMs) is crucial for sensitive applications. This work\nintroduces an interactive protocol adapting the Low-Rank Adaptation (LoRA)\ntechnique for private fine-tuning. Homomorphic Encryption (HE) protects the\nconfidentiality of training data and gradients handled by remote worker nodes\nperforming the bulk of computations involving the base model weights. The data\nowner orchestrates training, requiring minimal local computing power and\nmemory, thus alleviating the need for expensive client-side GPUs. We\ndemonstrate feasibility by fine-tuning a Llama-3.2-1B model, presenting\nconvergence results using HE-compatible quantization and performance benchmarks\nfor HE computations on GPU hardware. This approach enables applications such as\nconfidential knowledge base question answering, private codebase fine-tuning\nfor AI code assistants, AI agents for drafting emails based on a company's\nemail archive, and adapting models to analyze sensitive legal or healthcare\ndocuments."}
{"id": "2502.02283", "pdf": "https://arxiv.org/pdf/2502.02283", "abs": "https://arxiv.org/abs/2502.02283", "authors": ["Zhihao Guo", "Jingxuan Su", "Shenglin Wang", "Jinlong Fan", "Jing Zhang", "Wei Zhou", "Hadi Amirpour", "Yunlong Zhao", "Liangxiu Han", "Peng Wang"], "title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "68T45"], "comment": "12 pages, 7 figures", "summary": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds often limits scene reconstruction quality. To address the\nlimitation, this paper proposes a novel 3D reconstruction framework, Gaussian\nProcesses enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian\nProcess model is developed to enable adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. These densified point clouds\nprovide high-quality initial 3D Gaussians, enhancing reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework."}
{"id": "2410.05740", "pdf": "https://arxiv.org/pdf/2410.05740", "abs": "https://arxiv.org/abs/2410.05740", "authors": ["Guoqiang Wu", "Cheng Hu", "Wangjia Weng", "Zhouheng Li", "Yonghao Fu", "Lei Xie", "Hongye Su"], "title": "Learning to Drift in Extreme Turning with Active Exploration and Gaussian Process Based MPC", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Extreme cornering in racing often leads to large sideslip angles, presenting\na significant challenge for vehicle control. Conventional vehicle controllers\nstruggle to manage this scenario, necessitating the use of a drifting\ncontroller. However, the large sideslip angle in drift conditions introduces\nmodel mismatch, which in turn affects control precision. To address this issue,\nwe propose a model correction drift controller that integrates Model Predictive\nControl (MPC) with Gaussian Process Regression (GPR). GPR is employed to\ncorrect vehicle model mismatches during both drift equilibrium solving and the\nMPC optimization process. Additionally, the variance from GPR is utilized to\nactively explore different cornering drifting velocities, aiming to minimize\ntrajectory tracking errors. The proposed algorithm is validated through\nsimulations on the Simulink-Carsim platform and experiments with a 1:10 scale\nRC vehicle. In the simulation, the average lateral error with GPR is reduced by\n52.8% compared to the non-GPR case. Incorporating exploration further decreases\nthis error by 27.1%. The velocity tracking Root Mean Square Error (RMSE) also\ndecreases by 10.6% with exploration. In the RC car experiment, the average\nlateral error with GPR is 36.7% lower, and exploration further leads to a 29.0%\nreduction. Moreover, the velocity tracking RMSE decreases by 7.2% with the\ninclusion of exploration."}
{"id": "2505.07487", "pdf": "https://arxiv.org/pdf/2505.07487", "abs": "https://arxiv.org/abs/2505.07487", "authors": ["Heraldo Borges", "Juliana Alves Pereira", "Djamel Eddine Khelladi", "Mathieu Acher"], "title": "Linux Kernel Configurations at Scale: A Dataset for Performance and Evolution Analysis", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Configuring the Linux kernel to meet specific requirements, such as binary\nsize, is highly challenging due to its immense complexity-with over 15,000\ninterdependent options evolving rapidly across different versions. Although\nseveral studies have explored sampling strategies and machine learning methods\nto understand and predict the impact of configuration options, the literature\nstill lacks a comprehensive and large-scale dataset encompassing multiple\nkernel versions along with detailed quantitative measurements. To bridge this\ngap, we introduce LinuxData, an accessible collection of kernel configurations\nspanning several kernel releases, specifically from versions 4.13 to 5.8. This\ndataset, gathered through automated tools and build processes, comprises over\n240,000 kernel configurations systematically labeled with compilation outcomes\nand binary sizes. By providing detailed records of configuration evolution and\ncapturing the intricate interplay among kernel options, our dataset enables\ninnovative research in feature subset selection, prediction models based on\nmachine learning, and transfer learning across kernel versions. Throughout this\npaper, we describe how the dataset has been made easily accessible via OpenML\nand illustrate how it can be leveraged using only a few lines of Python code to\nevaluate AI-based techniques, such as supervised machine learning. We\nanticipate that this dataset will significantly enhance reproducibility and\nfoster new insights into configuration-space analysis at a scale that presents\nunique opportunities and inherent challenges, thereby advancing our\nunderstanding of the Linux kernel's configurability and evolution."}
{"id": "2502.02590", "pdf": "https://arxiv.org/pdf/2502.02590", "abs": "https://arxiv.org/abs/2502.02590", "authors": ["Xiaowen Qiu", "Jincheng Yang", "Yian Wang", "Zhehuan Chen", "Yufei Wang", "Tsun-Hsuan Wang", "Zhou Xian", "Chuang Gan"], "title": "Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D articulated objects modeling has long been a challenging problem, since it\nrequires to capture both accurate surface geometries and semantically\nmeaningful and spatially precise structures, parts, and joints. Existing\nmethods heavily depend on training data from a limited set of handcrafted\narticulated object categories (e.g., cabinets and drawers), which restricts\ntheir ability to model a wide range of articulated objects in an\nopen-vocabulary context. To address these limitations, we propose Articulate\nAnymesh, an automated framework that is able to convert any rigid 3D mesh into\nits articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our\nframework utilizes advanced Vision-Language Models and visual prompting\ntechniques to extract semantic information, allowing for both the segmentation\nof object parts and the construction of functional joints. Our experiments show\nthat Articulate Anymesh can generate large-scale, high-quality 3D articulated\nobjects, including tools, toys, mechanical devices, and vehicles, significantly\nexpanding the coverage of existing 3D articulated object datasets.\nAdditionally, we show that these generated assets can facilitate the\nacquisition of new articulated object manipulation skills in simulation, which\ncan then be transferred to a real robotic system. Our Github website is\nhttps://articulate-anymesh.github.io."}
{"id": "2410.07250", "pdf": "https://arxiv.org/pdf/2410.07250", "abs": "https://arxiv.org/abs/2410.07250", "authors": ["Yu Wang", "Yangguang Zhang", "Shengxiang Lin", "Xingyi Zhang", "Han Zhang"], "title": "Lightweight Deep Learning Framework for Accurate Particle Flow Energy Reconstruction", "categories": ["physics.ins-det", "cs.AI"], "comment": "12 pages, 8 figures", "summary": "Under extreme operating conditions, characterized by high particle\nmultiplicity and heavily overlapping shower energy deposits, classical particle\nflow algorithms encounter pronounced limitations in resolution, efficiency, and\naccuracy. To address this challenge, this paper proposes and systematically\nevaluates a deep learning reconstruction framework: For multichannel sparse\nfeatures, we design a hybrid loss function combining weighted mean squared\nerror with structural similarity index, effectively balancing pixel-level\naccuracy and structural fidelity. By integrating 3D convolutions,\nSqueeze-and-Excitation channel attention, and Offset self-attention modules\ninto baseline convolutional neural networks, we enhance the model's capability\nto capture cross-modal spatiotemporal correlations and energy-displacement\nnonlinearities. Validated on custom-constructed simulation data and Pythia jet\ndatasets, the framework's 90K-parameter lightweight variant approaches the\nperformance of 5M-parameter baselines, while the 25M-parameter 3D model\nachieves state-of-the-art results in both interpolation and extrapolation\ntasks. Comprehensive experiments quantitatively evaluate component\ncontributions and provide performance-parameter trade-off guidelines. All core\ncode and data processing scripts are open-sourced on a GitHub repository to\nfacilitate community reproducibility and extension."}
{"id": "2505.07594", "pdf": "https://arxiv.org/pdf/2505.07594", "abs": "https://arxiv.org/abs/2505.07594", "authors": ["Manish Prajapat", "Johannes Köhler", "Amon Lahr", "Andreas Krause", "Melanie N. Zeilinger"], "title": "Finite-Sample-Based Reachability for Safe Control with Gaussian Process Dynamics", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "comment": null, "summary": "Gaussian Process (GP) regression is shown to be effective for learning\nunknown dynamics, enabling efficient and safety-aware control strategies across\ndiverse applications. However, existing GP-based model predictive control\n(GP-MPC) methods either rely on approximations, thus lacking guarantees, or are\noverly conservative, which limits their practical utility. To close this gap,\nwe present a sampling-based framework that efficiently propagates the model's\nepistemic uncertainty while avoiding conservatism. We establish a novel sample\ncomplexity result that enables the construction of a reachable set using a\nfinite number of dynamics functions sampled from the GP posterior. Building on\nthis, we design a sampling-based GP-MPC scheme that is recursively feasible and\nguarantees closed-loop safety and stability with high probability. Finally, we\nshowcase the effectiveness of our method on two numerical examples,\nhighlighting accurate reachable set over-approximation and safe closed-loop\nperformance."}
{"id": "2502.04847", "pdf": "https://arxiv.org/pdf/2502.04847", "abs": "https://arxiv.org/abs/2502.04847", "authors": ["Qijun Gan", "Yi Ren", "Chen Zhang", "Zhenhui Ye", "Pan Xie", "Xiang Yin", "Zehuan Yuan", "Bingyue Peng", "Jianke Zhu"], "title": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation", "categories": ["cs.CV"], "comment": "https://agnjason.github.io/HumanDiT-page/", "summary": "Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios."}
{"id": "2411.10285", "pdf": "https://arxiv.org/pdf/2411.10285", "abs": "https://arxiv.org/abs/2411.10285", "authors": ["Pedro Palacios", "Rafael Medina", "Jean-Luc Rouas", "Giovanni Ansaloni", "David Atienza"], "title": "Systolic Arrays and Structured Pruning Co-design for Efficient Transformers in Edge Systems", "categories": ["cs.AR", "cs.AI", "68T50", "C.3; B.5.1; I.2.7"], "comment": "8 pages, GLSVLSI'25", "summary": "Efficient deployment of resource-intensive transformers on edge devices\nnecessitates cross-stack optimization. We thus study the interrelation between\nstructured pruning and systolic acceleration, matching the size of pruned\nblocks with the systolic array dimensions. In this setting, computations of\npruned weight blocks can be skipped, reducing run-time and energy consumption,\nbut potentially impacting quality of service (QoS). To evaluate the trade-offs\nbetween systolic array size and sparsity opportunities, we present a novel\nco-design framework that integrates algorithmic optimization, system\nsimulation, and hardware design. Targeting speech recognition and machine\ntranslation using transformers as case study, we analyze how configuration\nchoices across the stack affect performance metrics. Results demonstrate that\nstructured pruning on systems featuring systolic array acceleration can\neffectively increase performance, while maintaining high QoS levels. Up to 44%\nsystem-wide speedups due to structured pruning and quantization were measured,\nwith only 1.4% word error rate degradation on the standard LibriSpeech dataset."}
{"id": "2505.07607", "pdf": "https://arxiv.org/pdf/2505.07607", "abs": "https://arxiv.org/abs/2505.07607", "authors": ["Georg Schäfer", "Raphael Seliger", "Jakob Rehrl", "Stefan Huber", "Simon Hirlaender"], "title": "Multi-Objective Reinforcement Learning for Energy-Efficient Industrial Control", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "Accepted at DEXA 2025 (AI4IP)", "summary": "Industrial automation increasingly demands energy-efficient control\nstrategies to balance performance with environmental and cost constraints. In\nthis work, we present a multi-objective reinforcement learning (MORL) framework\nfor energy-efficient control of the Quanser Aero 2 testbed in its\none-degree-of-freedom configuration. We design a composite reward function that\nsimultaneously penalizes tracking error and electrical power consumption.\nPreliminary experiments explore the influence of varying the Energy penalty\nweight, alpha, on the trade-off between pitch tracking and energy savings. Our\nresults reveal a marked performance shift for alpha values between 0.0 and\n0.25, with non-Pareto optimal solutions emerging at lower alpha values, on both\nthe simulation and the real system. We hypothesize that these effects may be\nattributed to artifacts introduced by the adaptive behavior of the Adam\noptimizer, which could bias the learning process and favor bang-bang control\nstrategies. Future work will focus on automating alpha selection through\nGaussian Process-based Pareto front modeling and transitioning the approach\nfrom simulation to real-world deployment."}
{"id": "2502.16032", "pdf": "https://arxiv.org/pdf/2502.16032", "abs": "https://arxiv.org/abs/2502.16032", "authors": ["Lijun Yan", "Churan Wang", "Fangwei Zhong", "Yizhou Wang"], "title": "Clinical Inspired MRI Lesion Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "accepted in ISBI2025 oral", "summary": "Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting\npathological tissues in various diseases. Different MRI sequences have\ndifferent contrast mechanisms and sensitivities for different types of lesions,\nwhich pose challenges to accurate and consistent lesion segmentation. In\nclinical practice, radiologists commonly use the sub-sequence feature, i.e. the\ndifference between post contrast-enhanced T1-weighted (post) and\npre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we\npropose a residual fusion method to learn subsequence representation for MRI\nlesion segmentation. Specifically, we iteratively and adaptively fuse features\nfrom pre- and post-contrast sequences at multiple resolutions, using dynamic\nweights to achieve optimal fusion and address diverse lesion enhancement\npatterns. Our method achieves state-of-the-art performances on BraTS2023\ndataset for brain tumor segmentation and our in-house breast MRI dataset for\nbreast lesion segmentation. Our method is clinically inspired and has the\npotential to facilitate lesion segmentation in various applications."}
{"id": "2411.13280", "pdf": "https://arxiv.org/pdf/2411.13280", "abs": "https://arxiv.org/abs/2411.13280", "authors": ["Keyue Qiu", "Yuxuan Song", "Jie Yu", "Hongbo Ma", "Ziyao Cao", "Zhilong Zhang", "Yushuai Wu", "Mingyue Zheng", "Hao Zhou", "Wei-Ying Ma"], "title": "Empower Structure-Based Molecule Optimization with Gradient Guidance", "categories": ["q-bio.BM", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Structure-Based molecule optimization (SBMO) aims to optimize molecules with\nboth continuous coordinates and discrete types against protein targets. A\npromising direction is to exert gradient guidance on generative models given\nits remarkable success in images, but it is challenging to guide discrete data\nand risks inconsistencies between modalities. To this end, we leverage a\ncontinuous and differentiable space derived through Bayesian inference,\npresenting Molecule Joint Optimization (MolJO), the gradient-based SBMO\nframework that facilitates joint guidance signals across different modalities\nwhile preserving SE(3)-equivariance. We introduce a novel backward correction\nstrategy that optimizes within a sliding window of the past histories, allowing\nfor a seamless trade-off between explore-and-exploit during optimization. MolJO\nachieves state-of-the-art performance on CrossDocked2020 benchmark (Success\nRate 51.3%, Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success\nRate compared to the gradient-based counterpart, and 2x \"Me-Better\" Ratio as\nmuch as 3D baselines. Furthermore, we extend MolJO to a wide range of\noptimization settings, including multi-objective optimization and challenging\ntasks in drug design such as R-group optimization and scaffold hopping, further\nunderscoring its versatility."}
{"id": "2505.07640", "pdf": "https://arxiv.org/pdf/2505.07640", "abs": "https://arxiv.org/abs/2505.07640", "authors": ["Haolin Zou", "Arnab Auddy", "Yongchan Kwon", "Kamiar Rahnama Rad", "Arian Maleki"], "title": "Certified Data Removal Under High-dimensional Settings", "categories": ["stat.ML", "cs.LG"], "comment": "46 pages, 4 figures", "summary": "Machine unlearning focuses on the computationally efficient removal of\nspecific training data from trained models, ensuring that the influence of\nforgotten data is effectively eliminated without the need for full retraining.\nDespite advances in low-dimensional settings, where the number of parameters \\(\np \\) is much smaller than the sample size \\( n \\), extending similar\ntheoretical guarantees to high-dimensional regimes remains challenging. We\npropose an unlearning algorithm that starts from the original model parameters\nand performs a theory-guided sequence of Newton steps \\( T \\in \\{ 1,2\\}\\).\nAfter this update, carefully scaled isotropic Laplacian noise is added to the\nestimate to ensure that any (potential) residual influence of forget data is\ncompletely removed. We show that when both \\( n, p \\to \\infty \\) with a fixed\nratio \\( n/p \\), significant theoretical and computational obstacles arise due\nto the interplay between the complexity of the model and the finite\nsignal-to-noise ratio. Finally, we show that, unlike in low-dimensional\nsettings, a single Newton step is insufficient for effective unlearning in\nhigh-dimensional problems -- however, two steps are enough to achieve the\ndesired certifiebility. We provide numerical experiments to support the\ncertifiability and accuracy claims of this approach."}
{"id": "2502.19159", "pdf": "https://arxiv.org/pdf/2502.19159", "abs": "https://arxiv.org/abs/2502.19159", "authors": ["Xuan Ding", "Rui Sun", "Yunjian Zhang", "Xiu Yan", "Yueqi Zhou", "Kaihao Huang", "Suzhong Fu", "Chuanlong Xie", "Yao Zhu"], "title": "A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Compared to width-wise pruning, depth-wise pruning can significantly\naccelerate inference in resource-constrained scenarios. However, treating the\nentire Transformer layer as the minimum pruning unit may degrade model\nperformance by indiscriminately discarding the entire information of the layer.\nThis paper reveals the ``Patch-like'' feature relationship between layers in\nlarge language models by analyzing the correlation of the outputs of different\nlayers in the reproducing kernel Hilbert space. Building on this observation,\nwe propose a sliding layer merging method that dynamically selects and fuses\nconsecutive layers from top to bottom according to a pre-defined similarity\nthreshold, thereby simplifying the model structure while maintaining its\nperformance. Extensive experiments on LLMs with various architectures and\ndifferent parameter scales show that our method outperforms existing pruning\ntechniques in both zero-shot inference performance and retraining recovery\nquality after pruning. In particular, in the experiment with 35% pruning on the\nVicuna-7B model, our method achieved a 1.654% improvement in average\nperformance on zero-shot tasks compared to the existing method. Moreover, we\nfurther reveal the potential of combining depth pruning with width pruning to\nenhance the pruning effect. Our codes are available at\nhttps://github.com/920927/SLM-a-sliding-layer-merging-method."}
{"id": "2412.11293", "pdf": "https://arxiv.org/pdf/2412.11293", "abs": "https://arxiv.org/abs/2412.11293", "authors": ["Ashish Parmanand Pandey", "Alan John Varghese", "Sarang Patil", "Mengjia Xu"], "title": "A Comparative Study on Dynamic Graph Embedding based on Mamba and Transformers", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 6 figures", "summary": "Dynamic graph embedding has emerged as an important technique for modeling\ncomplex time-evolving networks across diverse domains. While transformer-based\nmodels have shown promise in capturing long-range dependencies in temporal\ngraph data, they face scalability challenges due to quadratic computational\ncomplexity. This study presents a comparative analysis of dynamic graph\nembedding approaches using transformers and the recently proposed Mamba\narchitecture, a state-space model with linear complexity. We introduce three\nnovel models: TransformerG2G augment with graph convolutional networks,\n\\mathcal{DG}-Mamba, and \\mathcal{GDG}-Mamba with graph isomorphism network edge\nconvolutions. Our experiments on multiple benchmark datasets demonstrate that\nMamba-based models achieve comparable or superior performance to\ntransformer-based approaches in link prediction tasks while offering\nsignificant computational efficiency gains on longer sequences. Notably,\n\\mathcal{DG}-Mamba variants consistently outperform transformer-based models on\ndatasets with high temporal variability, such as UCI, Bitcoin, and Reality\nMining, while maintaining competitive performance on more stable graphs like\nSBM. We provide insights into the learned temporal dependencies through\nanalysis of attention weights and state matrices, revealing the models' ability\nto capture complex temporal patterns. By effectively combining state-space\nmodels with graph neural networks, our work addresses key limitations of\nprevious approaches and contributes to the growing body of research on\nefficient temporal graph representation learning. These findings offer\npromising directions for scaling dynamic graph embedding to larger, more\ncomplex real-world networks, potentially enabling new applications in areas\nsuch as social network analysis, financial modeling, and biological system\ndynamics."}
{"id": "2505.07642", "pdf": "https://arxiv.org/pdf/2505.07642", "abs": "https://arxiv.org/abs/2505.07642", "authors": ["Yulong Lu", "Pierre Monmarché"], "title": "Convergence of Time-Averaged Mean Field Gradient Descent Dynamics for Continuous Multi-Player Zero-Sum Games", "categories": ["math.OC", "cs.LG", "math.AP", "math.PR", "stat.ML", "35Q89, 49N80, 91A16, 90C47"], "comment": "21 pages", "summary": "The approximation of mixed Nash equilibria (MNE) for zero-sum games with\nmean-field interacting players has recently raised much interest in machine\nlearning. In this paper we propose a mean-field gradient descent dynamics for\nfinding the MNE of zero-sum games involving $K$ players with $K\\geq 2$. The\nevolution of the players' strategy distributions follows coupled mean-field\ngradient descent flows with momentum, incorporating an exponentially discounted\ntime-averaging of gradients. First, in the case of a fixed entropic\nregularization, we prove an exponential convergence rate for the mean-field\ndynamics to the mixed Nash equilibrium with respect to the total variation\nmetric. This improves a previous polynomial convergence rate for a similar\ntime-averaged dynamics with different averaging factors. Moreover, unlike\nprevious two-scale approaches for finding the MNE, our approach treats all\nplayer types on the same time scale. We also show that with a suitable choice\nof decreasing temperature, a simulated annealing version of the mean-field\ndynamics converges to an MNE of the initial unregularized problem."}
{"id": "2503.01103", "pdf": "https://arxiv.org/pdf/2503.01103", "abs": "https://arxiv.org/abs/2503.01103", "authors": ["Kaiwen Zheng", "Yongxin Chen", "Huayu Chen", "Guande He", "Ming-Yu Liu", "Jun Zhu", "Qinsheng Zhang"], "title": "Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025 Spotlight Project Page:\n  https://research.nvidia.com/labs/dir/ddo/ Code: https://github.com/NVlabs/DDO", "summary": "While likelihood-based generative models, particularly diffusion and\nautoregressive models, have achieved remarkable fidelity in visual generation,\nthe maximum likelihood estimation (MLE) objective, which minimizes the forward\nKL divergence, inherently suffers from a mode-covering tendency that limits the\ngeneration quality under limited model capacity. In this work, we propose\nDirect Discriminative Optimization (DDO) as a unified framework that integrates\nlikelihood-based generative training and GAN-type discrimination to bypass this\nfundamental constraint by exploiting reverse KL and self-generated negative\nsignals. Our key insight is to parameterize a discriminator implicitly using\nthe likelihood ratio between a learnable target model and a fixed reference\nmodel, drawing parallels with the philosophy of Direct Preference Optimization\n(DPO). Unlike GANs, this parameterization eliminates the need for joint\ntraining of generator and discriminator networks, allowing for direct,\nefficient, and effective finetuning of a well-trained model to its full\npotential beyond the limits of MLE. DDO can be performed iteratively in a\nself-play manner for progressive model refinement, with each round requiring\nless than 1% of pretraining epochs. Our experiments demonstrate the\neffectiveness of DDO by significantly advancing the previous SOTA diffusion\nmodel EDM, reducing FID scores from 1.79/1.58/1.96 to new records of\n1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512x512 datasets without any\nguidance mechanisms, and by consistently improving both guidance-free and\nCFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256."}
{"id": "2412.16746", "pdf": "https://arxiv.org/pdf/2412.16746", "abs": "https://arxiv.org/abs/2412.16746", "authors": ["Tai-Quan Peng", "Kaiqi Yang", "Sanguk Lee", "Hang Li", "Yucheng Chu", "Yuping Lin", "Hui Liu"], "title": "Beyond Partisan Leaning: A Comparative Analysis of Political Bias in Large Language Models", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly embedded in civic,\neducational, and political information environments, concerns about their\npotential political bias have grown. Prior research often evaluates such bias\nthrough simulated personas or predefined ideological typologies, which may\nintroduce artificial framing effects or overlook how models behave in general\nuse scenarios. This study adopts a persona-free, topic-specific approach to\nevaluate political behavior in LLMs, reflecting how users typically interact\nwith these systems-without ideological role-play or conditioning. We introduce\na two-dimensional framework: one axis captures partisan orientation on highly\npolarized topics (e.g., abortion, immigration), and the other assesses\nsociopolitical engagement on less polarized issues (e.g., climate change,\nforeign policy). Using survey-style prompts drawn from the ANES and Pew\nResearch Center, we analyze responses from 43 LLMs developed in the U.S.,\nEurope, China, and the Middle East. We propose an entropy-weighted bias score\nto quantify both the direction and consistency of partisan alignment, and\nidentify four behavioral clusters through engagement profiles. Findings show\nmost models lean center-left or left ideologically and vary in their\nnonpartisan engagement patterns. Model scale and openness are not strong\npredictors of behavior, suggesting that alignment strategy and institutional\ncontext play a more decisive role in shaping political expression."}
{"id": "2505.07676", "pdf": "https://arxiv.org/pdf/2505.07676", "abs": "https://arxiv.org/abs/2505.07676", "authors": ["Nicolas Camenzind", "Damir Filipovic"], "title": "Transfer Learning Across Fixed-Income Product Classes", "categories": ["stat.ML", "cs.LG", "q-fin.CP", "q-fin.MF"], "comment": null, "summary": "We propose a framework for transfer learning of discount curves across\ndifferent fixed-income product classes. Motivated by challenges in estimating\ndiscount curves from sparse or noisy data, we extend kernel ridge regression\n(KR) to a vector-valued setting, formulating a convex optimization problem in a\nvector-valued reproducing kernel Hilbert space (RKHS). Each component of the\nsolution corresponds to the discount curve implied by a specific product class.\nWe introduce an additional regularization term motivated by economic\nprinciples, promoting smoothness of spread curves between product classes, and\nshow that it leads to a valid separable kernel structure. A main theoretical\ncontribution is a decomposition of the vector-valued RKHS norm induced by\nseparable kernels. We further provide a Gaussian process interpretation of\nvector-valued KR, enabling quantification of estimation uncertainty.\nIllustrative examples demonstrate that transfer learning significantly improves\nextrapolation performance and tightens confidence intervals compared to\nsingle-curve estimation."}
{"id": "2503.01234", "pdf": "https://arxiv.org/pdf/2503.01234", "abs": "https://arxiv.org/abs/2503.01234", "authors": ["Sijin Sun", "Ming Deng", "Xingrui Yu", "Xingyu Xi", "Liangbin Zhao"], "title": "Self-Adaptive Gamma Context-Aware SSM-based Model for Metal Defect Detection", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 5 figures; Accepted for publication at the 2025\n  International Joint Conference on Neural Networks (IJCNN 2025), Rome, Italy,\n  30 June - 5 July", "summary": "Metal defect detection is critical in industrial quality assurance, yet\nexisting methods struggle with grayscale variations and complex defect states,\nlimiting its robustness. To address these challenges, this paper proposes a\nSelf-Adaptive Gamma Context-Aware SSM-based model(GCM-DET). This advanced\ndetection framework integrating a Dynamic Gamma Correction (GC) module to\nenhance grayscale representation and optimize feature extraction for precise\ndefect reconstruction. A State-Space Search Management (SSM) architecture\ncaptures robust multi-scale features, effectively handling defects of varying\nshapes and scales. Focal Loss is employed to mitigate class imbalance and\nrefine detection accuracy. Additionally, the CD5-DET dataset is introduced,\nspecifically designed for port container maintenance, featuring significant\ngrayscale variations and intricate defect patterns. Experimental results\ndemonstrate that the proposed model achieves substantial improvements, with\nmAP@0.5 gains of 27.6\\%, 6.6\\%, and 2.6\\% on the CD5-DET, NEU-DET, and GC10-DET\ndatasets."}
{"id": "2501.06143", "pdf": "https://arxiv.org/pdf/2501.06143", "abs": "https://arxiv.org/abs/2501.06143", "authors": ["Gerd Kortemeyer", "Marina Babayeva", "Giulia Polverini", "Ralf Widenhorn", "Bor Gregorcic"], "title": "Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories", "categories": ["physics.ed-ph", "cs.AI"], "comment": null, "summary": "We investigate the multilingual and multimodal performance of a large\nlanguage model-based artificial intelligence (AI) system, GPT-4o, using a\ndiverse set of physics concept inventories spanning multiple languages and\nsubject categories. The inventories, sourced from the PhysPort website, cover\nclassical physics topics such as mechanics, electromagnetism, optics, and\nthermodynamics, as well as relativity, quantum mechanics, astronomy,\nmathematics, and laboratory skills. Unlike previous text-only studies, we\nuploaded the inventories as images to reflect what a student would see on\npaper, thereby assessing the system's multimodal functionality. Our results\nindicate variation in performance across subjects, with laboratory skills\nstanding out as the weakest. We also observe differences across languages, with\nEnglish and European languages showing the strongest performance. Notably, the\nrelative difficulty of an inventory item is largely independent of the language\nof the survey. When comparing AI results to existing literature on student\nperformance, we find that the AI system outperforms average post-instruction\nundergraduate students in all subject categories except laboratory skills.\nFurthermore, the AI performs worse on items requiring visual interpretation of\nimages than on those that are purely text-based. While our exploratory findings\nshow GPT-4o's potential usefulness in physics education, they highlight the\ncritical need for instructors to foster students' ability to critically\nevaluate AI outputs, adapt curricula thoughtfully in response to AI\nadvancements, and address equity concerns associated with AI integration."}
{"id": "2505.07688", "pdf": "https://arxiv.org/pdf/2505.07688", "abs": "https://arxiv.org/abs/2505.07688", "authors": ["Renzhe Xu", "Kang Wang", "Bo Li"], "title": "Heterogeneous Data Game: Characterizing the Model Competition Across Multiple Data Sources", "categories": ["cs.GT", "cs.LG"], "comment": "ICML 2025", "summary": "Data heterogeneity across multiple sources is common in real-world machine\nlearning (ML) settings. Although many methods focus on enabling a single model\nto handle diverse data, real-world markets often comprise multiple competing ML\nproviders. In this paper, we propose a game-theoretic framework -- the\nHeterogeneous Data Game -- to analyze how such providers compete across\nheterogeneous data sources. We investigate the resulting pure Nash equilibria\n(PNE), showing that they can be non-existent, homogeneous (all providers\nconverge on the same model), or heterogeneous (providers specialize in distinct\ndata sources). Our analysis spans monopolistic, duopolistic, and more general\nmarkets, illustrating how factors such as the \"temperature\" of data-source\nchoice models and the dominance of certain data sources shape equilibrium\noutcomes. We offer theoretical insights into both homogeneous and heterogeneous\nPNEs, guiding regulatory policies and practical strategies for competitive ML\nmarketplaces."}
{"id": "2503.06473", "pdf": "https://arxiv.org/pdf/2503.06473", "abs": "https://arxiv.org/abs/2503.06473", "authors": ["Hanze Li", "Xiande Huang"], "title": "Enhancing Layer Attention Efficiency through Pruning Redundant Retrievals", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures", "summary": "Growing evidence suggests that layer attention mechanisms, which enhance\ninteraction among layers in deep neural networks, have significantly advanced\nnetwork architectures. However, existing layer attention methods suffer from\nredundancy, as attention weights learned by adjacent layers often become highly\nsimilar. This redundancy causes multiple layers to extract nearly identical\nfeatures, reducing the model's representational capacity and increasing\ntraining time. To address this issue, we propose a novel approach to quantify\nredundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent\nlayers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM)\nmethod that accurately identifies and skips redundant layers, thereby\nmaintaining model stability. Our proposed Efficient Layer Attention (ELA)\narchitecture, improves both training efficiency and overall performance,\nachieving a 30% reduction in training time while enhancing performance in tasks\nsuch as image classification and object detection."}
{"id": "2501.15767", "pdf": "https://arxiv.org/pdf/2501.15767", "abs": "https://arxiv.org/abs/2501.15767", "authors": ["Muhammad Maaz", "Timothy C. Y. Chan"], "title": "Formal Verification of Markov Processes with Learned Parameters", "categories": ["cs.LG", "cs.AI", "math.OC", "68Q60 (primary) 90C30, 60J20, 60J22 (secondary)", "F.4.1; G.1.6; I.2.3"], "comment": "9 pages (main manuscript), 3 figures, 1 table", "summary": "We introduce the problem of formally verifying properties of Markov processes\nwhere the parameters are given by the output of machine learning models. For a\nbroad class of machine learning models, including linear models, tree-based\nmodels, and neural networks, verifying properties of Markov chains like\nreachability, hitting time, and total reward can be formulated as a bilinear\nprogram. We develop a decomposition and bound propagation scheme for solving\nthe bilinear program and show through computational experiments that our method\nsolves the problem to global optimality up to 100x faster than state-of-the-art\nsolvers. To demonstrate the practical utility of our approach, we apply it to a\nreal-world healthcare case study. Along with the paper, we release markovml, an\nopen-source tool for building Markov processes, integrating pretrained machine\nlearning models, and verifying their properties, available at\nhttps://github.com/mmaaz-git/markovml."}
{"id": "2505.07714", "pdf": "https://arxiv.org/pdf/2505.07714", "abs": "https://arxiv.org/abs/2505.07714", "authors": ["Almoatssimbillah Saifaldawla", "Eva Lagunas", "Flor Ortiz", "Abuzar B. M. Adam", "Symeon Chatzinotas"], "title": "SmartUT: Receive Beamforming for Spectral Coexistence of NGSO Satellite Systems", "categories": ["eess.SP", "cs.ET", "cs.LG"], "comment": null, "summary": "In this paper, we investigate downlink co-frequency interference (CFI)\nmitigation in non-geostationary satellites orbits (NGSOs) co-existing systems.\nTraditional mitigation techniques, such as Zero-forcing (ZF), produce a null\ntowards the direction of arrivals (DOAs) of the interfering signals, but they\nsuffer from high computational complexity due to matrix inversions and required\nknowledge of the channel state information (CSI). Furthermore, adaptive\nbeamformers, such as sample matrix inversion (SMI)-based minimum variance,\nprovide poor performance when the available snapshots are limited. We propose a\nMamba-based beamformer (MambaBF) that leverages an unsupervised deep learning\n(DL) approach and can be deployed on the user terminal (UT) antenna array, for\nassisting downlink beamforming and CFI mitigation using only a limited number\nof available array snapshots as input, and without CSI knowledge. Simulation\nresults demonstrate that MambaBF consistently outperforms conventional\nbeamforming techniques in mitigating interference and maximizing the\nsignal-to-interference-plus-noise ratio (SINR), particularly under challenging\nconditions characterized by low SINR, limited snapshots, and imperfect CSI."}
{"id": "2503.06587", "pdf": "https://arxiv.org/pdf/2503.06587", "abs": "https://arxiv.org/abs/2503.06587", "authors": ["Xiaoming Peng", "Yixin Yang", "Yang Zhou", "Hui Huang"], "title": "Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction", "categories": ["cs.CV"], "comment": "We found a major error in Sec. 4.3 Novel View Synthesis. We\n  mistakenly used the test-set images in training for NVS experiments, making\n  our results look better than they actually are", "summary": "Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry\nreconstruction quality than the popular 3DGS by using 2D surfels to approximate\nthin surfaces. However, it falls short when dealing with glossy surfaces,\nresulting in visible holes in these areas. We found the reflection\ndiscontinuity causes the issue. To fit the jump from diffuse to specular\nreflection at different viewing angles, depth bias is introduced in the\noptimized Gaussian primitives. To address that, we first replace the depth\ndistortion loss in 2DGS with a novel depth convergence loss, which imposes a\nstrong constraint on depth continuity. Then, we rectified the depth criterion\nin determining the actual surface, which fully accounts for all the\nintersecting Gaussians along the ray. Qualitative and quantitative evaluations\nacross various datasets reveal that our method significantly improves\nreconstruction quality, with more complete and accurate surfaces than 2DGS."}
{"id": "2501.15889", "pdf": "https://arxiv.org/pdf/2501.15889", "abs": "https://arxiv.org/abs/2501.15889", "authors": ["Federico Errica", "Henrik Christiansen", "Viktor Zaverkin", "Mathias Niepert", "Francesco Alesiani"], "title": "Adaptive Width Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "For almost 70 years, researchers have mostly relied on hyper-parameter tuning\nto select the width of neural networks' layers. This paper challenges the\nstatus quo by introducing an easy-to-use technique to learn an unbounded width\nof a neural network's layer during training. The technique does not rely on\nalternate optimization nor hand-crafted gradient heuristics; rather, it jointly\noptimizes the width and the parameters of each layer via simple\nbackpropagation. We apply the technique to a broad range of data domains such\nas tables, images, text, sequences, and graphs, showing how the width adapts to\nthe task's difficulty. The method imposes a soft ordering of importance among\nneurons, by which it also is possible to truncate the trained network at\nvirtually zero cost, achieving a smooth trade-off between performance and\ncompute resources in a structured way. Alternatively, one can dynamically\ncompress the network with no performance degradation. In light of recent\nfoundation models trained on large datasets, believed to require billions of\nparameters and where hyper-parameter tuning is unfeasible due to humongous\ntraining costs, our approach stands as a viable alternative for width learning."}
{"id": "2505.07719", "pdf": "https://arxiv.org/pdf/2505.07719", "abs": "https://arxiv.org/abs/2505.07719", "authors": ["Hyunwoo Oh"], "title": "Training neural control variates using correlated configurations", "categories": ["hep-lat", "cs.LG", "nucl-th"], "comment": "8 pages, 6 figures", "summary": "Neural control variates (NCVs) have emerged as a powerful tool for variance\nreduction in Monte Carlo (MC) simulations, particularly in high-dimensional\nproblems where traditional control variates are difficult to construct\nanalytically. By training neural networks to learn auxiliary functions\ncorrelated with the target observable, NCVs can significantly reduce estimator\nvariance while preserving unbiasedness. However, a critical but often\noverlooked aspect of NCV training is the role of autocorrelated samples\ngenerated by Markov Chain Monte Carlo (MCMC). While such samples are typically\ndiscarded for error estimation due to their statistical redundancy, they may\ncontain useful information about the structure of the underlying probability\ndistribution that can benefit the training process. In this work, we\nsystematically examine the effect of using correlated configurations in\ntraining neural control variates. We demonstrate, both conceptually and\nnumerically, that training on correlated data can improve control variate\nperformance, especially in settings with limited computational resources. Our\nanalysis includes empirical results from $U(1)$ gauge theory and scalar field\ntheory, illustrating when and how autocorrelated samples enhance NCV\nconstruction. These findings provide practical guidance for the efficient use\nof MCMC data in training neural networks."}
{"id": "2503.08507", "pdf": "https://arxiv.org/pdf/2503.08507", "abs": "https://arxiv.org/abs/2503.08507", "authors": ["Qing Jiang", "Lin Wu", "Zhaoyang Zeng", "Tianhe Ren", "Yuda Xiong", "Yihao Chen", "Qin Liu", "Lei Zhang"], "title": "Referring to Any Person", "categories": ["cs.CV"], "comment": null, "summary": "Humans are undoubtedly the most important participants in computer vision,\nand the ability to detect any individual given a natural language description,\na task we define as referring to any person, holds substantial practical value.\nHowever, we find that existing models generally fail to achieve real-world\nusability, and current benchmarks are limited by their focus on one-to-one\nreferring, that hinder progress in this area. In this work, we revisit this\ntask from three critical perspectives: task definition, dataset design, and\nmodel architecture. We first identify five aspects of referable entities and\nthree distinctive characteristics of this task. Next, we introduce HumanRef, a\nnovel dataset designed to tackle these challenges and better reflect real-world\napplications. From a model design perspective, we integrate a multimodal large\nlanguage model with an object detection framework, constructing a robust\nreferring model named RexSeek. Experimental results reveal that\nstate-of-the-art models, which perform well on commonly used benchmarks like\nRefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple\nindividuals. In contrast, RexSeek not only excels in human referring but also\ngeneralizes effectively to common object referring, making it broadly\napplicable across various perception tasks. Code is available at\nhttps://github.com/IDEA-Research/RexSeek"}
{"id": "2501.17888", "pdf": "https://arxiv.org/pdf/2501.17888", "abs": "https://arxiv.org/abs/2501.17888", "authors": ["Shuai Chen", "Yong Zu", "Zhixi Feng", "Shuyuan Yang", "Mengchang Li"], "title": "RadioLLM: Introducing Large Language Model into Cognitive Radio via Hybrid Prompt and Token Reprogrammings", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The growing scarcity of spectrum resources and rapid proliferation of\nwireless devices make efficient radio network management critical. While deep\nlearning-enhanced Cognitive Radio Technology (CRT) provides promising solutions\nfor tasks such as radio signal classification (RSC), denoising, and spectrum\nallocation, existing DL-based CRT frameworks are typically task-specific and\nlack scalability in diverse real-world applications. This limitation naturally\nleads to the exploration of Large Language Models (LLMs), whose exceptional\ncross-domain generalization capabilities offer new potential for advancing CRT.\nTo bridge this gap, we propose RadioLLM, a novel framework that integrates\nHybrid Prompt and Token Reprogramming (HPTR) for combining radio signal\nfeatures with expert knowledge, and a Frequency-Attuned Fusion (FAF) module for\nenhanced high-frequency feature modeling. Extensive evaluations on multiple\nbenchmark datasets demonstrate that RadioLLM achieves superior performance\ncompared to existing baselines in the majority of testing scenarios."}
{"id": "2505.07765", "pdf": "https://arxiv.org/pdf/2505.07765", "abs": "https://arxiv.org/abs/2505.07765", "authors": ["Zihan Shao", "Konstantin Pieper", "Xiaochuan Tian"], "title": "Solving Nonlinear PDEs with Sparse Radial Basis Function Networks", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": "35 pages, 7 figures", "summary": "We propose a novel framework for solving nonlinear PDEs using sparse radial\nbasis function (RBF) networks. Sparsity-promoting regularization is employed to\nprevent over-parameterization and reduce redundant features. This work is\nmotivated by longstanding challenges in traditional RBF collocation methods,\nalong with the limitations of physics-informed neural networks (PINNs) and\nGaussian process (GP) approaches, aiming to blend their respective strengths in\na unified framework. The theoretical foundation of our approach lies in the\nfunction space of Reproducing Kernel Banach Spaces (RKBS) induced by\none-hidden-layer neural networks of possibly infinite width. We prove a\nrepresenter theorem showing that the solution to the sparse optimization\nproblem in the RKBS admits a finite solution and establishes error bounds that\noffer a foundation for generalizing classical numerical analysis. The\nalgorithmic framework is based on a three-phase algorithm to maintain\ncomputational efficiency through adaptive feature selection, second-order\noptimization, and pruning of inactive neurons. Numerical experiments\ndemonstrate the effectiveness of our method and highlight cases where it offers\nnotable advantages over GP approaches. This work opens new directions for\nadaptive PDE solvers grounded in rigorous analysis with efficient,\nlearning-inspired implementation."}
{"id": "2503.08694", "pdf": "https://arxiv.org/pdf/2503.08694", "abs": "https://arxiv.org/abs/2503.08694", "authors": ["Mees M. Flapper", "Elian Bernard", "Sander G. Huisman"], "title": "Multi-camera orientation tracking method for anisotropic particles in particle-laden flows", "categories": ["cs.CV"], "comment": "14 pages, 22 figures", "summary": "A method for particle orientation tracking is developed and demonstrated\nspecifically for anisotropic particles. Using (high-speed) multi-camera\nrecordings of anisotropic particles from different viewpoints, we reconstruct\nthe 3D location and orientation of these particles using their known shape.\nThis paper describes an algorithm which tracks the location and orientation of\nmultiple anisotropic particles over time, enabling detailed investigations of\nlocation, orientation, and rotation statistics. The robustness and error of\nthis method is quantified, and we explore the effects of noise, image size, the\nnumber of used cameras, and the camera arrangement by applying the algorithm to\nsynthetic images. We showcase several use-cases of this method in several\nexperiments (in both quiescent and turbulent fluids), demonstrating the\neffectiveness and broad applicability of the described tracking method. The\nproposed method is shown to work for widely different particle shapes,\nsuccessfully tracks multiple particles simultaneously, and the method can\ndistinguish between different types of particles."}
{"id": "2501.18452", "pdf": "https://arxiv.org/pdf/2501.18452", "abs": "https://arxiv.org/abs/2501.18452", "authors": ["Xi Weng", "Jianing An", "Xudong Ma", "Binhang Qi", "Jie Luo", "Xi Yang", "Jin Song Dong", "Lei Huang"], "title": "Clustering Properties of Self-Supervised Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "Self-supervised learning (SSL) methods via joint embedding architectures have\nproven remarkably effective at capturing semantically rich representations with\nstrong clustering properties, magically in the absence of label supervision.\nDespite this, few of them have explored leveraging these untapped properties to\nimprove themselves. In this paper, we provide an evidence through various\nmetrics that the encoder's output $encoding$ exhibits superior and more stable\nclustering properties compared to other components. Building on this insight,\nwe propose a novel positive-feedback SSL method, termed Representation\nSelf-Assignment (ReSA), which leverages the model's clustering properties to\npromote learning in a self-guided manner. Extensive experiments on standard SSL\nbenchmarks reveal that models pretrained with ReSA outperform other\nstate-of-the-art SSL methods by a significant margin. Finally, we analyze how\nReSA facilitates better clustering properties, demonstrating that it\neffectively enhances clustering performance at both fine-grained and\ncoarse-grained levels, shaping representations that are inherently more\nstructured and semantically meaningful."}
{"id": "2505.07769", "pdf": "https://arxiv.org/pdf/2505.07769", "abs": "https://arxiv.org/abs/2505.07769", "authors": ["Jai Bardhan", "Tanumoy Mandal", "Subhadip Mitra", "Cyrin Neeraj", "Mihir Rawat"], "title": "Tagging fully hadronic exotic decays of the vectorlike $\\mathbf{B}$ quark using a graph neural network", "categories": ["hep-ph", "cs.LG", "hep-ex"], "comment": "13 pages, 10 figures, 3 tables", "summary": "Following up on our earlier study in [J. Bardhan et al., Machine\nlearning-enhanced search for a vectorlike singlet B quark decaying to a singlet\nscalar or pseudoscalar, Phys. Rev. D 107 (2023) 115001; arXiv:2212.02442], we\ninvestigate the LHC prospects of pair-produced vectorlike $B$ quarks decaying\nexotically to a new gauge-singlet (pseudo)scalar field $\\Phi$ and a $b$ quark.\nAfter the electroweak symmetry breaking, the $\\Phi$ decays predominantly to\n$gg/bb$ final states, leading to a fully hadronic $2b+4j$ or $6b$ signature.\nBecause of the large Standard Model background and the lack of leptonic\nhandles, it is a difficult channel to probe. To overcome the challenge, we\nemploy a hybrid deep learning model containing a graph neural network followed\nby a deep neural network. We estimate that such a state-of-the-art deep\nlearning analysis pipeline can lead to a performance comparable to that in the\nsemi-leptonic mode, taking the discovery (exclusion) reach up to about\n$M_B=1.8\\:(2.4)$~TeV at HL-LHC when $B$ decays fully exotically, i.e., BR$(B\n\\to b\\Phi) = 100\\%$."}
{"id": "2503.13179", "pdf": "https://arxiv.org/pdf/2503.13179", "abs": "https://arxiv.org/abs/2503.13179", "authors": ["Yi Zhang", "Ruonan Lin", "Ang Ping"], "title": "A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network", "categories": ["cs.CV"], "comment": null, "summary": "This study proposes a lightweight method for building image super-resolution\nusing a Dilated Contextual Feature Modulation Network (DCFMN). The process\nincludes obtaining high-resolution images, down-sampling them to\nlow-resolution, enhancing the low-resolution images, constructing and training\na lightweight network model, and generating super-resolution outputs. To\naddress challenges such as regular textures and long-range dependencies in\nbuilding images, the DCFMN integrates an expansion separable modulation unit\nand a local feature enhancement module. The former employs multiple expansion\nconvolutions equivalent to a large kernel to efficiently aggregate multi-scale\nfeatures while leveraging a simple attention mechanism for adaptivity. The\nlatter encodes local features, mixes channel information, and ensures no\nadditional computational burden during inference through reparameterization.\nThis approach effectively resolves the limitations of existing lightweight\nsuper-resolution networks in modeling long-range dependencies, achieving\naccurate and efficient global feature modeling without increasing computational\ncosts, and significantly improving both reconstruction quality and lightweight\nefficiency for building image super-resolution models."}
{"id": "2501.19047", "pdf": "https://arxiv.org/pdf/2501.19047", "abs": "https://arxiv.org/abs/2501.19047", "authors": ["Maja Pavlovic"], "title": "Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)", "categories": ["stat.ME", "cs.AI", "cs.CV", "cs.LG", "stat.ML"], "comment": "https://openreview.net/forum?id=BxBeCjQd2y", "summary": "To be considered reliable, a model must be calibrated so that its confidence\nin each decision closely reflects its true outcome. In this blogpost we'll take\na look at the most commonly used definition for calibration and then dive into\na frequently used evaluation measure for model calibration. We'll then cover\nsome of the drawbacks of this measure and how these surfaced the need for\nadditional notions of calibration, which require their own new evaluation\nmeasures. This post is not intended to be an in-depth dissection of all works\non calibration, nor does it focus on how to calibrate models. Instead, it is\nmeant to provide a gentle introduction to the different notions and their\nevaluation measures as well as to re-highlight some issues with a measure that\nis still widely used to evaluate calibration."}
{"id": "2505.07792", "pdf": "https://arxiv.org/pdf/2505.07792", "abs": "https://arxiv.org/abs/2505.07792", "authors": ["Francesco Mori", "Francesca Mignacco"], "title": "Analytic theory of dropout regularization", "categories": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG"], "comment": "17 pages, 8 figures", "summary": "Dropout is a regularization technique widely used in training artificial\nneural networks to mitigate overfitting. It consists of dynamically\ndeactivating subsets of the network during training to promote more robust\nrepresentations. Despite its widespread adoption, dropout probabilities are\noften selected heuristically, and theoretical explanations of its success\nremain sparse. Here, we analytically study dropout in two-layer neural networks\ntrained with online stochastic gradient descent. In the high-dimensional limit,\nwe derive a set of ordinary differential equations that fully characterize the\nevolution of the network during training and capture the effects of dropout. We\nobtain a number of exact results describing the generalization error and the\noptimal dropout probability at short, intermediate, and long training times.\nOur analysis shows that dropout reduces detrimental correlations between hidden\nnodes, mitigates the impact of label noise, and that the optimal dropout\nprobability increases with the level of noise in the data. Our results are\nvalidated by extensive numerical simulations."}
{"id": "2503.13859", "pdf": "https://arxiv.org/pdf/2503.13859", "abs": "https://arxiv.org/abs/2503.13859", "authors": ["Jinseok Bae", "Inwoo Hwang", "Young Yoon Lee", "Ziyu Guo", "Joseph Liu", "Yizhak Ben-Shabat", "Young Min Kim", "Mubbasir Kapadia"], "title": "Less is More: Improving Motion Diffusion Models with Sparse Keyframes", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in motion diffusion models have led to remarkable progress in\ndiverse motion generation tasks, including text-to-motion synthesis. However,\nexisting approaches represent motions as dense frame sequences, requiring the\nmodel to process redundant or less informative frames. The processing of dense\nanimation frames imposes significant training complexity, especially when\nlearning intricate distributions of large motion datasets even with modern\nneural architectures. This severely limits the performance of generative motion\nmodels for downstream tasks. Inspired by professional animators who mainly\nfocus on sparse keyframes, we propose a novel diffusion framework explicitly\ndesigned around sparse and geometrically meaningful keyframes. Our method\nreduces computation by masking non-keyframes and efficiently interpolating\nmissing frames. We dynamically refine the keyframe mask during inference to\nprioritize informative frames in later diffusion steps. Extensive experiments\nshow that our approach consistently outperforms state-of-the-art methods in\ntext alignment and motion realism, while also effectively maintaining high\nperformance at significantly fewer diffusion steps. We further validate the\nrobustness of our framework by using it as a generative prior and adapting it\nto different downstream tasks."}
{"id": "2502.00302", "pdf": "https://arxiv.org/pdf/2502.00302", "abs": "https://arxiv.org/abs/2502.00302", "authors": ["Yixuan He", "Aaron Sandel", "David Wipf", "Mihai Cucuringu", "John Mitani", "Gesine Reinert"], "title": "Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee Social Interactions", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.OC", "math.ST", "stat.TH"], "comment": null, "summary": "How can we identify groups of primate individuals which could be conjectured\nto drive social structure? To address this question, one of us has collected a\ntime series of data for social interactions between chimpanzees. Here we use a\nnetwork representation, leading to the task of combining these data into a time\nseries of a single weighted network per time stamp, where different proximities\nshould be given different weights reflecting their relative importance. We\noptimize these proximity-type weights in a principled way, using an innovative\nloss function which rewards structural consistency across time. The approach is\nempirically validated by carefully designed synthetic data. Using statistical\ntests, we provide a way of identifying groups of individuals that stay related\nfor a significant length of time. Applying the approach to the chimpanzee data\nset, we detect cliques in the animal social network time series, which can be\nvalidated by real-world intuition from prior research and qualitative\nobservations by chimpanzee experts."}
{"id": "2505.07801", "pdf": "https://arxiv.org/pdf/2505.07801", "abs": "https://arxiv.org/abs/2505.07801", "authors": ["Bernardo P. Ferreira", "Miguel A. Bessa"], "title": "Automatically Differentiable Model Updating (ADiMU): conventional, hybrid, and neural network material model discovery including history-dependency", "categories": ["math.NA", "cs.LG", "cs.NA", "physics.comp-ph"], "comment": "77 pages, 50 figures", "summary": "We introduce the first Automatically Differentiable Model Updating (ADiMU)\nframework that finds any history-dependent material model from full-field\ndisplacement and global force data (global, indirect discovery) or from\nstrain-stress data (local, direct discovery). We show that ADiMU can update\nconventional (physics-based), neural network (data-driven), and hybrid material\nmodels. Moreover, this framework requires no fine-tuning of hyperparameters or\nadditional quantities beyond those inherent to the user-selected material model\narchitecture and optimizer. The robustness and versatility of ADiMU is\nextensively exemplified by updating different models spanning tens to millions\nof parameters, in both local and global discovery settings. Relying on fully\ndifferentiable code, the algorithmic implementation leverages vectorizing maps\nthat enable history-dependent automatic differentiation via efficient batched\nexecution of shared computation graphs. This contribution also aims to\nfacilitate the integration, evaluation and application of future material model\narchitectures by openly supporting the research community. Therefore, ADiMU is\nreleased as an open-source computational tool, integrated into a carefully\ndesigned and documented software named HookeAI."}
{"id": "2503.13903", "pdf": "https://arxiv.org/pdf/2503.13903", "abs": "https://arxiv.org/abs/2503.13903", "authors": ["Qiang Qi", "Xiao Wang"], "title": "TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI2025", "summary": "Video object detection has made significant progress in recent years thanks\nto convolutional neural networks (CNNs) and vision transformers (ViTs).\nTypically, CNNs excel at capturing local features but struggle to model global\nrepresentations. Conversely, ViTs are adept at capturing long-range global\nfeatures but face challenges in representing local feature details.\nOff-the-shelf video object detection methods solely rely on CNNs or ViTs to\nconduct feature aggregation, which hampers their capability to simultaneously\nleverage global and local information, thereby resulting in limited detection\nperformance. In this paper, we propose a Transformer-GraphFormer Blender\nNetwork (TGBFormer) for video object detection, with three key technical\nimprovements to fully exploit the advantages of transformers and graph\nconvolutional networks while compensating for their limitations. First, we\ndevelop a spatial-temporal transformer module to aggregate global contextual\ninformation, constituting global representations with long-range feature\ndependencies. Second, we introduce a spatial-temporal GraphFormer module that\nutilizes local spatial and temporal relationships to aggregate features,\ngenerating new local representations that are complementary to the transformer\noutputs. Third, we design a global-local feature blender module to adaptively\ncouple transformer-based global representations and GraphFormer-based local\nrepresentations. Extensive experiments demonstrate that our TGBFormer\nestablishes new state-of-the-art results on the ImageNet VID dataset.\nParticularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS\non a single Tesla A100 GPU."}
{"id": "2502.00850", "pdf": "https://arxiv.org/pdf/2502.00850", "abs": "https://arxiv.org/abs/2502.00850", "authors": ["Chi Zhou", "Wang Luo", "Haoran Li", "Congying Han", "Tiande Guo", "Zicheng Zhang"], "title": "Dual Alignment Maximin Optimization for Offline Model-based RL", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Offline reinforcement learning agents face significant deployment challenges\ndue to the synthetic-to-real distribution mismatch. While most prior research\nhas focused on improving the fidelity of synthetic sampling and incorporating\noff-policy mechanisms, the directly integrated paradigm often fails to ensure\nconsistent policy behavior in biased models and underlying environmental\ndynamics, which inherently arise from discrepancies between behavior and\nlearning policies. In this paper, we first shift the focus from model\nreliability to policy discrepancies while optimizing for expected returns, and\nthen self-consistently incorporate synthetic data, deriving a novel\nactor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a\nunified framework to ensure both model-environment policy consistency and\nsynthetic and offline data compatibility. The inner minimization performs dual\nconservative value estimation, aligning policies and trajectories to avoid\nout-of-distribution states and actions, while the outer maximization ensures\nthat policy improvements remain consistent with inner value estimates.\nEmpirical evaluations demonstrate that DAMO effectively ensures model and\npolicy alignments, achieving competitive performance across diverse benchmark\ntasks."}
{"id": "2010.00788", "pdf": "https://arxiv.org/pdf/2010.00788", "abs": "https://arxiv.org/abs/2010.00788", "authors": ["Santiago Gonzalez", "Risto Miikkulainen"], "title": "Effective Regularization Through Loss-Function Metalearning", "categories": ["cs.LG", "cs.NE", "stat.ML"], "comment": null, "summary": "Evolutionary computation can be used to optimize several different aspects of\nneural network architectures. For instance, the TaylorGLO method discovers\nnovel, customized loss functions, resulting in improved performance, faster\ntraining, and improved data utilization. A likely reason is that such functions\ndiscourage overfitting, leading to effective regularization. This paper\ndemonstrates theoretically that this is indeed the case for TaylorGLO. Learning\nrule decomposition reveals that evolved loss functions balance two factors: the\npull toward zero error, and a push away from it to avoid overfitting. This is a\ngeneral principle that may be used to understand other regularization\ntechniques as well (as demonstrated in this paper for label smoothing). The\ntheoretical analysis leads to a constraint that can be utilized to find more\neffective loss functions in practice; the mechanism also results in networks\nthat are more robust (as demonstrated in this paper with adversarial inputs).\nThe analysis in this paper thus constitutes a first step towards understanding\nregularization, and demonstrates the power of evolutionary neural architecture\nsearch in general."}
{"id": "2503.15816", "pdf": "https://arxiv.org/pdf/2503.15816", "abs": "https://arxiv.org/abs/2503.15816", "authors": ["Abduljaleel Adejumo", "Faegheh Yeganli", "Clifford Broni-bediako", "Aoran Xiao", "Naoto Yokoya", "Mennatullah Siam"], "title": "A Vision Centric Remote Sensing Benchmark", "categories": ["cs.CV", "F.2.2; I.2.7"], "comment": "Eval-FoMo2 Workshop in CVPR 2025", "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision-language tasks but their remote sensing (RS) counterpart are relatively\nunder explored. Unlike natural images, RS imagery presents unique challenges\nthat current MLLMs struggle to handle, particularly in visual grounding and\nspatial reasoning. This study investigates the limitations of CLIP-based MLLMs\nin RS, highlighting their failure to differentiate visually distinct yet\nsemantically similar RS images. To address this, we introduce a remote sensing\nmultimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs\nin RS tasks by identifying the CLIP-blind pairs, where CLIP-based models\nincorrectly assign high similarity scores to visually distinct RS images.\nThrough a visual question answering (VQA) evaluation, we analyze the\nperformance of state-of-the-art MLLMs, revealing significant limitations in RS\nspecific representation learning. The results provide valuable insights into\nthe weaknesses of CLIP-based visual encoding and offer a foundation for future\nresearch to develop more effective MLLMs tailored for remote sensing\napplications."}
{"id": "2502.02456", "pdf": "https://arxiv.org/pdf/2502.02456", "abs": "https://arxiv.org/abs/2502.02456", "authors": ["Christopher J. MacLellan"], "title": "Model Human Learners: Computational Models to Guide Instructional Design", "categories": ["cs.HC", "cs.AI", "cs.SC"], "comment": "Published at CogSci 2025; 6 pages, 6 figures, 1 table", "summary": "Instructional designers face an overwhelming array of design choices, making\nit challenging to identify the most effective interventions. To address this\nissue, I propose the concept of a Model Human Learner, a unified computational\nmodel of learning that can aid designers in evaluating candidate interventions.\nThis paper presents the first successful demonstration of this concept, showing\nthat a computational model can accurately predict the outcomes of two human A/B\nexperiments -- one testing a problem sequencing intervention and the other\ntesting an item design intervention. It also demonstrates that such a model can\ngenerate learning curves without requiring human data and provide theoretical\ninsights into why an instructional intervention is effective. These findings\nlay the groundwork for future Model Human Learners that integrate cognitive and\nlearning theories to support instructional design across diverse tasks and\ninterventions."}
{"id": "2101.09306", "pdf": "https://arxiv.org/pdf/2101.09306", "abs": "https://arxiv.org/abs/2101.09306", "authors": ["Brendon G. Anderson", "Ziye Ma", "Jingqi Li", "Somayeh Sojoudi"], "title": "Towards Optimal Branching of Linear and Semidefinite Relaxations for Neural Network Robustness Certification", "categories": ["cs.LG", "math.OC"], "comment": "Accepted for publication in the Journal of Machine Learning Research\n  (JMLR). This is an extension of our IEEE CDC 2020 conference paper\n  arXiv:2004.00570", "summary": "In this paper, we study certifying the robustness of ReLU neural networks\nagainst adversarial input perturbations. To diminish the relaxation error\nsuffered by the popular linear programming (LP) and semidefinite programming\n(SDP) certification methods, we take a branch-and-bound approach to propose\npartitioning the input uncertainty set and solving the relaxations on each part\nseparately. We show that this approach reduces relaxation error, and that the\nerror is eliminated entirely upon performing an LP relaxation with a partition\nintelligently designed to exploit the nature of the ReLU activations. To scale\nthis approach to large networks, we consider using a coarser partition whereby\nthe number of parts in the partition is reduced. We prove that computing such a\ncoarse partition that directly minimizes the LP relaxation error is NP-hard. By\ninstead minimizing the worst-case LP relaxation error, we develop a closed-form\nbranching scheme in the single-hidden layer case. We extend the analysis to the\nSDP, where the feasible set geometry is exploited to design a branching scheme\nthat minimizes the worst-case SDP relaxation error. Experiments on MNIST,\nCIFAR-10, and Wisconsin breast cancer diagnosis classifiers demonstrate\nsignificant increases in the percentages of test samples certified. By\nindependently increasing the input size and the number of layers, we\nempirically illustrate under which regimes the branched LP and branched SDP are\nbest applied. Finally, we extend our LP branching method into a multi-layer\nbranching heuristic, which attains comparable performance to prior\nstate-of-the-art heuristics on large-scale, deep neural network certification\nbenchmarks."}
{"id": "2503.15831", "pdf": "https://arxiv.org/pdf/2503.15831", "abs": "https://arxiv.org/abs/2503.15831", "authors": ["Zihao Zhang", "Haoran Chen", "Haoyu Zhao", "Guansong Lu", "Yanwei Fu", "Hang Xu", "Zuxuan Wu"], "title": "EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Handling complex or nonlinear motion patterns has long posed challenges for\nvideo frame interpolation. Although recent advances in diffusion-based methods\noffer improvements over traditional optical flow-based approaches, they still\nstruggle to generate sharp, temporally consistent frames in scenarios with\nlarge motion. To address this limitation, we introduce EDEN, an Enhanced\nDiffusion for high-quality large-motion vidEo frame iNterpolation. Our approach\nfirst utilizes a transformer-based tokenizer to produce refined latent\nrepresentations of the intermediate frames for diffusion models. We then\nenhance the diffusion transformer with temporal attention across the process\nand incorporate a start-end frame difference embedding to guide the generation\nof dynamic motion. Extensive experiments demonstrate that EDEN achieves\nstate-of-the-art results across popular benchmarks, including nearly a 10%\nLPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD."}
{"id": "2502.05485", "pdf": "https://arxiv.org/pdf/2502.05485", "abs": "https://arxiv.org/abs/2502.05485", "authors": ["Yi Li", "Yuquan Deng", "Jesse Zhang", "Joel Jang", "Marius Memmel", "Raymond Yu", "Caelan Reed Garrett", "Fabio Ramos", "Dieter Fox", "Anqi Li", "Abhishek Gupta", "Ankit Goyal"], "title": "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "update related work and results on VQA benchmarks", "summary": "Large foundation models have shown strong open-world generalization to\ncomplex problems in vision and language, but similar levels of generalization\nhave yet to be achieved in robotics. One fundamental challenge is the lack of\nrobotic data, which are typically obtained through expensive on-robot\noperation. A promising remedy is to leverage cheaper, off-domain data such as\naction-free videos, hand-drawn sketches or simulation data. In this work, we\nposit that hierarchical vision-language-action (VLA) models can be more\neffective in utilizing off-domain data than standard monolithic VLA models that\ndirectly finetune vision-language models (VLMs) to predict actions. In\nparticular, we study a class of hierarchical VLA models, where the high-level\nVLM is finetuned to produce a coarse 2D path indicating the desired robot\nend-effector trajectory given an RGB image and a task description. The\nintermediate 2D path prediction is then served as guidance to the low-level,\n3D-aware control policy capable of precise manipulation. Doing so alleviates\nthe high-level VLM from fine-grained action prediction, while reducing the\nlow-level policy's burden on complex task-level reasoning. We show that, with\nthe hierarchical design, the high-level VLM can transfer across significant\ndomain gaps between the off-domain finetuning data and real-robot testing\nscenarios, including differences on embodiments, dynamics, visual appearances\nand task semantics, etc. In the real-robot experiments, we observe an average\nof 20% improvement in success rate across seven different axes of\ngeneralization over OpenVLA, representing a 50% relative gain. Visual results,\ncode, and dataset are provided at: https://hamster-robot.github.io/"}
{"id": "2105.09232", "pdf": "https://arxiv.org/pdf/2105.09232", "abs": "https://arxiv.org/abs/2105.09232", "authors": ["Lin Fan", "Peter W. Glynn"], "title": "Diffusion Approximations for Thompson Sampling", "categories": ["cs.LG", "math.ST", "stat.TH", "62B15, 60J70"], "comment": null, "summary": "We study the behavior of Thompson sampling from the perspective of weak\nconvergence. In the regime with small $\\gamma > 0$, where the gaps between arm\nmeans scale as $\\sqrt{\\gamma}$ and over time horizons that scale as $1/\\gamma$,\nwe show that the dynamics of Thompson sampling evolve according to discrete\nversions of SDE's and stochastic ODE's. As $\\gamma \\downarrow 0$, we show that\nthe dynamics converge weakly to solutions of the corresponding SDE's and\nstochastic ODE's. Our weak convergence theory is developed from first\nprinciples using the Continuous Mapping Theorem, and can be easily adapted to\nanalyze other sampling-based bandit algorithms. In this regime, we also show\nthat the weak limits of the dynamics of many sampling-based algorithms --\nincluding Thompson sampling designed for single-parameter exponential family\nrewards, and algorithms using bootstrap-based sampling to balance exploration\nand exploitation -- coincide with those of Gaussian Thompson sampling.\nMoreover, in this regime, these algorithms are generally robust to model\nmis-specification."}
{"id": "2503.15970", "pdf": "https://arxiv.org/pdf/2503.15970", "abs": "https://arxiv.org/abs/2503.15970", "authors": ["JunGyu Lee", "Kunyoung Lee", "Haesol Park", "Ig-Jae Kim", "Gi Pyo Nam"], "title": "V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition", "categories": ["cs.CV"], "comment": "Accepted by CVPRW2025", "summary": "Facial Expression Recognition (FER) plays a crucial role in human affective\nanalysis and has been widely applied in computer vision tasks such as\nhuman-computer interaction and psychological assessment. The 8th Affective\nBehavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions\nusing the video-based Aff-Wild2 dataset. This challenge includes various tasks,\nincluding the video-based EXPR recognition track, which is our primary focus.\nIn this paper, we demonstrate that addressing label ambiguity and class\nimbalance, which are known to cause performance degradation, can lead to\nmeaningful performance improvements. Specifically, we propose Video-based\nNoise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to\neach frame in a clip to address label ambiguity and effectively capture\ntemporal variations in facial expressions. Furthermore, we introduce a simple\nand effective augmentation strategy to reduce redundancy between consecutive\nframes, which is a primary cause of overfitting. Through extensive experiments,\nwe validate the effectiveness of our approach, demonstrating significant\nimprovements in video-based FER performance."}
{"id": "2502.06146", "pdf": "https://arxiv.org/pdf/2502.06146", "abs": "https://arxiv.org/abs/2502.06146", "authors": ["Annie Feng", "Nishanth Kumar", "Tomas Lozano-Perez", "Leslie Pack-Kaelbling"], "title": "Guided Exploration for Efficient Relational Model Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Efficient exploration is critical for learning relational models in\nlarge-scale environments with complex, long-horizon tasks. Random exploration\nmethods often collect redundant or irrelevant data, limiting their ability to\nlearn accurate relational models of the environment. Goal-literal babbling\n(GLIB) improves upon random exploration by setting and planning to novel goals,\nbut its reliance on random actions and random novel goal selection limits its\nscalability to larger domains. In this work, we identify the principles\nunderlying efficient exploration in relational domains: (1) operator\ninitialization with demonstrations that cover the distinct lifted effects\nnecessary for planning and (2) refining preconditions to collect maximally\ninformative transitions by selecting informative goal-action pairs and\nexecuting plans to them. To demonstrate these principles, we introduce\nBaking-Large, a challenging domain with extensive state-action spaces and\nlong-horizon tasks. We evaluate methods using oracle-driven demonstrations for\noperator initialization and precondition-targeting guidance to efficiently\ngather critical transitions. Experiments show that both the oracle\ndemonstrations and precondition-targeting oracle guidance significantly improve\nsample efficiency and generalization, paving the way for future methods to use\nthese principles to efficiently learn accurate relational models in complex\ndomains."}
{"id": "2301.12407", "pdf": "https://arxiv.org/pdf/2301.12407", "abs": "https://arxiv.org/abs/2301.12407", "authors": ["Lin Wang", "Zhichao Wang", "Ye Shi", "Sai Praneeth Karimireddy", "Xiaoying Tang"], "title": "Entropy-driven Fair and Effective Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed devices while preserving data privacy. Nonetheless, the\nheterogeneity of edge devices often leads to inconsistent performance of the\nglobally trained models, resulting in unfair outcomes among users. Existing\nfederated fairness algorithms strive to enhance fairness but often fall short\nin maintaining the overall performance of the global model, typically measured\nby the average accuracy across all clients. To address this issue, we propose a\nnovel algorithm that leverages entropy-based aggregation combined with model\nand gradient alignments to simultaneously optimize fairness and global model\nperformance. Our method employs a bi-level optimization framework, where we\nderive an analytic solution to the aggregation probability in the inner loop,\nmaking the optimization process computationally efficient. Additionally, we\nintroduce an innovative alignment update and an adaptive strategy in the outer\nloop to further balance global model's performance and fairness. Theoretical\nanalysis indicates that our approach guarantees convergence even in non-convex\nFL settings and demonstrates significant fairness improvements in generalized\nregression and strongly convex models. Empirically, our approach surpasses\nstate-of-the-art federated fairness algorithms, ensuring consistent performance\namong clients while improving the overall performance of the global model."}
{"id": "2503.16188", "pdf": "https://arxiv.org/pdf/2503.16188", "abs": "https://arxiv.org/abs/2503.16188", "authors": ["Ming Li", "Jike Zhong", "Shitian Zhao", "Yuxiang Lai", "Haoquan Zhang", "Wang Bill Zhu", "Kaipeng Zhang"], "title": "Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": "Preprint, work in progress. Add results on adaptive-thinking and\n  response inconsistency", "summary": "This paper investigates the role of explicit thinking process in rule-based\nreinforcement fine-tuning (RFT) for MLLMs. We first propose CLS-RL for MLLM\nimage classification, using verifiable rewards for fine-tuning. Experiments\nshow CLS-RL significantly outperforms SFT and yields a cross-dataset\ngeneralization effect. We then rethink and question whether explicit thinking\nin RFT is always necessary. Challenging the convention that explicit thinking\nis crucial for the success of RFT, we introduce No-Thinking-RL, exploring RFT\nwithout thinking by introducing a simple equality accuracy reward. We evaluate\nNo-Thinking-RL on 6 diverse tasks across different model sizes and types.\nExperimental results reveal three key findings: 1). Visual perception tasks do\nnot require thinking during RFT, as No-Thinking-RL consistently outperforms or\nmatches Thinking-based RFT across model sizes. 2).} Models with limited\ncapabilities struggle to generate high-quality CoT for RFT, making\nThinking-based RFT less effective than No-Thinking-RL. 3). There are\ninconsistencies between the answers in the thinking and answer tags for some\nresponses of thinking-based RFT, which show lower accuracy than the overall\naccuracy. We hypothesize that explicit thinking before verifiable answers may\nhinder reward convergence and reduce performance. To test this hypothesis, we\npropose Think-After-Answer, which places thinking after the answer to mitigate\nthis effect for experimental verification. Lastly, we conduct a pilot study to\nexplore whether MLLMs can learn when to think during RFT, introducing an\nAdaptive-Thinking method. Experiments show that it converges to a specific\nprompt depending on model capability and task complexity, achieving comparable\nor better performance than both Thinking and No-Thinking-RL. This suggests\nMLLMs can adaptively decide to think or not based on their capabilities and\ntask complexity."}
{"id": "2502.07693", "pdf": "https://arxiv.org/pdf/2502.07693", "abs": "https://arxiv.org/abs/2502.07693", "authors": ["Victor Morel", "Leonardo Iwaya", "Simone Fischer-Hübner"], "title": "AI-driven Personalized Privacy Assistants: a Systematic Literature Review", "categories": ["cs.CY", "cs.AI"], "comment": "Submitted to IEEE Access", "summary": "In recent years, several personalized assistants based on AI have been\nresearched and developed to help users make privacy-related decisions. These\nAI-driven Personalized Privacy Assistants (AI-driven PPAs) can provide\nsignificant benefits for users, who might otherwise struggle with making\ndecisions about their personal data in online environments that often overload\nthem with different privacy decision requests. So far, no studies have\nsystematically investigated the emerging topic of AI-driven PPAs, classifying\ntheir underlying technologies, architecture and features, including decision\ntypes or the accuracy of their decisions. To fill this gap, we present a\nSystematic Literature Review (SLR) to map the existing solutions found in the\nscientific literature, which allows reasoning about existing approaches and\nopen challenges for this research field. We screened several hundred unique\nresearch papers over the recent years (2013-2025), constructing a\nclassification from 41 included papers. As a result, this SLR reviews several\naspects of existing research on AI-driven PPAs in terms of types of\npublications, contributions, methodological quality, and other quantitative\ninsights. Furthermore, we provide a comprehensive classification for AI-driven\nPPAs, delving into their architectural choices, system contexts, types of AI\nused, data sources, types of decisions, and control over decisions, among other\nfacets. Based on our SLR, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research."}
{"id": "2306.15907", "pdf": "https://arxiv.org/pdf/2306.15907", "abs": "https://arxiv.org/abs/2306.15907", "authors": ["Jimeng Shi", "Zeda Yin", "Rukmangadh Myana", "Khandker Ishtiaq", "Anupama John", "Jayantha Obeysekera", "Arturo Leon", "Giri Narasimhan"], "title": "Deep Learning Models for Flood Predictions in South Florida", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Simulating and predicting the water level/stage in river systems is essential\nfor flood warnings, hydraulic operations, and flood mitigations. Physics-based\ndetailed hydrological and hydraulic computational tools, such as HEC-RAS, MIKE,\nand SWMM, can be used to simulate a complete watershed and compute the water\nstage at any point in the river system. However, these physics-based models are\ncomputationally intensive, especially for large watersheds and for longer\nsimulations, since they use detailed grid representations of terrain elevation\nmaps of the entire watershed and solve complex partial differential equations\n(PDEs) for each grid cell. To overcome this problem, we train several deep\nlearning (DL) models for use as surrogate models to rapidly predict the water\nstage. A portion of the Miami River in South Florida was chosen as a case study\nfor this paper. Extensive experiments show that the performance of various DL\nmodels (MLP, RNN, CNN, LSTM, and RCNN) is significantly better than that of the\nphysics-based model, HEC-RAS, even during extreme precipitation conditions\n(i.e., tropical storms), and with speedups exceeding 500x. To predict the water\nstages more accurately, our DL models use both measured variables of the river\nsystem from the recent past and covariates for which predictions are typically\navailable for the near future."}
{"id": "2504.04893", "pdf": "https://arxiv.org/pdf/2504.04893", "abs": "https://arxiv.org/abs/2504.04893", "authors": ["Justus Westerhoff", "Erblina Purelku", "Jakob Hackstein", "Jonas Loos", "Leo Pinetzki", "Lorenz Hufe"], "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPR 2025 Workshop EVAL-FoMo-2", "summary": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper along with the code for evaluations at\nwww.bliss.berlin/research/scam."}
{"id": "2502.08282", "pdf": "https://arxiv.org/pdf/2502.08282", "abs": "https://arxiv.org/abs/2502.08282", "authors": ["Vinod Kumar Chauhan", "Lei Clifton", "Gaurav Nigam", "David A. Clifton"], "title": "Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages (double column), 4 figures", "summary": "Estimating individualised treatment effect (ITE) -- that is the causal effect\nof a set of variables (also called exposures, treatments, actions, policies, or\ninterventions), referred to as \\textit{composite treatments}, on a set of\noutcome variables of interest, referred to as \\textit{composite outcomes}, for\na unit from observational data -- remains a fundamental problem in causal\ninference with applications across disciplines, such as healthcare, economics,\neducation, social science, marketing, and computer science. Previous work in\ncausal machine learning for ITE estimation is limited to simple settings, like\nsingle treatments and single outcomes. This hinders their use in complex\nreal-world scenarios; for example, consider studying the effect of different\nICU interventions, such as beta-blockers and statins for a patient admitted for\nheart surgery, on different outcomes of interest such as atrial fibrillation\nand in-hospital mortality. The limited research into composite treatments and\noutcomes is primarily due to data scarcity for all treatments and outcomes. To\naddress the above challenges, we propose a novel and innovative\nhypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation\nunder composite treatments and composite outcomes, which tackles the data\nscarcity issue by dynamically sharing information across treatments and\noutcomes. Our empirical analysis with binary and arbitrary composite treatments\nand outcomes demonstrates the effectiveness of the proposed approach compared\nto existing methods."}
{"id": "2308.12563", "pdf": "https://arxiv.org/pdf/2308.12563", "abs": "https://arxiv.org/abs/2308.12563", "authors": ["Thi Kieu Khanh Ho", "Narges Armanfard"], "title": "Contaminated Multivariate Time-Series Anomaly Detection with Spatio-Temporal Graph Conditional Diffusion Models", "categories": ["cs.LG", "eess.SP"], "comment": "Accepted to The Conference on Uncertainty in Artificial Intelligence\n  (UAI 2025)", "summary": "Mainstream unsupervised anomaly detection algorithms often excel in academic\ndatasets, yet their real-world performance is restricted due to the controlled\nexperimental conditions involving clean training data. Addressing the challenge\nof training with noise, a prevalent issue in practical anomaly detection, is\nfrequently overlooked. In a pioneering endeavor, this study delves into the\nrealm of label-level noise within sensory time-series anomaly detection (TSAD).\nThis paper presents a novel and practical end-to-end unsupervised TSAD when the\ntraining data is contaminated with anomalies. The introduced approach, called\nTSAD-C, is devoid of access to abnormality labels during the training phase.\nTSAD-C encompasses three core modules: a Decontaminator to rectify anomalies\n(aka noise) present during training, a Long-range Variable Dependency Modeling\nmodule to capture long-term intra- and inter-variable dependencies within the\ndecontaminated data that is considered as a surrogate of the pure normal data,\nand an Anomaly Scoring module to detect anomalies from all types. Our extensive\nexperiments conducted on four reliable and diverse datasets conclusively\ndemonstrate that TSAD-C surpasses existing methodologies, thus establishing a\nnew state-of-the-art in the TSAD field."}
{"id": "2504.06755", "pdf": "https://arxiv.org/pdf/2504.06755", "abs": "https://arxiv.org/abs/2504.06755", "authors": ["Li Yu", "Zhihui Li", "Yao Zhao", "Jimin Xiao", "Moncef Gabbouj"], "title": "FANeRV: Frequency Separation and Augmentation based Neural Representation for Video", "categories": ["cs.CV"], "comment": null, "summary": "Neural representations for video (NeRV) have gained considerable attention\nfor their strong performance across various video tasks. However, existing NeRV\nmethods often struggle to capture fine spatial details, resulting in vague\nreconstructions. In this paper, we present a Frequency Separation and\nAugmentation based Neural Representation for video (FANeRV), which addresses\nthese limitations with its core Wavelet Frequency Upgrade Block. This block\nexplicitly separates input frames into high and low-frequency components using\ndiscrete wavelet transform, followed by targeted enhancement using specialized\nmodules. Finally, a specially designed gated network effectively fuses these\nfrequency components for optimal reconstruction. Additionally, convolutional\nresidual enhancement blocks are integrated into the later stages of the network\nto balance parameter distribution and improve the restoration of high-frequency\ndetails. Experimental results demonstrate that FANeRV significantly improves\nreconstruction performance and excels in multiple tasks, including video\ncompression, inpainting, and interpolation, outperforming existing NeRV\nmethods."}
{"id": "2502.11013", "pdf": "https://arxiv.org/pdf/2502.11013", "abs": "https://arxiv.org/abs/2502.11013", "authors": ["Zhi Sheng", "Yuan Yuan", "Yudi Zhang", "Depeng Jin", "Yong Li"], "title": "Collaborative Deterministic-Diffusion Model for Probabilistic Spatiotemporal Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate prediction of urban spatiotemporal dynamics is essential for\nenhancing urban management and decision-making. Existing spatiotemporal\nprediction models are predominantly deterministic, focusing on primary\nspatiotemporal patterns. However, those dynamics are highly complex, exhibiting\nmulti-modal distributions that are challenging for deterministic models to\ncapture. In this paper, we highlight the critical role of probabilistic\nprediction in capturing the uncertainties and complexities inherent in\nspatiotemporal data. While mainstream probabilistic models can capture\nuncertainty, they struggle with accurately learning primary patterns and often\nsuffer from computational inefficiency. To address these challenges, we propose\nCoST, which collaborates deterministic and probabilistic models to improve both\npredictive accuracy and the ability to handle uncertainty. To achieve this, we\ndesign a mean-residual decomposition framework, where the mean value is modeled\nby a deterministic model, and the residual variations are learned by a\nprobabilistic model, specifically diffusion models. Moreover, we introduce a\nscale-aware diffusion process, which better accounts for spatially\nheterogeneous dynamics across different regions. Extensive experiments on eight\nreal-world datasets demonstrate that CoST significantly outperforms existing\nmethods in both deterministic and probabilistic metrics, achieving a 20%\nimprovement with low computational cost. CoST bridges the gap between\ndeterministic precision and probabilistic uncertainty, making a significant\nadvancement in the field of urban spatiotemporal prediction."}
{"id": "2312.04055", "pdf": "https://arxiv.org/pdf/2312.04055", "abs": "https://arxiv.org/abs/2312.04055", "authors": ["Fei Huang", "Jianrong Lv", "Yang Yue"], "title": "Jointly spatial-temporal representation learning for individual trajectories", "categories": ["cs.LG"], "comment": "27 pages, 3 tables, 7 figures", "summary": "Individual trajectories, rich in human-environment interaction information\nacross space and time, serve as vital inputs for geospatial foundation models\n(GeoFMs). However, existing attempts at learning trajectory representations\nhave overlooked the implicit spatial-temporal dependency within trajectories,\nfailing to encode such dependency in a deep learning-friendly format. That\nposes a challenge in obtaining general-purpose trajectory representations.\nTherefore, this paper proposes a spatial-temporal joint representation learning\nmethod (ST-GraphRL) to formalize learnable spatial-temporal dependencies into\ntrajectory representations. The proposed ST-GraphRL consists of three\ncompositions: (i) a weighted directed spatial-temporal graph to explicitly\nconstruct mobility interactions in both space and time dimensions; (ii) a\ntwo-stage jointly encoder (i.e., decoupling and fusion), to learn entangled\nspatial-temporal dependencies by independently decomposing and jointly\naggregating space and time information; (iii) a decoder guides ST-GraphRL to\nlearn explicit mobility regularities by simulating the spatial-temporal\ndistributions of trajectories. Tested on three real-world human mobility\ndatasets, the proposed ST-GraphRL outperformed all the baseline models in\npredicting movement spatial-temporal distributions and preserving trajectory\nsimilarity with high spatial-temporal correlations. Analyzing spatial-temporal\nfeatures presented in latent space validates that ST-GraphRL understands\nspatial-temporal patterns. This study may also benefit representation learnings\nof other geospatial data to achieve general-purpose data representations and\nadvance GeoFMs development."}
{"id": "2504.12157", "pdf": "https://arxiv.org/pdf/2504.12157", "abs": "https://arxiv.org/abs/2504.12157", "authors": ["Xiaojun Ye", "Chun Wang", "Yiren Song", "Sheng Zhou", "Liangcheng Li", "Jiajun Bu"], "title": "FocusedAD: Character-centric Movie Audio Description", "categories": ["cs.CV", "I.2.10"], "comment": "Code and Demo link: https://github.com/Thorin215/FocusedAD", "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD ."}
{"id": "2503.14376", "pdf": "https://arxiv.org/pdf/2503.14376", "abs": "https://arxiv.org/abs/2503.14376", "authors": ["Maximilian Beck", "Korbinian Pöppel", "Phillip Lippe", "Sepp Hochreiter"], "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels", "categories": ["cs.LG", "cs.AI"], "comment": "Code available at: https://github.com/NX-AI/mlstm_kernels", "summary": "Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels\n(Dao, 2024). Leveraging the chunkwise-parallel formulation of linear RNNs,\nFlash Linear Attention (FLA) (Yang & Zhang, 2024) shows that linear RNN kernels\nare faster than Flash Attention, by parallelizing over chunks of the input\nsequence. However, since the chunk size of FLA is limited, many intermediate\nstates must be materialized in GPU memory. This leads to low arithmetic\nintensity and causes high memory consumption and IO cost, especially for\nlong-context pre-training. In this work, we present Tiled Flash Linear\nAttention (TFLA), a novel kernel algorithm for linear RNNs, that enables\narbitrary large chunk sizes and high arithmetic intensity by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM (Beck et al., 2024). Second, we\npropose an mLSTM variant with sigmoid input gate and reduced computation for\neven faster kernel runtimes at equal language modeling performance. In our\nspeed benchmarks, we show that our new mLSTM kernels based on TFLA outperform\nhighly optimized Flash Attention, Linear Attention and Mamba kernels, setting a\nnew state of the art for efficient long-context sequence modeling primitives."}
{"id": "2312.07252", "pdf": "https://arxiv.org/pdf/2312.07252", "abs": "https://arxiv.org/abs/2312.07252", "authors": ["Pascal Iversen", "Simon Witzke", "Katharina Baum", "Bernhard Y. Renard"], "title": "Identifying Drivers of Predictive Aleatoric Uncertainty", "categories": ["cs.LG", "stat.ML"], "comment": "Simon Witzke and Pascal Iversen contributed equally", "summary": "Explainability and uncertainty quantification are key to trustable artificial\nintelligence. However, the reasoning behind uncertainty estimates is generally\nleft unexplained. Identifying the drivers of uncertainty complements\nexplanations of point predictions in recognizing model limitations and\nenhancing transparent decision-making. So far, explanations of uncertainties\nhave been rarely studied. The few exceptions rely on Bayesian neural networks\nor technically intricate approaches, such as auxiliary generative models,\nthereby hindering their broad adoption. We propose a straightforward approach\nto explain predictive aleatoric uncertainties. We estimate uncertainty in\nregression as predictive variance by adapting a neural network with a Gaussian\noutput distribution. Subsequently, we apply out-of-the-box explainers to the\nmodel's variance output. This approach can explain uncertainty influences more\nreliably than complex published approaches, which we demonstrate in a synthetic\nsetting with a known data-generating process. We substantiate our findings with\na nuanced, quantitative benchmark including synthetic and real, tabular and\nimage datasets. For this, we adapt metrics from conventional XAI research to\nuncertainty explanations. Overall, the proposed method explains uncertainty\nestimates with little modifications to the model architecture and outperforms\nmore intricate methods in most settings."}
{"id": "2504.12574", "pdf": "https://arxiv.org/pdf/2504.12574", "abs": "https://arxiv.org/abs/2504.12574", "authors": ["Zhenyu Yu", "Mohd Yamani Inda Idris", "Pei Wang"], "title": "ForgetMe: Evaluating Selective Forgetting in Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "The widespread adoption of diffusion models in image generation has increased\nthe demand for privacy-compliant unlearning. However, due to the\nhigh-dimensional nature and complex feature representations of diffusion\nmodels, achieving selective unlearning remains challenging, as existing methods\nstruggle to remove sensitive information while preserving the consistency of\nnon-sensitive regions. To address this, we propose an Automatic Dataset\nCreation Framework based on prompt-based layered editing and training-free\nlocal feature removal, constructing the ForgetMe dataset and introducing the\nEntangled evaluation metric. The Entangled metric quantifies unlearning\neffectiveness by assessing the similarity and consistency between the target\nand background regions and supports both paired (Entangled-D) and unpaired\n(Entangled-S) image data, enabling unsupervised evaluation. The ForgetMe\ndataset encompasses a diverse set of real and synthetic scenarios, including\nCUB-200-2011 (Birds), Stanford-Dogs, ImageNet, and a synthetic cat dataset. We\napply LoRA fine-tuning on Stable Diffusion to achieve selective unlearning on\nthis dataset and validate the effectiveness of both the ForgetMe dataset and\nthe Entangled metric, establishing them as benchmarks for selective unlearning.\nOur work provides a scalable and adaptable solution for advancing\nprivacy-preserving generative AI."}
{"id": "2503.15764", "pdf": "https://arxiv.org/pdf/2503.15764", "abs": "https://arxiv.org/abs/2503.15764", "authors": ["Yong Xiao", "Guangming Shi", "Ping Zhang"], "title": "Towards Agentic AI Networking in 6G: A Generative Foundation Model-as-Agent Approach", "categories": ["cs.NI", "cs.AI"], "comment": "Accepted at IEEE Communications Magazine", "summary": "The promising potential of AI and network convergence in improving networking\nperformance and enabling new service capabilities has recently attracted\nsignificant interest. Existing network AI solutions, while powerful, are mainly\nbuilt based on the close-loop and passive learning framework, resulting in\nmajor limitations in autonomous solution finding and dynamic environmental\nadaptation. Agentic AI has recently been introduced as a promising solution to\naddress the above limitations and pave the way for true generally intelligent\nand beneficial AI systems. The key idea is to create a networking ecosystem to\nsupport a diverse range of autonomous and embodied AI agents in fulfilling\ntheir goals. In this paper, we focus on the novel challenges and requirements\nof agentic AI networking. We propose AgentNet, a novel framework for supporting\ninteraction, collaborative learning, and knowledge transfer among AI agents. We\nintroduce a general architectural framework of AgentNet and then propose a\ngenerative foundation model (GFM)-based implementation in which multiple\nGFM-as-agents have been created as an interactive knowledge-base to bootstrap\nthe development of embodied AI agents according to different task requirements\nand environmental features. We consider two application scenarios,\ndigital-twin-based industrial automation and metaverse-based infotainment\nsystem, to describe how to apply AgentNet for supporting efficient task-driven\ncollaboration and interaction among AI agents."}
{"id": "2312.16560", "pdf": "https://arxiv.org/pdf/2312.16560", "abs": "https://arxiv.org/abs/2312.16560", "authors": ["Federico Errica", "Henrik Christiansen", "Viktor Zaverkin", "Takashi Maruyama", "Mathias Niepert", "Francesco Alesiani"], "title": "Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching", "categories": ["cs.LG"], "comment": null, "summary": "Long-range interactions are essential for the correct description of complex\nsystems in many scientific fields. The price to pay for including them in the\ncalculations, however, is a dramatic increase in the overall computational\ncosts. Recently, deep graph networks have been employed as efficient,\ndata-driven models for predicting properties of complex systems represented as\ngraphs. These models rely on a message passing strategy that should, in\nprinciple, capture long-range information without explicitly modeling the\ncorresponding interactions. In practice, most deep graph networks cannot really\nmodel long-range dependencies due to the intrinsic limitations of (synchronous)\nmessage passing, namely oversmoothing, oversquashing, and underreaching. This\nwork proposes a general framework that learns to mitigate these limitations:\nwithin a variational inference framework, we endow message passing\narchitectures with the ability to adapt their depth and filter messages along\nthe way. With theoretical and empirical arguments, we show that this strategy\nbetter captures long-range interactions, by competing with the state of the art\non five node and graph prediction datasets."}
{"id": "2504.13580", "pdf": "https://arxiv.org/pdf/2504.13580", "abs": "https://arxiv.org/abs/2504.13580", "authors": ["Yuchen Rao", "Stefan Ainetter", "Sinisa Stekovic", "Vincent Lepetit", "Friedrich Fraundorfer"], "title": "Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding", "categories": ["cs.CV"], "comment": "Project page: https://stefan-ainetter.github.io/SCANnotatepp; CVPR'25\n  Workshop", "summary": "High-level 3D scene understanding is essential in many applications. However,\nthe challenges of generating accurate 3D annotations make development of deep\nlearning models difficult. We turn to recent advancements in automatic\nretrieval of synthetic CAD models, and show that data generated by such methods\ncan be used as high-quality ground truth for training supervised deep learning\nmodels. More exactly, we employ a pipeline akin to the one previously used to\nautomatically annotate objects in ScanNet scenes with their 9D poses and CAD\nmodels. This time, we apply it to the recent ScanNet++ v1 dataset, which\npreviously lacked such annotations. Our findings demonstrate that it is not\nonly possible to train deep learning models on these automatically-obtained\nannotations but that the resulting models outperform those trained on manually\nannotated data. We validate this on two distinct tasks: point cloud completion\nand single-view CAD model retrieval and alignment. Our results underscore the\npotential of automatic 3D annotations to enhance model performance while\nsignificantly reducing annotation costs. To support future research in 3D scene\nunderstanding, we will release our annotations, which we call SCANnotate++,\nalong with our trained models."}
{"id": "2503.21098", "pdf": "https://arxiv.org/pdf/2503.21098", "abs": "https://arxiv.org/abs/2503.21098", "authors": ["Yedan Shen", "Kaixin Wu", "Yuechen Ding", "Jingyuan Wen", "Hong Liu", "Mingjie Zhong", "Zhouhan Lin", "Jia Xu", "Linjian Mo"], "title": "Alleviating LLM-based Generative Retrieval Hallucination in Alipay Search", "categories": ["cs.IR", "cs.AI"], "comment": "4 pages", "summary": "Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains."}
{"id": "2401.00364", "pdf": "https://arxiv.org/pdf/2401.00364", "abs": "https://arxiv.org/abs/2401.00364", "authors": ["Shaan Ul Haque", "Sajad Khodadadian", "Siva Theja Maguluri"], "title": "Tight Finite Time Bounds of Two-Time-Scale Linear Stochastic Approximation with Markovian Noise", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "comment": "83 pages, 6 figures", "summary": "Stochastic approximation (SA) is an iterative algorithm for finding the fixed\npoint of an operator using noisy samples and widely used in optimization and\nReinforcement Learning (RL). The noise in RL exhibits a Markovian structure,\nand in some cases, such as gradient temporal difference (GTD) methods, SA is\nemployed in a two-time-scale framework. This combination introduces significant\ntheoretical challenges for analysis.\n  We derive an upper bound on the error for the iterations of linear\ntwo-time-scale SA with Markovian noise. We demonstrate that the mean squared\nerror decreases as $trace (\\Sigma^y)/k + o(1/k)$ where $k$ is the number of\niterates, and $\\Sigma^y$ is an appropriately defined covariance matrix. A key\nfeature of our bounds is that the leading term, $\\Sigma^y$, exactly matches\nwith the covariance in the Central Limit Theorem (CLT) for the two-time-scale\nSA, and we call them tight finite-time bounds. We illustrate their use in RL by\nestablishing sample complexity for off-policy algorithms, TDC, GTD, and GTD2.\n  A special case of linear two-time-scale SA that is extensively studied is\nlinear SA with Polyak-Ruppert averaging. We present tight finite time bounds\ncorresponding to the covariance matrix of the CLT. Such bounds can be used to\nstudy TD-learning with Polyak-Ruppert averaging."}
{"id": "2504.13617", "pdf": "https://arxiv.org/pdf/2504.13617", "abs": "https://arxiv.org/abs/2504.13617", "authors": ["Zuyao Chen", "Jinlin Wu", "Zhen Lei", "Marc Pollefeys", "Chang Wen Chen"], "title": "Compile Scene Graphs with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Next-token prediction is the fundamental principle for training large\nlanguage models (LLMs), and reinforcement learning (RL) further enhances their\nreasoning performance. As an effective way to model language, image, video, and\nother modalities, the use of LLMs for end-to-end extraction of structured\nvisual representations, such as scene graphs, remains underexplored. It\nrequires the model to accurately produce a set of objects and relationship\ntriplets, rather than generating text token by token. To achieve this, we\nintroduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised\nfine-tuning (SFT) on the scene graph dataset and subsequently refined using\nreinforcement learning to enhance its ability to generate scene graphs in an\nend-to-end manner. The SFT follows a conventional prompt-response paradigm,\nwhile RL requires the design of effective reward signals. We design a set of\ngraph-centric rewards, including three recall-based variants -- Hard Recall,\nHard Recall+Relax, and Soft Recall -- which evaluate semantic and spatial\nalignment between predictions and ground truth at the object and relation\nlevels. A format consistency reward further ensures that outputs follow the\nexpected structural schema. Extensive experiments on the VG150 and PSG\nbenchmarks show that R1-SGG substantially reduces failure rates and achieves\nstrong performance in Recall and mean Recall, surpassing traditional SGG models\nand existing multimodal language models. Our code is available at\nhttps://github.com/gpt4vision/R1-SGG"}
{"id": "2503.21846", "pdf": "https://arxiv.org/pdf/2503.21846", "abs": "https://arxiv.org/abs/2503.21846", "authors": ["Yesmine Abdennadher", "Giovanni Perin", "Riccardo Mazzieri", "Jacopo Pegoraro", "Michele Rossi"], "title": "LightSNN: Lightweight Architecture Search for Sparse and Accurate Spiking Neural Networks", "categories": ["cs.NE", "cs.AI", "eess.SP"], "comment": "Accepted to AMLDS 2025 (Tokyo, July 2025). 6 pages, 3 figures, 2\n  tables", "summary": "Spiking Neural Networks (SNNs) are highly regarded for their energy\nefficiency, inherent activation sparsity, and suitability for real-time\nprocessing in edge devices. However, most current SNN methods adopt\narchitectures resembling traditional artificial neural networks (ANNs), leading\nto suboptimal performance when applied to SNNs. While SNNs excel in energy\nefficiency, they have been associated with lower accuracy levels than\ntraditional ANNs when utilizing conventional architectures. In response, in\nthis work we present LightSNN, a rapid and efficient Neural Network\nArchitecture Search (NAS) technique specifically tailored for SNNs that\nautonomously leverages the most suitable architecture, striking a good balance\nbetween accuracy and efficiency by enforcing sparsity. Based on the spiking NAS\nnetwork (SNASNet) framework, a cell-based search space including backward\nconnections is utilized to build our training-free pruning-based NAS mechanism.\nOur technique assesses diverse spike activation patterns across different data\nsamples using a sparsity-aware Hamming distance fitness evaluation. Thorough\nexperiments are conducted on both static (CIFAR10 and CIFAR100) and\nneuromorphic datasets (DVS128-Gesture). Our LightSNN model achieves\nstate-of-the-art results on CIFAR10 and CIFAR100, improves performance on\nDVS128Gesture by 4.49\\%, and significantly reduces search time most notably\noffering a $98\\times$ speedup over SNASNet and running 30\\% faster than the\nbest existing method on DVS128Gesture. Code is available on Github at:\nhttps://github.com/YesmineAbdennadher/LightSNN."}
{"id": "2401.08909", "pdf": "https://arxiv.org/pdf/2401.08909", "abs": "https://arxiv.org/abs/2401.08909", "authors": ["Renchunzi Xie", "Ambroise Odonnat", "Vasilii Feofanov", "Ievgen Redko", "Jianfeng Zhang", "Bo An"], "title": "Leveraging Gradients for Unsupervised Accuracy Estimation under Distribution Shift", "categories": ["cs.LG"], "comment": null, "summary": "Estimating the test performance of a model, possibly under distribution\nshift, without having access to the ground-truth labels is a challenging, yet\nvery important problem for the safe deployment of machine learning algorithms\nin the wild. Existing works mostly rely on information from either the outputs\nor the extracted features of neural networks to estimate a score that\ncorrelates with the ground-truth test accuracy. In this paper, we investigate\n-- both empirically and theoretically -- how the information provided by the\ngradients can be predictive of the ground-truth test accuracy even under\ndistribution shifts. More specifically, we use the norm of classification-layer\ngradients, backpropagated from the cross-entropy loss after only one gradient\nstep over test data. Our intuition is that these gradients should be of higher\nmagnitude when the model generalizes poorly. We provide the theoretical\ninsights behind our approach and the key ingredients that ensure its empirical\nsuccess. Extensive experiments conducted with various architectures on diverse\ndistribution shifts demonstrate that our method significantly outperforms\ncurrent state-of-the-art approaches. The code is available at\nhttps://github.com/Renchunzi-Xie/GdScore"}
{"id": "2504.17040", "pdf": "https://arxiv.org/pdf/2504.17040", "abs": "https://arxiv.org/abs/2504.17040", "authors": ["Zhenhailong Wang", "Senthil Purushwalkam", "Caiming Xiong", "Silvio Savarese", "Heng Ji", "Ran Xu"], "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/."}
{"id": "2504.00485", "pdf": "https://arxiv.org/pdf/2504.00485", "abs": "https://arxiv.org/abs/2504.00485", "authors": ["Mahade Hasan", "Farhana Yasmin", "Xue Yu"], "title": "Enhancing stroke disease classification through machine learning models by feature selection techniques", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Heart disease remains a leading cause of mortality and morbidity worldwide,\nnecessitating the development of accurate and reliable predictive models to\nfacilitate early detection and intervention. While state of the art work has\nfocused on various machine learning approaches for predicting heart disease,\nbut they could not able to achieve remarkable accuracy. In response to this\nneed, we applied nine machine learning algorithms XGBoost, logistic regression,\ndecision tree, random forest, k-nearest neighbors (KNN), support vector machine\n(SVM), gaussian na\\\"ive bayes (NB gaussian), adaptive boosting, and linear\nregression to predict heart disease based on a range of physiological\nindicators. Our approach involved feature selection techniques to identify the\nmost relevant predictors, aimed at refining the models to enhance both\nperformance and interpretability. The models were trained, incorporating\nprocesses such as grid search hyperparameter tuning, and cross-validation to\nminimize overfitting. Additionally, we have developed a novel voting system\nwith feature selection techniques to advance heart disease classification.\nFurthermore, we have evaluated the models using key performance metrics\nincluding accuracy, precision, recall, F1-score, and the area under the\nreceiver operating characteristic curve (ROC AUC). Among the models, XGBoost\ndemonstrated exceptional performance, achieving 99% accuracy, precision,\nF1-Score, 98% recall, and 100% ROC AUC. This study offers a promising approach\nto early heart disease diagnosis and preventive healthcare."}
{"id": "2402.06223", "pdf": "https://arxiv.org/pdf/2402.06223", "abs": "https://arxiv.org/abs/2402.06223", "authors": ["Yuhang Liu", "Zhen Zhang", "Dong Gong", "Erdun Gao", "Biwei Huang", "Mingming Gong", "Anton van den Hengel", "Kun Zhang", "Javen Qinfeng Shi"], "title": "Beyond DAGs: A Latent Partial Causal Model for Multimodal Learning", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Directed acyclic graphs (DAGs) are fundamental graph structures in causal\nmodeling, but identifying the desired DAG from observational data often\nrequires strong assumptions that may not hold in real-world scenarios,\nespecially for latent causal models and complex multimodal data. This raises\nthe question of whether we can relax or bypass the DAG assumption while\nmaintaining practical utility. In this work, we propose a novel latent partial\ncausal model for multimodal data, featuring two latent coupled variables,\nconnected by an undirected edge, to represent the transfer of knowledge across\nmodalities. Under specific statistical assumptions, we establish an\nidentifiability result, demonstrating that representations learned by\nmultimodal contrastive learning correspond to the latent coupled variables up\nto a trivial transformation. This result deepens our understanding of the why\nmultimodal contrastive learning works, highlights its potential for\ndisentanglement, and expands the utility of pre-trained models like CLIP.\nSynthetic experiments confirm the robustness of our findings, even when the\nassumptions are partially violated. Most importantly, experiments on a\npre-trained CLIP model embodies disentangled representations, enabling few-shot\nlearning and improving domain generalization across diverse real-world\ndatasets. Together, these contributions push the boundaries of multimodal\ncontrastive learning, both theoretically and, crucially, in practical\napplications."}
{"id": "2504.17522", "pdf": "https://arxiv.org/pdf/2504.17522", "abs": "https://arxiv.org/abs/2504.17522", "authors": ["Anyi Xiao", "Cihui Yang"], "title": "TableCenterNet: A one-stage network for table structure recognition", "categories": ["cs.CV"], "comment": null, "summary": "Table structure recognition aims to parse tables in unstructured data into\nmachine-understandable formats. Recent methods address this problem through a\ntwo-stage process or optimized one-stage approaches. However, these methods\neither require multiple networks to be serially trained and perform more\ntime-consuming sequential decoding, or rely on complex post-processing\nalgorithms to parse the logical structure of tables. They struggle to balance\ncross-scenario adaptability, robustness, and computational efficiency. In this\npaper, we propose a one-stage end-to-end table structure parsing network called\nTableCenterNet. This network unifies the prediction of table spatial and\nlogical structure into a parallel regression task for the first time, and\nimplicitly learns the spatial-logical location mapping laws of cells through a\nsynergistic architecture of shared feature extraction layers and task-specific\ndecoding. Compared with two-stage methods, our method is easier to train and\nfaster to infer. Experiments on benchmark datasets show that TableCenterNet can\neffectively parse table structures in diverse scenarios and achieve\nstate-of-the-art performance on the TableGraph-24k dataset. Code is available\nat https://github.com/dreamy-xay/TableCenterNet."}
{"id": "2504.11901", "pdf": "https://arxiv.org/pdf/2504.11901", "abs": "https://arxiv.org/abs/2504.11901", "authors": ["Luca Castri", "Gloria Beraldo", "Nicola Bellotto"], "title": "Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments", "categories": ["cs.RO", "cs.AI"], "comment": "Causal Discovery and Inference - Robot Autonomy - Human-Robot Spatial\n  Interaction - Decision-Making", "summary": "The growing integration of robots in shared environments -- such as\nwarehouses, shopping centres, and hospitals -- demands a deep understanding of\nthe underlying dynamics and human behaviours, including how, when, and where\nindividuals engage in various activities and interactions. This knowledge goes\nbeyond simple correlation studies and requires a more comprehensive causal\nanalysis. By leveraging causal inference to model cause-and-effect\nrelationships, we can better anticipate critical environmental factors and\nenable autonomous robots to plan and execute tasks more effectively. To this\nend, we propose a novel causality-based decision-making framework that reasons\nover a learned causal model to predict battery usage and human obstructions,\nunderstanding how these factors could influence robot task execution. Such\nreasoning framework assists the robot in deciding when and how to complete a\ngiven task. To achieve this, we developed also PeopleFlow, a new Gazebo-based\nsimulator designed to model context-sensitive human-robot spatial interactions\nin shared workspaces. PeopleFlow features realistic human and robot\ntrajectories influenced by contextual factors such as time, environment layout,\nand robot state, and can simulate a large number of agents. While the simulator\nis general-purpose, in this paper we focus on a warehouse-like environment as a\ncase study, where we conduct an extensive evaluation benchmarking our causal\napproach against a non-causal baseline. Our findings demonstrate the efficacy\nof the proposed solutions, highlighting how causal reasoning enables autonomous\nrobots to operate more efficiently and safely in dynamic environments shared\nwith humans."}
{"id": "2404.02108", "pdf": "https://arxiv.org/pdf/2404.02108", "abs": "https://arxiv.org/abs/2404.02108", "authors": ["Swetha Ganesh", "Washim Uddin Mondal", "Vaneet Aggarwal"], "title": "Order-Optimal Regret with Novel Policy Gradient Approaches in Infinite-Horizon Average Reward MDPs", "categories": ["cs.LG"], "comment": "In the Proceedings of the 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS), 2025", "summary": "We present two Policy Gradient-based algorithms with general parametrization\nin the context of infinite-horizon average reward Markov Decision Process\n(MDP). The first one employs Implicit Gradient Transport for variance\nreduction, ensuring an expected regret of the order\n$\\tilde{\\mathcal{O}}(T^{2/3})$. The second approach, rooted in Hessian-based\ntechniques, ensures an expected regret of the order\n$\\tilde{\\mathcal{O}}(\\sqrt{T})$. These results significantly improve the\nstate-of-the-art $\\tilde{\\mathcal{O}}(T^{3/4})$ regret and achieve the\ntheoretical lower bound. We also show that the average-reward function is\napproximately $L$-smooth, a result that was previously assumed in earlier\nworks."}
{"id": "2504.18864", "pdf": "https://arxiv.org/pdf/2504.18864", "abs": "https://arxiv.org/abs/2504.18864", "authors": ["Yunzhong Zhang", "Bo Xiong", "You Zhou", "Changqing Su", "Zhen Cheng", "Zhaofei Yu", "Xun Cao", "Tiejun Huang"], "title": "Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras", "categories": ["cs.CV"], "comment": null, "summary": "The need for accurate and non-intrusive flow measurement methods has led to\nthe widespread adoption of Particle Image Velocimetry (PIV), a powerful\ndiagnostic tool in fluid motion estimation. This study investigates the\ntremendous potential of spike cameras (a type of ultra-high-speed,\nhigh-dynamic-range camera) in PIV. We propose a deep learning framework, Spike\nImaging Velocimetry (SIV), designed specifically for highly turbulent and\nintricate flow fields. To aggregate motion features from the spike stream while\nminimizing information loss, we incorporate a Detail-Preserving Hierarchical\nTransform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to\nextract contextual features from highly complex fluid flows. Furthermore, we\npresent a spike-based PIV dataset, Particle Scenes with Spike and Displacement\n(PSSD), which provides labeled data for three challenging fluid dynamics\nscenarios. Our proposed method achieves superior performance compared to\nexisting baseline methods on PSSD. The datasets and our implementation of SIV\nare open-sourced in the supplementary materials."}
{"id": "2504.15587", "pdf": "https://arxiv.org/pdf/2504.15587", "abs": "https://arxiv.org/abs/2504.15587", "authors": ["Zimo Yan", "Jie Zhang", "Zheng Xie", "Chang Liu", "Yizhen Liu", "Yiping Song"], "title": "MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Molecular generation plays an important role in drug discovery and materials\nscience, especially in data-scarce scenarios where traditional generative\nmodels often struggle to achieve satisfactory conditional generalization. To\naddress this challenge, we propose MetaMolGen, a first-order\nmeta-learning-based molecular generator designed for few-shot and\nproperty-conditioned molecular generation. MetaMolGen standardizes the\ndistribution of graph motifs by mapping them to a normalized latent space, and\nemploys a lightweight autoregressive sequence model to generate SMILES\nsequences that faithfully reflect the underlying molecular structure. In\naddition, it supports conditional generation of molecules with target\nproperties through a learnable property projector integrated into the\ngenerative process.Experimental results demonstrate that MetaMolGen\nconsistently generates valid and diverse SMILES sequences under low-data\nregimes, outperforming conventional baselines. This highlights its advantage in\nfast adaptation and efficient conditional generation for practical molecular\ndesign."}
{"id": "2404.15731", "pdf": "https://arxiv.org/pdf/2404.15731", "abs": "https://arxiv.org/abs/2404.15731", "authors": ["Akshay Thakur", "Souvik Chakraborty"], "title": "MD-NOMAD: Mixture density nonlinear manifold decoder for emulating stochastic differential equations and uncertainty propagation", "categories": ["cs.LG"], "comment": null, "summary": "We propose a neural operator framework, termed mixture density nonlinear\nmanifold decoder (MD-NOMAD), for stochastic simulators. Our approach leverages\nan amalgamation of the pointwise operator learning neural architecture\nnonlinear manifold decoder (NOMAD) with mixture density-based methods to\nestimate conditional probability distributions for stochastic output functions.\nMD-NOMAD harnesses the ability of probabilistic mixture models to estimate\ncomplex probability and the high-dimensional scalability of pointwise neural\noperator NOMAD. We conduct empirical assessments on a wide array of stochastic\nordinary and partial differential equations and present the corresponding\nresults, which highlight the performance of the proposed framework."}
{"id": "2504.21414", "pdf": "https://arxiv.org/pdf/2504.21414", "abs": "https://arxiv.org/abs/2504.21414", "authors": ["Qi Fan", "Kaiqi Liu", "Nian Liu", "Hisham Cholakkal", "Rao Muhammad Anwer", "Wenbin Li", "Yang Gao"], "title": "Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining", "categories": ["cs.CV"], "comment": null, "summary": "Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel\nclasses in new domains, which is often challenging due to the diverse\ncharacteristics of target domains and the limited availability of support data.\nMost CD-FSS methods redesign and retrain in-domain FSS models using various\ndomain-generalization techniques, which are effective but costly to train. To\naddress these issues, we propose adapting informative model structures of the\nwell-trained FSS model for target domains by learning domain characteristics\nfrom few-shot labeled support samples during inference, thereby eliminating the\nneed for retraining. Specifically, we first adaptively identify domain-specific\nmodel structures by measuring parameter importance using a novel structure\nFisher score in a data-dependent manner. Then, we progressively train the\nselected informative model structures with hierarchically constructed training\nsamples, progressing from fewer to more support shots. The resulting\nInformative Structure Adaptation (ISA) method effectively addresses domain\nshifts and equips existing well-trained in-domain FSS models with flexible\nadaptation capabilities for new domains, eliminating the need to redesign or\nretrain CD-FSS models on base data. Extensive experiments validate the\neffectiveness of our method, demonstrating superior performance across multiple\nCD-FSS benchmarks."}
{"id": "2504.17058", "pdf": "https://arxiv.org/pdf/2504.17058", "abs": "https://arxiv.org/abs/2504.17058", "authors": ["Rahul Vishwakarma", "Shrey Dharmendra Modi", "Vishwanath Seshagiri"], "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 1 figure", "summary": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems."}
{"id": "2404.19460", "pdf": "https://arxiv.org/pdf/2404.19460", "abs": "https://arxiv.org/abs/2404.19460", "authors": ["Antonio Emanuele Cinà", "Jérôme Rony", "Maura Pintor", "Luca Demetrio", "Ambra Demontis", "Battista Biggio", "Ismail Ben Ayed", "Fabio Roli"], "title": "AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "Paper accepted at AAAI2025. Project page and leaderboard:\n  https://attackbench.github.io", "summary": "Adversarial examples are typically optimized with gradient-based attacks.\nWhile novel attacks are continuously proposed, each is shown to outperform its\npredecessors using different experimental setups, hyperparameter settings, and\nnumber of forward and backward calls to the target models. This provides\noverly-optimistic and even biased evaluations that may unfairly favor one\nparticular attack over the others. In this work, we aim to overcome these\nlimitations by proposing AttackBench, i.e., the first evaluation framework that\nenables a fair comparison among different attacks. To this end, we first\npropose a categorization of gradient-based attacks, identifying their main\ncomponents and differences. We then introduce our framework, which evaluates\ntheir effectiveness and efficiency. We measure these characteristics by (i)\ndefining an optimality metric that quantifies how close an attack is to the\noptimal solution, and (ii) limiting the number of forward and backward queries\nto the model, such that all attacks are compared within a given maximum query\nbudget. Our extensive experimental analysis compares more than $100$ attack\nimplementations with a total of over $800$ different configurations against\nCIFAR-10 and ImageNet models, highlighting that only very few attacks\noutperform all the competing approaches. Within this analysis, we shed light on\nseveral implementation issues that prevent many attacks from finding better\nsolutions or running at all. We release AttackBench as a publicly-available\nbenchmark, aiming to continuously update it to include and evaluate novel\ngradient-based attacks for optimizing adversarial examples."}
{"id": "2504.21476", "pdf": "https://arxiv.org/pdf/2504.21476", "abs": "https://arxiv.org/abs/2504.21476", "authors": ["Xinyu Li", "Qi Yao", "Yuanda Wang"], "title": "GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "The 34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)", "summary": "Garment sewing patterns are fundamental design elements that bridge the gap\nbetween design concepts and practical manufacturing. The generative modeling of\nsewing patterns is crucial for creating diversified garments. However, existing\napproaches are limited either by reliance on a single input modality or by\nsuboptimal generation efficiency. In this work, we present GarmentDiffusion, a\nnew generative model capable of producing centimeter-precise, vectorized 3D\nsewing patterns from multimodal inputs (text, image, and incomplete sewing\npattern). Our method efficiently encodes 3D sewing pattern parameters into\ncompact edge token representations, achieving a sequence length that is 10\ntimes shorter than that of the autoregressive SewingGPT in DressCode. By\nemploying a diffusion transformer, we simultaneously denoise all edge tokens\nalong the temporal axis, while maintaining a constant number of denoising steps\nregardless of dataset-specific edge and panel statistics. With all combination\nof designs of our model, the sewing pattern generation speed is accelerated by\n100 times compared to SewingGPT. We achieve new state-of-the-art results on\nDressCodeData, as well as on the largest sewing pattern dataset, namely\nGarmentCodeData. The project website is available at\nhttps://shenfu-research.github.io/Garment-Diffusion/."}
{"id": "2504.20055", "pdf": "https://arxiv.org/pdf/2504.20055", "abs": "https://arxiv.org/abs/2504.20055", "authors": ["Juan D. Pinto", "Luc Paquette"], "title": "A constraints-based approach to fully interpretable neural networks for detecting learner behaviors", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to International Conference on Educational Data Mining (EDM)\n  2025", "summary": "The increasing use of complex machine learning models in education has led to\nconcerns about their interpretability, which in turn has spurred interest in\ndeveloping explainability techniques that are both faithful to the model's\ninner workings and intelligible to human end-users. In this paper, we describe\na novel approach to creating a neural-network-based behavior detection model\nthat is interpretable by design. Our model is fully interpretable, meaning that\nthe parameters we extract for our explanations have a clear interpretation,\nfully capture the model's learned knowledge about the learner behavior of\ninterest, and can be used to create explanations that are both faithful and\nintelligible. We achieve this by implementing a series of constraints to the\nmodel that both simplify its inference process and bring it closer to a human\nconception of the task at hand. We train the model to detect gaming-the-system\nbehavior, evaluate its performance on this task, and compare its learned\npatterns to those identified by human experts. Our results show that the model\nis successfully able to learn patterns indicative of gaming-the-system behavior\nwhile providing evidence for fully interpretable explanations. We discuss the\nimplications of our approach and suggest ways to evaluate explainability using\na human-grounded approach."}
{"id": "2405.15913", "pdf": "https://arxiv.org/pdf/2405.15913", "abs": "https://arxiv.org/abs/2405.15913", "authors": ["Ryan McKenna"], "title": "Scaling up the Banded Matrix Factorization Mechanism for Differentially Private ML", "categories": ["cs.LG", "cs.CR", "cs.DS"], "comment": null, "summary": "Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have\nproven to be effective alternatives to DP-SGD in large-epsilon few-epoch\ntraining regimes. Significant work has been done to find the best correlated\nnoise strategies, and the current state-of-the-art approach is DP-BandMF, which\noptimally balances the benefits of privacy amplification and noise correlation.\nDespite it's utility advantages, severe scalability limitations prevent this\nmechanism from handling large-scale training scenarios where the number of\ntraining iterations may exceed $10^4$ and the number of model parameters may\nexceed $10^7$. In this work, we present techniques to scale up DP-BandMF along\nthese two dimensions, significantly extending it's reach and enabling it to\nhandle settings with virtually any number of model parameters and training\niterations, with negligible utility degradation."}
{"id": "2504.21497", "pdf": "https://arxiv.org/pdf/2504.21497", "abs": "https://arxiv.org/abs/2504.21497", "authors": ["Mengting Wei", "Yante Li", "Tuomas Varanka", "Yan Jiang", "Guoying Zhao"], "title": "MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance", "categories": ["cs.CV"], "comment": null, "summary": "In this study, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nnot only enables precise extraction of motion features from driving videos, but\nalso contributes to the faithful preservation of face shape and geometry.\nSpecifically, we enhance the latent diffusion model with rich 3D expression and\ndetailed pose information by incorporating depth maps, normal maps, and\nrendering maps derived from FLAME sequences. These maps serve as motion\nguidance and are encoded into the denoising UNet through a specifically\ndesigned Geometric Guidance Encoder (GGE). A multi-layer feature fusion module\nwith integrated self-attention mechanisms is used to combine facial appearance\nand motion latent features within the spatial domain. By utilizing the 3D face\nparametric model as motion guidance, our method enables parametric alignment of\nface identity between the reference image and the motion captured from the\ndriving video. Experimental results on benchmark datasets show that our method\nexcels at generating high-quality face animations with precise expression and\nhead pose variation modeling. In addition, it demonstrates strong\ngeneralization performance on out-of-domain images. Code is publicly available\nat https://github.com/weimengting/MagicPortrait."}
{"id": "2504.20834", "pdf": "https://arxiv.org/pdf/2504.20834", "abs": "https://arxiv.org/abs/2504.20834", "authors": ["Alan Lee", "Harry Tong"], "title": "Token-Efficient RL for LLM Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": "Title updated to \"Token-Efficient RL for LLM Reasoning\" to better\n  reflect algorithmic focus. Revised abstract, intro, and conclusion. Paper\n  shortened and typos fixed", "summary": "We propose reinforcement learning (RL) strategies tailored for reasoning in\nlarge language models (LLMs) under strict memory and compute limits, with a\nparticular focus on compatibility with LoRA fine-tuning. Building on early\npolicy gradient methods with baseline subtraction, we design critic-free\nmethods that operate on a small, informative subset of output tokens to reduce\nmemory usage and stabilize training. We introduce S-GRPO, a stochastic variant\nof Group Relative Policy Optimization, and T-SPMO, a token-level prefix\nmatching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,\nour methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show\nstrong performance on multi-digit multiplication. Surprisingly, full-token GRPO\nunder LoRA fails to improve over the base model, suggesting that selective\ntoken-level optimization may act as an implicit regularizer in low-parameter\ntraining regimes."}
{"id": "2406.01781", "pdf": "https://arxiv.org/pdf/2406.01781", "abs": "https://arxiv.org/abs/2406.01781", "authors": ["Alexander Denker", "Francisco Vargas", "Shreyas Padhy", "Kieran Didi", "Simon Mathis", "Vincent Dutordoir", "Riccardo Barbano", "Emile Mathieu", "Urszula Julia Komorowska", "Pietro Lio"], "title": "DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2312.09236", "summary": "Generative modelling paradigms based on denoising diffusion processes have\nemerged as a leading candidate for conditional sampling in inverse problems. In\nmany real-world applications, we often have access to large, expensively\ntrained unconditional diffusion models, which we aim to exploit for improving\nconditional sampling. Most recent approaches are motivated heuristically and\nlack a unifying framework, obscuring connections between them. Further, they\noften suffer from issues such as being very sensitive to hyperparameters, being\nexpensive to train or needing access to weights hidden behind a closed API. In\nthis work, we unify conditional training and sampling using the mathematically\nwell-understood Doob's h-transform. This new perspective allows us to unify\nmany existing methods under a common umbrella. Under this framework, we propose\nDEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional\ngeneration that simply fine-tunes a very small network to quickly learn the\nconditional $h$-transform, while keeping the larger unconditional network\nunchanged. DEFT is much faster than existing baselines while achieving\nstate-of-the-art performance across a variety of linear and non-linear\nbenchmarks. On image reconstruction tasks, we achieve speedups of up to\n1.6$\\times$, while having the best perceptual quality on natural images and\nreconstruction performance on medical images. Further, we also provide initial\nexperiments on protein motif scaffolding and outperform reconstruction guidance\nmethods."}
{"id": "2505.00308", "pdf": "https://arxiv.org/pdf/2505.00308", "abs": "https://arxiv.org/abs/2505.00308", "authors": ["Biling Wang", "Austen Maniscalco", "Ti Bai", "Siqiu Wang", "Michael Dohopolski", "Mu-Han Lin", "Chenyang Shen", "Dan Nguyen", "Junzhou Huang", "Steve Jiang", "Xinlei Wang"], "title": "AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality", "categories": ["cs.CV", "cs.AI", "stat.AP"], "comment": null, "summary": "Purpose: This study presents a Deep Learning (DL)-based quality assessment\n(QA) approach for evaluating auto-generated contours (auto-contours) in\nradiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging\nBayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds,\nthe method enables confident QA predictions without relying on ground truth\ncontours or extensive manual labeling. Methods: We developed a BOC model to\nclassify auto-contour quality and quantify prediction uncertainty. A\ncalibration step was used to optimize uncertainty thresholds that meet clinical\naccuracy needs. The method was validated under three data scenarios: no manual\nlabels, limited labels, and extensive labels. For rectum contours in prostate\ncancer, we applied geometric surrogate labels when manual labels were absent,\ntransfer learning when limited, and direct supervision when ample labels were\navailable. Results: The BOC model delivered robust performance across all\nscenarios. Fine-tuning with just 30 manual labels and calibrating with 34\nsubjects yielded over 90% accuracy on test data. Using the calibrated\nthreshold, over 93% of the auto-contours' qualities were accurately predicted\nin over 98% of cases, reducing unnecessary manual reviews and highlighting\ncases needing correction. Conclusion: The proposed QA model enhances contouring\nefficiency in OART by reducing manual workload and enabling fast, informed\nclinical decisions. Through uncertainty quantification, it ensures safer, more\nreliable radiotherapy workflows."}
{"id": "2504.21415", "pdf": "https://arxiv.org/pdf/2504.21415", "abs": "https://arxiv.org/abs/2504.21415", "authors": ["Yi Wang", "Chengyv Wu", "Yang Liao", "Maowei You"], "title": "Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges", "categories": ["cs.CR", "cs.AI"], "comment": "13pages, 10 figures", "summary": "User authentication is essential to ensure secure access to computer systems,\nyet traditional methods face limitations in usability, cost, and security.\nMouse dynamics authentication, based on the analysis of users' natural\ninteraction behaviors with mouse devices, offers a cost-effective,\nnon-intrusive, and adaptable solution. However, challenges remain in\ndetermining the optimal data volume, balancing accuracy and practicality, and\neffectively capturing temporal behavioral patterns. In this study, we propose a\nstatistical method using Gaussian kernel density estimate (KDE) and\nKullback-Leibler (KL) divergence to estimate the sufficient data volume for\ntraining authentication models. We introduce the Mouse Authentication Unit\n(MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for\nefficient and accurate behavioral representation. Furthermore, we design the\nLocal-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet\nfor local feature extraction and GRU for modeling long-term temporal\ndependencies. Taking the Balabit and DFL datasets as examples, we significantly\nreduced the data scale, particularly by a factor of 10 for the DFL dataset,\ngreatly alleviating the training burden. Additionally, we determined the\noptimal input recognition unit length for the user authentication system on\ndifferent datasets based on the slope of Approximate Entropy. Training with\nimbalanced samples, our model achieved a successful defense AUC 98.52% for\nblind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing\nthe current sota performance."}
{"id": "2406.02175", "pdf": "https://arxiv.org/pdf/2406.02175", "abs": "https://arxiv.org/abs/2406.02175", "authors": ["Ayman Chaouki", "Jesse Read", "Albert Bifet"], "title": "Branches: Efficiently Seeking Optimal Sparse Decision Trees with AO*", "categories": ["cs.LG"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  Vancouver, Canada. PMLR 267, 2025", "summary": "Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine\nLearning, yet it poses a formidable optimisation challenge. Practical\nalgorithms have recently emerged, primarily leveraging Dynamic Programming and\nBranch & Bound. However, most of these approaches rely on a Depth-First-Search\nstrategy, which is inefficient when searching for DTs at high depths and\nrequires the definition of a maximum depth hyperparameter. Best-First-Search\nwas also employed by other methods to circumvent these issues. The downside of\nthis strategy is its higher memory consumption, as such, it has to be designed\nin a fully efficient manner that takes full advantage of the problem's\nstructure. We formulate the problem within an AND/OR graph search framework and\nwe solve it with a novel AO*-type algorithm called Branches. We prove both\noptimality and complexity guarantees for Branches and we show that it is more\nefficient than the state of the art theoretically and on a variety of\nexperiments. Furthermore, Branches supports non-binary features unlike the\nother methods, we show that this property can further induce larger gains in\ncomputational efficiency."}
{"id": "2505.03113", "pdf": "https://arxiv.org/pdf/2505.03113", "abs": "https://arxiv.org/abs/2505.03113", "authors": ["Zherui Zhang", "Rongtao Xu", "Jie Zhou", "Changwei Wang", "Xingtian Pei", "Wenhao Xu", "Jiguang Zhang", "Li Guo", "Longxiang Gao", "Wenbo Xu", "Shibiao Xu"], "title": "Image Recognition with Online Lightweight Vision Transformer: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "The Transformer architecture has achieved significant success in natural\nlanguage processing, motivating its adaptation to computer vision tasks. Unlike\nconvolutional neural networks, vision transformers inherently capture\nlong-range dependencies and enable parallel processing, yet lack inductive\nbiases and efficiency benefits, facing significant computational and memory\nchallenges that limit its real-world applicability. This paper surveys various\nonline strategies for generating lightweight vision transformers for image\nrecognition, focusing on three key areas: Efficient Component Design, Dynamic\nNetwork, and Knowledge Distillation. We evaluate the relevant exploration for\neach topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,\nparameters, throughput, and more to highlight their respective advantages,\ndisadvantages, and flexibility. Finally, we propose future research directions\nand potential challenges in the lightweighting of vision transformers with the\naim of inspiring further exploration and providing practical guidance for the\ncommunity. Project Page: https://github.com/ajxklo/Lightweight-VIT"}
{"id": "2505.01288", "pdf": "https://arxiv.org/pdf/2505.01288", "abs": "https://arxiv.org/abs/2505.01288", "authors": ["Changhe Chen", "Quantao Yang", "Xiaohao Xu", "Nima Fazeli", "Olov Andersson"], "title": "ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "One of the central challenges preventing robots from acquiring complex\nmanipulation skills is the prohibitive cost of collecting large-scale robot\ndemonstrations. In contrast, humans are able to learn efficiently by watching\nothers interact with their environment. To bridge this gap, we introduce\nsemantic action flow as a core intermediate representation capturing the\nessential spatio-temporal manipulator-object interactions, invariant to\nsuperficial visual differences. We present ViSA-Flow, a framework that learns\nthis representation self-supervised from unlabeled large-scale video data.\nFirst, a generative model is pre-trained on semantic action flows automatically\nextracted from large-scale human-object interaction video data, learning a\nrobust prior over manipulation structure. Second, this prior is efficiently\nadapted to a target robot by fine-tuning on a small set of robot demonstrations\nprocessed through the same semantic abstraction pipeline. We demonstrate\nthrough extensive experiments on the CALVIN benchmark and real-world tasks that\nViSA-Flow achieves state-of-the-art performance, particularly in low-data\nregimes, outperforming prior methods by effectively transferring knowledge from\nhuman video observation to robotic execution. Videos are available at\nhttps://visaflow-web.github.io/ViSAFLOW."}
{"id": "2406.07246", "pdf": "https://arxiv.org/pdf/2406.07246", "abs": "https://arxiv.org/abs/2406.07246", "authors": ["Vijaya Krishna Yalavarthi", "Randolf Scholz", "Christian Kloetergens", "Kiran Madhusudhanan", "Stefan Born", "Lars Schmidt-Thieme"], "title": "Marginalization Consistent Probabilistic Forecasting of Irregular Time Series via Mixture of Separable flows", "categories": ["cs.LG"], "comment": null, "summary": "Probabilistic forecasting models for joint distributions of targets in\nirregular time series with missing values are a heavily under-researched area\nin machine learning, with, to the best of our knowledge, only two Models have\nbeen researched so far: The Gaussian Process Regression model, and ProFITi.\nWhile ProFITi, thanks to using multivariate normalizing flows, is very\nexpressive, leading to better predictive performance, it suffers from\nmarginalization inconsistency: It does not guarantee that the marginal\ndistributions of a subset of variables in its predictive distributions coincide\nwith the directly predicted distributions of these variables. When asked to\ndirectly predict marginal distributions, they are often vastly inaccurate. We\npropose MOSES (Marginalization Consistent Mixture of Separable Flows), a model\nthat parametrizes a stochastic process through a mixture of several latent\nmultivariate Gaussian Processes combined with separable univariate Normalizing\nFlows. In particular, MOSES can be analytically marginalized, allowing it to\ndirectly answer a wider range of probabilistic queries than most competitors.\nExperiments on four datasets show that MOSES achieves both accurate joint and\nmarginal predictions, surpassing all other marginalization consistent\nbaselines, while only trailing slightly behind ProFITi in joint prediction, but\nvastly superior when predicting marginal distributions."}
{"id": "2505.04088", "pdf": "https://arxiv.org/pdf/2505.04088", "abs": "https://arxiv.org/abs/2505.04088", "authors": ["Shang Zhang", "Huanbin Zhang", "Dali Feng", "Yujie Cui", "Ruoyan Xiong", "Cen He"], "title": "SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared (TIR) object tracking often suffers from challenges such as\ntarget occlusion, motion blur, and background clutter, which significantly\ndegrade the performance of trackers. To address these issues, this paper\npro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a\nbidirectional state-space model and a self-attention mechanism. Specifically,\nwe introduce the Motion Mamba module into the Siamese architecture to ex-tract\nmotion features and recover overlooked edge details using bidirectional\nmodeling and self-attention. We propose a Siamese parameter-sharing strate-gy\nthat allows certain convolutional layers to share weights. This approach\nreduces computational redundancy while preserving strong feature\nrepresen-tation. In addition, we design a motion edge-aware regression loss to\nimprove tracking accuracy, especially for motion-blurred targets. Extensive\nexperi-ments are conducted on four TIR tracking benchmarks, including\nLSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT\nachieves superior performance in TIR target tracking."}
{"id": "2505.02881", "pdf": "https://arxiv.org/pdf/2505.02881", "abs": "https://arxiv.org/abs/2505.02881", "authors": ["Kazuki Fujii", "Yukito Tajima", "Sakae Mizuki", "Hinari Shimada", "Taihei Shiotani", "Koshiro Saito", "Masanari Ohi", "Masaki Kawamura", "Taishi Nakamura", "Takumi Okamoto", "Shigeki Ishida", "Kakeru Hattori", "Youmi Ma", "Hiroya Takamura", "Rio Yokota", "Naoaki Okazaki"], "title": "Rewriting Pre-Training Data Boosts LLM Performance in Math and Code", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The performance of large language models (LLMs) in program synthesis and\nmathematical reasoning is fundamentally limited by the quality of their\npre-training corpora. We introduce two openly licensed datasets, released under\nthe Llama 3.3 Community License, that significantly enhance LLM performance by\nsystematically rewriting public data. SwallowCode (approximately 16.1 billion\ntokens) refines Python snippets from The-Stack-v2 through a novel four-stage\npipeline: syntax validation, pylint-based style filtering, and a two-stage LLM\nrewriting process that enforces style conformity and transforms snippets into\nself-contained, algorithmically efficient examples. Unlike prior methods that\nrely on exclusionary filtering or limited transformations, our\ntransform-and-retain approach upgrades low-quality code, maximizing data\nutility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by\nremoving boilerplate, restoring context, and reformatting solutions into\nconcise, step-by-step explanations. Within a fixed 50 billion token training\nbudget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1\nby +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing\nthe baseline model's code generation capabilities. Similarly, substituting\nSwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies\nconfirm that each pipeline stage contributes incrementally, with rewriting\ndelivering the largest gains. All datasets, prompts, and checkpoints are\npublicly available, enabling reproducible research and advancing LLM\npre-training for specialized domains."}
{"id": "2406.13200", "pdf": "https://arxiv.org/pdf/2406.13200", "abs": "https://arxiv.org/abs/2406.13200", "authors": ["Xinyi Gao", "Hongzhi Yin", "Tong Chen", "Guanhua Ye", "Wentao Zhang", "Bin Cui"], "title": "RobGC: Towards Robust Graph Condensation", "categories": ["cs.LG"], "comment": "Accepted by TKDE 2025", "summary": "Graph neural networks (GNNs) have attracted widespread attention for their\nimpressive capability of graph representation learning. However, the increasing\nprevalence of large-scale graphs presents a significant challenge for GNN\ntraining due to their computational demands, limiting the applicability of GNNs\nin various scenarios. In response to this challenge, graph condensation (GC) is\nproposed as a promising acceleration solution, focusing on generating an\ninformative compact graph that enables efficient training of GNNs while\nretaining performance. Despite the potential to accelerate GNN training,\nexisting GC methods overlook the quality of large training graphs during both\nthe training and inference stages. They indiscriminately emulate the training\ngraph distributions, making the condensed graphs susceptible to noises within\nthe training graph and significantly impeding the application of GC in\nintricate real-world scenarios. To address this issue, we propose robust graph\ncondensation (RobGC), a plug-and-play approach for GC to extend the robustness\nand applicability of condensed graphs in noisy graph structure environments.\nSpecifically, RobGC leverages the condensed graph as a feedback signal to guide\nthe denoising process on the original training graph. A label propagation-based\nalternating optimization strategy is in place for the condensation and\ndenoising processes, contributing to the mutual purification of the condensed\ngraph and training graph. Additionally, as a GC method designed for inductive\ngraph inference, RobGC facilitates test-time graph denoising by leveraging the\nnoise-free condensed graph to calibrate the structure of the test graph.\nExtensive experiments show that RobGC is compatible with various GC methods,\nsignificantly boosting their robustness under different types and levels of\ngraph structural noises."}
{"id": "2505.04594", "pdf": "https://arxiv.org/pdf/2505.04594", "abs": "https://arxiv.org/abs/2505.04594", "authors": ["Zhihao Zhang", "Abhinav Kumar", "Girish Chandar Ganesan", "Xiaoming Liu"], "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets."}
{"id": "2505.04558", "pdf": "https://arxiv.org/pdf/2505.04558", "abs": "https://arxiv.org/abs/2505.04558", "authors": ["Wenzhao Liu", "Haoran Li", "Congying Han", "Zicheng Zhang", "Anqi Li", "Tiande Guo"], "title": "Purity Law for Generalizable Neural TSP Solvers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Achieving generalization in neural approaches across different scales and\ndistributions remains a significant challenge for the Traveling Salesman\nProblem~(TSP). A key obstacle is that neural networks often fail to learn\nrobust principles for identifying universal patterns and deriving optimal\nsolutions from diverse instances. In this paper, we first uncover Purity Law\n(PuLa), a fundamental structural principle for optimal TSP solutions, defining\nthat edge prevalence grows exponentially with the sparsity of surrounding\nvertices. Statistically validated across diverse instances, PuLa reveals a\nconsistent bias toward local sparsity in global optima. Building on this\ninsight, we propose Purity Policy Optimization~(PUPO), a novel training\nparadigm that explicitly aligns characteristics of neural solutions with PuLa\nduring the solution construction process to enhance generalization. Extensive\nexperiments demonstrate that PUPO can be seamlessly integrated with popular\nneural solvers, significantly enhancing their generalization performance\nwithout incurring additional computational overhead during inference."}
{"id": "2406.19657", "pdf": "https://arxiv.org/pdf/2406.19657", "abs": "https://arxiv.org/abs/2406.19657", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "LLMEasyQuant: Scalable Quantization for Parallel and Distributed LLM Inference", "categories": ["cs.LG"], "comment": null, "summary": "As large language models (LLMs) grow in size and deployment scale,\nquantization has become an essential technique for reducing memory footprint\nand improving inference efficiency. However, existing quantization toolkits\noften lack transparency, flexibility, and system-level scalability across GPUs\nand distributed environments. We present \\textbf{LLMEasyQuant}, a modular,\nsystem-aware quantization framework designed for efficient, low-bit inference\nof LLMs on single-node multi-GPU, multi-node, and edge hardware. LLMEasyQuant\nsupports a wide range of quantization methods -- including Symmetric\nQuantization, ZeroQuant, SmoothQuant, and SimQuant -- with unified interfaces\nfor per-layer calibration, bitwidth assignment, and runtime adaptation. It\nintegrates fused CUDA kernels with NCCL-based distributed synchronization and\nsupports both static and online quantization. Empirical results show that\nLLMEasyQuant can achieve substantial speedups in GEMM execution, HBM load time,\nand near-linear multi-GPU scaling. Ablation studies further validate its\nability to balance latency, memory, and accuracy under diverse deployment\nconditions. LLMEasyQuant offers a practical quantization serving system for\nscalable, hardware-optimized LLM inference."}
{"id": "2505.04979", "pdf": "https://arxiv.org/pdf/2505.04979", "abs": "https://arxiv.org/abs/2505.04979", "authors": ["Zhuang Qi", "Sijin Zhou", "Lei Meng", "Han Hu", "Han Yu", "Xiangxu Meng"], "title": "Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization", "categories": ["cs.CV"], "comment": "IJCAI-25 Accepted", "summary": "Attribute bias in federated learning (FL) typically leads local models to\noptimize inconsistently due to the learning of non-causal associations,\nresulting degraded performance. Existing methods either use data augmentation\nfor increasing sample diversity or knowledge distillation for learning\ninvariant representations to address this problem. However, they lack a\ncomprehensive analysis of the inference paths, and the interference from\nconfounding factors limits their performance. To address these limitations, we\npropose the \\underline{Fed}erated \\underline{D}econfounding and\n\\underline{D}ebiasing \\underline{L}earning (FedDDL) method. It constructs a\nstructured causal graph to analyze the model inference process, and performs\nbackdoor adjustment to eliminate confounding paths. Specifically, we design an\nintra-client deconfounding learning module for computer vision tasks to\ndecouple background and objects, generating counterfactual samples that\nestablish a connection between the background and any label, which stops the\nmodel from using the background to infer the label. Moreover, we design an\ninter-client debiasing learning module to construct causal prototypes to reduce\nthe proportion of the background in prototype components. Notably, it bridges\nthe gap between heterogeneous representations via causal prototypical\nregularization. Extensive experiments on 2 benchmarking datasets demonstrate\nthat \\methodname{} significantly enhances the model capability to focus on main\nobjects in unseen data, leading to 4.5\\% higher Top-1 Accuracy on average over\n9 state-of-the-art existing methods."}
{"id": "2505.04608", "pdf": "https://arxiv.org/pdf/2505.04608", "abs": "https://arxiv.org/abs/2505.04608", "authors": ["Drew Prinster", "Xing Han", "Anqi Liu", "Suchi Saria"], "title": "WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal Martingales", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "To be published in The International Conference on Machine Learning\n  (ICML), 2025", "summary": "Responsibly deploying artificial intelligence (AI) / machine learning (ML)\nsystems in high-stakes settings arguably requires not only proof of system\nreliability, but moreover continual, post-deployment monitoring to quickly\ndetect and address any unsafe behavior. Statistical methods for nonparametric\nchange-point detection -- especially the tools of conformal test martingales\n(CTMs) and anytime-valid inference -- offer promising approaches to this\nmonitoring task. However, existing methods are restricted to monitoring limited\nhypothesis classes or ``alarm criteria'' (such as data shifts that violate\ncertain exchangeability assumptions), do not allow for online adaptation in\nresponse to shifts, and/or do not enable root-cause analysis of any\ndegradation. In this paper, we expand the scope of these monitoring methods by\nproposing a weighted generalization of conformal test martingales (WCTMs),\nwhich lay a theoretical foundation for online monitoring for any unexpected\nchangepoints in the data distribution while controlling false-alarms. For\npractical applications, we propose specific WCTM algorithms that adapt online\nto mild covariate shifts (in the marginal input distribution) while quickly\ndetecting and diagnosing more severe shifts, such as concept shifts (in the\nconditional label distribution) or extreme (out-of-support) covariate shifts\nthat cannot be easily adapted to. On real-world datasets, we demonstrate\nimproved performance relative to state-of-the-art baselines."}
{"id": "2407.16556", "pdf": "https://arxiv.org/pdf/2407.16556", "abs": "https://arxiv.org/abs/2407.16556", "authors": ["Christodoulos Kechris", "Jonathan Dan", "Jose Miranda", "David Atienza"], "title": "DC is all you need: describing ReLU from a signal processing standpoint", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Non-linear activation functions are crucial in Convolutional Neural Networks.\nHowever, until now they have not been well described in the frequency domain.\nIn this work, we study the spectral behavior of ReLU, a popular activation\nfunction. We use the ReLU's Taylor expansion to derive its frequency domain\nbehavior. We demonstrate that ReLU introduces higher frequency oscillations in\nthe signal and a constant DC component. Furthermore, we investigate the\nimportance of this DC component, where we demonstrate that it helps the model\nextract meaningful features related to the input frequency content. We\naccompany our theoretical derivations with experiments and real-world examples.\nFirst, we numerically validate our frequency response model. Then we observe\nReLU's spectral behavior on two example models and a real-world one. Finally,\nwe experimentally investigate the role of the DC component introduced by ReLU\nin the CNN's representations. Our results indicate that the DC helps to\nconverge to a weight configuration that is close to the initial random weights."}
{"id": "2505.05007", "pdf": "https://arxiv.org/pdf/2505.05007", "abs": "https://arxiv.org/abs/2505.05007", "authors": ["Xin Bi", "Zhichao Li", "Yuxuan Xia", "Panpan Tong", "Lijuan Zhang", "Yang Chen", "Junsheng Fu"], "title": "Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition", "categories": ["cs.CV"], "comment": "9 pages and 12 figures. Under review at IEEE RA-L", "summary": "Accurate online map matching is fundamental to vehicle navigation and the\nactivation of intelligent driving functions. Current online map matching\nmethods are prone to errors in complex road networks, especially in multilevel\nroad area. To address this challenge, we propose an online Standard Definition\n(SD) map matching method by constructing a Hidden Markov Model (HMM) with\nmultiple probability factors. Our proposed method can achieve accurate map\nmatching even in complex road networks by carefully leveraging lane markings\nand scenario recognition in the designing of the probability factors. First,\nthe lane markings are generated by a multi-lane tracking method and associated\nwith the SD map using HMM to build an enriched SD map. In areas covered by the\nenriched SD map, the vehicle can re-localize itself by performing Iterative\nClosest Point (ICP) registration for the lane markings. Then, the probability\nfactor accounting for the lane marking detection can be obtained using the\nassociation probability between adjacent lanes and roads. Second, the driving\nscenario recognition model is applied to generate the emission probability\nfactor of scenario recognition, which improves the performance of map matching\non elevated roads and ordinary urban roads underneath them. We validate our\nmethod through extensive road tests in Europe and China, and the experimental\nresults show that our proposed method effectively improves the online map\nmatching accuracy as compared to other existing methods, especially in\nmultilevel road area. Specifically, the experiments show that our proposed\nmethod achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset\nand test data of multilevel road areas in Shanghai respectively, significantly\noutperforming benchmark methods. The implementation is available at\nhttps://github.com/TRV-Lab/LMSR-OMM."}
{"id": "2505.04841", "pdf": "https://arxiv.org/pdf/2505.04841", "abs": "https://arxiv.org/abs/2505.04841", "authors": ["Nishikanta Mohanty", "Bikash K. Behera", "Badshah Mukherjee", "Christopher Ferrie"], "title": "Quantum-Inspired Optimization Process for Data Imputation", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Data imputation is a critical step in data pre-processing, particularly for\ndatasets with missing or unreliable values. This study introduces a novel\nquantum-inspired imputation framework evaluated on the UCI Diabetes dataset,\nwhich contains biologically implausible missing values across several clinical\nfeatures. The method integrates Principal Component Analysis (PCA) with\nquantum-assisted rotations, optimized through gradient-free classical\noptimizers -COBYLA, Simulated Annealing, and Differential Evolution to\nreconstruct missing values while preserving statistical fidelity. Reconstructed\nvalues are constrained within +/-2 standard deviations of original feature\ndistributions, avoiding unrealistic clustering around central tendencies. This\napproach achieves a substantial and statistically significant improvement,\nincluding an average reduction of over 85% in Wasserstein distance and\nKolmogorov-Smirnov test p-values between 0.18 and 0.22, compared to p-values >\n0.99 in classical methods such as Mean, KNN, and MICE. The method also\neliminates zero-value artifacts and enhances the realism and variability of\nimputed data. By combining quantum-inspired transformations with a scalable\nclassical framework, this methodology provides a robust solution for imputation\ntasks in domains such as healthcare and AI pipelines, where data quality and\nintegrity are crucial."}
{"id": "2409.01930", "pdf": "https://arxiv.org/pdf/2409.01930", "abs": "https://arxiv.org/abs/2409.01930", "authors": ["Rajesh Upadhayayaya", "Manish Raj Osti", "Zachary Smith", "Chritopher Kottmyer"], "title": "Efficient LLM Context Distillation", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate proficiency across diverse tasks but\noften require targeted adaptations for specific applications. Various methods\nhave been proposed to facilitate this adaptation, including fewshot\nfine-tuning, in-context learning, and context distillation. This paper\nspecifically investigates context distillation a method that extends the\nutility of task-specific examples by internalizing them, thus augmenting the\nexample set accessible for model inference. We conduct a comparative analysis\nof context distillation with in-context learning (ICL) and few-shot fine-tuning\n(FT), aiming to ascertain the efficacy of context distillation in adapting\nmodels using minimal in-context examples. Employing matched datasets from\nMobach, our experiments leverage OPT models of various sizes. The results\nindicate that context distillation effectively adapts models, with student\nmodels attaining comparable in-domain and out-of-domain accuracies to\nin-context learning. Although context distillation surpasses ICL in\nout-of-domain generalization, it does not achieve the performance levels of FT.\nHowever, the reduced dataset size and computational demands position context\ndistillation as a viable alternative, especially for smaller datasets. Overall,\nthis study presents context distillation as an efficient and potent method for\ncustomizing LLMs to specific tasks."}
{"id": "2505.05081", "pdf": "https://arxiv.org/pdf/2505.05081", "abs": "https://arxiv.org/abs/2505.05081", "authors": ["Jinyu Gu", "Haipeng Liu", "Meng Wang", "Yang Wang"], "title": "PIDiff: Image Customization for Personalized Identities with Diffusion Models", "categories": ["cs.CV"], "comment": "9 pages, 11 figures", "summary": "Text-to-image generation for personalized identities aims at incorporating\nthe specific identity into images using a text prompt and an identity image.\nBased on the powerful generative capabilities of DDPMs, many previous works\nadopt additional prompts, such as text embeddings and CLIP image embeddings, to\nrepresent the identity information, while they fail to disentangle the identity\ninformation and background information. As a result, the generated images not\nonly lose key identity characteristics but also suffer from significantly\nreduced diversity. To address this issue, previous works have combined the W+\nspace from StyleGAN with diffusion models, leveraging this space to provide a\nmore accurate and comprehensive representation of identity features through\nmulti-level feature extraction. However, the entanglement of identity and\nbackground information in in-the-wild images during training prevents accurate\nidentity localization, resulting in severe semantic interference between\nidentity and background. In this paper, we propose a novel fine-tuning-based\ndiffusion model for personalized identities text-to-image generation, named\nPIDiff, which leverages the W+ space and an identity-tailored fine-tuning\nstrategy to avoid semantic entanglement and achieves accurate feature\nextraction and localization. Style editing can also be achieved by PIDiff\nthrough preserving the characteristics of identity features in the W+ space,\nwhich vary from coarse to fine. Through the combination of the proposed\ncross-attention block and parameter optimization strategy, PIDiff preserves the\nidentity information and maintains the generation capability for in-the-wild\nimages of the pre-trained model during inference. Our experimental results\nvalidate the effectiveness of our method in this task."}
{"id": "2505.05181", "pdf": "https://arxiv.org/pdf/2505.05181", "abs": "https://arxiv.org/abs/2505.05181", "authors": ["Bojian Yin", "Federico Corradi"], "title": "Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 5 figures", "summary": "Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\nglobal gradient synchronization limits scalability and imposes significant\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\nscalable alternative that reframes training as hierarchical variational\ninference. SVP treats layer activations as latent variables and optimizes local\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\npreserving global coherence. However, directly applying KL divergence in\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\ncompression. To prevent this, SVP projects activations into low-dimensional\nspaces via fixed random matrices, ensuring information preservation and\nrepresentational diversity. Combined with a feature alignment loss for\ninter-layer consistency, SVP achieves competitive accuracy with BP across\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\nImageNet), reduces memory usage by up to 4x, and significantly improves\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\nrepresentation learning, opening pathways toward more modular and interpretable\nneural network design."}
{"id": "2409.02426", "pdf": "https://arxiv.org/pdf/2409.02426", "abs": "https://arxiv.org/abs/2409.02426", "authors": ["Peng Wang", "Huijie Zhang", "Zekai Zhang", "Siyi Chen", "Yi Ma", "Qing Qu"], "title": "Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering", "categories": ["cs.LG", "cs.CV"], "comment": "39 pages, 8 figures, 2 tables", "summary": "Recent empirical studies have demonstrated that diffusion models can\neffectively learn the image distribution and generate new samples. Remarkably,\nthese models can achieve this even with a small number of training samples\ndespite a large image dimension, circumventing the curse of dimensionality. In\nthis work, we provide theoretical insights into this phenomenon by leveraging\nkey empirical observations: (i) the low intrinsic dimensionality of image data,\n(ii) a union of manifold structure of image data, and (iii) the low-rank\nproperty of the denoising autoencoder in trained diffusion models. These\nobservations motivate us to assume the underlying data distribution of image\ndata as a mixture of low-rank Gaussians and to parameterize the denoising\nautoencoder as a low-rank model according to the score function of the assumed\ndistribution. With these setups, we rigorously show that optimizing the\ntraining loss of diffusion models is equivalent to solving the canonical\nsubspace clustering problem over the training samples. Based on this\nequivalence, we further show that the minimal number of samples required to\nlearn the underlying distribution scales linearly with the intrinsic dimensions\nunder the above data and model assumptions. This insight sheds light on why\ndiffusion models can break the curse of dimensionality and exhibit the phase\ntransition in learning distributions. Moreover, we empirically establish a\ncorrespondence between the subspaces and the semantic representations of image\ndata, facilitating image editing. We validate these results with corroborated\nexperimental results on both simulated distributions and image datasets."}
{"id": "2505.05101", "pdf": "https://arxiv.org/pdf/2505.05101", "abs": "https://arxiv.org/abs/2505.05101", "authors": ["Hongyang Zhu", "Haipeng Liu", "Bo Fu", "Yang Wang"], "title": "MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models", "categories": ["cs.CV"], "comment": "9 pages, 7 figures", "summary": "Multi-object editing aims to modify multiple objects or regions in complex\nscenes while preserving structural coherence. This task faces significant\nchallenges in scenarios involving overlapping or interacting objects: (1)\nInaccurate localization of target objects due to attention misalignment,\nleading to incomplete or misplaced edits; (2) Attribute-object mismatch, where\ncolor or texture changes fail to align with intended regions due to\ncross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color\nbleeding into non-target areas). Existing methods struggle with these\nchallenges: approaches relying on global cross-attention mechanisms suffer from\nattention dilution and spatial interference between objects, while mask-based\nmethods fail to bind attributes to geometrically accurate regions due to\nfeature entanglement in multi-object scenarios. To address these limitations,\nwe propose a training-free, inference-stage optimization approach that enables\nprecise localized image manipulation in complex multi-object scenes, named\nMDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via\ntwo key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention\nwith segmentation masks for precise object positioning, and Color Consistency\nLoss (CCL) amplifies target attribute attention within masks while suppressing\nleakage to adjacent regions. This dual-loss design ensures localized and\ncoherent multi-object edits. Extensive experiments demonstrate that MDE-Edit\noutperforms state-of-the-art methods in editing accuracy and visual quality,\noffering a robust solution for complex multi-object image manipulation tasks."}
{"id": "2505.05470", "pdf": "https://arxiv.org/pdf/2505.05470", "abs": "https://arxiv.org/abs/2505.05470", "authors": ["Jie Liu", "Gongye Liu", "Jiajun Liang", "Yangguang Li", "Jiaheng Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Wanli Ouyang"], "title": "Flow-GRPO: Training Flow Matching Models via Online RL", "categories": ["cs.CV", "cs.AI"], "comment": "Code: https://github.com/yifan123/flow_grpo", "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation. Flow-GRPO\nalso achieves substantial gains in human preference alignment. Notably, very\nlittle reward hacking occurred, meaning rewards did not increase at the cost of\nappreciable image quality or diversity degradation."}
{"id": "2409.15647", "pdf": "https://arxiv.org/pdf/2409.15647", "abs": "https://arxiv.org/abs/2409.15647", "authors": ["Ying Fan", "Yilun Du", "Kannan Ramchandran", "Kangwook Lee"], "title": "Looped Transformers for Length Generalization", "categories": ["cs.LG"], "comment": "ICLR 2025", "summary": "Recent work has shown that Transformers trained from scratch can successfully\nsolve various arithmetic and algorithmic tasks, such as adding numbers and\ncomputing parity. While these Transformers generalize well on unseen inputs of\nthe same length, they struggle with length generalization, i.e., handling\ninputs of unseen lengths. In this work, we demonstrate that looped Transformers\nwith an adaptive number of steps significantly improve length generalization.\nWe focus on tasks with a known iterative solution, involving multiple\niterations of a RASP-L operation - a length-generalizable operation that can be\nexpressed by a finite-sized Transformer. We train looped Transformers using our\nproposed learning algorithm and observe that they learn highly\nlength-generalizable solutions for various tasks."}
{"id": "2505.05376", "pdf": "https://arxiv.org/pdf/2505.05376", "abs": "https://arxiv.org/abs/2505.05376", "authors": ["Rachmadio Noval Lazuardi", "Artem Sevastopolsky", "Egor Zakharov", "Matthias Niessner", "Vanessa Sklyarova"], "title": "GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans", "categories": ["cs.CV"], "comment": "15 pages, 9 figures, 1 table", "summary": "We propose a novel method that reconstructs hair strands directly from\ncolorless 3D scans by leveraging multi-modal hair orientation extraction. Hair\nstrand reconstruction is a fundamental problem in computer vision and graphics\nthat can be used for high-fidelity digital avatar synthesis, animation, and\nAR/VR applications. However, accurately recovering hair strands from raw scan\ndata remains challenging due to human hair's complex and fine-grained\nstructure. Existing methods typically rely on RGB captures, which can be\nsensitive to the environment and can be a challenging domain for extracting the\norientation of guiding strands, especially in the case of challenging\nhairstyles. To reconstruct the hair purely from the observed geometry, our\nmethod finds sharp surface features directly on the scan and estimates strand\norientation through a neural 2D line detector applied to the renderings of scan\nshading. Additionally, we incorporate a diffusion prior trained on a diverse\nset of synthetic hair scans, refined with an improved noise schedule, and\nadapted to the reconstructed contents via a scan-specific text prompt. We\ndemonstrate that this combination of supervision signals enables accurate\nreconstruction of both simple and intricate hairstyles without relying on color\ninformation. To facilitate further research, we introduce Strands400, the\nlargest publicly available dataset of hair strands with detailed surface\ngeometry extracted from real-world data, which contains reconstructed hair\nstrands from the scans of 400 subjects."}
{"id": "2505.05522", "pdf": "https://arxiv.org/pdf/2505.05522", "abs": "https://arxiv.org/abs/2505.05522", "authors": ["Luke Darlow", "Ciaran Regan", "Sebastian Risi", "Jeffrey Seely", "Llion Jones"], "title": "Continuous Thought Machines", "categories": ["cs.LG", "cs.AI"], "comment": "Technical report accompanied by online project page:\n  https://pub.sakana.ai/ctm/", "summary": "Biological brains demonstrate complex neural activity, where the timing and\ninterplay between neurons is critical to how brains process information. Most\ndeep learning architectures simplify neural activity by abstracting away\ntemporal dynamics. In this paper we challenge that paradigm. By incorporating\nneuron-level processing and synchronization, we can effectively reintroduce\nneural timing as a foundational element. We present the Continuous Thought\nMachine (CTM), a model designed to leverage neural dynamics as its core\nrepresentation. The CTM has two core innovations: (1) neuron-level temporal\nprocessing, where each neuron uses unique weight parameters to process a\nhistory of incoming signals; and (2) neural synchronization employed as a\nlatent representation. The CTM aims to strike a balance between oversimplified\nneuron abstractions that improve computational efficiency, and biological\nrealism. It operates at a level of abstraction that effectively captures\nessential temporal dynamics while remaining computationally tractable for deep\nlearning. We demonstrate the CTM's strong performance and versatility across a\nrange of challenging tasks, including ImageNet-1K classification, solving 2D\nmazes, sorting, parity computation, question-answering, and RL tasks. Beyond\ndisplaying rich internal representations and offering a natural avenue for\ninterpretation owing to its internal process, the CTM is able to perform tasks\nthat require complex sequential reasoning. The CTM can also leverage adaptive\ncompute, where it can stop earlier for simpler tasks, or keep computing when\nfaced with more challenging instances. The goal of this work is to share the\nCTM and its associated innovations, rather than pushing for new\nstate-of-the-art results. To that end, we believe the CTM represents a\nsignificant step toward developing more biologically plausible and powerful\nartificial intelligence systems."}
{"id": "2409.19800", "pdf": "https://arxiv.org/pdf/2409.19800", "abs": "https://arxiv.org/abs/2409.19800", "authors": ["Guy Kornowski"], "title": "Differentially Private Bilevel Optimization", "categories": ["cs.LG", "cs.CR", "math.OC"], "comment": "Major rewrite: Sections 3 & 7 are new; various improvements in\n  presentation", "summary": "We present differentially private (DP) algorithms for bilevel optimization, a\nproblem class that received significant attention lately in various machine\nlearning applications. These are the first algorithms for such problems under\nstandard DP constraints, and are also the first to avoid Hessian computations\nwhich are prohibitive in large-scale settings. Under the well-studied setting\nin which the upper-level is not necessarily convex and the lower-level problem\nis strongly-convex, our proposed gradient-based $(\\epsilon,\\delta)$-DP\nalgorithm returns a point with hypergradient norm at most\n$\\widetilde{\\mathcal{O}}\\left((\\sqrt{d_\\mathrm{up}}/\\epsilon\nn)^{1/2}+(\\sqrt{d_\\mathrm{low}}/\\epsilon n)^{1/3}\\right)$ where $n$ is the\ndataset size, and $d_\\mathrm{up}/d_\\mathrm{low}$ are the upper/lower level\ndimensions. Our analysis covers constrained and unconstrained problems alike,\naccounts for mini-batch gradients, and applies to both empirical and population\nlosses. As an application, we specialize our analysis to derive a simple\nprivate rule for tuning a regularization hyperparameter."}
{"id": "2505.05472", "pdf": "https://arxiv.org/pdf/2505.05472", "abs": "https://arxiv.org/abs/2505.05472", "authors": ["Chao Liao", "Liyang Liu", "Xun Wang", "Zhengxiong Luo", "Xinyu Zhang", "Wenliang Zhao", "Jie Wu", "Liang Li", "Zhi Tian", "Weilin Huang"], "title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation", "categories": ["cs.CV"], "comment": "Mogao Technical Report", "summary": "Recent progress in unified models for image understanding and generation has\nbeen impressive, yet most approaches remain limited to single-modal generation\nconditioned on multiple modalities. In this paper, we present Mogao, a unified\nframework that advances this paradigm by enabling interleaved multi-modal\ngeneration through a causal approach. Mogao integrates a set of key technical\nimprovements in architecture design, including a deep-fusion design, dual\nvision encoders, interleaved rotary position embeddings, and multi-modal\nclassifier-free guidance, which allow it to harness the strengths of both\nautoregressive models for text generation and diffusion models for high-quality\nimage synthesis. These practical improvements also make Mogao particularly\neffective to process interleaved sequences of text and images arbitrarily. To\nfurther unlock the potential of unified models, we introduce an efficient\ntraining strategy on a large-scale, in-house dataset specifically curated for\njoint text and image generation. Extensive experiments show that Mogao not only\nachieves state-of-the-art performance in multi-modal understanding and\ntext-to-image generation, but also excels in producing high-quality, coherent\ninterleaved outputs. Its emergent capabilities in zero-shot image editing and\ncompositional generation highlight Mogao as a practical omni-modal foundation\nmodel, paving the way for future development and scaling the unified\nmulti-modal systems."}
{"id": "2505.05533", "pdf": "https://arxiv.org/pdf/2505.05533", "abs": "https://arxiv.org/abs/2505.05533", "authors": ["Zhiyuan Ning", "Pengfei Wang", "Ziyue Qiao", "Pengyang Wang", "Yuanchun Zhou"], "title": "Rethinking Graph Contrastive Learning through Relative Similarity Preservation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI2025; full version including appendix", "summary": "Graph contrastive learning (GCL) has achieved remarkable success by following\nthe computer vision paradigm of preserving absolute similarity between\naugmented views. However, this approach faces fundamental challenges in graphs\ndue to their discrete, non-Euclidean nature -- view generation often breaks\nsemantic validity and similarity verification becomes unreliable. Through\nanalyzing 11 real-world graphs, we discover a universal pattern transcending\nthe homophily-heterophily dichotomy: label consistency systematically\ndiminishes as structural distance increases, manifesting as smooth decay in\nhomophily graphs and oscillatory decay in heterophily graphs. We establish\ntheoretical guarantees for this pattern through random walk theory, proving\nlabel distribution convergence and characterizing the mechanisms behind\ndifferent decay behaviors. This discovery reveals that graphs naturally encode\nrelative similarity patterns, where structurally closer nodes exhibit\ncollectively stronger semantic relationships. Leveraging this insight, we\npropose RELGCL, a novel GCL framework with complementary pairwise and listwise\nimplementations that preserve these inherent patterns through collective\nsimilarity objectives. Extensive experiments demonstrate that our method\nconsistently outperforms 20 existing approaches across both homophily and\nheterophily graphs, validating the effectiveness of leveraging natural relative\nsimilarity over artificial absolute similarity."}
{"id": "2410.00215", "pdf": "https://arxiv.org/pdf/2410.00215", "abs": "https://arxiv.org/abs/2410.00215", "authors": ["Yejin Lee", "Anna Sun", "Basil Hosmer", "Bilge Acun", "Can Balioglu", "Changhan Wang", "Charles David Hernandez", "Christian Puhrsch", "Daniel Haziza", "Driss Guessous", "Francisco Massa", "Jacob Kahn", "Jeffrey Wan", "Jeremy Reizenstein", "Jiaqi Zhai", "Joe Isaacson", "Joel Schlosser", "Juan Pino", "Kaushik Ram Sadagopan", "Leonid Shamis", "Linjian Ma", "Min-Jae Hwang", "Mingda Chen", "Mostafa Elhoushi", "Pedro Rodriguez", "Ram Pasunuru", "Scott Yih", "Sravya Popuri", "Xing Liu", "Carole-Jean Wu"], "title": "Characterizing and Efficiently Accelerating Multimodal Generation Model Inference", "categories": ["cs.LG"], "comment": "13 pages including references. 8 Figures. Under review to HPCA 2025\n  Industry Track", "summary": "Generative artificial intelligence (AI) technology is revolutionizing the\ncomputing industry. Not only its applications have broadened to various sectors\nbut also poses new system design and optimization opportunities. The technology\nis capable of understanding and responding in multiple modalities. However, the\nadvanced capability currently comes with significant system resource demands.\nTo sustainably scale generative AI capabilities to billions of users in the\nworld, inference must be fast and efficient. This paper pinpoints key system\ndesign and optimization opportunities by characterizing a family of emerging\nmulti-modal generation models on real systems. Auto-regressive token generation\nis a critical latency performance bottleneck, typically dominated by GPU idle\ntime. In addition to memory-intensive attention across the generative AI\nmodels, linear operations constitute significant inference latency due to the\nfeed forward networks in Transformer-based models. We demonstrate that\nstate-of-the-art optimization levers, spanning from applications to system\nsoftware and hardware, set a 3.88x better baseline."}
{"id": "2505.05573", "pdf": "https://arxiv.org/pdf/2505.05573", "abs": "https://arxiv.org/abs/2505.05573", "authors": ["Mikhail Chaichuk", "Sushant Gautam", "Steven Hicks", "Elena Tutubalina"], "title": "Prompt to Polyp: Medical Text-Conditioned Image Synthesis with Diffusion Models", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.4.8; J.3"], "comment": "code available at\n  https://github.com/THunderCondOR/ImageCLEFmed-MEDVQA-GI-2024-MMCP-Team", "summary": "The generation of realistic medical images from text descriptions has\nsignificant potential to address data scarcity challenges in healthcare AI\nwhile preserving patient privacy. This paper presents a comprehensive study of\ntext-to-image synthesis in the medical domain, comparing two distinct\napproaches: (1) fine-tuning large pre-trained latent diffusion models and (2)\ntraining small, domain-specific models. We introduce a novel model named MSDM,\nan optimized architecture based on Stable Diffusion that integrates a clinical\ntext encoder, variational autoencoder, and cross-attention mechanisms to better\nalign medical text prompts with generated images. Our study compares two\napproaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus\ntraining compact domain-specific models (MSDM). Evaluation across colonoscopy\n(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models\nachieve higher fidelity, our optimized MSDM delivers comparable quality with\nlower computational costs. Quantitative metrics and qualitative evaluations by\nmedical experts reveal strengths and limitations of each approach."}
{"id": "2505.05877", "pdf": "https://arxiv.org/pdf/2505.05877", "abs": "https://arxiv.org/abs/2505.05877", "authors": ["Rong Yin", "Ruyue Liu", "Xiaoshuai Hao", "Xingrui Zhou", "Yong Liu", "Can Ma", "Weiping Wang"], "title": "Multi-Modal Molecular Representation Learning via Structure Awareness", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IEEE Transactions on Image Processing (TIP) 2025", "summary": "Accurate extraction of molecular representations is a critical step in the\ndrug discovery process. In recent years, significant progress has been made in\nmolecular representation learning methods, among which multi-modal molecular\nrepresentation methods based on images, and 2D/3D topologies have become\nincreasingly mainstream. However, existing these multi-modal approaches often\ndirectly fuse information from different modalities, overlooking the potential\nof intermodal interactions and failing to adequately capture the complex\nhigher-order relationships and invariant features between molecules. To\novercome these challenges, we propose a structure-awareness-based multi-modal\nself-supervised molecular representation pre-training framework (MMSA) designed\nto enhance molecular graph representations by leveraging invariant knowledge\nbetween molecules. The framework consists of two main modules: the multi-modal\nmolecular representation learning module and the structure-awareness module.\nThe multi-modal molecular representation learning module collaboratively\nprocesses information from different modalities of the same molecule to\novercome intermodal differences and generate a unified molecular embedding.\nSubsequently, the structure-awareness module enhances the molecular\nrepresentation by constructing a hypergraph structure to model higher-order\ncorrelations between molecules. This module also introduces a memory mechanism\nfor storing typical molecular representations, aligning them with memory\nanchors in the memory bank to integrate invariant knowledge, thereby improving\nthe model generalization ability. Extensive experiments have demonstrated the\neffectiveness of MMSA, which achieves state-of-the-art performance on the\nMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to\n9.6% over baseline methods."}
{"id": "2410.06300", "pdf": "https://arxiv.org/pdf/2410.06300", "abs": "https://arxiv.org/abs/2410.06300", "authors": ["Ali Gorji", "Andisheh Amrollahi", "Andreas Krause"], "title": "SHAP values via sparse Fourier representation", "categories": ["cs.LG"], "comment": "Under review", "summary": "SHAP (SHapley Additive exPlanations) values are a widely used method for\nlocal feature attribution in interpretable and explainable AI. We propose an\nefficient two-stage algorithm for computing SHAP values in both black-box\nsetting and tree-based models. Motivated by spectral bias in real-world\npredictors, we first approximate models using compact Fourier representations,\nexactly for trees and approximately for black-box models. In the second stage,\nwe introduce a closed-form formula for {\\em exactly} computing SHAP values\nusing the Fourier representation, that ``linearizes'' the computation into a\nsimple summation and is amenable to parallelization. As the Fourier\napproximation is computed only once, our method enables amortized SHAP value\ncomputation, achieving significant speedups over existing methods and a tunable\ntrade-off between efficiency and precision."}
{"id": "2505.06108", "pdf": "https://arxiv.org/pdf/2505.06108", "abs": "https://arxiv.org/abs/2505.06108", "authors": ["Lennart Justen"], "title": "LLMs Outperform Experts on Challenging Biology Benchmarks", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "This study systematically evaluates 27 frontier Large Language Models on\neight biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with OpenAI's o3 now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including the biology subsets of GPQA and WMDP and\nLAB-Bench CloningScenarios. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance."}
{"id": "2410.11539", "pdf": "https://arxiv.org/pdf/2410.11539", "abs": "https://arxiv.org/abs/2410.11539", "authors": ["M. Germán-Morales", "A. J. Rivera-Rivas", "M. J. del Jesus Díaz", "C. J. Carmona"], "title": "Transfer Learning with Foundational Models for Time Series Forecasting using Low-Rank Adaptations", "categories": ["cs.LG"], "comment": null, "summary": "Foundational Models are an emerging widely used technique of GenAI. These\nmodels are distinguished by their scalability and the ease with which they can\nbe adapted through the exploitation of Transfer Learning. The availability of\nhigh computational power and large datasets have supported their development,\nachieving a high generalization capacity due to the enormous and heterogeneous\namounts of data used in their initial training. These characteristics\ncontribute to a solid base that can be adapted or adjusted to a wide range of\ntasks, increasing their applicability. This study proposes the methodology\nLLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for\nthe Time Series Forecasting task. An adequate time-series prompting schema and\nLow-Rank Adaptations are used to enhance the knowledge of the model with\ndiverse time series datasets, known as the fine-tuning phase. A study divided\nin two stages has been performed for evaluating the effectiveness of the\nproposed methodology. Initially, a comparison was made between the performance\nof LLIAM and different state-of-the-art DL algorithms, including Recurrent\nNeural Networks and Temporal Convolutional Networks, as well as a LLM-based\nmethod, TimeLLM. Following this, a zero-shot study is presented in order to\nevaluate the generalization capacity of the proposed methodology with time\nseries datasets from unknown domains not considered in the model training. The\noutcomes of this investigation demonstrate the efficacy of LLIAM, highlighting\nthat this straightforward and general approach can attain competent results\nwithout the necessity for applying complex modifications. This work also\nencourages the use of available resources (such as these pre-trained models)\nand efficient fine-tuning techniques to avoid unnecessary and costly training,\nnarrowing the gap between the goals of traditional AI and Green AI."}
{"id": "2411.05673", "pdf": "https://arxiv.org/pdf/2411.05673", "abs": "https://arxiv.org/abs/2411.05673", "authors": ["Tony Lindeberg"], "title": "Relationships between the degrees of freedom in the affine Gaussian derivative model for visual receptive fields and 2-D affine image transformations, with application to covariance properties of simple cells in the primary visual cortex", "categories": ["q-bio.NC", "cs.CV"], "comment": "22 pages, 9 figures", "summary": "When observing the surface patterns of objects delimited by smooth surfaces,\nthe projections of the surface patterns to the image domain will be subject to\nsubstantial variabilities, as induced by variabilities in the geometric viewing\nconditions, and as generated by either monocular or binocular imaging\nconditions, or by relative motions between the object and the observer over\ntime. To first order of approximation, the image deformations of such projected\nsurface patterns can be modelled as local linearizations in terms of local 2-D\nspatial affine transformations.\n  This paper presents a theoretical analysis of relationships between the\ndegrees of freedom in 2-D spatial affine image transformations and the degrees\nof freedom in the affine Gaussian derivative model for visual receptive fields.\nFor this purpose, we first describe a canonical decomposition of 2-D affine\ntransformations on a product form, closely related to a singular value\ndecomposition, while in closed form, and which reveals the degrees of freedom\nin terms of (i) uniform scaling transformations, (ii) an overall amount of\nglobal rotation, (iii) a complementary non-uniform scaling transformation and\n(iv) a relative normalization to a preferred symmetry orientation in the image\ndomain. Then, we show how these degrees of freedom relate to the degrees of\nfreedom in the affine Gaussian derivative model.\n  Finally, we use these theoretical results to consider whether we could regard\nthe biological receptive fields in the primary visual cortex of higher mammals\nas being able to span the degrees of freedom of 2-D spatial affine\ntransformations, based on interpretations of existing neurophysiological\nexperimental results."}
{"id": "2410.14081", "pdf": "https://arxiv.org/pdf/2410.14081", "abs": "https://arxiv.org/abs/2410.14081", "authors": ["Shangzhe Li", "Zhiao Huang", "Hao Su"], "title": "Reward-free World Models for Online Imitation Learning", "categories": ["cs.LG"], "comment": "ICML 2025; Code available at: https://github.com/TobyLeelsz/iqmpc", "summary": "Imitation learning (IL) enables agents to acquire skills directly from expert\ndemonstrations, providing a compelling alternative to reinforcement learning.\nHowever, prior online IL approaches struggle with complex tasks characterized\nby high-dimensional inputs and complex dynamics. In this work, we propose a\nnovel approach to online imitation learning that leverages reward-free world\nmodels. Our method learns environmental dynamics entirely in latent spaces\nwithout reconstruction, enabling efficient and accurate modeling. We adopt the\ninverse soft-Q learning objective, reformulating the optimization process in\nthe Q-policy space to mitigate the instability associated with traditional\noptimization in the reward-policy space. By employing a learned latent dynamics\nmodel and planning for control, our approach consistently achieves stable,\nexpert-level performance in tasks with high-dimensional observation or action\nspaces and intricate dynamics. We evaluate our method on a diverse set of\nbenchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating\nsuperior empirical performance compared to existing approaches."}
{"id": "2412.00259", "pdf": "https://arxiv.org/pdf/2412.00259", "abs": "https://arxiv.org/abs/2412.00259", "authors": ["Yifan Zhu", "Tianyi Xiang", "Aaron Dollar", "Zherong Pan"], "title": "One-Shot Real-to-Sim via End-to-End Differentiable Simulation and Rendering", "categories": ["cs.RO", "cs.CV", "cs.GR"], "comment": "8 pages, 8 figures. Published at IEEE Robotics Automation Letters", "summary": "Identifying predictive world models for robots in novel environments from\nsparse online observations is essential for robot task planning and execution\nin novel environments. However, existing methods that leverage differentiable\nprogramming to identify world models are incapable of jointly optimizing the\ngeometry, appearance, and physical properties of the scene. In this work, we\nintroduce a novel rigid object representation that allows the joint\nidentification of these properties. Our method employs a novel differentiable\npoint-based geometry representation coupled with a grid-based appearance field,\nwhich allows differentiable object collision detection and rendering. Combined\nwith a differentiable physical simulator, we achieve end-to-end optimization of\nworld models, given the sparse visual and tactile observations of a physical\nmotion sequence. Through a series of world model identification tasks in\nsimulated and real environments, we show that our method can learn both\nsimulation- and rendering-ready world models from only one robot action\nsequence. The code and additional videos are available at our project website:\nhttps://tianyi20.github.io/rigid-world-model.github.io/"}
{"id": "2410.24023", "pdf": "https://arxiv.org/pdf/2410.24023", "abs": "https://arxiv.org/abs/2410.24023", "authors": ["Suhan Guo", "Jiahong Deng", "Yi Wei", "Hui Dou", "Furao Shen", "Jian Zhao"], "title": "RAM: Replace Attention with MLP for Efficient Multivariate Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Attention-based architectures have become ubiquitous in time series\nforecasting tasks, including spatio-temporal (STF) and long-term time series\nforecasting (LTSF). Yet, our understanding of the reasons for their\neffectiveness remains limited. In this work, we propose a novel pruning\nstrategy, $\\textbf{R}$eplace $\\textbf{A}$ttention with $\\textbf{M}$LP (RAM),\nthat approximates the attention mechanism using only feedforward layers,\nresidual connections, and layer normalization for temporal and/or spatial\nmodeling in multivariate time series forecasting. Specifically, the Q, K, and V\nprojections, the attention score calculation, the dot-product between the\nattention score and the V, and the final projection can be removed from the\nattention-based networks without significantly degrading the performance, so\nthat the given network remains the top-tier compared to other SOTA methods. RAM\nachieves a $62.579\\%$ reduction in FLOPs for spatio-temporal models with less\nthan $2.5\\%$ performance drop, and a $42.233\\%$ FLOPs reduction for LTSF models\nwith less than $2\\%$ performance drop."}
{"id": "2412.03887", "pdf": "https://arxiv.org/pdf/2412.03887", "abs": "https://arxiv.org/abs/2412.03887", "authors": ["Hyesu Jang", "Wooseong Yang", "Hanguen Kim", "Dongje Lee", "Yongjin Kim", "Jinbum Park", "Minsoo Jeon", "Jaeseong Koh", "Yejin Kang", "Minwoo Jung", "Sangwoo Jung", "Chng Zhen Hao", "Wong Yu Hin", "Chew Yihang", "Ayoung Kim"], "title": "MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous Navigation Application", "categories": ["cs.RO", "cs.CV"], "comment": "10 pages, 9 figures, 3 tables", "summary": "Maritime environmental sensing requires overcoming challenges from complex\nconditions such as harsh weather, platform perturbations, large dynamic\nobjects, and the requirement for long detection ranges. While cameras and LiDAR\nare commonly used in ground vehicle navigation, their applicability in maritime\nsettings is limited by range constraints and hardware maintenance issues. Radar\nsensors, however, offer robust long-range detection capabilities and resilience\nto physical contamination from weather and saline conditions, making it a\npowerful sensor for maritime navigation. Among various radar types, X-band\nradar is widely employed for maritime vessel navigation, providing effective\nlong-range detection essential for situational awareness and collision\navoidance. Nevertheless, it exhibits limitations during berthing operations\nwhere near-field detection is critical. To address this shortcoming, we\nincorporate W-band radar, which excels in detecting nearby objects with a\nhigher update rate. We present a comprehensive maritime sensor dataset\nfeaturing multi-range detection capabilities. This dataset integrates\nshort-range LiDAR data, medium-range W-band radar data, and long-range X-band\nradar data into a unified framework. Additionally, it includes object labels\nfor oceanic object detection usage, derived from radar and stereo camera\nimages. The dataset comprises seven sequences collected from diverse regions\nwith varying levels of \\bl{navigation algorithm} estimation difficulty, ranging\nfrom easy to challenging, and includes common locations suitable for global\nlocalization tasks. This dataset serves as a valuable resource for advancing\nresearch in place recognition, odometry estimation, SLAM, object detection, and\ndynamic object elimination within maritime environments. Dataset can be found\nat https://sites.google.com/view/rpmmoana."}
{"id": "2412.04134", "pdf": "https://arxiv.org/pdf/2412.04134", "abs": "https://arxiv.org/abs/2412.04134", "authors": ["Tao Zhang", "Zhenhai Liu", "Feipeng Qi", "Yongjun Jiao", "Tailin Wu"], "title": "M2PDE: Compositional Generative Multiphysics and Multi-component PDE Simulation", "categories": ["cs.LG"], "comment": "29pages,14 figures", "summary": "Multiphysics simulation, which models the interactions between multiple\nphysical processes, and multi-component simulation of complex structures are\ncritical in fields like nuclear and aerospace engineering. Previous studies use\nnumerical solvers or ML-based surrogate models for these simulations. However,\nmultiphysics simulations typically require integrating multiple specialized\nsolvers-each for a specific physical process-into a coupled program, which\nintroduces significant development challenges. Furthermore, existing numerical\nalgorithms struggle with highly complex large-scale structures in\nmulti-component simulations. Here we propose compositional Multiphysics and\nMulti-component PDE Simulation with Diffusion models (M2PDE) to overcome these\nchallenges. During diffusion-based training, M2PDE learns energy functions\nmodeling the conditional probability of one physical process/component\nconditioned on other processes/components. In inference, M2PDE generates\ncoupled multiphysics and multi-component solutions by sampling from the joint\nprobability distribution. We evaluate M2PDE on two multiphysics\ntasks-reaction-diffusion and nuclear thermal coupling-where it achieves more\naccurate predictions than surrogate models in challenging scenarios. We then\napply it to a multi-component prismatic fuel element problem, demonstrating\nthat M2PDE scales from single-component training to a 64-component structure\nand outperforms existing domain-decomposition and graph-based approaches. The\ncode is available at https://github.com/AI4Science-WestlakeU/M2PDE."}
{"id": "2412.07487", "pdf": "https://arxiv.org/pdf/2412.07487", "abs": "https://arxiv.org/abs/2412.07487", "authors": ["Yik Lung Pang", "Alessio Xompero", "Changjae Oh", "Andrea Cavallaro"], "title": "Stereo Hand-Object Reconstruction for Human-to-Robot Handover", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 9 figures, 1 table. (Website:\n  https://qm-ipalab.github.io/StereoHO/)", "summary": "Jointly estimating hand and object shape facilitates the grasping task in\nhuman-to-robot handovers. However, relying on hand-crafted prior knowledge\nabout the geometric structure of the object fails when generalising to unseen\nobjects, and depth sensors fail to detect transparent objects such as drinking\nglasses. In this work, we propose a stereo-based method for hand-object\nreconstruction that combines single-view reconstructions probabilistically to\nform a coherent stereo reconstruction. We learn 3D shape priors from a large\nsynthetic hand-object dataset to ensure that our method is generalisable, and\nuse RGB inputs to better capture transparent objects. We show that our method\nreduces the object Chamfer distance compared to existing RGB based hand-object\nreconstruction methods on single view and stereo settings. We process the\nreconstructed hand-object shape with a projection-based outlier removal step\nand use the output to guide a human-to-robot handover pipeline with\nwide-baseline stereo RGB cameras. Our hand-object reconstruction enables a\nrobot to successfully receive a diverse range of household objects from the\nhuman."}
{"id": "2412.08501", "pdf": "https://arxiv.org/pdf/2412.08501", "abs": "https://arxiv.org/abs/2412.08501", "authors": ["Yuang Zhang", "Liping Wang", "Yihong Huang", "Yuanxing Zheng", "Fan Zhang", "Xuemin Lin"], "title": "GradStop: Exploring Training Dynamics in Unsupervised Outlier Detection through Gradient", "categories": ["cs.LG"], "comment": null, "summary": "Unsupervised Outlier Detection (UOD) is a critical task in data mining and\nmachine learning, aiming to identify instances that significantly deviate from\nthe majority. Without any label, deep UOD methods struggle with the\nmisalignment between the model's direct optimization goal and the final\nperformance goal of Outlier Detection (OD) task. Through the perspective of\ntraining dynamics, this paper proposes an early stopping algorithm to optimize\nthe training of deep UOD models, ensuring they perform optimally in OD rather\nthan overfitting the entire contaminated dataset.\n  Inspired by UOD mechanism and inlier priority phenomenon, where intuitively\nmodels fit inliers more quickly than outliers, we propose GradStop, a\nsampling-based label-free algorithm to estimate model's real-time performance\nduring training. First, a sampling method generates two sets: one likely\ncontaining more outliers and the other more inliers, then a metric based on\ngradient cohesion is applied to probe into current training dynamics, which\nreflects model's performance on OD task.\n  Experimental results on 4 deep UOD algorithms and 47 real-world datasets and\ntheoretical proofs demonstrate the effectiveness of our proposed early stopping\nalgorithm in enhancing the performance of deep UOD models. Auto Encoder (AE)\nenhanced by GradStop achieves better performance than itself, other SOTA UOD\nmethods, and even ensemble AEs. Our method provides a robust and effective\nsolution to the problem of performance degradation during training, enabling\ndeep UOD models to achieve better potential in anomaly detection tasks."}
{"id": "2504.06304", "pdf": "https://arxiv.org/pdf/2504.06304", "abs": "https://arxiv.org/abs/2504.06304", "authors": ["Matvei Popov", "Aymen Kallala", "Anirudha Ramesh", "Narimane Hennouni", "Shivesh Khaitan", "Rick Gentry", "Alain-Sam Cohen"], "title": "Leveraging State Space Models in Long Range Genomics", "categories": ["q-bio.GN", "cs.CV", "cs.LG"], "comment": "Accepted at ICLR 2025 (Spotlight @ LMRL) - Project page:\n  https://anirudharamesh.github.io/iclr-long-range-genomics/", "summary": "Long-range dependencies are critical for understanding genomic structure and\nfunction, yet most conventional methods struggle with them. Widely adopted\ntransformer-based models, while excelling at short-context tasks, are limited\nby the attention module's quadratic computational complexity and inability to\nextrapolate to sequences longer than those seen in training. In this work, we\nexplore State Space Models (SSMs) as a promising alternative by benchmarking\ntwo SSM-inspired architectures, Caduceus and Hawk, on long-range genomics\nmodeling tasks under conditions parallel to a 50M parameter transformer\nbaseline. We discover that SSMs match transformer performance and exhibit\nimpressive zero-shot extrapolation across multiple tasks, handling contexts 10\nto 100 times longer than those seen during training, indicating more\ngeneralizable representations better suited for modeling the long and complex\nhuman genome. Moreover, we demonstrate that these models can efficiently\nprocess sequences of 1M tokens on a single GPU, allowing for modeling entire\ngenomic regions at once, even in labs with limited compute. Our findings\nestablish SSMs as efficient and scalable for long-context genomic analysis."}
{"id": "2501.08425", "pdf": "https://arxiv.org/pdf/2501.08425", "abs": "https://arxiv.org/abs/2501.08425", "authors": ["Davide Barbieri", "Matteo Bonforte", "Peio Ibarrondo"], "title": "Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning processes", "categories": ["cs.LG", "math.AP", "math.PR", "35Q68, 68T07, 35K65, 35B40, 60J60, 35J70, 35K15"], "comment": null, "summary": "In this paper we analyze the behaviour of the stochastic gradient descent\n(SGD), a widely used method in supervised learning for optimizing neural\nnetwork weights via a minimization of non-convex loss functions. Since the\npioneering work of E, Li and Tai (2017), the underlying structure of such\nprocesses can be understood via parabolic PDEs of Fokker-Planck type, which are\nat the core of our analysis. Even if Fokker-Planck equations have a long\nhistory and a extensive literature, almost nothing is known when the potential\nis non-convex or when the diffusion matrix is degenerate, and this is the main\ndifficulty that we face in our analysis.\n  We identify two different regimes: in the initial phase of SGD, the loss\nfunction drives the weights to concentrate around the nearest local minimum. We\nrefer to this phase as the drift regime and we provide quantitative estimates\non this concentration phenomenon. Next, we introduce the diffusion regime,\nwhere stochastic fluctuations help the learning process to escape suboptimal\nlocal minima. We analyze the Mean Exit Time (MET) and prove upper and lower\nbounds of the MET. Finally, we address the asymptotic convergence of SGD, for a\nnon-convex cost function and a degenerate diffusion matrix, that do not allow\nto use the standard approaches, and require new techniques. For this purpose,\nwe exploit two different methods: duality and entropy methods.\n  We provide new results about the dynamics and effectiveness of SGD, offering\na deep connection between stochastic optimization and PDE theory, and some\nanswers and insights to basic questions in the Machine Learning processes: How\nlong does SGD take to escape from a bad minimum? Do neural network parameters\nconverge using SGD? How do parameters evolve in the first stage of training\nwith SGD?"}
{"id": "2504.14257", "pdf": "https://arxiv.org/pdf/2504.14257", "abs": "https://arxiv.org/abs/2504.14257", "authors": ["Yilin Liu", "Duoteng Xu", "Xingyao Yu", "Xiang Xu", "Daniel Cohen-Or", "Hao Zhang", "Hui Huang"], "title": "HoLa: B-Rep Generation using a Holistic Latent Representation", "categories": ["cs.GR", "cs.CV"], "comment": "ACM TOG and SIGGRAPH 2025 (Patent Protected); Project page:\n  https://vcc.tech/research/2025/HolaBrep; Demo page:\n  https://huggingface.co/spaces/YuXingyao/HoLa-BRep", "summary": "We introduce a novel representation for learning and generating\nComputer-Aided Design (CAD) models in the form of $\\textit{boundary\nrepresentations}$ (B-Reps). Our representation unifies the continuous geometric\nproperties of B-Rep primitives in different orders (e.g., surfaces and curves)\nand their discrete topological relations in a $\\textit{holistic latent}$ (HoLa)\nspace. This is based on the simple observation that the topological connection\nbetween two surfaces is intrinsically tied to the geometry of their\nintersecting curve. Such a prior allows us to reformulate topology learning in\nB-Reps as a geometric reconstruction problem in Euclidean space. Specifically,\nwe eliminate the presence of curves, vertices, and all the topological\nconnections in the latent space by learning to distinguish and derive curve\ngeometries from a pair of surface primitives via a neural intersection network.\nTo this end, our holistic latent space is only defined on surfaces but encodes\na full B-Rep model, including the geometry of surfaces, curves, vertices, and\ntheir topological relations. Our compact and holistic latent space facilitates\nthe design of a first diffusion-based generator to take on a large variety of\ninputs including point clouds, single/multi-view images, 2D sketches, and text\nprompts. Our method significantly reduces ambiguities, redundancies, and\nincoherences among the generated B-Rep primitives, as well as training\ncomplexities inherent in prior multi-step B-Rep learning pipelines, while\nachieving greatly improved validity rate over current state of the art: 82% vs.\n$\\approx$50%."}
{"id": "2501.15458", "pdf": "https://arxiv.org/pdf/2501.15458", "abs": "https://arxiv.org/abs/2501.15458", "authors": ["Cen-You Li", "Marc Toussaint", "Barbara Rakitsch", "Christoph Zimmer"], "title": "Amortized Safe Active Learning for Real-Time Data Acquisition: Pretrained Neural Policies from Simulated Nonparametric Functions", "categories": ["cs.LG"], "comment": "Part of the content published earlier at arXiv:2407.17992", "summary": "Safe active learning (AL) is a sequential scheme for learning unknown systems\nwhile respecting safety constraints during data acquisition. Existing methods\noften rely on Gaussian processes (GPs) to model the task and safety\nconstraints, requiring repeated GP updates and constrained acquisition\noptimization-incurring in significant computations which are challenging for\nreal-time decision-making. We propose an amortized safe AL framework that\nreplaces expensive online computations with a pretrained neural policy.\nInspired by recent advances in amortized Bayesian experimental design, we turn\nGPs into a pretraining simulator. We train our policy prior to the AL\ndeployment on simulated nonparametric functions, using Fourier feature-based GP\nsampling and a differentiable, safety-aware acquisition objective. At\ndeployment, our policy selects safe and informative queries via a single\nforward pass, eliminating the need for GP inference or constrained\noptimization. This leads to substantial speed improvements while preserving\nsafety and learning quality. Our framework is modular and can be adapted to\nunconstrained, time-sensitive AL tasks by omitting the safety requirement."}
{"id": "2504.19200", "pdf": "https://arxiv.org/pdf/2504.19200", "abs": "https://arxiv.org/abs/2504.19200", "authors": ["Tristan Manchester", "Adam Anders", "Julio Spadotto", "Hannah Eccleston", "William Beavan", "Hugues Arcis", "Brian J. Connolly"], "title": "Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "comment": null, "summary": "In situ synchrotron X-ray computed tomography enables dynamic material\nstudies, but automated segmentation remains challenging due to complex imaging\nartefacts and limited training data. We present a methodology for deep\nlearning-based segmentation by transforming high-quality ex situ laboratory\ndata to train models for binary segmentation of in situ synchrotron data,\ndemonstrated through copper oxide dissolution studies. Using a modified\nSegFormer architecture, our approach achieves high segmentation performance on\nunseen data while reducing processing time from hours to seconds per 3D\ndataset. The method maintains consistent performance over significant\nmorphological changes during experiments, despite training only on static\nspecimens. This methodology can be readily applied to diverse materials\nsystems, accelerating the analysis of time-resolved tomographic data across\nscientific disciplines."}
{"id": "2501.15461", "pdf": "https://arxiv.org/pdf/2501.15461", "abs": "https://arxiv.org/abs/2501.15461", "authors": ["Xin He", "Yili Wang", "Wenqi Fan", "Xu Shen", "Xin Juan", "Rui Miao", "Xin Wang"], "title": "Mamba-Based Graph Convolutional Networks: Tackling Over-smoothing with Selective State Space", "categories": ["cs.LG"], "comment": "11 pages, 4 figures", "summary": "Graph Neural Networks (GNNs) have shown great success in various graph-based\nlearning tasks. However, it often faces the issue of over-smoothing as the\nmodel depth increases, which causes all node representations to converge to a\nsingle value and become indistinguishable. This issue stems from the inherent\nlimitations of GNNs, which struggle to distinguish the importance of\ninformation from different neighborhoods. In this paper, we introduce MbaGCN, a\nnovel graph convolutional architecture that draws inspiration from the Mamba\nparadigm-originally designed for sequence modeling. MbaGCN presents a new\nbackbone for GNNs, consisting of three key components: the Message Aggregation\nLayer, the Selective State Space Transition Layer, and the Node State\nPrediction Layer. These components work in tandem to adaptively aggregate\nneighborhood information, providing greater flexibility and scalability for\ndeep GNN models. While MbaGCN may not consistently outperform all existing\nmethods on each dataset, it provides a foundational framework that demonstrates\nthe effective integration of the Mamba paradigm into graph representation\nlearning. Through extensive experiments on benchmark datasets, we demonstrate\nthat MbaGCN paves the way for future advancements in graph neural network\nresearch."}
{"id": "2505.01932", "pdf": "https://arxiv.org/pdf/2505.01932", "abs": "https://arxiv.org/abs/2505.01932", "authors": ["Xinmu Wang", "Xiang Gao", "Xiyun Song", "Heather Yu", "Zongfang Lin", "Liang Peng", "Xianfeng Gu"], "title": "OT-Talk: Animating 3D Talking Head with Optimal Transportation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Animating 3D head meshes using audio inputs has significant applications in\nAR/VR, gaming, and entertainment through 3D avatars. However, bridging the\nmodality gap between speech signals and facial dynamics remains a challenge,\noften resulting in incorrect lip syncing and unnatural facial movements. To\naddress this, we propose OT-Talk, the first approach to leverage optimal\ntransportation to optimize the learning model in talking head animation.\nBuilding on existing learning frameworks, we utilize a pre-trained Hubert model\nto extract audio features and a transformer model to process temporal\nsequences. Unlike previous methods that focus solely on vertex coordinates or\ndisplacements, we introduce Chebyshev Graph Convolution to extract geometric\nfeatures from triangulated meshes. To measure mesh dissimilarities, we go\nbeyond traditional mesh reconstruction errors and velocity differences between\nadjacent frames. Instead, we represent meshes as probability measures and\napproximate their surfaces. This allows us to leverage the sliced Wasserstein\ndistance for modeling mesh variations. This approach facilitates the learning\nof smooth and accurate facial motions, resulting in coherent and natural facial\nanimations. Our experiments on two public audio-mesh datasets demonstrate that\nour method outperforms state-of-the-art techniques both quantitatively and\nqualitatively in terms of mesh reconstruction accuracy and temporal alignment.\nIn addition, we conducted a user perception study with 20 volunteers to further\nassess the effectiveness of our approach."}
{"id": "2502.08227", "pdf": "https://arxiv.org/pdf/2502.08227", "abs": "https://arxiv.org/abs/2502.08227", "authors": ["Suqin Yuan", "Lei Feng", "Bo Han", "Tongliang Liu"], "title": "Enhancing Sample Selection Against Label Noise by Cutting Mislabeled Easy Examples", "categories": ["cs.LG"], "comment": null, "summary": "Sample selection is a prevalent approach in learning with noisy labels,\naiming to identify confident samples for training. Although existing sample\nselection methods have achieved decent results by reducing the noise rate of\nthe selected subset, they often overlook that not all mislabeled examples harm\nthe model's performance equally. In this paper, we demonstrate that mislabeled\nexamples correctly predicted by the model early in the training process are\nparticularly harmful to model performance. We refer to these examples as\nMislabeled Easy Examples (MEEs). To address this, we propose Early Cutting,\nwhich introduces a recalibration step that employs the model's later training\nstate to re-select the confident subset identified early in training, thereby\navoiding misleading confidence from early learning and effectively filtering\nout MEEs. Experiments on the CIFAR, WebVision, and full ImageNet-1k datasets\ndemonstrate that our method effectively improves sample selection and model\nperformance by reducing MEEs."}
{"id": "2505.02350", "pdf": "https://arxiv.org/pdf/2505.02350", "abs": "https://arxiv.org/abs/2505.02350", "authors": ["Bobo Lian", "Dandan Wang", "Chenjian Wu", "Minxin Chen"], "title": "Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Point cloud surface representation is a fundamental problem in computer\ngraphics and vision. This paper presents a machine learning approach for\napproximating the signed distance function (SDF) of a point cloud using a\nsparse ellipsoidal radial basis function network, enabling a compact and\naccurate surface representation. Given the SDF values defined on the grid\npoints constructed from the point cloud, our method approximates the SDF\naccurately with as few ellipsoidal radial basis functions (ERBFs) as possible,\ni.e., represents the SDF of a point cloud by sparse ERBFs. To balance sparsity\nand approximation precision, a dynamic multi-objective optimization strategy is\nintroduced, which adaptively adds the regularization terms and jointly\noptimizes the weights, centers, shapes, and orientations of ERBFs. To improve\ncomputational efficiency, a nearest-neighbor-based data structure is employed,\nrestricting function calculations to points near each Gaussian kernel center.\nThe computations for each kernel are further parallelized on CUDA, which\nsignificantly improves the optimization speed. Additionally, a hierarchical\noctree-based refinement strategy is designed for training. Specifically, the\ninitialization and optimization of network parameters are conducted using\ncoarse grid points in the octree lattice structure. Subsequently, fine lattice\npoints are progressively incorporated to accelerate model convergence and\nenhance training efficiency. Extensive experiments on multiple benchmark\ndatasets demonstrate that our method outperforms previous sparse representation\napproaches in terms of accuracy, robustness, and computational efficiency. The\ncorresponding executable program is publicly available at\nhttps://github.com/lianbobo/SE-RBFNet.git."}
{"id": "2502.08231", "pdf": "https://arxiv.org/pdf/2502.08231", "abs": "https://arxiv.org/abs/2502.08231", "authors": ["Evgeniia Tokarchuk", "Hua Chang Bakker", "Vlad Niculae"], "title": "Keep your distance: learning dispersed embeddings on $\\mathbb{S}_m$", "categories": ["cs.LG"], "comment": null, "summary": "Learning well-separated features in high-dimensional spaces, such as text or\nimage embeddings, is crucial for many machine learning applications. Achieving\nsuch separation can be effectively accomplished through the dispersion of\nembeddings, where unrelated vectors are pushed apart as much as possible. By\nconstraining features to be on a hypersphere, we can connect dispersion to\nwell-studied problems in mathematics and physics, where optimal solutions are\nknown for limited low-dimensional cases. However, in representation learning we\ntypically deal with a large number of features in high-dimensional space, and\nmoreover, dispersion is usually traded off with some other task-oriented\ntraining objective, making existing theoretical and numerical solutions\ninapplicable. Therefore, it is common to rely on gradient-based methods to\nencourage dispersion, usually by minimizing some function of the pairwise\ndistances. In this work, we first give an overview of existing methods from\ndisconnected literature, making new connections and highlighting similarities.\nNext, we introduce some new angles. We propose to reinterpret pairwise\ndispersion using a maximum mean discrepancy (MMD) motivation. We then propose\nan online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an\neffective alternative regularizer for dispersion on generic domains. Finally,\nwe derive a novel dispersion method that directly exploits properties of the\nhypersphere. Our experiments show the importance of dispersion in image\nclassification and natural language processing tasks, and how algorithms\nexhibit different trade-offs in different regimes."}
{"id": "2505.05132", "pdf": "https://arxiv.org/pdf/2505.05132", "abs": "https://arxiv.org/abs/2505.05132", "authors": ["Luis Alvarez", "Jean-Michel Morel"], "title": "An Active Contour Model for Silhouette Vectorization using Bézier Curves", "categories": ["cs.GR", "cs.CV", "math.FA"], "comment": "14 pages, 5 figures and 1 table", "summary": "In this paper, we propose an active contour model for silhouette\nvectorization using cubic B\\'ezier curves. Among the end points of the B\\'ezier\ncurves, we distinguish between corner and regular points where the orientation\nof the tangent vector is prescribed. By minimizing the distance of the B\\'ezier\ncurves to the silhouette boundary, the active contour model optimizes the\nlocation of the B\\'ezier curves end points, the orientation of the tangent\nvectors in the regular points, and the estimation of the B\\'ezier curve\nparameters. This active contour model can use the silhouette vectorization\nobtained by any method as an initial guess. The proposed method significantly\nreduces the average distance between the silhouette boundary and its\nvectorization obtained by the world-class graphic software Inkscape, Adobe\nIllustrator, and a curvature-based vectorization method, which we introduce for\ncomparison. Our method also allows us to impose additional regularity on the\nB\\'ezier curves by reducing their lengths."}
{"id": "2503.04318", "pdf": "https://arxiv.org/pdf/2503.04318", "abs": "https://arxiv.org/abs/2503.04318", "authors": ["Tim Maurer", "Abdulrahman Mohamed Selim", "Hasan Md Tusfiqur Alam", "Matthias Eiletz", "Michael Barz", "Daniel Sonntag"], "title": "InFL-UX: A Toolkit for Web-Based Interactive Federated Learning", "categories": ["cs.LG", "cs.HC"], "comment": "Accepted in the 17th ACM SIGCHI Symposium on Engineering Interactive\n  Computing Systems (EICS 2025)", "summary": "This paper presents InFL-UX, an interactive, proof-of-concept browser-based\nFederated Learning (FL) toolkit designed to integrate user contributions\nseamlessly into the machine learning (ML) workflow. InFL-UX enables users\nacross multiple devices to upload datasets, define classes, and collaboratively\ntrain classification models directly in the browser using modern web\ntechnologies. Unlike traditional FL toolkits, which often focus on backend\nsimulations, InFL-UX provides a simple user interface for researchers to\nexplore how users interact with and contribute to FL systems in real-world,\ninteractive settings. By prioritising usability and decentralised model\ntraining, InFL-UX bridges the gap between FL and Interactive Machine Learning\n(IML), empowering non-technical users to actively participate in ML\nclassification tasks."}
{"id": "2505.05592", "pdf": "https://arxiv.org/pdf/2505.05592", "abs": "https://arxiv.org/abs/2505.05592", "authors": ["Noriaki Hirose", "Lydia Ignatova", "Kyle Stachowicz", "Catherine Glossop", "Sergey Levine", "Dhruv Shah"], "title": "Learning to Drive Anywhere with Model-Based Reannotation", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "19 pages, 11 figures, 8 tables", "summary": "Developing broadly generalizable visual navigation policies for robots is a\nsignificant challenge, primarily constrained by the availability of\nlarge-scale, diverse training data. While curated datasets collected by\nresearchers offer high quality, their limited size restricts policy\ngeneralization. To overcome this, we explore leveraging abundant, passively\ncollected data sources, including large volumes of crowd-sourced teleoperation\ndata and unlabeled YouTube videos, despite their potential for lower quality or\nmissing action labels. We propose Model-Based ReAnnotation (MBRA), a framework\nthat utilizes a learned short-horizon, model-based expert model to relabel or\ngenerate high-quality actions for these passive datasets. This relabeled data\nis then distilled into LogoNav, a long-horizon navigation policy conditioned on\nvisual goals or GPS waypoints. We demonstrate that LogoNav, trained using\nMBRA-processed data, achieves state-of-the-art performance, enabling robust\nnavigation over distances exceeding 300 meters in previously unseen indoor and\noutdoor environments. Our extensive real-world evaluations, conducted across a\nfleet of robots (including quadrupeds) in six cities on three continents,\nvalidate the policy's ability to generalize and navigate effectively even\namidst pedestrians in crowded settings."}
{"id": "2503.14004", "pdf": "https://arxiv.org/pdf/2503.14004", "abs": "https://arxiv.org/abs/2503.14004", "authors": ["Eyal Marantz", "Ori Plonsky"], "title": "Predicting Human Choice Between Textually Described Lotteries", "categories": ["cs.LG"], "comment": null, "summary": "Predicting human decision-making under risk and uncertainty is a\nlong-standing challenge in cognitive science, economics, and AI. While prior\nresearch has focused on numerically described lotteries, real-world decisions\noften rely on textual descriptions. This study conducts the first large-scale\nexploration of human decision-making in such tasks using a large dataset of\none-shot binary choices between textually described lotteries. We evaluate\nmultiple computational approaches, including fine-tuning Large Language Models\n(LLMs), leveraging embeddings, and integrating behavioral theories of choice\nunder risk. Our results show that fine-tuned LLMs, specifically GPT-4o,\noutperform hybrid models that incorporate behavioral theory, challenging\nestablished methods in numerical settings. These findings highlight fundamental\ndifferences in how textual and numerical information influence decision-making\nand underscore the need for new modeling strategies to bridge this gap."}
{"id": "2503.19300", "pdf": "https://arxiv.org/pdf/2503.19300", "abs": "https://arxiv.org/abs/2503.19300", "authors": ["Xiangzhe Kong", "Zishen Zhang", "Ziting Zhang", "Rui Jiao", "Jianzhu Ma", "Wenbing Huang", "Kai Liu", "Yang Liu"], "title": "UniMoMo: Unified Generative Modeling of 3D Molecules for De Novo Binder Design", "categories": ["cs.LG", "q-bio.BM"], "comment": "Accepted to ICML 2025", "summary": "The design of target-specific molecules such as small molecules, peptides,\nand antibodies is vital for biological research and drug discovery. Existing\ngenerative methods are restricted to single-domain molecules, failing to\naddress versatile therapeutic needs or utilize cross-domain transferability to\nenhance model performance. In this paper, we introduce Unified generative\nModeling of 3D Molecules (UniMoMo), the first framework capable of designing\nbinders of multiple molecular domains using a single model. In particular,\nUniMoMo unifies the representations of different molecules as graphs of blocks,\nwhere each block corresponds to either a standard amino acid or a molecular\nfragment. Subsequently, UniMoMo utilizes a geometric latent diffusion model for\n3D molecular generation, featuring an iterative full-atom autoencoder to\ncompress blocks into latent space points, followed by an E(3)-equivariant\ndiffusion process. Extensive benchmarks across peptides, antibodies, and small\nmolecules demonstrate the superiority of our unified framework over existing\ndomain-specific models, highlighting the benefits of multi-domain training."}
{"id": "2503.20697", "pdf": "https://arxiv.org/pdf/2503.20697", "abs": "https://arxiv.org/abs/2503.20697", "authors": ["Yankai Chen", "Taotao Wang", "Yixiang Fang", "Yunyu Xiao"], "title": "Semi-supervised Node Importance Estimation with Informative Distribution Modeling for Uncertainty Regularization", "categories": ["cs.LG"], "comment": "Accepted by WWW'25. A few typos corrected", "summary": "Node importance estimation, a classical problem in network analysis,\nunderpins various web applications. Previous methods either exploit intrinsic\ntopological characteristics, e.g., graph centrality, or leverage additional\ninformation, e.g., data heterogeneity, for node feature enhancement. However,\nthese methods follow the supervised learning setting, overlooking the fact that\nground-truth node-importance data are usually partially labeled in practice. In\nthis work, we propose the first semi-supervised node importance estimation\nframework, i.e., EASING, to improve learning quality for unlabeled data in\nheterogeneous graphs. Different from previous approaches, EASING explicitly\ncaptures uncertainty to reflect the confidence of model predictions. To jointly\nestimate the importance values and uncertainties, EASING incorporates DJE, a\ndeep encoder-decoder neural architecture. DJE introduces distribution modeling\nfor graph nodes, where the distribution representations derive both importance\nand uncertainty estimates. Additionally, DJE facilitates effective pseudo-label\ngeneration for the unlabeled data to enrich the training samples. Based on\nlabeled and pseudo-labeled data, EASING develops effective semi-supervised\nheteroscedastic learning with varying node uncertainty regularization.\nExtensive experiments on three real-world datasets highlight the superior\nperformance of EASING compared to competing methods. Codes are available via\nhttps://github.com/yankai-chen/EASING."}
{"id": "2504.04799", "pdf": "https://arxiv.org/pdf/2504.04799", "abs": "https://arxiv.org/abs/2504.04799", "authors": ["Maosheng Yang"], "title": "Topological Schrödinger Bridge Matching", "categories": ["cs.LG", "stat.ML"], "comment": "ICLR 2025 Spotlight, 42 pages", "summary": "Given two boundary distributions, the Schr\\\"odinger Bridge (SB) problem seeks\nthe ``most likely`` random evolution between them with respect to a reference\nprocess. It has revealed rich connections to recent machine learning methods\nfor generative modeling and distribution matching. While these methods perform\nwell in Euclidean domains, they are not directly applicable to topological\ndomains such as graphs and simplicial complexes, which are crucial for data\ndefined over network entities, such as node signals and edge flows. In this\nwork, we propose the Topological Schr\\\"odinger Bridge problem (TSBP) for\nmatching signal distributions on a topological domain. We set the reference\nprocess to follow some linear tractable topology-aware stochastic dynamics such\nas topological heat diffusion. For the case of Gaussian boundary distributions,\nwe derive a closed-form topological SB (TSB) in terms of its time-marginal and\nstochastic differential. In the general case, leveraging the well-known result,\nwe show that the optimal process follows the forward-backward topological\ndynamics governed by some unknowns. Building on these results, we develop\nTSB-based models for matching topological signals by parameterizing the\nunknowns in the optimal process as (topological) neural networks and learning\nthem through likelihood training. We validate the theoretical results and\ndemonstrate the practical applications of TSB-based models on both synthetic\nand real-world networks, emphasizing the role of topology. Additionally, we\ndiscuss the connections of TSB-based models to other emerging models, and\noutline future directions for topological signal matching."}
{"id": "2504.08377", "pdf": "https://arxiv.org/pdf/2504.08377", "abs": "https://arxiv.org/abs/2504.08377", "authors": ["Avrim Blum", "Steve Hanneke", "Chirag Pabbaraju", "Donya Saless"], "title": "Proofs as Explanations: Short Certificates for Reliable Predictions", "categories": ["cs.LG", "stat.ML"], "comment": "Updated bibliography", "summary": "We consider a model for explainable AI in which an explanation for a\nprediction $h(x)=y$ consists of a subset $S'$ of the training data (if it\nexists) such that all classifiers $h' \\in H$ that make at most $b$ mistakes on\n$S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has\nlabel $y$ under the assumption that (1) the target function $h^\\star$ belongs\nto $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example,\nif $b=0$ and $H$ is the family of linear classifiers in $\\mathbb{R}^d$, and if\n$x$ lies inside the convex hull of the positive data points in $S$ (and hence\nevery consistent linear classifier labels $x$ as positive), then\nCarath\\'eodory's theorem states that $x$ lies inside the convex hull of $d+1$\nof those points. So, a set $S'$ of size $d+1$ could be released as an\nexplanation for a positive prediction, and would serve as a short proof of\ncorrectness of the prediction under the assumption of realizability.\n  In this work, we consider this problem more generally, for general hypothesis\nclasses $H$ and general values $b\\geq 0$. We define the notion of the robust\nhollow star number of $H$ (which generalizes the standard hollow star number),\nand show that it precisely characterizes the worst-case size of the smallest\ncertificate achievable, and analyze its size for natural classes. We also\nconsider worst-case distributional bounds on certificate size, as well as\ndistribution-dependent bounds that we show tightly control the sample size\nneeded to get a certificate for any given test example. In particular, we\ndefine a notion of the certificate coefficient $\\varepsilon_x$ of an example\n$x$ with respect to a data distribution $D$ and target function $h^\\star$, and\nprove matching upper and lower bounds on sample size as a function of\n$\\varepsilon_x$, $b$, and the VC dimension $d$ of $H$."}
{"id": "2504.13034", "pdf": "https://arxiv.org/pdf/2504.13034", "abs": "https://arxiv.org/abs/2504.13034", "authors": ["Yangxin Fan", "Haolai Che", "Yinghui Wu"], "title": "Inference-friendly Graph Compression for Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated promising performance in graph\nanalysis. Nevertheless, the inference process of GNNs remains costly, hindering\ntheir applications for large graphs. This paper proposes inference-friendly\ngraph compression (IFGC), a graph compression scheme to accelerate GNNs\ninference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed\ngraph $G_c$, to best preserve the inference results of $M$ over $G$, such that\nthe result can be directly inferred by accessing $G_c$ with no or little\ndecompression cost. (1) We characterize IFGC with a class of inference\nequivalence relation. The relation captures the node pairs in $G$ that are not\ndistinguishable for GNN inference. (2) We introduce three practical\nspecifications of IFGC for representative GNNs: structural preserving\ncompression (SPGC), which computes $G_c$ that can be directly processed by GNN\ninference without decompression; ($\\alpha$, $r$)-compression, that allows for a\nconfigurable trade-off between compression ratio and inference quality, and\nanchored compression that preserves inference results for specific nodes of\ninterest. For each scheme, we introduce compression and inference algorithms\nwith guarantees of efficiency and quality of the inferred results. We conduct\nextensive experiments on diverse sets of large-scale graphs, which verifies the\neffectiveness and efficiency of our graph compression approaches."}
{"id": "2504.16506", "pdf": "https://arxiv.org/pdf/2504.16506", "abs": "https://arxiv.org/abs/2504.16506", "authors": ["Ruxue Shi", "Yili Wang", "Mengnan Du", "Xu Shen", "Xin Wang"], "title": "A Comprehensive Survey of Synthetic Tabular Data Generation", "categories": ["cs.LG"], "comment": null, "summary": "Tabular data remains one of the most prevalent and critical data formats\nacross diverse real-world applications. However, its effective use in machine\nlearning (ML) is often constrained by challenges such as data scarcity, privacy\nconcerns, and class imbalance. Synthetic data generation has emerged as a\npromising solution, leveraging generative models to learn the distribution of\nreal datasets and produce high-fidelity, privacy-preserving samples. Various\ngenerative paradigms have been explored, including energy-based models (EBMs),\nvariational autoencoders (VAEs), generative adversarial networks (GANs), large\nlanguage models (LLMs), and diffusion models. While several surveys have\ninvestigated synthetic tabular data generation, most focus on narrow subdomains\nor specific generative methods, such as GANs, diffusion models, or\nprivacy-preserving techniques. This limited scope often results in fragmented\ninsights, lacking a comprehensive synthesis that bridges diverse approaches. In\nparticular, recent advances driven by LLMs and diffusion-based models remain\nunderexplored. This gap hinders a holistic understanding of the field`s\nevolution, methodological interplay, and open challenges. To address this, our\nsurvey provides a unified and systematic review of synthetic tabular data\ngeneration. Our contributions are threefold: (1) we propose a comprehensive\ntaxonomy that organizes existing methods into traditional approaches,\ndiffusion-based methods, and LLM-based models, and provide an in-depth\ncomparative analysis; (2) we detail the complete pipeline for synthetic tabular\ndata generation, including data synthesis, post-processing, and evaluation; (3)\nwe identify major challenges, explore real-world applications, and outline open\nresearch questions and future directions to guide future work in this rapidly\nevolving area."}
{"id": "2504.20078", "pdf": "https://arxiv.org/pdf/2504.20078", "abs": "https://arxiv.org/abs/2504.20078", "authors": ["Kalyan Cherukuri", "Aarav Lala"], "title": "Low-Rank Matrix Approximation for Neural Network Compression", "categories": ["cs.LG", "cs.CC"], "comment": null, "summary": "Deep Neural Networks (DNNs) have encountered an emerging deployment challenge\ndue to large and expensive memory and computation requirements. In this paper,\nwe present a new Adaptive-Rank Singular Value Decomposition (ARSVD) method that\napproximates the optimal rank for compressing weight matrices in neural\nnetworks using spectral entropy. Unlike conventional SVD-based methods that\napply a fixed-rank truncation across all layers, ARSVD uses an adaptive\nselection of the rank per layer through the entropy distribution of its\nsingular values. This approach ensures that each layer will retain a certain\namount of its informational content, thereby reducing redundancy. Our method\nenables efficient, layer-wise compression, yielding improved performance with\nreduced space and time complexity compared to static-rank reduction techniques."}
{"id": "2505.00941", "pdf": "https://arxiv.org/pdf/2505.00941", "abs": "https://arxiv.org/abs/2505.00941", "authors": ["Wenxin Zhang", "Ding Xu", "Guangzhen Yao", "Xiaojian Lin", "Renxiang Guan", "Chengze Du", "Renda Han", "Xi Xuan", "Cuicui Luo"], "title": "FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection", "categories": ["cs.LG"], "comment": null, "summary": "Time series anomaly detection is critical for system monitoring and risk\nidentification, across various domains, such as finance and healthcare.\nHowever, for most reconstruction-based approaches, detecting anomalies remains\na challenge due to the complexity of sequential patterns in time series data.\nOn the one hand, reconstruction-based techniques are susceptible to\ncomputational deviation stemming from anomalies, which can lead to impure\nrepresentations of normal sequence patterns. On the other hand, they often\nfocus on the time-domain dependencies of time series, while ignoring the\nalignment of frequency information beyond the time domain. To address these\nchallenges, we propose a novel Frequency-augmented Convolutional Transformer\n(FreCT). FreCT utilizes patch operations to generate contrastive views and\nemploys an improved Transformer architecture integrated with a convolution\nmodule to capture long-term dependencies while preserving local topology\ninformation. The introduced frequency analysis based on Fourier transformation\ncould enhance the model's ability to capture crucial characteristics beyond the\ntime domain. To protect the training quality from anomalies and improve the\nrobustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and\nabsolute error to optimize consistency information in both time and frequency\ndomains. Extensive experiments on four public datasets demonstrate that FreCT\noutperforms existing methods in identifying anomalies."}
{"id": "2505.02222", "pdf": "https://arxiv.org/pdf/2505.02222", "abs": "https://arxiv.org/abs/2505.02222", "authors": ["Essential AI", ":", "Ishaan Shah", "Anthony M. Polloreno", "Karl Stratos", "Philip Monk", "Adarsh Chaluvaraju", "Andrew Hojel", "Andrew Ma", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Michael Pust", "Mohit Parmar", "Peter Rushton", "Platon Mazarakis", "Ritvik Kapila", "Saurabh Srivastava", "Somanshu Singla", "Tim Romanski", "Yash Vanjani", "Ashish Vaswani"], "title": "Practical Efficiency of Muon for Pretraining", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We demonstrate that Muon, the simplest instantiation of a second-order\noptimizer, explicitly expands the Pareto frontier over AdamW on the\ncompute-time tradeoff. We find that Muon is more effective than AdamW in\nretaining data efficiency at large batch sizes, far beyond the so-called\ncritical batch size, while remaining computationally efficient, thus enabling\nmore economical training. We study the combination of Muon and the maximal\nupdate parameterization (muP) for efficient hyperparameter transfer and present\na simple telescoping algorithm that accounts for all sources of error in muP\nwhile introducing only a modest overhead in resources. We validate our findings\nthrough extensive experiments with model sizes up to four billion parameters\nand ablations on the data distribution and architecture."}
{"id": "2505.03368", "pdf": "https://arxiv.org/pdf/2505.03368", "abs": "https://arxiv.org/abs/2505.03368", "authors": ["Stef De Sabbata", "Stefano Mizzaro", "Kevin Roitero"], "title": "Geospatial Mechanistic Interpretability of Large Language Models", "categories": ["cs.LG"], "comment": "Figures 2 and 3: fixed issue with min boundary in colorbar", "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography."}
{"id": "2505.05082", "pdf": "https://arxiv.org/pdf/2505.05082", "abs": "https://arxiv.org/abs/2505.05082", "authors": ["Sagnik Bhattacharya", "Abhiram Gorle", "Ahmed Mohsin", "Ahsan Bilal", "Connor Ding", "Amit Kumar Singh Yadav", "Tsachy Weissman"], "title": "ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model", "categories": ["cs.LG", "cs.IT", "math.IT", "math.PR"], "comment": "Pre-print", "summary": "Existing methods for generative modeling of discrete data, such as symbolic\nmusic tokens, face two primary challenges: (1) they either embed discrete\ninputs into continuous state-spaces or (2) rely on variational losses that only\napproximate the true negative log-likelihood. Previous efforts have\nindividually targeted these limitations. While information-theoretic Gaussian\ndiffusion models alleviate the suboptimality of variational losses, they still\nperform modeling in continuous domains. In this work, we introduce the\nInformation-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which\nsimultaneously addresses both limitations by directly operating in a discrete\nstate-space via a Poisson diffusion process inspired by photon arrival\nprocesses in camera sensors. We introduce a novel Poisson Reconstruction Loss\n(PRL) and derive an exact relationship between PRL and the true negative\nlog-likelihood, thereby eliminating the need for approximate evidence lower\nbounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the\nCIFAR-10 image benchmark demonstrate that ItDPDM delivers significant\nimprovements, reducing test NLL by up to 80% compared to prior baselines, while\nalso achieving faster convergence."}
{"id": "2505.05950", "pdf": "https://arxiv.org/pdf/2505.05950", "abs": "https://arxiv.org/abs/2505.05950", "authors": ["Yuxin Zhou", "Zheng Li", "Jun Zhang", "Jue Wang", "Yiping Wang", "Zhongle Xie", "Ke Chen", "Lidan Shou"], "title": "FloE: On-the-Fly MoE Inference on Memory-constrained GPU", "categories": ["cs.LG"], "comment": "Accepted by ICML 2025", "summary": "With the widespread adoption of Mixture-of-Experts (MoE) models, there is a\ngrowing demand for efficient inference on memory-constrained devices. While\noffloading expert parameters to CPU memory and loading activated experts on\ndemand has emerged as a potential solution, the large size of activated experts\noverburdens the limited PCIe bandwidth, hindering the effectiveness in\nlatency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly\nMoE inference system on memory-constrained GPUs. FloE is built on the insight\nthat there exists substantial untapped redundancy within sparsely activated\nexperts. It employs various compression techniques on the expert's internal\nparameter matrices to reduce the data movement load, combined with low-cost\nsparse prediction, achieving perceptible inference acceleration in wall-clock\ntime on resource-constrained devices. Empirically, FloE achieves a 9.3x\ncompression of parameters per expert in Mixtral-8x7B; enables deployment on a\nGPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and\ndelivers a 48.7x inference speedup compared to DeepSpeed-MII on a single\nGeForce RTX 3090 - all with only a 4.4$\\%$ - 7.6$\\%$ average performance\ndegradation."}
{"id": "2009.09822", "pdf": "https://arxiv.org/pdf/2009.09822", "abs": "https://arxiv.org/abs/2009.09822", "authors": ["Kwei-Herng Lai", "Daochen Zha", "Guanchu Wang", "Junjie Xu", "Yue Zhao", "Devesh Kumar", "Yile Chen", "Purav Zumkhawaka", "Mingyang Wan", "Diego Martinez", "Xia Hu"], "title": "TODS: An Automated Time Series Outlier Detection System", "categories": ["cs.DB", "cs.LG", "stat.ML"], "comment": "Accepted by AAAI'21 demo track", "summary": "We present TODS, an automated Time Series Outlier Detection System for\nresearch and industrial applications. TODS is a highly modular system that\nsupports easy pipeline construction. The basic building block of TODS is\nprimitive, which is an implementation of a function with hyperparameters. TODS\ncurrently supports 70 primitives, including data processing, time series\nprocessing, feature analysis, detection algorithms, and a reinforcement module.\nUsers can freely construct a pipeline using these primitives and perform end-\nto-end outlier detection with the constructed pipeline. TODS provides a\nGraphical User Interface (GUI), where users can flexibly design a pipeline with\ndrag-and-drop. Moreover, a data-driven searcher is provided to automatically\ndiscover the most suitable pipelines given a dataset. TODS is released under\nApache 2.0 license at https://github.com/datamllab/tods."}
{"id": "2306.14851", "pdf": "https://arxiv.org/pdf/2306.14851", "abs": "https://arxiv.org/abs/2306.14851", "authors": ["Ryan Cory-Wright", "Andrés Gómez"], "title": "Optimal Cross-Validation for Sparse Linear Regression", "categories": ["math.OC", "cs.LG", "stat.ME"], "comment": "Moved stability-adjustment content to a different paper, as it was a\n  separate idea to the main point of the paper", "summary": "Given a high-dimensional covariate matrix and a response vector,\nridge-regularized sparse linear regression selects a subset of features that\nexplains the relationship between covariates and the response in an\ninterpretable manner. To select the sparsity and robustness of linear\nregressors, techniques like k-fold cross-validation are commonly used for\nhyperparameter tuning. However, cross-validation substantially increases the\ncomputational cost of sparse regression as it requires solving many\nmixed-integer optimization problems (MIOs) for each hyperparameter combination.\nTo improve upon this state of affairs, we obtain computationally tractable\nrelaxations of k-fold cross-validation metrics, facilitating hyperparameter\nselection after solving 50-80% fewer MIOs in practice. These relaxations result\nin an efficient cyclic coordinate descent scheme, achieving 10%-30% lower\nvalidation errors than via traditional methods such as grid search with MCP or\nGLMNet across a suite of 13 real-world datasets."}
{"id": "2401.02890", "pdf": "https://arxiv.org/pdf/2401.02890", "abs": "https://arxiv.org/abs/2401.02890", "authors": ["Zhongjie Shi", "Jun Fan", "Linhao Song", "Ding-Xuan Zhou", "Johan A. K. Suykens"], "title": "Nonlinear functional regression by functional deep neural network with kernel embedding", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Recently, deep learning has been widely applied in functional data analysis\n(FDA) with notable empirical success. However, the infinite dimensionality of\nfunctional data necessitates an effective dimension reduction approach for\nfunctional learning tasks, particularly in nonlinear functional regression. In\nthis paper, we introduce a functional deep neural network with an adaptive and\ndiscretization-invariant dimension reduction method. Our functional network\narchitecture consists of three parts: first, a kernel embedding step that\nfeatures an integral transformation with an adaptive smooth kernel; next, a\nprojection step that utilizes eigenfunction bases based on a projection Mercer\nkernel for the dimension reduction; and finally, a deep ReLU neural network is\nemployed for the prediction. Explicit rates of approximating nonlinear smooth\nfunctionals across various input function spaces by our proposed functional\nnetwork are derived. Additionally, we conduct a generalization analysis for the\nempirical risk minimization (ERM) algorithm applied to our functional net, by\nemploying a novel two-stage oracle inequality and the established functional\napproximation results. Ultimately, we conduct numerical experiments on both\nsimulated and real datasets to demonstrate the effectiveness and benefits of\nour functional net."}
{"id": "2401.13231", "pdf": "https://arxiv.org/pdf/2401.13231", "abs": "https://arxiv.org/abs/2401.13231", "authors": ["Suning Huang", "Boyuan Chen", "Huazhe Xu", "Vincent Sitzmann"], "title": "DittoGym: Learning to Control Soft Shape-Shifting Robots", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Robot co-design, where the morphology of a robot is optimized jointly with a\nlearned policy to solve a specific task, is an emerging area of research. It\nholds particular promise for soft robots, which are amenable to novel\nmanufacturing techniques that can realize learned morphologies and actuators.\nInspired by nature and recent novel robot designs, we propose to go a step\nfurther and explore the novel reconfigurable robots, defined as robots that can\nchange their morphology within their lifetime. We formalize control of\nreconfigurable soft robots as a high-dimensional reinforcement learning (RL)\nproblem. We unify morphology change, locomotion, and environment interaction in\nthe same action space, and introduce an appropriate, coarse-to-fine curriculum\nthat enables us to discover policies that accomplish fine-grained control of\nthe resulting robots. We also introduce DittoGym, a comprehensive RL benchmark\nfor reconfigurable soft robots that require fine-grained morphology changes to\naccomplish the tasks. Finally, we evaluate our proposed coarse-to-fine\nalgorithm on DittoGym and demonstrate robots that learn to change their\nmorphology several times within a sequence, uniquely enabled by our RL\nalgorithm. More results are available at\nhttps://suninghuang19.github.io/dittogym_page/."}
{"id": "2403.02051", "pdf": "https://arxiv.org/pdf/2403.02051", "abs": "https://arxiv.org/abs/2403.02051", "authors": ["Umut Şimşekli", "Mert Gürbüzbalaban", "Sinan Yıldırım", "Lingjiong Zhu"], "title": "Privacy of SGD under Gaussian or Heavy-Tailed Noise: Guarantees without Gradient Clipping", "categories": ["stat.ML", "cs.CR", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "The injection of heavy-tailed noise into the iterates of stochastic gradient\ndescent (SGD) has garnered growing interest in recent years due to its\ntheoretical and empirical benefits for optimization and generalization.\nHowever, its implications for privacy preservation remain largely unexplored.\nAiming to bridge this gap, we provide differential privacy (DP) guarantees for\nnoisy SGD, when the injected noise follows an $\\alpha$-stable distribution,\nwhich includes a spectrum of heavy-tailed distributions (with infinite\nvariance) as well as the light-tailed Gaussian distribution. Considering the\n$(\\epsilon, \\delta)$-DP framework, we show that SGD with heavy-tailed\nperturbations achieves $(0, O(1/n))$-DP for a broad class of loss functions\nwhich can be non-convex, where $n$ is the number of data points. As a\nremarkable byproduct, contrary to prior work that necessitates bounded\nsensitivity for the gradients or clipping the iterates, our theory can handle\nunbounded gradients without clipping, and reveals that under mild assumptions,\nsuch a projection step is not actually necessary. Our results suggest that,\ngiven other benefits of heavy-tails in optimization, heavy-tailed noising\nschemes can be a viable alternative to their light-tailed counterparts."}
{"id": "2403.10266", "pdf": "https://arxiv.org/pdf/2403.10266", "abs": "https://arxiv.org/abs/2403.10266", "authors": ["Xuanlei Zhao", "Shenggan Cheng", "Chang Chen", "Zangwei Zheng", "Ziming Liu", "Zheming Yang", "Yang You"], "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers", "categories": ["cs.DC", "cs.LG"], "comment": "ICML 2025", "summary": "Scaling multi-dimensional transformers to long sequences is indispensable\nacross various domains. However, the challenges of large memory requirements\nand slow speeds of such sequences necessitate sequence parallelism. All\nexisting approaches fall under the category of embedded sequence parallelism,\nwhich are limited to shard along a single sequence dimension, thereby\nintroducing significant communication overhead. However, the nature of\nmulti-dimensional transformers involves independent calculations across\nmultiple sequence dimensions. To this end, we propose Dynamic Sequence\nParallelism (DSP) as a novel abstraction of sequence parallelism. DSP\ndynamically switches the parallel dimension among all sequences according to\nthe computation stage with efficient resharding strategy. DSP offers\nsignificant reductions in communication costs, adaptability across modules, and\nease of implementation with minimal constraints. Experimental evaluations\ndemonstrate DSP's superiority over state-of-the-art embedded sequence\nparallelism methods by remarkable throughput improvements ranging from 32.2% to\n10x, with less than 25% communication volume."}
{"id": "2403.11522", "pdf": "https://arxiv.org/pdf/2403.11522", "abs": "https://arxiv.org/abs/2403.11522", "authors": ["Massinissa Merouani", "Khaled Afif Boudaoud", "Iheb Nassim Aouadj", "Nassim Tchoulak", "Islem Kara Bernou", "Hamza Benyamina", "Fatima Benbouzid-Si Tayeb", "Karima Benatchba", "Hugh Leather", "Riyadh Baghdadi"], "title": "LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers", "categories": ["cs.PL", "cs.DC", "cs.LG"], "comment": null, "summary": "While polyhedral compilers have shown success in implementing advanced code\ntransformations, they still face challenges in selecting the ones that lead to\nthe most profitable speedups. This has motivated the use of machine learning\nbased cost models to guide the search for polyhedral optimizations.\nState-of-the-art polyhedral compilers have demonstrated a viable\nproof-of-concept of such an approach. While promising, this approach still\nfaces significant limitations. State-of-the-art polyhedral compilers that use a\ndeep learning cost model only support a small subset of affine transformations,\nlimiting their ability to explore complex code transformations. Furthermore,\ntheir applicability does not scale beyond simple programs, thus excluding many\nprogram classes from their scope, such as those with non-rectangular iteration\ndomains or multiple loop nests. These limitations significantly impact the\ngenerality of such compilers and autoschedulers and put into question the whole\napproach. In this paper, we introduce LOOPer, the first polyhedral\nautoscheduler that uses a deep learning based cost model and covers a large\nspace of affine transformations and programs. LOOPer allows the optimization of\nan extensive set of programs while being effective at applying complex\nsequences of polyhedral transformations. We implement and evaluate LOOPer and\nshow that it achieves competitive speedups over the state-of-the-art. On the\nPolyBench benchmarks, LOOPer achieves a geometric mean speedup of 1.84x over\nTiramisu and 1.42x over Pluto, two state-of-the-art polyhedral autoschedulers."}
{"id": "2403.12338", "pdf": "https://arxiv.org/pdf/2403.12338", "abs": "https://arxiv.org/abs/2403.12338", "authors": ["Mario Bravo", "Juan Pablo Contreras"], "title": "Stochastic Halpern iteration in normed spaces and applications to reinforcement learning", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "New version after reviews. Updated literature for MDPs and\n  application to the weakly communicating average case", "summary": "We analyze the oracle complexity of the stochastic Halpern iteration with\nminibatch, where we aim to approximate fixed-points of nonexpansive and\ncontractive operators in a normed finite-dimensional space. We show that if the\nunderlying stochastic oracle has uniformly bounded variance, our method\nexhibits an overall oracle complexity of $\\tilde{O}(\\varepsilon^{-5})$, to\nobtain $\\varepsilon$ expected fixed-point residual for nonexpansive operators,\nimproving recent rates established for the stochastic Krasnoselskii-Mann\niteration. Also, we establish a lower bound of $\\Omega(\\varepsilon^{-3})$ which\napplies to a wide range of algorithms, including all averaged iterations even\nwith minibatching. Using a suitable modification of our approach, we derive a\n$O(\\varepsilon^{-2}(1-\\gamma)^{-3})$ complexity bound in the case in which the\noperator is a $\\gamma$-contraction to obtain an approximation of the\nfixed-point. As an application, we propose new model-free algorithms for\naverage and discounted reward MDPs. For the average reward case, our method\napplies to weakly communicating MDPs without requiring prior parameter\nknowledge."}
{"id": "2405.12783", "pdf": "https://arxiv.org/pdf/2405.12783", "abs": "https://arxiv.org/abs/2405.12783", "authors": ["Tian Qin", "Wei-Min Huang"], "title": "On Kernel-based Variational Autoencoder", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In this paper, we bridge Variational Autoencoders (VAEs) and kernel density\nestimations (KDEs) by approximating the posterior by KDEs and deriving an upper\nbound of the Kullback-Leibler (KL) divergence in the evidence lower bound\n(ELBO). The flexibility of KDEs makes the optimization of posteriors in VAEs\npossible, which not only addresses the limitations of Gaussian latent space in\nvanilla VAE but also provides a new perspective of estimating the KL-divergence\nin ELBO. Under appropriate conditions, we show that the Epanechnikov kernel is\nthe optimal choice in minimizing the derived upper bound of KL-divergence\nasymptotically. Compared with Gaussian kernel, Epanechnikov kernel has compact\nsupport which should make the generated sample less noisy and blurry. The\nimplementation of Epanechnikov kernel in ELBO is straightforward as it lies in\nthe \"location-scale\" family of distributions where the reparametrization tricks\ncan be directly employed. A series of experiments on benchmark datasets such as\nMNIST, Fashion-MNIST, CIFAR-10 and CelebA further demonstrate the superiority\nof Epanechnikov Variational Autoenocoder (EVAE) over vanilla VAE in the quality\nof reconstructed images, as measured by the FID score and Sharpness."}
{"id": "2406.00183", "pdf": "https://arxiv.org/pdf/2406.00183", "abs": "https://arxiv.org/abs/2406.00183", "authors": ["Sebastien Röcken", "Anton F. Burnet", "Julija Zavadlav"], "title": "Predicting solvation free energies with an implicit solvent machine learning potential", "categories": ["physics.chem-ph", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Machine learning (ML) potentials are a powerful tool in molecular modeling,\nenabling ab initio accuracy for comparably small computational costs.\nNevertheless, all-atom simulations employing best-performing graph neural\nnetwork architectures are still too expensive for applications requiring\nextensive sampling, such as free energy computations. Implicit solvent models\ncould provide the necessary speed-up due to reduced degrees of freedom and\nfaster dynamics. Here, we introduce a Solvation Free Energy Path Reweighting\n(ReSolv) framework to parametrize an implicit solvent ML potential for small\norganic molecules that accurately predicts the hydration free energy, an\nessential parameter in drug design and pollutant modeling. With a combination\nof top-down (experimental hydration free energy data) and bottom-up (ab initio\ndata of molecules in a vacuum) learning, ReSolv bypasses the need for\nintractable ab initio data of molecules in explicit bulk solvent and does not\nhave to resort to less accurate data-generating models. On the FreeSolv\ndataset, ReSolv achieves a mean absolute error close to average experimental\nuncertainty, significantly outperforming standard explicit solvent force\nfields. Compared to the explicit solvent ML potential, ReSolv offers a\ncomputational speedup of four orders of magnitude and attains closer agreement\nwith experiments. The presented framework paves the way toward deep molecular\nmodels that are more accurate yet computationally cheaper than classical\natomistic models."}
{"id": "2406.00920", "pdf": "https://arxiv.org/pdf/2406.00920", "abs": "https://arxiv.org/abs/2406.00920", "authors": ["Kyurae Kim", "Joohwan Ko", "Yi-An Ma", "Jacob R. Gardner"], "title": "Demystifying SGD with Doubly Stochastic Gradients", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": "Accepted to ICML'24; v2: fixed typo in complexity statements", "summary": "Optimization objectives in the form of a sum of intractable expectations are\nrising in importance (e.g., diffusion models, variational autoencoders, and\nmany more), a setting also known as \"finite sum with infinite data.\" For these\nproblems, a popular strategy is to employ SGD with doubly stochastic gradients\n(doubly SGD): the expectations are estimated using the gradient estimator of\neach component, while the sum is estimated by subsampling over these\nestimators. Despite its popularity, little is known about the convergence\nproperties of doubly SGD, except under strong assumptions such as bounded\nvariance. In this work, we establish the convergence of doubly SGD with\nindependent minibatching and random reshuffling under general conditions, which\nencompasses dependent component gradient estimators. In particular, for\ndependent estimators, our analysis allows fined-grained analysis of the effect\ncorrelations. As a result, under a per-iteration computational budget of $b\n\\times m$, where $b$ is the minibatch size and $m$ is the number of Monte Carlo\nsamples, our analysis suggests where one should invest most of the budget in\ngeneral. Furthermore, we prove that random reshuffling (RR) improves the\ncomplexity dependence on the subsampling noise."}
{"id": "2406.03562", "pdf": "https://arxiv.org/pdf/2406.03562", "abs": "https://arxiv.org/abs/2406.03562", "authors": ["Max Hirsch", "Federico Pichi", "Jan S. Hesthaven"], "title": "Neural empirical interpolation method for nonlinear model reduction", "categories": ["math.NA", "cs.LG", "cs.NA", "41A05, 41A46, 65N22, 68T07, 76A15"], "comment": null, "summary": "In this paper, we introduce the neural empirical interpolation method (NEIM),\na neural network-based alternative to the discrete empirical interpolation\nmethod for reducing the time complexity of computing the nonlinear term in a\nreduced order model (ROM) for a parameterized nonlinear partial differential\nequation. NEIM is a greedy algorithm which accomplishes this reduction by\napproximating an affine decomposition of the nonlinear term of the ROM, where\nthe vector terms of the expansion are given by neural networks depending on the\nROM solution, and the coefficients are given by an interpolation of some\n\"optimal\" coefficients. Because NEIM is based on a greedy strategy, we are able\nto provide a basic error analysis to investigate its performance. NEIM has the\nadvantages of being easy to implement in models with automatic differentiation,\nof being a nonlinear projection of the ROM nonlinearity, of being efficient for\nboth nonlocal and local nonlinearities, and of relying solely on data and not\nthe explicit form of the ROM nonlinearity. We demonstrate the effectiveness of\nthe methodology on solution-dependent and solution-independent nonlinearities,\na nonlinear elliptic problem, and a nonlinear parabolic model of liquid\ncrystals.\n  Code availability: https://github.com/maxhirsch/NEIM"}
{"id": "2406.09567", "pdf": "https://arxiv.org/pdf/2406.09567", "abs": "https://arxiv.org/abs/2406.09567", "authors": ["Carlos Fernández-Loría", "Yanfang Hou", "Foster Provost", "Jennifer Hill"], "title": "Causal Post-Processing of Predictive Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Decision makers across various domains rely on predictive models to guide\nindividual-level intervention decisions. However, these models are typically\ntrained to predict outcomes rather than causal effects, leading to\nmisalignments when they are used for causal decision making. Experimental data\nto train effective causal effect models often is limited. To address this\nissue, we propose causal post-processing (CPP), a family of techniques for\nrefining predictive scores to better align with causal effects using limited\nexperimental data. Rather than training separate causal models for each\nintervention, causal post-processing can adapt existing predictive scores to\nsupport different decision-making requirements, such as estimating effect\nsizes, ranking individuals by expected effects, or classifying individuals\nbased on an intervention threshold. We introduce three main CPP approaches --\nmonotonic post-processing, correction post-processing, and model-based\npost-processing -- each balancing statistical efficiency and flexibility\ndifferently. Through simulations and an empirical application in advertising,\nwe demonstrate that causal post-processing improves intervention decisions,\nparticularly in settings where experimental data is expensive or difficult to\nobtain at scale. Our findings highlight the advantages of integrating\nnon-causal predictive models with experimental data, rather than treating them\nas competing alternatives, which provides a scalable and data-efficient\napproach to causal inference for decision making."}
{"id": "2406.16834", "pdf": "https://arxiv.org/pdf/2406.16834", "abs": "https://arxiv.org/abs/2406.16834", "authors": ["Jeremiah Birrell"], "title": "Statistical Error Bounds for GANs with Nonlinear Objective Functionals", "categories": ["stat.ML", "cs.LG"], "comment": "29 pages", "summary": "Generative adversarial networks (GANs) are unsupervised learning methods for\ntraining a generator distribution to produce samples that approximate those\ndrawn from a target distribution. Many such methods can be formulated as\nminimization of a metric or divergence between probability distributions.\nRecent works have derived statistical error bounds for GANs that are based on\nintegral probability metrics (IPMs), e.g., WGAN which is based on the\n1-Wasserstein metric. In general, IPMs are defined by optimizing a linear\nfunctional (difference of expectations) over a space of discriminators. A much\nlarger class of GANs, which we here call $(f,\\Gamma)$-GANs, can be constructed\nusing $f$-divergences (e.g., Jensen-Shannon, KL, or $\\alpha$-divergences)\ntogether with a regularizing discriminator space $\\Gamma$ (e.g., $1$-Lipschitz\nfunctions). These GANs have nonlinear objective functions, depending on the\nchoice of $f$, and have been shown to exhibit improved performance in a number\nof applications. In this work we derive statistical error bounds for\n$(f,\\Gamma)$-GANs for general classes of $f$ and $\\Gamma$ in the form of\nfinite-sample concentration inequalities. These results prove the statistical\nconsistency of $(f,\\Gamma)$-GANs and reduce to the known results for IPM-GANs\nin the appropriate limit. Our results use novel Rademacher complexity bounds\nwhich provide new insight into the performance of IPM-GANs for distributions\nwith unbounded support and have application to statistical learning tasks\nbeyond GANs."}
{"id": "2407.21407", "pdf": "https://arxiv.org/pdf/2407.21407", "abs": "https://arxiv.org/abs/2407.21407", "authors": ["Su I Iao", "Yidong Zhou", "Hans-Georg Müller"], "title": "Deep Fréchet Regression", "categories": ["stat.ME", "cs.LG"], "comment": "74 pages, 6 figures, 9 tables", "summary": "Advancements in modern science have led to the increasing availability of\nnon-Euclidean data in metric spaces. This paper addresses the challenge of\nmodeling relationships between non-Euclidean responses and multivariate\nEuclidean predictors. We propose a flexible regression model capable of\nhandling high-dimensional predictors without imposing parametric assumptions.\nTwo primary challenges are addressed: the curse of dimensionality in\nnonparametric regression and the absence of linear structure in general metric\nspaces. The former is tackled using deep neural networks, while for the latter\nwe demonstrate the feasibility of mapping the metric space where responses\nreside to a low-dimensional Euclidean space using manifold learning. We\nintroduce a reverse mapping approach, employing local Fr\\'echet regression, to\nmap the low-dimensional manifold representations back to objects in the\noriginal metric space. We develop a theoretical framework, investigating the\nconvergence rate of deep neural networks under dependent sub-Gaussian noise\nwith bias. The convergence rate of the proposed regression model is then\nobtained by expanding the scope of local Fr\\'echet regression to accommodate\nmultivariate predictors in the presence of errors in predictors. Simulations\nand case studies show that the proposed model outperforms existing methods for\nnon-Euclidean responses, focusing on the special cases of probability\ndistributions and networks."}
{"id": "2408.06003", "pdf": "https://arxiv.org/pdf/2408.06003", "abs": "https://arxiv.org/abs/2408.06003", "authors": ["Zhiwen Mo", "Lei Wang", "Jianyu Wei", "Zhichen Zeng", "Shijie Cao", "Lingxiao Ma", "Naifeng Jing", "Ting Cao", "Jilong Xue", "Fan Yang", "Mao Yang"], "title": "LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM Inference", "categories": ["cs.AR", "cs.LG", "C.1.0; C.3; B.2.4"], "comment": "Conference Version (ISCA'25)", "summary": "As large language model (LLM) inference continues to demand increasing\ncomputational resources, there is a rapidly growing trend toward using low-bit\nweights to reduce memory footprint and improve inference efficiency. However,\nlow-bit LLMs introduce the need for mixed-precision general matrix\nmultiplication (mpGEMM), which involves multiplying low-precision weights with\nhigher-precision activations - a critical yet under-explored operation. Current\nhardware lacks native support for mpGEMM, leading to inefficient\ndequantization-based implementations.\n  To address this, we explore a lookup table (LUT)-based approach to accelerate\nmpGEMM. While conventional LUT implementations fall short in performance and\nflexibility, we propose LUT Tensor Core, a software-hardware co-designed\nsolution optimized for low-bit LLM inference. On the software side, we\nintroduce operator fusion and table symmetrization techniques to optimize LUT\ngeneration and storage. On the hardware side, LUT Tensor Core adopts an\nelongated tiling shape to maximize table reuse and employs a bit-serial\narchitecture to flexibly support a variety of precision combinations.\nAdditionally, we design an end-to-end compilation stack with custom\ninstructions to enable efficient code generation and optimization for LUT-based\nmpGEMM. Experimental results on low-bit LLMs such as BitNet and LLaMA\ndemonstrate that LUT Tensor Core delivers over an order-of-magnitude\nimprovement in both compute density and energy efficiency."}
{"id": "2408.06258", "pdf": "https://arxiv.org/pdf/2408.06258", "abs": "https://arxiv.org/abs/2408.06258", "authors": ["Oliver Weißl", "Amr Abdellatif", "Xingcheng Chen", "Giorgi Merabishvili", "Vincenzo Riccio", "Severin Kacianka", "Andrea Stocco"], "title": "Targeted Deep Learning System Boundary Testing", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Evaluating the behavioral boundaries of deep learning (DL) systems is crucial\nfor understanding their reliability across diverse, unseen inputs. Existing\nsolutions fall short as they rely on untargeted random, model- or latent-based\nperturbations, due to difficulties in generating controlled input variations.\nIn this work, we introduce Mimicry, a novel black-box test generator for\nfine-grained, targeted exploration of DL system boundaries. Mimicry performs\nboundary testing by leveraging the probabilistic nature of DL outputs to\nidentify promising directions for exploration. It uses style-based GANs to\ndisentangle input representations into content and style components, enabling\ncontrolled feature mixing to approximate the decision boundary. We evaluated\nMimicry's effectiveness in generating boundary inputs for five widely used DL\nimage classification systems of increasing complexity, comparing it to two\nbaseline approaches. Our results show that Mimicry consistently identifies\ninputs closer to the decision boundary. It generates semantically meaningful\nboundary test cases that reveal new functional (mis)behaviors, while the\nbaselines produce mainly corrupted or invalid inputs. Thanks to its enhanced\ncontrol over latent space manipulations, Mimicry remains effective as dataset\ncomplexity increases, maintaining competitive diversity and higher validity\nrates, confirmed by human assessors."}
{"id": "2409.02684", "pdf": "https://arxiv.org/pdf/2409.02684", "abs": "https://arxiv.org/abs/2409.02684", "authors": ["Roxana Zeraati", "Anna Levina", "Jakob H. Macke", "Richard Gao"], "title": "Neural timescales from a computational perspective", "categories": ["q-bio.NC", "cs.LG", "stat.ML"], "comment": "21 pages, 5 figures, 3 boxes, 1 table", "summary": "Neural activity fluctuates over a wide range of timescales within and across\nbrain areas. Experimental observations suggest that diverse neural timescales\nreflect information in dynamic environments. However, how timescales are\ndefined and measured from brain recordings vary across the literature.\nMoreover, these observations do not specify the mechanisms underlying timescale\nvariations, nor whether specific timescales are necessary for neural\ncomputation and brain function. Here, we synthesize three directions where\ncomputational approaches can distill the broad set of empirical observations\ninto quantitative and testable theories: We review (i) how different data\nanalysis methods quantify timescales across distinct behavioral states and\nrecording modalities, (ii) how biophysical models provide mechanistic\nexplanations for the emergence of diverse timescales, and (iii) how\ntask-performing networks and machine learning models uncover the functional\nrelevance of neural timescales. This integrative computational perspective thus\ncomplements experimental investigations, providing a holistic view on how\nneural timescales reflect the relationship between brain structure, dynamics,\nand behavior."}
{"id": "2409.08282", "pdf": "https://arxiv.org/pdf/2409.08282", "abs": "https://arxiv.org/abs/2409.08282", "authors": ["Peng Zhu", "Yuante Li", "Yifan Hu", "Qinyuan Liu", "Dawei Cheng", "Yuqi Liang"], "title": "LSR-IGRU: Stock Trend Prediction Based on Long Short-Term Relationships and Improved GRU", "categories": ["q-fin.ST", "cs.CE", "cs.LG"], "comment": null, "summary": "Stock price prediction is a challenging problem in the field of finance and\nreceives widespread attention. In recent years, with the rapid development of\ntechnologies such as deep learning and graph neural networks, more research\nmethods have begun to focus on exploring the interrelationships between stocks.\nHowever, existing methods mostly focus on the short-term dynamic relationships\nof stocks and directly integrating relationship information with temporal\ninformation. They often overlook the complex nonlinear dynamic characteristics\nand potential higher-order interaction relationships among stocks in the stock\nmarket. Therefore, we propose a stock price trend prediction model named\nLSR-IGRU in this paper, which is based on long short-term stock relationships\nand an improved GRU input. Firstly, we construct a long short-term relationship\nmatrix between stocks, where secondary industry information is employed for the\nfirst time to capture long-term relationships of stocks, and overnight price\ninformation is utilized to establish short-term relationships. Next, we improve\nthe inputs of the GRU model at each step, enabling the model to more\neffectively integrate temporal information and long short-term relationship\ninformation, thereby significantly improving the accuracy of predicting stock\ntrend changes. Finally, through extensive experiments on multiple datasets from\nstock markets in China and the United States, we validate the superiority of\nthe proposed LSR-IGRU model over the current state-of-the-art baseline models.\nWe also apply the proposed model to the algorithmic trading system of a\nfinancial company, achieving significantly higher cumulative portfolio returns\ncompared to other baseline methods. Our sources are released at\nhttps://github.com/ZP1481616577/Baselines_LSR-IGRU."}
{"id": "2410.13800", "pdf": "https://arxiv.org/pdf/2410.13800", "abs": "https://arxiv.org/abs/2410.13800", "authors": ["Abhijith Jayakumar", "Andrey Y. Lokhov", "Sidhant Misra", "Marc Vuffray"], "title": "Discrete distributions are learnable from metastable samples", "categories": ["stat.ML", "cond-mat.stat-mech", "cs.LG"], "comment": "Submitted version, 31 pages", "summary": "Physically motivated stochastic dynamics are often used to sample from\nhigh-dimensional distributions. However such dynamics often get stuck in\nspecific regions of their state space and mix very slowly to the desired\nstationary state. This causes such systems to approximately sample from a\nmetastable distribution which is usually quite different from the desired,\nstationary distribution of the dynamic. We rigorously show that, in the case of\nmulti-variable discrete distributions, the true model describing the stationary\ndistribution can be recovered from samples produced from a metastable\ndistribution under minimal assumptions about the system. This follows from a\nfundamental observation that the single-variable conditionals of metastable\ndistributions that satisfy a strong metastability condition are on average\nclose to those of the stationary distribution. This holds even when the\nmetastable distribution differs considerably from the true model in terms of\nglobal metrics like Kullback-Leibler divergence or total variation distance.\nThis property allows us to learn the true model using a conditional likelihood\nbased estimator, even when the samples come from a metastable distribution\nconcentrated in a small region of the state space. Explicit examples of such\nmetastable states can be constructed from regions that effectively bottleneck\nthe probability flow and cause poor mixing of the Markov chain. For specific\ncases of binary pairwise undirected graphical models (i.e. Ising models), we\nextend our results to further rigorously show that data coming from metastable\nstates can be used to learn the parameters of the energy function and recover\nthe structure of the model."}
{"id": "2410.16314", "pdf": "https://arxiv.org/pdf/2410.16314", "abs": "https://arxiv.org/abs/2410.16314", "authors": ["Joris Postmus", "Steven Abreu"], "title": "Steering Large Language Models using Conceptors: Improving Addition-Based Activation Engineering", "categories": ["cs.NE", "cs.LG"], "comment": "Presented at the MINT workshop at NeurIPS 2024. v4: fix sign in\n  equation 10", "summary": "Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering."}
{"id": "2410.21419", "pdf": "https://arxiv.org/pdf/2410.21419", "abs": "https://arxiv.org/abs/2410.21419", "authors": ["Chris Camaño", "Daniel Huang"], "title": "High-Dimensional Gaussian Process Regression with Soft Kernel Interpolation", "categories": ["stat.ML", "cs.LG"], "comment": "12 pages", "summary": "We introduce Soft Kernel Interpolation (SoftKI), a method that combines\naspects of Structured Kernel Interpolation (SKI) and variational inducing point\nmethods, to achieve scalable Gaussian Process (GP) regression on\nhigh-dimensional datasets. SoftKI approximates a kernel via softmax\ninterpolation from a smaller number of interpolation points learned by\noptimizing a combination of the SoftKI marginal log-likelihood (MLL), and when\nneeded, an approximate MLL for improved numerical stability. Consequently, it\ncan overcome the dimensionality scaling challenges that SKI faces when\ninterpolating from a dense and static lattice while retaining the flexibility\nof variational methods to adapt inducing points to the dataset. We demonstrate\nthe effectiveness of SoftKI across various examples and show that it is\ncompetitive with other approximated GP methods when the data dimensionality is\nmodest (around 10)."}
{"id": "2411.09851", "pdf": "https://arxiv.org/pdf/2411.09851", "abs": "https://arxiv.org/abs/2411.09851", "authors": ["Ho Fung Tsoi", "Dylan Rankin", "Cecile Caillol", "Miles Cranmer", "Sridhara Dasu", "Javier Duarte", "Philip Harris", "Elliot Lipeles", "Vladimir Loncar"], "title": "SymbolFit: Automatic Parametric Modeling with Symbolic Regression", "categories": ["hep-ex", "cs.LG", "physics.data-an"], "comment": "52 pages, 35 figures. Under review. The API can be used\n  out-of-the-box and is available at https://github.com/hftsoi/symbolfit", "summary": "We introduce SymbolFit, a framework that automates parametric modeling by\nusing symbolic regression to perform a machine-search for functions that fit\nthe data while simultaneously providing uncertainty estimates in a single run.\nTraditionally, constructing a parametric model to accurately describe binned\ndata has been a manual and iterative process, requiring an adequate functional\nform to be determined before the fit can be performed. The main challenge\narises when the appropriate functional forms cannot be derived from first\nprinciples, especially when there is no underlying true closed-form function\nfor the distribution. In this work, we develop a framework that automates and\nstreamlines the process by utilizing symbolic regression, a machine learning\ntechnique that explores a vast space of candidate functions without requiring a\npredefined functional form because the functional form itself is treated as a\ntrainable parameter, making the process far more efficient and effortless than\ntraditional regression methods. We demonstrate the framework in high-energy\nphysics experiments at the CERN Large Hadron Collider (LHC) using five real\nproton-proton collision datasets from new physics searches, including\nbackground modeling in resonance searches for high-mass dijet, trijet,\npaired-dijet, diphoton, and dimuon events. We show that our framework can\nflexibly and efficiently generate a wide range of candidate functions that fit\na nontrivial distribution well using a simple fit configuration that varies\nonly by random seed, and that the same fit configuration, which defines a vast\nfunction space, can also be applied to distributions of different shapes,\nwhereas achieving a comparable result with traditional methods would have\nrequired extensive manual effort."}
{"id": "2411.18825", "pdf": "https://arxiv.org/pdf/2411.18825", "abs": "https://arxiv.org/abs/2411.18825", "authors": ["Letian Chen", "Nina Moorman", "Matthew Gombolay"], "title": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics", "categories": ["cs.RO", "cs.LG"], "comment": "ICML 2025", "summary": "Reinforcement learning (RL) has demonstrated compelling performance in\nrobotic tasks, but its success often hinges on the design of complex, ad hoc\nreward functions. Researchers have explored how Large Language Models (LLMs)\ncould enable non-expert users to specify reward functions more easily. However,\nLLMs struggle to balance the importance of different features, generalize\npoorly to out-of-distribution robotic tasks, and cannot represent the problem\nproperly with only text-based descriptions. To address these challenges, we\npropose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a\nnovel framework that combines natural language guidance with visual user\ndemonstrations to align robot behavior with user intentions better. By\nincorporating visual inputs, ELEMENTAL overcomes the limitations of text-only\ntask specifications, while leveraging inverse reinforcement learning (IRL) to\nbalance feature weights and match the demonstrated behaviors optimally.\nELEMENTAL also introduces an iterative feedback-loop through self-reflection to\nimprove feature, reward, and policy learning. Our experiment results\ndemonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, and\nachieves 41.3% better generalization in out-of-distribution tasks, highlighting\nits robustness in LfD."}
{"id": "2412.10161", "pdf": "https://arxiv.org/pdf/2412.10161", "abs": "https://arxiv.org/abs/2412.10161", "authors": ["Simon Wein", "Marco Riebel", "Lisa-Marie Brunner", "Caroline Nothdurfter", "Rainer Rupprecht", "Jens V. Schwarzbach"], "title": "Data Integration with Fusion Searchlight: Classifying Brain States from Resting-state fMRI", "categories": ["q-bio.NC", "cs.LG"], "comment": null, "summary": "Resting-state fMRI captures spontaneous neural activity characterized by\ncomplex spatiotemporal dynamics. Various metrics, such as local and global\nbrain connectivity and low-frequency amplitude fluctuations, quantify distinct\naspects of these dynamics. However, these measures are typically analyzed\nindependently, overlooking their interrelations and potentially limiting\nanalytical sensitivity. Here, we introduce the Fusion Searchlight (FuSL)\nframework, which integrates complementary information from multiple\nresting-state fMRI metrics. We demonstrate that combining these metrics\nenhances the accuracy of pharmacological treatment prediction from rs-fMRI\ndata, enabling the identification of additional brain regions affected by\nsedation with alprazolam. Furthermore, we leverage explainable AI to delineate\nthe differential contributions of each metric, which additionally improves\nspatial specificity of the searchlight analysis. Moreover, this framework can\nbe adapted to combine information across imaging modalities or experimental\nconditions, providing a versatile and interpretable tool for data fusion in\nneuroimaging."}
{"id": "2412.15826", "pdf": "https://arxiv.org/pdf/2412.15826", "abs": "https://arxiv.org/abs/2412.15826", "authors": ["Joshua B. Moore", "Hugo P. Stackhouse", "Ben D. Fulcher", "Sahand Mahmoodian"], "title": "Using matrix-product states for time-series machine learning", "categories": ["stat.ML", "cs.LG", "quant-ph"], "comment": "31 pages, 14 figures", "summary": "Matrix-product states (MPS) have proven to be a versatile ansatz for modeling\nquantum many-body physics. For many applications, and particularly in\none-dimension, they capture relevant quantum correlations in many-body\nwavefunctions while remaining tractable to store and manipulate on a classical\ncomputer. This has motivated researchers to also apply the MPS ansatz to\nmachine learning (ML) problems where capturing complex correlations in datasets\nis also a key requirement. Here, we develop and apply an MPS-based algorithm,\nMPSTime, for learning a joint probability distribution underlying an observed\ntime-series dataset, and show how it can be used to tackle important\ntime-series ML problems, including classification and imputation. MPSTime can\nefficiently learn complicated time-series probability distributions directly\nfrom data, requires only moderate maximum MPS bond dimension $\\chi_{\\rm max}$,\nwith values for our applications ranging between $\\chi_{\\rm max} = 20-160$, and\ncan be trained for both classification and imputation tasks under a single\nlogarithmic loss function. Using synthetic and publicly available real-world\ndatasets, spanning applications in medicine, energy, and astronomy, we\ndemonstrate performance competitive with state-of-the-art ML approaches, but\nwith the key advantage of encoding the full joint probability distribution\nlearned from the data, which is useful for analyzing and interpreting its\nunderlying structure. This manuscript is supplemented with the release of a\npublicly available code package MPSTime that implements our approach. The\neffectiveness of the MPS-based ansatz for capturing complex correlation\nstructures in time-series data makes it a powerful foundation for tackling\nchallenging time-series analysis problems across science, industry, and\nmedicine."}
{"id": "2412.18432", "pdf": "https://arxiv.org/pdf/2412.18432", "abs": "https://arxiv.org/abs/2412.18432", "authors": ["O. Deniz Akyildiz", "Pierre Del Moral", "Joaquín Miguez"], "title": "Gaussian entropic optimal transport: Schrödinger bridges and the Sinkhorn algorithm", "categories": ["stat.ML", "cs.LG", "math.PR", "stat.CO"], "comment": "80 pages", "summary": "Entropic optimal transport problems are regularized versions of optimal\ntransport problems. These models play an increasingly important role in machine\nlearning and generative modelling. For finite spaces, these problems are\ncommonly solved using Sinkhorn algorithm (a.k.a. iterative proportional fitting\nprocedure). However, in more general settings the Sinkhorn iterations are based\non nonlinear conditional/conjugate transformations and exact finite-dimensional\nsolutions cannot be computed.\n  This article presents a finite-dimensional recursive formulation of the\niterative proportional fitting procedure for general Gaussian multivariate\nmodels. As expected, this recursive formulation is closely related to the\ncelebrated Kalman filter and related Riccati matrix difference equations, and\nit yields algorithms that can be implemented in practical settings without\nfurther approximations. We extend this filtering methodology to develop a\nrefined and self-contained convergence analysis of Gaussian Sinkhorn\nalgorithms, including closed form expressions of entropic transport maps and\nSchr\\\"odinger bridges."}
{"id": "2502.00017", "pdf": "https://arxiv.org/pdf/2502.00017", "abs": "https://arxiv.org/abs/2502.00017", "authors": ["Ikram Gagaoua", "Armelle Brun", "Anne Boyer"], "title": "A Frugal Model for Accurate Early Student Failure Prediction", "categories": ["cs.CY", "cs.LG"], "comment": "LICE - London International Conference on Education, London\n  International Conference on Education, Nov 2024, London, United Kingdom", "summary": "Predicting student success or failure is vital for timely interventions and\npersonalized support. Early failure prediction is particularly crucial, yet\nlimited data availability in the early stages poses challenges, one of the\npossible solutions is to make use of additional data from other contexts,\nhowever, this might lead to overconsumption with no guarantee of better\nresults. To address this, we propose the Frugal Early Prediction (FEP) model, a\nnew hybrid model that selectively incorporates additional data, promoting data\nfrugality and efficient resource utilization. Experiments conducted on a public\ndataset from a VLE demonstrate FEP's effectiveness in reducing data usage, a\nprimary goal of this research.Experiments showcase a remarkable 27% reduction\nin data consumption, compared to a systematic use of additional data, aligning\nwith our commitment to data frugality and offering substantial benefits to\neducational institutions seeking efficient data consumption. Additionally, FEP\nalso excels in enhancing prediction accuracy. Compared to traditional\napproaches, FEP achieves an average accuracy gain of 7.3%. This not only\nhighlights the practicality and efficiency of FEP but also its superiority in\nperformance, while respecting resource constraints, providing beneficial\nfindings for educational institutions seeking data frugality."}
{"id": "2502.02853", "pdf": "https://arxiv.org/pdf/2502.02853", "abs": "https://arxiv.org/abs/2502.02853", "authors": ["Shuanghao Bai", "Wanqi Zhou", "Pengxiang Ding", "Wei Zhao", "Donglin Wang", "Badong Chen"], "title": "Rethinking Latent Redundancy in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Behavior Cloning (BC) is a widely adopted visual imitation learning method in\nrobot manipulation. Current BC approaches often enhance generalization by\nleveraging large datasets and incorporating additional visual and textual\nmodalities to capture more diverse information. However, these methods overlook\nwhether the learned representations contain redundant information and lack a\nsolid theoretical foundation to guide the learning process. To address these\nlimitations, we adopt an information-theoretic perspective and introduce mutual\ninformation to quantify and mitigate redundancy in latent representations.\nBuilding on this, we incorporate the Information Bottleneck (IB) principle into\nBC, which extends the idea of reducing redundancy by providing a structured\nframework for compressing irrelevant information while preserving task-relevant\nfeatures. This work presents the first comprehensive study on redundancy in\nlatent representations across various methods, backbones, and experimental\nsettings, while extending the generalizability of the IB to BC. Extensive\nexperiments and analyses on the CortexBench and LIBERO benchmarks demonstrate\nsignificant performance improvements with IB, underscoring the importance of\nreducing input data redundancy and highlighting its practical value for more\npractical applications. Project Page:\nhttps://baishuanghao.github.io/BC-IB.github.io."}
{"id": "2502.08414", "pdf": "https://arxiv.org/pdf/2502.08414", "abs": "https://arxiv.org/abs/2502.08414", "authors": ["Samuel Erickson", "Tobias Rydén"], "title": "Inverse Covariance and Partial Correlation Matrix Estimation via Joint Partial Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We present a method for estimating sparse high-dimensional inverse covariance\nand partial correlation matrices, which exploits the connection between the\ninverse covariance matrix and linear regression. The method is a two-stage\nestimation method wherein each individual feature is regressed on all other\nfeatures while positive semi-definiteness is enforced simultaneously. We derive\nnon-asymptotic estimation rates for both inverse covariance and partial\ncorrelation matrix estimation. An efficient proximal splitting algorithm for\nnumerically computing the estimate is also dervied. The effectiveness of the\nproposed method is demonstrated on both synthetic and real-world data."}
{"id": "2503.03656", "pdf": "https://arxiv.org/pdf/2503.03656", "abs": "https://arxiv.org/abs/2503.03656", "authors": ["Tushar Aggarwal", "Swayam Singh", "Abhijeet Awasthi", "Aditya Kanade", "Nagarajan Natarajan"], "title": "Robust Learning of Diverse Code Edits", "categories": ["cs.SE", "cs.LG"], "comment": "To appear in ICML 2025 as 'NextCoder: Robust Adaptation of Code LMs\n  to Diverse Code Edits'", "summary": "Software engineering activities frequently involve edits to existing code.\nHowever, contemporary code language models (LMs) lack the ability to handle\ndiverse types of code-edit requirements. In this work, we attempt to overcome\nthis shortcoming through (1) a novel synthetic data generation pipeline and (2)\na robust model adaptation algorithm. Starting with seed code examples and\ndiverse editing criteria, our pipeline generates high-quality samples\ncomprising original and modified code, along with natural language instructions\nin different styles and verbosity. Today's code LMs come bundled with strong\nabilities, such as code generation and instruction following, which should not\nbe lost due to fine-tuning. To ensure this, we propose a novel adaptation\nalgorithm, SeleKT, that (a) leverages a dense gradient-based step to identify\nthe weights that are most important for code editing, and (b) does a sparse\nprojection onto the base model to avoid overfitting. Using our approach, we\nobtain a new series of models NextCoder (adapted from QwenCoder-2.5) that\nachieves strong results on five code-editing benchmarks, outperforming\ncomparable size models and even several larger ones. We show the generality of\nour approach on two model families (DeepSeekCoder and QwenCoder), compare\nagainst other fine-tuning approaches, and demonstrate robustness by showing\nretention of code generation and general problem-solving abilities post\nadaptation. We opensource the models, synthetic dataset, and implementation at\nhttps://aka.ms/nextcoder."}
{"id": "2503.09085", "pdf": "https://arxiv.org/pdf/2503.09085", "abs": "https://arxiv.org/abs/2503.09085", "authors": ["Ryan K. Krueger", "Sharon Aviran", "David H. Mathews", "Jeffrey Zuber", "Max Ward"], "title": "Differentiable Folding for Nearest Neighbor Model Optimization", "categories": ["q-bio.BM", "cs.LG", "q-bio.QM"], "comment": null, "summary": "The Nearest Neighbor model is the $\\textit{de facto}$ thermodynamic model of\nRNA secondary structure formation and is a cornerstone of RNA structure\nprediction and sequence design. The current functional form (Turner 2004)\ncontains $\\approx13,000$ underlying thermodynamic parameters, and fitting these\nto both experimental and structural data is computationally challenging. Here,\nwe leverage recent advances in $\\textit{differentiable folding}$, a method for\ndirectly computing gradients of the RNA folding algorithms, to devise an\nefficient, scalable, and flexible means of parameter optimization that uses\nknown RNA structures and thermodynamic experiments. Our method yields a\nsignificantly improved parameter set that outperforms existing baselines on all\nmetrics, including an increase in the average predicted probability of\nground-truth sequence-structure pairs for a single RNA family by over 23 orders\nof magnitude. Our framework provides a path towards drastically improved RNA\nmodels, enabling the flexible incorporation of new experimental data,\ndefinition of novel loss terms, large training sets, and even treatment as a\nmodule in larger deep learning pipelines. We make available a new database,\nRNAometer, with experimentally-determined stabilities for small RNA model\nsystems."}
{"id": "2503.12533", "pdf": "https://arxiv.org/pdf/2503.12533", "abs": "https://arxiv.org/abs/2503.12533", "authors": ["Haoqi Yuan", "Yu Bai", "Yuhui Fu", "Bohan Zhou", "Yicheng Feng", "Xinrun Xu", "Yi Zhan", "Börje F. Karlsson", "Zongqing Lu"], "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Building autonomous robotic agents capable of achieving human-level\nperformance in real-world embodied tasks is an ultimate goal in humanoid robot\nresearch. Recent advances have made significant progress in high-level\ncognition with Foundation Models (FMs) and low-level skill development for\nhumanoid robots. However, directly combining these components often results in\npoor robustness and efficiency due to compounding errors in long-horizon tasks\nand the varied latency of different modules. We introduce Being-0, a\nhierarchical agent framework that integrates an FM with a modular skill\nlibrary. The FM handles high-level cognitive tasks such as instruction\nunderstanding, task planning, and reasoning, while the skill library provides\nstable locomotion and dexterous manipulation for low-level control. To bridge\nthe gap between these levels, we propose a novel Connector module, powered by a\nlightweight vision-language model (VLM). The Connector enhances the FM's\nembodied capabilities by translating language-based plans into actionable skill\ncommands and dynamically coordinating locomotion and manipulation to improve\ntask success. With all components, except the FM, deployable on low-cost\nonboard computation devices, Being-0 achieves efficient, real-time performance\non a full-sized humanoid robot equipped with dexterous hands and active vision.\nExtensive experiments in large indoor environments demonstrate Being-0's\neffectiveness in solving complex, long-horizon tasks that require challenging\nnavigation and manipulation subtasks. For further details and videos, visit\nhttps://beingbeyond.github.io/Being-0."}
{"id": "2503.21526", "pdf": "https://arxiv.org/pdf/2503.21526", "abs": "https://arxiv.org/abs/2503.21526", "authors": ["Christine W. Bang", "Vanessa Didelez"], "title": "Constraint-based causal discovery with tiered background knowledge and latent variables in single or overlapping datasets", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "Accepted for the 4th Conference on Causal Learning and Reasoning\n  (CLeaR 2025). Version 2: Corrected numbering in Example 1", "summary": "In this paper we consider the use of tiered background knowledge within\nconstraint based causal discovery. Our focus is on settings relaxing causal\nsufficiency, i.e. allowing for latent variables which may arise because\nrelevant information could not be measured at all, or not jointly, as in the\ncase of multiple overlapping datasets. We first present novel insights into the\nproperties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce\na new extension of the IOD (integrating overlapping datasets) algorithm\nincorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm.\nWe show that under full usage of the tiered background knowledge tFCI and tIOD\nare sound, while simple versions of the tIOD and tFCI are sound and complete.\nWe further show that the tIOD algorithm can often be expected to be\nconsiderably more efficient and informative than the IOD algorithm even beyond\nthe obvious restriction of the Markov equivalence classes. We provide a formal\nresult on the conditions for this gain in efficiency and informativeness. Our\nresults are accompanied by a series of examples illustrating the exact role and\nusefulness of tiered background knowledge."}
{"id": "2504.10208", "pdf": "https://arxiv.org/pdf/2504.10208", "abs": "https://arxiv.org/abs/2504.10208", "authors": ["Erxue Min", "Hsiu-Yuan Huang", "Min Yang", "Xihong Yang", "Xin Jia", "Yunfang Wu", "Hengyi Cai", "Junfeng Wang", "Shuaiqiang Wang", "Dawei Yin"], "title": "From Prompting to Alignment: A Generative Framework for Query Recommendation", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "In modern search systems, search engines often suggest relevant queries to\nusers through various panels or components, helping refine their information\nneeds. Traditionally, these recommendations heavily rely on historical search\nlogs to build models, which suffer from cold-start or long-tail issues.\nFurthermore, tasks such as query suggestion, completion or clarification are\nstudied separately by specific design, which lacks generalizability and hinders\nadaptation to novel applications. Despite recent attempts to explore the use of\nLLMs for query recommendation, these methods mainly rely on the inherent\nknowledge of LLMs or external sources like few-shot examples, retrieved\ndocuments, or knowledge bases, neglecting the importance of the calibration and\nalignment with user feedback, thus limiting their practical utility. To address\nthese challenges, we first propose a general Generative Query Recommendation\n(GQR) framework that aligns LLM-based query generation with user preference.\nSpecifically, we unify diverse query recommendation tasks by a universal prompt\nframework, leveraging the instruct-following capability of LLMs for effective\ngeneration. Secondly, we align LLMs with user feedback via presenting a\nCTR-alignment framework, which involves training a query-wise CTR predictor as\na process reward model and employing list-wise preference alignment to maximize\nthe click probability of the generated query list. Furthermore, recognizing the\ninconsistency between LLM knowledge and proactive search intents arising from\nthe separation of user-initiated queries from models, we align LLMs with user\ninitiative via retrieving co-occurrence queries as side information when\nhistorical logs are available."}
{"id": "2504.19657", "pdf": "https://arxiv.org/pdf/2504.19657", "abs": "https://arxiv.org/abs/2504.19657", "authors": ["Shotaro Takasu", "Toshio Aoyagi"], "title": "Neuronal correlations shape the scaling behavior of memory capacity and nonlinear computational capability of recurrent neural networks", "categories": ["cond-mat.dis-nn", "cs.LG", "q-bio.NC"], "comment": "20 pages, 8 figures", "summary": "Reservoir computing is a powerful framework for real-time information\nprocessing, characterized by its high computational ability and quick learning,\nwith applications ranging from machine learning to biological systems. In this\npaper, we demonstrate that the memory capacity of a reservoir recurrent neural\nnetwork scales sublinearly with the number of readout neurons. To elucidate\nthis phenomenon, we develop a theoretical framework for analytically deriving\nmemory capacity, attributing the decaying growth of memory capacity to neuronal\ncorrelations. In addition, numerical simulations reveal that once memory\ncapacity becomes sublinear, increasing the number of readout neurons\nsuccessively enables nonlinear processing at progressively higher polynomial\norders. Furthermore, our theoretical framework suggests that neuronal\ncorrelations govern not only memory capacity but also the sequential growth of\nnonlinear computational capabilities. Our findings establish a foundation for\ndesigning scalable and cost-effective reservoir computing, providing novel\ninsights into the interplay among neuronal correlations, linear memory, and\nnonlinear processing."}
{"id": "2504.20127", "pdf": "https://arxiv.org/pdf/2504.20127", "abs": "https://arxiv.org/abs/2504.20127", "authors": ["Huiyang Hong", "Xinkai Wu", "Hongyu Sun", "Chaoyang Xie", "Qi Wang", "Yuquan Li"], "title": "Learning Hierarchical Interaction for Accurate Molecular Property Prediction", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Discovering molecules with desirable molecular properties, including ADMET\nprofiles, is of great importance in drug discovery. Existing approaches\ntypically employ deep learning models, such as Graph Neural Networks (GNNs) and\nTransformers, to predict these molecular properties by learning from diverse\nchemical information. However, these models often fail to efficiently capture\nand utilize the hierarchical nature of molecular structures, and often lack\nmechanisms for effective interaction among multi-level features. To address\nthese limitations, we propose a Hierarchical Interaction Message Passing\nMechanism, which serves as the foundation of our novel model, the Hierarchical\nInteraction Message Net (HimNet). Our method enables interaction-aware\nrepresentation learning across atomic, motif, and molecular levels via\nhierarchical attention-guided message passing. This design allows HimNet to\neffectively balance global and local information, ensuring rich and\ntask-relevant feature extraction for downstream property prediction tasks, such\nas Blood-Brain Barrier Permeability (BBBP). We systematically evaluate HimNet\non eleven datasets, including eight widely-used MoleculeNet benchmarks and\nthree challenging, high-value datasets for metabolic stability, malaria\nactivity, and liver microsomal clearance, covering a broad range of\npharmacologically relevant properties. Extensive experiments demonstrate that\nHimNet achieves the best or near-best performance in most molecular property\nprediction tasks. Furthermore, our method exhibits promising hierarchical\ninterpretability, aligning well with chemical intuition on representative\nmolecules. We believe that HimNet offers an accurate and efficient solution for\nmolecular activity and ADMET property prediction, contributing to advanced\ndecision-making in the early stages of drug discovery."}
{"id": "2504.20395", "pdf": "https://arxiv.org/pdf/2504.20395", "abs": "https://arxiv.org/abs/2504.20395", "authors": ["Tiantian Zhang"], "title": "Partial Answer of How Transformers Learn Automata", "categories": ["cs.FL", "cs.LG"], "comment": null, "summary": "We introduce a novel framework for simulating finite automata using\nrepresentation-theoretic semidirect products and Fourier modules, achieving\nmore efficient Transformer-based implementations."}
{"id": "2505.04886", "pdf": "https://arxiv.org/pdf/2505.04886", "abs": "https://arxiv.org/abs/2505.04886", "authors": ["Mukund Telukunta", "Venkata Sriram Siddhardh Nadendla", "Morgan Stuart", "Casey Canfield"], "title": "Fairness Perceptions in Regression-based Predictive Models", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Regression-based predictive analytics used in modern kidney transplantation\nis known to inherit biases from training data. This leads to social\ndiscrimination and inefficient organ utilization, particularly in the context\nof a few social groups. Despite this concern, there is limited research on\nfairness in regression and its impact on organ utilization and placement. This\npaper introduces three novel divergence-based group fairness notions: (i)\nindependence, (ii) separation, and (iii) sufficiency to assess the fairness of\nregression-based analytics tools. In addition, fairness preferences are\ninvestigated from crowd feedback, in order to identify a socially accepted\ngroup fairness criterion for evaluating these tools. A total of 85 participants\nwere recruited from the Prolific crowdsourcing platform, and a Mixed-Logit\ndiscrete choice model was used to model fairness feedback and estimate social\nfairness preferences. The findings clearly depict a strong preference towards\nthe separation and sufficiency fairness notions, and that the predictive\nanalytics is deemed fair with respect to gender and race groups, but unfair in\nterms of age groups."}
{"id": "2505.05713", "pdf": "https://arxiv.org/pdf/2505.05713", "abs": "https://arxiv.org/abs/2505.05713", "authors": ["Jinkun Lin", "Ziheng Jiang", "Zuquan Song", "Sida Zhao", "Menghan Yu", "Zhanghan Wang", "Chenyuan Wang", "Zuocheng Shi", "Xiang Shi", "Wei Jia", "Zherui Liu", "Shuguang Wang", "Haibin Lin", "Xin Liu", "Aurojit Panda", "Jinyang Li"], "title": "Understanding Stragglers in Large Model Training Using What-if Analysis", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Large language model (LLM) training is one of the most demanding distributed\ncomputations today, often requiring thousands of GPUs with frequent\nsynchronization across machines. Such a workload pattern makes it susceptible\nto stragglers, where the training can be stalled by few slow workers. At\nByteDance we find stragglers are not trivially always caused by hardware\nfailures, but can arise from multiple complex factors. This work aims to\npresent a comprehensive study on the straggler issues in LLM training, using a\nfive-month trace collected from our ByteDance LLM training cluster. The core\nmethodology is what-if analysis that simulates the scenario without any\nstragglers and contrasts with the actual case. We use this method to study the\nfollowing questions: (1) how often do stragglers affect training jobs, and what\neffect do they have on job performance; (2) do stragglers exhibit temporal or\nspatial patterns; and (3) what are the potential root causes for stragglers?"}
