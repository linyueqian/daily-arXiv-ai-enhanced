<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 110]
- [cs.CV](#cs.CV) [Total: 231]
- [cs.AI](#cs.AI) [Total: 53]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.LG](#cs.LG) [Total: 172]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 6]
- [eess.IV](#eess.IV) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages](https://arxiv.org/abs/2504.18560)
*Alessio Buscemi, Cédric Lothritz, Sergio Morales, Marcos Gomez-Vazquez, Robert Clarisó, Jordi Cabot, German Castignani*

Main category: cs.CL

TL;DR: MLA-BiTe is a framework for multilingual bias testing in LLMs, improving evaluation methods by using automated translation and paraphrasing.


<details>
  <summary>Details</summary>
Motivation: LLMs often perpetuate social biases from training data, necessitating better evaluation methods.

Method: MLA-BiTe employs automated translation and paraphrasing to test biases in multiple languages, including low-resource ones.

Result: The framework was tested on four LLMs across six languages, focusing on seven sensitive discrimination categories.

Conclusion: MLA-BiTe provides a systematic approach to multilingual bias testing, enhancing bias evaluation in LLMs.

Abstract: Large Language Models (LLMs) have exhibited impressive natural language
processing capabilities but often perpetuate social biases inherent in their
training data. To address this, we introduce MultiLingual Augmented Bias
Testing (MLA-BiTe), a framework that improves prior bias evaluation methods by
enabling systematic multilingual bias testing. MLA-BiTe leverages automated
translation and paraphrasing techniques to support comprehensive assessments
across diverse linguistic settings. In this study, we evaluate the
effectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six
languages -- including two low-resource languages -- focusing on seven
sensitive categories of discrimination.

</details>


### [2] [Span-Level Hallucination Detection for LLM-Generated Answers](https://arxiv.org/abs/2504.18639)
*Passant Elchafei, Mervet Abu-Elkheir*

Main category: cs.CL

TL;DR: A framework for detecting hallucinated spans in LLM-generated answers using Semantic Role Labeling and DeBERTa-based entailment, tested on Mu-SHROOM with GPT-4 and LLaMA verification.


<details>
  <summary>Details</summary>
Motivation: Improving factual consistency in LLM-generated answers by detecting hallucinated spans.

Method: Integrates Semantic Role Labeling (SRL) to decompose answers, compares with retrieved context using DeBERTa-based entailment, and refines scores with token-level confidence measures.

Result: Competitive performance on Mu-SHROOM dataset; hallucinated spans verified via GPT-4 and LLaMA fact-checking.

Conclusion: The framework effectively improves hallucination detection in LLM-generated responses.

Abstract: Detecting spans of hallucination in LLM-generated answers is crucial for
improving factual consistency. This paper presents a span-level hallucination
detection framework for the SemEval-2025 Shared Task, focusing on English and
Arabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose
the answer into atomic roles, which are then compared with a retrieved
reference context obtained via question-based LLM prompting. Using a
DeBERTa-based textual entailment model, we evaluate each role semantic
alignment with the retrieved context. The entailment scores are further refined
through token-level confidence measures derived from output logits, and the
combined scores are used to detect hallucinated spans. Experiments on the
Mu-SHROOM dataset demonstrate competitive performance. Additionally,
hallucinated spans have been verified through fact-checking by prompting GPT-4
and LLaMA. Our findings contribute to improving hallucination detection in
LLM-generated responses.

</details>


### [3] [Can Third-parties Read Our Emotions?](https://arxiv.org/abs/2504.18673)
*Jiayi Li, Yingfan Zhou, Pranav Narayanan Venkit, Halima Binte Islam, Sneha Arya, Shomir Wilson, Sarah Rajtmajer*

Main category: cs.CL

TL;DR: The paper examines the accuracy of third-party annotations (human and LLMs) in capturing authors' private states (emotions/opinions) compared to first-party labels, finding significant limitations but better performance by LLMs. It suggests demographic similarity improves annotation quality and proposes a framework for better practices.


<details>
  <summary>Details</summary>
Motivation: To evaluate the assumption that third-party annotators accurately capture authors' private states, which is crucial for NLP tasks like emotion recognition.

Method: Human subjects experiments comparing third-party (human and LLMs) and first-party annotations, exploring demographic similarity and prompting strategies.

Result: Third-party annotations (human and LLMs) have limitations in representing authors' private states, though LLMs outperform humans. Demographic similarity improves human annotation quality, and demographic prompts marginally enhance LLMs.

Conclusion: The study highlights the need for refined annotation practices to better represent authors' private states, proposing a framework for evaluation and improvement.

Abstract: Natural Language Processing tasks that aim to infer an author's private
states, e.g., emotions and opinions, from their written text, typically rely on
datasets annotated by third-party annotators. However, the assumption that
third-party annotators can accurately capture authors' private states remains
largely unexamined. In this study, we present human subjects experiments on
emotion recognition tasks that directly compare third-party annotations with
first-party (author-provided) emotion labels. Our findings reveal significant
limitations in third-party annotations-whether provided by human annotators or
large language models (LLMs)-in faithfully representing authors' private
states. However, LLMs outperform human annotators nearly across the board. We
further explore methods to improve third-party annotation quality. We find that
demographic similarity between first-party authors and third-party human
annotators enhances annotation performance. While incorporating first-party
demographic information into prompts leads to a marginal but statistically
significant improvement in LLMs' performance. We introduce a framework for
evaluating the limitations of third-party annotations and call for refined
annotation practices to accurately represent and model authors' private states.

</details>


### [4] [Spatial Speech Translation: Translating Across Space With Binaural Hearables](https://arxiv.org/abs/2504.18715)
*Tuochao Chen, Qirui Wang, Runlin He, Shyam Gollakota*

Main category: cs.CL

TL;DR: A novel spatial speech translation system for hearables preserves speaker direction and voice characteristics while translating languages in real-time, achieving high BLEU scores despite interference.


<details>
  <summary>Details</summary>
Motivation: To enable seamless communication in multilingual crowded spaces by translating speech while maintaining spatial auditory cues.

Method: Combines blind source separation, localization, real-time expressive translation, and binaural rendering on Apple M2 silicon.

Result: Achieves a BLEU score of 22.01 in noisy environments and effectively renders translated speech spatially.

Conclusion: This work pioneers spatial perception integration in speech translation, validated by user studies in real-world settings.

Abstract: Imagine being in a crowded space where people speak a different language and
having hearables that transform the auditory space into your native language,
while preserving the spatial cues for all speakers. We introduce spatial speech
translation, a novel concept for hearables that translate speakers in the
wearer's environment, while maintaining the direction and unique voice
characteristics of each speaker in the binaural output. To achieve this, we
tackle several technical challenges spanning blind source separation,
localization, real-time expressive translation, and binaural rendering to
preserve the speaker directions in the translated audio, while achieving
real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation
with a prototype binaural headset shows that, unlike existing models, which
fail in the presence of interference, we achieve a BLEU score of up to 22.01
when translating between languages, despite strong interference from other
speakers in the environment. User studies further confirm the system's
effectiveness in spatially rendering the translated speech in previously unseen
real-world reverberant environments. Taking a step back, this work marks the
first step towards integrating spatial perception into speech translation.

</details>


### [5] [Building UD Cairo for Old English in the Classroom](https://arxiv.org/abs/2504.18718)
*Lauren Levine, Junghyun Min, Amir Zeldes*

Main category: cs.CL

TL;DR: A sample Old English treebank was created using UD Cairo sentences, combining LLM prompting and authentic data searches. Beginner annotators, despite limited UD knowledge, produced good results through collaboration. LLM outputs required post-editing for accuracy, and parsing experiments showed improved performance with annotated features.


<details>
  <summary>Details</summary>
Motivation: To create a sample Old English treebank for educational purposes in Historical Linguistics, leveraging classroom involvement and modern tools like LLMs.

Method: Combined LLM prompting and authentic Old English data searches to collect sentences. Multiple beginner annotators compared and adjudicated annotations. Parsing experiments used Modern English training data and annotated features.

Result: LLM outputs lacked authentic syntax but improved with post-editing. Beginner annotators collectively produced good results. Parsing performance on Old English was poor but improved with annotated features.

Conclusion: Collaborative annotation and post-editing can mitigate limitations of LLMs and beginner annotators. Annotated features enhance parsing performance, suggesting potential for further research.

Abstract: In this paper we present a sample treebank for Old English based on the UD
Cairo sentences, collected and annotated as part of a classroom curriculum in
Historical Linguistics. To collect the data, a sample of 20 sentences
illustrating a range of syntactic constructions in the world's languages, we
employ a combination of LLM prompting and searches in authentic Old English
data. For annotation we assigned sentences to multiple students with limited
prior exposure to UD, whose annotations we compare and adjudicate. Our results
suggest that while current LLM outputs in Old English do not reflect authentic
syntax, this can be mitigated by post-editing, and that although beginner
annotators do not possess enough background to complete the task perfectly,
taken together they can produce good results and learn from the experience. We
also conduct preliminary parsing experiments using Modern English training
data, and find that although performance on Old English is poor, parsing on
annotated features (lemma, hyperlemma, gloss) leads to improved performance.

</details>


### [6] [EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers](https://arxiv.org/abs/2504.18736)
*Jianyou Wang, Weili Cao, Kaicheng Wang, Xiaoyue Wang, Ashish Dalvi, Gino Prasad, Qishan Liang, Hsuan-lin Her, Ming Wang, Qin Yang, Gene W. Yeo, David E. Neal, Maxim Khan, Christopher D. Rosin, Ramamohan Paturi, Leon Bergen*

Main category: cs.CL

TL;DR: The paper introduces EvidenceBench, a benchmark for evaluating models on finding evidence for biomedical hypotheses, validated by expert annotations, and shows current models fall short of expert performance.


<details>
  <summary>Details</summary>
Motivation: Automating the process of finding relevant evidence for biomedical hypotheses is crucial for research efficiency, but current models lack expert-level accuracy.

Method: A novel pipeline for hypothesis generation and sentence-by-sentence annotation of biomedical papers, validated by human experts, was used to create EvidenceBench.

Result: Models evaluated on EvidenceBench perform significantly below expert level. A larger dataset, EvidenceBench-100k, was also created for scalability.

Conclusion: EvidenceBench provides a reliable benchmark for evidence-finding tasks, highlighting the gap between model and expert performance, and offers a scalable dataset for future research.

Abstract: We study the task of automatically finding evidence relevant to hypotheses in
biomedical papers. Finding relevant evidence is an important step when
researchers investigate scientific hypotheses. We introduce EvidenceBench to
measure models performance on this task, which is created by a novel pipeline
that consists of hypothesis generation and sentence-by-sentence annotation of
biomedical papers for relevant evidence, completely guided by and faithfully
following existing human experts judgment. We demonstrate the pipeline's
validity and accuracy with multiple sets of human-expert annotations. We
evaluated a diverse set of language models and retrieval systems on the
benchmark and found that model performances still fall significantly short of
the expert level on this task. To show the scalability of our proposed
pipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated
papers with hypotheses to facilitate model training and development. Both
datasets are available at https://github.com/EvidenceBench/EvidenceBench

</details>


### [7] [SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning](https://arxiv.org/abs/2504.18762)
*Ojasw Upadhyay, Abishek Saravankumar, Ayman Ismail*

Main category: cs.CL

TL;DR: SynLexLM is a novel method for pre-training legal LLMs using curriculum learning and synthetic data augmentation to improve performance on legal tasks.


<details>
  <summary>Details</summary>
Motivation: General-purpose LLMs lack legal nuance, and acquiring sufficient legal data is challenging. SynLexLM aims to address these issues.

Method: Uses curriculum learning (simple to complex legal texts) and synthetic data augmentation (e.g., Gemini Pro) to pre-train the model.

Result: Aims to outperform traditional models and fine-tuned versions on legal benchmarks like BigLaw-Bench and EUR-Lex-Sum.

Conclusion: SynLexLM could enhance legal document analysis and democratize access to advanced legal AI tools.

Abstract: Large Language Models (LLMs) are powerful but often require extensive
fine-tuning and large datasets for specialized domains like law.
General-purpose pre-training may not capture legal nuances, and acquiring
sufficient legal data is challenging. We introduce SynLexLM, a novel approach
to efficiently pre-train a legal LLM. Our method employs curriculum learning,
progressing from simple to complex legal texts and queries, combined with
synthetic data augmentation using models like Gemini Pro to address data
scarcity. We aim to achieve improved performance on legal benchmarks
(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned
versions. Preliminary work involves generating synthetic QA pairs reflecting
legal reasoning. This work aims to enhance legal document analysis and research
tools, potentially democratizing access to advanced legal AI.

</details>


### [8] [Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation](https://arxiv.org/abs/2504.18805)
*Jong Inn Park, Maanas Taneja, Qianwen Wang, Dongyeop Kang*

Main category: cs.CL

TL;DR: SciTalk is a multi-LLM framework for generating accurate and engaging short-form videos from scientific papers, using iterative feedback to improve quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for video generation from papers often produce inaccuracies and visual flaws, hindering scientific dissemination.

Method: SciTalk employs specialized agents for summarization, scene planning, and editing, with iterative feedback from simulated users.

Result: SciTalk outperforms simple prompting methods in accuracy and engagement, though it doesn't yet match human creators.

Conclusion: The framework offers insights into feedback-driven video generation, with plans to release code, data, and videos.

Abstract: Generating engaging, accurate short-form videos from scientific papers is
challenging due to content complexity and the gap between expert authors and
readers. Existing end-to-end methods often suffer from factual inaccuracies and
visual artifacts, limiting their utility for scientific dissemination. To
address these issues, we propose SciTalk, a novel multi-LLM agentic framework,
grounding videos in various sources, such as text, figures, visual styles, and
avatars. Inspired by content creators' workflows, SciTalk uses specialized
agents for content summarization, visual scene planning, and text and layout
editing, and incorporates an iterative feedback mechanism where video agents
simulate user roles to give feedback on generated videos from previous
iterations and refine generation prompts. Experimental evaluations show that
SciTalk outperforms simple prompting methods in generating scientifically
accurate and engaging content over the refined loop of video generation.
Although preliminary results are still not yet matching human creators'
quality, our framework provides valuable insights into the challenges and
benefits of feedback-driven video generation. Our code, data, and generated
videos will be publicly available.

</details>


### [9] [Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks](https://arxiv.org/abs/2504.18838)
*Yixin Cao, Shibo Hong, Xinze Li, Jiahao Ying, Yubo Ma, Haiyuan Liang, Yantao Liu, Zijun Yao, Xiaozhi Wang, Dan Huang, Wenxuan Zhang, Lifu Huang, Muhao Chen, Lei Hou, Qianru Sun, Xingjun Ma, Zuxuan Wu, Min-Yen Kan, David Lo, Qi Zhang, Heng Ji, Jing Jiang, Juanzi Li, Aixin Sun, Xuanjing Huang, Tat-Seng Chua, Yu-Gang Jiang*

Main category: cs.CL

TL;DR: The survey explores challenges in evaluating Large Language Models (LLMs), focusing on transitions to capability-based and automated evaluation, while addressing the persistent issue of evaluation generalization.


<details>
  <summary>Details</summary>
Motivation: To address the rapid advancement of LLMs and the need for updated evaluation methods that match their growing capabilities.

Method: Analyzes transitions from task-specific to capability-based evaluation and from manual to automated evaluation, including dynamic datasets and LLM-as-a-judge scoring.

Result: Identifies the evaluation generalization issue as a major obstacle, where bounded test sets fail to scale with LLMs' expanding abilities.

Conclusion: Proposes ongoing updates via a GitHub repository to keep pace with the evolving field, inviting collaboration.

Abstract: Large Language Models (LLMs) are advancing at an amazing speed and have
become indispensable across academia, industry, and daily applications. To keep
pace with the status quo, this survey probes the core challenges that the rise
of LLMs poses for evaluation. We identify and analyze two pivotal transitions:
(i) from task-specific to capability-based evaluation, which reorganizes
benchmarks around core competencies such as knowledge, reasoning, instruction
following, multi-modal understanding, and safety; and (ii) from manual to
automated evaluation, encompassing dynamic dataset curation and
"LLM-as-a-judge" scoring.
  Yet, even with these transitions, a crucial obstacle persists: the evaluation
generalization issue. Bounded test sets cannot scale alongside models whose
abilities grow seemingly without limit. We will dissect this issue, along with
the core challenges of the above two transitions, from the perspectives of
methods, datasets, evaluators, and metrics. Due to the fast evolving of this
field, we will maintain a living GitHub repository (links are in each section)
to crowd-source updates and corrections, and warmly invite contributors and
collaborators.

</details>


### [10] [Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning](https://arxiv.org/abs/2504.18839)
*Abdellah Ghassel, Xianzhi Li, Xiaodan Zhu*

Main category: cs.CL

TL;DR: The paper proposes a method to detect and mitigate dialogue breakdowns in LLM-driven systems using fine-tuning and advanced prompting, achieving improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce incoherent or contradictory responses (breakdowns), undermining user trust. The paper aims to address this issue.

Method: Combines specialized fine-tuning with advanced prompting strategies (few-shot learning, chain-of-thought reasoning, analogical prompting) on an 8B model.

Result: Achieves 7% accuracy improvement on BETOLD dataset and reduces operational costs by selectively escalating responses.

Conclusion: The approach offers a scalable, efficient, and reliable solution for robust conversational AI.

Abstract: Large language models (LLMs) are rapidly changing various domains. However,
their capabilities in handling conversational breakdowns still require an
in-depth exploration. This paper addresses the challenge of detecting and
mitigating dialogue breakdowns within LLM-driven conversational systems. While
powerful models from OpenAI and Anthropic excel in many dialogue tasks, they
can still produce incoherent or contradictory responses, commonly referred to
as breakdowns, which undermine user trust. To tackle this, we propose an
approach that combines specialized fine-tuning with advanced prompting
strategies, including few-shot learning, chain-of-thought reasoning, and
analogical prompting. In particular, we fine-tune a small 8B model and
demonstrate its robust classification and calibration capabilities in English
and Japanese dialogue. We also validate its generalization on the BETOLD
dataset, achieving a 7\% accuracy improvement over its base model. Furthermore,
we introduce a real-time deployment architecture that selectively escalates
suspicious responses to more resource-intensive frontier models only when
breakdowns are detected, significantly cutting operational expenses and energy
consumption. Experimental results show our method surpasses prior
state-of-the-art specialized classifiers while also narrowing performance gaps
between smaller open-source models and large proprietary ones. Our approach
offers a scalable solution for robust conversational AI in high-impact domains
by combining efficiency, interpretability, and reliability.

</details>


### [11] [Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking](https://arxiv.org/abs/2504.19940)
*Luigia Costabile, Gian Marco Orlando, Valerio La Gatta, Vincenzo Moscato*

Main category: cs.CL

TL;DR: LLM-powered generative agents outperform human crowds in fact-checking tasks, showing higher consistency and reduced bias.


<details>
  <summary>Details</summary>
Motivation: The need for scalable, reliable fact-checking solutions due to online misinformation, and the unexplored potential of LLMs in crowdsourced workflows.

Method: Simulated crowds of generative agents with diverse profiles to perform fact-checking tasks using evidence retrieval and veracity judgments.

Result: Agent crowds outperformed humans in truthfulness classification, consistency, and bias reduction, relying more on structured criteria.

Conclusion: Generative agents are promising for scalable, consistent, and less biased crowd-based fact-checking.

Abstract: The growing spread of online misinformation has created an urgent need for
scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where
non-experts evaluate claim veracity - offers a cost-effective alternative to
expert verification, despite concerns about variability in quality and bias.
Encouraged by promising results in certain contexts, major platforms such as X
(formerly Twitter), Facebook, and Instagram have begun shifting from
centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong
performance across core fact-checking tasks, including claim detection and
evidence evaluation. However, their potential role in crowdsourced workflows
remains unexplored. This paper investigates whether LLM-powered generative
agents - autonomous entities that emulate human behavior and decision-making -
can meaningfully contribute to fact-checking tasks traditionally reserved for
human crowds. Using the protocol of La Barbera et al. (2024), we simulate
crowds of generative agents with diverse demographic and ideological profiles.
Agents retrieve evidence, assess claims along multiple quality dimensions, and
issue final veracity judgments.
  Our results show that agent crowds outperform human crowds in truthfulness
classification, exhibit higher internal consistency, and show reduced
susceptibility to social and cognitive biases. Compared to humans, agents rely
more systematically on informative criteria such as Accuracy, Precision, and
Informativeness, suggesting a more structured decision-making process. Overall,
our findings highlight the potential of generative agents as scalable,
consistent, and less biased contributors to crowd-based fact-checking systems.

</details>


### [12] [When2Call: When (not) to Call Tools](https://arxiv.org/abs/2504.18851)
*Hayley Ross, Ameya Sunil Mahabaleshwarkar, Yoshi Suhara*

Main category: cs.CL

TL;DR: The paper introduces When2Call, a benchmark evaluating LMs' decision-making on tool usage, highlighting gaps in current benchmarks and proposing a preference optimization training method.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on tool-calling accuracy but neglect evaluating when LMs should or shouldn't call tools, prompting the need for When2Call.

Method: Developed When2Call benchmark and training set, leveraging multiple-choice questions for preference optimization training.

Result: State-of-the-art LMs show significant improvement potential on When2Call, with preference optimization outperforming traditional fine-tuning.

Conclusion: When2Call addresses a critical gap in LM tool-calling evaluation, offering a benchmark and training method for better decision-making.

Abstract: Leveraging external tools is a key feature for modern Language Models (LMs)
to expand their capabilities and integrate them into existing systems. However,
existing benchmarks primarily focus on the accuracy of tool calling -- whether
the correct tool is called with the correct parameters -- and less on
evaluating when LMs should (not) call tools. We develop a new benchmark,
When2Call, which evaluates tool-calling decision-making: when to generate a
tool call, when to ask follow-up questions and when to admit the question can't
be answered with the tools provided. We find that state-of-the-art tool-calling
LMs show significant room for improvement on When2Call, indicating the
importance of this benchmark. We also develop a training set for When2Call and
leverage the multiple-choice nature of the benchmark to develop a preference
optimization training regime, which shows considerably more improvement than
traditional fine-tuning. We release the benchmark and training data as well as
evaluation scripts at https://github.com/NVIDIA/When2Call.

</details>


### [13] [Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation](https://arxiv.org/abs/2504.18857)
*Yi Lu, Wanxu Zhao, Xin Zhou, Chenxin An, Chenglong Wang, Shuo Li, Yuming Yang, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang*

Main category: cs.CL

TL;DR: DPE is a training-free framework to extend LLMs' context window by optimizing RoPE's hidden dimensions, outperforming baselines and improving performance within training length.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long-context processing, and existing methods require costly training. DPE aims to extend context windows without retraining.

Method: DPE analyzes RoPE's dimensions, identifies key ones for extension, and adjusts their position indices for optimal extrapolation, reusing pre-trained embeddings.

Result: DPE enables Llama3-8k 8B to handle 128k tokens without training, improves Llama3.1 70B by 18 points on benchmarks, and surpasses GPT-4-128K.

Conclusion: DPE offers an efficient, training-free solution for long-context extension, outperforming existing methods and enhancing model performance.

Abstract: Large Language Models (LLMs) often struggle to process and generate coherent
context when the number of input tokens exceeds the pre-trained length. Recent
advancements in long-context extension have significantly expanded the context
window of LLMs but require expensive overhead to train the large-scale models
with longer context. In this work, we propose Dimension-Wise Positional
Embeddings Manipulation (DPE), a training-free framework to extrapolate the
context window of LLMs by diving into RoPE's different hidden dimensions.
Instead of manipulating all dimensions equally, DPE detects the effective
length for every dimension and finds the key dimensions for context extension.
We reuse the original position indices with their embeddings from the
pre-trained model and manipulate the key dimensions' position indices to their
most effective lengths. In this way, DPE adjusts the pre-trained models with
minimal modifications while ensuring that each dimension reaches its optimal
state for extrapolation. DPE significantly surpasses well-known baselines such
as YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of
128k tokens without continual training and integrates seamlessly with Flash
Attention 2. In addition to its impressive extrapolation capability, DPE also
dramatically improves the models' performance within training length, such as
Llama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When
compared with commercial models, Llama 3.1 70B with DPE even achieves better
performance than GPT-4-128K.

</details>


### [14] [Latent Adversarial Training Improves the Representation of Refusal](https://arxiv.org/abs/2504.18872)
*Alexandra Abbas, Nora Petrova, Helios Ael Lyons, Natalia Perez-Campanero*

Main category: cs.CL

TL;DR: LAT alters refusal behavior encoding in language models, concentrating it in fewer SVD components, improving robustness but increasing vulnerability to self-generated attacks.


<details>
  <summary>Details</summary>
Motivation: To understand how LAT affects the latent representation of refusal behavior in language models, evaluating its effectiveness and limitations.

Method: Analyzed Llama 2 7B using activation differences and SVD to compare LAT with SSFT and AT.

Result: LAT concentrates refusal behavior in the first two SVD components (75% variance), improving robustness to external attacks but increasing vulnerability to self-generated vectors.

Conclusion: LAT's noise-based training enhances refusal representation but reveals trade-offs in robustness, highlighting its potential and risks for model safety.

Abstract: Recent work has shown that language models' refusal behavior is primarily
encoded in a single direction in their latent space, making it vulnerable to
targeted attacks. Although Latent Adversarial Training (LAT) attempts to
improve robustness by introducing noise during training, a key question
remains: How does this noise-based training affect the underlying
representation of refusal behavior? Understanding this encoding is crucial for
evaluating LAT's effectiveness and limitations, just as the discovery of linear
refusal directions revealed vulnerabilities in traditional supervised safety
fine-tuning (SSFT).
  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the
refusal behavior in the model's latent space compared to SSFT and embedding
space adversarial training (AT). By computing activation differences between
harmful and harmless instruction pairs and applying Singular Value
Decomposition (SVD), we find that LAT significantly alters the refusal
representation, concentrating it in the first two SVD components which explain
approximately 75 percent of the activation differences variance - significantly
higher than in reference models. This concentrated representation leads to more
effective and transferable refusal vectors for ablation attacks: LAT models
show improved robustness when attacked with vectors from reference models but
become more vulnerable to self-generated vectors compared to SSFT and AT. Our
findings suggest that LAT's training perturbations enable a more comprehensive
representation of refusal behavior, highlighting both its potential strengths
and vulnerabilities for improving model safety.

</details>


### [15] [A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification](https://arxiv.org/abs/2504.18884)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: Ensemble strategy for LLMs improves sentiment analysis robustness and accuracy, reducing RMSE by 18.6%.


<details>
  <summary>Details</summary>
Motivation: Address variability and reproducibility issues in LLM results, inspired by human annotation's majority voting.

Method: Straightforward ensemble strategy using medium-sized LLMs for sentiment analysis.

Result: Ensemble of multiple inferences outperforms single large model, reducing RMSE by 18.6%.

Conclusion: Ensemble approach with medium-sized LLMs enhances reliability and accuracy in sentiment analysis.

Abstract: With the advance of large language models (LLMs), LLMs have been utilized for
the various tasks. However, the issues of variability and reproducibility of
results from each trial of LLMs have been largely overlooked in existing
literature while actual human annotation uses majority voting to resolve
disagreements among annotators. Therefore, this study introduces the
straightforward ensemble strategy to a sentiment analysis using LLMs. As the
results, we demonstrate that the ensemble of multiple inference using
medium-sized LLMs produces more robust and accurate results than using a large
model with a single attempt with reducing RMSE by 18.6%.

</details>


### [16] [MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction](https://arxiv.org/abs/2504.18938)
*Junhong Liang, Yu Zhou*

Main category: cs.CL

TL;DR: The paper introduces MTCSC, a framework for variable-length Chinese Spelling Correction, addressing domain adaptation and output length consistency using RAG and a length reflection mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing CSC methods struggle with domain-specific corrections and rigid length constraints, limiting their applicability.

Method: Proposes MTCSC, leveraging RAG with a retrieval database from domain-specific data and dictionaries, plus a multi-source combination strategy with iterative length reflection.

Result: Outperforms current methods in correction quality, especially for domain-specific and variable-length tasks.

Conclusion: MTCSC effectively addresses limitations of traditional CSC, offering improved flexibility and performance in diverse scenarios.

Abstract: Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens
in sentences. While Large Language Models (LLMs) have shown remarkable success
in identifying and rectifying potential errors, they often struggle with
maintaining consistent output lengths and adapting to domain-specific
corrections. Furthermore, existing CSC task impose rigid constraints requiring
input and output lengths to be identical, limiting their applicability. In this
work, we extend traditional CSC to variable-length correction scenarios,
including Chinese Splitting Error Correction (CSEC) and ASR N-best Error
Correction. To address domain adaptation and length consistency, we propose
MTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection
mechanism. Our approach constructs a retrieval database from domain-specific
training data and dictionaries, fine-tuning retrievers to optimize performance
for error-containing inputs. Additionally, we introduce a multi-source
combination strategy with iterative length reflection to ensure output length
fidelity. Experiments across diverse domain datasets demonstrate that our
method significantly outperforms current approaches in correction quality,
particularly in handling domain-specific and variable-length error correction
tasks.

</details>


### [17] [LawFlow : Collecting and Simulating Lawyers' Thought Processes](https://arxiv.org/abs/2504.18942)
*Debarati Das, Khanh Chi Le, Ritik Sachin Parkar, Karin De Langis, Brendan Madson, Chad M. Berryman, Robin M. Willis, Daniel H. Moses, Brett McDonnell, Daniel Schwarcz, Dongyeop Kang*

Main category: cs.CL

TL;DR: LawFlow introduces a dataset capturing end-to-end legal workflows, revealing differences between human and LLM reasoning. It suggests AI should support, not replace, legal professionals.


<details>
  <summary>Details</summary>
Motivation: Current AI models lack the ability to handle the dynamic, iterative reasoning of real-world legal practice, necessitating a dataset like LawFlow.

Method: LawFlow collects complete legal workflows from trained law students, comparing human and LLM-generated workflows to analyze reasoning differences.

Result: Human workflows are modular and adaptive, while LLM workflows are sequential and less flexible. Legal professionals prefer AI in supportive roles.

Conclusion: AI should assist, not replace, legal workflows, with design suggestions for hybrid planning and adaptive execution to enhance collaboration.

Abstract: Legal practitioners, particularly those early in their careers, face complex,
high-stakes tasks that require adaptive, context-sensitive reasoning. While AI
holds promise in supporting legal work, current datasets and models are
narrowly focused on isolated subtasks and fail to capture the end-to-end
decision-making required in real-world practice. To address this gap, we
introduce LawFlow, a dataset of complete end-to-end legal workflows collected
from trained law students, grounded in real-world business entity formation
scenarios. Unlike prior datasets focused on input-output pairs or linear chains
of thought, LawFlow captures dynamic, modular, and iterative reasoning
processes that reflect the ambiguity, revision, and client-adaptive strategies
of legal practice. Using LawFlow, we compare human and LLM-generated workflows,
revealing systematic differences in structure, reasoning flexibility, and plan
execution. Human workflows tend to be modular and adaptive, while LLM workflows
are more sequential, exhaustive, and less sensitive to downstream implications.
Our findings also suggest that legal professionals prefer AI to carry out
supportive roles, such as brainstorming, identifying blind spots, and surfacing
alternatives, rather than executing complex workflows end-to-end. Building on
these findings, we propose a set of design suggestions, rooted in empirical
observations, that align AI assistance with human goals of clarity,
completeness, creativity, and efficiency, through hybrid planning, adaptive
execution, and decision-point support. Our results highlight both the current
limitations of LLMs in supporting complex legal workflows and opportunities for
developing more collaborative, reasoning-aware legal AI systems. All data and
code are available on our project page
(https://minnesotanlp.github.io/LawFlow-website/).

</details>


### [18] [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://arxiv.org/abs/2504.18992)
*Sanwoo Lee, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Yunfang Wu*

Main category: cs.CL

TL;DR: DF-Merge unifies model-wise and parameter-wise merging into a dynamic framework using Bayesian optimization and Fisher information, outperforming baselines with minimal validation data.


<details>
  <summary>Details</summary>
Motivation: Existing merging approaches have weaknesses, creating a performance gap compared to multi-task fine-tuning. A unified, dynamic method is needed.

Method: DF-Merge dynamically adjusts coefficients for model parameters using Bayesian optimization and Fisher information to maximize performance.

Result: DF-Merge outperforms baselines across models and tasks, achieving near-optimal performance in few iterations with minimal validation data.

Conclusion: DF-Merge provides a unified, efficient merging framework, bridging the performance gap and enabling effective multi-task model creation.

Abstract: The fine-tuning of pre-trained language models has resulted in the widespread
availability of task-specific models. Model merging offers an efficient way to
create multi-task models by combining these fine-tuned models at the parameter
level, without the need for training data or joint training on multiple
datasets. Existing merging approaches typically involve scaling the parameters
model-wise or integrating parameter importance parameter-wise. Both approaches
exhibit their own weaknesses, leading to a notable performance gap compared to
multi-task fine-tuning. In this paper, we unify these seemingly distinct
strategies into a more general merging framework, and introduce Dynamic
Fisher-weighted Merging (DF-Merge). Specifically, candidate models are
associated with a set of coefficients that linearly scale their fine-tuned
parameters. Bayesian optimization is applied to dynamically adjust these
coefficients, aiming to maximize overall performance on validation sets. Each
iteration of this process integrates parameter importance based on the Fisher
information conditioned by the coefficients. Experimental results show that
DF-Merge outperforms strong baselines across models of different sizes and a
variety of tasks. Our analysis shows that the effectiveness of DF-Merge arises
from the unified view of merging and that near-optimal performance is
achievable in a few iterations, even with minimal validation data.

</details>


### [19] [Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs](https://arxiv.org/abs/2504.19019)
*Mohammad Akbar-Tajari, Mohammad Taher Pilehvar, Mohammad Mahmoody*

Main category: cs.CL

TL;DR: GoAT is a method for generating adversarial prompts to test LLM robustness, achieving higher jailbreak success rates with fewer queries and human-readable prompts.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to adversarial jailbreaks, and identifying these vulnerabilities is crucial for improving their alignment with societal standards.

Method: GoAT uses the Graph of Thoughts framework to generate adversarial prompts, refining a graph structure iteratively for better reasoning and attack synergy.

Result: GoAT achieves up to five times better jailbreak success rates than state-of-the-art attacks, even against robust models like Llama, without needing model parameters.

Conclusion: GoAT's dynamic graph-based approach enhances adversarial prompt generation, offering a powerful tool for testing and improving LLM robustness.

Abstract: The challenge of ensuring Large Language Models (LLMs) align with societal
standards is of increasing interest, as these models are still prone to
adversarial jailbreaks that bypass their safety mechanisms. Identifying these
vulnerabilities is crucial for enhancing the robustness of LLMs against such
exploits. We propose Graph of ATtacks (GoAT), a method for generating
adversarial prompts to test the robustness of LLM alignment using the Graph of
Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly
effective jailbreak prompts with fewer queries to the victim model than
state-of-the-art attacks, achieving up to five times better jailbreak success
rate against robust models like Llama. Notably, GoAT creates high-quality,
human-readable prompts without requiring access to the targeted model's
parameters, making it a black-box attack. Unlike approaches constrained by
tree-based reasoning, GoAT's reasoning is based on a more intricate graph
structure. By making simultaneous attack paths aware of each other's progress,
this dynamic framework allows a deeper integration and refinement of reasoning
paths, significantly enhancing the collaborative exploration of adversarial
vulnerabilities in LLMs. At a technical level, GoAT starts with a graph
structure and iteratively refines it by combining and improving thoughts,
enabling synergy between different thought paths. The code for our
implementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.

</details>


### [20] [Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting](https://arxiv.org/abs/2504.19021)
*Zhyar Rzgar K Rostam, Gábor Kertész*

Main category: cs.CL

TL;DR: The paper explores fine-tuning pre-trained language models (PLMs) like BERT, SciBERT, BioBERT, and BlueBERT on the Web of Science dataset for scientific text classification. Dataset augmentation, hard-voting, and dynamic learning rates improve accuracy, with domain-specific models outperforming general ones.


<details>
  <summary>Details</summary>
Motivation: Efficient text classification is needed due to the growing volume of academic publications. The study aims to enhance classification accuracy using PLMs and dataset augmentation.

Method: The study fine-tunes PLMs on the WoS-46985 dataset, augments it with 1,000 articles per category via targeted queries, and uses hard-voting for label prediction. Dynamic learning rates and early stopping are applied during fine-tuning.

Result: Domain-specific models (SciBERT, BioBERT) outperform general-purpose models (BERT). Dataset augmentation and hard-voting significantly boost classification accuracy, especially in specialized domains.

Conclusion: The study highlights the effectiveness of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning for robust and scalable academic text classification.

Abstract: Efficient text classification is essential for handling the increasing volume
of academic publications. This study explores the use of pre-trained language
models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on
the Web of Science (WoS-46985) dataset for scientific text classification. To
enhance performance, we augment the dataset by executing seven targeted queries
in the WoS database, retrieving 1,000 articles per category aligned with
WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a
hard-voting strategy combines predictions for improved accuracy and confidence.
Fine-tuning on the expanded dataset with dynamic learning rates and early
stopping significantly boosts classification accuracy, especially in
specialized domains. Domain-specific models like SciBERT and BioBERT
consistently outperform general-purpose models such as BERT. These findings
underscore the efficacy of dataset augmentation, inference-driven label
prediction, hard-voting, and fine-tuning techniques in creating robust and
scalable solutions for automated academic text classification.

</details>


### [21] [KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation](https://arxiv.org/abs/2504.19024)
*Jiabin Fan, Guoqing Luo, Michael Bowling, Lili Mou*

Main category: cs.CL

TL;DR: KETCHUP, a novel k-step return estimation method for RL-based knowledge distillation in text generation, reduces gradient variance and improves optimization, especially for large student models.


<details>
  <summary>Details</summary>
Motivation: To enhance RL-based knowledge distillation in text generation by reducing gradient variance through a k-step return formulation.

Method: Uses the Bellman Optimality Equation for multiple steps to induce a K-step return, theoretically reducing gradient variance.

Result: Superior performance in text generation tasks, validated by standard metrics and LLM-based evaluation.

Conclusion: KETCHUP offers a promising direction for improving RL-based KD in large language model research.

Abstract: We propose a novel k-step return estimation method (called KETCHUP) for
Reinforcement Learning(RL)-based knowledge distillation (KD) in text generation
tasks. Our idea is to induce a K-step return by using the Bellman Optimality
Equation for multiple steps. Theoretical analysis shows that this K-step
formulation reduces the variance of the gradient estimates, thus leading to
improved RL optimization especially when the student model size is large.
Empirical evaluation on three text generation tasks demonstrates that our
approach yields superior performance in both standard task metrics and large
language model (LLM)-based evaluation. These results suggest that our K-step
return induction offers a promising direction for enhancing RL-based KD in LLM
research.

</details>


### [22] [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/abs/2504.19044)
*Di Wu, Yibin Lei, Christof Monz*

Main category: cs.CL

TL;DR: The paper proposes calibrating hypothesis likelihoods in NMT to align decoding objectives with real-world translation quality, improving performance even on top LLMs.


<details>
  <summary>Details</summary>
Motivation: MAP decoding in NMT often produces low-quality translations due to misalignment with real-world quality.

Method: Calibrate hypothesis likelihoods by optimizing their Pearson correlation with translation quality.

Result: Substantial improvements in translation quality across metrics and human evaluations, even on specialized LLMs.

Conclusion: Calibration enhances MAP decoding efficiency and serves as a strong proxy for translation quality, outperforming some state-of-the-art models.

Abstract: Neural machine translation (NMT) systems typically employ maximum a
posteriori (MAP) decoding to select the highest-scoring translation from the
distribution mass. However, recent evidence highlights the inadequacy of MAP
decoding, often resulting in low-quality or even pathological hypotheses -- the
decoding objective is not aligned with real-world translation quality. This
paper proposes calibrating hypothesis likelihoods with translation quality from
a distribution view by directly optimizing their Pearson correlation -- thereby
enhancing the effectiveness of translation decoding. With our method,
translation on large language models (LLMs) improves substantially after
limited training (2K instances per direction). This improvement is orthogonal
to those achieved through supervised fine-tuning, leading to substantial gains
across a broad range of metrics and human evaluations -- even when applied to
top-performing translation-specialized LLMs fine-tuned on high-quality
translation data, such as Tower, or when compared to recent preference
optimization methods, like CPO. Moreover, the calibrated translation likelihood
can directly serve as a strong proxy for translation quality, closely
approximating or even surpassing some state-of-the-art translation quality
estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates
that calibration enhances the effectiveness of MAP decoding, thereby enabling
greater efficiency in real-world deployment. The resulting state-of-the-art
translation model, which covers 10 languages, along with the accompanying code
and human evaluation data, has been released to the community:
https://github.com/moore3930/calibrating-llm-mt.

</details>


### [23] [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)
*Anindya Bijoy Das, Shibbir Ahmed, Shahnewaz Karim Sakib*

Main category: cs.CL

TL;DR: The paper explores the use of open-source large language models (LLMs) for clinical summarization, focusing on discharge reports, while also evaluating hallucination prevalence to ensure reliability.


<details>
  <summary>Details</summary>
Motivation: Clinical summarization is essential for patient care, and LLMs can automate and improve its accuracy, but hallucinations must be addressed for reliability.

Method: The study evaluates open-source LLMs by extracting key events from discharge reports and assessing hallucination prevalence through numerical simulations.

Result: The research rigorously tests LLM performance in clinical summarization, focusing on accuracy and fidelity of extracted content.

Conclusion: Open-source LLMs show promise for clinical summarization, but hallucination detection is critical for ensuring reliable information transfer in healthcare.

Abstract: Clinical summarization is crucial in healthcare as it distills complex
medical data into digestible information, enhancing patient understanding and
care management. Large language models (LLMs) have shown significant potential
in automating and improving the accuracy of such summarizations due to their
advanced natural language understanding capabilities. These models are
particularly applicable in the context of summarizing medical/clinical texts,
where precise and concise information transfer is essential. In this paper, we
investigate the effectiveness of open-source LLMs in extracting key events from
discharge reports, such as reasons for hospital admission, significant
in-hospital events, and critical follow-up actions. In addition, we also assess
the prevalence of various types of hallucinations in the summaries produced by
these models. Detecting hallucinations is vital as it directly influences the
reliability of the information, potentially affecting patient care and
treatment outcomes. We conduct comprehensive numerical simulations to
rigorously evaluate the performance of these models, further probing the
accuracy and fidelity of the extracted content in clinical summarization.

</details>


### [24] [ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics](https://arxiv.org/abs/2504.19066)
*Deeksha Varshney, Keane Ong, Rui Mao, Erik Cambria, Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: The paper proposes EWRA and ExtremeWeatherNews to enhance small language models (SLMs) for extreme weather analytics, improving categorization, labeling, and emotion analysis.


<details>
  <summary>Details</summary>
Motivation: Localized data on extreme weather is scarce, limiting analysis and decision-making. LLMs can bridge this gap by processing unstructured data and transferring knowledge to SLMs.

Method: Introduces EWRA, a method to align SLMs with reasoning paths from LLMs, and ExtremeWeatherNews, a dataset for training. The framework, ClimaEmpact, addresses categorization, topic labeling, and emotion analysis.

Result: EWRA improves SLMs' domain-specific responses, outperforming task-specific models in extreme weather analytics.

Conclusion: The proposed framework enhances SLMs' real-world applicability for extreme weather tasks, offering better decision-making support.

Abstract: Accurate assessments of extreme weather events are vital for research and
policy, yet localized and granular data remain scarce in many parts of the
world. This data gap limits our ability to analyze potential outcomes and
implications of extreme weather events, hindering effective decision-making.
Large Language Models (LLMs) can process vast amounts of unstructured text
data, extract meaningful insights, and generate detailed assessments by
synthesizing information from multiple sources. Furthermore, LLMs can
seamlessly transfer their general language understanding to smaller models,
enabling these models to retain key knowledge while being fine-tuned for
specific tasks. In this paper, we propose Extreme Weather Reasoning-Aware
Alignment (EWRA), a method that enhances small language models (SLMs) by
incorporating structured reasoning paths derived from LLMs, and
ExtremeWeatherNews, a large dataset of extreme weather event-related news
articles. EWRA and ExtremeWeatherNews together form the overall framework,
ClimaEmpact, that focuses on addressing three critical extreme-weather tasks:
categorization of tangible vulnerabilities/impacts, topic labeling, and emotion
analysis. By aligning SLMs with advanced reasoning strategies on
ExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for
SLM alignment), EWRA improves the SLMs' ability to generate well-grounded and
domain-specific responses for extreme weather analytics. Our results show that
the approach proposed guides SLMs to output domain-aligned responses,
surpassing the performance of task-specific models and offering enhanced
real-world applicability for extreme weather analytics.

</details>


### [25] [Sample-Efficient Language Model for Hinglish Conversational AI](https://arxiv.org/abs/2504.19070)
*Sakshi Singh, Abhinav Prakash, Aakriti Shah, Chaitanya Sachdeva, Sanjana Dumpala*

Main category: cs.CL

TL;DR: The paper introduces a sample-efficient language model for a Hinglish chatbot, addressing challenges like inconsistent spelling and data scarcity by fine-tuning pre-trained models on synthetic and existing datasets.


<details>
  <summary>Details</summary>
Motivation: Hinglish, a Hindi-English code-mixed language, poses computational challenges due to lack of standardization and limited conversational data, necessitating efficient modeling solutions.

Method: Evaluates pre-trained models (Gemma3-4B, Qwen2.5-7B) and fine-tunes them using synthetic dialogues and existing Hinglish datasets to improve performance.

Result: Smaller, fine-tuned models achieve competitive performance in Hinglish conversation generation while remaining computationally efficient.

Conclusion: Fine-tuning smaller models on high-quality code-mixed data is effective for Hinglish chatbot development, balancing performance and efficiency.

Abstract: This paper presents our process for developing a sample-efficient language
model for a conversational Hinglish chatbot. Hinglish, a code-mixed language
that combines Hindi and English, presents a unique computational challenge due
to inconsistent spelling, lack of standardization, and limited quality of
conversational data. This work evaluates multiple pre-trained cross-lingual
language models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning
techniques to improve performance on Hinglish conversational tasks. The
proposed approach integrates synthetically generated dialogues with insights
from existing Hinglish datasets to address data scarcity. Experimental results
demonstrate that models with fewer parameters, when appropriately fine-tuned on
high-quality code-mixed data, can achieve competitive performance for Hinglish
conversation generation while maintaining computational efficiency.

</details>


### [26] [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)
*Jikai Wang, Juntao Li, Lijun Wu, Min Zhang*

Main category: cs.CL

TL;DR: SCoT reduces reasoning latency by 48%-66% for large models like Deepseek-R1-Distill-Qwen-32B while maintaining performance, using a draft model for thought-level drafting and error correction.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models face high costs and latency due to size and lengthy thought chains. Existing methods focus on parameter reduction or shortening chains, but SCoT addresses latency by improving reasoning speed through model collaboration.

Method: SCoT uses a lightweight draft model for thought-level drafting, selects the best draft, and corrects errors with the target model. It aligns thinking behavior for efficiency and maintains accuracy with a draft selection strategy.

Result: Experiments on GSM8K, MATH, GaoKao, CollegeMath, and Olympiad datasets show SCoT reduces latency by 48%-66% while achieving near-target-model performance.

Conclusion: SCoT effectively reduces reasoning latency without compromising accuracy, demonstrating the potential of collaborative model approaches for efficient reasoning.

Abstract: Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have
recently attracted widespread attention due to their impressive task-solving
abilities. However, the enormous model size and the generation of lengthy
thought chains introduce significant reasoning costs and response latency.
Existing methods for efficient reasoning mainly focus on reducing the number of
model parameters or shortening the chain-of-thought length. In this paper, we
introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency
from another perspective by accelerated average reasoning speed through large
and small model collaboration. SCoT conducts thought-level drafting using a
lightweight draft model. Then it selects the best CoT draft and corrects the
error cases with the target model. The proposed thinking behavior alignment
improves the efficiency of drafting and the draft selection strategy maintains
the prediction accuracy for complex problems. Experimental results on GSM8K,
MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces
reasoning latency by 48\%$\sim$66\% for Deepseek-R1-Distill-Qwen-32B while
achieving near-target-model-level performance. Our code is available at
https://github.com/Jikai0Wang/Speculative_CoT.

</details>


### [27] [Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19101)
*Qianren Mao, Qili Zhang, Hanwen Hao, Zhentao Han, Runhua Xu, Weifeng Jiang, Qi Hu, Zhijun Chen, Tyler Zhou, Bo Li, Yangqiu Song, Jin Dong, Jianxin Li, Philip S. Yu*

Main category: cs.CL

TL;DR: FedE4RAG is a novel framework combining federated learning and RAG to enhance privacy-preserving question-answering systems without sharing raw data.


<details>
  <summary>Details</summary>
Motivation: Private RAG systems face challenges like data scarcity and privacy issues, hindering deployment.

Method: Proposes FedE4RAG, using federated learning for collaborative training, knowledge distillation, and homomorphic encryption.

Result: Experiments show FedE4RAG improves private RAG performance while ensuring data privacy.

Conclusion: FedE4RAG effectively balances data security and availability for private RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution for enhancing the accuracy and credibility of Large Language Models
(LLMs), particularly in Question & Answer tasks. This is achieved by
incorporating proprietary and private data from integrated databases. However,
private RAG systems face significant challenges due to the scarcity of private
domain data and critical data privacy issues. These obstacles impede the
deployment of private RAG systems, as developing privacy-preserving RAG systems
requires a delicate balance between data security and data availability. To
address these challenges, we regard federated learning (FL) as a highly
promising technology for privacy-preserving RAG services. We propose a novel
framework called Federated Retrieval-Augmented Generation (FedE4RAG). This
framework facilitates collaborative training of client-side RAG retrieval
models. The parameters of these models are aggregated and distributed on a
central-server, ensuring data privacy without direct sharing of raw data. In
FedE4RAG, knowledge distillation is employed for communication between the
server and client models. This technique improves the generalization of local
RAG retrievers during the federated learning process. Additionally, we apply
homomorphic encryption within federated learning to safeguard model parameters
and mitigate concerns related to data leakage. Extensive experiments conducted
on the real-world dataset have validated the effectiveness of FedE4RAG. The
results demonstrate that our proposed framework can markedly enhance the
performance of private RAG systems while maintaining robust data privacy
protection.

</details>


### [28] [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)
*Huajian Xin, Luming Li, Xiaoran Jin, Jacques Fleuriot, Wenda Li*

Main category: cs.CL

TL;DR: The paper introduces Automated Proof Engineering (APE) to automate proof engineering tasks using LLMs, presents APE-Bench I for realistic benchmarking, and develops Eleanstic for scalable verification. Results show strong performance on localized edits but challenges in complex proof engineering.


<details>
  <summary>Details</summary>
Motivation: To address the gap in benchmarks for iterative, engineering-intensive workflows in formal mathematics, inspired by software engineering advances.

Method: Introduces APE, APE-Bench I (from Mathlib4 commit histories), and Eleanstic for verification. Uses Lean compiler and LLM-as-a-Judge for hybrid verification.

Result: State-of-the-art LLMs perform well on localized edits but struggle with complex proof engineering tasks.

Conclusion: Lays groundwork for agentic workflows in proof engineering, with future benchmarks targeting multi-file coordination and autonomous agents.

Abstract: Recent progress in large language models (LLMs) has shown promise in formal
theorem proving, yet existing benchmarks remain limited to isolated, static
proof tasks, failing to capture the iterative, engineering-intensive workflows
of real-world formal mathematics libraries. Motivated by analogous advances in
software engineering, we introduce the paradigm of Automated Proof Engineering
(APE), which aims to automate proof engineering tasks such as feature addition,
proof refactoring, and bug fixing using LLMs. To facilitate research in this
direction, we present APE-Bench I, the first realistic benchmark built from
real-world commit histories of Mathlib4, featuring diverse file-level tasks
described in natural language and verified via a hybrid approach combining the
Lean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable
parallel verification infrastructure optimized for proof checking across
multiple versions of Mathlib. Empirical results on state-of-the-art LLMs
demonstrate strong performance on localized edits but substantial degradation
on handling complex proof engineering. This work lays the foundation for
developing agentic workflows in proof engineering, with future benchmarks
targeting multi-file coordination, project-scale verification, and autonomous
agents capable of planning, editing, and repairing formal libraries.

</details>


### [29] [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)
*Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, Kwan-Yee K. Wong*

Main category: cs.CL

TL;DR: Self-Play Critic (SPC) improves LLM reasoning reliability via adversarial self-play, eliminating manual step-level supervision.


<details>
  <summary>Details</summary>
Motivation: Challenges in evaluating LLM reasoning reliability due to costly step-level supervision.

Method: SPC uses adversarial self-play between a 'sneaky generator' and 'critic' to iteratively improve error detection.

Result: SPC enhances error detection (e.g., 70.8% to 77.7% on ProcessBench) and boosts LLM reasoning performance.

Conclusion: SPC outperforms baselines and state-of-the-art models, proving effective for LLM reasoning evaluation.

Abstract: Evaluating the step-by-step reliability of large language model (LLM)
reasoning, such as Chain-of-Thought, remains challenging due to the difficulty
and cost of obtaining high-quality step-level supervision. In this paper, we
introduce Self-Play Critic (SPC), a novel approach where a critic model evolves
its ability to assess reasoning steps through adversarial self-play games,
eliminating the need for manual step-level annotation. SPC involves fine-tuning
two copies of a base model to play two roles, namely a "sneaky generator" that
deliberately produces erroneous steps designed to be difficult to detect, and a
"critic" that analyzes the correctness of reasoning steps. These two models
engage in an adversarial game in which the generator aims to fool the critic,
while the critic model seeks to identify the generator's errors. Using
reinforcement learning based on the game outcomes, the models iteratively
improve; the winner of each confrontation receives a positive reward and the
loser receives a negative reward, driving continuous self-evolution.
Experiments on three reasoning process benchmarks (ProcessBench, PRM800K,
DeltaBench) demonstrate that our SPC progressively enhances its error detection
capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and
surpasses strong baselines, including distilled R1 model. Furthermore, applying
SPC to guide the test-time search of diverse LLMs significantly improves their
mathematical reasoning performance on MATH500 and AIME2024, outperforming
state-of-the-art process reward models.

</details>


### [30] [WuNeng: Hybrid State with Attention](https://arxiv.org/abs/2504.19191)
*Liu Xiao, Li Zhiyuan, Lin Yueyu*

Main category: cs.CL

TL;DR: WuNeng enhances large language models by integrating RWKV-7 RNN with attention mechanisms, improving contextual coherence without sacrificing efficiency.


<details>
  <summary>Details</summary>
Motivation: To boost the expressivity and power of large language models while maintaining computational efficiency.

Method: Combines RWKV-7 RNN with attention mechanisms, adds state-driven heads, and uses cross-head interaction and multi-token state processing.

Result: Achieves heightened expressivity and contextual coherence with minimal additional parameters.

Conclusion: WuNeng balances expressivity and efficiency, setting a new standard for neural architectures.

Abstract: The WuNeng architecture introduces a novel approach to enhancing the
expressivity and power of large language models by integrating recurrent neural
network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing
heightened contextual coherence over reducing KV cache size. Building upon the
hybrid-head concept from Hymba, WuNeng augments standard multi-head attention
with additional RWKV-7 state-driven heads, rather than replacing existing
heads, to enrich the model's representational capacity. A cross-head
interaction technique fosters dynamic synergy among standard, state-driven, and
newly introduced middle heads, leveraging concatenation, additive modulation,
and gated fusion for robust information integration. Furthermore, a multi-token
state processing mechanism harnesses the continuous RWKV-7 state to capture
intricate, sequence-wide dependencies, significantly boosting expressivity.
Remarkably, these enhancements are achieved with minimal additional parameters,
ensuring efficiency while empowering the model to excel in complex reasoning
and sequence generation tasks. WuNeng sets a new standard for balancing
expressivity and computational efficiency in modern neural architectures.

</details>


### [31] [Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora](https://arxiv.org/abs/2504.19209)
*Elisabeth Fittschen, Bella Xia, Leib Celnik, Paul Dilley, Tom Lippincott*

Main category: cs.CL

TL;DR: Analysis of implementation choices for the Dynamic Embedded Topic Model on diachronic corpora, identifying key priorities for practical use and development.


<details>
  <summary>Details</summary>
Motivation: To isolate important decisions for the use and further development of the Dynamic Embedded Topic Model in applied scholarship.

Method: Measuring effects of implementation choices on five distinct diachronic corpora.

Result: Identified priorities include scalable vocabulary size and flexible interval modeling; performance is unaffected by certain limiting factors.

Conclusion: Key implementation choices are identified to enhance utility and scalability, with some factors having negligible impact on performance.

Abstract: We measure the effects of several implementation choices for the Dynamic
Embedded Topic Model, as applied to five distinct diachronic corpora, with the
goal of isolating important decisions for its use and further development. We
identify priorities that will maximize utility in applied scholarship,
including the practical scalability of vocabulary size to best exploit the
strengths of embedded representations, and more flexible modeling of intervals
to accommodate the uneven temporal distributions of historical writing. Of
similar importance, we find performance is not significantly or consistently
affected by several aspects that otherwise limit the model's application or
might consume the resources of a grid search.

</details>


### [32] [Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers](https://arxiv.org/abs/2504.19254)
*Dylan Bouchard, Mohit Singh Chauhan*

Main category: cs.CL

TL;DR: Proposes a zero-resource hallucination detection framework for LLMs using uncertainty quantification techniques, offering a tunable ensemble approach and a Python toolkit (UQLM).


<details>
  <summary>Details</summary>
Motivation: Addresses the critical need for reliable hallucination detection in high-stakes domains like healthcare and finance as LLMs become more prevalent.

Method: Adapts uncertainty quantification techniques (black-box UQ, white-box UQ, LLM-as-a-Judge) into standardized confidence scores and introduces a tunable ensemble for flexibility.

Result: The tunable ensemble outperforms individual components and existing methods, improving LLM reliability.

Conclusion: Customized hallucination detection strategies enhance LLM accuracy and reliability, supported by the UQLM toolkit.

Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As
these models become increasingly used in high-stakes domains, such as
healthcare and finance, the need for effective hallucination detection is
crucial. To this end, we propose a versatile framework for zero-resource
hallucination detection that practitioners can apply to real-world use cases.
To achieve this, we adapt a variety of existing uncertainty quantification (UQ)
techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,
transforming them as necessary into standardized response-level confidence
scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable
ensemble approach that incorporates any combination of the individual
confidence scores. This approach enables practitioners to optimize the ensemble
for a specific use case for improved performance. To streamline implementation,
the full suite of scorers is offered in this paper's companion Python toolkit,
UQLM. To evaluate the performance of the various scorers, we conduct an
extensive set of experiments using several LLM question-answering benchmarks.
We find that our tunable ensemble typically surpasses its individual components
and outperforms existing hallucination detection methods. Our results
demonstrate the benefits of customized hallucination detection strategies for
improving the accuracy and reliability of LLMs.

</details>


### [33] [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/abs/2504.19267)
*Mohamed Gado, Towhid Taliee, Muhammad Memon, Dmitry Ignatov, Radu Timofte*

Main category: cs.CL

TL;DR: The paper introduces VIST-GPT, a transformer-based model for visual storytelling, using novel metrics (RoViST and GROOVIST) to evaluate narrative quality beyond traditional NLP metrics.


<details>
  <summary>Details</summary>
Motivation: To improve visual storytelling by addressing the limitations of existing evaluation metrics and leveraging advancements in multimodal models.

Method: Adapts transformer-based architectures and large multimodal models, trained on the VIST dataset, to generate narratives. Introduces RoViST and GROOVIST for evaluation.

Result: VIST-GPT produces visually grounded, coherent narratives, with evaluation metrics aligning better with human judgment.

Conclusion: The proposed model and metrics advance visual storytelling by ensuring better narrative quality and evaluation.

Abstract: Visual storytelling is an interdisciplinary field combining computer vision
and natural language processing to generate cohesive narratives from sequences
of images. This paper presents a novel approach that leverages recent
advancements in multimodal models, specifically adapting transformer-based
architectures and large multimodal models, for the visual storytelling task.
Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT
model produces visually grounded, contextually appropriate narratives. We
address the limitations of traditional evaluation metrics, such as BLEU,
METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we
utilize RoViST and GROOVIST, novel reference-free metrics designed to assess
visual storytelling, focusing on visual grounding, coherence, and
non-redundancy. These metrics provide a more nuanced evaluation of narrative
quality, aligning closely with human judgment.

</details>


### [34] [AndroidGen: Building an Android Language Agent under Data Scarcity](https://arxiv.org/abs/2504.19298)
*Hanyu Lai, Junjie Gao, Xiao Liu, Yifan Xu, Shudan Zhang, Yuxiao Dong, Jie Tang*

Main category: cs.CL

TL;DR: AndroidGen is a framework to enhance LLM-based agents for mobile tasks under data scarcity, using human task trajectories to train open-source LLMs without manual labeling.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of limited high-quality data for LLM-based mobile agents due to time and labor constraints in human annotation.

Method: Develop AndroidGen to collect human task trajectories and train open-source LLMs on these trajectories, avoiding manual labeling.

Result: Evaluated on AndroidWorld, AitW, and popular apps, AndroidGen shows improvements and identifies areas for future enhancement.

Conclusion: AndroidGen effectively tackles data scarcity for LLM-based mobile agents, with potential for further refinement.

Abstract: Large language models have opened up a world of possibilities for various NLP
tasks, sparking optimism for the future. Despite their potential, LLMs have yet
to be widely used as agents on real mobile devices. The main challenge is the
need for high-quality data sources. Time constraints and labor intensity often
hinder human annotation. On the other hand, existing LLMs exhibit inadequate
completion rates and need a robust data filtration strategy. Given these
challenges, we develop a framework called AndroidGen to enhance the
capabilities of LLM-based agents under data scarcity. In addition, we leverage
AndroidGen to collect trajectories given human tasks and train open-source LLMs
on these trajectories to develop an open-source mobile agent without manually
labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld,
AitW, and various popular applications, demonstrating its improvements and
revealing potential areas for future improvement. Code, model, and data are
available at https://github.com/THUDM/AndroidGen.

</details>


### [35] [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese](https://arxiv.org/abs/2504.19314)
*Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, Yining Hua*

Main category: cs.CL

TL;DR: BrowseComp-ZH is a high-difficulty benchmark for evaluating LLM agents on the Chinese web, revealing significant challenges for current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook complexities of non-English ecosystems, particularly Chinese, necessitating a specialized evaluation tool.

Method: BrowseComp-ZH includes 289 multi-hop questions across 11 domains, with rigorous quality control for difficulty and answer uniqueness.

Result: Most models perform poorly (below 10% accuracy), with the best (OpenAI's DeepResearch) achieving only 42.9%.

Conclusion: BrowseComp-ZH highlights the need for improved retrieval, reasoning, and information reconciliation in LLM agents.

Abstract: As large language models (LLMs) evolve into tool-using agents, the ability to
browse the web in real-time has become a critical yardstick for measuring their
reasoning and retrieval competence. Existing benchmarks such as BrowseComp
concentrate on English and overlook the linguistic, infrastructural, and
censorship-related complexities of other major information ecosystems -- most
notably Chinese. To address this gap, we introduce BrowseComp-ZH, a
high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents
on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning
11 diverse domains. Each question is reverse-engineered from a short,
objective, and easily verifiable answer (e.g., a date, number, or proper noun).
A two-stage quality control protocol is applied to strive for high question
difficulty and answer uniqueness. We benchmark over 20 state-of-the-art
language models and agentic search systems on our proposed BrowseComp-ZH.
Despite their strong conversational and retrieval capabilities, most models
struggle severely: a large number achieve accuracy rates below 10%, and only a
handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,
reaches just 42.9%. These results demonstrate the considerable difficulty of
BrowseComp-ZH, where success demands not only effective retrieval strategies,
but also sophisticated reasoning and information reconciliation -- capabilities
that current models still struggle to master. Our dataset, construction
guidelines, and benchmark results have been publicly released at
https://github.com/PALIN2018/BrowseComp-ZH.

</details>


### [36] [Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing](https://arxiv.org/abs/2504.19333)
*James O' Neill, Santhosh Subramanian, Eric Lin, Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: Task-specific data generation and model merging improve guardrail classifiers, outperforming large language models (LLMs) in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Address the prohibitive costs and inefficiencies of using LLMs for guardrailing by developing smaller, task-specific models.

Method: Fine-tuned classifiers, a pretrained multi-task model (MultiTaskGuard), and a search-based model merging approach (UniGuard).

Result: Significant F1 score improvements (29.92 over Aegis-LlamaGuard, 21.62 over GPT-4o) on guardrail benchmarks.

Conclusion: Task-specific models and synthetic data generation offer efficient, high-performance alternatives to LLMs for guardrailing.

Abstract: The trend towards large language models (LLMs) for guardrailing against
undesired behaviors is increasing and has shown promise for censoring user
inputs. However, increased latency, memory consumption, hosting expenses and
non-structured outputs can make their use prohibitive.
  In this work, we show that task-specific data generation can lead to
fine-tuned classifiers that significantly outperform current state of the art
(SoTA) while being orders of magnitude smaller. Secondly, we show that using a
single model, \texttt{MultiTaskGuard}, that is pretrained on a large
synthetically generated dataset with unique task instructions further improves
generalization. Thirdly, our most performant models, \texttt{UniGuard}, are
found using our proposed search-based model merging approach that finds an
optimal set of parameters to combine single-policy models and multi-policy
guardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,
our efficient guardrail classifiers improve over the best performing SoTA
publicly available LLMs and 3$^{\text{rd}}$ party guardrail APIs in detecting
unsafe and safe behaviors by an average F1 score improvement of \textbf{29.92}
points over Aegis-LlamaGuard and \textbf{21.62} over \texttt{gpt-4o},
respectively. Lastly, our guardrail synthetic data generation process that uses
custom task-specific guardrail poli

</details>


### [37] [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)
*Dongqi Liu, Xi Yu, Vera Demberg, Mirella Lapata*

Main category: cs.CL

TL;DR: A plan-based approach using discourse frameworks improves lay summaries by guiding explanatory content, outperforming current methods in quality and robustness.


<details>
  <summary>Details</summary>
Motivation: Current automatic summarization methods lack explicit modeling of explanations, leading to misalignment with human-written summaries.

Method: Proposes two discourse-driven planning strategies: conditioning the plan as input or output prefix to guide explanatory sentences.

Result: Outperforms state-of-the-art methods on three datasets, improving summary quality, robustness, controllability, and reducing hallucination.

Conclusion: The plan-based approach effectively enhances lay summarization by integrating explanatory content through discourse-driven strategies.

Abstract: Lay summaries for scientific documents typically include explanations to help
readers grasp sophisticated concepts or arguments. However, current automatic
summarization methods do not explicitly model explanations, which makes it
difficult to align the proportion of explanatory content with human-written
summaries. In this paper, we present a plan-based approach that leverages
discourse frameworks to organize summary generation and guide explanatory
sentences by prompting responses to the plan. Specifically, we propose two
discourse-driven planning strategies, where the plan is conditioned as part of
the input or part of the output prefix, respectively. Empirical experiments on
three lay summarization datasets show that our approach outperforms existing
state-of-the-art methods in terms of summary quality, and it enhances model
robustness, controllability, and mitigates hallucination.

</details>


### [38] [ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via Substitution Ciphers](https://arxiv.org/abs/2504.19395)
*Zhouxiang Fang, Aayush Mishra, Muhan Gao, Anqi Liu, Daniel Khashabi*

Main category: cs.CL

TL;DR: The paper introduces ICL CIPHERS, a method using reversible substitution ciphers to study dual modes of In-Context Learning (ICL) in LLMs, showing they perform better with bijective mappings.


<details>
  <summary>Details</summary>
Motivation: Disentangling task retrieval and task learning in ICL is challenging. The study aims to quantify learning in ICL using cipher-based task reformulations.

Method: ICL CIPHERS uses reversible substitution ciphers to transform in-context inputs, testing LLMs' ability to decipher latent patterns.

Result: LLMs perform better with bijective (reversible) ciphers than non-bijective ones, showing consistent results across datasets and models.

Conclusion: The study provides a novel way to quantify learning in ICL and finds evidence of LLMs decoding ciphered inputs.

Abstract: Recent works have suggested that In-Context Learning (ICL) operates in dual
modes, i.e. task retrieval (remember learned patterns from pre-training) and
task learning (inference-time ``learning'' from demonstrations). However,
disentangling these the two modes remains a challenging goal. We introduce ICL
CIPHERS, a class of task reformulations based on substitution ciphers borrowed
from classic cryptography. In this approach, a subset of tokens in the
in-context inputs are substituted with other (irrelevant) tokens, rendering
English sentences less comprehensible to human eye. However, by design, there
is a latent, fixed pattern to this substitution, making it reversible. This
bijective (reversible) cipher ensures that the task remains a well-defined task
in some abstract sense, despite the transformations. It is a curious question
if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires
deciphering the latent cipher. We show that LLMs are better at solving ICL
CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,
providing a novel approach to quantify ``learning'' in ICL. While this gap is
small, it is consistent across the board on four datasets and six models.
Finally, we examine LLMs' internal representations and identify evidence in
their ability to decode the ciphered inputs.

</details>


### [39] [Context Selection and Rewriting for Video-based EducationalQuestion Generation](https://arxiv.org/abs/2504.19406)
*Mengxia Yu, Bang Nguyen, Olivia Zino, Meng Jiang*

Main category: cs.CL

TL;DR: The paper introduces a dataset and framework for educational question generation (EQG) from real-world lectures, addressing challenges in context selection and answer alignment using large language models.


<details>
  <summary>Details</summary>
Motivation: Existing EQG datasets rely on edited texts, lacking real-world classroom content. The paper aims to bridge this gap by using lecture speech and slides.

Method: A novel framework dynamically selects and rewrites contexts from transcripts and keyframes, integrating them into answer-containing knowledge statements.

Result: The approach improves question quality and relevance by better aligning contexts with target answers and timestamps.

Conclusion: The released dataset and framework enhance EQG for real-world educational content.

Abstract: Educational question generation (EQG) is a crucial component of intelligent
educational systems, significantly aiding self-assessment, active learning, and
personalized education. While EQG systems have emerged, existing datasets
typically rely on predefined, carefully edited texts, failing to represent
real-world classroom content, including lecture speech with a set of
complementary slides. To bridge this gap, we collect a dataset of educational
questions based on lectures from real-world classrooms. On this realistic
dataset, we find that current methods for EQG struggle with accurately
generating questions from educational videos, particularly in aligning with
specific timestamps and target answers. Common challenges include selecting
informative contexts from extensive transcripts and ensuring generated
questions meaningfully incorporate the target answer. To address the
challenges, we introduce a novel framework utilizing large language models for
dynamically selecting and rewriting contexts based on target timestamps and
answers. First, our framework selects contexts from both lecture transcripts
and video keyframes based on answer relevance and temporal proximity. Then, we
integrate the contexts selected from both modalities and rewrite them into
answer-containing knowledge statements, to enhance the logical connection
between the contexts and the desired answer. This approach significantly
improves the quality and relevance of the generated questions. Our dataset and
code are released in https://github.com/mengxiayu/COSER.

</details>


### [40] [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)
*Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, Deshraj Yadav*

Main category: cs.CL

TL;DR: Mem0, a scalable memory-centric architecture, improves LLM consistency in multi-session dialogues by dynamically managing conversational memory, outperforming baselines in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Fixed context windows in LLMs hinder long-term conversational consistency, necessitating a dynamic memory solution.

Method: Introduces Mem0, a memory-centric architecture, and an enhanced graph-based variant for relational memory. Evaluated against six baseline categories on the LOCOMO benchmark.

Result: Mem0 outperforms baselines, achieving 26% improvement over OpenAI in LLM-as-a-Judge and 91% lower latency. Graph-based Mem0 adds 2% overall score.

Conclusion: Structured, persistent memory mechanisms are crucial for reliable and efficient LLM-driven AI agents in long-term conversations.

Abstract: Large Language Models (LLMs) have demonstrated remarkable prowess in
generating contextually coherent responses, yet their fixed context windows
pose fundamental challenges for maintaining consistency over prolonged
multi-session dialogues. We introduce Mem0, a scalable memory-centric
architecture that addresses this issue by dynamically extracting,
consolidating, and retrieving salient information from ongoing conversations.
Building on this foundation, we further propose an enhanced variant that
leverages graph-based memory representations to capture complex relational
structures among conversational elements. Through comprehensive evaluations on
LOCOMO benchmark, we systematically compare our approaches against six baseline
categories: (i) established memory-augmented systems, (ii) retrieval-augmented
generation (RAG) with varying chunk sizes and k-values, (iii) a full-context
approach that processes the entire conversation history, (iv) an open-source
memory solution, (v) a proprietary model system, and (vi) a dedicated memory
management platform. Empirical results show that our methods consistently
outperform all existing memory systems across four question categories:
single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%
relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with
graph memory achieves around 2% higher overall score than the base
configuration. Beyond accuracy gains, we also markedly reduce computational
overhead compared to full-context method. In particular, Mem0 attains a 91%
lower p95 latency and saves more than 90% token cost, offering a compelling
balance between advanced reasoning capabilities and practical deployment
constraints. Our findings highlight critical role of structured, persistent
memory mechanisms for long-term conversational coherence, paving the way for
more reliable and efficient LLM-driven AI agents.

</details>


### [41] [Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models](https://arxiv.org/abs/2504.19436)
*Jacky He, Guiran Liu, Binrong Zhu, Hanlu Zhang, Hongye Zheng, Xiaokai Wang*

Main category: cs.CL

TL;DR: The paper introduces a dynamic optimization method for Retrieval-Augmented Generation (RAG) architecture, improving semantic understanding and knowledge scheduling efficiency for open-domain tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations of static RAG structures in context adaptation and knowledge access for large language models.

Method: Proposes a state-aware dynamic knowledge retrieval mechanism with multi-level perceptive retrieval vectors and differentiable document matching paths for joint training.

Result: Significant improvements in BLEU and ROUGE-L scores on the Natural Questions dataset, with enhanced robustness and generation consistency.

Conclusion: The method shows broad application potential and practical value for high-quality language generation systems.

Abstract: This paper focuses on the dynamic optimization of the Retrieval-Augmented
Generation (RAG) architecture. It proposes a state-aware dynamic knowledge
retrieval mechanism to enhance semantic understanding and knowledge scheduling
efficiency in large language models for open-domain question answering and
complex generation tasks. The method introduces a multi-level perceptive
retrieval vector construction strategy and a differentiable document matching
path. These components enable end-to-end joint training and collaborative
optimization of the retrieval and generation modules. This effectively
addresses the limitations of static RAG structures in context adaptation and
knowledge access. Experiments are conducted on the Natural Questions dataset.
The proposed structure is thoroughly evaluated across different large models,
including GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments
from multiple perspectives confirm the significant improvements in BLEU and
ROUGE-L scores. The approach also demonstrates stronger robustness and
generation consistency in tasks involving semantic ambiguity and multi-document
fusion. These results highlight its broad application potential and practical
value in building high-quality language generation systems.

</details>


### [42] [Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks](https://arxiv.org/abs/2504.19445)
*Yi-Long Lu, Chunhui Zhang, Wei Wang*

Main category: cs.CL

TL;DR: LLMs show a consistent negative bias in binary response formats compared to continuous ones, impacting their reliability in decision tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate how response formats (binary vs. continuous) influence LLMs' judgments and reveal potential biases.

Method: Tested LLMs in value statement judgments and text sentiment analysis tasks, comparing binary and continuous response formats across various models.

Result: LLMs exhibited a consistent negative bias in binary formats compared to continuous ones, confirmed across tasks.

Conclusion: Response format significantly affects LLMs' judgments, emphasizing the need for careful task design to mitigate biases.

Abstract: Large Language Models (LLMs) are increasingly used in tasks such as
psychological text analysis and decision-making in automated workflows.
However, their reliability remains a concern due to potential biases inherited
from their training process. In this study, we examine how different response
format: binary versus continuous, may systematically influence LLMs' judgments.
In a value statement judgments task and a text sentiment analysis task, we
prompted LLMs to simulate human responses and tested both formats across
several models, including both open-source and commercial models. Our findings
revealed a consistent negative bias: LLMs were more likely to deliver
"negative" judgments in binary formats compared to continuous ones. Control
experiments further revealed that this pattern holds across both tasks. Our
results highlight the importance of considering response format when applying
LLMs to decision tasks, as small changes in task design can introduce
systematic biases.

</details>


### [43] [Towards Long Context Hallucination Detection](https://arxiv.org/abs/2504.19457)
*Siyi Liu, Kishaloy Halder, Zheng Qi, Wei Xiao, Nikolaos Pappas, Phu Mon Htut, Neha Anna John, Yassine Benajiba, Dan Roth*

Main category: cs.CL

TL;DR: A new method addresses contextual hallucinations in LLMs for long-context inputs by creating a specialized dataset and proposing an efficient architecture using pre-trained encoders like BERT.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate unsubstantiated or contradictory information in long-context inputs, a problem not fully addressed by existing research.

Method: Constructed a dataset for long-context hallucination detection and introduced a decomposition-aggregation architecture for pre-trained encoders.

Result: The proposed architecture outperforms similar-sized models and LLM-based models in accuracy and speed.

Conclusion: The work provides an effective solution for detecting hallucinations in long-context LLM inputs, improving both performance and efficiency.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various tasks. However, they are prone to contextual hallucination, generating
information that is either unsubstantiated or contradictory to the given
context. Although many studies have investigated contextual hallucinations in
LLMs, addressing them in long-context inputs remains an open problem. In this
work, we take an initial step toward solving this problem by constructing a
dataset specifically designed for long-context hallucination detection.
Furthermore, we propose a novel architecture that enables pre-trained encoder
models, such as BERT, to process long contexts and effectively detect
contextual hallucinations through a decomposition and aggregation mechanism.
Our experimental results show that the proposed architecture significantly
outperforms previous models of similar size as well as LLM-based models across
various metrics, while providing substantially faster inference.

</details>


### [44] [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/abs/2504.19467)
*Jiageng Wu, Bowen Gu, Ren Zhou, Kevin Xie, Doug Snyder, Yixing Jiang, Valentina Carducci, Richard Wyss, Rishi J Desai, Emily Alsentzer, Leo Anthony Celi, Adam Rodman, Sebastian Schneeweiss, Jonathan H. Chen, Santiago Romero-Brufau, Kueiyu Joshua Lin, Jie Yang*

Main category: cs.CL

TL;DR: BRIDGE is a multilingual benchmark for evaluating LLMs in clinical contexts, revealing performance variations and showing open-source models can match proprietary ones.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs in clinical settings are limited, relying on narrow benchmarks that don't reflect real-world EHR complexity.

Method: BRIDGE includes 87 tasks from real-world clinical data across nine languages, evaluating 52 LLMs with 13,572 experiments.

Result: Performance varies by model size, language, task, and specialty; open-source models can rival proprietary ones, while older medically fine-tuned models underperform.

Conclusion: BRIDGE provides a foundational resource for developing and evaluating LLMs in clinical text understanding.

Abstract: Large language models (LLMs) hold great promise for medical applications and
are evolving rapidly, with new models being released at an accelerated pace.
However, current evaluations of LLMs in clinical contexts remain limited. Most
existing benchmarks rely on medical exam-style questions or PubMed-derived
text, failing to capture the complexity of real-world electronic health record
(EHR) data. Others focus narrowly on specific application scenarios, limiting
their generalizability across broader clinical use. To address this gap, we
present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks
sourced from real-world clinical data sources across nine languages. We
systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,
GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total
of 13,572 experiments, our results reveal substantial performance variation
across model sizes, languages, natural language processing tasks, and clinical
specialties. Notably, we demonstrate that open-source LLMs can achieve
performance comparable to proprietary models, while medically fine-tuned LLMs
based on older architectures often underperform versus updated general-purpose
models. The BRIDGE and its corresponding leaderboard serve as a foundational
resource and a unique reference for the development and evaluation of new LLMs
in real-world clinical text understanding.

</details>


### [45] [Conflicts in Texts: Data, Implications and Challenges](https://arxiv.org/abs/2504.19472)
*Siyi Liu, Dan Roth*

Main category: cs.CL

TL;DR: The paper surveys conflicts in NLP models, categorizing them into natural texts, human-annotated data, and model interactions, and discusses mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Addressing conflicts in NLP models is crucial to improve reliability and trustworthiness, as ignoring them leads to undesired behaviors.

Method: Categorizes conflicts into three areas: natural texts, human-annotated data, and model interactions, analyzing their implications.

Result: Identifies key challenges and proposes mitigation strategies for conflict-aware NLP systems.

Conclusion: Highlights the need for future work to develop systems that effectively reason over and reconcile conflicting information.

Abstract: As NLP models become increasingly integrated into real-world applications, it
becomes clear that there is a need to address the fact that models often rely
on and generate conflicting information. Conflicts could reflect the complexity
of situations, changes that need to be explained and dealt with, difficulties
in data annotation, and mistakes in generated outputs. In all cases,
disregarding the conflicts in data could result in undesired behaviors of
models and undermine NLP models' reliability and trustworthiness. This survey
categorizes these conflicts into three key areas: (1) natural texts on the web,
where factual inconsistencies, subjective biases, and multiple perspectives
introduce contradictions; (2) human-annotated data, where annotator
disagreements, mistakes, and societal biases impact model training; and (3)
model interactions, where hallucinations and knowledge conflicts emerge during
deployment. While prior work has addressed some of these conflicts in
isolation, we unify them under the broader concept of conflicting information,
analyze their implications, and discuss mitigation strategies. We highlight key
challenges and future directions for developing conflict-aware NLP systems that
can reason over and reconcile conflicting information more effectively.

</details>


### [46] [Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment](https://arxiv.org/abs/2504.19556)
*Kristen Sussman, Daniel Carter*

Main category: cs.CL

TL;DR: Study analyzes AI's impact on social media language, showing increased positivity and sentiment in tweets about Donald Trump post-ChatGPT.


<details>
  <summary>Details</summary>
Motivation: To detect the influence of AI-mediated communication (AI-MC) on linguistic patterns and emotional expression in social media.

Method: Compared 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from 2024, analyzing changes in readability (Flesch-Kincaid) and sentiment (polarity scores).

Result: Significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift from neutral to more positive expressions (28.6% to 45.9%).

Conclusion: AI's presence in social media is growing, influencing language complexity and emotional expression patterns.

Abstract: Given the subtle human-like effects of large language models on linguistic
patterns, this study examines shifts in language over time to detect the impact
of AI-mediated communication (AI- MC) on social media. We compare a replicated
dataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the
same period in 2024, all of which mention Donald Trump during election periods.
Using a combination of Flesch-Kincaid readability and polarity scores, we
analyze changes in text complexity and sentiment. Our findings reveal a
significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift
from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more
positive expressions (28.6% to 45.9%). These findings suggest not only an
increasing presence of AI in social media communication but also its impact on
language and emotional expression patterns.

</details>


### [47] [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)
*Meng Xiao, Xunxin Cai, Chengrui Wang, Yuanchun Zhou*

Main category: cs.CL

TL;DR: A multi-agent framework for distilling high-quality biomedical corpora improves LLM performance, surpassing proprietary models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: Existing biomedical corpora are limited in quality and quantity, hindering LLM training in the domain.

Method: A knowledge-driven, multi-agent framework guided by MeSH hierarchy autonomously extracts and refines biomedical QA pairs.

Result: LLMs trained on the distilled datasets outperform baselines and proprietary models, including GPT-4.

Conclusion: Multi-agent collaboration effectively enhances biomedical LLM training, demonstrating significant performance gains.

Abstract: The rapid progress of large language models (LLMs) in biomedical research has
underscored the limitations of existing open-source annotated scientific
corpora, which are often insufficient in quantity and quality. Addressing the
challenge posed by the complex hierarchy of biomedical knowledge, we propose a
knowledge-driven, multi-agent framework for scientific corpus distillation
tailored for LLM training in the biomedical domain. Central to our approach is
a collaborative multi-agent architecture, where specialized agents, each guided
by the Medical Subject Headings (MeSH) hierarchy, work in concert to
autonomously extract, synthesize, and self-evaluate high-quality textual data
from vast scientific literature. These agents collectively generate and refine
domain-specific question-answer pairs, ensuring comprehensive coverage and
consistency with biomedical ontologies while minimizing manual involvement.
Extensive experimental results show that language models trained on our
multi-agent distilled datasets achieve notable improvements in biomedical
question-answering tasks, outperforming both strong life sciences LLM baselines
and advanced proprietary models. Notably, our AI-Ready dataset enables
Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger
scale. Detailed ablation studies and case analyses further validate the
effectiveness and synergy of each agent within the framework, highlighting the
potential of multi-agent collaboration in biomedical LLM training.

</details>


### [48] [Arabic Metaphor Sentiment Classification Using Semantic Information](https://arxiv.org/abs/2504.19590)
*Israa Alsiyat*

Main category: cs.CL

TL;DR: The paper tests the Arabic Metaphor Corpus (AMC) using new automatic tools for sentiment classification based on semantic tags, evaluating performance with F-score, recall, and precision.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of Arabic online metaphors on sentiment using semantic tags, a novel approach in the field.

Method: Designs automatic tools incorporating semantic emotional tags for sentiment classification of AMC, evaluated with standard metrics.

Result: The tool's performance is assessed using F-score, recall, and precision, though specific results are not detailed in the abstract.

Conclusion: This is the first approach to sentiment classification for Arabic metaphors using semantic tags, highlighting its novelty and potential impact.

Abstract: In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]
using newly designed automatic tools for sentiment classification for AMC based
on semantic tags. The tool incorporates semantic emotional tags for sentiment
classification. I evaluate the tool using standard methods, which are F-score,
recall, and precision. The method is to show the impact of Arabic online
metaphors on sentiment through the newly designed tools. To the best of our
knowledge, this is the first approach to conduct sentiment classification for
Arabic metaphors using semantic tags to find the impact of the metaphor.

</details>


### [49] [Coreference Resolution for Vietnamese Narrative Texts](https://arxiv.org/abs/2504.19606)
*Hieu-Dai Tran, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: The paper introduces a Vietnamese coreference resolution dataset from VnExpress and evaluates GPT-3.5-Turbo and GPT-4, finding GPT-4 superior.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of annotated datasets for coreference resolution in Vietnamese, a low-resource language.

Method: Developed a dataset using VnExpress texts with detailed annotation guidelines and evaluated GPT-3.5-Turbo and GPT-4.

Result: GPT-4 outperformed GPT-3.5-Turbo in accuracy and consistency for Vietnamese coreference resolution.

Conclusion: GPT-4 is more reliable for Vietnamese coreference resolution, highlighting the value of high-quality datasets.

Abstract: Coreference resolution is a vital task in natural language processing (NLP)
that involves identifying and linking different expressions in a text that
refer to the same entity. This task is particularly challenging for Vietnamese,
a low-resource language with limited annotated datasets. To address these
challenges, we developed a comprehensive annotated dataset using narrative
texts from VnExpress, a widely-read Vietnamese online news platform. We
established detailed guidelines for annotating entities, focusing on ensuring
consistency and accuracy. Additionally, we evaluated the performance of large
language models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.
Our results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in
terms of both accuracy and response consistency, making it a more reliable tool
for coreference resolution in Vietnamese.

</details>


### [50] [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)
*Run Luo, Renke Shan, Longze Chen, Ziqiang Liu, Lu Wang, Min Yang, Xiaobo Xia*

Main category: cs.CL

TL;DR: VCM is a self-supervised visual concept modeling framework that improves efficiency and performance of LVLMs by reducing computational costs and enhancing visual concept perception.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs process images inefficiently at the token level, unlike humans who analyze at the conceptual level, limiting real-world usability.

Method: VCM uses implicit contrastive learning and vision-language fine-tuning to build a visual concept model without costly annotations.

Result: VCM reduces computational costs (e.g., 85% fewer FLOPs) while maintaining performance and improving visual concept perception.

Conclusion: VCM is effective and efficient, validated by extensive experiments, enhancing LVLMs for real-world applications.

Abstract: Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like
embodied intelligence due to their strong vision-language reasoning abilities.
However, current LVLMs process entire images at the token level, which is
inefficient compared to humans who analyze information and generate content at
the conceptual level, extracting relevant visual concepts with minimal effort.
This inefficiency, stemming from the lack of a visual concept model, limits
LVLMs' usability in real-world applications. To address this, we propose VCM,
an end-to-end self-supervised visual concept modeling framework. VCM leverages
implicit contrastive learning across multiple sampled instances and
vision-language fine-tuning to construct a visual concept model without
requiring costly concept-level annotations. Our results show that VCM
significantly reduces computational costs (e.g., 85\% fewer FLOPs for
LLaVA-1.5-7B) while maintaining strong performance across diverse image
understanding tasks. Moreover, VCM enhances visual encoders' capabilities in
classic visual concept perception tasks. Extensive quantitative and qualitative
experiments validate the effectiveness and efficiency of VCM.

</details>


### [51] [A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks](https://arxiv.org/abs/2504.19645)
*Shadan Shukr Sabr, Nazira Sabr Mustafa, Talar Sabah Omar, Salah Hwayyiz Rasool, Nawzad Anwer Omer, Darya Sabir Hamad, Hemin Abdulhameed Shams, Omer Mahmood Kareem, Rozhan Noori Abdullah, Khabat Atar Abdullah, Mahabad Azad Mohammad, Haneen Al-Raghefy, Safar M. Asaad, Sara Jamal Mohammed, Twana Saeed Ali, Fazil Shawrow, Halgurd S. Maghdid*

Main category: cs.CL

TL;DR: The paper introduces a standardized and comprehensive POS tagset for the Central-Kurdish language (CKL) to enhance NLP tasks, addressing the lack of resources for low-resourced languages.


<details>
  <summary>Details</summary>
Motivation: Low-resourced languages like CKL lack standardized POS tagsets, hindering NLP task performance. This study aims to fill this gap.

Method: The study collected POS tags from existing studies and Kurdish linguistic experts to create a standardized tagset, validated against the Universal Dependencies framework.

Result: The proposed POS tagset improves accuracy and standardization for Kurdish NLP tasks, as shown by initial comparisons.

Conclusion: The standardized POS tagset for CKL supports better NLP task performance and corrects sentences more accurately.

Abstract: - The field of natural language processing (NLP) has dramatically expanded
within the last decade. Many human-being applications are conducted daily via
NLP tasks, starting from machine translation, speech recognition, text
generation and recommendations, Part-of-Speech tagging (POS), and Named-Entity
Recognition (NER). However, low-resourced languages, such as the
Central-Kurdish language (CKL), mainly remain unexamined due to shortage of
necessary resources to support their development. The POS tagging task is the
base of other NLP tasks; for example, the POS tag set has been used to
standardized languages to provide the relationship between words among the
sentences, followed by machine translation and text recommendation.
Specifically, for the CKL, most of the utilized or provided POS tagsets are
neither standardized nor comprehensive. To this end, this study presented an
accurate and comprehensive POS tagset for the CKL to provide better performance
of the Kurdish NLP tasks. The article also collected most of the POS tags from
different studies as well as from Kurdish linguistic experts to standardized
part-of-speech tags. The proposed POS tagset is designed to annotate a large
CKL corpus and support Kurdish NLP tasks. The initial investigations of this
study via comparison with the Universal Dependencies framework for standard
languages, show that the proposed POS tagset can streamline or correct
sentences more accurately for Kurdish NLP tasks.

</details>


### [52] [Multimodal Conditioned Diffusive Time Series Forecasting](https://arxiv.org/abs/2504.19669)
*Chen Su, Yuanhe Tian, Yan Song*

Main category: cs.CL

TL;DR: MCD-TSF is a multimodal diffusion model for time series forecasting that leverages timestamps and text for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for time series forecasting ignore multimodal information, limiting their potential.

Method: MCD-TSF integrates timestamps and text as guidance, using temporal-semantic correlations and adaptive text alignment.

Result: Achieves state-of-the-art performance on real-world datasets across eight domains.

Conclusion: MCD-TSF effectively utilizes multimodal data for superior time series forecasting.

Abstract: Diffusion models achieve remarkable success in processing images and text,
and have been extended to special domains such as time series forecasting
(TSF). Existing diffusion-based approaches for TSF primarily focus on modeling
single-modality numerical sequences, overlooking the rich multimodal
information in time series data. To effectively leverage such information for
prediction, we propose a multimodal conditioned diffusion model for TSF,
namely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for
time series modeling, especially for forecasting. Specifically, Timestamps are
combined with time series to establish temporal and semantic correlations among
different data points when aggregating information along the temporal
dimension. Texts serve as supplementary descriptions of time series' history,
and adaptively aligned with data points as well as dynamically controlled in a
classifier-free manner. Extensive experiments on real-world benchmark datasets
across eight domains demonstrate that the proposed MCD-TSF model achieves
state-of-the-art performance.

</details>


### [53] [Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs](https://arxiv.org/abs/2504.19675)
*Osma Suominen, Juho Inkinen, Mona Lehtinen*

Main category: cs.CL

TL;DR: Annif system combines traditional NLP/ML with LLM techniques for subject indexing, achieving top rankings in SemEval-2025 Task 5.


<details>
  <summary>Details</summary>
Motivation: To improve subject indexing accuracy and efficiency in multilingual contexts using LLMs.

Method: Combines traditional NLP/ML (Annif toolkit) with LLM-based translation, synthetic data generation, and merging monolingual predictions.

Result: Ranked first in all-subjects, second in tib-core-subjects (quantitative), and fourth in qualitative evaluations.

Conclusion: Combining traditional XMTC algorithms with LLM techniques enhances subject indexing performance.

Abstract: This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),
which focussed on subject indexing using large language models (LLMs). The task
required creating subject predictions for bibliographic records from the
bilingual TIBKAT database using the GND subject vocabulary. Our approach
combines traditional natural language processing and machine learning
techniques implemented in the Annif toolkit with innovative LLM-based methods
for translation and synthetic data generation, and merging predictions from
monolingual models. The system ranked first in the all-subjects category and
second in the tib-core-subjects category in the quantitative evaluation, and
fourth in qualitative evaluations. These findings demonstrate the potential of
combining traditional XMTC algorithms with modern LLM techniques to improve the
accuracy and efficiency of subject indexing in multilingual contexts.

</details>


### [54] [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720)
*Ranran Zhen, Juntao Li, Yixin Ji, Zhenlin Yang, Tong Liu, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Min Zhang*

Main category: cs.CL

TL;DR: A survey of methods to optimize LLM inference services, covering instance-level, cluster-level, and emerging strategies, plus future research directions.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of high memory and computational demands in LLM inference to improve latency and throughput.

Method: Comprehensive review of optimization techniques at instance, cluster, and scenario levels, including niche areas.

Result: Organized insights into current advancements and practical solutions for LLM inference serving.

Conclusion: Identifies gaps and suggests future research to further enhance LLM inference efficiency.

Abstract: Large Language Models (LLMs) for Generative AI have achieved remarkable
progress, evolving into sophisticated and versatile tools widely adopted across
various domains and applications. However, the substantial memory overhead
caused by their vast number of parameters, combined with the high computational
demands of the attention mechanism, poses significant challenges in achieving
low latency and high throughput for LLM inference services. Recent
advancements, driven by groundbreaking research, have significantly accelerated
progress in this field. This paper provides a comprehensive survey of these
methods, covering fundamental instance-level approaches, in-depth cluster-level
strategies, emerging scenario directions, and other miscellaneous but important
areas. At the instance level, we review model placement, request scheduling,
decoding length prediction, storage management, and the disaggregation
paradigm. At the cluster level, we explore GPU cluster deployment,
multi-instance load balancing, and cloud service solutions. For emerging
scenarios, we organize the discussion around specific tasks, modules, and
auxiliary methods. To ensure a holistic overview, we also highlight several
niche yet critical areas. Finally, we outline potential research directions to
further advance the field of LLM inference serving.

</details>


### [55] [LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding](https://arxiv.org/abs/2504.19734)
*Ying Na, Shihui Feng*

Main category: cs.CL

TL;DR: The paper introduces an LLM-assisted automated coding approach for dialogue data, leveraging communicative acts and events, multiple LLMs, and contextual consistency checking to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Dialogue data is crucial for understanding learning processes, but its contextual complexity poses challenges for automated coding using LLMs.

Method: Developed a framework using dialogue-specific characteristics (acts and events), multiple LLMs (GPT-4-turbo, GPT-4o, DeepSeek), and contextual consistency checking with GPT-4o.

Result: Contextual consistency checking improved accuracy; act predictions were more accurate than event predictions.

Conclusion: The study provides a scalable, precise framework for automated dialogue coding, addressing contextual challenges in dialogue analysis.

Abstract: Dialogue data has been a key source for understanding learning processes,
offering critical insights into how students engage in collaborative
discussions and how these interactions shape their knowledge construction. The
advent of Large Language Models (LLMs) has introduced promising opportunities
for advancing qualitative research, particularly in the automated coding of
dialogue data. However, the inherent contextual complexity of dialogue presents
unique challenges for these models, especially in understanding and
interpreting complex contextual information. This study addresses these
challenges by developing a novel LLM-assisted automated coding approach for
dialogue data. The novelty of our proposed framework is threefold: 1) We
predict the code for an utterance based on dialogue-specific characteristics --
communicative acts and communicative events -- using separate prompts following
the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs
including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We
leveraged the interrelation between events and acts to implement consistency
checking using GPT-4o. In particular, our contextual consistency checking
provided a substantial accuracy improvement. We also found the accuracy of act
predictions was consistently higher than that of event predictions. This study
contributes a new methodological framework for enhancing the precision of
automated coding of dialogue data as well as offers a scalable solution for
addressing the contextual challenges inherent in dialogue analysis.

</details>


### [56] [Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs](https://arxiv.org/abs/2504.19759)
*Huichi Zhou, Zehao Xu, Munan Zhao, Kaihong Li, Yiqiang Li, Hongtao Wang*

Main category: cs.CL

TL;DR: The paper introduces MMRB to assess LLMs' moral reasoning in five languages, showing performance drops with context complexity, especially in low-resource languages like Vietnamese. Fine-tuning LLaMA-3-8B reveals low-resource languages' outsized impact on multilingual reasoning.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve the moral reasoning abilities of LLMs across diverse languages and contextual complexities, addressing gaps in multilingual NLP.

Method: Introduces MMRB for evaluation, fine-tunes LLaMA-3-8B with monolingual data for alignment and poisoning, and tests performance across languages and context levels.

Result: Moral reasoning performance declines with context complexity, notably in low-resource languages. Fine-tuning shows low-resource languages significantly influence multilingual reasoning.

Conclusion: Low-resource languages play a critical role in multilingual NLP, and their impact on reasoning is stronger than high-resource languages, necessitating focused attention.

Abstract: In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)
to evaluate the moral reasoning abilities of large language models (LLMs)
across five typologically diverse languages and three levels of contextual
complexity: sentence, paragraph, and document. Our results show moral reasoning
performance degrades with increasing context complexity, particularly for
low-resource languages such as Vietnamese. We further fine-tune the open-source
LLaMA-3-8B model using curated monolingual data for alignment and poisoning.
Surprisingly, low-resource languages have a stronger impact on multilingual
reasoning than high-resource ones, highlighting their critical role in
multilingual NLP.

</details>


### [57] [Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance](https://arxiv.org/abs/2504.19811)
*Takuya Tamura, Taro Yano, Masafumi Enomoto, Masafumi Oyamada*

Main category: cs.CL

TL;DR: LRMF, a lineage-regularized matrix factorization method, improves LLM performance forecasting by leveraging ancestral ties, outperforming baselines by 7-10% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLM performance forecasting methods ignore lineage relationships, leading to inefficiencies in computational expense and development time.

Method: Proposes LRMF, which uses a graph Laplacian regularizer to encode ancestral ties among LLMs, enhancing performance prediction.

Result: LRMF outperforms baselines by 7-10% in correlation with actual performance and addresses the cold-start problem effectively.

Conclusion: LRMF offers a resource-efficient solution for hyperparameter tuning, data selection, and model combination in LLM development.

Abstract: Accurately forecasting the performance of Large Language Models (LLMs) before
extensive fine-tuning or merging can substantially reduce both computational
expense and development time. Although prior approaches like scaling laws
account for global factors such as parameter size or training tokens, they
often overlook explicit lineage relationships - i.e., which models are derived
or merged from which parents. In this work, we propose a novel
Lineage-Regularized Matrix Factorization (LRMF) framework that encodes
ancestral ties among LLMs via a graph Laplacian regularizer. By leveraging
multi-hop parent-child connections, LRMF consistently outperforms conventional
matrix factorization and collaborative filtering methods in both instance-level
and benchmark-level performance prediction. Our large-scale study includes
2,934 publicly available Hugging Face models and 21,000+ instances across 6
major benchmarks, showing that lineage constraints yield up to 7-10 percentage
points higher correlation with actual performance compared to baselines.
Moreover, LRMF effectively addresses the cold-start problem, providing accurate
estimates for newly derived or merged models even with minimal data. This
lineage-guided strategy thus offers a resource-efficient way to inform
hyperparameter tuning, data selection, and model combination in modern LLM
development.

</details>


### [58] [To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels](https://arxiv.org/abs/2504.19850)
*Kyo Gerrits, Ana Guerberof-Arenas*

Main category: cs.CL

TL;DR: The study explores how creativity and errors in different translation methods (MT, PE, HT, ST) affect cognitive load, finding that creative potential increases cognitive load, with HT having the highest impact and MT the lowest.


<details>
  <summary>Details</summary>
Motivation: To understand how translation modalities influence cognitive load and reader experience, focusing on creativity and errors.

Method: Pilot study with eight participants using questionnaires, eye-tracking, and retrospective think-aloud interviews to analyze cognitive load.

Result: Creative potential (UCP) increases cognitive load, highest in HT and lowest in MT; no error effect observed. Higher cognitive load may link to reader enjoyment.

Conclusion: Translation creativity affects cognitive load, with novel findings at word-level, suggesting further research. Data and code are openly available.

Abstract: This article presents the results of a pilot study involving the reception of
a fictional short story translated from English into Dutch under four
conditions: machine translation (MT), post-editing (PE), human translation (HT)
and original source text (ST). The aim is to understand how creativity and
errors in different translation modalities affect readers, specifically
regarding cognitive load. Eight participants filled in a questionnaire, read a
story using an eye-tracker, and conducted a retrospective think-aloud (RTA)
interview. The results show that units of creative potential (UCP) increase
cognitive load and that this effect is highest for HT and lowest for MT; no
effect of error was observed. Triangulating the data with RTAs leads us to
hypothesize that the higher cognitive load in UCPs is linked to increases in
reader enjoyment and immersion. The effect of translation creativity on
cognitive load in different translation modalities at word-level is novel and
opens up new avenues for further research. All the code and data are available
at https://github.com/INCREC/Pilot_to_MT_or_not_to_MT

</details>


### [59] [Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language](https://arxiv.org/abs/2504.19856)
*Anastasia Zhukova, Christian E. Matt, Terry Ruas, Bela Gipp*

Main category: cs.CL

TL;DR: ICL-APT, an efficient alternative to DAPT, uses in-context learning and kNN to reduce GPU time and improve performance in low-resource domains.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of obtaining domain-specific data for non-English languages and resource-limited industries.

Method: Leverages in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data, reducing computational demands.

Result: Outperforms DAPT by 3.5 in IR metrics and reduces computing time by 4x.

Conclusion: ICL-APT offers a cost-effective, scalable solution for low-resource domains, enhancing NLP accessibility.

Abstract: Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique
that further trains a language model (LM) on its pretraining task, e.g.,
language masking. Although popular, it requires a significant corpus of
domain-related data, which is difficult to obtain for specific domains in
languages other than English, such as the process industry in the German
language. This paper introduces an efficient approach called ICL-augmented
pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest
neighbors (kNN) to augment target data with domain-related and in-domain texts,
significantly reducing GPU time while maintaining strong model performance. Our
results show that this approach performs better than traditional DAPT by 3.5 of
the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times
less computing time, providing a cost-effective solution for industries with
limited computational capacity. The findings highlight the broader
applicability of this framework to other low-resource industries, making
NLP-based solutions more accessible and feasible in production environments.

</details>


### [60] [semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage](https://arxiv.org/abs/2504.19867)
*Ke Hong, Lufang Chen, Zhong Wang, Xiuhong Li, Qiuli Mao, Jianping Ma, Chao Xiong, Guanyu Wu, Buhe Han, Guohao Dai, Yun Liang, Yu Wang*

Main category: cs.CL

TL;DR: The paper proposes semi-PD, a novel LLM serving system with disaggregated computation and unified storage to address storage inefficiencies in existing systems, improving latency and request handling.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving systems face storage inefficiencies due to replicated weights, KV cache transfer overhead, storage imbalance, and suboptimal resource adjustment, leading to poor performance under high request rates.

Method: The semi-PD system introduces a computation resource controller for disaggregated computation at the SM level and a unified memory manager for asynchronous memory access. It includes a low-overhead resource adjustment mechanism and an SLO-aware dynamic partitioning algorithm.

Result: semi-PD reduces average end-to-end latency by 1.27-2.58x on DeepSeek models and serves 1.55-1.72x more requests under latency constraints on Llama models compared to state-of-the-art systems.

Conclusion: semi-PD effectively addresses storage inefficiencies in LLM serving systems, improving performance and scalability under high request rates.

Abstract: Existing large language model (LLM) serving systems fall into two categories:
1) a unified system where prefill phase and decode phase are co-located on the
same GPU, sharing the unified computational resource and storage, and 2) a
disaggregated system where the two phases are disaggregated to different GPUs.
The design of the disaggregated system addresses the latency interference and
sophisticated scheduling issues in the unified system but leads to storage
challenges including 1) replicated weights for both phases that prevent
flexible deployment, 2) KV cache transfer overhead between the two phases, 3)
storage imbalance that causes substantial wasted space of the GPU capacity, and
4) suboptimal resource adjustment arising from the difficulties in migrating KV
cache. Such storage inefficiency delivers poor serving performance under high
request rates.
  In this paper, we identify that the advantage of the disaggregated system
lies in the disaggregated computation, i.e., partitioning the computational
resource to enable the asynchronous computation of two phases. Thus, we propose
a novel LLM serving system, semi-PD, characterized by disaggregated computation
and unified storage. In semi-PD, we introduce a computation resource controller
to achieve disaggregated computation at the streaming multi-processor (SM)
level, and a unified memory manager to manage the asynchronous memory access
from both phases. semi-PD has a low-overhead resource adjustment mechanism
between the two phases, and a service-level objective (SLO) aware dynamic
partitioning algorithm to optimize the SLO attainment. Compared to
state-of-the-art systems, semi-PD maintains lower latency at higher request
rates, reducing the average end-to-end latency per request by 1.27-2.58x on
DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency
constraints on Llama series models.

</details>


### [61] [GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets](https://arxiv.org/abs/2504.19898)
*Mingqian He, Fei Zhao, Chonggang Lu, Ziyan Liu, Yue Wang, Haofu Qian*

Main category: cs.CL

TL;DR: GenCLS++ introduces a framework combining SFT and RL for generative text classification, improving accuracy by 3.46% on average, and highlights the inefficacy of explicit reasoning steps in classification tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional discriminative methods underutilize LLMs' generative strengths, and existing generative classifiers lack systematic integration of SFT, RL, and inference prompting.

Method: GenCLS++ jointly optimizes SFT and RL, exploring five strategy dimensions (e.g., in-context learning, category definitions) during training and inference.

Result: Achieves 3.46% average accuracy improvement over SFT baselines, with 4.00% on public datasets, and shows explicit reasoning steps hinder performance.

Conclusion: The framework advances generative text classification and provides insights into avoiding explicit reasoning for better performance in LLM applications.

Abstract: As a fundamental task in machine learning, text classification plays a
crucial role in many areas. With the rapid scaling of Large Language Models
(LLMs), particularly through reinforcement learning (RL), there is a growing
need for more capable discriminators. Consequently, advances in classification
are becoming increasingly vital for enhancing the overall capabilities of LLMs.
Traditional discriminative methods map text to labels but overlook LLMs'
intrinsic generative strengths. Generative classification addresses this by
prompting the model to directly output labels. However, existing studies still
rely on simple SFT alone, seldom probing the interplay between training and
inference prompts, and no work has systematically leveraged RL for generative
text classifiers and unified SFT, RL, and inference-time prompting in one
framework. We bridge this gap with GenCLS++, a framework that jointly optimizes
SFT and RL while systematically exploring five high-level strategy
dimensions-in-context learning variants, category definitions, explicit
uncertainty labels, semantically irrelevant numeric labels, and
perplexity-based decoding-during both training and inference. After an SFT
"policy warm-up," we apply RL with a simple rule-based reward, yielding sizable
extra gains. Across seven datasets, GenCLS++ achieves an average accuracy
improvement of 3.46% relative to the naive SFT baseline; on public datasets,
this improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that
benefit from explicit thinking processes, we find that classification tasks
perform better without such reasoning steps. These insights into the role of
explicit reasoning provide valuable guidance for future LLM applications.

</details>


### [62] [TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons](https://arxiv.org/abs/2504.19982)
*Emre Can Acikgoz, Carl Guo, Suvodip Dey, Akul Datta, Takyoung Kim, Gokhan Tur, Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: TD-EVAL is a two-step evaluation framework for task-oriented dialogue systems, combining turn-level and dialogue-level analysis to improve error detection and alignment with human judgments.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics for TOD systems are insufficient for modern LLM-driven systems, lacking granularity to detect intermediate errors.

Method: TD-EVAL evaluates turn-level responses on three dimensions (conversation cohesion, backend knowledge consistency, policy compliance) and uses TOD Agent Arena for dialogue-level pairwise comparisons.

Result: TD-EVAL outperforms conventional and LLM-based metrics in identifying conversational errors and aligns better with human judgments.

Conclusion: TD-EVAL offers a plug-and-play framework for comprehensive TOD system evaluation, addressing gaps in current methodologies.

Abstract: Task-oriented dialogue (TOD) systems are experiencing a revolution driven by
Large Language Models (LLMs), yet the evaluation methodologies for these
systems remain insufficient for their growing sophistication. While traditional
automatic metrics effectively assessed earlier modular systems, they focus
solely on the dialogue level and cannot detect critical intermediate errors
that can arise during user-agent interactions. In this paper, we introduce
TD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework
that unifies fine-grained turn-level analysis with holistic dialogue-level
comparisons. At turn level, we evaluate each response along three TOD-specific
dimensions: conversation cohesion, backend knowledge consistency, and policy
compliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons
to provide a measure of dialogue-level quality. Through experiments on MultiWOZ
2.4 and {\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the
conversational errors that conventional metrics miss. Furthermore, TD-EVAL
exhibits better alignment with human judgments than traditional and LLM-based
metrics. These findings demonstrate that TD-EVAL introduces a new paradigm for
TOD system evaluation, efficiently assessing both turn and system levels with a
plug-and-play framework for future research.

</details>


### [63] [Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom](https://arxiv.org/abs/2504.20000)
*Rishika Sen, Sujoy Roychowdhury, Sumit Soman, H. G. Ranjani, Srikhetra Mohanty*

Main category: cs.CL

TL;DR: The paper explores Knowledge Distillation (KD) for domain-specific tasks in telecom QA, comparing SFT of teacher, student, or both, and the impact of vocabulary and KD algorithms.


<details>
  <summary>Details</summary>
Motivation: To determine the best approach for domain adaptation in KD for telecom QA tasks, considering the roles of teacher and student models.

Method: Systematic experiments with SFT of teacher, student, or both, varying vocabulary and KD algorithms (vanilla KD and DSKD), evaluated using 14 metrics.

Result: SFT of teacher improves performance when vocabularies match; SFT of both models yields the best results, though significance varies with teacher vocabulary.

Conclusion: SFT of both teacher and student before KD is optimal for telecom QA, with vocabulary alignment playing a key role in performance.

Abstract: Knowledge Distillation (KD) is one of the approaches to reduce the size of
Large Language Models (LLMs). A LLM with smaller number of model parameters
(student) is trained to mimic the performance of a LLM of a larger size
(teacher model) on a specific task. For domain-specific tasks, it is not clear
if teacher or student model, or both, must be considered for domain adaptation.
In this work, we study this problem from perspective of telecom domain
Question-Answering (QA) task. We systematically experiment with Supervised
Fine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to
KD. We design experiments to study the impact of vocabulary (same and
different) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the
distilled model. Multi-faceted evaluation of the distillation using 14
different metrics (N-gram, embedding and LLM-based metrics) is considered.
Experimental results show that SFT of teacher improves performance of distilled
model when both models have same vocabulary, irrespective of algorithm and
metrics. Overall, SFT of both teacher and student results in better performance
across all metrics, although the statistical significance of the same depends
on the vocabulary of the teacher models.

</details>


### [64] [LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation](https://arxiv.org/abs/2504.20013)
*Beizhe Hu, Qiang Sheng, Juan Cao, Yang Li, Danding Wang*

Main category: cs.CL

TL;DR: The study explores the impact of LLM-generated fake news on neural news recommendation systems, revealing a 'truth decay' phenomenon where fake news gains advantage over real news. It also suggests countermeasures to address this threat.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-generated fake news poses a new challenge to online moderation, and its large-scale impact on the news ecosystem is underexplored.

Method: A simulation pipeline and a dataset of ~56k diverse LLM-generated news items were used to study effects on neural news recommendation systems.

Result: Findings show 'truth decay,' where fake news outperforms real news in rankings, with perplexity positively correlating to ranking.

Conclusion: The study highlights the threat of LLM-generated fake news and calls for stakeholder action to protect news ecosystem integrity.

Abstract: Online fake news moderation now faces a new challenge brought by the
malicious use of large language models (LLMs) in fake news production. Though
existing works have shown LLM-generated fake news is hard to detect from an
individual aspect, it remains underexplored how its large-scale release will
impact the news ecosystem. In this study, we develop a simulation pipeline and
a dataset with ~56k generated news of diverse types to investigate the effects
of LLM-generated fake news within neural news recommendation systems. Our
findings expose a truth decay phenomenon, where real news is gradually losing
its advantageous position in news ranking against fake news as LLM-generated
news is involved in news recommendation. We further provide an explanation
about why truth decay occurs from a familiarity perspective and show the
positive correlation between perplexity and news ranking. Finally, we discuss
the threats of LLM-generated fake news and provide possible countermeasures. We
urge stakeholders to address this emerging challenge to preserve the integrity
of news ecosystems.

</details>


### [65] [Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages](https://arxiv.org/abs/2504.20022)
*Pritika Rohera, Chaitrali Ginimav, Gayatri Sawant, Raviraj Joshi*

Main category: cs.CL

TL;DR: LLMs like GPT-4o and Gemma perform better in English than Indic languages, with higher hallucination rates in low-resource Indic languages.


<details>
  <summary>Details</summary>
Motivation: To evaluate the factual accuracy of multilingual LLMs in English and Indic languages, focusing on regional context questions.

Method: Assessed LLMs (GPT-4o, Gemma-2-9B, Gemma-2-2B, Llama-3.1-8B) using the IndicQuest dataset with Q&A pairs in English and 19 Indic languages.

Result: LLMs perform better in English, even for Indic context questions, with more hallucinations in low-resource Indic languages.

Conclusion: Current LLMs face challenges in multilingual understanding, especially for low-resource languages like Indic languages.

Abstract: Multilingual Large Language Models (LLMs) have demonstrated significant
effectiveness across various languages, particularly in high-resource languages
such as English. However, their performance in terms of factual accuracy across
other low-resource languages, especially Indic languages, remains an area of
investigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,
Gemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in
English and Indic languages using the IndicQuest dataset, which contains
question-answer pairs in English and 19 Indic languages. By asking the same
questions in English and their respective Indic translations, we analyze
whether the models are more reliable for regional context questions in Indic
languages or when operating in English. Our findings reveal that LLMs often
perform better in English, even for questions rooted in Indic contexts.
Notably, we observe a higher tendency for hallucination in responses generated
in low-resource Indic languages, highlighting challenges in the multilingual
understanding capabilities of current LLMs.

</details>


### [66] [AutoJudge: Judge Decoding Without Manual Annotation](https://arxiv.org/abs/2504.20039)
*Roman Garipov, Fedor Velikonivtsev, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin*

Main category: cs.CL

TL;DR: AutoJudge accelerates LLM inference by identifying and skipping unimportant tokens during speculative decoding, maintaining quality while improving speed.


<details>
  <summary>Details</summary>
Motivation: To speed up LLM inference without significant loss in output quality by focusing on task-specific important tokens.

Method: Uses a semi-greedy search algorithm and a lightweight classifier to predict which mismatching tokens can be skipped. Tested with Llama models on GSM8K and LiveCodeBench.

Result: Achieves 1.5x more accepted tokens with <1% accuracy loss and 2x speedup with minor accuracy trade-offs.

Conclusion: AutoJudge generalizes across tasks, balancing speed and quality effectively.

Abstract: We introduce AutoJudge, a framework that accelerates large language model
(LLM) inference with task-specific lossy speculative decoding. Instead of
matching the original model output distribution token-by-token, we identify
which of the generated tokens affect the downstream quality of the generated
response, relaxing the guarantee so that the "unimportant" tokens can be
generated faster. Our approach relies on a semi-greedy search algorithm to test
which of the mismatches between target and draft model should be corrected to
preserve quality, and which ones may be skipped. We then train a lightweight
classifier based on existing LLM embeddings to predict, at inference time,
which mismatching tokens can be safely accepted without compromising the final
answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B
(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more
accepted tokens per verification cycle with under 1% degradation in answer
accuracy compared to standard speculative decoding and over 2x with small loss
in accuracy. When applied to the LiveCodeBench benchmark, our approach
automatically detects other, programming-specific important tokens and shows
similar speedups, demonstrating its ability to generalize across tasks.

</details>


### [67] [A Bayesian approach to modeling topic-metadata relationships](https://arxiv.org/abs/2104.02496)
*P. Schulze, S. Wiegrebe, P. W. Thurner, C. Heumann, M. Aßenmacher*

Main category: cs.CL

TL;DR: The paper proposes improvements to topic modeling by refining the method of composition with Beta regression and introducing a fully Bayesian approach for better uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: To enhance the estimation of relationships between discovered topics and metadata, addressing limitations in current methods.

Method: Replaces linear regression with Beta regression in the method of composition and shifts to a fully Bayesian framework.

Result: Improved methodology demonstrated by analyzing Twitter posts of German parliamentarians and their electoral district metadata.

Conclusion: The proposed modifications offer more accurate and robust topic-metadata relationship estimation.

Abstract: The objective of advanced topic modeling is not only to explore latent
topical structures, but also to estimate relationships between the discovered
topics and theoretically relevant metadata. Methods used to estimate such
relationships must take into account that the topical structure is not directly
observed, but instead being estimated itself in an unsupervised fashion,
usually by common topic models. A frequently used procedure to achieve this is
the method of composition, a Monte Carlo sampling technique performing multiple
repeated linear regressions of sampled topic proportions on metadata
covariates. In this paper, we propose two modifications of this approach:
First, we substantially refine the existing implementation of the method of
composition from the R package stm by replacing linear regression with the more
appropriate Beta regression. Second, we provide a fundamental enhancement of
the entire estimation framework by substituting the current blending of
frequentist and Bayesian methods with a fully Bayesian approach. This allows
for a more appropriate quantification of uncertainty. We illustrate our
improved methodology by investigating relationships between Twitter posts by
German parliamentarians and different metadata covariates related to their
electoral districts, using the Structural Topic Model to estimate topic
proportions.

</details>


### [68] [Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information](https://arxiv.org/abs/2110.08420)
*Kawin Ethayarajh, Yejin Choi, Swabha Swayamdipta*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Estimating the difficulty of a dataset typically involves comparing
state-of-the-art models to humans; the bigger the performance gap, the harder
the dataset is said to be. However, this comparison provides little
understanding of how difficult each instance in a given distribution is, or
what attributes make the dataset difficult for a given model. To address these
questions, we frame dataset difficulty -- w.r.t. a model $\mathcal{V}$ -- as
the lack of $\mathcal{V}$-$\textit{usable information}$ (Xu et al., 2019),
where a lower value indicates a more difficult dataset for $\mathcal{V}$. We
further introduce $\textit{pointwise $\mathcal{V}$-information}$ (PVI) for
measuring the difficulty of individual instances w.r.t. a given distribution.
While standard evaluation metrics typically only compare different models for
the same dataset, $\mathcal{V}$-$\textit{usable information}$ and PVI also
permit the converse: for a given model $\mathcal{V}$, we can compare different
datasets, as well as different instances/slices of the same dataset.
Furthermore, our framework allows for the interpretability of different input
attributes via transformations of the input, which we use to discover
annotation artefacts in widely-used NLP benchmarks.

</details>


### [69] [Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach](https://arxiv.org/abs/2112.06876)
*Eugene Yu Ji*

Main category: cs.CL

TL;DR: The paper introduces a geometric metric for mapping semantic and pragmatic features in NLP, outperforming traditional benchmarks and highlighting socio-cultural relevance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in modeling interactions between semantic and pragmatic features and integrating insights from related disciplines.

Method: A novel geometric metric using word co-occurrence patterns to map semantic typicality and pragmatic salience in hyperbolic space.

Result: The metric outperforms cognitive semantics benchmarks and shows socio-cultural relevance, suggesting basic-level categories are cognitive-cultural interfaces.

Conclusion: The study contributes interpretable, human-centric language models blending semantic and pragmatic dimensions to understand linguistic categories.

Abstract: In recent years, the field of NLP has seen growing interest in modeling both
semantic and pragmatic dimensions. Despite this progress, two key challenges
persist: firstly, the complex task of mapping and analyzing the interactions
between semantic and pragmatic features; secondly, the insufficient
incorporation of relevant insights from related disciplines outside NLP.
Addressing these issues, this study introduces a novel geometric metric that
utilizes word co-occurrence patterns. This metric maps two fundamental
properties - semantic typicality (cognitive) and pragmatic salience
(socio-cultural) - for basic-level categories within a two-dimensional
hyperbolic space. Our evaluations reveal that this semantic-pragmatic metric
produces mappings for basic-level categories that not only surpass traditional
cognitive semantics benchmarks but also demonstrate significant socio-cultural
relevance. This finding proposes that basic-level categories, traditionally
viewed as semantics-driven cognitive constructs, should be examined through the
lens of both semantic and pragmatic dimensions, highlighting their role as a
cognitive-cultural interface. The broad contribution of this paper lies in the
development of medium-sized, interpretable, and human-centric language
embedding models, which can effectively blend semantic and pragmatic dimensions
to elucidate both the cognitive and socio-cultural significance of linguistic
categories.

</details>


### [70] [Generative Meta-Learning for Zero-Shot Relation Triplet Extraction](https://arxiv.org/abs/2305.01920)
*Wanli Li, Tieyun Qian, Yi Song, Zeyu Zhang, Jiawei Li, Zhuang Chen, Lixin Zou*

Main category: cs.CL

TL;DR: The paper proposes a generative meta-learning framework integrating bi-level optimization (BLO) with pre-trained language models to enhance generalization in Zero-shot Relation Triplet Extraction (ZeroRTE).


<details>
  <summary>Details</summary>
Motivation: Existing methods for ZeroRTE lack explicit focus on improving generalization to unseen relation types, relying solely on pre-trained language models.

Method: The framework uses BLO to balance data fitting and generalization, introducing upper-level (generalization) and lower-level (data fitting) losses. Three generative meta-learning methods are developed.

Result: Experiments show the framework performs well on ZeroRTE tasks.

Conclusion: The proposed approach effectively boosts generalization in ZeroRTE by leveraging meta-learning and BLO.

Abstract: Zero-shot Relation Triplet Extraction (ZeroRTE) aims to extract relation
triplets from texts containing unseen relation types. This capability benefits
various downstream information retrieval (IR) tasks. The primary challenge lies
in enabling models to generalize effectively to unseen relation categories.
Existing approaches typically leverage the knowledge embedded in pre-trained
language models to accomplish the generalization process. However, these
methods focus solely on fitting the training data during training, without
specifically improving the model's generalization performance, resulting in
limited generalization capability. For this reason, we explore the integration
of bi-level optimization (BLO) with pre-trained language models for learning
generalized knowledge directly from the training data, and propose a generative
meta-learning framework which exploits the `learning-to-learn' ability of
meta-learning to boost the generalization capability of generative models.
  Specifically, we introduce a BLO approach that simultaneously addresses data
fitting and generalization. This is achieved by constructing an upper-level
loss to focus on generalization and a lower-level loss to ensure accurate data
fitting. Building on this, we subsequently develop three generative
meta-learning methods, each tailored to a distinct category of meta-learning.
Extensive experimental results demonstrate that our framework performs well on
the ZeroRTE task. Our code is available at
https://github.com/leeworry/TGM-MetaLearning.

</details>


### [71] [Benchmarking large language models for biomedical natural language processing applications and recommendations](https://arxiv.org/abs/2305.16326)
*Qingyu Chen, Yan Hu, Xueqing Peng, Qianqian Xie, Qiao Jin, Aidan Gilson, Maxwell B. Singer, Xuguang Ai, Po-Ting Lai, Zhizheng Wang, Vipina Kuttichi Keloth, Kalpana Raja, Jiming Huang, Huan He, Fongci Lin, Jingcheng Du, Rui Zhang, W. Jim Zheng, Ron A. Adelman, Zhiyong Lu, Hua Xu*

Main category: cs.CL

TL;DR: Traditional fine-tuning outperforms zero/few-shot LLMs in BioNLP tasks, but GPT-4 excels in reasoning tasks. Open-source LLMs need fine-tuning to match performance, with issues like hallucinations noted.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of biomedical literature necessitates automated NLP solutions, but LLMs' effectiveness in BioNLP lacks clear benchmarks and guidelines.

Method: Systematic evaluation of four LLMs (GPT and LLaMA) on 12 BioNLP benchmarks across six applications, comparing zero-shot, few-shot, and fine-tuning performance against BERT/BART models.

Result: Traditional fine-tuning beats zero/few-shot LLMs in most tasks; GPT-4 excels in reasoning tasks. Open-source LLMs lag without fine-tuning, with issues like hallucinations.

Conclusion: LLMs show promise in BioNLP but require fine-tuning and caution due to inconsistencies. GPT-4 is strong in reasoning, while open-source models need improvement.

Abstract: The rapid growth of biomedical literature poses challenges for manual
knowledge curation and synthesis. Biomedical Natural Language Processing
(BioNLP) automates the process. While Large Language Models (LLMs) have shown
promise in general domains, their effectiveness in BioNLP tasks remains unclear
due to limited benchmarks and practical guidelines.
  We perform a systematic evaluation of four LLMs, GPT and LLaMA
representatives on 12 BioNLP benchmarks across six applications. We compare
their zero-shot, few-shot, and fine-tuning performance with traditional
fine-tuning of BERT or BART models. We examine inconsistencies, missing
information, hallucinations, and perform cost analysis. Here we show that
traditional fine-tuning outperforms zero or few shot LLMs in most tasks.
However, closed-source LLMs like GPT-4 excel in reasoning-related tasks such as
medical question answering. Open source LLMs still require fine-tuning to close
performance gaps. We find issues like missing information and hallucinations in
LLM outputs. These results offer practical insights for applying LLMs in
BioNLP.

</details>


### [72] [Ragas: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)
*Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert*

Main category: cs.CL

TL;DR: Ragas is a framework for evaluating RAG pipelines without ground truth annotations, focusing on retrieval, generation, and faithfulness.


<details>
  <summary>Details</summary>
Motivation: Evaluating RAG systems is complex due to multiple dimensions like retrieval relevance and generation quality, and ground truth annotations are often unavailable.

Method: Ragas introduces a suite of metrics to assess retrieval, generation, and faithfulness in RAG pipelines without human annotations.

Result: The framework enables faster evaluation cycles for RAG architectures, addressing the rapid adoption of LLMs.

Conclusion: Ragas provides a scalable, reference-free solution for evaluating RAG systems, crucial for efficient development and deployment.

Abstract: We introduce Ragas (Retrieval Augmented Generation Assessment), a framework
for reference-free evaluation of Retrieval Augmented Generation (RAG)
pipelines. RAG systems are composed of a retrieval and an LLM based generation
module, and provide LLMs with knowledge from a reference textual database,
which enables them to act as a natural language layer between a user and
textual databases, reducing the risk of hallucinations. Evaluating RAG
architectures is, however, challenging because there are several dimensions to
consider: the ability of the retrieval system to identify relevant and focused
context passages, the ability of the LLM to exploit such passages in a faithful
way, or the quality of the generation itself. With Ragas, we put forward a
suite of metrics which can be used to evaluate these different dimensions
\textit{without having to rely on ground truth human annotations}. We posit
that such a framework can crucially contribute to faster evaluation cycles of
RAG architectures, which is especially important given the fast adoption of
LLMs.

</details>


### [73] [Large language models for newspaper sentiment analysis during COVID-19: The Guardian](https://arxiv.org/abs/2405.13056)
*Rohitash Chandra, Baicheng Zhu, Qingying Fang, Eka Shinjikashvili*

Main category: cs.CL

TL;DR: The study analyzes sentiment in The Guardian newspaper during COVID-19 stages (transmission, lockdowns, vaccination) using LLMs and expert-labelled data, revealing a dominance of negative sentiments compared to social media's diverse emotional reflection.


<details>
  <summary>Details</summary>
Motivation: To understand media sentiment during COVID-19 and compare it with pre-pandemic sentiments and social media analyses.

Method: Employed novel large language models (LLMs) refined with expert-labelled sentiment analysis data on The Guardian's coverage.

Result: Early pandemic sentiment focused on crisis response, later shifting to health and economy. The Guardian showed a grim narrative with dominant negative sentiments, unlike social media's diversity.

Conclusion: The Guardian's coverage was consistently negative, differing from social media's varied emotional responses, highlighting media's role in shaping public sentiment during crises.

Abstract: During the COVID-19 pandemic, the news media coverage encompassed a wide
range of topics that includes viral transmission, allocation of medical
resources, and government response measures. There have been studies on
sentiment analysis of social media platforms during COVID-19 to understand the
public response given the rise of cases and government strategies implemented
to control the spread of the virus. Sentiment analysis can provide a better
understanding of changes in societal opinions and emotional trends during the
pandemic. Apart from social media, newspapers have played a vital role in the
dissemination of information, including information from the government,
experts, and also the public about various topics. A study of sentiment
analysis of newspaper sources during COVID-19 for selected countries can give
an overview of how the media covered the pandemic. In this study, we select The
Guardian newspaper and provide a sentiment analysis during various stages of
COVID-19 that includes initial transmission, lockdowns and vaccination. We
employ novel large language models (LLMs) and refine them with expert-labelled
sentiment analysis data. We also provide an analysis of sentiments experienced
pre-pandemic for comparison. The results indicate that during the early
pandemic stages, public sentiment prioritised urgent crisis response, later
shifting focus to addressing the impact on health and the economy. In
comparison with related studies about social media sentiment analyses, we found
a discrepancy between The Guardian with dominance of negative sentiments (sad,
annoyed, anxious and denial), suggesting that social media offers a more
diversified emotional reflection. We found a grim narrative in The Guardian
with overall dominance of negative sentiments, pre and during COVID-19 across
news sections including Australia, UK, World News, and Opinion

</details>


### [74] [Fake News Detection: It's All in the Data!](https://arxiv.org/abs/2407.02122)
*Soveatin Kuntur, Anna Wróblewska, Marcin Paprzycki, Maria Ganzha*

Main category: cs.CL

TL;DR: A survey on fake news detection emphasizing dataset quality, diversity, and ethical issues, with a GitHub repository for accessible datasets.


<details>
  <summary>Details</summary>
Motivation: To provide researchers with a resource highlighting the importance of dataset quality and diversity in fake news detection, addressing biases and ethical concerns.

Method: Survey of key dataset features, labeling systems, biases, and ethical issues, supplemented by a GitHub repository of datasets.

Result: Identifies critical dataset attributes and biases, offers a centralized dataset repository to aid research.

Conclusion: The survey and repository aim to enhance fake news detection research by improving dataset accessibility and awareness of ethical considerations.

Abstract: This comprehensive survey serves as an indispensable resource for researchers
embarking on the journey of fake news detection. By highlighting the pivotal
role of dataset quality and diversity, it underscores the significance of these
elements in the effectiveness and robustness of detection models. The survey
meticulously outlines the key features of datasets, various labeling systems
employed, and prevalent biases that can impact model performance. Additionally,
it addresses critical ethical issues and best practices, offering a thorough
overview of the current state of available datasets. Our contribution to this
field is further enriched by the provision of GitHub repository, which
consolidates publicly accessible datasets into a single, user-friendly portal.
This repository is designed to facilitate and stimulate further research and
development efforts aimed at combating the pervasive issue of fake news.

</details>


### [75] [Pula: Training Large Language Models for Setswana](https://arxiv.org/abs/2408.02239)
*Nathan Brown, Vukosi Marivate*

Main category: cs.CL

TL;DR: Pula is a bilingual (Setswana-English) language model suite outperforming GPT-4o and Gemini 1.5 Pro in translation and reasoning tasks. It includes released weights, datasets, and benchmarks.


<details>
  <summary>Details</summary>
Motivation: To advance Setswana language processing by creating high-performing bilingual models and comprehensive datasets.

Method: Leveraged data availability and efficient fine-tuning to develop Pula models (1B to 14B), alongside datasets (Marothodi, Medupi) and benchmarks (MMLU-tsn, GSM8K-tsn).

Result: Pula 8B and 14B outperform GPT-4o and Gemini 1.5 Pro in English-Setswana translation and Setswana reasoning tasks.

Conclusion: Pula and its resources significantly advance Setswana NLP, providing tools and benchmarks for future research.

Abstract: In this work we present Pula, a suite of bilingual language models proficient
in both Setswana and English. Leveraging recent advancements in data
availability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o
and Gemini 1.5 Pro on English-Setswana translation tasks and achieve
state-of-the-art performance on Setswana reasoning tasks for their size. We
release the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and
training and evaluation code. Alongside Pula, we release the largest-ever
Setswana text corpus, Marothodi, and the first comprehensive Setswana
instruction-tuning dataset, Medupi, consisting of reformatted datasets,
translated corpora, and synthetic LLM-generated text. To accompany this data,
we release the code used for dataset construction, formatting, filtering, and
scraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and
GSM8K-tsn, to measure Setswana knowledge and reasoning capabilities.

</details>


### [76] [AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising](https://arxiv.org/abs/2408.05906)
*Peinan Zhang, Yusuke Sakai, Masato Mita, Hiroki Ouchi, Taro Watanabe*

Main category: cs.CL

TL;DR: AdTEC is the first public benchmark for evaluating ad texts, defining five tasks and validating PLMs and human performance, showing PLMs are practical but humans still outperform in some areas.


<details>
  <summary>Details</summary>
Motivation: To address the need for verifying the quality of automatically generated ad texts in real-world settings.

Method: Proposed AdTEC benchmark with five evaluation tasks, built a Japanese dataset from advertising agency experiences, and tested PLMs and human evaluators.

Result: PLMs are practical for some tasks, but humans outperform in certain domains, indicating room for improvement.

Conclusion: AdTEC provides a valuable benchmark for ad text evaluation, highlighting the gap between PLMs and human performance.

Abstract: With the increase in the fluency of ad texts automatically created by natural
language generation technology, there is high demand to verify the quality of
these creatives in a real-world setting. We propose AdTEC (Ad Text Evaluation
Benchmark by CyberAgent), the first public benchmark to evaluate ad texts from
multiple perspectives within practical advertising operations. Our
contributions are as follows: (i) Defining five tasks for evaluating the
quality of ad texts, as well as building a Japanese dataset based on the
practical operational experiences of building a Japanese dataset based on the
practical operational experiences of advertising agencies, which are typically
kept in-house. (ii) Validating the performance of existing pre-trained language
models (PLMs) and human evaluators on the dataset. (iii) Analyzing the
characteristics and providing challenges of the benchmark. The results show
that while PLMs have already reached practical usage level in several tasks,
humans still outperform in certain domains, implying that there is significant
room for improvement in this area.

</details>


### [77] [W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering](https://arxiv.org/abs/2408.08444)
*Jinming Nian, Zhiyuan Peng, Qifan Wang, Yi Fang*

Main category: cs.CL

TL;DR: W-RAG improves retrieval and OpenQA performance by using weak training signals from LLM tasks to fine-tune retrievers, achieving results close to human-labeled data.


<details>
  <summary>Details</summary>
Motivation: Addresses LLMs' limitations in factual accuracy by enhancing retrievers with task-specific signals, avoiding costly human annotation.

Method: Reranks BM25-retrieved passages using LLM-generated answer probabilities, then fine-tunes dense retrieval with top passages.

Result: Outperforms baselines in retrieval and OpenQA, matching performance of human-labeled fine-tuning.

Conclusion: W-RAG offers a cost-effective alternative to human annotation for improving retrieval-augmented LLMs.

Abstract: In knowledge-intensive tasks such as open-domain question answering (OpenQA),
large language models (LLMs) often struggle to generate factual answers,
relying solely on their internal (parametric) knowledge. To address this
limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by
retrieving relevant information from external sources, thereby positioning the
retriever as a pivotal component. Although dense retrieval demonstrates
state-of-the-art performance, its training poses challenges due to the scarcity
of ground-truth evidence, largely attributed to the high costs of human
annotation. In this paper, we propose W-RAG, a method that draws weak training
signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the
retriever to prioritize passages that most benefit the task. Specifically, we
rerank the top-$k$ passages retrieved via BM25 by assessing the probability
that the LLM will generate the correct answer for a question given each
passage. The highest-ranking passages are then used as positive fine-tuning
examples for dense retrieval. We conduct comprehensive experiments across four
publicly available OpenQA datasets to demonstrate that our approach enhances
both retrieval and OpenQA performance compared to baseline models, achieving
results comparable to models fine-tuned with human-labeled data.

</details>


### [78] [LaMsS: When Large Language Models Meet Self-Skepticism](https://arxiv.org/abs/2409.06601)
*Yetao Wu, Yihong Wang, Teng Chen, Ningyuan Xi, Qingqing Gu, Hongyang Lei, Luo Ji*

Main category: cs.CL

TL;DR: LaMsS introduces self-skepticism into LLMs to reduce hallucinations by augmenting skepticism tokens and thresholds, improving performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address LLM hallucinations by leveraging human skeptical thinking for self-cognition and reflection.

Method: Augment LLM vocabulary with skepticism tokens, perform pre-training and fine-tuning, and use skepticism thresholds to filter responses.

Result: LaMsS outperforms baselines in accuracy, AUC, and AP on multi-choice and open-domain QA tasks, generalizing to multi-task and out-of-domain settings.

Conclusion: Self-skepticism modeling in LLMs can mitigate hallucinations and enhance reliability, with potential for broader AI applications.

Abstract: Hallucination is a major challenge for large language models (LLMs),
preventing their further application in some fields. The skeptical thinking of
humankind could be useful for LLMs to self-cognition, self-reflection and
alleviate their hallucinations. Inspired by this consideration, we propose a
novel approach called LaMsS, which combines the semantic understanding
capability of LLMs with self-skepticism. By introducing a series of skepticism
tokens and augmenting them into the vocabulary, we conduct both pertaining and
finetuning, which allow the LLM to decode each normal token followed by a
skeptical token, representing different skepticism levels. By calculating the
response skepticism given a query, one can define a new self-aware LLM which is
only willing to answer with relative lower skepticism level than the threshold.
By examining the accuracy, AUC and AP of willingly answering questions, we
demonstrate that LaMsS achieves better performance than baselines on both
multi-choice questions and open-domain question-answering benchmarks, and can
generalize to multi-task and out-of-domain settings. Our study sheds some
lights on the self-skepticism modeling on further artificial intelligence.
Project code and model checkpoints can be found in
https://anonymous.4open.science/r/SM-1E76.

</details>


### [79] [Data Processing for the OpenGPT-X Model Family](https://arxiv.org/abs/2410.08800)
*Nicolo' Brandizzi, Hammam Abdelwahab, Anirban Bhowmick, Lennard Helmer, Benny Jörg Stein, Pavel Denisov, Qasid Saleem, Michael Fromm, Mehdi Ali, Richard Rutmann, Farzad Naderi, Mohamad Saif Agy, Alexander Schwirjow, Fabian Küch, Luzian Hahn, Malte Ostendorff, Pedro Ortiz Suarez, Georg Rehm, Dennis Wegener, Nicolas Flores-Herr, Joachim Köhler, Johannes Leveling*

Main category: cs.CL

TL;DR: The paper details the data preparation pipeline for OpenGPT-X, focusing on multilingual LLMs for European languages, emphasizing data selection, processing, and regulatory compliance.


<details>
  <summary>Details</summary>
Motivation: To create open, high-performance multilingual LLMs for European languages, addressing real-world EU applications and ensuring transparency with data regulations.

Method: Distinct pipelines for curated and web data, with minimal filtering for curated data and extensive filtering/deduplication for web data, alongside specialized algorithms.

Result: Comprehensive datasets prepared for model training, with insights into challenges and recommendations for future multilingual data projects.

Conclusion: The project successfully developed a scalable data pipeline, highlighting the importance of tailored processing and regulatory alignment for multilingual LLMs.

Abstract: This paper presents a comprehensive overview of the data preparation pipeline
developed for the OpenGPT-X project, a large-scale initiative aimed at creating
open and high-performance multilingual large language models (LLMs). The
project goal is to deliver models that cover all major European languages, with
a particular focus on real-world applications within the European Union. We
explain all data processing steps, starting with the data selection and
requirement definition to the preparation of the final datasets for model
training. We distinguish between curated data and web data, as each of these
categories is handled by distinct pipelines, with curated data undergoing
minimal filtering and web data requiring extensive filtering and deduplication.
This distinction guided the development of specialized algorithmic solutions
for both pipelines. In addition to describing the processing methodologies, we
provide an in-depth analysis of the datasets, increasing transparency and
alignment with European data regulations. Finally, we share key insights and
challenges faced during the project, offering recommendations for future
endeavors in large-scale multilingual data preparation for LLMs.

</details>


### [80] [Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling](https://arxiv.org/abs/2410.11325)
*Wenda Xu, Rujun Han, Zifeng Wang, Long T. Le, Dhruv Madeka, Lei Li, William Yang Wang, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister*

Main category: cs.CL

TL;DR: Speculative Knowledge Distillation (SKD) improves knowledge transfer between teacher and student models by dynamically generating high-quality training data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of supervised and on-policy KD, which suffer from distribution mismatch and low-quality training examples.

Method: SKD involves student proposing tokens and teacher replacing poorly ranked ones, ensuring adaptive knowledge transfer.

Result: SKD outperforms existing KD methods across text generation tasks like translation, summarization, and more.

Conclusion: SKD offers a robust solution for effective knowledge distillation by aligning training with student inference-time distribution.

Abstract: Recent advances in knowledge distillation (KD) have enabled smaller student
models to approach the performance of larger teacher models. However, popular
methods such as supervised KD and on-policy KD, are adversely impacted by the
knowledge gaps between teacher-student in practical scenarios. Supervised KD
suffers from a distribution mismatch between training with a static dataset and
inference over final student-generated outputs. Conversely, on-policy KD, which
uses student-generated samples for training, can suffer from low-quality
training examples with which teacher models are not familiar, resulting in
inaccurate teacher feedback. To address these limitations, we introduce
Speculative Knowledge Distillation (SKD), a novel approach that leverages
cooperation between student and teacher models to generate high-quality
training data on-the-fly while aligning with the student's inference-time
distribution. In SKD, the student proposes tokens, and the teacher replaces
poorly ranked ones based on its own distribution, transferring high-quality
knowledge adaptively. We evaluate SKD on various text generation tasks,
including translation, summarization, math, and instruction following, and show
that SKD consistently outperforms existing KD methods across different domains,
data sizes, and model initialization strategies.

</details>


### [81] [Open Domain Question Answering with Conflicting Contexts](https://arxiv.org/abs/2410.12311)
*Siyi Liu, Qiang Ning, Kishaloy Halder, Wei Xiao, Zheng Qi, Phu Mon Htut, Yi Zhang, Neha Anna John, Bonan Min, Yassine Benajiba, Dan Roth*

Main category: cs.CL

TL;DR: The paper addresses the issue of conflicting information in open-domain QA systems, introduces a dataset (QACC), benchmarks LLMs, and improves their performance by finetuning them to provide explanations.


<details>
  <summary>Details</summary>
Motivation: Open-domain QA systems often retrieve conflicting information, leading to untruthful answers. The study aims to quantify this problem and improve LLM performance.

Method: The authors collect a human-annotated dataset (QACC), evaluate three LLMs, and finetune them to generate explanations for answers.

Result: 25% of unambiguous questions lead to conflicting contexts. Finetuning LLMs with explanations improves their reasoning with conflicting information.

Conclusion: Explanations enhance LLMs' ability to handle conflicting contexts, suggesting a path for improving QA systems.

Abstract: Open domain question answering systems frequently rely on information
retrieved from large collections of text (such as the Web) to answer questions.
However, such collections of text often contain conflicting information, and
indiscriminately depending on this information may result in untruthful and
inaccurate answers. To understand the gravity of this problem, we collect a
human-annotated dataset, Question Answering with Conflicting Contexts (QACC),
and find that as much as 25% of unambiguous, open domain questions can lead to
conflicting contexts when retrieved using Google Search. We evaluate and
benchmark three powerful Large Language Models (LLMs) with our dataset QACC and
demonstrate their limitations in effectively addressing questions with
conflicting information. To explore how humans reason through conflicting
contexts, we request our annotators to provide explanations for their
selections of correct answers. We demonstrate that by finetuning LLMs to
explain their answers, we can introduce richer information into their training
that guide them through the process of reasoning with conflicting contexts.

</details>


### [82] [From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization](https://arxiv.org/abs/2410.13961)
*Catarina G. Belem, Pouya Pezeshkpour, Hayate Iso, Seiji Maekawa, Nikita Bhutani, Estevam Hruschka*

Main category: cs.CL

TL;DR: The paper investigates hallucinations in multi-document summarization (MDS) by LLMs, revealing high hallucination rates (up to 75%) and tendencies to fabricate content, especially with non-existent topics. It introduces new benchmarks and finds current mitigation methods only moderately effective.


<details>
  <summary>Details</summary>
Motivation: To explore the under-researched area of hallucinations in MDS tasks, particularly how handling multiple documents affects LLM outputs, and to create benchmarks for evaluation.

Method: The study uses annotated news and conversation datasets to create MDS benchmarks, evaluates 5 LLMs, and manually analyzes 700+ insights to understand hallucination characteristics.

Result: Up to 75% of LLM-generated summaries contain hallucinations, with higher rates for non-existent topics (79.35% for GPT-3.5-turbo, 44% for GPT-4). Post-hoc mitigation methods are only moderately effective.

Conclusion: The findings highlight the need for better approaches to reduce hallucinations in MDS, as current methods are insufficient. The dataset and code are released for further research.

Abstract: Although many studies have investigated and reduced hallucinations in large
language models (LLMs) for single-document tasks, research on hallucination in
multi-document summarization (MDS) tasks remains largely unexplored.
Specifically, it is unclear how the challenges arising from handling multiple
documents (e.g., repetition and diversity of information) affect models
outputs. In this work, we investigate how hallucinations manifest in LLMs when
summarizing topic-specific information from multiple documents. Since no
benchmarks exist for investigating hallucinations in MDS, we use existing news
and conversation datasets, annotated with topic-specific insights, to create
two novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks,
we observe that on average, up to 75% of the content in LLM-generated summary
is hallucinated, with hallucinations more likely to occur towards the end of
the summaries. Moreover, when summarizing non-existent topic-related
information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and
44% of the time, raising concerns about their tendency to fabricate content. To
understand the characteristics of these hallucinations, we manually evaluate
700+ insights and find that most errors stem from either failing to follow
instructions or producing overly generic insights. Motivated by these
observations, we investigate the efficacy of simple post-hoc baselines in
mitigating hallucinations but find them only moderately effective. Our results
underscore the need for more effective approaches to systematically mitigate
hallucinations in MDS. We release our dataset and code at
github.com/megagonlabs/Hallucination_MDS.

</details>


### [83] [WikiNER-fr-gold: A Gold-Standard NER Corpus](https://arxiv.org/abs/2411.00030)
*Danrun Cao, Nicolas Béchet, Pierre-François Marteau*

Main category: cs.CL

TL;DR: The paper introduces WikiNER-fr-gold, a revised version of the French portion of the WikiNER corpus, addressing its quality issues and providing a gold-standard annotation.


<details>
  <summary>Details</summary>
Motivation: To improve the quality of the WikiNER corpus, which was originally created semi-supervised (silver-standard) without manual verification, by producing a manually revised version for French.

Method: Randomly sampled 20% of the original French sub-corpus (26,818 sentences), defined annotation guidelines, and revised the corpus manually.

Result: A gold-standard version (WikiNER-fr-gold) was created, and errors/inconsistencies in the original corpus were analyzed.

Conclusion: The revised corpus provides higher quality annotations, and future work directions are discussed to further improve such resources.

Abstract: We address in this article the the quality of the WikiNER corpus, a
multilingual Named Entity Recognition corpus, and provide a consolidated
version of it. The annotation of WikiNER was produced in a semi-supervised
manner i.e. no manual verification has been carried out a posteriori. Such
corpus is called silver-standard. In this paper we propose WikiNER-fr-gold
which is a revised version of the French proportion of WikiNER. Our corpus
consists of randomly sampled 20% of the original French sub-corpus (26,818
sentences with 700k tokens). We start by summarizing the entity types included
in each category in order to define an annotation guideline, and then we
proceed to revise the corpus. Finally we present an analysis of errors and
inconsistency observed in the WikiNER-fr corpus, and we discuss potential
future work directions.

</details>


### [84] [Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/abs/2411.07611)
*Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Yida Xu, Yunya Song, Xian Yang*

Main category: cs.CL

TL;DR: ClinRaGen enhances small language models (SLMs) with LLM-derived reasoning and domain knowledge for trustworthy multimodal rationale generation in disease diagnosis.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle to balance accuracy and interpretability, with LLMs being computationally expensive and SLMs lacking advanced reasoning.

Method: Sequential rationale distillation and knowledge-augmented attention mechanism to unify multimodal data.

Result: State-of-the-art performance in disease diagnosis and rationale generation on real-world datasets.

Conclusion: Combining LLM reasoning with knowledge augmentation improves interpretability and performance.

Abstract: Interpretation is critical for disease diagnosis, but existing models
struggle to balance predictive accuracy with human-understandable rationales.
While large language models (LLMs) offer strong reasoning abilities, their
clinical use is limited by high computational costs and restricted multimodal
reasoning ability. Small language models (SLMs) are efficient but lack advanced
reasoning for integrating multimodal medical data. In addition, both LLMs and
SLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose
ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via
rationale distillation and domain knowledge injection for trustworthy
multimodal rationale generation. Key innovations include a sequential rationale
distillation framework that equips SLMs with LLM-comparable mutlimodal
reasoning abilities, and a knowledge-augmented attention mechanism that jointly
unifies multimodal representation from time series and textual data in a same
encoding space, enabling it naturally interpreted by SLMs while incorporating
domain knowledge for reliable rationale generation. Experiments on real-world
medical datasets show that ClinRaGen achieves state-of-the-art performance in
disease diagnosis and rationale generation, demonstrating the effectiveness of
combining LLM-driven reasoning with knowledge augmentation for improved
interpretability.

</details>


### [85] [An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese](https://arxiv.org/abs/2411.17270)
*Duc-Vu Nguyen, Thang Chau Phan, Quoc-Nam Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: A neural parser for Vietnamese based on simplified HPSG was developed, achieving state-of-the-art F-scores in constituency parsing but lower LAS in dependency parsing due to label constraints.


<details>
  <summary>Details</summary>
Motivation: To address the issue of existing Vietnamese corpora not fully adhering to simplified HPSG rules and improve parsing performance.

Method: Modified a simplified HPSG Neural Parser by replacing models with PhoBERT/XLM-RoBERTa for Vietnamese text encoding and permuted non-compliant corpus samples.

Result: Achieved 82% F-score in constituency parsing and higher UAS in dependency parsing, but lower LAS due to unchanged labels.

Conclusion: Simplified HPSG requires linguistic expert input for better label handling in Vietnamese treebank development.

Abstract: In this paper, we aimed to develop a neural parser for Vietnamese based on
simplified Head-Driven Phrase Structure Grammar (HPSG). The existing corpora,
VietTreebank and VnDT, had around 15% of constituency and dependency tree pairs
that did not adhere to simplified HPSG rules. To attempt to address the issue
of the corpora not adhering to simplified HPSG rules, we randomly permuted
samples from the training and development sets to make them compliant with
simplified HPSG. We then modified the first simplified HPSG Neural Parser for
the Penn Treebank by replacing it with the PhoBERT or XLM-RoBERTa models, which
can encode Vietnamese texts. We conducted experiments on our modified
VietTreebank and VnDT corpora. Our extensive experiments showed that the
simplified HPSG Neural Parser achieved a new state-of-the-art F-score of 82%
for constituency parsing when using the same predicted part-of-speech (POS)
tags as the self-attentive constituency parser. Additionally, it outperformed
previous studies in dependency parsing with a higher Unlabeled Attachment Score
(UAS). However, our parser obtained lower Labeled Attachment Score (LAS) scores
likely due to our focus on arc permutation without changing the original
labels, as we did not consult with a linguistic expert. Lastly, the research
findings of this paper suggest that simplified HPSG should be given more
attention to linguistic expert when developing treebanks for Vietnamese natural
language processing.

</details>


### [86] [Investigating Length Issues in Document-level Machine Translation](https://arxiv.org/abs/2412.17592)
*Ziqian Peng, Rachel Bawden, François Yvon*

Main category: cs.CL

TL;DR: The paper explores the challenges of document-level machine translation (MT) with long texts, showing performance declines with length and positional effects.


<details>
  <summary>Details</summary>
Motivation: To assess the ability of MT systems to handle very long texts and understand the impact of length and sentence position on translation quality.

Method: Designed a new approach to measure length increments' effects on MT outputs, tested with two representative architectures.

Result: Translation performance decreases with input length, and earlier sentences in documents are translated better. Adjusting document length distribution and positional embeddings had minimal impact.

Conclusion: Document-level MT is feasible but underperforms compared to sentence-based MT.

Abstract: Transformer architectures are increasingly effective at processing and
generating very long chunks of texts, opening new perspectives for
document-level machine translation (MT). In this work, we challenge the ability
of MT systems to handle texts comprising up to several thousands of tokens. We
design and implement a new approach designed to precisely measure the effect of
length increments on MT outputs. Our experiments with two representative
architectures unambiguously show that (a)~translation performance decreases
with the length of the input text; (b)~the position of sentences within the
document matters, and translation quality is higher for sentences occurring
earlier in a document. We further show that manipulating the distribution of
document lengths and of positional embeddings only marginally mitigates such
problems. Our results suggest that even though document-level MT is
computationally feasible, it does not yet match the performance of
sentence-based MT.

</details>


### [87] [LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context](https://arxiv.org/abs/2412.17596)
*Kai Ruan, Xuan Wang, Jixiang Hong, Peng Wang, Yang Liu, Hao Sun*

Main category: cs.CL

TL;DR: LiveIdeaBench evaluates LLMs' scientific idea generation using single-keyword prompts, revealing that creative performance doesn't align with general intelligence metrics.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on rich contextual inputs, but LiveIdeaBench addresses the gap in assessing divergent thinking for scientific idea generation.

Method: The benchmark uses single-keyword prompts and evaluates ideas across five dimensions (originality, feasibility, fluency, flexibility, clarity) with 40+ models across 22 domains.

Result: Creative performance (e.g., QwQ-32B-preview) doesn't correlate with general intelligence scores, highlighting the need for specialized benchmarks.

Conclusion: Specialized training strategies may be required to enhance LLMs' scientific idea generation, enabling tailored AI tools for scientific processes.

Abstract: While Large Language Models (LLMs) demonstrate remarkable capabilities in
scientific tasks such as literature analysis and experimental design (e.g.,
accurately extracting key findings from papers or generating coherent
experimental procedures), existing evaluation benchmarks primarily assess
performance using rich contextual inputs. We introduce LiveIdeaBench, a
comprehensive benchmark evaluating LLMs' scientific idea generation by
assessing divergent thinking capabilities using single-keyword prompts. Drawing
from Guilford's creativity theory, our benchmark employs a dynamic panel of
state-of-the-art LLMs to assess generated ideas across five key dimensions:
originality, feasibility, fluency, flexibility, and clarity. Through extensive
experimentation with over 40 leading models across 1,180 keywords spanning 22
scientific domains, we reveal that the scientific idea generation capabilities
measured by our benchmark, are poorly predicted by standard metrics of general
intelligence. Our results demonstrate that models like QwQ-32B-preview achieve
creative performance comparable to top-tier models such as
claude-3.7-sonnet:thinking, despite significant gaps in their general
intelligence scores. These findings highlight the need for specialized
evaluation benchmarks for scientific idea generation and suggest that enhancing
these idea generation capabilities in LLMs may require different training
strategies than those used for improving general problem-solving abilities,
potentially enabling a wider range of AI tools tailored for different stages of
the scientific process.

</details>


### [88] [Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora](https://arxiv.org/abs/2502.00090)
*Logan Born, M. Willis Monroe, Kathryn Kelley, Anoop Sarkar*

Main category: cs.CL

TL;DR: The paper addresses the ambiguity in numeral readings of the proto-Elamite script by proposing algorithmic disambiguation techniques and a test set for evaluation.


<details>
  <summary>Details</summary>
Motivation: The proto-Elamite script's numerals are ambiguous, hindering understanding of the corpus, which is accounting-heavy.

Method: Algorithmic extraction of possible numeral readings, structural document analysis, and bootstrapped classifiers for disambiguation.

Result: Confirmed existing intuitions and revealed new correlations between tablet content and numeral magnitude.

Conclusion: The work aids in deciphering proto-Elamite by resolving numeral ambiguity, crucial for understanding its accounting-focused corpus.

Abstract: A numeration system encodes abstract numeric quantities as concrete strings
of written characters. The numeration systems used by modern scripts tend to be
precise and unambiguous, but this was not so for the ancient and
partially-deciphered proto-Elamite (PE) script, where written numerals can have
up to four distinct readings depending on the system that is used to read them.
We consider the task of disambiguating between these readings in order to
determine the values of the numeric quantities recorded in this corpus. We
algorithmically extract a list of possible readings for each PE numeral
notation, and contribute two disambiguation techniques based on structural
properties of the original documents and classifiers learned with the
bootstrapping algorithm. We also contribute a test set for evaluating
disambiguation techniques, as well as a novel approach to cautious rule
selection for bootstrapped classifiers. Our analysis confirms existing
intuitions about this script and reveals previously-unknown correlations
between tablet content and numeral magnitude. This work is crucial to
understanding and deciphering PE, as the corpus is heavily accounting-focused
and contains many more numeric tokens than tokens of text.

</details>


### [89] [Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations](https://arxiv.org/abs/2502.01220)
*Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé*

Main category: cs.CL

TL;DR: The paper investigates how well language models handle temporal context in factual knowledge, revealing significant limitations.


<details>
  <summary>Details</summary>
Motivation: To assess the robustness of LMs in associating temporal contexts with facts and identify their shortcomings.

Method: Evaluated 18 LMs using the TimeStress dataset, measuring accuracy based on context distance and granularity.

Result: Best LM achieved only 6% perfect accuracy, with errors humans wouldn't make.

Conclusion: Current LMs struggle with temporal representation; dataset and code are shared for future research.

Abstract: This paper explores the robustness of language models (LMs) to variations in
the temporal context within factual knowledge. It examines whether LMs can
correctly associate a temporal context with a past fact valid over a defined
period, by asking them to differentiate correct from incorrect contexts. The
accuracy of LMs is analyzed along two dimensions: the distance of the incorrect
context from the validity period and the granularity of the context. To this
end, a dataset called TimeStress is introduced, enabling the evaluation of 18
diverse LMs. Results reveal that the best LM achieves perfect accuracy for only
6% of the studied facts, with critical errors that humans would not make. This
work highlights the limitations of current LMs in temporal representation. We
provide all data and code for further research.

</details>


### [90] [InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context](https://arxiv.org/abs/2502.12257)
*Bryan L. M. de Oliveira, Luana G. B. Martins, Bruno Brandão, Luckeciano C. Melo*

Main category: cs.CL

TL;DR: InfoQuest is a benchmark for evaluating dialogue agents' ability to handle ambiguous requests by seeking clarification, revealing current models' struggles with hidden context.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of language models defaulting to generic responses for ambiguous or incomplete requests instead of seeking clarification.

Method: Introduce InfoQuest, a multi-turn chat benchmark with ambiguous scenarios, and evaluate open and closed models on their information-seeking dialogue capabilities.

Result: Proprietary models perform better but all struggle with effective information gathering, often requiring multiple turns or defaulting to generic responses.

Conclusion: The benchmark provides a methodology for evaluating and improving models' handling of ambiguous requests, highlighting current limitations in multi-turn interactions.

Abstract: Large language models excel at following explicit instructions, but they
often struggle with ambiguous or incomplete user requests, defaulting to
verbose, generic responses instead of seeking clarification. We introduce
InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents
handle hidden context in open-ended user requests. This benchmark presents
intentionally ambiguous scenarios that require models to engage in
information-seeking dialogue by asking clarifying questions before providing
appropriate responses. Our evaluation of both open and closed models reveals
that, while proprietary models generally perform better, all current assistants
struggle to gather critical information effectively. They often require
multiple turns to infer user intent and frequently default to generic responses
without proper clarification. We provide a systematic methodology for
generating diverse scenarios and evaluating models' information-seeking
capabilities, which can be leveraged to automatically generate data for
self-improvement. We also offer insights into the current limitations of
language models in handling ambiguous requests through multi-turn interactions.

</details>


### [91] [Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation](https://arxiv.org/abs/2502.13019)
*Sha Li, Naren Ramakrishnan*

Main category: cs.CL

TL;DR: A compact module refines retrieved chunks in RAG to improve LLM outputs by prioritizing relevant information through a three-stage training paradigm.


<details>
  <summary>Details</summary>
Motivation: Retrieval-Augmented Generation (RAG) can suffer from erroneous or distracting retrieved information, reducing its effectiveness.

Method: A pluggable module refines retrieved chunks via supervised fine-tuning, contrastive multi-task learning, and reinforcement learning-based alignment.

Result: The module enhances LLM outputs by aligning retrieved knowledge with the generator's preferences, improving accuracy and contextual appropriateness.

Conclusion: The proposed module effectively refines retrieved information, making RAG more reliable and effective for downstream tasks.

Abstract: Retrieval-Augmented Generation (RAG) aims to augment the capabilities of
Large Language Models (LLMs) by retrieving and incorporate external documents
or chunks prior to generation. However, even improved retriever relevance can
brings erroneous or contextually distracting information, undermining the
effectiveness of RAG in downstream tasks. We introduce a compact, efficient,
and pluggable module designed to refine retrieved chunks before using them for
generation. The module aims to extract and reorganize the most relevant and
supportive information into a concise, query-specific format. Through a
three-stage training paradigm - comprising supervised fine - tuning,
contrastive multi-task learning, and reinforcement learning-based alignment -
it prioritizes critical knowledge and aligns it with the generator's
preferences. This approach enables LLMs to produce outputs that are more
accurate, reliable, and contextually appropriate.

</details>


### [92] [PSCon: Product Search Through Conversations](https://arxiv.org/abs/2502.13881)
*Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen*

Main category: cs.CL

TL;DR: The paper introduces PSCon, a new dataset for Conversational Product Search (CPS) using human-like language, addressing limitations of existing datasets by supporting cross-market and multi-lingual usage.


<details>
  <summary>Details</summary>
Motivation: Existing CPS research relies on simulated conversations and lacks real datasets with human-like language, limiting cross-market and multi-lingual applicability.

Method: A coached human-human data collection protocol is used to create the PSCon dataset, enabling research on six CPS subtasks.

Result: PSCon supports dual markets and two languages, and a benchmark model is proposed for the dataset.

Conclusion: PSCon and the benchmark model advance CPS research by providing a realistic, versatile dataset.

Abstract: Conversational Product Search ( CPS ) systems interact with users via natural
language to offer personalized and context-aware product lists. However, most
existing research on CPS is limited to simulated conversations, due to the lack
of a real CPS dataset driven by human-like language. Moreover, existing
conversational datasets for e-commerce are constructed for a particular market
or a particular language and thus can not support cross-market and
multi-lingual usage. In this paper, we propose a CPS data collection protocol
and create a new CPS dataset, called PSCon, which assists product search
through conversations with human-like language. The dataset is collected by a
coached human-human data collection protocol and is available for dual markets
and two languages. By formulating the task of CPS, the dataset allows for
comprehensive and in-depth research on six subtasks: user intent detection,
keyword extraction, system action prediction, question selection, item ranking,
and response generation. Moreover, we present a concise analysis of the dataset
and propose a benchmark model on the proposed CPS dataset. Our proposed dataset
and model will be helpful for facilitating future research on CPS.

</details>


### [93] [LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning](https://arxiv.org/abs/2502.14644)
*Yansheng Mao, Yufei Xu, Jiaqi Li, Fanxu Meng, Haotong Yang, Zilong Zheng, Xiyuan Wang, Muhan Zhang*

Main category: cs.CL

TL;DR: LIFT is a framework for improving long-context performance in short-context LLMs by fine-tuning long inputs into model parameters, avoiding endless context window extensions.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of long-context understanding in LLMs with limited context windows.

Method: Introduces Long Input Fine-Tuning (LIFT) and Gated Memory, an attention adapter to balance memorization and in-context learning.

Result: Enables short-context LLMs to answer questions without requiring all information in the context during inference.

Conclusion: LIFT offers a promising approach for long-context modeling, with potential for future research.

Abstract: Long context understanding remains challenging for large language models due
to their limited context windows. This paper presents Long Input Fine-Tuning
(LIFT), a novel framework for long-context modeling that can improve the
long-context performance of arbitrary (short-context) LLMs by dynamically
adapting model parameters based on the long input. Importantly, LIFT, rather
than endlessly extending the context window size to accommodate increasingly
longer inputs in context, chooses to store and absorb the long input in
parameter. By fine-tuning the long input into model parameters, LIFT allows
short-context LLMs to answer questions even when the required information is
not provided in the context during inference. Furthermore, to enhance LIFT
performance while maintaining the original in-context learning (ICL)
capabilities, we introduce Gated Memory, a specialized attention adapter that
automatically balances long input memorization and ICL. We provide a
comprehensive analysis of the strengths and limitations of LIFT on long context
understanding, offering valuable directions for future research.

</details>


### [94] [Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision](https://arxiv.org/abs/2502.15147)
*Zhouhang Xie, Tushar Khot, Bhavana Dalvi Mishra, Harshit Surana, Julian McAuley, Peter Clark, Bodhisattwa Prasad Majumder*

Main category: cs.CL

TL;DR: Instruct-LF combines LLMs with statistical models to improve latent factor discovery in noisy datasets, outperforming baselines by 5-52%.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs in discovering high-quality hidden concepts from noisy or complex data.

Method: Uses LLMs to propose goal-related properties, estimates their presence, and applies gradient-based optimization to uncover latent factors.

Result: Improves downstream task performance by 5-52% and is preferred 1.8 times more in human evaluations.

Conclusion: Instruct-LF effectively integrates LLMs with statistical methods for robust latent factor discovery.

Abstract: Instruction-following LLMs have recently allowed systems to discover hidden
concepts from a collection of unstructured documents based on a natural
language description of the purpose of the discovery (i.e., goal). Still, the
quality of the discovered concepts remains mixed, as it depends heavily on
LLM's reasoning ability and drops when the data is noisy or beyond LLM's
knowledge. We present Instruct-LF, a goal-oriented latent factor discovery
system that integrates LLM's instruction-following ability with statistical
models to handle large, noisy datasets where LLM reasoning alone falls short.
  Instruct-LF uses LLMs to propose fine-grained, goal-related properties from
documents, estimates their presence across the dataset, and applies
gradient-based optimization to uncover hidden factors, where each factor is
represented by a cluster of co-occurring properties. We evaluate latent factors
produced by Instruct-LF on movie recommendation, text-world navigation, and
legal document categorization tasks. These interpretable representations
improve downstream task performance by 5-52% than the best baselines and were
preferred 1.8 times as often as the best alternative, on average, in human
evaluation.

</details>


### [95] [Protecting multimodal large language models against misleading visualizations](https://arxiv.org/abs/2502.20503)
*Jonathan Tonglet, Tinne Tuytelaars, Marie-Francine Moens, Iryna Gurevych*

Main category: cs.CL

TL;DR: MLLMs struggle with misleading visualizations, performing at random baseline accuracy. A new method using data table extraction and text-only LLMs improves performance without affecting accuracy on non-misleading charts.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of MLLMs in handling misleading visualizations, which can lead to inaccurate conclusions and disinformation.

Method: Extract underlying data tables from charts and use text-only LLMs for question-answering, ensuring robustness without compromising accuracy on non-misleading charts.

Result: MLLM accuracy on misleading visualizations drops to random baseline levels. The proposed method improves performance without affecting non-misleading chart accuracy.

Conclusion: The study highlights a critical blind spot in MLLM research and sets benchmarks for future work on reliable MLLMs.

Abstract: Visualizations play a pivotal role in daily communication in an increasingly
data-driven world. Research on multimodal large language models (MLLMs) for
automated chart understanding has accelerated massively, with steady
improvements on standard benchmarks. However, for MLLMs to be reliable, they
must be robust to misleading visualizations, charts that distort the underlying
data, leading readers to draw inaccurate conclusions that may support
disinformation. Here, we uncover an important vulnerability: MLLM
question-answering accuracy on misleading visualizations drops on average to
the level of a random baseline. To address this, we introduce the first
inference-time methods to improve performance on misleading visualizations,
without compromising accuracy on non-misleading ones. The most effective method
extracts the underlying data table and uses a text-only LLM to answer the
question based on the table. Our findings expose a critical blind spot in
current research and establish benchmark results to guide future efforts in
reliable MLLMs.

</details>


### [96] [Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models](https://arxiv.org/abs/2503.10617)
*Andy Zhou*

Main category: cs.CL

TL;DR: CS-ReFT is a method to prevent cross-skill interference in large language models by learning orthonormal subspace transformations for distinct skills, achieving high performance with minimal parameter overhead.


<details>
  <summary>Details</summary>
Motivation: Addressing cross-skill interference in multi-task adaptation of large language models, where traditional methods like LoRA fail to fully mitigate hidden-state representation conflicts.

Method: Proposes Compositional Subspace Representation Fine-tuning (CS-ReFT), which learns orthonormal subspace transformations for each skill and composes them via a lightweight router, isolating edits in hidden states.

Result: Achieves a 93.94% win rate on AlpacaEval with Llama-2-7B, outperforming GPT-3.5 Turbo (86.30%) using only 0.0098% of model parameters.

Conclusion: CS-ReFT demonstrates that specialized representation edits with a simple router significantly improve multi-task performance efficiently.

Abstract: Adapting large language models to multiple tasks can cause cross-skill
interference, where improvements for one skill degrade another. While methods
such as LoRA impose orthogonality constraints at the weight level, they do not
fully address interference in hidden-state representations. We propose
Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel
representation-based approach that learns multiple orthonormal subspace
transformations, each specializing in a distinct skill, and composes them via a
lightweight router. By isolating these subspace edits in the hidden state,
rather than weight matrices, CS-ReFT prevents cross-task conflicts more
effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B
achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring
only 0.0098% of model parameters. These findings show that specialized
representation edits, composed via a simple router, significantly enhance
multi-task instruction following with minimal overhead.

</details>


### [97] [Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games](https://arxiv.org/abs/2504.06868)
*Seungwon Lim, Seungbeen Lee, Dongjun Min, Youngjae Yu*

Main category: cs.CL

TL;DR: PANDA introduces a method to align AI agent behavior with human personality traits, showing performance benefits for certain traits like Openness.


<details>
  <summary>Details</summary>
Motivation: Aligning AI behaviors with human values is challenging; this work explores how human-like personality traits can guide agent behavior.

Method: PANDA trains a personality classifier and integrates personality profiles into policy-learning for text-based game agents.

Result: Agents with specific personality traits (e.g., Openness) showed improved performance in text-based games.

Conclusion: Personality-adapted agents enhance alignment and effectiveness in interactive environments.

Abstract: Artificial agents are increasingly central to complex interactions and
decision-making tasks, yet aligning their behaviors with desired human values
remains an open challenge. In this work, we investigate how human-like
personality traits influence agent behavior and performance within text-based
interactive environments. We introduce PANDA: Personality Adapted Neural
Decision Agents, a novel method for projecting human personality traits onto
agents to guide their behavior. To induce personality in a text-based game
agent, (i) we train a personality classifier to identify what personality type
the agent's actions exhibit, and (ii) we integrate the personality profiles
directly into the agent's policy-learning pipeline. By deploying agents
embodying 16 distinct personality types across 25 text-based games and
analyzing their trajectories, we demonstrate that an agent's action decisions
can be guided toward specific personality profiles. Moreover, certain
personality types, such as those characterized by higher levels of Openness,
display marked advantages in performance. These findings underscore the promise
of personality-adapted agents for fostering more aligned, effective, and
human-centric decision-making in interactive environments.

</details>


### [98] [Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish](https://arxiv.org/abs/2504.09714)
*Ayşe Aysu Cengiz, Ahmet Kaan Sever, Elif Ecem Ümütlü, Naime Şeyma Erdem, Burak Aytan, Büşra Tufan, Abdullah Topraksoy, Esra Darıcı, Cagri Toraman*

Main category: cs.CL

TL;DR: 70% of Turkish benchmark datasets fail quality standards, with LLMs underperforming humans in cultural and linguistic tasks. GPT-4o and Llama3.3-70B show strengths in specific areas, highlighting the need for better dataset quality control.


<details>
  <summary>Details</summary>
Motivation: Address the lack of culturally and linguistically suitable benchmarks for Turkish by evaluating existing datasets.

Method: Evaluate 17 Turkish benchmark datasets using a framework with six criteria, assessed by human and LLM annotators.

Result: 70% of datasets fail quality standards; LLMs lag behind humans in cultural and linguistic understanding, though GPT-4o and Llama3.3-70B excel in specific tasks.

Conclusion: Urgent need for stricter quality control in dataset creation for low-resource languages like Turkish.

Abstract: The reliance on translated or adapted datasets from English or multilingual
resources introduces challenges regarding linguistic and cultural suitability.
This study addresses the need for robust and culturally appropriate benchmarks
by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using
a comprehensive framework that assesses six criteria, both human and LLM-judge
annotators provide detailed evaluations to identify dataset strengths and
shortcomings.
  Our results reveal that 70% of the benchmark datasets fail to meet our
heuristic quality standards. The correctness of the usage of technical terms is
the strongest criterion, but 85% of the criteria are not satisfied in the
examined datasets. Although LLM judges demonstrate potential, they are less
effective than human annotators, particularly in understanding cultural common
sense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger
labeling capabilities for grammatical and technical tasks, while Llama3.3-70B
excels at correctness and cultural knowledge evaluation. Our findings emphasize
the urgent need for more rigorous quality control in creating and adapting
datasets for low-resource languages.

</details>


### [99] [Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs](https://arxiv.org/abs/2504.10982)
*Yingjian Chen, Feiyang Li, Xingyu Song, Tianxiao Li, Zixin Xu, Xiujie Chen, Issey Sukeda, Irene Li*

Main category: cs.CL

TL;DR: The paper explores a knowledge graph-based RAG framework for Japanese medical QA using small-scale open-source LLMs, finding limited impact due to sensitivity to retrieved content quality.


<details>
  <summary>Details</summary>
Motivation: Commercial LLMs like GPT-4 can't be used in Japanese clinical settings due to privacy constraints, prompting research into open-source alternatives combined with RAG.

Method: The study employs a knowledge graph-based RAG framework for Japanese medical QA, testing small-scale open-source LLMs.

Result: KG-based RAG shows limited impact, with effectiveness highly dependent on the quality and relevance of retrieved content.

Conclusion: The findings highlight challenges and potential for RAG in Japanese medical QA, offering insights for low-resource languages.

Abstract: Large language models (LLMs) perform well in medical QA, but their
effectiveness in Japanese contexts is limited due to privacy constraints that
prevent the use of commercial models like GPT-4 in clinical settings. As a
result, recent efforts focus on instruction-tuning open-source LLMs, though the
potential of combining them with retrieval-augmented generation (RAG) remains
underexplored. To bridge this gap, we are the first to explore a knowledge
graph-based (KG) RAG framework for Japanese medical QA small-scale open-source
LLMs. Experimental results show that KG-based RAG has only a limited impact on
Japanese medical QA using small-scale open-source LLMs. Further case studies
reveal that the effectiveness of the RAG is sensitive to the quality and
relevance of the external retrieved content. These findings offer valuable
insights into the challenges and potential of applying RAG in Japanese medical
QA, while also serving as a reference for other low-resource languages.

</details>


### [100] [SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes](https://arxiv.org/abs/2504.11975)
*Raúl Vázquez, Timothee Mickus, Elaine Zosa, Teemu Vahtola, Jörg Tiedemann, Aman Sinha, Vincent Segonne, Fernando Sánchez-Vega, Alessandro Raganato, Jindřich Libovický, Jussi Karlgren, Shaoxiong Ji, Jindřich Helcl, Liane Guillou, Ona de Gibert, Jaione Bengoetxea, Joseph Attieh, Marianna Apidianaki*

Main category: cs.CL

TL;DR: Mu-SHROOM is a shared task for detecting hallucinations in LLM outputs across 14 languages, framed as a span-labeling task with 2,618 submissions from 43 teams.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting hallucinations and overgeneration mistakes in instruction-tuned LLMs, highlighting community interest and language-specific variations.

Method: The task is framed as a span-labeling problem, with diverse methodologies employed by participating teams.

Result: High participation (2,618 submissions) shows strong community interest. Empirical analysis identifies key performance factors and challenges like language variability and annotator disagreement.

Conclusion: Mu-SHROOM highlights the importance of hallucination detection in LLMs, revealing challenges such as language differences and labeling consistency.

Abstract: We present the Mu-SHROOM shared task which is focused on detecting
hallucinations and other overgeneration mistakes in the output of
instruction-tuned large language models (LLMs). Mu-SHROOM addresses
general-purpose LLMs in 14 languages, and frames the hallucination detection
problem as a span-labeling task. We received 2,618 submissions from 43
participating teams employing diverse methodologies. The large number of
submissions underscores the interest of the community in hallucination
detection. We present the results of the participating systems and conduct an
empirical analysis to identify key factors contributing to strong performance
in this task. We also emphasize relevant current challenges, notably the
varying degree of hallucinations across languages and the high annotator
disagreement when labeling hallucination spans.

</details>


### [101] [Gauging Overprecision in LLMs: An Empirical Study](https://arxiv.org/abs/2504.12098)
*Adil Bahaj, Hamed Rahimi, Mohamed Chetouani, Mounir Ghogho*

Main category: cs.CL

TL;DR: The paper studies overconfidence in LLMs, focusing on 'overprecision' in numerical tasks. It introduces a framework with generation, refinement, and evaluation phases, revealing LLMs' poor calibration and lack of correlation between interval length and confidence.


<details>
  <summary>Details</summary>
Motivation: To address biases in existing methods for measuring LLM confidence by exploring overprecision, inspired by cognitive science, and to provide insights into LLM trustworthiness.

Method: A three-phase framework: 1) Generate numerical answers with imposed confidence levels, 2) Refine answers, and 3) Evaluate LLM behavior and precision.

Result: LLMs are uncalibrated for numerical tasks, show no correlation between interval length and confidence, and refinement rarely improves precision.

Conclusion: The study offers new insights into LLM overconfidence and establishes a baseline for future research on overprecision.

Abstract: Recently, overconfidence in large language models (LLMs) has garnered
considerable attention due to its fundamental importance in quantifying the
trustworthiness of LLM generation. However, existing approaches prompt the
\textit{black box LLMs} to produce their confidence (\textit{verbalized
confidence}), which can be subject to many biases and hallucinations. Inspired
by a different aspect of overconfidence in cognitive science called
\textit{overprecision}, we designed a framework for its study in black box
LLMs. This framework contains three main phases: 1) generation, 2) refinement
and 3) evaluation. In the generation phase we prompt the LLM to generate
answers to numerical questions in the form of intervals with a certain level of
confidence. This confidence level is imposed in the prompt and not required for
the LLM to generate as in previous approaches. We use various prompting
techniques and use the same prompt multiple times to gauge the effects of
randomness in the generation process. In the refinement phase, answers from the
previous phase are refined to generate better answers. The LLM answers are
evaluated and studied in the evaluation phase to understand its internal
workings. This study allowed us to gain various insights into LLM
overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) there is
no correlation between the length of the interval and the imposed confidence
level, which can be symptomatic of a a) lack of understanding of the concept of
confidence or b) inability to adjust self-confidence by following instructions,
{3) LLM numerical precision differs depending on the task, scale of answer and
prompting technique 4) Refinement of answers doesn't improve precision in most
cases. We believe this study offers new perspectives on LLM overconfidence and
serves as a strong baseline for overprecision in LLMs.

</details>


### [102] [Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://arxiv.org/abs/2504.13828)
*Shijie Xia, Yiwei Qin, Xuefeng Li, Yan Ma, Run-Ze Fan, Steffi Chern, Haoyang Zou, Fan Zhou, Xiangkun Hu, Jiahe Jin, Yanheng He, Yixin Ye, Yixiu Liu, Pengfei Liu*

Main category: cs.CL

TL;DR: The paper discusses the transition from early generative AI (Act I) to advanced cognition engineering (Act II), highlighting limitations of Act I and the potential of Act II.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of early generative AI (knowledge latency, shallow reasoning) and explore the potential of cognition engineering in Act II.

Method: Systematic breakdown of advanced approaches through tutorials and optimized implementations, with a focus on test-time scaling techniques.

Result: Democratization of cognition engineering, enabling broader participation in AI's second act.

Conclusion: The paper emphasizes the critical moment for cognition engineering's development and provides resources for practitioners to engage with it.

Abstract: The first generation of Large Language Models - what might be called "Act I"
of generative AI (2020-2023) - achieved remarkable success through massive
parameter and data scaling, yet exhibited fundamental limitations such as
knowledge latency, shallow reasoning, and constrained cognitive processes.
During this era, prompt engineering emerged as our primary interface with AI,
enabling dialogue-level communication through natural language. We now witness
the emergence of "Act II" (2024-present), where models are transitioning from
knowledge-retrieval systems (in latent space) to thought-construction engines
through test-time scaling techniques. This new paradigm establishes a
mind-level connection with AI through language-based thoughts. In this paper,
we clarify the conceptual foundations of cognition engineering and explain why
this moment is critical for its development. We systematically break down these
advanced approaches through comprehensive tutorials and optimized
implementations, democratizing access to cognition engineering and enabling
every practitioner to participate in AI's second act. We provide a regularly
updated collection of papers on test-time scaling in the GitHub Repository:
https://github.com/GAIR-NLP/cognition-engineering

</details>


### [103] [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)
*Tyler A. Chang, Benjamin K. Bergen*

Main category: cs.CL

TL;DR: Bigram subnetworks in Transformer models are minimal, critical for performance, and concentrated in the first MLP layer, aiding next token predictions.


<details>
  <summary>Details</summary>
Motivation: To isolate and understand the minimal transformation from current token embeddings to next token predictions in language models.

Method: Identify and analyze bigram subnetworks in fully trained Transformer models up to 1B parameters.

Result: Bigram subnetworks are essential (even with <0.2% of parameters), concentrated in the first MLP layer, and overlap with optimal pruning subnetworks.

Conclusion: Bigram subnetworks are necessary and sufficient for basic next token predictions, providing a foundation for studying more complex circuits.

Abstract: In Transformer language models, activation vectors transform from current
token embeddings to next token predictions as they pass through the model. To
isolate a minimal form of this transformation, we identify language model
subnetworks that make bigram predictions, naive next token predictions based
only on the current token. We find that bigram subnetworks can be found in
fully trained language models up to 1B parameters, and these subnetworks are
critical for model performance even when they consist of less than 0.2% of
model parameters. Bigram subnetworks are concentrated in the first Transformer
MLP layer, and they overlap significantly with subnetworks trained to optimally
prune a given model. Mechanistically, the bigram subnetworks often recreate a
pattern from the full models where the first layer induces a sharp change that
aligns activations with next token predictions rather than current token
representations. Our results demonstrate that bigram subnetworks comprise a
minimal subset of parameters that are both necessary and sufficient for basic
next token predictions in language models, and they help drive the
transformation from current to next token activations in the residual stream.
These subnetworks can lay a foundation for studying more complex language model
circuits by building up from a minimal circuit.

</details>


### [104] [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
*Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li*

Main category: cs.CL

TL;DR: RL enhances audio-language reasoning via structured, curriculum-guided methods, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Explore the transfer of RL gains from text to audio-language reasoning, addressing a gap in current research.

Method: Extend GRPO to LALM, use a two-stage regimen (SFT and curriculum-guided GRPO), and compare reasoning types.

Result: 16.35% accuracy improvement over base model; SARI variant achieves 67.08% on MMAU benchmark.

Conclusion: Structured reasoning and curriculum learning significantly boost audio-language understanding.

Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the
reasoning ability of large language models (LLMs) by prompting them to "think
before answering." Yet whether and how these gains transfer to audio-language
reasoning remains largely unexplored. We extend the Group-Relative Policy
Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model
(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage
regimen supervised fine-tuning on structured and unstructured
chains-of-thought, followed by curriculum-guided GRPO, we systematically
compare implicit vs. explicit, and structured vs. free form reasoning under
identical architectures. Our structured audio reasoning model, SARI (Structured
Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a
16.35% improvement in average accuracy over the base model
Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni
reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.
Ablation experiments show that on the base model we use: (i) SFT warm-up is
important for stable RL training, (ii) structured chains yield more robust
generalization than unstructured ones, and (iii) easy-to-hard curricula
accelerate convergence and improve final performance. These findings
demonstrate that explicit, structured reasoning and curriculum learning
substantially enhances audio-language understanding.

</details>


### [105] [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)
*Li Weigang, Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: The study evaluates LLMs and traditional tools for Chinese-English translation, highlighting challenges in cultural and poetic retention, and proposes a new BLEU variant.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges in preserving poetic intent, cultural heritage, and specialized terminology in Chinese-English translation using LLMs.

Method: Constructs a diverse corpus and uses a BT-Fried evaluation system to assess BLEU, CHRF, TER, and semantic similarity across six LLMs and three traditional tools.

Result: LLMs struggle with cultural and literary retention, while traditional tools excel in distinct texts. A new BLEU variant is proposed.

Conclusion: The study advances empirical evaluation of Chinese NLP and cultural fidelity in AI translation.

Abstract: The rapid advancement of large language models (LLMs) has reshaped the
landscape of machine translation, yet challenges persist in preserving poetic
intent, cultural heritage, and handling specialized terminology in
Chinese-English translation. This study constructs a diverse corpus
encompassing Chinese scientific terminology, historical translation paradoxes,
and literary metaphors. Utilizing a back-translation and Friedman test-based
evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic
similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three
traditional translation tools. Key findings include: (1) Scientific abstracts
often benefit from back-translation, while traditional tools outperform LLMs in
linguistically distinct texts; (2) LLMs struggle with cultural and literary
retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit
"verbatim back-translation", reflecting emergent memory behavior; (4) A novel
BLEU variant using Jieba segmentation and n-gram weighting is proposed. The
study contributes to the empirical evaluation of Chinese NLP performance and
advances understanding of cultural fidelity in AI-mediated translation.

</details>


### [106] [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)
*Fengze Liu, Weidong Zhou, Binbin Liu, Zhimiao Yu, Yifan Zhang, Haobin Lin, Yifeng Yu, Bingni Zhang, Xiaohuan Zhou, Taifeng Wang, Yong Cao*

Main category: cs.CL

TL;DR: QuaDMix is a unified data selection framework for LLM pretraining that jointly optimizes data quality and diversity, achieving a 7.2% performance improvement.


<details>
  <summary>Details</summary>
Motivation: Existing methods optimize quality and diversity separately, ignoring their trade-off, which limits LLM performance.

Method: QuaDMix uses quality criteria and domain classification for diversity, then employs a parameterized sampling function. Simulated experiments and LightGBM optimize parameters.

Result: QuaDMix improves performance by 7.2% on average, outperforming separate quality/diversity strategies.

Conclusion: Balancing quality and diversity is essential for LLM pretraining, and QuaDMix effectively achieves this.

Abstract: Quality and diversity are two critical metrics for the training data of large
language models (LLMs), positively impacting performance. Existing studies
often optimize these metrics separately, typically by first applying quality
filtering and then adjusting data proportions. However, these approaches
overlook the inherent trade-off between quality and diversity, necessitating
their joint consideration. Given a fixed training quota, it is essential to
evaluate both the quality of each data point and its complementary effect on
the overall dataset. In this paper, we introduce a unified data selection
framework called QuaDMix, which automatically optimizes the data distribution
for LLM pretraining while balancing both quality and diversity. Specifically,
we first propose multiple criteria to measure data quality and employ domain
classification to distinguish data points, thereby measuring overall diversity.
QuaDMix then employs a unified parameterized data sampling function that
determines the sampling probability of each data point based on these quality
and diversity related labels. To accelerate the search for the optimal
parameters involved in the QuaDMix framework, we conduct simulated experiments
on smaller models and use LightGBM for parameters searching, inspired by the
RegMix method. Our experiments across diverse models and datasets demonstrate
that QuaDMix achieves an average performance improvement of 7.2% across
multiple benchmarks. These results outperform the independent strategies for
quality and diversity, highlighting the necessity and ability to balance data
quality and diversity.

</details>


### [107] [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)
*Sarah Jabbour, Trenton Chang, Anindya Das Antar, Joseph Peper, Insu Jang, Jiachen Liu, Jae-Won Chung, Shiqi He, Michael Wellman, Bryan Goodman, Elizabeth Bondi-Kelly, Kevin Samy, Rada Mihalcea, Mosharaf Chowdhury, David Jurgens, Lu Wang*

Main category: cs.CL

TL;DR: The paper proposes a dynamic, holistic framework for evaluating GenAI systems in real-world settings, focusing on diverse inputs, ethics, and societal impact.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for GenAI rely on static benchmarks, failing to reflect real-world performance and societal implications.

Method: The paper suggests a comprehensive framework combining continuous, outcome-oriented assessments, human and automated evaluations, and transparency.

Result: The proposed framework ensures GenAI models are technically proficient, ethically responsible, and impactful.

Conclusion: The paper advocates for evolving evaluation methods that prioritize real-world applicability, fairness, and trust among stakeholders.

Abstract: Generative AI (GenAI) models have become vital across industries, yet current
evaluation methods have not adapted to their widespread use. Traditional
evaluations often rely on benchmarks and fixed datasets, frequently failing to
reflect real-world performance, which creates a gap between lab-tested outcomes
and practical applications. This white paper proposes a comprehensive framework
for how we should evaluate real-world GenAI systems, emphasizing diverse,
evolving inputs and holistic, dynamic, and ongoing assessment approaches. The
paper offers guidance for practitioners on how to design evaluation methods
that accurately reflect real-time capabilities, and provides policymakers with
recommendations for crafting GenAI policies focused on societal impacts, rather
than fixed performance numbers or parameter sizes. We advocate for holistic
frameworks that integrate performance, fairness, and ethics and the use of
continuous, outcome-oriented methods that combine human and automated
assessments while also being transparent to foster trust among stakeholders.
Implementing these strategies ensures GenAI models are not only technically
proficient but also ethically responsible and impactful.

</details>


### [108] [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)
*Joseph M. Denning, Xiaohan Hannah Guo, Bryor Snefjella, Idan A. Blank*

Main category: cs.CL

TL;DR: LLMs' sentence representations prioritize syntax over thematic roles, but some attention heads can capture thematic roles independently.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs' word prediction training results in representations that capture thematic roles (who did what to whom).

Method: Analyzed sentence representations in four LLMs, comparing them to human similarity judgments and examining hidden units and attention heads.

Result: LLMs' representations reflected syntactic similarity but not thematic role assignments. Thematic roles were weakly captured, mainly by some attention heads.

Conclusion: LLMs can extract thematic roles, but this information is less influential in their representations compared to humans.

Abstract: Large Language Models (LLMs) are commonly criticized for not understanding
language. However, many critiques focus on cognitive abilities that, in humans,
are distinct from language processing. Here, we instead study a kind of
understanding tightly linked to language: inferring who did what to whom
(thematic roles) in a sentence. Does the central training objective of
LLMs-word prediction-result in sentence representations that capture thematic
roles? In two experiments, we characterized sentence representations in four
LLMs. In contrast to human similarity judgments, in LLMs the overall
representational similarity of sentence pairs reflected syntactic similarity
but not whether their agent and patient assignments were identical vs.
reversed. Furthermore, we found little evidence that thematic role information
was available in any subset of hidden units. However, some attention heads
robustly captured thematic roles, independently of syntax. Therefore, LLMs can
extract thematic roles but, relative to humans, this information influences
their representations more weakly.

</details>


### [109] [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)
*Hannah Cyberey, David Evans*

Main category: cs.CL

TL;DR: The paper explores how safety-tuned LLMs censor responses, using representation engineering to detect and control censorship, and reveals thought suppression in reasoning models.


<details>
  <summary>Details</summary>
Motivation: To understand and manipulate the censorship mechanisms in safety-tuned LLMs, ensuring alignment with user preferences.

Method: Uses representation engineering to identify refusal-compliance vectors and thought suppression vectors in models like DeepSeek-R1.

Result: Demonstrates control over censorship levels and uncovers thought suppression, enabling censorship removal via vector manipulation.

Conclusion: The study provides tools to analyze and adjust censorship in LLMs, offering insights into model behavior and control.

Abstract: Large language models (LLMs) have transformed the way we access information.
These models are often tuned to refuse to comply with requests that are
considered harmful and to produce responses that better align with the
preferences of those who control the models. To understand how this
"censorship" works. We use representation engineering techniques to study
open-weights safety-tuned models. We present a method for finding a
refusal--compliance vector that detects and controls the level of censorship in
model outputs. We also analyze recent reasoning LLMs, distilled from
DeepSeek-R1, and uncover an additional dimension of censorship through "thought
suppression". We show a similar approach can be used to find a vector that
suppresses the model's reasoning process, allowing us to remove censorship by
applying the negative multiples of this vector. Our code is publicly available
at: https://github.com/hannahxchen/llm-censorship-steering

</details>


### [110] [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
*Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang*

Main category: cs.CL

TL;DR: PaperCoder is a multi-agent LLM framework that converts machine learning papers into functional code repositories, excelling in quality and faithfulness.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of available code implementations in ML research, which hinders reproducibility and progress.

Method: A three-stage process (planning, analysis, generation) with specialized agents, evaluated via model-based and human assessments.

Result: PaperCoder produces high-quality, faithful implementations, outperforming baselines on the PaperBench benchmark.

Conclusion: PaperCoder effectively bridges the gap between ML papers and code, enhancing reproducibility and research efficiency.

Abstract: Despite the rapid growth of machine learning research, corresponding code
implementations are often unavailable, making it slow and labor-intensive for
researchers to reproduce results and build upon prior work. In the meantime,
recent Large Language Models (LLMs) excel at understanding scientific documents
and generating high-quality code. Inspired by this, we introduce PaperCoder, a
multi-agent LLM framework that transforms machine learning papers into
functional code repositories. PaperCoder operates in three stages: planning,
where it constructs a high-level roadmap, designs the system architecture with
diagrams, identifies file dependencies, and generates configuration files;
analysis, which focuses on interpreting implementation-specific details; and
generation, where modular, dependency-aware code is produced. Moreover, each
phase is instantiated through a set of specialized agents designed to
collaborate effectively across the pipeline. We then evaluate PaperCoder on
generating code implementations from machine learning papers based on both
model-based and human evaluations, specifically from the original paper
authors, with author-released repositories as ground truth if available. Our
results demonstrate the effectiveness of PaperCoder in creating high-quality,
faithful implementations. Furthermore, it consistently shows strengths in the
recently released PaperBench benchmark, surpassing strong baselines by
substantial margins. Code is available at:
https://github.com/going-doer/Paper2Code.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [111] [A Decade of You Only Look Once (YOLO) for Object Detection](https://arxiv.org/abs/2504.18586)
*Leo Thomas Ramos, Angel D. Sappa*

Main category: cs.CV

TL;DR: A review of YOLO's evolution over 10 years, covering its technical advancements, applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: To critically analyze YOLO's development, impact, and future potential in real-time object detection.

Method: Technical overview of YOLO versions, architectural trends, application areas, evaluation practices, and ethical considerations.

Result: YOLO has evolved into a versatile, efficient, and widely adopted framework with diverse applications.

Conclusion: The review provides a comprehensive perspective on YOLO's trajectory and suggests future development directions.

Abstract: This review marks the tenth anniversary of You Only Look Once (YOLO), one of
the most influential frameworks in real-time object detection. Over the past
decade, YOLO has evolved from a streamlined detector into a diverse family of
architectures characterized by efficient design, modular scalability, and
cross-domain adaptability. The paper presents a technical overview of the main
versions, highlights key architectural trends, and surveys the principal
application areas in which YOLO has been adopted. It also addresses evaluation
practices, ethical considerations, and potential future directions for the
framework's continued development. The analysis aims to provide a comprehensive
and critical perspective on YOLO's trajectory and ongoing transformation.

</details>


### [112] [Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency](https://arxiv.org/abs/2504.18589)
*Zhikai Wang, Jiashuo Sun, Wenqi Zhang, Zhiqiang Hu, Xin Li, Fan Wang, Deli Zhao*

Main category: cs.CV

TL;DR: VCBENCH is introduced to evaluate LVLMs on multimodal mathematical reasoning with visual dependencies, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks overlook elementary-level math problems with visual dependencies, crucial for AGI progress.

Method: VCBENCH includes 1,720 problems across six domains, using 6,697 images for multi-image reasoning.

Result: Top LVLMs scored below 50% accuracy, showing challenges in visual-mathematical integration.

Conclusion: VCBENCH highlights LVLM limitations and suggests future improvements in visual reasoning.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have
significantly enhanced their ability to integrate visual and linguistic
information, achieving near-human proficiency in tasks like object recognition,
captioning, and visual question answering. However, current benchmarks
typically focus on knowledge-centric evaluations that assess domain-specific
expertise, often neglecting the core ability to reason about fundamental
mathematical elements and visual concepts. We identify a gap in evaluating
elementary-level math problems, which rely on explicit visual
dependencies-requiring models to discern, integrate, and reason across multiple
images while incorporating commonsense knowledge, all of which are crucial for
advancing toward broader AGI capabilities. To address this gap, we introduce
VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with
explicit visual dependencies. VCBENCH includes 1,720 problems across six
cognitive domains, featuring 6,697 images (averaging 3.9 per question) to
ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,
revealing substantial performance disparities, with even the top models unable
to exceed 50% accuracy. Our findings highlight the ongoing challenges in
visual-mathematical integration and suggest avenues for future LVLM
advancements.

</details>


### [113] [Co-Training with Active Contrastive Learning and Meta-Pseudo-Labeling on 2D Projections for Deep Semi-Supervised Learning](https://arxiv.org/abs/2504.18666)
*David Aparco-Cardenas, Jancarlo F. Gomes, Alexandre X. Falcão, Pedro J. de Rezende*

Main category: cs.CV

TL;DR: Active-DeepFA combines contrastive learning, meta-pseudo-labeling, and active learning to train CNNs with limited labeled data, outperforming SoTA methods and reducing annotation effort.


<details>
  <summary>Details</summary>
Motivation: The challenge of limited labeled data in DL, especially in error-prone annotation tasks, motivates the need for methods like SSL. However, existing methods rely on pre-trained features or large validation sets, and random sampling neglects informative data.

Method: Active-DeepFA integrates DeepFA into a co-training setup with two networks to mitigate pseudo-label bias. It uses supervised contrastive learning for warm-up, label propagation on 2D feature projections, cross-training for pseudo-label exchange, and active learning for meaningful sample annotation.

Result: Evaluated on biological image datasets with 5% labeled data, Active-DeepFA improves baselines and outperforms six SoTA methods. It achieves comparable results with only 3% labeled data, reducing annotation effort.

Conclusion: Active-DeepFA effectively addresses the scarcity of labeled data by combining multiple learning strategies, demonstrating superior performance and efficiency in image classification tasks.

Abstract: A major challenge that prevents the training of DL models is the limited
availability of accurately labeled data. This shortcoming is highlighted in
areas where data annotation becomes a time-consuming and error-prone task. In
this regard, SSL tackles this challenge by capitalizing on scarce labeled and
abundant unlabeled data; however, SoTA methods typically depend on pre-trained
features and large validation sets to learn effective representations for
classification tasks. In addition, the reduced set of labeled data is often
randomly sampled, neglecting the selection of more informative samples. Here,
we present active-DeepFA, a method that effectively combines CL,
teacher-student-based meta-pseudo-labeling and AL to train non-pretrained CNN
architectures for image classification in scenarios of scarcity of labeled and
abundance of unlabeled data. It integrates DeepFA into a co-training setup that
implements two cooperative networks to mitigate confirmation bias from
pseudo-labels. The method starts with a reduced set of labeled samples by
warming up the networks with supervised CL. Afterward and at regular epoch
intervals, label propagation is performed on the 2D projections of the
networks' deep features. Next, the most reliable pseudo-labels are exchanged
between networks in a cross-training fashion, while the most meaningful samples
are annotated and added into the labeled set. The networks independently
minimize an objective loss function comprising supervised contrastive,
supervised and semi-supervised loss components, enhancing the representations
towards image classification. Our approach is evaluated on three challenging
biological image datasets using only 5% of labeled samples, improving baselines
and outperforming six other SoTA methods. In addition, it reduces annotation
effort by achieving comparable results to those of its counterparts with only
3% of labeled data.

</details>


### [114] [SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models](https://arxiv.org/abs/2504.18684)
*Nader Zantout, Haochen Zhang, Pujith Kachana, Jinkai Qiu, Ji Zhang, Wenshan Wang*

Main category: cs.CV

TL;DR: SORT3D combines 2D object attributes, spatial reasoning, and LLMs for zero-shot 3D object grounding without needing text-to-3D training data.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in 3D object grounding due to scene diversity, fine-grained objects, and limited natural language training data.

Method: Uses 2D object attributes, spatial reasoning heuristics, and LLMs for sequential reasoning, requiring no text-to-3D training.

Result: Achieves state-of-the-art performance on view-dependent grounding tasks and works in real-time on an autonomous vehicle.

Conclusion: SORT3D is effective for zero-shot generalization in unseen environments, with practical applications in robotics.

Abstract: Interpreting object-referential language and grounding objects in 3D with
spatial relations and attributes is essential for robots operating alongside
humans. However, this task is often challenging due to the diversity of scenes,
large number of fine-grained objects, and complex free-form nature of language
references. Furthermore, in the 3D domain, obtaining large amounts of natural
language training data is difficult. Thus, it is important for methods to learn
from little data and zero-shot generalize to new environments. To address these
challenges, we propose SORT3D, an approach that utilizes rich object attributes
from 2D data and merges a heuristics-based spatial reasoning toolbox with the
ability of large language models (LLMs) to perform sequential reasoning.
Importantly, our method does not require text-to-3D data for training and can
be applied zero-shot to unseen environments. We show that SORT3D achieves
state-of-the-art performance on complex view-dependent grounding tasks on two
benchmarks. We also implement the pipeline to run real-time on an autonomous
vehicle and demonstrate that our approach can be used for object-goal
navigation on previously unseen real-world environments. All source code for
the system pipeline is publicly released at https://github.com/nzantout/SORT3D .

</details>


### [115] [CAMeL: Cross-modality Adaptive Meta-Learning for Text-based Person Retrieval](https://arxiv.org/abs/2504.18782)
*Hang Yu, Jiahao Wen, Zhedong Zheng*

Main category: cs.CV

TL;DR: A domain-agnostic pretraining framework (CAMeL) is introduced to address domain biases in synthesized data for text-based person retrieval, improving model generalization and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: High annotation costs and privacy concerns lead to synthesized data, but domain biases in such data limit model scalability.

Method: CAMeL uses cross-modality adaptive meta-learning, dynamic error memory, and dual-speed updates for multi-task adaptation.

Result: CAMeL outperforms state-of-the-art methods on benchmarks (CUHK-PEDES, ICFG-PEDES, RSTPReid) and handles biased/noisy data robustly.

Conclusion: CAMeL enhances generalization and scalability, proving effective for real-world text-based person retrieval tasks.

Abstract: Text-based person retrieval aims to identify specific individuals within an
image database using textual descriptions. Due to the high cost of annotation
and privacy protection, researchers resort to synthesized data for the paradigm
of pretraining and fine-tuning. However, these generated data often exhibit
domain biases in both images and textual annotations, which largely compromise
the scalability of the pre-trained model. Therefore, we introduce a
domain-agnostic pretraining framework based on Cross-modality Adaptive
Meta-Learning (CAMeL) to enhance the model generalization capability during
pretraining to facilitate the subsequent downstream tasks. In particular, we
develop a series of tasks that reflect the diversity and complexity of
real-world scenarios, and introduce a dynamic error sample memory unit to
memorize the history for errors encountered within multiple tasks. To further
ensure multi-task adaptation, we also adopt an adaptive dual-speed update
strategy, balancing fast adaptation to new tasks and slow weight updates for
historical tasks. Albeit simple, our proposed model not only surpasses existing
state-of-the-art methods on real-world benchmarks, including CUHK-PEDES,
ICFG-PEDES, and RSTPReid, but also showcases robustness and scalability in
handling biased synthetic images and noisy text annotations. Our code is
available at https://github.com/Jahawn-Wen/CAMeL-reID.

</details>


### [116] [HierSum: A Global and Local Attention Mechanism for Video Summarization](https://arxiv.org/abs/2504.18689)
*Apoorva Beedu, Irfan Essa*

Main category: cs.CV

TL;DR: HierSum is a hierarchical video summarization method for instructional videos, combining local subtitle cues and global context, outperforming benchmarks in F1-score and rank correlation.


<details>
  <summary>Details</summary>
Motivation: To create effective summaries of instructional videos by identifying essential steps and leveraging multimodal data.

Method: HierSum integrates fine-grained subtitle cues with video-level instructions, using "most replayed" statistics as supervision.

Result: HierSum outperforms existing methods on TVSum, BLiSS, Mr.HiSum, and WikiHow datasets, and a new curated dataset enhances performance.

Conclusion: HierSum is effective for instructional video summarization, with multimodal training data further improving results.

Abstract: Video summarization creates an abridged version (i.e., a summary) that
provides a quick overview of the video while retaining pertinent information.
In this work, we focus on summarizing instructional videos and propose a method
for breaking down a video into meaningful segments, each corresponding to
essential steps in the video. We propose \textbf{HierSum}, a hierarchical
approach that integrates fine-grained local cues from subtitles with global
contextual information provided by video-level instructions. Our approach
utilizes the ``most replayed" statistic as a supervisory signal to identify
critical segments, thereby improving the effectiveness of the summary. We
evaluate on benchmark datasets such as TVSum, BLiSS, Mr.HiSum, and the WikiHow
test set, and show that HierSum consistently outperforms existing methods in
key metrics such as F1-score and rank correlation. We also curate a new
multi-modal dataset using WikiHow and EHow videos and associated articles
containing step-by-step instructions. Through extensive ablation studies, we
demonstrate that training on this dataset significantly enhances summarization
on the target datasets.

</details>


### [117] [Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions](https://arxiv.org/abs/2504.19056)
*Mohammad Mahdi Abootorabi, Omid Ghahroodi, Pardis Sadat Zahraei, Hossein Behzadasl, Alireza Mirrokni, Mobina Salimipanah, Arash Rasouli, Bahar Behzadipour, Sara Azarnoush, Benyamin Maleki, Erfan Sadraiye, Kiarash Kiani Feriz, Mahdi Teymouri Nahad, Ali Moghadasi, Abolfazl Eshagh Abianeh, Nizi Nazar, Hamid R. Rabiee, Mahdieh Soleymani Baghshah, Meisam Ahmadi, Ehsaneddin Asgari*

Main category: cs.CV

TL;DR: A survey on generative AI applications for character animation, covering facial animation, gesture modeling, and more, with resources for newcomers.


<details>
  <summary>Details</summary>
Motivation: The rapid advances in generative AI for animation create a need for an integrative review to provide a coherent view of the field.

Method: Examines state-of-the-art in facial animation, expression rendering, avatar creation, etc., and provides foundational background and evaluation metrics.

Result: Highlights leading research, practical deployments, datasets, and trends, while identifying open challenges.

Conclusion: Offers a roadmap for future research and serves as a resource for newcomers in generative AI animation.

Abstract: Generative AI is reshaping art, gaming, and most notably animation. Recent
breakthroughs in foundation and diffusion models have reduced the time and cost
of producing animated content. Characters are central animation components,
involving motion, emotions, gestures, and facial expressions. The pace and
breadth of advances in recent months make it difficult to maintain a coherent
view of the field, motivating the need for an integrative review. Unlike
earlier overviews that treat avatars, gestures, or facial animation in
isolation, this survey offers a single, comprehensive perspective on all the
main generative AI applications for character animation. We begin by examining
the state-of-the-art in facial animation, expression rendering, image
synthesis, avatar creation, gesture modeling, motion synthesis, object
generation, and texture synthesis. We highlight leading research, practical
deployments, commonly used datasets, and emerging trends for each area. To
support newcomers, we also provide a comprehensive background section that
introduces foundational models and evaluation metrics, equipping readers with
the knowledge needed to enter the field. We discuss open challenges and map
future research directions, providing a roadmap to advance AI-driven
character-animation technologies. This survey is intended as a resource for
researchers and developers entering the field of generative AI animation or
adjacent fields. Resources are available at:
https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.

</details>


### [118] [A Review of 3D Object Detection with Vision-Language Models](https://arxiv.org/abs/2504.18738)
*Ranjan Sapkota, Konstantinos I Roumeliotis, Rahul Harsha Cheppally, Marco Flores Calero, Manoj Karkee*

Main category: cs.CV

TL;DR: A systematic review of 3D object detection using vision-language models (VLMs), comparing traditional methods with modern frameworks like CLIP and 3D LLMs, and discussing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges of 3D object detection with VLMs, such as spatial reasoning and data complexity, and to provide a comprehensive analysis of this emerging field.

Method: Examines over 100 research papers, comparing traditional point cloud and voxel grid approaches with modern VLMs, and reviews architectures, pretraining strategies, and prompt engineering.

Result: Highlights the capabilities of VLMs for open-vocabulary detection and zero-shot generalization, along with performance benchmarks.

Conclusion: Identifies challenges like limited datasets and computational demands, and suggests future research directions to advance the field.

Abstract: This review provides a systematic analysis of comprehensive survey of 3D
object detection with vision-language models(VLMs) , a rapidly advancing area
at the intersection of 3D vision and multimodal AI. By examining over 100
research papers, we provide the first systematic analysis dedicated to 3D
object detection with vision-language models. We begin by outlining the unique
challenges of 3D object detection with vision-language models, emphasizing
differences from 2D detection in spatial reasoning and data complexity.
Traditional approaches using point clouds and voxel grids are compared to
modern vision-language frameworks like CLIP and 3D LLMs, which enable
open-vocabulary detection and zero-shot generalization. We review key
architectures, pretraining strategies, and prompt engineering methods that
align textual and 3D features for effective 3D object detection with
vision-language models. Visualization examples and evaluation benchmarks are
discussed to illustrate performance and behavior. Finally, we highlight current
challenges, such as limited 3D-language datasets and computational demands, and
propose future research directions to advance 3D object detection with
vision-language models. >Object Detection, Vision-Language Models, Agents,
VLMs, LLMs, AI

</details>


### [119] [DeepSPG: Exploring Deep Semantic Prior Guidance for Low-light Image Enhancement with Multimodal Learning](https://arxiv.org/abs/2504.19127)
*Jialang Lu, Huayu Zhao, Huiyu Zhai, Xingxing Yang, Shini Han*

Main category: cs.CV

TL;DR: DeepSPG is a new framework for low-light image enhancement (LLIE) that leverages semantic knowledge from pre-trained models to guide the enhancement process, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLIE methods ignore semantic information, especially in extremely dark regions, leading to suboptimal results. DeepSPG addresses this by incorporating semantic priors.

Method: DeepSPG uses Retinex decomposition and multimodal learning, integrating image-level (from semantic segmentation) and text-level (from vision-language models) semantic priors, along with a multi-scale structure.

Result: DeepSPG achieves superior performance on five benchmark datasets compared to state-of-the-art methods.

Conclusion: Incorporating semantic priors significantly improves LLIE, and DeepSPG provides a robust framework for this task.

Abstract: There has long been a belief that high-level semantics learning can benefit
various downstream computer vision tasks. However, in the low-light image
enhancement (LLIE) community, existing methods learn a brutal mapping between
low-light and normal-light domains without considering the semantic information
of different regions, especially in those extremely dark regions that suffer
from severe information loss. To address this issue, we propose a new deep
semantic prior-guided framework (DeepSPG) based on Retinex image decomposition
for LLIE to explore informative semantic knowledge via a pre-trained semantic
segmentation model and multimodal learning. Notably, we incorporate both
image-level semantic prior and text-level semantic prior and thus formulate a
multimodal learning framework with combinatorial deep semantic prior guidance
for LLIE. Specifically, we incorporate semantic knowledge to guide the
enhancement process via three designs: an image-level semantic prior guidance
by leveraging hierarchical semantic features from a pre-trained semantic
segmentation model; a text-level semantic prior guidance by integrating natural
language semantic constraints via a pre-trained vision-language model; a
multi-scale semantic-aware structure that facilitates effective semantic
feature incorporation. Eventually, our proposed DeepSPG demonstrates superior
performance compared to state-of-the-art methods across five benchmark
datasets. The implementation details and code are publicly available at
https://github.com/Wenyuzhy/DeepSPG.

</details>


### [120] [Dream-Box: Object-wise Outlier Generation for Out-of-Distribution Detection](https://arxiv.org/abs/2504.18746)
*Brian K. S. Isaac-Medina, Toby P. Breckon*

Main category: cs.CV

TL;DR: Dream-Box introduces a method for OOD detection using diffusion models to generate object-wise outliers in pixel space, improving visualization and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of OOD detection, especially in object detection tasks, where traditional methods lack visualization of outliers.

Method: Uses diffusion models to generate object-wise outliers in pixel space, training an object detector for in-distribution tasks and OOD detection.

Result: Achieves comparable performance to traditional methods while providing visualization of OOD objects.

Conclusion: Dream-Box bridges the gap in OOD detection by enabling outlier visualization and maintaining detection performance.

Abstract: Deep neural networks have demonstrated great generalization capabilities for
tasks whose training and test sets are drawn from the same distribution.
Nevertheless, out-of-distribution (OOD) detection remains a challenging task
that has received significant attention in recent years. Specifically, OOD
detection refers to the detection of instances that do not belong to the
training distribution, while still having good performance on the
in-distribution task (e.g., classification or object detection). Recent work
has focused on generating synthetic outliers and using them to train an outlier
detector, generally achieving improved OOD detection than traditional OOD
methods. In this regard, outliers can be generated either in feature or pixel
space. Feature space driven methods have shown strong performance on both the
classification and object detection tasks, at the expense that the
visualization of training outliers remains unknown, making further analysis on
OOD failure modes challenging. On the other hand, pixel space outlier
generation techniques enabled by diffusion models have been used for image
classification using, providing improved OOD detection performance and outlier
visualization, although their adaption to the object detection task is as yet
unexplored. We therefore introduce Dream-Box, a method that provides a link to
object-wise outlier generation in the pixel space for OOD detection.
Specifically, we use diffusion models to generate object-wise outliers that are
used to train an object detector for an in-distribution task and OOD detection.
Our method achieves comparable performance to previous traditional methods
while being the first technique to provide concrete visualization of generated
OOD objects.

</details>


### [121] [Adversarial Shallow Watermarking](https://arxiv.org/abs/2504.19529)
*Guobiao Li, Lei Tan, Yuliang Xue, Gaozhi Liu, Zhenxing Qian, Sheng Li, Xinpeng Zhang*

Main category: cs.CV

TL;DR: ASW is a novel watermarking framework that uses a shallow decoder to resist unknown distortions, outperforming traditional deep learning-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep neural network-based watermarking methods are weak against unknown distortions not included in training.

Method: ASW employs a shallow, randomly parameterized decoder, adversarially optimizes the host image for embedding, and leverages the decoder's insensitivity to distortions for extraction.

Result: ASW shows strong robustness against unknown distortions and performs comparably to traditional methods on known distortions.

Conclusion: ASW is a training-free, encoder-free, and noise layer-free solution that enhances robustness in watermarking.

Abstract: Recent advances in digital watermarking make use of deep neural networks for
message embedding and extraction. They typically follow the ``encoder-noise
layer-decoder''-based architecture. By deliberately establishing a
differentiable noise layer to simulate the distortion of the watermarked
signal, they jointly train the deep encoder and decoder to fit the noise layer
to guarantee robustness. As a result, they are usually weak against unknown
distortions that are not used in their training pipeline. In this paper, we
propose a novel watermarking framework to resist unknown distortions, namely
Adversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder
that is randomly parameterized and designed to be insensitive to distortions
for watermarking extraction. During the watermark embedding, ASW freezes the
shallow decoder and adversarially optimizes a host image until its updated
version (i.e., the watermarked image) stably triggers the shallow decoder to
output the watermark message. During the watermark extraction, it accurately
recovers the message from the watermarked image by leveraging the insensitive
nature of the shallow decoder against arbitrary distortions. Our ASW is
training-free, encoder-free, and noise layer-free. Experiments indicate that
the watermarked images created by ASW have strong robustness against various
unknown distortions. Compared to the existing ``encoder-noise layer-decoder''
approaches, ASW achieves comparable results on known distortions and better
robustness on unknown distortions.

</details>


### [122] [Multi-Stage Boundary-Aware Transformer Network for Action Segmentation in Untrimmed Surgical Videos](https://arxiv.org/abs/2504.18756)
*Rezowan Shuvo, M S Mekala, Eyad Elyan*

Main category: cs.CV

TL;DR: The paper introduces MSBATN, a transformer-based model with hierarchical sliding window attention, to improve surgical action segmentation by addressing over- and under-segmentation issues.


<details>
  <summary>Details</summary>
Motivation: Capturing surgical workflows is challenging due to variability in surgeons' approaches and subtle action transitions in untrimmed videos.

Method: Proposes MSBATN with a unified loss function and boundary voting mechanism for precise action segmentation.

Result: Achieves state-of-the-art F1 scores at 25% and 50% thresholds on three surgical datasets.

Conclusion: MSBATN effectively addresses segmentation challenges in surgical workflows, outperforming traditional methods.

Abstract: Understanding actions within surgical workflows is essential for evaluating
post-operative outcomes. However, capturing long sequences of actions performed
in surgical settings poses challenges, as individual surgeons have their unique
approaches shaped by their expertise, leading to significant variability. To
tackle this complex problem, we focused on segmentation with precise
boundaries, a demanding task due to the inherent variability in action
durations and the subtle transitions often observed in untrimmed videos. These
transitions, marked by ambiguous starting and ending points, complicate the
segmentation process. Traditional models, such as MS-TCN, which depend on large
receptive fields, frequently face challenges of over-segmentation (resulting in
fragmented segments) or under-segmentation (merging distinct actions). Both of
these issues negatively impact the quality of segmentation. To overcome these
challenges, we present the Multi-Stage Boundary-Aware Transformer Network
(MSBATN) with hierarchical sliding window attention, designed to enhance action
segmentation. Our proposed approach incorporates a novel unified loss function
that treats action classification and boundary detection as distinct yet
interdependent tasks. Unlike traditional binary boundary detection methods, our
boundary voting mechanism accurately identifies start and end points by
leveraging contextual information. Extensive experiments using three
challenging surgical datasets demonstrate the superior performance of the
proposed method, achieving state-of-the-art results in F1 scores at thresholds
of 25% and 50%, while also delivering comparable performance in other metrics.

</details>


### [123] [PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data](https://arxiv.org/abs/2504.18770)
*Manuel Weber, Carly Beneke*

Main category: cs.CV

TL;DR: PyViT-FUSE is a foundation model for multi-modal earth observation data, using attention-based fusion and a pyramidal vision transformer, trained self-supervised for interpretability and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To handle multi-modal, mixed-resolution earth observation data by learning a unified representation through attention fusion.

Method: Uses attention to fuse arbitrary input bands, processes with pyramidal vision transformers, and trains self-supervised via SwAV concepts.

Result: Demonstrates interpretable fusion via attention scores and applicability to downstream tasks.

Conclusion: PyViT-FUSE effectively unifies multi-modal earth observation data with interpretable fusion and broad applicability.

Abstract: We propose PyViT-FUSE, a foundation model for earth observation data
explicitly designed to handle multi-modal imagery by learning to fuse an
arbitrary number of mixed-resolution input bands into a single representation
through an attention mechanism. The learned patch tokens are further processed
by a stack of vision transformers with a novel pyramidal structure. We train
the model on a globally sampled dataset in a self-supervised manner, leveraging
core concepts of the SwAV algorithm. We show the interpretability of the fusion
mechanism by visualization of the attention scores and the models applicability
to downstream tasks.

</details>


### [124] [Depth as Points: Center Point-based Depth Estimation](https://arxiv.org/abs/2504.18773)
*Zhiheng Tu, Xinjian Huang, Yong He, Ruiyang Zhou, Bo Du, Weitao Wu*

Main category: cs.CV

TL;DR: The paper introduces VirDepth, a virtual dataset for autonomous driving, and CenterDepth, a lightweight monocular depth estimation method, both improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational and hardware demands of vehicle and pedestrian perception in urban autonomous driving scenarios.

Method: Developed a virtual dataset generation method (VirDepth) and proposed CenterDepth, a lightweight architecture integrating global semantics and multi-scale features.

Result: Superior performance in computational speed and prediction accuracy for depth estimation tasks.

Conclusion: The proposed methods efficiently tackle limitations in autonomous driving perception, offering scalable and accurate solutions.

Abstract: The perception of vehicles and pedestrians in urban scenarios is crucial for
autonomous driving. This process typically involves complicated data
collection, imposes high computational and hardware demands. To address these
limitations, we first develop a highly efficient method for generating virtual
datasets, which enables the creation of task- and scenario-specific datasets in
a short time. Leveraging this method, we construct the virtual depth estimation
dataset VirDepth, a large-scale, multi-task autonomous driving dataset.
Subsequently, we propose CenterDepth, a lightweight architecture for monocular
depth estimation that ensures high operational efficiency and exhibits superior
performance in depth estimation tasks with highly imbalanced height-scale
distributions. CenterDepth integrates global semantic information through the
innovative Center FC-CRFs algorithm, aggregates multi-scale features based on
object key points, and enables detection-based depth estimation of targets.
Experiments demonstrate that our proposed method achieves superior performance
in terms of both computational speed and prediction accuracy.

</details>


### [125] [FineVQ: Fine-Grained User Generated Content Video Quality Assessment](https://arxiv.org/abs/2412.19238)
*Huiyu Duan, Qiang Hu, Jiarui Wang, Liu Yang, Zitong Xu, Lu Liu, Xiongkuo Min, Chunlei Cai, Tianxiao Ye, Xiaoyun Zhang, Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces FineVD, a large-scale database for fine-grained video quality assessment (VQA) of UGC videos, and proposes the FineVQ model to provide detailed quality ratings, scores, and attributions.


<details>
  <summary>Details</summary>
Motivation: Current VQA models lack fine-grained labels for UGC videos, limiting their utility for video processing and recommendation.

Method: The authors create the FineVD database with 6104 UGC videos and fine-grained scores, then develop the FineVQ model for multi-dimensional quality assessment.

Result: FineVQ achieves state-of-the-art performance on FineVD and other UGC-VQA datasets, providing detailed quality insights.

Conclusion: FineVD and FineVQ address the gap in fine-grained VQA, enhancing video quality monitoring and optimization for UGC content.

Abstract: The rapid growth of user-generated content (UGC) videos has produced an
urgent need for effective video quality assessment (VQA) algorithms to monitor
video quality and guide optimization and recommendation procedures. However,
current VQA models generally only give an overall rating for a UGC video, which
lacks fine-grained labels for serving video processing and recommendation
applications. To address the challenges and promote the development of UGC
videos, we establish the first large-scale Fine-grained Video quality
assessment Database, termed FineVD, which comprises 6104 UGC videos with
fine-grained quality scores and descriptions across multiple dimensions. Based
on this database, we propose a Fine-grained Video Quality assessment (FineVQ)
model to learn the fine-grained quality of UGC videos, with the capabilities of
quality rating, quality scoring, and quality attribution. Extensive
experimental results demonstrate that our proposed FineVQ can produce
fine-grained video-quality results and achieve state-of-the-art performance on
FineVD and other commonly used UGC-VQA datasets.

</details>


### [126] [IoT Botnet Detection: Application of Vision Transformer to Classification of Network Flow Traffic](https://arxiv.org/abs/2504.18781)
*Hassan Wasswa, Timothy Lynar, Aziida Nanyonga, Hussein Abbass*

Main category: cs.CV

TL;DR: A novel preprocessing method adapts Vision Transformers (ViT) for IoT botnet attack detection by transforming network flow packets into 1-channel 2D images, enabling ViT-based classification with improved classifier options.


<details>
  <summary>Details</summary>
Motivation: Existing tools for IoT network flow packet feature extraction lack sequential and spatial pattern capture, limiting transformer model effectiveness.

Method: Feature extraction from .pcap files, transforming instances into 1-channel 2D images, and enhancing ViT to support various classifiers (DNN, LSTM, BLSTM).

Result: Competitive performance in precision, recall, and F1-score for multiclass attack detection on two IoT attack datasets.

Conclusion: The proposed method successfully adapts ViT for IoT botnet detection, outperforming traditional classifiers.

Abstract: Despite the demonstrated effectiveness of transformer models in NLP, and
image and video classification, the available tools for extracting features
from captured IoT network flow packets fail to capture sequential patterns in
addition to the absence of spatial patterns consequently limiting transformer
model application. This work introduces a novel preprocessing method to adapt
transformer models, the vision transformer (ViT) in particular, for IoT botnet
attack detection using network flow packets. The approach involves feature
extraction from .pcap files and transforming each instance into a 1-channel 2D
image shape, enabling ViT-based classification. Also, the ViT model was
enhanced to allow use any classifier besides Multilayer Perceptron (MLP) that
was deployed in the initial ViT paper. Models including the conventional feed
forward Deep Neural Network (DNN), LSTM and Bidirectional-LSTM (BLSTM)
demonstrated competitive performance in terms of precision, recall, and
F1-score for multiclass-based attack detection when evaluated on two IoT attack
datasets.

</details>


### [127] [Video CLIP Model for Multi-View Echocardiography Interpretation](https://arxiv.org/abs/2504.18800)
*Ryo Takizawa, Satoshi Kodera, Tempei Kabayama, Ryo Matsuoka, Yuta Ando, Yuto Nakamura, Haruki Settai, Norihiko Takeda*

Main category: cs.CV

TL;DR: A video-language model using multiple echocardiographic views improves diagnostic accuracy over single-view or image-based models.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models for echocardiography rely on single-frame inputs, limiting accuracy for motion-dependent conditions and ignoring the benefits of multiple views.

Method: Developed a video-language model trained on 60,747 cases, incorporating five views and full video sequences paired with clinical reports.

Result: The model outperformed single-view video and image-based models in interpretation accuracy.

Conclusion: Using multiple views and full video sequences enhances diagnostic accuracy in echocardiography.

Abstract: Echocardiography involves recording videos of the heart using ultrasound,
enabling clinicians to evaluate its condition. Recent advances in large-scale
vision-language models (VLMs) have garnered attention for automating the
interpretation of echocardiographic videos. However, most existing VLMs
proposed for medical interpretation thus far rely on single-frame (i.e., image)
inputs. Consequently, these image-based models often exhibit lower diagnostic
accuracy for conditions identifiable through cardiac motion. Moreover,
echocardiographic videos are recorded from various views that depend on the
direction of ultrasound emission, and certain views are more suitable than
others for interpreting specific conditions. Incorporating multiple views could
potentially yield further improvements in accuracy. In this study, we developed
a video-language model that takes five different views and full video sequences
as input, training it on pairs of echocardiographic videos and clinical reports
from 60,747 cases. Our experiments demonstrate that this expanded approach
achieves higher interpretation accuracy than models trained with only
single-view videos or with still images.

</details>


### [128] [Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning](https://arxiv.org/abs/2504.18810)
*Yifan Xie, Fei Ma, Yi Bin, Ying He, Fei Yu*

Main category: cs.CV

TL;DR: Proposes JULNet for talking face video generation, focusing on visual uncertainty learning to improve quality and robustness.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of attention to visual uncertainty in existing systems, leading to inconsistent quality and performance.

Method: Introduces an uncertainty module to predict error and uncertainty maps, using KL divergence and histogram techniques for joint optimization.

Result: Achieves superior high-fidelity and audio-lip synchronization compared to previous methods.

Conclusion: JULNet enhances performance and robustness in talking face video generation by learning visual uncertainty.

Abstract: Talking face video generation with arbitrary speech audio is a significant
challenge within the realm of digital human technology. The previous studies
have emphasized the significance of audio-lip synchronization and visual
quality. Currently, limited attention has been given to the learning of visual
uncertainty, which creates several issues in existing systems, including
inconsistent visual quality and unreliable performance across different input
conditions. To address the problem, we propose a Joint Uncertainty Learning
Network (JULNet) for high-quality talking face video generation, which
incorporates a representation of uncertainty that is directly related to visual
error. Specifically, we first design an uncertainty module to individually
predict the error map and uncertainty map after obtaining the generated image.
The error map represents the difference between the generated image and the
ground truth image, while the uncertainty map is used to predict the
probability of incorrect estimates. Furthermore, to match the uncertainty
distribution with the error distribution through a KL divergence term, we
introduce a histogram technique to approximate the distributions. By jointly
optimizing error and uncertainty, the performance and robustness of our model
can be enhanced. Extensive experiments demonstrate that our method achieves
superior high-fidelity and audio-lip synchronization in talking face video
generation compared to previous methods.

</details>


### [129] [WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic Localization System](https://arxiv.org/abs/2504.18870)
*Guodong Sun, Mingjing Li, Dingjie Liu, Mingxuan Liu, Bo Wu, Yang Zhang*

Main category: cs.CV

TL;DR: The paper proposes a 3-D LiDAR system for precise automatic positioning of truck compartments in cluttered environments, addressing adaptability and reliability issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with varying truck sizes, lack a unified coordinate system, and face reliability challenges in cluttered settings.

Method: The system uses wide field-of-view 3-D LiDAR for high-density point clouds, incorporates parking constraints for segmentation, and leverages geometric features for key point positioning.

Result: The system achieves reliable positioning accuracy and reduced computational resource use, validated on collected and public datasets.

Conclusion: The proposed system is effective for automated loading, with potential for broader application in logistics automation.

Abstract: As an essential component of logistics automation, the automated loading
system is becoming a critical technology for enhancing operational efficiency
and safety. Precise automatic positioning of the truck compartment, which
serves as the loading area, is the primary step in automated loading. However,
existing methods have difficulty adapting to truck compartments of various
sizes, do not establish a unified coordinate system for LiDAR and mobile
manipulators, and often exhibit reliability issues in cluttered environments.
To address these limitations, our study focuses on achieving precise automatic
positioning of key points in large, medium, and small fence-style truck
compartments in cluttered scenarios. We propose an innovative wide
field-of-view 3-D LiDAR vehicle compartment automatic localization system. For
vehicles of various sizes, this system leverages the LiDAR to generate
high-density point clouds within an extensive field-of-view range. By
incorporating parking area constraints, our vehicle point cloud segmentation
method more effectively segments vehicle point clouds within the scene. Our
compartment key point positioning algorithm utilizes the geometric features of
the compartments to accurately locate the corner points, providing stackable
spatial regions. Extensive experiments on our collected data and public
datasets demonstrate that this system offers reliable positioning accuracy and
reduced computational resource consumption, leading to its application and
promotion in relevant fields.

</details>


### [130] [Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation](https://arxiv.org/abs/2504.18856)
*Shahad Albastaki, Anabia Sohail, Iyyakutti Iyappan Ganapathi, Basit Alawode, Asim Khan, Sajid Javed, Naoufel Werghi, Mohammed Bennamoun, Arif Mahmood*

Main category: cs.CV

TL;DR: A novel multi-resolution paradigm for Vision-Language Models in Computational Pathology improves cancer subtype classification and tissue phenotyping by aligning image-text pairs at multiple resolutions.


<details>
  <summary>Details</summary>
Motivation: Single-resolution alignment in Vision-Language Models (VLMs) lacks detail for tasks like cancer subtype classification and survival analysis. Multi-resolution alignment is proposed to address this limitation.

Method: Leverages Whole Slide Images (WSIs) to extract histology patches at multiple resolutions, generates textual descriptions, and introduces visual-textual and cross-resolution alignment using a multimodal encoder. Novel loss functions enrich feature representation.

Result: Pre-trained on TCGA with 34M image-language pairs, the model outperforms SOTA in multiple datasets and tasks.

Conclusion: The multi-resolution approach enhances discriminative ability and generalization, proving effective for Computational Pathology.

Abstract: In Computational Pathology (CPath), the introduction of Vision-Language
Models (VLMs) has opened new avenues for research, focusing primarily on
aligning image-text pairs at a single magnification level. However, this
approach might not be sufficient for tasks like cancer subtype classification,
tissue phenotyping, and survival analysis due to the limited level of detail
that a single-resolution image can provide. Addressing this, we propose a novel
multi-resolution paradigm leveraging Whole Slide Images (WSIs) to extract
histology patches at multiple resolutions and generate corresponding textual
descriptions through advanced CPath VLM. We introduce visual-textual alignment
at multiple resolutions as well as cross-resolution alignment to establish more
effective text-guided visual representations. Cross-resolution alignment using
a multimodal encoder enhances the model's ability to capture context from
multiple resolutions in histology images. Our model aims to capture a broader
range of information, supported by novel loss functions, enriches feature
representation, improves discriminative ability, and enhances generalization
across different resolutions. Pre-trained on a comprehensive TCGA dataset with
34 million image-language pairs at various resolutions, our fine-tuned model
outperforms state-of-the-art (SOTA) counterparts across multiple datasets and
tasks, demonstrating its effectiveness in CPath. The code is available on
GitHub at: https://github.com/BasitAlawode/MR-PLIP

</details>


### [131] [PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification](https://arxiv.org/abs/2504.19136)
*Huiling Zheng, Xian Zhong, Bin Liu, Yi Xiao, Bihan Wen, Xiaofeng Li*

Main category: cs.CV

TL;DR: Proposes Phase-Amplitude Decoupling (PAD) for SAR and RGB fusion, separating phase (shared) and amplitude (specific) features in the Fourier domain for better land cover classification.


<details>
  <summary>Details</summary>
Motivation: Addresses modality heterogeneity and underutilized spectral complementarity in SAR-RGB fusion, avoiding feature conflicts and information loss.

Method: Uses Phase Spectrum Correction (PSC) for geometric consistency and Amplitude Spectrum Fusion (ASF) for dynamic detail integration via frequency-aware MLPs.

Result: Achieves state-of-the-art performance on WHU-OPT-SAR and DDHR-SK datasets.

Conclusion: PAD sets a new physics-aware multi-modal fusion paradigm in remote sensing; code available on GitHub.

Abstract: The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover
classification remains challenging due to modality heterogeneity and the
underutilization of spectral complementarity. Existing methods often fail to
decouple shared structural features from modality-specific radiometric
attributes, leading to feature conflicts and information loss. To address this
issue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework
that separates phase (modality-shared) and amplitude (modality-specific)
components in the Fourier domain. Specifically, PAD consists of two key
components: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase
features through convolution-guided scaling to enhance geometric consistency,
and 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates
high-frequency details and low-frequency structures using frequency-adaptive
multilayer perceptrons. This approach leverages SAR's sensitivity to
morphological features and RGB's spectral richness. Extensive experiments on
WHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our
work establishes a new paradigm for physics-aware multi-modal fusion in remote
sensing. The code will be available at https://github.com/RanFeng2/PAD.

</details>


### [132] [Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras](https://arxiv.org/abs/2504.18864)
*Yunzhong Zhang, Bo Xiong, You Zhou, Changqing Su, Zhen Cheng, Zhaofei Yu, Xun Cao, Tiejun Huang*

Main category: cs.CV

TL;DR: The paper introduces Spike Imaging Velocimetry (SIV), a deep learning framework for flow measurement using spike cameras in Particle Image Velocimetry (PIV), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate and non-intrusive flow measurement is crucial, and PIV is widely used. Spike cameras offer potential for high-speed, high-dynamic-range imaging in complex flows.

Method: Proposes SIV with a Detail-Preserving Hierarchical Transform (DPHT) module and Graph Encoder (GE) for feature extraction. Introduces the PSSD dataset for validation.

Result: SIV achieves superior performance on the PSSD dataset compared to baseline methods.

Conclusion: The study demonstrates the effectiveness of SIV for complex flow fields and provides open-sourced datasets and implementation.

Abstract: The need for accurate and non-intrusive flow measurement methods has led to
the widespread adoption of Particle Image Velocimetry (PIV), a powerful
diagnostic tool in fluid motion estimation. This study investigates the
tremendous potential of spike cameras (a type of ultra-high-speed,
high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike
Imaging Velocimetry (SIV), designed specifically for highly turbulent and
intricate flow fields. To aggregate motion features from the spike stream while
minimizing information loss, we incorporate a Detail-Preserving Hierarchical
Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to
extract contextual features from highly complex fluid flows. Furthermore, we
present a spike-based PIV dataset, Particle Scenes with Spike and Displacement
(PSSD), which provides labeled data for three challenging fluid dynamics
scenarios. Our proposed method achieves superior performance compared to
existing baseline methods on PSSD. The datasets and our implementation of SIV
are open-sourced in the supplementary materials.

</details>


### [133] [Adaptive Dual-domain Learning for Underwater Image Enhancement](https://arxiv.org/abs/2504.19198)
*Lingtao Peng, Liheng Bian*

Main category: cs.CV

TL;DR: SS-UIE proposes a spatial-spectral dual-domain adaptive learning method for underwater image enhancement, addressing inconsistent degradation and high-frequency detail reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing UIE methods fail to address inconsistent degradation levels across spatial regions and spectral bands, and neglect the difficulty of reconstructing high-frequency details.

Method: Introduces MCSS and SWSA modules in parallel to form SS-blocks, enabling adaptive UIE. Uses Frequency-Wise Loss (FWL) to focus on high-frequency details.

Result: SS-UIE outperforms state-of-the-art methods with lower computational and memory costs.

Conclusion: SS-UIE effectively addresses key challenges in UIE, offering superior performance and efficiency.

Abstract: Recently, learning-based Underwater Image Enhancement (UIE) methods have
demonstrated promising performance. However, existing learning-based methods
still face two challenges. 1) They rarely consider the inconsistent degradation
levels in different spatial regions and spectral bands simultaneously. 2) They
treat all regions equally, ignoring that the regions with high-frequency
details are more difficult to reconstruct. To address these challenges, we
propose a novel UIE method based on spatial-spectral dual-domain adaptive
learning, termed SS-UIE. Specifically, we first introduce a spatial-wise
Multi-scale Cycle Selective Scan (MCSS) module and a Spectral-Wise
Self-Attention (SWSA) module, both with linear complexity, and combine them in
parallel to form a basic Spatial-Spectral block (SS-block). Benefiting from the
global receptive field of MCSS and SWSA, SS-block can effectively model the
degradation levels of different spatial regions and spectral bands, thereby
enabling degradation level-based dual-domain adaptive UIE. By stacking multiple
SS-blocks, we build our SS-UIE network. Additionally, a Frequency-Wise Loss
(FWL) is introduced to narrow the frequency-wise discrepancy and reinforce the
model's attention on the regions with high-frequency details. Extensive
experiments validate that the SS-UIE technique outperforms state-of-the-art UIE
methods while requiring cheaper computational and memory costs.

</details>


### [134] [PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance](https://arxiv.org/abs/2504.18866)
*Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Mengjingcheng Mo, Jiankang Zheng, Qingqing Li, Ji Gan, Xinbo Gao*

Main category: cs.CV

TL;DR: PiercingEye improves weakly supervised video violence detection by combining Euclidean and hyperbolic geometries, using hierarchical modeling and logic-guided ambiguous samples for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with distinguishing visually similar but semantically distinct events due to limited hierarchical modeling and ambiguous samples.

Method: Proposes PiercingEye, a dual-space learning framework with hyperbolic aggregation, cross-space attention, and logic-guided ambiguous samples via large language models.

Result: Achieves state-of-the-art performance on XD-Violence and UCF-Crime benchmarks, especially on ambiguous event subsets.

Conclusion: PiercingEye excels in fine-grained violence detection by leveraging dual-space learning and explicit supervision.

Abstract: Existing weakly supervised video violence detection (VVD) methods primarily
rely on Euclidean representation learning, which often struggles to distinguish
visually similar yet semantically distinct events due to limited hierarchical
modeling and insufficient ambiguous training samples. To address this
challenge, we propose PiercingEye, a novel dual-space learning framework that
synergizes Euclidean and hyperbolic geometries to enhance discriminative
feature representation. Specifically, PiercingEye introduces a layer-sensitive
hyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to
progressively model event hierarchies, and a cross-space attention mechanism to
facilitate complementary feature interactions between Euclidean and hyperbolic
spaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage
large language models to generate logic-guided ambiguous event descriptions,
enabling explicit supervision through a hyperbolic vision-language contrastive
loss that prioritizes high-confusion samples via dynamic similarity-aware
weighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks
demonstrate that PiercingEye achieves state-of-the-art performance, with
particularly strong results on a newly curated ambiguous event subset,
validating its superior capability in fine-grained violence detection.

</details>


### [135] [Exploiting Multiple Representations: 3D Face Biometrics Fusion with Application to Surveillance](https://arxiv.org/abs/2504.18886)
*Simone Maurizio La Cava, Roberto Casula, Sara Concas, Giulia Orrù, Ruben Tolosana, Martin Drahansky, Julian Fierrez, Gian Luca Marcialis*

Main category: cs.CV

TL;DR: The paper explores using multiple 3D face reconstruction (3DFR) algorithms and fusion methods to improve face recognition in uncontrolled scenarios, demonstrating enhanced robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of single 3DFR algorithms in face recognition systems, especially in challenging, uncontrolled scenarios, by leveraging multiple algorithms and fusion techniques.

Method: Analyzes various 3DFR algorithms and employs parametric and non-parametric score-level fusion methods to combine their strengths. Tests face recognition systems under diverse conditions (e.g., varying distances, camera setups) to evaluate robustness.

Result: Shows that combining multiple 3DFR algorithms improves generalization across scenarios. Advanced fusion strategies enhance the reliability of face recognition systems.

Conclusion: Proposes fusion-based 3DFR methods as effective for improving face recognition robustness, with potential applications beyond identity recognition.

Abstract: 3D face reconstruction (3DFR) algorithms are based on specific assumptions
tailored to the limits and characteristics of the different application
scenarios. In this study, we investigate how multiple state-of-the-art 3DFR
algorithms can be used to generate a better representation of subjects, with
the final goal of improving the performance of face recognition systems in
challenging uncontrolled scenarios. We also explore how different parametric
and non-parametric score-level fusion methods can exploit the unique strengths
of multiple 3DFR algorithms to enhance biometric recognition robustness. With
this goal, we propose a comprehensive analysis of several face recognition
systems across diverse conditions, such as varying distances and camera setups,
intra-dataset and cross-dataset, to assess the robustness of the proposed
ensemble method. The results demonstrate that the distinct information provided
by different 3DFR algorithms can alleviate the problem of generalizing over
multiple application scenarios. In addition, the present study highlights the
potential of advanced fusion strategies to enhance the reliability of
3DFR-based face recognition systems, providing the research community with key
insights to exploit them in real-world applications effectively. Although the
experiments are carried out in a specific face verification setup, our proposed
fusion-based 3DFR methods may be applied to other tasks around face biometrics
that are not strictly related to identity recognition.

</details>


### [136] [Magnifier: A Multi-grained Neural Network-based Architecture for Burned Area Delineation](https://arxiv.org/abs/2504.19589)
*Daniele Rege Cambrin, Luca Colomba, Paolo Garza*

Main category: cs.CV

TL;DR: The paper introduces Magnifier, a dual-encoder methodology for improving image segmentation in crisis management with limited data, achieving better performance (+2.65% IoU) with minimal parameter increase.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and lack of benchmark datasets hinder neural network training for disaster-related image segmentation.

Method: Magnifier uses a dual-encoder (local and global) approach to merge contextual information at different granularities, enhancing segmentation with limited data.

Result: Magnifier improves IoU by 2.65% and achieves comparable or better performance with fewer GFLOPs than state-of-the-art models.

Conclusion: Magnifier is a scalable, efficient solution for improving segmentation in data-scarce scenarios, applicable to existing encoder-decoder architectures.

Abstract: In crisis management and remote sensing, image segmentation plays a crucial
role, enabling tasks like disaster response and emergency planning by analyzing
visual data. Neural networks are able to analyze satellite acquisitions and
determine which areas were affected by a catastrophic event. The problem in
their development in this context is the data scarcity and the lack of
extensive benchmark datasets, limiting the capabilities of training large
neural network models. In this paper, we propose a novel methodology, namely
Magnifier, to improve segmentation performance with limited data availability.
The Magnifier methodology is applicable to any existing encoder-decoder
architecture, as it extends a model by merging information at different
contextual levels through a dual-encoder approach: a local and global encoder.
Magnifier analyzes the input data twice using the dual-encoder approach. In
particular, the local and global encoders extract information from the same
input at different granularities. This allows Magnifier to extract more
information than the other approaches given the same set of input images.
Magnifier improves the quality of the results of +2.65% on average IoU while
leading to a restrained increase in terms of the number of trainable parameters
compared to the original model. We evaluated our proposed approach with
state-of-the-art burned area segmentation models, demonstrating, on average,
comparable or better performances in less than half of the GFLOPs.

</details>


### [137] [Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness](https://arxiv.org/abs/2504.18906)
*Yufeng Wu, Xin Liao, Baowei Wang, Han Fang, Xiaoshuai Wu, Guiling Wang*

Main category: cs.CV

TL;DR: Proposes Simulation-to-Real (S2R), an unsupervised method to improve watermarking robustness against screen-camera noise by learning noise distribution discrepancies.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods for screen-camera images fail to effectively approximate noise due to biased mathematical modeling or supervised learning limitations.

Method: Uses unsupervised learning with unpaired data to bridge the gap between simulated and real-world noise distributions.

Result: Outperforms state-of-the-art methods in watermark robustness and generalization.

Conclusion: S2R provides a simpler, more effective solution for watermarking in screen-camera scenarios.

Abstract: Unauthorized screen capturing and dissemination pose severe security threats
such as data leakage and information theft. Several studies propose robust
watermarking methods to track the copyright of Screen-Camera (SC) images,
facilitating post-hoc certification against infringement. These techniques
typically employ heuristic mathematical modeling or supervised neural network
fitting as the noise layer, to enhance watermarking robustness against SC.
However, both strategies cannot fundamentally achieve an effective
approximation of SC noise. Mathematical simulation suffers from biased
approximations due to the incomplete decomposition of the noise and the absence
of interdependence among the noise components. Supervised networks require
paired data to train the noise-fitting model, and it is difficult for the model
to learn all the features of the noise. To address the above issues, we propose
Simulation-to-Real (S2R). Specifically, an unsupervised noise layer employs
unpaired data to learn the discrepancy between the modeling simulated noise
distribution and the real-world SC noise distribution, rather than directly
learning the mapping from sharp images to real-world images. Learning this
transformation from simulation to reality is inherently simpler, as it
primarily involves bridging the gap in noise distributions, instead of the
complex task of reconstructing fine-grained image details. Extensive
experimental results validate the efficacy of the proposed method,
demonstrating superior watermark robustness and generalization compared to
those of state-of-the-art methods.

</details>


### [138] [Kinship Verification through a Forest Neural Network](https://arxiv.org/abs/2504.18910)
*Ali Nazari, Mohsen Ebrahimi Moghaddam, Omidreza Borzoei*

Main category: cs.CV

TL;DR: Proposed a GNN-based approach for kinship verification, improving accuracy over traditional face representations and achieving top results on KinFaceW-II.


<details>
  <summary>Details</summary>
Motivation: Early face representations for kinship verification were less accurate than joint representations, prompting a need for better methods.

Method: Used graph neural networks to leverage face representations, designed a classification module, and introduced a gradual center loss combination.

Result: Achieved best results on KinFaceW-II (1.6% average improvement) and near-best on KinFaceW-I.

Conclusion: The GNN-based approach is effective for kinship verification, outperforming traditional methods.

Abstract: Early methods used face representations in kinship verification, which are
less accurate than joint representations of parents' and children's facial
images learned from scratch. We propose an approach featuring graph neural
network concepts to utilize face representations and have comparable results to
joint representation algorithms. Moreover, we designed the structure of the
classification module and introduced a new combination of losses to engage the
center loss gradually in training our network. Additionally, we conducted
experiments on KinFaceW-I and II, demonstrating the effectiveness of our
approach. We achieved the best result on KinFaceW-II, an average improvement of
nearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The
code is available at https://github.com/ali-nazari/Kinship-Verification

</details>


### [139] [R-Sparse R-CNN: SAR Ship Detection Based on Background-Aware Sparse Learnable Proposals](https://arxiv.org/abs/2504.18959)
*Kamirul Kamirul, Odysseas Pappas, Alin Achim*

Main category: cs.CV

TL;DR: R-Sparse R-CNN introduces a streamlined pipeline for ship detection in SAR images using sparse, background-aware proposals and a transformer-based Interaction Module, achieving superior accuracy.


<details>
  <summary>Details</summary>
Motivation: The complexity of SAR imagery and overlapping ship detections necessitate a more efficient and accurate detection method.

Method: The pipeline uses background-aware proposals (BAPs) and Dual-Context Pooling (DCP) to integrate ship and background features, alongside a transformer-based Interaction Module for contextual learning.

Result: R-Sparse R-CNN outperforms state-of-the-art models by up to 12.8% and 11.9% on SSDD and RSDD-SAR datasets.

Conclusion: The framework is effective and competitive for oriented ship detection in SAR imagery, with code publicly available.

Abstract: We introduce R-Sparse R-CNN, a novel pipeline for oriented ship detection in
Synthetic Aperture Radar (SAR) images that leverages sparse learnable proposals
enriched with background contextual information, termed background-aware
proposals (BAPs). The adoption of sparse proposals streamlines the pipeline by
eliminating the need for proposal generators and post-processing for
overlapping predictions. The proposed BAPs enrich object representation by
integrating ship and background features, allowing the model to learn their
contextual relationships for more accurate distinction of ships in complex
environments. To complement BAPs, we propose Dual-Context Pooling (DCP), a
novel strategy that jointly extracts ship and background features in a single
unified operation. This unified design improves efficiency by eliminating
redundant computation inherent in separate pooling. Moreover, by ensuring that
ship and background features are pooled from the same feature map level, DCP
provides aligned features that improve contextual relationship learning.
Finally, as a core component of contextual relationship learning in R-Sparse
R-CNN, we design a dedicated transformer-based Interaction Module. This module
interacts pooled ship and background features with corresponding proposal
features and models their relationships. Experimental results show that
R-Sparse R-CNN delivers outstanding accuracy, surpassing state-of-the-art
models by margins of up to 12.8% and 11.9% on SSDD and RSDD-SAR inshore
datasets, respectively. These results demonstrate the effectiveness and
competitiveness of R-Sparse R-CNN as a robust framework for oriented ship
detection in SAR imagery. The code is available at:
www.github.com/ka-mirul/R-Sparse-R-CNN.

</details>


### [140] [3DPyranet Features Fusion for Spatio-temporal Feature Learning](https://arxiv.org/abs/2504.18977)
*Ihsan Ullah, Alfredo Petrosino*

Main category: cs.CV

TL;DR: The paper introduces 3DPyraNet, a 3D pyramidal neural network, and 3DPyraNet-F, a discriminative approach for spatio-temporal feature learning, addressing the parameter inefficiency of deep CNNs while maintaining biological plausibility and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Deep CNNs increase parameters, reducing their advantage over fully connected NNs. The paper aims to propose a more efficient model for spatio-temporal feature learning in videos.

Method: 3DPyraNet uses a new weighting scheme to learn features from spatial and temporal dimensions, analyzing multiple frames. 3DPyraNet-F fuses feature maps into a vector for SVM-based action and scene recognition.

Result: 3DPyraNet shows promising results in real-world environments, especially with camera motion. 3DPyraNet-F outperforms state-of-the-art on three benchmarks and matches performance on a fourth.

Conclusion: The proposed 3DPyraNet and 3DPyraNet-F offer efficient, biologically plausible solutions for spatio-temporal feature learning, demonstrating superior performance in action and scene recognition tasks.

Abstract: Convolutional neural network (CNN) slides a kernel over the whole image to
produce an output map. This kernel scheme reduces the number of parameters with
respect to a fully connected neural network (NN). While CNN has proven to be an
effective model in recognition of handwritten characters and traffic signal
sign boards, etc. recently, its deep variants have proven to be effective in
similar as well as more challenging applications like object, scene and action
recognition. Deep CNN add more layers and kernels to the classical CNN,
increasing the number of parameters, and partly reducing the main advantage of
CNN which is less parameters. In this paper, a 3D pyramidal neural network
called 3DPyraNet and a discriminative approach for spatio-temporal feature
learning based on it, called 3DPyraNet-F, are proposed. 3DPyraNet introduces a
new weighting scheme which learns features from both spatial and temporal
dimensions analyzing multiple adjacent frames and keeping a biological
plausible structure. It keeps the spatial topology of the input image and
presents fewer parameters and lower computational and memory costs compared to
both fully connected NNs and recent deep CNNs. 3DPyraNet-F extract the features
maps of the highest layer of the learned network, fuse them in a single vector,
and provide it as input in such a way to a linear-SVM classifier that enhances
the recognition of human actions and dynamic scenes from the videos.
Encouraging results are reported with 3DPyraNet in real-world environments,
especially in the presence of camera induced motion. Further, 3DPyraNet-F
clearly outperforms the state-of-the-art on three benchmark datasets and shows
comparable result for the fourth.

</details>


### [141] [MediAug: Exploring Visual Augmentation in Medical Imaging](https://arxiv.org/abs/2504.18983)
*Xuyin Qi, Zeyu Zhang, Canxuan Gang, Hao Zhang, Lei Zhang, Zhiwei Zhang, Yang Zhao*

Main category: cs.CV

TL;DR: The paper proposes MediAug, a benchmark for evaluating mix-based data augmentation methods in medical imaging, showing performance improvements across tasks and architectures.


<details>
  <summary>Details</summary>
Motivation: Addressing the domain gap between natural and medical images and the fragmented study of augmentation methods in medical imaging.

Method: A unified framework evaluating six mix-based augmentation methods (MixUp, YOCO, CropMix, CutMix, AugMix, SnapMix) with ResNet-50 and ViT-B backbones on brain tumour MRI and eye disease datasets.

Result: MixUp and SnapMix improved brain tumour classification (79.19% and 99.44% accuracy, respectively), while YOCO and CutMix improved eye disease classification (91.60% and 97.94% accuracy, respectively).

Conclusion: MediAug provides a reproducible benchmark, demonstrating the effectiveness of mix-based augmentation in medical imaging tasks.

Abstract: Data augmentation is essential in medical imaging for improving
classification accuracy, lesion detection, and organ segmentation under limited
data conditions. However, two significant challenges remain. First, a
pronounced domain gap between natural photographs and medical images can
distort critical disease features. Second, augmentation studies in medical
imaging are fragmented and limited to single tasks or architectures, leaving
the benefits of advanced mix-based strategies unclear. To address these
challenges, we propose a unified evaluation framework with six mix-based
augmentation methods integrated with both convolutional and transformer
backbones on brain tumour MRI and eye disease fundus datasets. Our
contributions are threefold. (1) We introduce MediAug, a comprehensive and
reproducible benchmark for advanced data augmentation in medical imaging. (2)
We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix
with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive
experiments that MixUp yields the greatest improvement on the brain tumor
classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the
greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the
greatest improvement on the eye disease classification task for ResNet-50 with
91.60% accuracy and CutMix yields the greatest improvement for ViT-B with
97.94% accuracy. Code will be available at
https://github.com/AIGeeksGroup/MediAug.

</details>


### [142] [VISUALCENT: Visual Human Analysis using Dynamic Centroid Representation](https://arxiv.org/abs/2504.19032)
*Niaz Ahmad, Youngmoon Lee, Guanghui Wang*

Main category: cs.CV

TL;DR: VISUALCENT is a unified framework for human pose and instance segmentation, improving generalizability and scalability with centroid-based keypoint detection and dynamic MaskCentroid for segmentation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in multi-person visual human analysis, particularly in generalizability and scalability.

Method: Uses centroid-based bottom-up keypoint detection with Keypoint Heatmap, Disk Representation, and KeyCentroid for keypoints, and MaskCentroid for segmentation.

Result: Outperforms existing methods in mAP scores and frame rate on COCO and OCHuman datasets.

Conclusion: VISUALCENT offers accurate, real-time performance for human pose and instance segmentation.

Abstract: We introduce VISUALCENT, a unified human pose and instance segmentation
framework to address generalizability and scalability limitations to multi
person visual human analysis. VISUALCENT leverages centroid based bottom up
keypoint detection paradigm and uses Keypoint Heatmap incorporating Disk
Representation and KeyCentroid to identify the optimal keypoint coordinates.
For the unified segmentation task, an explicit keypoint is defined as a dynamic
centroid called MaskCentroid to swiftly cluster pixels to specific human
instance during rapid changes in human body movement or significantly occluded
environment. Experimental results on COCO and OCHuman datasets demonstrate
VISUALCENTs accuracy and real time performance advantages, outperforming
existing methods in mAP scores and execution frame rate per second. The
implementation is available on the project page.

</details>


### [143] [Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype](https://arxiv.org/abs/2504.19074)
*Anyong Qin, Chaoqi Yuan, Qiang Li, Feng Yang, Tiecheng Song, Chenqiang Gao*

Main category: cs.CV

TL;DR: A dual-branch residual network integrates spatial and spectral features for hyperspectral image classification, addressing computational costs, few-shot limitations, and domain shifts via refined prototypes and kernel probability matching.


<details>
  <summary>Details</summary>
Motivation: CNNs for HSI classification face high computational costs, limited generalization in few-shot scenarios, and domain shifts due to sensor/environmental differences.

Method: Proposes a dual-branch residual network with parallel branches for spatial-spectral feature integration, refined prototypes via regulation, and kernel probability matching for domain alignment.

Result: Outperforms other methods on four HSI datasets, demonstrating superior performance.

Conclusion: The proposed method effectively addresses computational, few-shot, and domain-shift challenges in HSI classification.

Abstract: Convolutional neural networks (CNNs) are effective for hyperspectral image
(HSI) classification, but their 3D convolutional structures introduce high
computational costs and limited generalization in few-shot scenarios. Domain
shifts caused by sensor differences and environmental variations further hinder
cross-dataset adaptability. Metric-based few-shot learning (FSL) prototype
networks mitigate this problem, yet their performance is sensitive to prototype
quality, especially with limited samples. To overcome these challenges, a
dual-branch residual network that integrates spatial and spectral features via
parallel branches is proposed in this letter. Additionally, more robust refined
prototypes are obtained through a regulation term. Furthermore, a kernel
probability matching strategy aligns source and target domain features,
alleviating domain shift. Experiments on four publicly available HSI datasets
illustrate that the proposal achieves superior performance compared to other
methods.

</details>


### [144] [HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2504.19075)
*Qiuhui Chen, Jintao Wang, Gang Wang, Yi Hong*

Main category: cs.CV

TL;DR: HoloDx is a knowledge- and data-driven framework for Alzheimer's disease diagnosis, integrating multimodal data and domain knowledge via dynamic mechanisms, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully utilize multimodal data or incorporate dynamic domain knowledge, limiting AD diagnosis accuracy.

Method: HoloDx uses knowledge injection (gated cross-attention) and memory injection (prototypical memory attention) to align domain knowledge with clinical data.

Result: Evaluations on five AD datasets show HoloDx achieves superior accuracy and generalization compared to state-of-the-art methods.

Conclusion: HoloDx enhances interpretability, robustness, and diagnostic accuracy by effectively integrating knowledge and data.

Abstract: Accurate diagnosis of Alzheimer's disease (AD) requires effectively
integrating multimodal data and clinical expertise. However, existing methods
often struggle to fully utilize multimodal information and lack structured
mechanisms to incorporate dynamic domain knowledge. To address these
limitations, we propose HoloDx, a knowledge- and data-driven framework that
enhances AD diagnosis by aligning domain knowledge with multimodal clinical
data. HoloDx incorporates a knowledge injection module with a knowledge-aware
gated cross-attention, allowing the model to dynamically integrate
domain-specific insights from both large language models (LLMs) and clinical
expertise. Also, a memory injection module with a designed prototypical memory
attention enables the model to retain and retrieve subject-specific
information, ensuring consistency in decision-making. By jointly leveraging
these mechanisms, HoloDx enhances interpretability, improves robustness, and
effectively aligns prior knowledge with current subject data. Evaluations on
five AD datasets demonstrate that HoloDx outperforms state-of-the-art methods,
achieving superior diagnostic accuracy and strong generalization across diverse
cohorts. The source code will be released upon publication acceptance.

</details>


### [145] [ComFace: Facial Representation Learning with Synthetic Data for Comparing Faces](https://arxiv.org/abs/2405.16016)
*Yusuke Akamatsu, Terumi Umematsu, Hitoshi Imaoka, Shizuko Gomi, Hideo Tsurushima*

Main category: cs.CV

TL;DR: ComFace uses synthetic images to learn facial representations for capturing intra-personal changes, outperforming methods using real images in tasks like expression, weight, and age change estimation.


<details>
  <summary>Details</summary>
Motivation: Daily monitoring of facial changes for health and emotion tracking is valuable but limited by the difficulty of collecting real intra-personal face images.

Method: ComFace learns two feature representations (inter-personal differences and intra-personal changes) using synthetic face images to bypass real data collection challenges.

Result: ComFace achieves comparable or better performance than state-of-the-art methods trained on real images in downstream tasks like expression, weight, and age change estimation.

Conclusion: Synthetic data can effectively train facial representation models for intra-personal change detection, offering a practical solution to data collection limitations.

Abstract: Daily monitoring of intra-personal facial changes associated with health and
emotional conditions has great potential to be useful for medical, healthcare,
and emotion recognition fields. However, the approach for capturing
intra-personal facial changes is relatively unexplored due to the difficulty of
collecting temporally changing face images. In this paper, we propose a facial
representation learning method using synthetic images for comparing faces,
called ComFace, which is designed to capture intra-personal facial changes. For
effective representation learning, ComFace aims to acquire two feature
representations, i.e., inter-personal facial differences and intra-personal
facial changes. The key point of our method is the use of synthetic face images
to overcome the limitations of collecting real intra-personal face images.
Facial representations learned by ComFace are transferred to three extensive
downstream tasks for comparing faces: estimating facial expression changes,
weight changes, and age changes from two face images of the same individual.
Our ComFace, trained using only synthetic data, achieves comparable to or
better transfer performance than general pre-training and state-of-the-art
representation learning methods trained using real images.

</details>


### [146] [Learning to Drive from a World Model](https://arxiv.org/abs/2504.19077)
*Mitchell Goff, Greg Hogan, George Hotz, Armand du Parc Locmaria, Kacper Raczy, Harald Schäfer, Adeeb Shihadeh, Weixing Zhang, Yassine Yousfi*

Main category: cs.CV

TL;DR: The paper proposes an end-to-end training architecture for self-driving systems using real driving data, eliminating the need for hand-coded rules. It compares two simulation methods and evaluates their performance in simulations and real-world deployments.


<details>
  <summary>Details</summary>
Motivation: To simplify and scale self-driving systems by learning directly from human driving data, avoiding reliance on hand-coded perception and rules.

Method: Two simulation methods are used: reprojective simulation and a learned world model, both trained with real driving data to develop a driving policy.

Result: Both methods successfully train policies without hand-coded rules, evaluated in closed-loop simulations and real-world driver-assistance systems.

Conclusion: End-to-end training with real driving data is viable for self-driving systems, offering simplicity and scalability.

Abstract: Most self-driving systems rely on hand-coded perception outputs and
engineered driving rules. Learning directly from human driving data with an
end-to-end method can allow for a training architecture that is simpler and
scales well with compute and data.
  In this work, we propose an end-to-end training architecture that uses real
driving data to train a driving policy in an on-policy simulator. We show two
different methods of simulation, one with reprojective simulation and one with
a learned world model. We show that both methods can be used to train a policy
that learns driving behavior without any hand-coded driving rules. We evaluate
the performance of these policies in a closed-loop simulation and when deployed
in a real-world advanced driver-assistance system.

</details>


### [147] [A Comprehensive Survey on Machine Learning Driven Material Defect Detection](https://arxiv.org/abs/2406.07880)
*Jun Bai, Di Wu, Tristan Shelley, Peter Schubel, David Twine, John Russell, Xuesen Zeng, Ji Zhang*

Main category: cs.CV

TL;DR: A survey on ML techniques for material defect detection (MDD), categorizing methods into five types, analyzing principles, advantages, challenges, and focusing on composite materials. Future directions are also explored.


<details>
  <summary>Details</summary>
Motivation: Material defects impact product performance and safety, necessitating rapid and accurate detection. ML, especially deep learning, has become central to MDD research.

Method: Literature review categorizing ML techniques into unsupervised, supervised, semi-supervised, reinforcement, and generative learning, with analysis of principles and challenges.

Result: Systematic survey of ML-based MDD, highlighting techniques for composite materials and identifying advantages and challenges.

Conclusion: The survey consolidates ML-based MDD research, providing a foundation for future studies and practical applications.

Abstract: Material defects (MD) represent a primary challenge affecting product
performance and giving rise to safety issues in related products. The rapid and
accurate identification and localization of MD constitute crucial research
endeavors in addressing contemporary challenges associated with MD. In recent
years, propelled by the swift advancement of machine learning (ML)
technologies, particularly exemplified by deep learning, ML has swiftly emerged
as the core technology and a prominent research direction for material defect
detection (MDD). Through a comprehensive review of the latest literature, we
systematically survey the ML techniques applied in MDD into five categories:
unsupervised learning, supervised learning, semi-supervised learning,
reinforcement learning, and generative learning. We provide a detailed analysis
of the main principles and techniques used, together with the advantages and
potential challenges associated with these techniques. Furthermore, the survey
focuses on the techniques for defect detection in composite materials, which
are important types of materials enjoying increasingly wide application in
various industries such as aerospace, automotive, construction, and renewable
energy. Finally, the survey explores potential future directions in MDD
utilizing ML technologies. This survey consolidates ML-based MDD literature and
provides a foundation for future research and practice.

</details>


### [148] [MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore](https://arxiv.org/abs/2504.19080)
*Zhenkai Qin, Jiaquan Liang, Qiao Fang*

Main category: cs.CV

TL;DR: MIA-Mind is a lightweight, modular Multidimensional Interactive Attention Mechanism that jointly models spatial and channel features for improved feature recalibration. It achieves high accuracy on diverse datasets with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing attention mechanisms independently model channel and spatial features, ignoring their interdependence, which limits effectiveness.

Method: Proposes MIA-Mind, a unified cross-attentive fusion strategy for joint modeling of spatial and channel features within the MindSpore framework.

Result: Achieves accuracies of 82.9% (CIFAR-10), 78.7% (ISBI2012), and 91.9% (CIC-IDS2017), demonstrating versatility and generalization.

Conclusion: MIA-Mind is effective and lightweight; future work includes scaling to large datasets, adaptive fusion strategies, and distributed deployment.

Abstract: Attention mechanisms have significantly advanced deep learning by enhancing
feature representation through selective focus. However, existing approaches
often independently model channel importance and spatial saliency, overlooking
their inherent interdependence and limiting their effectiveness. To address
this limitation, we propose MIA-Mind, a lightweight and modular
Multidimensional Interactive Attention Mechanism, built upon the MindSpore
framework. MIA-Mind jointly models spatial and channel features through a
unified cross-attentive fusion strategy, enabling fine-grained feature
recalibration with minimal computational overhead. Extensive experiments are
conducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an
accuracy of 82.9\%; on ISBI2012, it achieves an accuracy of 78.7\%; and on
CIC-IDS2017, it achieves an accuracy of 91.9\%. These results validate the
versatility, lightweight design, and generalization ability of MIA-Mind across
heterogeneous tasks. Future work will explore the extension of MIA-Mind to
large-scale datasets, the development of ada,ptive attention fusion strategies,
and distributed deployment to further enhance scalability and robustness.

</details>


### [149] [Hierarchical Attention Diffusion Networks with Object Priors for Video Change Detection](https://arxiv.org/abs/2408.10619)
*Andrew Kiruluta, Eric Lundy, Andreas Lemos*

Main category: cs.CV

TL;DR: A unified change detection pipeline combining instance-level masking, multi-scale attention in a diffusion model, and semantic classification, refined via SSIM, outperforms existing methods by 10-25 points in F1 and IoU.


<details>
  <summary>Details</summary>
Motivation: To improve remote sensing change detection by providing detailed, interpretable multi-class maps that match human perception.

Method: Combines Mask R-CNN for isolating novel objects, hierarchical cross-attention in a diffusion model, and per-pixel semantic classification, refined using SSIM.

Result: Outperforms traditional differencing, Siamese CNNs, and GAN-based detectors by 10-25 points in F1 and IoU on synthetic and real-world benchmarks.

Conclusion: Sets a new state of the art in remote sensing change detection with detailed and interpretable results.

Abstract: We present a unified change detection pipeline that combines instance level
masking, multi\-scale attention within a denoising diffusion model, and per
pixel semantic classification, all refined via SSIM to match human perception.
By first isolating only temporally novel objects with Mask R\-CNN, then guiding
diffusion updates through hierarchical cross attention to object and global
contexts, and finally categorizing each pixel into one of C change types, our
method delivers detailed, interpretable multi\-class maps. It outperforms
traditional differencing, Siamese CNNs, and GAN\-based detectors by 10\-25
points in F1 and IoU on both synthetic and real world benchmarks, marking a new
state of the art in remote sensing change detection.

</details>


### [150] [Boosting Single-domain Generalized Object Detection via Vision-Language Knowledge Interaction](https://arxiv.org/abs/2504.19086)
*Xiaoran Xu, Jiangang Yang, Wenyue Chong, Wenhui Shi, Shichu Sun, Jing Xing, Jian Liu*

Main category: cs.CV

TL;DR: A new cross-modal feature learning method for Single-Domain Generalized Object Detection (S-DGOD) improves generalization to unseen domains by leveraging fine-grained textual and visual interactions and cross-domain proposal refining.


<details>
  <summary>Details</summary>
Motivation: Current S-DGOD methods rely on coarse-grained vision-language knowledge, limiting accurate region- and object-level feature learning across domains.

Method: Proposes Cross-modal and Region-aware Feature Interaction for fine-grained feature learning and Cross-domain Proposal Refining and Mixing for proposal alignment and diversification.

Result: Achieves state-of-the-art results (+8.8% mPC on Cityscapes-C, +7.9% mPC on DWD) over baselines.

Conclusion: The method effectively enhances generalization and localization in unseen domains.

Abstract: Single-Domain Generalized Object Detection~(S-DGOD) aims to train an object
detector on a single source domain while generalizing well to diverse unseen
target domains, making it suitable for multimedia applications that involve
various domain shifts, such as intelligent video surveillance and VR/AR
technologies. With the success of large-scale Vision-Language Models, recent
S-DGOD approaches exploit pre-trained vision-language knowledge to guide
invariant feature learning across visual domains. However, the utilized
knowledge remains at a coarse-grained level~(e.g., the textual description of
adverse weather paired with the image) and serves as an implicit regularization
for guidance, struggling to learn accurate region- and object-level features in
varying domains. In this work, we propose a new cross-modal feature learning
method, which can capture generalized and discriminative regional features for
S-DGOD tasks. The core of our method is the mechanism of Cross-modal and
Region-aware Feature Interaction, which simultaneously learns both inter-modal
and intra-modal regional invariance through dynamic interactions between
fine-grained textual and visual features. Moreover, we design a simple but
effective strategy called Cross-domain Proposal Refining and Mixing, which
aligns the position of region proposals across multiple domains and diversifies
them, enhancing the localization ability of detectors in unseen scenarios. Our
method achieves new state-of-the-art results on S-DGOD benchmark datasets, with
improvements of +8.8\%~mPC on Cityscapes-C and +7.9\%~mPC on DWD over
baselines, demonstrating its efficacy.

</details>


### [151] [Towards Latency-Aware 3D Streaming Perception for Autonomous Driving](https://arxiv.org/abs/2504.19115)
*Jiaqi Peng, Tai Wang, Jiangmiao Pang, Yuan Shen*

Main category: cs.CV

TL;DR: A new benchmark and framework (LASP) address runtime latency in 3D perception on edge devices, improving online performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D perception algorithms face deployment challenges due to high runtime latency on edge devices.

Method: Proposes LASP with latency-aware history integration and predictive detection to handle varying latency.

Result: Achieves online performance close to 80% of offline evaluation on Jetson AGX Orin without acceleration.

Conclusion: LASP effectively generalizes across latency levels, enhancing real-time 3D perception.

Abstract: Although existing 3D perception algorithms have demonstrated significant
improvements in performance, their deployment on edge devices continues to
encounter critical challenges due to substantial runtime latency. We propose a
new benchmark tailored for online evaluation by considering runtime latency.
Based on the benchmark, we build a Latency-Aware 3D Streaming Perception (LASP)
framework that addresses the latency issue through two primary components: 1)
latency-aware history integration, which extends query propagation into a
continuous process, ensuring the integration of historical feature regardless
of varying latency; 2) latency-aware predictive detection, a module that
compensates the detection results with the predicted trajectory and the
posterior accessed latency. By incorporating the latency-aware mechanism, our
method shows generalization across various latency levels, achieving an online
performance that closely aligns with 80\% of its offline evaluation on the
Jetson AGX Orin without any acceleration techniques.

</details>


### [152] [Blind Source Separation Based on Sparsity](https://arxiv.org/abs/2504.19124)
*Zhongxuan Li*

Main category: cs.CV

TL;DR: The paper reviews ICA and sparsity-based methods for BSS, introduces MCA and its variants, and proposes an improved algorithm (SAC+BK-SVD) for better blind image separation.


<details>
  <summary>Details</summary>
Motivation: To address ICA's limitations by leveraging sparsity-based methods and improving dictionary learning for BSS.

Method: Reviews ICA and sparsity-based methods, introduces MCA and variants (MMCA, GMCA), and proposes SAC+BK-SVD for block-sparse dictionary learning.

Result: Simulations show SAC+BK-SVD improves blind image separation quality compared to K-SVD.

Conclusion: Sparsity-based methods, especially SAC+BK-SVD, enhance BSS performance, offering promising results for applications like image segmentation.

Abstract: Blind source separation (BSS) is a key technique in array processing and data
analysis, aiming to recover unknown sources from observed mixtures without
knowledge of the mixing matrix. Classical independent component analysis (ICA)
methods rely on the assumption that sources are mutually independent. To
address limitations of ICA, sparsity-based methods have been introduced, which
decompose source signals sparsely in a predefined dictionary. Morphological
Component Analysis (MCA), based on sparse representation theory, assumes that a
signal is a linear combination of components with distinct geometries, each
sparsely representable in one dictionary and not in others. This approach has
recently been applied to BSS with promising results.
  This report reviews key approaches derived from classical ICA and explores
sparsity-based methods for BSS. It introduces the theory of sparse
representation and decomposition, followed by a block coordinate relaxation MCA
algorithm, whose variants are used in Multichannel MCA (MMCA) and Generalized
MCA (GMCA). A local dictionary learning method using K-SVD is then presented.
Finally, we propose an improved algorithm, SAC+BK-SVD, which enhances K-SVD by
learning a block-sparsifying dictionary that clusters and updates similar atoms
in blocks.
  The implementation includes experiments on image segmentation and blind image
source separation using the discussed techniques. We also compare the proposed
block-sparse dictionary learning algorithm with K-SVD. Simulation results
demonstrate that our method yields improved blind image separation quality.

</details>


### [153] [RadioFormer: A Multiple-Granularity Radio Map Estimation Transformer with 1\textpertenthousand Spatial Sampling](https://arxiv.org/abs/2504.19161)
*Zheng Fang, Kangjun Liu, Ke Chen, Qingyu Liu, Jianguo Zhang, Lingyang Song, Yaowei Wang*

Main category: cs.CV

TL;DR: RadioFormer, a transformer-based model, improves radio map estimation with sparse spatial observations by leveraging dual-stream self-attention and cross-attention modules.


<details>
  <summary>Details</summary>
Motivation: Existing deep vision models require sufficient spatial observations (0.01%-1% of pixels), which is impractical in real-world sparse sampling scenarios.

Method: RadioFormer uses a dual-stream self-attention (DSA) module for pixel-wise and patch-wise correlations and a cross-attention (CCA) module for multi-scale integration.

Result: Outperforms state-of-the-art methods on the RadioMapSeer dataset with lower computational cost and strong zero-shot generalization.

Conclusion: RadioFormer advances radio map estimation in practical sparse observation settings.

Abstract: The task of radio map estimation aims to generate a dense representation of
electromagnetic spectrum quantities, such as the received signal strength at
each grid point within a geographic region, based on measurements from a subset
of spatially distributed nodes (represented as pixels). Recently, deep vision
models such as the U-Net have been adapted to radio map estimation, whose
effectiveness can be guaranteed with sufficient spatial observations (typically
0.01% to 1% of pixels) in each map, to model local dependency of observed
signal power. However, such a setting of sufficient measurements can be less
practical in real-world scenarios, where extreme sparsity in spatial sampling
can be widely encountered. To address this challenge, we propose RadioFormer, a
novel multiple-granularity transformer designed to handle the constraints posed
by spatial sparse observations. Our RadioFormer, through a dual-stream
self-attention (DSA) module, can respectively discover the correlation of
pixel-wise observed signal power and also learn patch-wise buildings'
geometries in a style of multiple granularities, which are integrated into
multi-scale representations of radio maps by a cross stream cross-attention
(CCA) module. Extensive experiments on the public RadioMapSeer dataset
demonstrate that RadioFormer outperforms state-of-the-art methods in radio map
estimation while maintaining the lowest computational cost. Furthermore, the
proposed approach exhibits exceptional generalization capabilities and robust
zero-shot performance, underscoring its potential to advance radio map
estimation in a more practical setting with very limited observation nodes.

</details>


### [154] [IM-Portrait: Learning 3D-aware Video Diffusion for PhotorealisticTalking Heads from Monocular Videos](https://arxiv.org/abs/2504.19165)
*Yuan Li, Ziqian Bai, Feitong Tan, Zhaopeng Cui, Sean Fanello, Yinda Zhang*

Main category: cs.CV

TL;DR: A 3D-aware diffusion method generates photorealistic talking head videos from a single image and control signals, using Multiplane Images (MPIs) for geometric consistency without post-processing.


<details>
  <summary>Details</summary>
Motivation: To create immersive talking head videos efficiently without relying on explicit 3D reconstruction or multi-view data.

Method: Generates MPIs directly via a single denoising process, with a training mechanism alternating between target and reference camera spaces.

Result: Achieves competitive avatar quality and novel-view rendering without explicit 3D reconstruction.

Conclusion: The method is effective for generating realistic talking heads with geometric consistency and efficiency.

Abstract: We propose a novel 3D-aware diffusion-based method for generating
photorealistic talking head videos directly from a single identity image and
explicit control signals (e.g., expressions). Our method generates Multiplane
Images (MPIs) that ensure geometric consistency, making them ideal for
immersive viewing experiences like binocular videos for VR headsets. Unlike
existing methods that often require a separate stage or joint optimization to
reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach
directly generates the final output through a single denoising process,
eliminating the need for post-processing steps to render novel views
efficiently. To effectively learn from monocular videos, we introduce a
training mechanism that reconstructs the output MPI randomly in either the
target or the reference camera space. This approach enables the model to
simultaneously learn sharp image details and underlying 3D information.
Extensive experiments demonstrate the effectiveness of our method, which
achieves competitive avatar quality and novel-view rendering capabilities, even
without explicit 3D reconstruction or high-quality multi-view training data.

</details>


### [155] [Segmenting Objectiveness and Task-awareness Unknown Region for Autonomous Driving](https://arxiv.org/abs/2504.19183)
*Mi Zheng, Guanglei Yang, Zitong Huang, Zhenhua Guo, Kevin Han, Wangmeng Zuo*

Main category: cs.CV

TL;DR: SOTA framework improves OOD detection in road scenes by focusing on objectiveness and task-awareness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current road scene segmentation lacks robustness for OOD objects due to closed-set training and inadequate anomaly detection methods.

Method: SOTA uses Semantic Fusion Block (SFB) for objectiveness segmentation and Scene-understanding Guided Prompt-Context Adaptor (SG-PCA) to filter irrelevant anomalies.

Result: SOTA achieves superior OOD detection performance on benchmarks like Fishyscapes and RoadAnomaly.

Conclusion: SOTA enhances segmentation accuracy and robustness for autonomous driving by addressing objectiveness and task-awareness.

Abstract: With the emergence of transformer-based architectures and large language
models (LLMs), the accuracy of road scene perception has substantially
advanced. Nonetheless, current road scene segmentation approaches are
predominantly trained on closed-set data, resulting in insufficient detection
capabilities for out-of-distribution (OOD) objects. To overcome this
limitation, road anomaly detection methods have been proposed. However,
existing methods primarily depend on image inpainting and OOD distribution
detection techniques, facing two critical issues: (1) inadequate consideration
of the objectiveness attributes of anomalous regions, causing incomplete
segmentation when anomalous objects share similarities with known classes, and
(2) insufficient attention to environmental constraints, leading to the
detection of anomalies irrelevant to autonomous driving tasks. In this paper,
we propose a novel framework termed Segmenting Objectiveness and Task-Awareness
(SOTA) for autonomous driving scenes. Specifically, SOTA enhances the
segmentation of objectiveness through a Semantic Fusion Block (SFB) and filters
anomalies irrelevant to road navigation tasks using a Scene-understanding
Guided Prompt-Context Adaptor (SG-PCA). Extensive empirical evaluations on
multiple benchmark datasets, including Fishyscapes Lost and Found,
Segment-Me-If-You-Can, and RoadAnomaly, demonstrate that the proposed SOTA
consistently improves OOD detection performance across diverse detectors,
achieving robust and accurate segmentation outcomes.

</details>


### [156] [LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition](https://arxiv.org/abs/2504.19186)
*Zhangshuo Qi, Luqi Cheng, Zijie Zhou, Guangming Xiong*

Main category: cs.CV

TL;DR: LRFusionPR improves place recognition in GPS-denied environments by fusing LiDAR and radar data, using a dual-branch network for cross-modality feature interaction and robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of noisy radar data and heterogeneous configurations in LiDAR-radar fusion for place recognition.

Method: Proposes a dual-branch network for fusing LiDAR and radar data in a unified BEV representation, using cross-attention and knowledge distillation.

Result: Achieves accurate place recognition and robustness under varying weather conditions, validated on multiple datasets.

Conclusion: LRFusionPR effectively leverages LiDAR-radar fusion, enhancing recognition accuracy and robustness, with open-source code available.

Abstract: In autonomous driving, place recognition is critical for global localization
in GPS-denied environments. LiDAR and radar-based place recognition methods
have garnered increasing attention, as LiDAR provides precise ranging, whereas
radar excels in adverse weather resilience. However, effectively leveraging
LiDAR-radar fusion for place recognition remains challenging. The noisy and
sparse nature of radar data limits its potential to further improve recognition
accuracy. In addition, heterogeneous radar configurations complicate the
development of unified cross-modality fusion frameworks. In this paper, we
propose LRFusionPR, which improves recognition accuracy and robustness by
fusing LiDAR with either single-chip or scanning radar. Technically, a
dual-branch network is proposed to fuse different modalities within the unified
polar coordinate bird's eye view (BEV) representation. In the fusion branch,
cross-attention is utilized to perform cross-modality feature interactions. The
knowledge from the fusion branch is simultaneously transferred to the
distillation branch, which takes radar as its only input to further improve the
robustness. Ultimately, the descriptors from both branches are concatenated,
producing the multimodal global descriptor for place retrieval. Extensive
evaluations on multiple datasets demonstrate that our LRFusionPR achieves
accurate place recognition, while maintaining robustness under varying weather
conditions. Our open-source code will be released at
https://github.com/QiZS-BIT/LRFusionPR.

</details>


### [157] [FlexPara: Flexible Neural Surface Parameterization](https://arxiv.org/abs/2504.19210)
*Yuming Zhao, Qijian Zhang, Junhui Hou, Jiazhi Xia, Wenping Wang, Ying He*

Main category: cs.CV

TL;DR: FlexPara is an unsupervised neural framework for flexible and controllable surface parameterization, enabling global and multi-chart mappings without manual cutting seams.


<details>
  <summary>Details</summary>
Motivation: Conventional parameterization methods require high-quality meshes and are limited to simple topologies, lacking flexibility for varied surface structures and tasks.

Method: FlexPara uses geometrically-interpretable sub-networks for cutting, deforming, unwrapping, and wrapping, forming a bi-directional cycle mapping framework. It also adaptively learns chart assignments for multi-chart parameterization.

Result: Experiments show FlexPara's universality and superiority in neural surface parameterization.

Conclusion: FlexPara offers a flexible, unsupervised solution for surface parameterization, with potential for broader applications.

Abstract: Surface parameterization is a fundamental geometry processing task, laying
the foundations for the visual presentation of 3D assets and numerous
downstream shape analysis scenarios. Conventional parameterization approaches
demand high-quality mesh triangulation and are restricted to certain simple
topologies unless additional surface cutting and decomposition are provided. In
practice, the optimal configurations (e.g., type of parameterization domains,
distribution of cutting seams, number of mapping charts) may vary drastically
with different surface structures and task characteristics, thus requiring more
flexible and controllable processing pipelines. To this end, this paper
introduces FlexPara, an unsupervised neural optimization framework to achieve
both global and multi-chart surface parameterizations by establishing
point-wise mappings between 3D surface points and adaptively-deformed 2D UV
coordinates. We ingeniously design and combine a series of
geometrically-interpretable sub-networks, with specific functionalities of
cutting, deforming, unwrapping, and wrapping, to construct a bi-directional
cycle mapping framework for global parameterization without the need for
manually specified cutting seams. Furthermore, we construct a multi-chart
parameterization framework with adaptively-learned chart assignment. Extensive
experiments demonstrate the universality, superiority, and inspiring potential
of our neural surface parameterization paradigm. The code will be publicly
available at https://github.com/AidenZhao/FlexPara

</details>


### [158] [CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes](https://arxiv.org/abs/2504.19212)
*Tuan Nguyen, Naseem Khan, Issa Khalil*

Main category: cs.CV

TL;DR: CapsFake, a multimodal capsule network, detects deepfake image edits by integrating visual, textual, and frequency-domain features, outperforming existing methods by up to 20% in accuracy.


<details>
  <summary>Details</summary>
Motivation: The rise of subtle, context-aware deepfake image manipulations challenges current detection systems, necessitating advanced solutions.

Method: CapsFake uses low-level capsules from multiple modalities and high-level capsules with competitive routing to identify manipulated regions.

Result: CapsFake achieves over 94% detection accuracy under natural perturbations and 96% against adversarial attacks, with strong generalization.

Conclusion: CapsFake provides a robust framework for detecting sophisticated deepfake image edits, addressing current defense limitations.

Abstract: The rapid evolution of deepfake technology, particularly in
instruction-guided image editing, threatens the integrity of digital images by
enabling subtle, context-aware manipulations. Generated conditionally from real
images and textual prompts, these edits are often imperceptible to both humans
and existing detection systems, revealing significant limitations in current
defenses. We propose a novel multimodal capsule network, CapsFake, designed to
detect such deepfake image edits by integrating low-level capsules from visual,
textual, and frequency-domain modalities. High-level capsules, predicted
through a competitive routing mechanism, dynamically aggregate local features
to identify manipulated regions with precision. Evaluated on diverse datasets,
including MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits,
CapsFake outperforms state-of-the-art methods by up to 20% in detection
accuracy. Ablation studies validate its robustness, achieving detection rates
above 94% under natural perturbations and 96% against adversarial attacks, with
excellent generalization to unseen editing scenarios. This approach establishes
a powerful framework for countering sophisticated image manipulations.

</details>


### [159] [CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis](https://arxiv.org/abs/2504.19223)
*Alexander Baumann, Leonardo Ayala, Silvia Seidlitz, Jan Sellner, Alexander Studier-Fischer, Berkin Özdemir, Lena Maier-Hein, Slobodan Ilic*

Main category: cs.CV

TL;DR: CARL introduces a camera-agnostic model for spectral imaging, addressing variability in channel dimensionality and wavelengths to improve AI-driven methodologies.


<details>
  <summary>Details</summary>
Motivation: Variability in spectral cameras limits AI model generalizability and cross-camera applicability, necessitating a unified solution.

Method: CARL uses wavelength positional encoding and a self-attention-cross-attention mechanism to create camera-agnostic embeddings, with spectral-spatial pre-training via a JEPA-inspired strategy.

Result: CARL outperforms in robustness to spectral heterogeneity across medical, autonomous driving, and satellite imaging datasets.

Conclusion: CARL's scalability and versatility make it a potential backbone for future spectral foundation models.

Abstract: Spectral imaging offers promising applications across diverse domains,
including medicine and urban scene understanding, and is already established as
a critical modality in remote sensing. However, variability in channel
dimensionality and captured wavelengths among spectral cameras impede the
development of AI-driven methodologies, leading to camera-specific models with
limited generalizability and inadequate cross-camera applicability. To address
this bottleneck, we introduce $\textbf{CARL}$, a model for
$\textbf{C}$amera-$\textbf{A}$gnostic $\textbf{R}$epresentation
$\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging
modalities. To enable the conversion of a spectral image with any channel
dimensionality to a camera-agnostic embedding, we introduce wavelength
positional encoding and a self-attention-cross-attention mechanism to compress
spectral information into learned query representations. Spectral-spatial
pre-training is achieved with a novel spectral self-supervised JEPA-inspired
strategy tailored to CARL. Large-scale experiments across the domains of
medical imaging, autonomous driving, and satellite imaging demonstrate our
model's unique robustness to spectral heterogeneity, outperforming on datasets
with simulated and real-world cross-camera spectral variations. The scalability
and versatility of the proposed approach position our model as a backbone for
future spectral foundation models.

</details>


### [160] [Unsupervised 2D-3D lifting of non-rigid objects using local constraints](https://arxiv.org/abs/2504.19227)
*Shalini Maiti, Lourdes Agapito, Benjamin Graham*

Main category: cs.CV

TL;DR: High-capacity models with unsupervised loss outperform specialized low-rank models for 3D shape prediction from 2D keypoints, reducing reconstruction error by 70%.


<details>
  <summary>Details</summary>
Motivation: The challenge of predicting 3D shapes from 2D keypoints for non-rigid objects is ill-posed due to occlusions and viewpoint-shape disentanglement. Existing low-rank models are limited by alignment dependencies and reconstruction quality.

Method: Proposes generic, high-capacity models trained with an unsupervised loss, applying low-rank constraints to localized shape subsets for better accuracy.

Result: Achieves a 70% reduction in reconstruction error on the S-Up3D dataset compared to state-of-the-art methods.

Conclusion: High-capacity models with localized constraints offer superior performance for 3D shape prediction from 2D observations.

Abstract: For non-rigid objects, predicting the 3D shape from 2D keypoint observations
is ill-posed due to occlusions, and the need to disentangle changes in
viewpoint and changes in shape. This challenge has often been addressed by
embedding low-rank constraints into specialized models. These models can be
hard to train, as they depend on finding a canonical way of aligning
observations, before they can learn detailed geometry. These constraints have
limited the reconstruction quality. We show that generic, high capacity models,
trained with an unsupervised loss, allow for more accurate predicted shapes. In
particular, applying low-rank constraints to localized subsets of the full
shape allows the high capacity to be suitably constrained. We reduce the
state-of-the-art reconstruction error on the S-Up3D dataset by over 70%.

</details>


### [161] [Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID](https://arxiv.org/abs/2504.19244)
*De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao*

Main category: cs.CV

TL;DR: The paper proposes SALCR, a framework for unsupervised visible-infrared person re-identification, addressing cross-modality variations by aligning fine-grained patterns and refining pseudo-labels collaboratively.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook cross-modality variations in feature representation and pseudo-label distributions, leading to insufficient modality-shared learning.

Method: SALCR includes DAGI for bi-directional pseudo-label unification, FGSAL for part-level semantic alignment, and GPCR for dynamic refinement of noisy pseudo-labels.

Result: The method outperforms state-of-the-art approaches in experiments.

Conclusion: SALCR effectively addresses cross-modality variations and noisy pseudo-labels, achieving superior performance in unsupervised VI-ReID.

Abstract: Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to
match pedestrian images of the same individual across different modalities
without human annotations for model learning. Previous methods unify
pseudo-labels of cross-modality images through label association algorithms and
then design contrastive learning framework for global feature learning.
However, these methods overlook the cross-modality variations in feature
representation and pseudo-label distributions brought by fine-grained patterns.
This insight results in insufficient modality-shared learning when only global
features are optimized. To address this issue, we propose a Semantic-Aligned
Learning with Collaborative Refinement (SALCR) framework, which builds up
optimization objective for specific fine-grained patterns emphasized by each
modality, thereby achieving complementary alignment between the label
distributions of different modalities. Specifically, we first introduce a Dual
Association with Global Learning (DAGI) module to unify the pseudo-labels of
cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained
Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level
semantic-aligned patterns emphasized by each modality from cross-modality
instances. Optimization objective is then formulated based on the
semantic-aligned features and their corresponding label space. To alleviate the
side-effects arising from noisy pseudo-labels, we propose a Global-Part
Collaborative Refinement (GPCR) module to mine reliable positive sample sets
for the global and part features dynamically and optimize the inter-instance
relationships. Extensive experiments demonstrate the effectiveness of the
proposed method, which achieves superior performances to state-of-the-art
methods. Our code is available at
\href{https://github.com/FranklinLingfeng/code-for-SALCR}.

</details>


### [162] [ODExAI: A Comprehensive Object Detection Explainable AI Evaluation](https://arxiv.org/abs/2504.19249)
*Loc Phuc Truong Nguyen, Hung Truong Thanh Nguyen, Hung Cao*

Main category: cs.CV

TL;DR: The paper introduces ODExAI, a framework to evaluate XAI methods for object detection, highlighting trade-offs between localization, faithfulness, and computational complexity.


<details>
  <summary>Details</summary>
Motivation: The lack of standards for evaluating XAI in object detection hinders method comparison and selection.

Method: ODExAI evaluates XAI methods on three dimensions: localization accuracy, faithfulness, and computational complexity, tested on YOLOX, Faster R-CNN, MS-COCO, and PASCAL VOC.

Result: Region-based methods excel in faithfulness but are slow; CAM-based methods are fast but less faithful.

Conclusion: Task-specific evaluation is crucial for deploying XAI in object detection, with ODExAI providing a benchmark.

Abstract: Explainable Artificial Intelligence (XAI) techniques for interpreting object
detection models remain in an early stage, with no established standards for
systematic evaluation. This absence of consensus hinders both the comparative
analysis of methods and the informed selection of suitable approaches. To
address this gap, we introduce the Object Detection Explainable AI Evaluation
(ODExAI), a comprehensive framework designed to assess XAI methods in object
detection based on three core dimensions: localization accuracy, faithfulness
to model behavior, and computational complexity. We benchmark a set of XAI
methods across two widely used object detectors (YOLOX and Faster R-CNN) and
standard datasets (MS-COCO and PASCAL VOC). Empirical results demonstrate that
region-based methods (e.g., D-CLOSE) achieve strong localization (PG = 88.49%)
and high model faithfulness (OA = 0.863), though with substantial computational
overhead (Time = 71.42s). On the other hand, CAM-based methods (e.g., G-CAME)
achieve superior localization (PG = 96.13%) and significantly lower runtime
(Time = 0.54s), but at the expense of reduced faithfulness (OA = 0.549). These
findings demonstrate critical trade-offs among existing XAI approaches and
reinforce the need for task-specific evaluation when deploying them in object
detection pipelines. Our implementation and evaluation benchmarks are publicly
available at: https://github.com/Analytics-Everywhere-Lab/odexai.

</details>


### [163] [LM-MCVT: A Lightweight Multi-modal Multi-view Convolutional-Vision Transformer Approach for 3D Object Recognition](https://arxiv.org/abs/2504.19256)
*Songsong Xiong, Hamidreza Kasaei*

Main category: cs.CV

TL;DR: A novel LM-MCVT network is proposed for 3D object recognition in robotics, achieving 95.6% accuracy on ModelNet40 and robust performance on real-world data.


<details>
  <summary>Details</summary>
Motivation: Robots struggle with 3D object recognition in complex human-centered environments due to diverse object shapes and variability.

Method: The LM-MCVT network uses GEEF for multi-view fusion, combining convolutional encoders and transformers for feature extraction.

Result: Achieves 95.6% accuracy on ModelNet40 and superior performance on OmniObject3D via 5-fold cross-validation.

Conclusion: LM-MCVT is robust and outperforms state-of-the-art methods in 3D object recognition for both synthetic and real-world data.

Abstract: In human-centered environments such as restaurants, homes, and warehouses,
robots often face challenges in accurately recognizing 3D objects. These
challenges stem from the complexity and variability of these environments,
including diverse object shapes. In this paper, we propose a novel Lightweight
Multi-modal Multi-view Convolutional-Vision Transformer network (LM-MCVT) to
enhance 3D object recognition in robotic applications. Our approach leverages
the Globally Entropy-based Embeddings Fusion (GEEF) method to integrate
multi-views efficiently. The LM-MCVT architecture incorporates pre- and
mid-level convolutional encoders and local and global transformers to enhance
feature extraction and recognition accuracy. We evaluate our method on the
synthetic ModelNet40 dataset and achieve a recognition accuracy of 95.6% using
a four-view setup, surpassing existing state-of-the-art methods. To further
validate its effectiveness, we conduct 5-fold cross-validation on the
real-world OmniObject3D dataset using the same configuration. Results
consistently show superior performance, demonstrating the method's robustness
in 3D object recognition across synthetic and real-world 3D data.

</details>


### [164] [OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion](https://arxiv.org/abs/2504.19258)
*Shuhao Kang, Martin Y. Liao, Yan Xia, Olaf Wysocki, Boris Jutzi, Daniel Cremers*

Main category: cs.CV

TL;DR: OPAL is a novel LiDAR place recognition network using OpenStreetMap as a lightweight prior, outperforming existing methods in recall and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on dense maps or aerial imagery, which are storage-heavy and lack real-time adaptability.

Method: OPAL bridges LiDAR and OSM data using a cross-modal visibility mask and adaptive radial fusion module.

Result: Achieves 15.98% higher recall at @1m threshold and 12x faster inference speeds than state-of-the-art methods.

Conclusion: OPAL offers an efficient, scalable solution for LiDAR place recognition with superior performance.

Abstract: LiDAR place recognition is a critical capability for autonomous navigation
and cross-modal localization in large-scale outdoor environments. Existing
approaches predominantly depend on pre-built 3D dense maps or aerial imagery,
which impose significant storage overhead and lack real-time adaptability. In
this paper, we propose OPAL, a novel network for LiDAR place recognition that
leverages OpenStreetMap as a lightweight and up-to-date prior. Our key
innovation lies in bridging the domain disparity between sparse LiDAR scans and
structured OSM data through two carefully designed components: a cross-modal
visibility mask that identifies maximal observable regions from both modalities
to guide feature learning, and an adaptive radial fusion module that
dynamically consolidates multiscale radial features into discriminative global
descriptors. Extensive experiments on the augmented KITTI and KITTI-360
datasets demonstrate OPAL's superiority, achieving 15.98% higher recall at @1m
threshold for top-1 retrieved matches while operating at 12x faster inference
speeds compared to state-of-the-art approaches. Code and datasets are publicly
available at: https://github.com/WHU-USI3DV/OPAL .

</details>


### [165] [Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting](https://arxiv.org/abs/2504.19261)
*Xiaofeng Jin, Yan Fang, Matteo Frosi, Jianfei Ge, Jiangjian Xiao, Matteo Matteucci*

Main category: cs.CV

TL;DR: The paper introduces RF-GS, a method for scene view synthesis that improves rendering stability by using a renderability field and pseudo-view sampling, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Scene view synthesis is crucial for VR, AR, and robotics, but non-uniform observations in environments challenge rendering quality.

Method: RF-GS uses a renderability field to guide pseudo-view sampling, trains an image restoration model for wide-baseline views, and employs hybrid data optimization.

Result: The method outperforms existing approaches in rendering stability on both simulated and real-world data.

Conclusion: RF-GS effectively addresses inhomogeneity in scene view synthesis, enhancing visual consistency and rendering quality.

Abstract: Scene view synthesis, which generates novel views from limited perspectives,
is increasingly vital for applications like virtual reality, augmented reality,
and robotics. Unlike object-based tasks, such as generating 360{\deg} views of
a car, scene view synthesis handles entire environments where non-uniform
observations pose unique challenges for stable rendering quality. To address
this issue, we propose a novel approach: renderability field-guided gaussian
splatting (RF-GS). This method quantifies input inhomogeneity through a
renderability field, guiding pseudo-view sampling to enhanced visual
consistency. To ensure the quality of wide-baseline pseudo-views, we train an
image restoration model to map point projections to visible-light styles.
Additionally, our validated hybrid data optimization strategy effectively fuses
information of pseudo-view angles and source view textures. Comparative
experiments on simulated and real-world data show that our method outperforms
existing approaches in rendering stability.

</details>


### [166] [OpenFusion++: An Open-vocabulary Real-time Scene Understanding System](https://arxiv.org/abs/2504.19266)
*Xiaofeng Jin, Matteo Frosi, Matteo Matteucci*

Main category: cs.CV

TL;DR: OpenFusion++ improves real-time 3D scene understanding by refining point clouds, dynamically updating semantics, and enhancing query handling, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing imprecise instance segmentation, static semantic updates, and limited query handling in real-time 3D perception.

Method: TSDF-based system fusing confidence maps, adaptive semantic cache, and dual-path encoding for object-environment integration.

Result: Outperforms baselines in semantic accuracy and query responsiveness on ICL, Replica, ScanNet, and ScanNet++.

Conclusion: OpenFusion++ advances real-time 3D semantic-geometric reconstruction for applications like embodied intelligence and AR.

Abstract: Real-time open-vocabulary scene understanding is essential for efficient 3D
perception in applications such as vision-language navigation, embodied
intelligence, and augmented reality. However, existing methods suffer from
imprecise instance segmentation, static semantic updates, and limited handling
of complex queries. To address these issues, we present OpenFusion++, a
TSDF-based real-time 3D semantic-geometric reconstruction system. Our approach
refines 3D point clouds by fusing confidence maps from foundational models,
dynamically updates global semantic labels via an adaptive cache based on
instance area, and employs a dual-path encoding framework that integrates
object attributes with environmental context for precise query responses.
Experiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate
that OpenFusion++ significantly outperforms the baseline in both semantic
accuracy and query responsiveness.

</details>


### [167] [VI3NR: Variance Informed Initialization for Implicit Neural Representations](https://arxiv.org/abs/2504.19270)
*Chamin Hewa Koneputugodage, Yizhak Ben-Shabat, Sameera Ramasinghe, Stephen Gould*

Main category: cs.CV

TL;DR: The paper introduces a stable initialization method for Implicit Neural Representations (INRs) that works with any activation function, improving convergence and accuracy across various data modalities.


<details>
  <summary>Details</summary>
Motivation: Common neural network initializations are not suitable for many activation functions used in INRs, limiting their effectiveness. The paper aims to address this gap.

Method: The authors derive a new initialization method ensuring stable variance across layers, applicable to any activation function, and generalize previous methods.

Result: The proposed initialization improves stability and performance for INR activation functions, particularly in Gaussian INRs, enhancing tasks like image, audio, and 3D reconstruction.

Conclusion: The new initialization method is versatile, outperforms prior approaches, and is validated across multiple signal modalities.

Abstract: Implicit Neural Representations (INRs) are a versatile and powerful tool for
encoding various forms of data, including images, videos, sound, and 3D shapes.
A critical factor in the success of INRs is the initialization of the network,
which can significantly impact the convergence and accuracy of the learned
model. Unfortunately, commonly used neural network initializations are not
widely applicable for many activation functions, especially those used by INRs.
In this paper, we improve upon previous initialization methods by deriving an
initialization that has stable variance across layers, and applies to any
activation function. We show that this generalizes many previous initialization
methods, and has even better stability for well studied activations. We also
show that our initialization leads to improved results with INR activation
functions in multiple signal modalities. Our approach is particularly effective
for Gaussian INRs, where we demonstrate that the theory of our initialization
matches with task performance in multiple experiments, allowing us to achieve
improvements in image, audio, and 3D surface reconstruction.

</details>


### [168] [Leveraging Multi-Modal Saliency and Fusion for Gaze Target Detection](https://arxiv.org/abs/2504.19271)
*Athul M. Mathew, Arshad Ali Khan, Thariq Khalid, Faroq AL-Tam, Riad Souissi*

Main category: cs.CV

TL;DR: A novel method for gaze target detection (GTD) fuses 3D depth, saliency, and face modalities to outperform state-of-the-art methods on three datasets.


<details>
  <summary>Details</summary>
Motivation: GTD is challenging due to the need to understand relationships between head, body, eyes, and environment.

Method: Projects 2D images into 3D using depth estimation, extracts depth-infused saliency maps, and fuses face and depth modalities for gaze target prediction.

Result: Outperforms state-of-the-art methods on VideoAttentionTarget, GazeFollow, and GOO-Real datasets.

Conclusion: The proposed method is a promising new approach for GTD.

Abstract: Gaze target detection (GTD) is the task of predicting where a person in an
image is looking. This is a challenging task, as it requires the ability to
understand the relationship between the person's head, body, and eyes, as well
as the surrounding environment. In this paper, we propose a novel method for
GTD that fuses multiple pieces of information extracted from an image. First,
we project the 2D image into a 3D representation using monocular depth
estimation. We then extract a depth-infused saliency module map, which
highlights the most salient (\textit{attention-grabbing}) regions in image for
the subject in consideration. We also extract face and depth modalities from
the image, and finally fuse all the extracted modalities to identify the gaze
target. We quantitatively evaluated our method, including the ablation analysis
on three publicly available datasets, namely VideoAttentionTarget, GazeFollow
and GOO-Real, and showed that it outperforms other state-of-the-art methods.
This suggests that our method is a promising new approach for GTD.

</details>


### [169] [Optimal Hyperspectral Undersampling Strategy for Satellite Imaging](https://arxiv.org/abs/2504.19279)
*Vita V. Vlasova, Vladimir G. Kuzmin, Maria S. Varetsa, Natalia A. Ibragimova, Oleg Y. Rogov, Elena V. Lyapuntsova*

Main category: cs.CV

TL;DR: A novel band selection method, IWGS, is proposed for HSI classification, outperforming state-of-the-art techniques in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address challenges of high dimensionality, spectral redundancy, and limited labeled data in HSI classification.

Method: Iterative Wavelet-based Gradient Sampling (IWGS) selects informative bands via wavelet-transformed gradient analysis.

Result: Achieves up to 97.8% accuracy on benchmark datasets, outperforming existing methods.

Conclusion: IWGS is effective, efficient, and suitable for resource-constrained environments.

Abstract: Hyperspectral image (HSI) classification presents significant challenges due
to the high dimensionality, spectral redundancy, and limited labeled data
typically available in real-world applications. To address these issues and
optimize classification performance, we propose a novel band selection strategy
known as Iterative Wavelet-based Gradient Sampling (IWGS). This method
incrementally selects the most informative spectral bands by analyzing
gradients within the wavelet-transformed domain, enabling efficient and
targeted dimensionality reduction. Unlike traditional selection methods, IWGS
leverages the multi-resolution properties of wavelets to better capture subtle
spectral variations relevant for classification. The iterative nature of the
approach ensures that redundant or noisy bands are systematically excluded
while maximizing the retention of discriminative features. We conduct
comprehensive experiments on two widely-used benchmark HSI datasets: Houston
2013 and Indian Pines. Results demonstrate that IWGS consistently outperforms
state-of-the-art band selection and classification techniques in terms of both
accuracy and computational efficiency. These improvements make our method
especially suitable for deployment in edge devices or other
resource-constrained environments, where memory and processing power are
limited. In particular, IWGS achieved an overall accuracy up to 97.8% on Indian
Pines for selected classes, confirming its effectiveness and generalizability
across different HSI scenarios.

</details>


### [170] [Marine Snow Removal Using Internally Generated Pseudo Ground Truth](https://arxiv.org/abs/2504.19289)
*Alexandra Malyugina, Guoxi Huang, Eduardo Ruiz, Benjamin Leslie, Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: Proposes a framework for generating paired datasets to train models for removing marine snow from underwater videos, improving restoration without ground truth.


<details>
  <summary>Details</summary>
Motivation: Underwater videos suffer from degraded quality due to marine snow, and existing methods lack paired training data.

Method: Introduces a novel approach to create paired datasets (snowy and snow-free) from raw underwater videos for supervised training.

Result: Demonstrates effectiveness in enhancing underwater image restoration without ground truth.

Conclusion: The proposed framework addresses the lack of paired data, improving marine snow removal in underwater videos.

Abstract: Underwater videos often suffer from degraded quality due to light absorption,
scattering, and various noise sources. Among these, marine snow, which is
suspended organic particles appearing as bright spots or noise, significantly
impacts machine vision tasks, particularly those involving feature matching.
Existing methods for removing marine snow are ineffective due to the lack of
paired training data. To address this challenge, this paper proposes a novel
enhancement framework that introduces a new approach for generating paired
datasets from raw underwater videos. The resulting dataset consists of paired
images of generated snowy and snow, free underwater videos, enabling supervised
training for video enhancement. We describe the dataset creation process,
highlight its key characteristics, and demonstrate its effectiveness in
enhancing underwater image restoration in the absence of ground truth.

</details>


### [171] [FusionNet: Multi-model Linear Fusion Framework for Low-light Image Enhancement](https://arxiv.org/abs/2504.19295)
*Kangbiao Shi, Yixu Feng, Tao Hu, Yu Cao, Peng Wu, Yijin Liang, Yanning Zhang, Qingsen Yan*

Main category: cs.CV

TL;DR: FusionNet introduces a multi-model linear fusion framework for low-light image enhancement, addressing challenges like parameter explosion and feature misalignment, achieving top performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing fusion strategies for low-light image enhancement face issues like parameter explosion and optimization instability, limiting performance improvements.

Method: FusionNet uses a parallel multi-model linear fusion framework with Hilbert space guarantees to capture global and local features across color spaces.

Result: FusionNet outperforms state-of-the-art methods in benchmarks, winning the CVPR2025 NTIRE Low Light Enhancement Challenge.

Conclusion: FusionNet provides a robust solution for low-light image enhancement, addressing key challenges and delivering superior performance.

Abstract: The advent of Deep Neural Networks (DNNs) has driven remarkable progress in
low-light image enhancement (LLIE), with diverse architectures (e.g., CNNs and
Transformers) and color spaces (e.g., sRGB, HSV, HVI) yielding impressive
results. Recent efforts have sought to leverage the complementary strengths of
these paradigms, offering promising solutions to enhance performance across
varying degradation scenarios. However, existing fusion strategies are hindered
by challenges such as parameter explosion, optimization instability, and
feature misalignment, limiting further improvements. To overcome these issues,
we introduce FusionNet, a novel multi-model linear fusion framework that
operates in parallel to effectively capture global and local features across
diverse color spaces. By incorporating a linear fusion strategy underpinned by
Hilbert space theoretical guarantees, FusionNet mitigates network collapse and
reduces excessive training costs. Our method achieved 1st place in the CVPR2025
NTIRE Low Light Enhancement Challenge. Extensive experiments conducted on
synthetic and real-world benchmark datasets demonstrate that the proposed
method significantly outperforms state-of-the-art methods in terms of both
quantitative and qualitative results, delivering robust enhancement under
diverse low-light conditions.

</details>


### [172] [Myocardial Region-guided Feature Aggregation Net for Automatic Coronary artery Segmentation and Stenosis Assessment using Coronary Computed Tomography Angiography](https://arxiv.org/abs/2504.19300)
*Ni Yao, Xiangyu Liu, Danyang Sun, Chuang Han, Yanting Li, Jiaofen Nan, Chengyang Li, Fubao Zhu, Weihua Zhou, Chen Zhao*

Main category: cs.CV

TL;DR: Proposes MGFA-Net, a U-shaped dual-encoder architecture for robust coronary artery segmentation and stenosis detection in CCTA, integrating anatomical prior knowledge and achieving superior performance metrics.


<details>
  <summary>Details</summary>
Motivation: Address challenges in CAD diagnosis, such as low contrast, morphological variability, and small vessel segmentation, by leveraging anatomical prior knowledge for improved accuracy.

Method: Introduces three innovations: Myocardial Region-guided Module, Residual Feature Extraction Encoding Module, and Multi-scale Feature Fusion Module, alongside Monte Carlo dropout for uncertainty quantification and a morphology-based stenosis detection algorithm.

Result: Achieves Dice score of 85.04%, accuracy of 84.24%, HD95 of 6.1294 mm, and 5.46% improvement in true positive rate for stenosis detection compared to 3D U-Net.

Conclusion: MGFA-Net provides an automated, interpretable CAD assessment pipeline, combining deep learning with anatomical knowledge for precision medicine.

Abstract: Coronary artery disease (CAD) remains a leading cause of mortality worldwide,
requiring accurate segmentation and stenosis detection using Coronary Computed
Tomography angiography (CCTA). Existing methods struggle with challenges such
as low contrast, morphological variability and small vessel segmentation. To
address these limitations, we propose the Myocardial Region-guided Feature
Aggregation Net, a novel U-shaped dual-encoder architecture that integrates
anatomical prior knowledge to enhance robustness in coronary artery
segmentation. Our framework incorporates three key innovations: (1) a
Myocardial Region-guided Module that directs attention to coronary regions via
myocardial contour expansion and multi-scale feature fusion, (2) a Residual
Feature Extraction Encoding Module that combines parallel spatial channel
attention with residual blocks to enhance local-global feature discrimination,
and (3) a Multi-scale Feature Fusion Module for adaptive aggregation of
hierarchical vascular features. Additionally, Monte Carlo dropout f quantifies
prediction uncertainty, supporting clinical interpretability. For stenosis
detection, a morphology-based centerline extraction algorithm separates the
vascular tree into anatomical branches, enabling cross-sectional area
quantification and stenosis grading. The superiority of MGFA-Net was
demonstrated by achieving an Dice score of 85.04%, an accuracy of 84.24%, an
HD95 of 6.1294 mm, and an improvement of 5.46% in true positive rate for
stenosis detection compared to3D U-Net. The integrated segmentation-to-stenosis
pipeline provides automated, clinically interpretable CAD assessment, bridging
deep learning with anatomical prior knowledge for precision medicine. Our code
is publicly available at http://github.com/chenzhao2023/MGFA_CCTA

</details>


### [173] [Platonic Grounding for Efficient Multimodal Language Models](https://arxiv.org/abs/2504.19327)
*Moulik Choraria, Xinbo Wu, Akhil Bhimaraju, Nitesh Sekhar, Yue Wu, Xu Zhang, Prateek Singhal, Lav R. Varshney*

Main category: cs.CV

TL;DR: The paper addresses diminishing returns in Transformer-based models by proposing a simple modification for efficient multimodal learning, balancing performance and compute costs.


<details>
  <summary>Details</summary>
Motivation: The plateauing performance gains of large Transformer models, coupled with high training and inference costs, highlight the need for efficient finetuning and inference methods, especially in multimodal contexts.

Method: The authors propose a simple modification to existing multimodal frameworks, inspired by insights into implicit alignment in deeper layers of pretrained models.

Result: The approach maintains or improves baseline performance while significantly reducing training and inference compute costs.

Conclusion: The work offers a practical solution for efficient multimodal learning and has broader implications for integrating pretrained models into larger systems.

Abstract: The hyperscaling of data and parameter count in Transformer-based models is
yielding diminishing performance improvement, especially when weighed against
training costs. Such plateauing indicates the importance of methods for more
efficient finetuning and inference, while retaining similar performance. This
is especially relevant for multimodal learning paradigms, where inference costs
of processing multimodal tokens can determine the model's practical viability.
At the same time, research on representations and mechanistic interpretability
has improved our understanding of the inner workings of Transformer-based
models; one such line of work reveals an implicit alignment in the deeper
layers of pretrained models, across modalities. Taking inspiration from this,
we motivate and propose a simple modification to existing multimodal frameworks
that rely on aligning pretrained models. We demonstrate that our approach
maintains and, in some cases, even improves performance of baseline methods
while achieving significant gains in both training and inference-time compute.
Our work also has implications for combining pretrained models into larger
systems efficiently.

</details>


### [174] [Enhancing seeding efficiency using a computer vision system to monitor furrow quality in real-time](https://arxiv.org/abs/2504.19334)
*Sidharth Rai, Aryan Dalal, Riley Slichter, Ajay Sharda*

Main category: cs.CV

TL;DR: A computer vision-based method was developed to quantify row cleaner performance in precision agriculture, improving trench cleanliness assessment and seeding efficiency.


<details>
  <summary>Details</summary>
Motivation: Challenges like residue accumulation and hair pinning hinder optimal trench formation, but existing methods lack quantitative assessment of row cleaner performance.

Method: A video acquisition system captured trench conditions post-row cleaner operation. A segmentation model analyzed soil, straw, and machinery to quantify performance.

Result: The method effectively compared row cleaner performance, providing an objective assessment tool.

Conclusion: This approach enhances row cleaner selection and seeding efficiency in precision agriculture.

Abstract: Effective seed sowing in precision agriculture is hindered by challenges such
as residue accumulation, low soil temperatures, and hair pinning (crop residue
pushed in the trench by furrow opener), which obstruct optimal trench
formation. Row cleaners are employed to mitigate these issues, but there is a
lack of quantitative methods to assess trench cleanliness. In this study, a
novel computer vision-based method was developed to evaluate row cleaner
performance. Multiple air seeders were equipped with a video acquisition system
to capture trench conditions after row cleaner operation, enabling an effective
comparison of the performance of each row cleaner. The captured data were used
to develop a segmentation model that analyzed key elements such as soil, straw,
and machinery. Using the results from the segmentation model, an objective
method was developed to quantify row cleaner performance. The results
demonstrated the potential of this method to improve row cleaner selection and
enhance seeding efficiency in precision agriculture.

</details>


### [175] [Improving Small Drone Detection Through Multi-Scale Processing and Data Augmentation](https://arxiv.org/abs/2504.19347)
*Rayson Laroca, Marcelo dos Santos, David Menotti*

Main category: cs.CV

TL;DR: A drone detection method using YOLOv11 with multi-scale processing, data augmentation, and post-processing achieved top-3 in a detection challenge.


<details>
  <summary>Details</summary>
Motivation: Detecting small drones indistinguishable from birds is critical for surveillance.

Method: Uses YOLOv11 with multi-scale input processing, copy-paste data augmentation, and frame-to-frame consistency post-processing.

Result: Ranked top-3 in the WOSDETC Drone-vsBird Detection Grand Challenge.

Conclusion: The approach effectively detects drones in complex environments.

Abstract: Detecting small drones, often indistinguishable from birds, is crucial for
modern surveillance. This work introduces a drone detection methodology built
upon the medium-sized YOLOv11 object detection model. To enhance its
performance on small targets, we implemented a multi-scale approach in which
the input image is processed both as a whole and in segmented parts, with
subsequent prediction aggregation. We also utilized a copy-paste data
augmentation technique to enrich the training dataset with diverse drone and
bird examples. Finally, we implemented a post-processing technique that
leverages frame-to-frame consistency to mitigate missed detections. The
proposed approach attained a top-3 ranking in the 8th WOSDETC Drone-vsBird
Detection Grand Challenge, held at the 2025 International Joint Conference on
Neural Networks (IJCNN), showcasing its capability to detect drones in complex
environments effectively.

</details>


### [176] [MERA: Multimodal and Multiscale Self-Explanatory Model with Considerably Reduced Annotation for Lung Nodule Diagnosis](https://arxiv.org/abs/2504.19357)
*Jiahao Lu, Chong Yin, Silvia Ingala, Kenny Erleben, Michael Bachmann Nielsen, Sune Darkner*

Main category: cs.CV

TL;DR: MERA is a multimodal, multiscale self-explanatory model for lung nodule diagnosis, reducing annotation needs while providing comprehensive explanations and achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Early detection of lung cancer via pulmonary nodules is critical, but existing XAI systems lack clarity and require extensive labeled data.

Method: MERA combines unsupervised and weakly supervised learning (self-supervised learning, Vision Transformer) with hierarchical prediction and semi-supervised active learning.

Result: MERA matches or surpasses state-of-the-art accuracy with only 1% annotated data and offers multilevel explanations.

Conclusion: MERA enhances trust and transparency in diagnostic AI, demonstrating the viability of reduced annotation approaches for broader medical use.

Abstract: Lung cancer, a leading cause of cancer-related deaths globally, emphasises
the importance of early detection for better patient outcomes. Pulmonary
nodules, often early indicators of lung cancer, necessitate accurate, timely
diagnosis. Despite Explainable Artificial Intelligence (XAI) advances, many
existing systems struggle providing clear, comprehensive explanations,
especially with limited labelled data. This study introduces MERA, a Multimodal
and Multiscale self-Explanatory model designed for lung nodule diagnosis with
considerably Reduced Annotation requirements. MERA integrates unsupervised and
weakly supervised learning strategies (self-supervised learning techniques and
Vision Transformer architecture for unsupervised feature extraction) and a
hierarchical prediction mechanism leveraging sparse annotations via
semi-supervised active learning in the learned latent space. MERA explains its
decisions on multiple levels: model-level global explanations via semantic
latent space clustering, instance-level case-based explanations showing similar
instances, local visual explanations via attention maps, and concept
explanations using critical nodule attributes. Evaluations on the public LIDC
dataset show MERA's superior diagnostic accuracy and self-explainability. With
only 1% annotated samples, MERA achieves diagnostic accuracy comparable to or
exceeding state-of-the-art methods requiring full annotation. The model's
inherent design delivers comprehensive, robust, multilevel explanations aligned
closely with clinical practice, enhancing trustworthiness and transparency.
Demonstrated viability of unsupervised and weakly supervised learning lowers
the barrier to deploying diagnostic AI in broader medical domains. Our complete
code is open-source available: https://github.com/diku-dk/credanno.

</details>


### [177] [Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization](https://arxiv.org/abs/2504.19370)
*Jean-Rémy Conti, Stéphan Clémençon*

Main category: cs.CV

TL;DR: A post-processing method improves fairness in facial recognition (FR) systems by optimizing centroid-based scores, balancing fairness and global accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for fair AI systems, especially in FR, due to disparate error rates across demographic groups and regulatory pressures.

Method: A novel post-processing approach optimizing a regression loss on centroid-based scores to enhance fairness in pre-trained FR models.

Result: The method shows significant fairness improvements while maintaining global accuracy, supported by empirical evidence.

Conclusion: The proposed approach effectively addresses fairness in FR systems without compromising overall performance.

Abstract: The urging societal demand for fair AI systems has put pressure on the
research community to develop predictive models that are not only globally
accurate but also meet new fairness criteria, reflecting the lack of disparate
mistreatment with respect to sensitive attributes ($\textit{e.g.}$ gender,
ethnicity, age). In particular, the variability of the errors made by certain
Facial Recognition (FR) systems across specific segments of the population
compromises the deployment of the latter, and was judged unacceptable by
regulatory authorities. Designing fair FR systems is a very challenging
problem, mainly due to the complex and functional nature of the performance
measure used in this domain ($\textit{i.e.}$ ROC curves) and because of the
huge heterogeneity of the face image datasets usually available for training.
In this paper, we propose a novel post-processing approach to improve the
fairness of pre-trained FR models by optimizing a regression loss which acts on
centroid-based scores. Beyond the computational advantages of the method, we
present numerical experiments providing strong empirical evidence of the gain
in fairness and of the ability to preserve global accuracy.

</details>


### [178] [HumMorph: Generalized Dynamic Human Neural Fields from Few Views](https://arxiv.org/abs/2504.19390)
*Jakub Zadrożny, Hakan Bilen*

Main category: cs.CV

TL;DR: HumMorph is a method for free-viewpoint rendering of dynamic human bodies with pose control, using few observed views and robust to noisy pose estimates.


<details>
  <summary>Details</summary>
Motivation: To enable high-quality rendering of human actors in any pose from limited views, addressing practical scenarios with noisy pose estimates.

Method: Constructs a coarse canonical representation and fine-grained pixel-aligned features from observed views, using feed-forward passes for fast inference.

Result: Competitive with state-of-the-art for single-view input and superior with two views; robust to noisy pose parameters.

Conclusion: HumMorph advances generalized free-viewpoint rendering by improving visual quality and robustness in practical settings.

Abstract: We introduce HumMorph, a novel generalized approach to free-viewpoint
rendering of dynamic human bodies with explicit pose control. HumMorph renders
a human actor in any specified pose given a few observed views (starting from
just one) in arbitrary poses. Our method enables fast inference as it relies
only on feed-forward passes through the model. We first construct a coarse
representation of the actor in the canonical T-pose, which combines visual
features from individual partial observations and fills missing information
using learned prior knowledge. The coarse representation is complemented by
fine-grained pixel-aligned features extracted directly from the observed views,
which provide high-resolution appearance information. We show that HumMorph is
competitive with the state-of-the-art when only a single input view is
available, however, we achieve results with significantly better visual quality
given just 2 monocular observations. Moreover, previous generalized methods
assume access to accurate body shape and pose parameters obtained using
synchronized multi-camera setups. In contrast, we consider a more practical
scenario where these body parameters are noisily estimated directly from the
observed views. Our experimental results demonstrate that our architecture is
more robust to errors in the noisy parameters and clearly outperforms the state
of the art in this setting.

</details>


### [179] [Dynamic Arthroscopic Navigation System for Anterior Cruciate Ligament Reconstruction Based on Multi-level Memory Architecture](https://arxiv.org/abs/2504.19398)
*Shuo Wang, Weili Shi, Shuai Yang, Jiahao Cui, Qinwei Guo*

Main category: cs.CV

TL;DR: A dynamic arthroscopic navigation system for ACL surgery improves tracking accuracy and speed by integrating a multi-level memory architecture, outperforming static methods.


<details>
  <summary>Details</summary>
Motivation: To enhance surgical precision in ACL reconstruction by overcoming limitations of static image matching with a dynamic, real-time tracking system.

Method: Extends markerless navigation to dynamic video tracking using the Atkinson-Shiffrin memory model (sensory, working, long-term memory) for continuous femoral condyle tracking.

Result: Achieves 25.3 FPS with 39.5 ms latency, reducing tracking error by ~45% (5.3±1.5 pixels vs. 12.6±3.7 pixels) in long sequences.

Conclusion: The system provides stable, real-time navigation without extra hardware, significantly improving surgical precision in ACL reconstruction.

Abstract: This paper presents a dynamic arthroscopic navigation system based on
multi-level memory architecture for anterior cruciate ligament (ACL)
reconstruction surgery. The system extends our previously proposed markerless
navigation method from static image matching to dynamic video sequence
tracking. By integrating the Atkinson-Shiffrin memory model's three-level
architecture (sensory memory, working memory, and long-term memory), our system
maintains continuous tracking of the femoral condyle throughout the surgical
procedure, providing stable navigation support even in complex situations
involving viewpoint changes, instrument occlusion, and tissue deformation.
Unlike existing methods, our system operates in real-time on standard
arthroscopic equipment without requiring additional tracking hardware,
achieving 25.3 FPS with a latency of only 39.5 ms, representing a 3.5-fold
improvement over our previous static system. For extended sequences (1000
frames), the dynamic system maintained an error of 5.3 plus-minus 1.5 pixels,
compared to the static system's 12.6 plus-minus 3.7 pixels - an improvement of
approximately 45 percent. For medium-length sequences (500 frames) and short
sequences (100 frames), the system achieved approximately 35 percent and 19
percent accuracy improvements, respectively. Experimental results demonstrate
the system overcomes limitations of traditional static matching methods,
providing new technical support for improving surgical precision in ACL
reconstruction.

</details>


### [180] [Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations](https://arxiv.org/abs/2504.19402)
*Khoa Tuan Nguyen, Francesca Tozzi, Wouter Willaert, Joris Vankerschaver, Nikdokht Rashidian, Wesley De Neve*

Main category: cs.CV

TL;DR: The paper addresses disorganization and artifacts in open 3D medical shape datasets by proposing a diffusion model and implicit neural representation (INR) solution to augment liver datasets, improving diversity and model robustness.


<details>
  <summary>Details</summary>
Motivation: Existing 3D liver shape datasets are disorganized and contain artifacts, hindering robust model training for accurate 3D reconstruction tasks.

Method: Uses diffusion models combined with implicit neural representations (INRs) to generate realistic, diverse 3D liver shapes, addressing data scarcity.

Result: The method enhances dataset diversity, improving accuracy and reliability in 3D liver reconstruction and generation.

Conclusion: Diffusion models can also benefit other downstream tasks in 3D medical imaging.

Abstract: While the availability of open 3D medical shape datasets is increasing,
offering substantial benefits to the research community, we have found that
many of these datasets are, unfortunately, disorganized and contain artifacts.
These issues limit the development and training of robust models, particularly
for accurate 3D reconstruction tasks. In this paper, we examine the current
state of available 3D liver shape datasets and propose a solution using
diffusion models combined with implicit neural representations (INRs) to
augment and expand existing datasets. Our approach utilizes the generative
capabilities of diffusion models to create realistic, diverse 3D liver shapes,
capturing a wide range of anatomical variations and addressing the problem of
data scarcity. Experimental results indicate that our method enhances dataset
diversity, providing a scalable solution to improve the accuracy and
reliability of 3D liver reconstruction and generation in medical applications.
Finally, we suggest that diffusion models can also be applied to other
downstream tasks in 3D medical imaging.

</details>


### [181] [GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability](https://arxiv.org/abs/2504.19414)
*Sehyeong Jo, Gangjae Jang, Haesol Park*

Main category: cs.CV

TL;DR: GMAR improves ViT interpretability by quantifying attention head importance using gradient-based scores, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: ViT's multi-head attention lacks interpretability, with some heads being more meaningful than others, highlighting the need for better interpretability methods.

Method: Introduces GMAR, which uses gradient-based scores to quantify and normalize the importance of each attention head for clearer interpretability.

Result: GMAR consistently outperforms traditional attention rollout techniques, providing precise head-level interpretability.

Conclusion: GMAR enhances ViT interpretability, offering a robust framework for understanding transformer-based models.

Abstract: The Vision Transformer (ViT) has made significant advancements in computer
vision, utilizing self-attention mechanisms to achieve state-of-the-art
performance across various tasks, including image classification, object
detection, and segmentation. Its architectural flexibility and capabilities
have made it a preferred choice among researchers and practitioners. However,
the intricate multi-head attention mechanism of ViT presents significant
challenges to interpretability, as the underlying prediction process remains
opaque. A critical limitation arises from an observation commonly noted in
transformer architectures: "Not all attention heads are equally meaningful."
Overlooking the relative importance of specific heads highlights the
limitations of existing interpretability methods. To address these challenges,
we introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel
method that quantifies the importance of each attention head using
gradient-based scores. These scores are normalized to derive a weighted
aggregate attention score, effectively capturing the relative contributions of
individual heads. GMAR clarifies the role of each head in the prediction
process, enabling more precise interpretability at the head level. Experimental
results demonstrate that GMAR consistently outperforms traditional attention
rollout techniques. This work provides a practical contribution to
transformer-based architectures, establishing a robust framework for enhancing
the interpretability of Vision Transformer models.

</details>


### [182] [A Real-Time Event-Based Normal Flow Estimator](https://arxiv.org/abs/2504.19417)
*Dehao Yuan, Cornelia Fermüller*

Main category: cs.CV

TL;DR: The paper introduces a real-time, asynchronous, event-based normal flow estimator with an optimized implementation, reducing computational cost while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of normal flow prediction for event cameras by optimizing the computational process.

Method: Reformulates the representation step as a pooling operation, leveraging integer event coordinates to reduce time complexity.

Result: Achieves real-time performance with 4-6 million normal flows per second on GPUs, using only 1 GB of CUDA memory.

Conclusion: The optimized method enables efficient real-time normal flow prediction and is made publicly available.

Abstract: This paper presents a real-time, asynchronous, event-based normal flow
estimator. It follows the same algorithm as Learning Normal Flow Directly From
Event Neighborhoods, but with a more optimized implementation. The original
method treats event slices as 3D point clouds, encodes each event's local
geometry into a fixed-length vector, and uses a multi-layer perceptron to
predict normal flow. It constructs representations by multiplying an adjacency
matrix with a feature matrix, resulting in quadratic time complexity with
respect to the number of events. In contrast, we leverage the fact that event
coordinates are integers and reformulate the representation step as a pooling
operation. This achieves the same effect as the adjacency matrix but with much
lower computational cost. As a result, our method supports real-time normal
flow prediction on event cameras. Our estimator uses 1 GB of CUDA memory and
runs at 4 million normal flows per second on an RTX 3070, or 6 million per
second on an RTX A5000. We release the CUDA implementation along with a Python
interface at https://github.com/dhyuan99/VecKM_flow_cpp.

</details>


### [183] [EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation](https://arxiv.org/abs/2504.19432)
*Zhe Dong, Yuzhe Sun, Tianzhu Liu, Wangmeng Zuo, Yanfeng Gu*

Main category: cs.CV

TL;DR: EarthMapper is an autoregressive framework for bidirectional satellite-map translation, addressing alignment and abstraction challenges with novel mechanisms and a large dataset.


<details>
  <summary>Details</summary>
Motivation: The bidirectional translation between satellite images and maps (BSMT) is valuable for urban planning and disaster response but faces challenges like misalignment and the need for high-level abstraction and visual synthesis.

Method: EarthMapper uses geographic coordinate embeddings, multi-scale feature alignment (GJSA), semantic infusion (SI), and key point adaptive guidance (KPAG) for controllable translation.

Result: EarthMapper outperforms state-of-the-art methods in visual realism, semantic consistency, and structural fidelity, and excels in zero-shot tasks.

Conclusion: EarthMapper is a versatile and effective solution for BSMT, demonstrated by its performance on the CNSatMap and New York datasets.

Abstract: Satellite imagery and maps, as two fundamental data modalities in remote
sensing, offer direct observations of the Earth's surface and
human-interpretable geographic abstractions, respectively. The task of
bidirectional translation between satellite images and maps (BSMT) holds
significant potential for applications in urban planning and disaster response.
However, this task presents two major challenges: first, the absence of precise
pixel-wise alignment between the two modalities substantially complicates the
translation process; second, it requires achieving both high-level abstraction
of geographic features and high-quality visual synthesis, which further
elevates the technical complexity. To address these limitations, we introduce
EarthMapper, a novel autoregressive framework for controllable bidirectional
satellite-map translation. EarthMapper employs geographic coordinate embeddings
to anchor generation, ensuring region-specific adaptability, and leverages
multi-scale feature alignment within a geo-conditioned joint scale
autoregression (GJSA) process to unify bidirectional translation in a single
training cycle. A semantic infusion (SI) mechanism is introduced to enhance
feature-level consistency, while a key point adaptive guidance (KPAG) mechanism
is proposed to dynamically balance diversity and precision during inference. We
further contribute CNSatMap, a large-scale dataset comprising 302,132 precisely
aligned satellite-map pairs across 38 Chinese cities, enabling robust
benchmarking. Extensive experiments on CNSatMap and the New York dataset
demonstrate EarthMapper's superior performance, achieving significant
improvements in visual realism, semantic consistency, and structural fidelity
over state-of-the-art methods. Additionally, EarthMapper excels in zero-shot
tasks like in-painting, out-painting and coordinate-conditional generation,
underscoring its versatility.

</details>


### [184] [CLIP-KOA: Enhancing Knee Osteoarthritis Diagnosis with Multi-Modal Learning and Symmetry-Aware Loss Functions](https://arxiv.org/abs/2504.19443)
*Yejin Jeong, Donghun Lee*

Main category: cs.CV

TL;DR: The paper proposes CLIP-KOA, a deep learning framework using image and text integration to improve consistency and accuracy in knee osteoarthritis (KOA) severity prediction, achieving 71.86% accuracy.


<details>
  <summary>Details</summary>
Motivation: Current KOA severity assessment methods like the KL grading system suffer from subjectivity and variability, prompting the need for automated, reliable diagnostic tools.

Method: The CLIP-KOA framework integrates image and text data, employing Symmetry Loss and Consistency Loss to ensure prediction consistency between original and flipped images.

Result: CLIP-KOA achieves 71.86% accuracy, outperforming the standard CLIP model by 2.36%, demonstrating improved reliability in KOA severity prediction.

Conclusion: The study highlights the potential of multimodal deep learning for enhancing fine-grained medical diagnosis and reliability in medical image analysis.

Abstract: Knee osteoarthritis (KOA) is a universal chronic musculoskeletal disorders
worldwide, making early diagnosis crucial. Currently, the Kellgren and Lawrence
(KL) grading system is widely used to assess KOA severity. However, its high
inter-observer variability and subjectivity hinder diagnostic consistency. To
address these limitations, automated diagnostic techniques using deep learning
have been actively explored in recent years. In this study, we propose a
CLIP-based framework (CLIP-KOA) to enhance the consistency and reliability of
KOA grade prediction. To achieve this, we introduce a learning approach that
integrates image and text information and incorporate Symmetry Loss and
Consistency Loss to ensure prediction consistency between the original and
flipped images. CLIP-KOA achieves state-of-the-art accuracy of 71.86\% on KOA
severity prediction task, and ablation studies show that CLIP-KOA has 2.36\%
improvement in accuracy over the standard CLIP model due to our contribution.
This study shows a novel direction for data-driven medical prediction not only
to improve reliability of fine-grained diagnosis and but also to explore
multimodal methods for medical image analysis. Our code is available at
https://github.com/anonymized-link.

</details>


### [185] [Masked Language Prompting for Generative Data Augmentation in Few-shot Fashion Style Recognition](https://arxiv.org/abs/2504.19455)
*Yuki Hirakawa, Ryotaro Shimizu*

Main category: cs.CV

TL;DR: The paper proposes Masked Language Prompting (MLP) for style-consistent and diverse image generation in fashion style recognition, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in fashion style recognition due to subjectivity and ambiguity of style concepts, and limitations of existing generative data augmentation methods.

Method: Uses MLP, a prompting strategy masking words in captions and leveraging large language models for diverse yet coherent completions.

Result: MLP-based augmentation outperforms class-name and caption-based baselines on the FashionStyle14 dataset.

Conclusion: MLP is effective for fashion style recognition under limited supervision, balancing diversity and style consistency.

Abstract: Constructing dataset for fashion style recognition is challenging due to the
inherent subjectivity and ambiguity of style concepts. Recent advances in
text-to-image models have facilitated generative data augmentation by
synthesizing images from labeled data, yet existing methods based solely on
class names or reference captions often fail to balance visual diversity and
style consistency. In this work, we propose \textbf{Masked Language Prompting
(MLP)}, a novel prompting strategy that masks selected words in a reference
caption and leverages large language models to generate diverse yet
semantically coherent completions. This approach preserves the structural
semantics of the original caption while introducing attribute-level variations
aligned with the intended style, enabling style-consistent and diverse image
generation without fine-tuning. Experimental results on the FashionStyle14
dataset demonstrate that our MLP-based augmentation consistently outperforms
class-name and caption-based baselines, validating its effectiveness for
fashion style recognition under limited supervision.

</details>


### [186] [Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/abs/2504.19475)
*Sonia Joseph, Praneet Suresh, Lorenz Hufe, Edward Stevinson, Robert Graham, Yash Vadi, Danilo Bzdok, Sebastian Lapuschkin, Lee Sharkey, Blake Aaron Richards*

Main category: cs.CV

TL;DR: Prisma is an open-source framework for vision mechanistic interpretability, offering tools, pre-trained weights, and educational resources to advance research in understanding vision models.


<details>
  <summary>Details</summary>
Motivation: Progress in vision mechanistic interpretability has lagged due to lack of accessible frameworks and pre-trained models. Prisma aims to bridge this gap.

Method: Prisma provides a toolkit for accessing 75+ vision transformers, supports SAE/transcoder/crosscoder training, offers pre-trained weights, and includes analysis/visualization tools.

Result: Findings include lower sparsity in vision SAEs than language SAEs and cases where SAE reconstructions reduce model loss.

Conclusion: Prisma lowers barriers to vision interpretability research and opens new directions for understanding vision model internals.

Abstract: Robust tooling and publicly available pre-trained models have helped drive
recent advances in mechanistic interpretability for language models. However,
similar progress in vision mechanistic interpretability has been hindered by
the lack of accessible frameworks and pre-trained weights. We present Prisma
(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an
open-source framework designed to accelerate vision mechanistic
interpretability research, providing a unified toolkit for accessing 75+ vision
and video transformers; support for sparse autoencoder (SAE), transcoder, and
crosscoder training; a suite of 80+ pre-trained SAE weights; activation
caching, circuit analysis tools, and visualization tools; and educational
resources. Our analysis reveals surprising findings, including that effective
vision SAEs can exhibit substantially lower sparsity patterns than language
SAEs, and that in some instances, SAE reconstructions can decrease model loss.
Prisma enables new research directions for understanding vision model internals
while lowering barriers to entry in this emerging field.

</details>


### [187] [CasaGPT: Cuboid Arrangement and Scene Assembly for Interior Design](https://arxiv.org/abs/2504.19478)
*Weitao Feng, Hang Zhou, Jing Liao, Li Cheng, Wenbo Zhou*

Main category: cs.CV

TL;DR: A novel method, CasaGPT, uses cuboids for indoor scene synthesis, reducing object intersections and improving realism.


<details>
  <summary>Details</summary>
Motivation: To improve 3D scene synthesis by replacing bounding boxes with cuboids for better object representation and fewer intersections.

Method: Uses an autoregressive model to arrange cuboids sequentially and applies rejection sampling to filter out scenes with collisions.

Result: Outperforms state-of-the-art methods, enhancing scene realism and reducing intersections.

Conclusion: CasaGPT offers a promising direction for 3D scene synthesis with improved quality and realism.

Abstract: We present a novel approach for indoor scene synthesis, which learns to
arrange decomposed cuboid primitives to represent 3D objects within a scene.
Unlike conventional methods that use bounding boxes to determine the placement
and scale of 3D objects, our approach leverages cuboids as a straightforward
yet highly effective alternative for modeling objects. This allows for compact
scene generation while minimizing object intersections. Our approach, coined
CasaGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive
model to sequentially arrange cuboids, producing physically plausible scenes.
By applying rejection sampling during the fine-tuning stage to filter out
scenes with object collisions, our model further reduces intersections and
enhances scene quality. Additionally, we introduce a refined dataset,
3DFRONT-NC, which eliminates significant noise presented in the original
dataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our
dataset demonstrate that our approach consistently outperforms the
state-of-the-art methods, enhancing the realism of generated scenes, and
providing a promising direction for 3D scene synthesis.

</details>


### [188] [Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2504.19500)
*Yan Wang, Baoxiong Jia, Ziyu Zhu, Siyuan Huang*

Main category: cs.CV

TL;DR: MPEC introduces a Masked Point-Entity Contrastive learning method for open-vocabulary 3D semantic segmentation, achieving state-of-the-art results on ScanNet and demonstrating strong zero-shot capabilities.


<details>
  <summary>Details</summary>
Motivation: Enhancing physical intelligence by enabling embodied agents to interpret and interact dynamically in real-world environments through open-vocabulary 3D scene understanding.

Method: MPEC leverages 3D entity-language alignment and point-entity consistency across point cloud views to foster entity-specific feature representations.

Result: Achieves state-of-the-art results on ScanNet for open-vocabulary 3D semantic segmentation and shows superior zero-shot scene understanding. Fine-tuning on 8 datasets demonstrates consistent performance gains.

Conclusion: MPEC's approach drives advancements in 3D scene understanding, showcasing the potential of learned 3D features across diverse tasks.

Abstract: Open-vocabulary 3D scene understanding is pivotal for enhancing physical
intelligence, as it enables embodied agents to interpret and interact
dynamically within real-world environments. This paper introduces MPEC, a novel
Masked Point-Entity Contrastive learning method for open-vocabulary 3D semantic
segmentation that leverages both 3D entity-language alignment and point-entity
consistency across different point cloud views to foster entity-specific
feature representations. Our method improves semantic discrimination and
enhances the differentiation of unique instances, achieving state-of-the-art
results on ScanNet for open-vocabulary 3D semantic segmentation and
demonstrating superior zero-shot scene understanding capabilities. Extensive
fine-tuning experiments on 8 datasets, spanning from low-level perception to
high-level reasoning tasks, showcase the potential of learned 3D features,
driving consistent performance gains across varied 3D scene understanding
tasks. Project website: https://mpec-3d.github.io/

</details>


### [189] [SynergyAmodal: Deocclude Anything with Text Control](https://arxiv.org/abs/2504.19506)
*Xinyang Li, Chengjie Yi, Jiawei Lai, Mingbao Lin, Yansong Qu, Shengchuan Zhang, Liujuan Cao*

Main category: cs.CV

TL;DR: SynergyAmodal is a framework for co-synthesizing high-quality amodal datasets by combining in-the-wild data, human expertise, and generative priors, achieving zero-shot generalization and textual controllability.


<details>
  <summary>Details</summary>
Motivation: The scarcity of diverse, plausible, and high-fidelity data for image deocclusion hinders progress. The paper aims to address this by integrating data, human input, and models.

Method: The framework uses a self-supervised learning algorithm for diversity, a co-synthesis pipeline for plausibility and fidelity, and trains a full completion diffusion model with text prompts.

Result: A high-quality paired amodal dataset (~16K pairs) is generated, and the model demonstrates zero-shot generalization and textual controllability.

Conclusion: SynergyAmodal effectively bridges the gap in amodal dataset quality and utility, with public release of code, dataset, and models.

Abstract: Image deocclusion (or amodal completion) aims to recover the invisible
regions (\ie, shape and appearance) of occluded instances in images. Despite
recent advances, the scarcity of high-quality data that balances diversity,
plausibility, and fidelity remains a major obstacle. To address this challenge,
we identify three critical elements: leveraging in-the-wild image data for
diversity, incorporating human expertise for plausibility, and utilizing
generative priors for fidelity. We propose SynergyAmodal, a novel framework for
co-synthesizing in-the-wild amodal datasets with comprehensive shape and
appearance annotations, which integrates these elements through a tripartite
data-human-model collaboration. First, we design an occlusion-grounded
self-supervised learning algorithm to harness the diversity of in-the-wild
image data, fine-tuning an inpainting diffusion model into a partial completion
diffusion model. Second, we establish a co-synthesis pipeline to iteratively
filter, refine, select, and annotate the initial deocclusion results of the
partial completion diffusion model, ensuring plausibility and fidelity through
human expert guidance and prior model constraints. This pipeline generates a
high-quality paired amodal dataset with extensive category and scale diversity,
comprising approximately 16K pairs. Finally, we train a full completion
diffusion model on the synthesized dataset, incorporating text prompts as
conditioning signals. Extensive experiments demonstrate the effectiveness of
our framework in achieving zero-shot generalization and textual
controllability. Our code, dataset, and models will be made publicly available
at https://github.com/imlixinyang/SynergyAmodal.

</details>


### [190] [FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding](https://arxiv.org/abs/2504.19514)
*Rong Gao, Xin Liu, Zhuozhao Hu, Bohao Xing, Baiqiang Xia, Zitong Yu, Heikki Kälviäinen*

Main category: cs.CV

TL;DR: The paper introduces FSAnno, a large-scale dataset for figure skating, addressing the lack of comprehensive annotations for technical and artistic evaluation. It includes FSBench for benchmarking model performance.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for figure skating focus on single tasks, and sports research is biased toward ball games, neglecting artistic sports. FSAnno aims to fill this gap.

Method: FSAnno provides a dataset with comprehensive annotations and introduces FSBench, a benchmark with text and motion data for tasks like technical analysis and performance commentary.

Result: Initial tests on FSBench show existing models struggle with understanding artistic sports, highlighting the need for improved datasets and models.

Conclusion: FSAnno and FSBench aim to advance understanding of artistic sports like figure skating and serve as tools for evaluating and enhancing model comprehension.

Abstract: Figure skating, known as the "Art on Ice," is among the most artistic sports,
challenging to understand due to its blend of technical elements (like jumps
and spins) and overall artistic expression. Existing figure skating datasets
mainly focus on single tasks, such as action recognition or scoring, lacking
comprehensive annotations for both technical and artistic evaluation. Current
sports research is largely centered on ball games, with limited relevance to
artistic sports like figure skating. To address this, we introduce FSAnno, a
large-scale dataset advancing artistic sports understanding through figure
skating. FSAnno includes an open-access training and test dataset, alongside a
benchmark dataset, FSBench, for fair model evaluation. FSBench consists of
FSBench-Text, with multiple-choice questions and explanations, and
FSBench-Motion, containing multimodal data and Question and Answer (QA) pairs,
supporting tasks from technical analysis to performance commentary. Initial
tests on FSBench reveal significant limitations in existing models'
understanding of artistic sports. We hope FSBench will become a key tool for
evaluating and enhancing model comprehension of figure skating.

</details>


### [191] [LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning](https://arxiv.org/abs/2504.19524)
*Peijian Zeng, Feiyan Pang, Zhanbo Wang, Aimin Yang*

Main category: cs.CV

TL;DR: A novel reward function and mask-free reasoning framework for industrial anomaly detection (IAD) improves accuracy and reduces costs by addressing class imbalance and eliminating mask dependencies.


<details>
  <summary>Details</summary>
Motivation: Traditional IAD methods face scalability issues and high costs due to mask annotations. Class imbalance in datasets like MVTec-AD and VisA further complicates defect detection.

Method: Proposes a dynamic reward function for class imbalance and a mask-free framework using Chain of Thought (CoT) and Group Relative Policy Optimization (GRPO) for direct anomaly detection from raw images.

Result: Achieves state-of-the-art performance, improving accuracy by 36% on MVTec-AD and 16% on VisA, with interpretable defect localization.

Conclusion: The method advances IAD by reducing costs, eliminating mask dependency, and providing explainable outputs, supporting scalable quality control in manufacturing.

Abstract: Industrial Anomaly Detection (IAD) is critical for ensuring product quality
by identifying defects. Traditional methods such as feature embedding and
reconstruction-based approaches require large datasets and struggle with
scalability. Existing vision-language models (VLMs) and Multimodal Large
Language Models (MLLMs) address some limitations but rely on mask annotations,
leading to high implementation costs and false positives. Additionally,
industrial datasets like MVTec-AD and VisA suffer from severe class imbalance,
with defect samples constituting only 23.8% and 11.1% of total data
respectively. To address these challenges, we propose a reward function that
dynamically prioritizes rare defect patterns during training to handle class
imbalance. We also introduce a mask-free reasoning framework using Chain of
Thought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms,
enabling anomaly detection directly from raw images without annotated masks.
This approach generates interpretable step-by-step explanations for defect
localization. Our method achieves state-of-the-art performance, outperforming
prior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating
mask dependency and reducing costs while providing explainable outputs, this
work advances industrial anomaly detection and supports scalable quality
control in manufacturing. Code to reproduce the experiment is available at
https://github.com/LilaKen/LR-IAD.

</details>


### [192] [Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction](https://arxiv.org/abs/2504.19545)
*Zezeng Li, Zhihui Qi, Weimin Wang, Ziliang Wang, Junyi Duan, Na Lei*

Main category: cs.CV

TL;DR: Point2Quad is the first learning-based method for generating quad-only meshes from point clouds, addressing challenges like coplanarity and convexity through fused pointwise and facewise features.


<details>
  <summary>Details</summary>
Motivation: Quad mesh generation is under-explored in learning-based methods due to constraints like coplanarity and convexity, despite its importance in geometric modeling and computational mechanics.

Method: Point2Quad uses k-NN-based candidate generation, two encoders for geometric/topological features, and a classifier trained with compound loss, followed by quad-specific post-processing.

Result: Extensive experiments show Point2Quad outperforms baselines on both clean and noisy data under comprehensive metrics.

Conclusion: Point2Quad effectively addresses quad mesh generation challenges, demonstrating superiority over existing methods.

Abstract: Quad meshes are essential in geometric modeling and computational mechanics.
Although learning-based methods for triangle mesh demonstrate considerable
advancements, quad mesh generation remains less explored due to the challenge
of ensuring coplanarity, convexity, and quad-only meshes. In this paper, we
present Point2Quad, the first learning-based method for quad-only mesh
generation from point clouds. The key idea is learning to identify quad mesh
with fused pointwise and facewise features. Specifically, Point2Quad begins
with a k-NN-based candidate generation considering the coplanarity and
squareness. Then, two encoders are followed to extract geometric and
topological features that address the challenge of quad-related constraints,
especially by combining in-depth quadrilaterals-specific characteristics.
Subsequently, the extracted features are fused to train the classifier with a
designed compound loss. The final results are derived after the refinement by a
quad-specific post-processing. Extensive experiments on both clear and noise
data demonstrate the effectiveness and superiority of Point2Quad, compared to
baseline methods under comprehensive metrics.

</details>


### [193] [Crowd Detection Using Very-Fine-Resolution Satellite Imagery](https://arxiv.org/abs/2504.19546)
*Tong Xiao, Qunming Wang, Ping Lu, Tenghai Huang, Xiaohua Tong, Peter M. Atkinson*

Main category: cs.CV

TL;DR: CrowdSat-Net, a novel CNN for crowd detection using VFR satellite imagery, outperforms existing methods with innovative modules (DCPAN and HFGDU) and a new dataset (CrowdSat).


<details>
  <summary>Details</summary>
Motivation: Existing crowd detection methods lack spatio-temporal coverage; VFR satellite imagery offers new opportunities but hasn't been explored for this task.

Method: Proposed CrowdSat-Net with DCPAN for feature aggregation and HFGDU for high-frequency recovery, validated on the CrowdSat dataset.

Result: Achieved F1-score of 66.12% and Precision of 73.23%, outperforming other methods by 1.71% and 2.42%, respectively.

Conclusion: CrowdSat-Net advances crowd detection with a new architecture and benchmark dataset, demonstrating spatial generalizability.

Abstract: Accurate crowd detection (CD) is critical for public safety and historical
pattern analysis, yet existing methods relying on ground and aerial imagery
suffer from limited spatio-temporal coverage. The development of
very-fine-resolution (VFR) satellite sensor imagery (e.g., ~0.3 m spatial
resolution) provides unprecedented opportunities for large-scale crowd activity
analysis, but it has never been considered for this task. To address this gap,
we proposed CrowdSat-Net, a novel point-based convolutional neural network,
which features two innovative components: Dual-Context Progressive Attention
Network (DCPAN) to improve feature representation of individuals by aggregating
scene context and local individual characteristics, and High-Frequency Guided
Deformable Upsampler (HFGDU) that recovers high-frequency information during
upsampling through frequency-domain guided deformable convolutions. To validate
the effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR
satellite imagery dataset designed specifically for CD tasks, comprising over
120k manually labeled individuals from multi-source satellite platforms
(Beijing-3N, Jilin-1 Gaofen-04A and Google Earth) across China. In the
experiments, CrowdSat-Net was compared with five state-of-the-art point-based
CD methods (originally designed for ground or aerial imagery) using CrowdSat
and achieved the largest F1-score of 66.12% and Precision of 73.23%, surpassing
the second-best method by 1.71% and 2.42%, respectively. Moreover, extensive
ablation experiments validated the importance of the DCPAN and HFGDU modules.
Furthermore, cross-regional evaluation further demonstrated the spatial
generalizability of CrowdSat-Net. This research advances CD capability by
providing both a newly developed network architecture for CD and a pioneering
benchmark dataset to facilitate future CD development.

</details>


### [194] [DEEMO: De-identity Multimodal Emotion Recognition and Reasoning](https://arxiv.org/abs/2504.19549)
*Deng Li, Bohao Xing, Xin Liu, Baiqiang Xia, Bihan Wen, Heikki Kälviäinen*

Main category: cs.CV

TL;DR: DEEMO introduces a privacy-preserving method for emotion recognition using de-identified data, achieving state-of-the-art results with DEEMO-LLaMA.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in emotion recognition by avoiding identity-sensitive data like facial expressions and speech.

Method: Proposes DEEMO dataset (DEEMO-NFBL and DEEMO-MER) and DEEMO-LLaMA, a Multimodal Large Language Model for emotion recognition and reasoning.

Result: DEEMO-LLaMA achieves 74.49% accuracy and 74.45% F1-score in recognition, and 6.20 clue overlap and 7.66 label overlap in reasoning.

Conclusion: Advances ethical AI by enabling privacy-preserving emotion understanding.

Abstract: Emotion understanding is a critical yet challenging task. Most existing
approaches rely heavily on identity-sensitive information, such as facial
expressions and speech, which raises concerns about personal privacy. To
address this, we introduce the De-identity Multimodal Emotion Recognition and
Reasoning (DEEMO), a novel task designed to enable emotion understanding using
de-identified video and audio inputs. The DEEMO dataset consists of two
subsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body
Language (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion
Recognition and Reasoning using identity-free cues. This design supports
emotion understanding without compromising identity privacy. In addition, we
propose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates
de-identified audio, video, and textual information to enhance both emotion
recognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves
state-of-the-art performance on both tasks, outperforming existing MLLMs by a
significant margin, achieving 74.49% accuracy and 74.45% F1-score in
de-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap
in de-identity emotion reasoning. Our work contributes to ethical AI by
advancing privacy-preserving emotion understanding and promoting responsible
affective computing.

</details>


### [195] [CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes](https://arxiv.org/abs/2504.19557)
*Mohammad Altillawi, Fengyi Shen, Liudi Yang, Sai Manoj Prakhya, Ziyuan Liu*

Main category: cs.CV

TL;DR: CE-NPBG improves novel view synthesis in large-scale scenes by addressing visibility mismatches between geometry and appearance using a neural point-based method with connectivity graphs and joint adversarial training.


<details>
  <summary>Details</summary>
Motivation: Current point-based methods struggle with scalability and rendering quality in large 3D point cloud maps due to visibility mismatches between geometry and appearance.

Method: CE-NPBG uses a connectivity graph to retrieve relevant points from a large 3D point cloud map, associates neural descriptors with points, and employs joint adversarial and point rasterization training for improved rendering.

Result: The method enhances rendering quality and scalability by using a subset of points and improves runtime efficiency.

Conclusion: CE-NPBG effectively addresses visibility mismatches and integrates with 3D Gaussian Splatting for superior rendering in autonomous driving scenes.

Abstract: Current point-based approaches encounter limitations in scalability and
rendering quality when using large 3D point cloud maps because using them
directly for novel view synthesis (NVS) leads to degraded visualizations. We
identify the primary issue behind these low-quality renderings as a visibility
mismatch between geometry and appearance, stemming from using these two
modalities together. To address this problem, we present CE-NPBG, a new
approach for novel view synthesis (NVS) in large-scale autonomous driving
scenes. Our method is a neural point-based technique that leverages two
modalities: posed images (cameras) and synchronized raw 3D point clouds
(LiDAR). We first employ a connectivity relationship graph between appearance
and geometry, which retrieves points from a large 3D point cloud map observed
from the current camera perspective and uses them for rendering. By leveraging
this connectivity, our method significantly improves rendering quality and
enhances run-time and scalability by using only a small subset of points from
the large 3D point cloud map. Our approach associates neural descriptors with
the points and uses them to synthesize views. To enhance the encoding of these
descriptors and elevate rendering quality, we propose a joint adversarial and
point rasterization training. During training, we pair an image-synthesizer
network with a multi-resolution discriminator. At inference, we decouple them
and use the image-synthesizer to generate novel views. We also integrate our
proposal into the recent 3D Gaussian Splatting work to highlight its benefits
for improved rendering and scalability.

</details>


### [196] [Category-Level and Open-Set Object Pose Estimation for Robotics](https://arxiv.org/abs/2504.19572)
*Peter Hönig, Matthias Hirschmanner, Markus Vincze*

Main category: cs.CV

TL;DR: This paper compares datasets, metrics, and algorithms for 6D pose estimation at the category-level, aiming to bridge the gap to open-set scenarios and provide actionable recommendations.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in estimating 6D poses for objects with unknown texture, shape, and size, especially in category-level and open-set scenarios, where current methods struggle.

Method: The study compares existing datasets, accuracy metrics, and algorithmic solutions for 6D pose estimation at the category-level.

Result: The analysis identifies gaps and challenges in current approaches, particularly in handling unknown variables like texture and symmetries.

Conclusion: The paper provides actionable recommendations to bridge category-level and open-set pose estimation, aiming for better generalization.

Abstract: Object pose estimation enables a variety of tasks in computer vision and
robotics, including scene understanding and robotic grasping. The complexity of
a pose estimation task depends on the unknown variables related to the target
object. While instance-level methods already excel for opaque and Lambertian
objects, category-level and open-set methods, where texture, shape, and size
are partially or entirely unknown, still struggle with these basic material
properties. Since texture is unknown in these scenarios, it cannot be used for
disambiguating object symmetries, another core challenge of 6D object pose
estimation. The complexity of estimating 6D poses with such a manifold of
unknowns led to various datasets, accuracy metrics, and algorithmic solutions.
This paper compares datasets, accuracy metrics, and algorithms for solving 6D
pose estimation on the category-level. Based on this comparison, we analyze how
to bridge category-level and open-set object pose estimation to reach
generalization and provide actionable recommendations.

</details>


### [197] [DG-DETR: Toward Domain Generalized Detection Transformer](https://arxiv.org/abs/2504.19574)
*Seongmin Hwang, Daeyoung Han, Moongu Jeon*

Main category: cs.CV

TL;DR: DG-DETR enhances DETR's out-of-distribution robustness via domain-agnostic query selection and wavelet-based feature disentanglement.


<details>
  <summary>Details</summary>
Motivation: Existing domain generalization research focuses on CNN-based detectors, neglecting DETRs. DG-DETR aims to improve DETR's robustness to unseen domains.

Method: Proposes domain-agnostic query selection (removes biases via orthogonal projection) and wavelet decomposition (disentangles features into domain-invariant/specific components).

Result: Experimental results confirm DG-DETR's effectiveness in improving DETR's out-of-distribution robustness.

Conclusion: DG-DETR is a simple, plug-and-play solution for enhancing DETR's domain generalization capabilities.

Abstract: End-to-end Transformer-based detectors (DETRs) have demonstrated strong
detection performance. However, domain generalization (DG) research has
primarily focused on convolutional neural network (CNN)-based detectors, while
paying little attention to enhancing the robustness of DETRs. In this letter,
we introduce a Domain Generalized DEtection TRansformer (DG-DETR), a simple,
effective, and plug-and-play method that improves out-of-distribution (OOD)
robustness for DETRs. Specifically, we propose a novel domain-agnostic query
selection strategy that removes domain-induced biases from object queries via
orthogonal projection onto the instance-specific style space. Additionally, we
leverage a wavelet decomposition to disentangle features into domain-invariant
and domain-specific components, enabling synthesis of diverse latent styles
while preserving the semantic features of objects. Experimental results
validate the effectiveness of DG-DETR. Our code is available at
https://github.com/sminhwang/DG-DETR.

</details>


### [198] [SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity](https://arxiv.org/abs/2504.19581)
*Chengzhi Wu, Yuxin Wan, Hao Fu, Julius Pfrommer, Zeyun Zhong, Junwei Zheng, Jiaming Zhang, Jürgen Beyerer*

Main category: cs.CV

TL;DR: SAMBLE is a learning-based method for point cloud sampling that balances edge detail preservation and global shape uniformity, outperforming prior methods in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing learning-to-sample methods, which either produce unrecognizable patterns or biased results by ignoring shape-specific variations.

Method: Proposes SAMBLE, using sparse attention maps and bin-based learning to tailor sampling strategies to individual point cloud shapes.

Result: Achieves better balance between edge detail and global uniformity, improving performance in downstream tasks, even with few-point sampling.

Conclusion: SAMBLE offers a robust, shape-specific approach to point cloud sampling, enhancing accuracy and efficiency in 3D data representation.

Abstract: Driven by the increasing demand for accurate and efficient representation of
3D data in various domains, point cloud sampling has emerged as a pivotal
research topic in 3D computer vision. Recently, learning-to-sample methods have
garnered growing interest from the community, particularly for their ability to
be jointly trained with downstream tasks. However, previous learning-based
sampling methods either lead to unrecognizable sampling patterns by generating
a new point cloud or biased sampled results by focusing excessively on sharp
edge details. Moreover, they all overlook the natural variations in point
distribution across different shapes, applying a similar sampling strategy to
all point clouds. In this paper, we propose a Sparse Attention Map and
Bin-based Learning method (termed SAMBLE) to learn shape-specific sampling
strategies for point cloud shapes. SAMBLE effectively achieves an improved
balance between sampling edge points for local details and preserving
uniformity in the global shape, resulting in superior performance across
multiple common point cloud downstream tasks, even in scenarios with few-point
sampling.

</details>


### [199] [ShowMak3r: Compositional TV Show Reconstruction](https://arxiv.org/abs/2504.19584)
*Sangmin Kim, Seunguk Do, Jaesik Park*

Main category: cs.CV

TL;DR: ShowMak3r is a pipeline for reconstructing and editing dynamic radiance fields from challenging TV show videos, addressing occlusion, cluttered stages, and shot changes.


<details>
  <summary>Details</summary>
Motivation: Reconstructing dynamic radiance fields from entertainment videos is difficult due to actor occlusion, diverse expressions, cluttered stages, and shot changes.

Method: ShowMak3r uses a 3DLocator for actor positioning and pose estimation, a ShotMatcher for tracking across shots, and a face-fitting network for expression recovery.

Result: Experiments on Sitcoms3D show successful scene reassembly with new cameras and timestamps, enabling applications like synthetic shot-making and actor manipulation.

Conclusion: ShowMak3r effectively reconstructs and edits dynamic scenes from TV shows, offering practical applications in video production.

Abstract: Reconstructing dynamic radiance fields from video clips is challenging,
especially when entertainment videos like TV shows are given. Many challenges
make the reconstruction difficult due to (1) actors occluding with each other
and having diverse facial expressions, (2) cluttered stages, and (3) small
baseline views or sudden shot changes. To address these issues, we present
ShowMak3r, a comprehensive reconstruction pipeline that allows the editing of
scenes like how video clips are made in a production control room. In
ShowMak3r, a 3DLocator module locates recovered actors on the stage using depth
prior and estimates unseen human poses via interpolation. The proposed
ShotMatcher module then tracks the actors under shot changes. Furthermore,
ShowMak3r introduces a face-fitting network that dynamically recovers the
actors' expressions. Experiments on Sitcoms3D dataset show that our pipeline
can reassemble TV show scenes with new cameras at different timestamps. We also
demonstrate that ShowMak3r enables interesting applications such as synthetic
shot-making, actor relocation, insertion, deletion, and pose manipulation.
Project page : https://nstar1125.github.io/showmak3r

</details>


### [200] [Neural network task specialization via domain constraining](https://arxiv.org/abs/2504.19592)
*Roman Malashin, Daniil Ilyukhin*

Main category: cs.CV

TL;DR: Specializing neural networks by constraining their domain improves performance without extra data or training changes, focusing on coherent data subsets.


<details>
  <summary>Details</summary>
Motivation: Enhance neural network performance on specific data subspaces by specializing them for tasks like image classification and object detection.

Method: Task-specific domain constraining, modifying fine-tuning methods, and extracting specialists before tuning. Analyzes feature space evolution.

Result: Specialization boosts generalist accuracy by constraining class label space, requiring coherent data subsets for effectiveness.

Conclusion: Paves the way for dynamically configurable systems and improves performance by excluding irrelevant data domains.

Abstract: This paper introduces a concept of neural network specialization via
task-specific domain constraining, aimed at enhancing network performance on
data subspace in which the network operates. The study presents experiments on
training specialists for image classification and object detection tasks. The
results demonstrate that specialization can enhance a generalist's accuracy
even without additional data or changing training regimes: solely by
constraining class label space in which the network performs. Theoretical and
experimental analyses indicate that effective specialization requires modifying
traditional fine-tuning methods and constraining data space to semantically
coherent subsets. The specialist extraction phase before tuning the network is
proposed for maximal performance gains. We also provide analysis of the
evolution of the feature space during specialization. This study paves way to
future research for developing more advanced dynamically configurable image
analysis systems, where computations depend on the specific input.
Additionally, the proposed methods can help improve system performance in
scenarios where certain data domains should be excluded from consideration of
the generalist network.

</details>


### [201] [Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection](https://arxiv.org/abs/2504.19598)
*Dou Quan, Rufan Zhou, Shuang Wang, Ning Huyan, Dong Zhao, Yunan Li, Licheng Jiao*

Main category: cs.CV

TL;DR: The paper proposes CANet, a change adapter network for universal remote sensing image change detection, combining dataset-shared and dataset-specific modules for better generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for change detection are dataset-specific and lack generalization across datasets due to data distribution and labeling differences.

Method: CANet uses shared and specific modules, including a lightweight adapter with an interesting change region mask (ICM) and dataset-specific batch normalization.

Result: CANet achieves strong generalization, lower training costs (4.1%-7.7% parameter updates), and better performance with limited data compared to existing methods.

Conclusion: CANet is effective, flexible, and outperforms other deep learning methods in change detection tasks across diverse datasets.

Abstract: Deep learning methods have shown promising performances in remote sensing
image change detection (CD). However, existing methods usually train a
dataset-specific deep network for each dataset. Due to the significant
differences in the data distribution and labeling between various datasets, the
trained dataset-specific deep network has poor generalization performances on
other datasets. To solve this problem, this paper proposes a change adapter
network (CANet) for a more universal and generalized CD. CANet contains
dataset-shared and dataset-specific learning modules. The former explores the
discriminative features of images, and the latter designs a lightweight adapter
model, to deal with the characteristics of different datasets in data
distribution and labeling. The lightweight adapter can quickly generalize the
deep network for new CD tasks with a small computation cost. Specifically, this
paper proposes an interesting change region mask (ICM) in the adapter, which
can adaptively focus on interested change objects and decrease the influence of
labeling differences in various datasets. Moreover, CANet adopts a unique batch
normalization layer for each dataset to deal with data distribution
differences. Compared with existing deep learning methods, CANet can achieve
satisfactory CD performances on various datasets simultaneously. Experimental
results on several public datasets have verified the effectiveness and
advantages of the proposed CANet on CD. CANet has a stronger generalization
ability, smaller training costs (merely updating 4.1%-7.7% parameters), and
better performances under limited training datasets than other deep learning
methods, which also can be flexibly inserted with existing deep models.

</details>


### [202] [Image Generation Method Based on Heat Diffusion Models](https://arxiv.org/abs/2504.19600)
*Pengfei Zhang, Shouqing Jia*

Main category: cs.CV

TL;DR: HDM improves image generation by incorporating pixel-level heat diffusion into DDPM, outperforming DDPM, CDM, LDM, and VQGAN.


<details>
  <summary>Details</summary>
Motivation: Adjacent pixels likely belong to the same object, so preserving details via pixel-level operations can enhance realism.

Method: Integrates the 2D heat equation into DDPM's diffusion and generation formulas to compute neighbor pixel relationships.

Result: HDM generates higher-quality images than DDPM, CDM, LDM, and VQGAN.

Conclusion: HDM enhances image generation by leveraging pixel-level heat diffusion, outperforming existing models.

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image
generation without adversarial training, but they process images as a whole.
Since adjacent pixels are highly likely to belong to the same object, we
propose the Heat Diffusion Model (HDM) to further preserve image details and
generate more realistic images. HDM is a model that incorporates pixel-level
operations while maintaining the same training process as DDPM. In HDM, the
discrete form of the two-dimensional heat equation is integrated into the
diffusion and generation formulas of DDPM, enabling the model to compute
relationships between neighboring pixels during image processing. Our
experiments demonstrate that HDM can generate higher-quality samples compared
to models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion
Models (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN).

</details>


### [203] [DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer](https://arxiv.org/abs/2504.19614)
*Junpeng Jiang, Gangyi Hong, Miao Zhang, Hengtong Hu, Kun Zhan, Rui Shao, Liqiang Nie*

Main category: cs.CV

TL;DR: DiVE is a diffusion transformer-based framework for generating high-fidelity, temporally coherent multi-view driving videos, addressing quality and consistency issues in prior works.


<details>
  <summary>Details</summary>
Motivation: High costs and challenges in collecting real multi-view driving data motivate the need for generative models that produce realistic, consistent videos.

Method: DiVE uses a diffusion transformer with cross-attention, SketchFormer, and view-inflated attention for consistency. Innovations include Multi-Control Auxiliary Branch Distillation and Resolution Progressive Sampling for efficiency.

Result: DiVE achieves state-of-the-art performance on nuScenes, producing photorealistic, temporally coherent videos with a 2.62x speedup.

Conclusion: DiVE advances multi-view video generation by addressing quality, consistency, and computational challenges, offering a viable alternative to real data collection.

Abstract: Collecting multi-view driving scenario videos to enhance the performance of
3D visual perception tasks presents significant challenges and incurs
substantial costs, making generative models for realistic data an appealing
alternative. Yet, the videos generated by recent works suffer from poor quality
and spatiotemporal consistency, undermining their utility in advancing
perception tasks under driving scenarios. To address this gap, we propose DiVE,
a diffusion transformer-based generative framework meticulously engineered to
produce high-fidelity, temporally coherent, and cross-view consistent
multi-view videos, aligning seamlessly with bird's-eye view layouts and textual
descriptions. DiVE leverages a unified cross-attention and a SketchFormer to
exert precise control over multimodal data, while incorporating a view-inflated
attention mechanism that adds no extra parameters, thereby guaranteeing
consistency across views. Despite these advancements, synthesizing
high-resolution videos under multimodal constraints introduces dual challenges:
investigating the optimal classifier-free guidance coniguration under intricate
multi-condition inputs and mitigating excessive computational latency in
high-resolution rendering--both of which remain underexplored in prior
researches. To resolve these limitations, we introduce two innovations:
Multi-Control Auxiliary Branch Distillation, which streamlines multi-condition
CFG selection while circumventing high computational overhead, and Resolution
Progressive Sampling, a training-free acceleration strategy that staggers
resolution scaling to reduce high latency due to high resolution. These
innovations collectively achieve a 2.62x speedup with minimal quality
degradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance
in multi-view video generation, yielding photorealistic outputs with
exceptional temporal and cross-view coherence.

</details>


### [204] [NSegment : Noisy Segment Improves Remote Sensing Image Segmentation](https://arxiv.org/abs/2504.19634)
*Yechan Kim, DongHo Yoon, SooYeon Kim, Moongu Jeon*

Main category: cs.CV

TL;DR: NSegment is a data augmentation method for RS image segmentation that applies elastic transformations to labels, improving model performance despite labeling errors.


<details>
  <summary>Details</summary>
Motivation: Labeling errors in RS datasets are common due to ambiguous boundaries, mixed pixels, and subjective bias, and annotated data is scarce, making noise-robust training challenging.

Method: NSegment uses elastic transformations on segmentation labels, varying deformation intensity per sample in each epoch to address annotation inconsistencies.

Result: The approach enhances performance of RS image segmentation across state-of-the-art models.

Conclusion: NSegment offers a simple, effective solution to mitigate labeling errors without increasing training complexity.

Abstract: Labeling errors in remote sensing (RS) image segmentation datasets often
remain implicit and subtle due to ambiguous class boundaries, mixed pixels,
shadows, complex terrain features, and subjective annotator bias. Furthermore,
the scarcity of annotated RS data due to high image acquisition and labeling
costs complicates training noise-robust models. While sophisticated mechanisms
such as label selection or noise correction might address this issue, they tend
to increase training time and add implementation complexity. In this letter, we
propose NSegment-a simple yet effective data augmentation solution to mitigate
this issue. Unlike traditional methods, it applies elastic transformations only
to segmentation labels, varying deformation intensity per sample in each
training epoch to address annotation inconsistencies. Experimental results
demonstrate that our approach improves the performance of RS image segmentation
on various state-of-the-art models.

</details>


### [205] [Exploiting Inter-Sample Correlation and Intra-Sample Redundancy for Partially Relevant Video Retrieval](https://arxiv.org/abs/2504.19637)
*Junlong Ren, Gangjian Zhang, Yu Hu, Jian Shu, Hao Wang*

Main category: cs.CV

TL;DR: A novel PRVR framework addresses semantic asymmetry in video retrieval by leveraging inter-sample correlation and intra-sample redundancy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge in PRVR is semantic asymmetry between text and video, with existing methods neglecting inter-sample correlation and intra-sample redundancy.

Method: Proposes three modules: ICE for inter-sample correlation, IRM for intra-sample redundancy, and TCP for temporal coherence.

Result: Achieves state-of-the-art results on three datasets.

Conclusion: The framework effectively addresses PRVR challenges by exploiting dual cross-modal characteristics.

Abstract: Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video
that is partially relevant to the text query. The primary challenge in PRVR
arises from the semantic asymmetry between textual and visual modalities, as
videos often contain substantial content irrelevant to the query. Existing
methods coarsely align paired videos and text queries to construct the semantic
space, neglecting the critical cross-modal dual nature inherent in this task:
inter-sample correlation and intra-sample redundancy. To this end, we propose a
novel PRVR framework to systematically exploit these two characteristics. Our
framework consists of three core modules. First, the Inter Correlation
Enhancement (ICE) module captures inter-sample correlation by identifying
semantically similar yet unpaired text queries and video moments, combining
them to form pseudo-positive pairs for more robust semantic space construction.
Second, the Intra Redundancy Mining (IRM) module mitigates intra-sample
redundancy by mining redundant video moment features and treating them as hard
negative samples, thereby encouraging the model to learn more discriminative
representations. Finally, to reinforce these modules, we introduce the Temporal
Coherence Prediction (TCP) module, which enhances feature discrimination by
training the model to predict the original temporal order of randomly shuffled
video frames and moments. Extensive experiments on three datasets demonstrate
the superiority of our approach compared to previous methods, achieving
state-of-the-art results.

</details>


### [206] [BARIS: Boundary-Aware Refinement with Environmental Degradation Priors for Robust Underwater Instance Segmentation](https://arxiv.org/abs/2504.19643)
*Pin-Chi Pan, Soo-Chang Pei*

Main category: cs.CV

TL;DR: BARIS-ERA improves underwater instance segmentation by refining features and modeling degradation patterns, outperforming Mask R-CNN by 3.4-3.8 mAP.


<details>
  <summary>Details</summary>
Motivation: Underwater conditions degrade segmentation performance due to light and color issues.

Method: Proposes BARIS-Decoder for feature refinement and ERA for modeling underwater degradations with fewer parameters.

Result: BARIS-ERA achieves state-of-the-art performance, surpassing Mask R-CNN by 3.4-3.8 mAP.

Conclusion: BARIS-ERA provides a robust and efficient solution for underwater instance segmentation.

Abstract: Underwater instance segmentation is challenging due to adverse visual
conditions such as light attenuation, scattering, and color distortion, which
degrade model performance. In this work, we propose BARIS-Decoder
(Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that
enhances segmentation accuracy through feature refinement. To address
underwater degradations, we introduce the Environmental Robust Adapter (ERA),
which efficiently models underwater degradation patterns while reducing
trainable parameters by over 90\% compared to full fine-tuning. The integration
of BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves
state-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B
backbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the
effectiveness of BARIS-ERA in advancing underwater instance segmentation,
providing a robust and efficient solution.

</details>


### [207] [xEdgeFace: Efficient Cross-Spectral Face Recognition for Edge Devices](https://arxiv.org/abs/2504.19646)
*Anjith George, Sebastien Marcel*

Main category: cs.CV

TL;DR: A lightweight HFR framework using a hybrid CNN-Transformer architecture achieves strong performance with minimal data and low computational cost.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of matching face images across different modalities while ensuring practicality for resource-constrained devices.

Method: Adapts a hybrid CNN-Transformer architecture for efficient end-to-end training with minimal paired heterogeneous data.

Result: Outperforms state-of-the-art methods on HFR and face recognition benchmarks with low computational overhead.

Conclusion: The framework is effective for both homogeneous and heterogeneous face recognition, offering a practical solution for edge devices.

Abstract: Heterogeneous Face Recognition (HFR) addresses the challenge of matching face
images across different sensing modalities, such as thermal to visible or
near-infrared to visible, expanding the applicability of face recognition
systems in real-world, unconstrained environments. While recent HFR methods
have shown promising results, many rely on computation-intensive architectures,
limiting their practicality for deployment on resource-constrained edge
devices. In this work, we present a lightweight yet effective HFR framework by
adapting a hybrid CNN-Transformer architecture originally designed for face
recognition. Our approach enables efficient end-to-end training with minimal
paired heterogeneous data while preserving strong performance on standard RGB
face recognition tasks. This makes it a compelling solution for both
homogeneous and heterogeneous scenarios. Extensive experiments across multiple
challenging HFR and face recognition benchmarks demonstrate that our method
consistently outperforms state-of-the-art approaches while maintaining a low
computational overhead.

</details>


### [208] [Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification](https://arxiv.org/abs/2504.19682)
*Nikolaos Chaidos, Angeliki Dimitriou, Nikolaos Spanos, Athanasios Voulodimos, Giorgos Stamou*

Main category: cs.CV

TL;DR: The paper explores the explainability of GNN-based image classifiers by analyzing semantic consistency and graph connections across layers, comparing standard and adversarial settings, and visualizing information flow.


<details>
  <summary>Details</summary>
Motivation: Despite GNNs' efficiency for vision tasks, their explainability is underexplored, even though graphs are naturally interpretable.

Method: Analyzes semantic consistency of graphs at different GNN layers, quantifying inter-layer connections for semantic similarity and spatial coherence. Compares explanations in standard and adversarial settings and visualizes information flow.

Result: Demonstrates effective explainability of GNN decision-making but reveals misalignment with human perception, especially in deeper layers.

Conclusion: GNN-based models can be explained, but their reasoning may not align with human intuition, particularly in deeper layers.

Abstract: Graph Neural Networks (GNNs) have emerged as an efficient alternative to
convolutional approaches for vision tasks such as image classification,
leveraging patch-based representations instead of raw pixels. These methods
construct graphs where image patches serve as nodes, and edges are established
based on patch similarity or classification relevance. Despite their
efficiency, the explainability of GNN-based vision models remains
underexplored, even though graphs are naturally interpretable. In this work, we
analyze the semantic consistency of the graphs formed at different layers of
GNN-based image classifiers, focusing on how well they preserve object
structures and meaningful relationships. A comprehensive analysis is presented
by quantifying the extent to which inter-layer graph connections reflect
semantic similarity and spatial coherence. Explanations from standard and
adversarial settings are also compared to assess whether they reflect the
classifiers' robustness. Additionally, we visualize the flow of information
across layers through heatmap-based visualization techniques, thereby
highlighting the models' explainability. Our findings demonstrate that the
decision-making processes of these models can be effectively explained, while
also revealing that their reasoning does not necessarily align with human
perception, especially in deeper layers.

</details>


### [209] [ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery](https://arxiv.org/abs/2504.19684)
*Anush Lakshman Sivaraman, Kojo Adu-Gyamfi, Ibne Farabi Shihab, Anuj Sharma*

Main category: cs.CV

TL;DR: A scalable framework combining generative domain adaptation and contrastive learning improves weather classification from low-quality traffic camera images, especially at night.


<details>
  <summary>Details</summary>
Motivation: Accurate weather classification from low-quality nighttime traffic camera imagery is challenging.

Method: Uses CycleGAN for domain translation and SigLIP-2 for contrastive learning to enhance feature extraction.

Result: SigLIP-2 improves nighttime accuracy to 85.90%, while EVA-02 with CycleGAN achieves 97.01% overall accuracy.

Conclusion: Combining domain adaptation and efficient contrastive learning enhances practical, resource-efficient weather classification systems.

Abstract: Accurate weather classification from low-quality traffic camera imagery
remains a challenging task, particularly under adverse nighttime conditions. In
this study, we propose a scalable framework that combines generative domain
adaptation with efficient contrastive learning to enhance classification
performance. Using CycleGAN-based domain translation, we improve the quality of
nighttime images, enabling better feature extraction by downstream models.
While the baseline EVA-02 model employing CLIP-based contrastive loss achieves
an overall accuracy of 96.55\%, it exhibits a significant performance gap
between daytime (97.21\%) and nighttime conditions (63.40\%). Replacing CLIP
with the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive
overall accuracy of 94.00\%, with substantial improvements in nighttime
performance (85.90\% accuracy). The combination of Vision-SigLIP-2,
Text-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime
accuracy (85.90\%) among all models tested, while EVA-02 with CycleGAN
maintains the highest overall accuracy (97.01\%) and per-class accuracies.
These findings demonstrate the potential of combining domain adaptation and
efficient contrastive learning to build practical, resource-efficient weather
classification systems for intelligent transportation infrastructure.

</details>


### [210] [Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network for Low-Dose CT MAR](https://arxiv.org/abs/2504.19687)
*Baoshun Shi, Bing Chen, Shaolei Zhang, Huazhu Fu, Zhanli Hu*

Main category: cs.CV

TL;DR: Proposes PMSRNet for low-dose CT reconstruction and metal artifact reduction, addressing multi-scale information neglect and storage inefficiency with a single adaptive model.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack multi-scale and within-scale information integration and require separate models for each dose level, increasing storage needs.

Method: Introduces PMSRNet with a prompt guiding scale-adaptive threshold generator (PSATG) and multi-scale coefficient fusion module (MSFuM) for adaptive thresholding and feature fusion. Also develops PDuMSRNet, a dual-domain framework, trained with a prompt guiding strategy for multiple doses.

Result: Outperforms state-of-the-art methods in experiments across various dose levels.

Conclusion: PMSRNet and PDuMSRNet effectively address limitations in LDCT reconstruction and metal artifact reduction, offering improved performance and efficiency.

Abstract: Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it
will potentially degrade image quality, even yields metal artifacts at the case
of metallic implants. For simultaneous LDCT reconstruction and metal artifact
reduction (LDMAR), existing deep learning-based efforts face two main
limitations: i) the network design neglects multi-scale and within-scale
information; ii) training a distinct model for each dose necessitates
significant storage space for multiple doses. To fill these gaps, we propose a
prompt guiding multi-scale adaptive sparse representation-driven network,
abbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet
inspired from multi-scale sparsifying frames, and it can simultaneously employ
within-scale characteristics and cross-scale complementarity owing to an
elaborated prompt guiding scale-adaptive threshold generator (PSATG) and a
built multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively
capture multiple contextual information to generate more faithful thresholds,
achieved by fusing features from local, regional, and global levels.
Furthermore, we elaborate a model interpretable dual domain LDMAR framework
called PDuMSRNet, and train single model with a prompt guiding strategy for
multiple dose levels. We build a prompt guiding module, whose input contains
dose level, metal mask and input instance, to provide various guiding
information, allowing a single model to accommodate various CT dose settings.
Extensive experiments at various dose levels demonstrate that the proposed
methods outperform the state-of-the-art LDMAR methods.

</details>


### [211] [SubGrapher: Visual Fingerprinting of Chemical Structures](https://arxiv.org/abs/2504.19695)
*Lucas Morin, Gerhard Ingmar Meijer, Valéry Weber, Luc Van Gool, Peter W. J. Staar*

Main category: cs.CV

TL;DR: SubGrapher extracts molecular fingerprints directly from chemical structure images, outperforming traditional methods in retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Chemical structures in patents are often inaccessible via text searches, necessitating visual extraction methods.

Method: SubGrapher uses learning-based instance segmentation to identify functional groups and carbon backbones for fingerprinting.

Result: Outperforms state-of-the-art OCSR and fingerprinting methods in retrieval and robustness.

Conclusion: SubGrapher offers a superior approach for chemical structure retrieval, with public release of resources.

Abstract: Automatic extraction of chemical structures from scientific literature plays
a crucial role in accelerating research across fields ranging from drug
discovery to materials science. Patent documents, in particular, contain
molecular information in visual form, which is often inaccessible through
traditional text-based searches. In this work, we introduce SubGrapher, a
method for the visual fingerprinting of chemical structure images. Unlike
conventional Optical Chemical Structure Recognition (OCSR) models that attempt
to reconstruct full molecular graphs, SubGrapher focuses on extracting
molecular fingerprints directly from chemical structure images. Using
learning-based instance segmentation, SubGrapher identifies functional groups
and carbon backbones, constructing a substructure-based fingerprint that
enables chemical structure retrieval. Our approach is evaluated against
state-of-the-art OCSR and fingerprinting methods, demonstrating superior
retrieval performance and robustness across diverse molecular depictions. The
dataset, models, and code will be made publicly available.

</details>


### [212] [Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment](https://arxiv.org/abs/2504.19755)
*Kapil Kashyap, Sean Fargose, Chrisil Dabre, Fatema Dolaria, Nilesh Patil, Aniket Kore*

Main category: cs.CV

TL;DR: A hybrid model combining machine learning, clinical data, and ultrasound scans improves liver cirrhosis detection with 92.5% accuracy, offering a non-invasive alternative to liver biopsy.


<details>
  <summary>Details</summary>
Motivation: Liver biopsy is invasive and unsuitable for regular screening, necessitating a non-invasive, accurate diagnostic method for liver cirrhosis.

Method: The hybrid model integrates fixed blood test probabilities with deep learning (DenseNet-201) predictions from ultrasound scans.

Result: The model achieved 92.5% accuracy in detecting liver fibrosis and cirrhosis.

Conclusion: The hybrid model enhances diagnosis accuracy and supports early intervention, proving viable for liver disease care.

Abstract: Liver cirrhosis is an insidious condition involving the substitution of
normal liver tissue with fibrous scar tissue and causing major health
complications. The conventional method of diagnosis using liver biopsy is
invasive and, therefore, inconvenient for use in regular screening. In this
paper,we present a hybrid model that combines machine learning techniques with
clinical data and ultrasoundscans to improve liver fibrosis and cirrhosis
detection accuracy is presented. The model integrates fixed blood test
probabilities with deep learning model predictions (DenseNet-201) for
ultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The
findings establish the viability of the combined model in enhancing diagnosis
accuracy and supporting early intervention in liver disease care.

</details>


### [213] [Open-set Anomaly Segmentation in Complex Scenarios](https://arxiv.org/abs/2504.19706)
*Song Xia, Yi Yu, Henghui Ding, Wenhan Yang, Shifei Liu, Alex C. Kot, Xudong Jiang*

Main category: cs.CV

TL;DR: The paper introduces ComsAmy, a benchmark for anomaly segmentation in adverse weather, and proposes DiffEEL, a method combining energy-entropy learning and diffusion-based data synthesis to improve model robustness.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks overlook adverse weather conditions, leading to unreliable evaluations for safety-critical applications like autonomous driving.

Method: Proposes DiffEEL, integrating energy-entropy learning and a diffusion-based data synthesizer to enhance anomaly segmentation.

Result: DiffEEL improves performance by 4.96% in AUPRC and 9.87% in FPR95 on benchmarks.

Conclusion: DiffEEL is a plug-and-play solution for robust anomaly segmentation in complex open-world scenarios.

Abstract: Precise segmentation of out-of-distribution (OoD) objects, herein referred to
as anomalies, is crucial for the reliable deployment of semantic segmentation
models in open-set, safety-critical applications, such as autonomous driving.
Current anomalous segmentation benchmarks predominantly focus on favorable
weather conditions, resulting in untrustworthy evaluations that overlook the
risks posed by diverse meteorological conditions in open-set environments, such
as low illumination, dense fog, and heavy rain. To bridge this gap, this paper
introduces the ComsAmy, a challenging benchmark specifically designed for
open-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide
spectrum of adverse weather conditions, dynamic driving environments, and
diverse anomaly types to comprehensively evaluate the model performance in
realistic open-world scenarios. Our extensive evaluation of several
state-of-the-art anomalous segmentation models reveals that existing methods
demonstrate significant deficiencies in such challenging scenarios,
highlighting their serious safety risks for real-world deployment. To solve
that, we propose a novel energy-entropy learning (EEL) strategy that integrates
the complementary information from energy and entropy to bolster the robustness
of anomaly segmentation under complex open-world environments. Additionally, a
diffusion-based anomalous training data synthesizer is proposed to generate
diverse and high-quality anomalous images to enhance the existing copy-paste
training data synthesizer. Extensive experimental results on both public and
ComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer
with energy and entropy learning (DiffEEL) serves as an effective and
generalizable plug-and-play method to enhance existing models, yielding an
average improvement of around 4.96% in $\rm{AUPRC}$ and 9.87% in
$\rm{FPR}_{95}$.

</details>


### [214] [Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration](https://arxiv.org/abs/2504.19847)
*Juhan Park, Kyungjae Lee, Hyung Jin Chang, Jungchan Cho*

Main category: cs.CV

TL;DR: Seg2HOI integrates segmentation models with HOI tasks, introducing quadruplets (HOI triplets + masks) and leveraging vision foundation model properties without extra training. It matches state-of-the-art performance and supports zero-shot and prompt-based applications.


<details>
  <summary>Details</summary>
Motivation: To enhance HOI detection by incorporating segmentation masks and leveraging vision foundation models' properties for more versatile and efficient HOI tasks.

Method: Seg2HOI combines segmentation-based vision models with an HOI decoder, using quadruplets (triplets + masks) and inheriting promptable/interactive features from foundation models.

Result: Achieves performance comparable to state-of-the-art methods on benchmark datasets, even in zero-shot scenarios.

Conclusion: Seg2HOI is versatile, generating HOI quadruplets and interactive segmentation from novel prompts, broadening its application potential.

Abstract: In this work, we introduce Segmentation to Human-Object Interaction
(\textit{\textbf{Seg2HOI}}) approach, a novel framework that integrates
segmentation-based vision foundation models with the human-object interaction
task, distinguished from traditional detection-based Human-Object Interaction
(HOI) methods. Our approach enhances HOI detection by not only predicting the
standard triplets but also introducing quadruplets, which extend HOI triplets
by including segmentation masks for human-object pairs. More specifically,
Seg2HOI inherits the properties of the vision foundation model (e.g.,
promptable and interactive mechanisms) and incorporates a decoder that applies
these attributes to HOI task. Despite training only for HOI, without additional
training mechanisms for these properties, the framework demonstrates that such
features still operate efficiently. Extensive experiments on two public
benchmark datasets demonstrate that Seg2HOI achieves performance comparable to
state-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that
Seg2HOI can generate HOI quadruplets and interactive HOI segmentation from
novel text and visual prompts that were not used during training, making it
versatile for a wide range of applications by leveraging this flexibility.

</details>


### [215] [A computer vision method to estimate ventilation rate of Atlantic salmon in sea fish farms](https://arxiv.org/abs/2504.19719)
*Lukas Folkman, Quynh LK Vo, Colin Johnston, Bela Stantic, Kylie A Pitt*

Main category: cs.CV

TL;DR: A computer vision method was developed to monitor Atlantic salmon ventilation rates in commercial sea fish farms, achieving high accuracy and potential for improving fish health monitoring.


<details>
  <summary>Details</summary>
Motivation: The need for innovative tools to monitor fish health in real-world aquaculture settings, as existing methods are limited to controlled environments.

Method: Uses a fish head detection model with a CNN to classify mouth states (open/closed) and multiple object tracking to estimate ventilation rates from underwater videos.

Result: Achieved a Pearson correlation coefficient of 0.82 between predicted and ground truth ventilation rates, demonstrating high efficiency.

Conclusion: The method can identify respiratory distress in fish, offering broad applicability and transformative potential for aquaculture health monitoring.

Abstract: The increasing demand for aquaculture production necessitates the development
of innovative, intelligent tools to effectively monitor and manage fish health
and welfare. While non-invasive video monitoring has become a common practice
in finfish aquaculture, existing intelligent monitoring methods predominantly
focus on assessing body condition or fish swimming patterns and are often
developed and evaluated in controlled tank environments, without demonstrating
their applicability to real-world aquaculture settings in open sea farms. This
underscores the necessity for methods that can monitor physiological traits
directly within the production environment of sea fish farms. To this end, we
have developed a computer vision method for monitoring ventilation rates of
Atlantic salmon (Salmo salar), which was specifically designed for videos
recorded in the production environment of commercial sea fish farms using the
existing infrastructure. Our approach uses a fish head detection model, which
classifies the mouth state as either open or closed using a convolutional
neural network. This is followed with multiple object tracking to create
temporal sequences of fish swimming across the field of view of the underwater
video camera to estimate ventilation rates. The method demonstrated high
efficiency, achieving a Pearson correlation coefficient of 0.82 between ground
truth and predicted ventilation rates in a test set of 100 fish collected
independently of the training data. By accurately identifying pens where fish
exhibit signs of respiratory distress, our method offers broad applicability
and the potential to transform fish health and welfare monitoring in finfish
aquaculture.

</details>


### [216] [The ATLAS of Traffic Lights: A Reliable Perception Framework for Autonomous Driving](https://arxiv.org/abs/2504.19722)
*Rupert Polley, Nikolai Polley, Dominik Heid, Marc Heinrich, Sven Ochs, J. Marius Zöllner*

Main category: cs.CV

TL;DR: A modularized traffic light perception framework for autonomous vehicles is proposed, leveraging a new dataset (ATLAS) and demonstrating improved accuracy and robustness in real-world testing.


<details>
  <summary>Details</summary>
Motivation: To enhance traffic light perception for autonomous vehicles, addressing dataset limitations and ensuring reliable real-time operation.

Method: Integration of state-of-the-art detection models with a real-time association and decision framework, trained and evaluated on the ATLAS dataset.

Result: Significant performance improvements in accuracy and robustness, validated in real-world autonomous driving scenarios.

Conclusion: The proposed framework is reliable and effective for real-time traffic light perception in autonomous vehicles.

Abstract: Traffic light perception is an essential component of the camera-based
perception system for autonomous vehicles, enabling accurate detection and
interpretation of traffic lights to ensure safe navigation through complex
urban environments. In this work, we propose a modularized perception framework
that integrates state-of-the-art detection models with a novel real-time
association and decision framework, enabling seamless deployment into an
autonomous driving stack. To address the limitations of existing public
datasets, we introduce the ATLAS dataset, which provides comprehensive
annotations of traffic light states and pictograms across diverse environmental
conditions and camera setups. This dataset is publicly available at
https://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art
traffic light detection architectures on ATLAS, demonstrating significant
performance improvements in both accuracy and robustness. Finally, we evaluate
the framework in real-world scenarios by deploying it in an autonomous vehicle
to make decisions at traffic light-controlled intersections, highlighting its
reliability and effectiveness for real-time operation.

</details>


### [217] [RepText: Rendering Visual Text via Replicating](https://arxiv.org/abs/2504.19724)
*Haofan Wang, Yujia Xu, Yimeng Li, Junchen Li, Chaowei Zhang, Jing Wang, Kejia Yang, Zhibo Chen*

Main category: cs.CV

TL;DR: RepText enhances text-to-image models to accurately render multilingual text without understanding it, using glyph and position integration, perceptual loss, and region masks.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with precise typographic elements, especially non-Latin alphabets. RepText addresses this by focusing on rendering rather than understanding text.

Method: RepText integrates glyph and position data, uses text perceptual loss, initializes with noisy glyph latent, and employs region masks to stabilize rendering.

Result: RepText outperforms open-source methods and matches closed-source models in multilingual text rendering.

Conclusion: RepText effectively improves multilingual text rendering but has limitations, as discussed.

Abstract: Although contemporary text-to-image generation models have achieved
remarkable breakthroughs in producing visually appealing images, their capacity
to generate precise and flexible typographic elements, especially non-Latin
alphabets, remains constrained. To address these limitations, we start from an
naive assumption that text understanding is only a sufficient condition for
text rendering, but not a necessary condition. Based on this, we present
RepText, which aims to empower pre-trained monolingual text-to-image generation
models with the ability to accurately render, or more precisely, replicate,
multilingual visual text in user-specified fonts, without the need to really
understand them. Specifically, we adopt the setting from ControlNet and
additionally integrate language agnostic glyph and position of rendered text to
enable generating harmonized visual text, allowing users to customize text
content, font and position on their needs. To improve accuracy, a text
perceptual loss is employed along with the diffusion loss. Furthermore, to
stabilize rendering process, at the inference phase, we directly initialize
with noisy glyph latent instead of random initialization, and adopt region
masks to restrict the feature injection to only the text region to avoid
distortion of the background. We conducted extensive experiments to verify the
effectiveness of our RepText relative to existing works, our approach
outperforms existing open-source methods and achieves comparable results to
native multi-language closed-source models. To be more fair, we also
exhaustively discuss its limitations in the end.

</details>


### [218] [Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast Videos via Physically Grounded Synthetic-to-Real Transfer](https://arxiv.org/abs/2504.19863)
*Daniel Kienzle, Robin Schön, Rainer Lienhart, Shin'Ichi Satoh*

Main category: cs.CV

TL;DR: A method to infer table tennis ball spin and 3D trajectory from 2D broadcast videos using synthetic data, achieving high accuracy without real training data.


<details>
  <summary>Details</summary>
Motivation: Spin is crucial for analyzing player technique but isn't directly observable in videos. The goal is to predict spin and trajectory from 2D video frames.

Method: Train a neural network on synthetic data with physically correct representations and targeted augmentations, eliminating the need for real-world training data.

Result: Achieves 92.0% accuracy in spin classification and a 2D reprojection error of 0.19% of the image diagonal.

Conclusion: The method successfully generalizes to real data using only synthetic training, pioneering spin and trajectory prediction in monocular videos.

Abstract: Analyzing a player's technique in table tennis requires knowledge of the
ball's 3D trajectory and spin. While, the spin is not directly observable in
standard broadcasting videos, we show that it can be inferred from the ball's
trajectory in the video. We present a novel method to infer the initial spin
and 3D trajectory from the corresponding 2D trajectory in a video. Without
ground truth labels for broadcast videos, we train a neural network solely on
synthetic data. Due to the choice of our input data representation, physically
correct synthetic training data, and using targeted augmentations, the network
naturally generalizes to real data. Notably, these simple techniques are
sufficient to achieve generalization. No real data at all is required for
training. To the best of our knowledge, we are the first to present a method
for spin and trajectory prediction in simple monocular broadcast videos,
achieving an accuracy of 92.0% in spin classification and a 2D reprojection
error of 0.19% of the image diagonal.

</details>


### [219] [Measuring Train Driver Performance as Key to Approval of Driverless Trains](https://arxiv.org/abs/2504.19735)
*Rustam Tagiew, Prasannavenkatesh Balaji*

Main category: cs.CV

TL;DR: The paper addresses the lack of published data on obstacle detection performance for driverless trains, providing a new dataset of 711 measurements to aid research and regulation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to fill the gap in quantifying obstacle detection performance for computer vision systems replacing human drivers in trains, as current data is insufficient.

Method: The method involves controlled experiments measuring reaction time and distance to obstacles under varying conditions like speed, obstacle size, and color contrast.

Result: The result is a comprehensive, public dataset of 711 performance measurements, aiding standardization and regulation.

Conclusion: The paper concludes by offering an unbiased dataset to improve research and regulatory frameworks for driverless train safety.

Abstract: Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation
(EU) No. 402/2013 allow a simplified approach for the safety approval of
computer vision systems for driverless trains, if they have 'similar' functions
and interfaces as the replaced human driver. The human driver is not replaced
one-to-one by a technical system - only a limited set of cognitive functions
are replaced. However, performance in the most challenging function, obstacle
detection, is difficult to quantify due to the deficiency of published
measurement results. This article summarizes the data published so far. This
article also goes a long way to remedy this situation by providing a new public
and anonymized dataset of 711 train driver performance measurements from
controlled experiments. The measurements are made for different speeds,
obstacle sizes, train protection systems and obstacle color contrasts
respectively. The measured values are reaction time and distance to the
obstacle. The goal of this paper is an unbiased and exhaustive description of
the presented dataset for research, standardization and regulation. Further
project related information including the dataset and source code is available
at https://atosense-02371c.usercontent.opencode.de/

</details>


### [220] [Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning](https://arxiv.org/abs/2504.19900)
*Han Chen, Anne L. Martel*

Main category: cs.CV

TL;DR: MVPT-NET, a Multi-view Visual Prompt Tuning Network, improves breast cancer detection by efficiently integrating multi-view mammogram data with minimal parameter tuning, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Breast cancer detection from high-resolution mammograms benefits from multi-view data, but existing methods face challenges in scalability and efficiency.

Method: Pretrains a single-view model, then adapts multi-view learning via prompt tuning, tuning only 7% of parameters to retain robustness.

Result: Achieves AUROC of 0.852 for classifying Benign, DCIS, and Invasive classes, outperforming conventional methods.

Conclusion: MVPT-NET offers a scalable, efficient solution for multi-view mammogram analysis, advancing breast cancer detection.

Abstract: Accurate detection of breast cancer from high-resolution mammograms is
crucial for early diagnosis and effective treatment planning. Previous studies
have shown the potential of using single-view mammograms for breast cancer
detection. However, incorporating multi-view data can provide more
comprehensive insights. Multi-view classification, especially in medical
imaging, presents unique challenges, particularly when dealing with
large-scale, high-resolution data. In this work, we propose a novel Multi-view
Visual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening
mammograms. We first pretrain a robust single-view classification model on
high-resolution mammograms and then innovatively adapt multi-view feature
learning into a task-specific prompt tuning process. This technique selectively
tunes a minimal set of trainable parameters (7\%) while retaining the
robustness of the pre-trained single-view model, enabling efficient integration
of multi-view data without the need for aggressive downsampling. Our approach
offers an efficient alternative to traditional feature fusion methods,
providing a more robust, scalable, and efficient solution for high-resolution
mammogram analysis. Experimental results on a large multi-institution dataset
demonstrate that our method outperforms conventional approaches while
maintaining detection efficiency, achieving an AUROC of 0.852 for
distinguishing between Benign, DCIS, and Invasive classes. This work highlights
the potential of MVPT-NET for medical imaging tasks and provides a scalable
solution for integrating multi-view data in breast cancer detection.

</details>


### [221] [CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis](https://arxiv.org/abs/2504.19737)
*Abhishek Kuriyal, Elliot Vincent, Mathieu Aubry, Loic Landrieu*

Main category: cs.CV

TL;DR: A domain-generalization framework for satellite images uses multiple expert models and a selection module to improve performance across diverse terrains.


<details>
  <summary>Details</summary>
Motivation: Global terrain variations in satellite images cause poor model generalization, even with large datasets.

Method: Train one expert model per training domain, learn expert similarity, ensure consistency among similar experts, and use a model selection module to aggregate predictions.

Result: Outperforms existing domain generalization and adaptation methods on four datasets (DynamicEarthNet, MUDS, OSCD, FMoW).

Conclusion: The proposed framework effectively addresses terrain variation challenges in satellite image analysis.

Abstract: Global variations in terrain appearance raise a major challenge for satellite
image analysis, leading to poor model performance when training on locations
that differ from those encountered at test time. This remains true even with
recent large global datasets. To address this challenge, we propose a novel
domain-generalization framework for satellite images. Instead of trying to
learn a single generalizable model, we train one expert model per training
domain, while learning experts' similarity and encouraging similar experts to
be consistent. A model selection module then identifies the most suitable
experts for a given test sample and aggregates their predictions. Experiments
on four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent
gains over existing domain generalization and adaptation methods. Our code is
publicly available at https://github.com/Abhishek19009/CoDEx.

</details>


### [222] [Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI](https://arxiv.org/abs/2504.19918)
*Hugo Georgenthum, Cristian Cosentino, Fabrizio Marozzo, Pietro Liò*

Main category: cs.CV

TL;DR: A novel multi-modal framework for surgical video summarization using AI, achieving high precision in tool detection and context summarization.


<details>
  <summary>Details</summary>
Motivation: To enhance procedural documentation, surgical training, and post-operative analysis through automated video summarization.

Method: A three-stage approach: frame-level feature extraction, clip-level summarization with temporal context, and full-report generation using tailored LLMs.

Result: 96% precision in tool detection and a BERT score of 0.74 for temporal context summarization on the CholecT50 dataset.

Conclusion: Advances AI-assisted surgical reporting, improving clinical documentation reliability.

Abstract: The automatic summarization of surgical videos is essential for enhancing
procedural documentation, supporting surgical training, and facilitating
post-operative analysis. This paper presents a novel method at the intersection
of artificial intelligence and medicine, aiming to develop machine learning
models with direct real-world applications in surgical contexts. We propose a
multi-modal framework that leverages recent advancements in computer vision and
large language models to generate comprehensive video summaries. % The approach
is structured in three key stages. First, surgical videos are divided into
clips, and visual features are extracted at the frame level using visual
transformers. This step focuses on detecting tools, tissues, organs, and
surgical actions. Second, the extracted features are transformed into
frame-level captions via large language models. These are then combined with
temporal features, captured using a ViViT-based encoder, to produce clip-level
summaries that reflect the broader context of each video segment. Finally, the
clip-level descriptions are aggregated into a full surgical report using a
dedicated LLM tailored for the summarization task. % We evaluate our method on
the CholecT50 dataset, using instrument and action annotations from 50
laparoscopic videos. The results show strong performance, achieving 96\%
precision in tool detection and a BERT score of 0.74 for temporal context
summarization. This work contributes to the advancement of AI-assisted tools
for surgical reporting, offering a step toward more intelligent and reliable
clinical documentation.

</details>


### [223] [Contrastive Language-Image Learning with Augmented Textual Prompts for 3D/4D FER Using Vision-Language Model](https://arxiv.org/abs/2504.19739)
*Muzammil Behzad, Guoying Zhao*

Main category: cs.CV

TL;DR: AffectVLM is a vision-language model for facial emotion understanding from 3D/4D data, using joint representation learning and a novel loss function for faster convergence. It includes augmented textual prompts, mixed view augmentation, and a Streamlit app for real-time interaction.


<details>
  <summary>Details</summary>
Motivation: To achieve a semantically rich and visually comprehensive understanding of facial emotions by integrating multiviews from 3D/4D data.

Method: Joint representation learning with a gradient-friendly loss function, augmented textual prompts, mixed view augmentation, and distributed learning.

Result: Superior performance across multiple benchmarks.

Conclusion: AffectVLM effectively integrates multiviews and enhances linguistic and visual capabilities for facial emotion analysis.

Abstract: In this paper, we introduce AffectVLM, a vision-language model designed to
integrate multiviews for a semantically rich and visually comprehensive
understanding of facial emotions from 3D/4D data. To effectively capture visual
features, we propose a joint representation learning framework paired with a
novel gradient-friendly loss function that accelerates model convergence
towards optimal feature representation. Additionally, we introduce augmented
textual prompts to enhance the model's linguistic capabilities and employ mixed
view augmentation to expand the visual dataset. We also develop a Streamlit app
for a real-time interactive inference and enable the model for distributed
learning. Extensive experiments validate the superior performance of AffectVLM
across multiple benchmarks.

</details>


### [224] [EcoWikiRS: Learning Ecological Representation of Satellite Images from Weak Supervision with Species Observations and Wikipedia](https://arxiv.org/abs/2504.19742)
*Valerie Zermatten, Javiera Castillo-Navarro, Pallavi Jain, Devis Tuia, Diego Marcos*

Main category: cs.CV

TL;DR: The paper proposes a method to predict ecological properties from remote sensing images using species habitat descriptions, introducing the EcoWikiRS dataset and WINCEL, a weighted InfoNCE loss, for improved ecological understanding.


<details>
  <summary>Details</summary>
Motivation: To leverage species habitat descriptions for ecological insights from remote sensing images, addressing weak and noisy supervision.

Method: Aligns remote sensing images with species habitat descriptions using the EcoWikiRS dataset and WINCEL, a weighted InfoNCE loss.

Result: Improves ecosystem zero-shot classification, aligning with EUNIS habitat definitions.

Conclusion: The approach enhances ecological understanding of remote sensing images, with code and dataset publicly available.

Abstract: The presence of species provides key insights into the ecological properties
of a location such as land cover, climatic conditions or even soil properties.
We propose a method to predict such ecological properties directly from remote
sensing (RS) images by aligning them with species habitat descriptions. We
introduce the EcoWikiRS dataset, consisting of high-resolution aerial images,
the corresponding geolocated species observations, and, for each species, the
textual descriptions of their habitat from Wikipedia. EcoWikiRS offers a
scalable way of supervision for RS vision language models (RS-VLMs) for
ecology. This is a setting with weak and noisy supervision, where, for
instance, some text may describe properties that are specific only to part of
the species' niche or is irrelevant to a specific image. We tackle this by
proposing WINCEL, a weighted version of the InfoNCE loss. We evaluate our model
on the task of ecosystem zero-shot classification by following the habitat
definitions from the European Nature Information System (EUNIS). Our results
show that our approach helps in understanding RS images in a more ecologically
meaningful manner. The code and the dataset are available at
https://github.com/eceo-epfl/EcoWikiRS.

</details>


### [225] [STCOcc: Sparse Spatial-Temporal Cascade Renovation for 3D Occupancy and Scene Flow Prediction](https://arxiv.org/abs/2504.19749)
*Zhimin Liao, Ping Wei, Shuaijia Chen, Haoxuan Wang, Ziyang Ren*

Main category: cs.CV

TL;DR: A novel explicit state-based method improves 3D occupancy and scene flow prediction by leveraging occupied states, reducing computational costs, and enhancing spatial details.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of implicit learning-based approaches in capturing local details and spatial discriminative ability in 3D scene representation.

Method: Proposes a sparse occlusion-aware attention mechanism with cascade refinement and a method for modeling long-term dynamic interactions.

Result: Superior performance in RayIoU and mAVE metrics, with reduced GPU memory usage (8.7GB).

Conclusion: The explicit state-based approach effectively enhances 3D feature renovation and dynamic interaction modeling, outperforming previous methods.

Abstract: 3D occupancy and scene flow offer a detailed and dynamic representation of 3D
scene. Recognizing the sparsity and complexity of 3D space, previous
vision-centric methods have employed implicit learning-based approaches to
model spatial and temporal information. However, these approaches struggle to
capture local details and diminish the model's spatial discriminative ability.
To address these challenges, we propose a novel explicit state-based modeling
method designed to leverage the occupied state to renovate the 3D features.
Specifically, we propose a sparse occlusion-aware attention mechanism,
integrated with a cascade refinement strategy, which accurately renovates 3D
features with the guidance of occupied state information. Additionally, we
introduce a novel method for modeling long-term dynamic interactions, which
reduces computational costs and preserves spatial information. Compared to the
previous state-of-the-art methods, our efficient explicit renovation strategy
not only delivers superior performance in terms of RayIoU and mAVE for
occupancy and scene flow prediction but also markedly reduces GPU memory usage
during training, bringing it down to 8.7GB. Our code is available on
https://github.com/lzzzzzm/STCOcc

</details>


### [226] [Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video](https://arxiv.org/abs/2504.19819)
*Hoang Chuong Nguyen, Wei Mao, Jose M. Alvarez, Miaomiao Liu*

Main category: cs.CV

TL;DR: A novel method improves NeRF by learning continuous camera motions without relying on accurate initial poses or depth priors, achieving superior pose and depth estimation.


<details>
  <summary>Details</summary>
Motivation: NeRF requires accurate camera poses for training, which is limiting. Existing methods struggle with large rotations or lack of priors.

Method: Models camera motions as time-dependent velocities, learns relative motions first, and aggregates them to define poses. Uses a time-dependent NeRF for local geometry and motion.

Result: Outperforms state-of-the-art in camera pose and depth estimation on Co3D and Scannet, with comparable novel-view synthesis.

Conclusion: The method eliminates dependency on pose priors, enabling robust NeRF training in challenging scenarios.

Abstract: Neural Radiance Fields (NeRF) has demonstrated its superior capability to
represent 3D geometry but require accurately precomputed camera poses during
training. To mitigate this requirement, existing methods jointly optimize
camera poses and NeRF often relying on good pose initialisation or depth
priors. However, these approaches struggle in challenging scenarios, such as
large rotations, as they map each camera to a world coordinate system. We
propose a novel method that eliminates prior dependencies by modeling
continuous camera motions as time-dependent angular velocity and velocity.
Relative motions between cameras are learned first via velocity integration,
while camera poses can be obtained by aggregating such relative motions up to a
world coordinate system defined at a single time step within the video.
Specifically, accurate continuous camera movements are learned through a
time-dependent NeRF, which captures local scene geometry and motion by training
from neighboring frames for each time step. The learned motions enable
fine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D
and Scannet show our approach achieves superior camera pose and depth
estimation and comparable novel-view synthesis performance compared to
state-of-the-art methods. Our code is available at
https://github.com/HoangChuongNguyen/cope-nerf.

</details>


### [227] [Taming the Randomness: Towards Label-Preserving Cropping in Contrastive Learning](https://arxiv.org/abs/2504.19824)
*Mohamed Hassan, Mohammad Wasil, Sebastian Houben*

Main category: cs.CV

TL;DR: The paper introduces two novel parameterized cropping methods for contrastive learning to improve self-labeling robustness and model accuracy.


<details>
  <summary>Details</summary>
Motivation: Random augmentations in contrastive learning can lead to semantically distant images, causing false labeling and reducing efficacy.

Method: Two parameterized cropping methods are proposed to enhance self-labeling robustness.

Result: The methods improve model accuracy by 2.7% to 12.4% on CIFAR-10 classification.

Conclusion: Parameterized cropping methods significantly enhance contrastive learning efficacy by reducing false labeling.

Abstract: Contrastive learning (CL) approaches have gained great recognition as a very
successful subset of self-supervised learning (SSL) methods. SSL enables
learning from unlabeled data, a crucial step in the advancement of deep
learning, particularly in computer vision (CV), given the plethora of unlabeled
image data. CL works by comparing different random augmentations (e.g.,
different crops) of the same image, thus achieving self-labeling. Nevertheless,
randomly augmenting images and especially random cropping can result in an
image that is semantically very distant from the original and therefore leads
to false labeling, hence undermining the efficacy of the methods. In this
research, two novel parameterized cropping methods are introduced that increase
the robustness of self-labeling and consequently increase the efficacy. The
results show that the use of these methods significantly improves the accuracy
of the model by between 2.7\% and 12.4\% on the downstream task of classifying
CIFAR-10, depending on the crop size compared to that of the non-parameterized
random cropping method.

</details>


### [228] [HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination](https://arxiv.org/abs/2504.19828)
*Zhiming Hu, Daniel Haeufle, Syn Schmitt, Andreas Bulling*

Main category: cs.CV

TL;DR: HOIGaze is a learning-based method for gaze estimation in XR during hand-object interactions, leveraging eye-hand-head coordination to denoise training data and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing gaze estimation methods treat all training samples equally, ignoring the coordination between eye, hand, and head movements during HOIs. HOIGaze aims to exploit this coordination for improved accuracy.

Method: 1) Hierarchical framework for attended hand recognition and gaze estimation. 2) Cross-modal Transformers for fusing head and hand-object features. 3) Eye-head coordination loss for better training samples.

Result: HOIGaze achieves 15.6% and 6.0% improvements in mean angular error on HOT3D and ADT datasets, respectively, and enhances eye-based activity recognition.

Conclusion: Eye-hand-head coordination provides valuable information for gaze estimation, opening a new direction for learning-based methods.

Abstract: We present HOIGaze - a novel learning-based approach for gaze estimation
during hand-object interactions (HOI) in extended reality (XR). HOIGaze
addresses the challenging HOI setting by building on one key insight: The eye,
hand, and head movements are closely coordinated during HOIs and this
coordination can be exploited to identify samples that are most useful for gaze
estimator training - as such, effectively denoising the training data. This
denoising approach is in stark contrast to previous gaze estimation methods
that treated all training samples as equal. Specifically, we propose: 1) a
novel hierarchical framework that first recognises the hand currently visually
attended to and then estimates gaze direction based on the attended hand; 2) a
new gaze estimator that uses cross-modal Transformers to fuse head and
hand-object features extracted using a convolutional neural network and a
spatio-temporal graph convolutional network; and 3) a novel eye-head
coordination loss that upgrades training samples belonging to the coordinated
eye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin
(ADT) datasets and show that it significantly outperforms state-of-the-art
methods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in
mean angular error. To demonstrate the potential of our method, we further
report significant performance improvements for the sample downstream task of
eye-based activity recognition on ADT. Taken together, our results underline
the significant information content available in eye-hand-head coordination
and, as such, open up an exciting new direction for learning-based gaze
estimation.

</details>


### [229] [AnimateAnywhere: Rouse the Background in Human Image Animation](https://arxiv.org/abs/2504.19834)
*Xiaoyu Liu, Mingshuai Yao, Yabo Zhang, Xianhui Lin, Peiran Ren, Xiaoming Li, Ming Liu, Wangmeng Zuo*

Main category: cs.CV

TL;DR: AnimateAnywhere framework generates human videos with dynamic backgrounds by learning background motions from human pose sequences, avoiding the need for camera trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect background generation, leading to static or inharmonious results. Preparing camera trajectories is impractical for users.

Method: Uses a background motion learner (BML) to infer background motion from human poses and applies an epipolar constraint on 3D attention maps for accuracy.

Result: Achieves state-of-the-art performance in generating realistic human animations with dynamic backgrounds.

Conclusion: AnimateAnywhere effectively learns background motion from poses, enhancing animation realism without requiring camera trajectories.

Abstract: Human image animation aims to generate human videos of given characters and
backgrounds that adhere to the desired pose sequence. However, existing methods
focus more on human actions while neglecting the generation of background,
which typically leads to static results or inharmonious movements. The
community has explored camera pose-guided animation tasks, yet preparing the
camera trajectory is impractical for most entertainment applications and
ordinary users. As a remedy, we present an AnimateAnywhere framework, rousing
the background in human image animation without requirements on camera
trajectories. In particular, based on our key insight that the movement of the
human body often reflects the motion of the background, we introduce a
background motion learner (BML) to learn background motions from human pose
sequences. To encourage the model to learn more accurate cross-frame
correspondences, we further deploy an epipolar constraint on the 3D attention
map. Specifically, the mask used to suppress geometrically unreasonable
attention is carefully constructed by combining an epipolar mask and the
current 3D attention map. Extensive experiments demonstrate that our
AnimateAnywhere effectively learns the background motion from human pose
sequences, achieving state-of-the-art performance in generating human animation
results with vivid and realistic backgrounds. The source code and model will be
available at https://github.com/liuxiaoyu1104/AnimateAnywhere.

</details>


### [230] [SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail UHR Satellite Image Segmentation](https://arxiv.org/abs/2504.19839)
*Yulong Guo, Zilun Zhang, Yongheng Shang, Tiancheng Zhao, Shuiguang Deng, Yingchun Yang, Jianwei Yin*

Main category: cs.CV

TL;DR: The paper introduces SRMF, a framework for semantic segmentation in UHR satellite imagery, addressing the long-tail problem through data augmentation and multimodal feature fusion, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The long-tail problem in UHR satellite imagery semantic segmentation is often overlooked, despite its impact on performance. The paper aims to address this by focusing on data augmentation and multimodal fusion.

Method: The SRMF framework uses multi-scale cropping and semantic reordering/resampling for data augmentation, alongside a novel multimodal fusion method combining text and visual features without region-specific text descriptions.

Result: Experiments on URUR, GID, and FBP datasets show mIoU improvements of 3.33%, 0.66%, and 0.98%, respectively, achieving state-of-the-art performance.

Conclusion: SRMF effectively tackles the long-tail problem in UHR semantic segmentation, outperforming existing methods through innovative data augmentation and multimodal fusion.

Abstract: The long-tail problem presents a significant challenge to the advancement of
semantic segmentation in ultra-high-resolution (UHR) satellite imagery. While
previous efforts in UHR semantic segmentation have largely focused on
multi-branch network architectures that emphasize multi-scale feature
extraction and fusion, they have often overlooked the importance of addressing
the long-tail issue. In contrast to prior UHR methods that focused on
independent feature extraction, we emphasize data augmentation and multimodal
feature fusion to alleviate the long-tail problem. In this paper, we introduce
SRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our
approach addresses the long-tail class distribution by incorporating a
multi-scale cropping technique alongside a data augmentation strategy based on
semantic reordering and resampling. To further enhance model performance, we
propose a multimodal fusion-based general representation knowledge injection
method, which, for the first time, fuses text and visual features without the
need for individual region text descriptions, extracting more robust features.
Extensive experiments on the URUR, GID, and FBP datasets demonstrate that our
method improves mIoU by 3.33\%, 0.66\%, and 0.98\%, respectively, achieving
state-of-the-art performance. Code is available at:
https://github.com/BinSpa/SRMF.git.

</details>


### [231] [Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery](https://arxiv.org/abs/2504.19996)
*Andreas Kalogeras, Dimitrios Bormpoudakis, Iason Tsardanidis, Dimitra A. Loka, Charalampos Kontoes*

Main category: cs.CV

TL;DR: The study uses Sentinel-2 satellite imagery and ML models to detect digestate application in agriculture, achieving high accuracy and highlighting its potential for scalable monitoring.


<details>
  <summary>Details</summary>
Motivation: To assess the effects of Exogenous Organic Matter (EOM) on soil and crop health, particularly digestate application, which poses environmental risks.

Method: Analyzed Sentinel-2 satellite imagery using specific indices (EOMI, NDVI, EVI) and employed ML models (Random Forest, k-NN, Gradient Boosting, Feed-Forward Neural Network) for detection.

Result: Achieved F1-scores up to 0.85, demonstrating effective detection of digestate presence.

Conclusion: Combining remote sensing and ML offers scalable, cost-effective monitoring for precision agriculture and sustainability.

Abstract: The widespread use of Exogenous Organic Matter in agriculture necessitates
monitoring to assess its effects on soil and crop health. This study evaluates
optical Sentinel-2 satellite imagery for detecting digestate application, a
practice that enhances soil fertility but poses environmental risks like
microplastic contamination and nitrogen losses. In the first instance,
Sentinel-2 satellite image time series (SITS) analysis of specific indices
(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after
application on the soils of four different crop types in Thessaly, Greece.
Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient
Boosting and a Feed-Forward Neural Network), were used to investigate digestate
presence detection, achieving F1-scores up to 0.85. The findings highlight the
potential of combining remote sensing and ML for scalable and cost-effective
monitoring of EOM applications, supporting precision agriculture and
sustainability.

</details>


### [232] [CoherenDream: Boosting Holistic Text Coherence in 3D Generation via Multimodal Large Language Models Feedback](https://arxiv.org/abs/2504.19860)
*Chenhan Jiang, Yihan Zeng, Hang Xu, Dit-Yan Yeung*

Main category: cs.CV

TL;DR: The paper introduces Textual Coherent Score Distillation (TCSD) to improve text-to-3D generation by addressing semantic fidelity issues in SDS-based methods, leveraging MLLMs for alignment feedback, and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing SDS-based methods struggle with semantic fidelity for complex prompts, especially for multiple objects with intricate interactions, due to view-independent biases.

Method: Proposes TCSD, integrating MLLM feedback for text-3D alignment, develops 3DLLaVA-CRITIC for evaluation, and introduces LLM-layout initialization for faster convergence.

Result: CoherenDream achieves state-of-the-art performance in text-aligned 3D generation, preserving textual consistency and semantic interactions.

Conclusion: TCSD effectively addresses SDS limitations by leveraging MLLMs, demonstrating superior performance and opening new avenues for MLLM integration in 3D generation.

Abstract: Score Distillation Sampling (SDS) has achieved remarkable success in
text-to-3D content generation. However, SDS-based methods struggle to maintain
semantic fidelity for user prompts, particularly when involving multiple
objects with intricate interactions. While existing approaches often address 3D
consistency through multiview diffusion model fine-tuning on 3D datasets, this
strategy inadvertently exacerbates text-3D alignment degradation. The
limitation stems from SDS's inherent accumulation of view-independent biases
during optimization, which progressively diverges from the ideal text alignment
direction. To alleviate this limitation, we propose a novel SDS objective,
dubbed as Textual Coherent Score Distillation (TCSD), which integrates
alignment feedback from multimodal large language models (MLLMs). Our TCSD
leverages cross-modal understanding capabilities of MLLMs to assess and guide
the text-3D correspondence during the optimization. We further develop
3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text
alignment in 3D generations. Additionally, we introduce an LLM-layout
initialization that significantly accelerates optimization convergence through
semantic-aware spatial configuration. Comprehensive evaluations demonstrate
that our framework, CoherenDream, establishes state-of-the-art performance in
text-aligned 3D generation across multiple benchmarks, including T$^3$Bench and
TIFA subset. Qualitative results showcase the superior performance of
CoherenDream in preserving textual consistency and semantic interactions. As
the first study to incorporate MLLMs into SDS optimization, we also conduct
extensive ablation studies to explore optimal MLLM adaptations for 3D
generation tasks.

</details>


### [233] [DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images](https://arxiv.org/abs/2504.19876)
*Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, Abdenour Hadid*

Main category: cs.CV

TL;DR: DeeCLIP is a framework for detecting AI-generated images using CLIP-ViT and fusion learning, improving robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods struggle with generalization and sensitivity to perturbations, prompting the need for a more robust solution.

Method: DeeCLIP uses DeeFuser for feature fusion, triplet loss for embedding refinement, and LoRA for parameter-efficient fine-tuning.

Result: Achieves 89.00% accuracy on 19 test subsets, outperforming existing methods with fewer parameters.

Conclusion: DeeCLIP demonstrates superior robustness and generalization, with potential for lightweight adaptation.

Abstract: This paper introduces DeeCLIP, a novel framework for detecting AI-generated
images using CLIP-ViT and fusion learning. Despite significant advancements in
generative models capable of creating highly photorealistic images, existing
detection methods often struggle to generalize across different models and are
highly sensitive to minor perturbations. To address these challenges, DeeCLIP
incorporates DeeFuser, a fusion module that combines high-level and low-level
features, improving robustness against degradations such as compression and
blurring. Additionally, we apply triplet loss to refine the embedding space,
enhancing the model's ability to distinguish between real and synthetic
content. To further enable lightweight adaptation while preserving pre-trained
knowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation
(LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot
learning without sacrificing generalization. Trained exclusively on 4-class
ProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets
composed of generative adversarial network (GAN) and diffusion models. Despite
having fewer trainable parameters, DeeCLIP outperforms existing methods,
demonstrating superior robustness against various generative models and
real-world distortions. The code is publicly available at
https://github.com/Mamadou-Keita/DeeCLIP for research purposes.

</details>


### [234] [Using Fixed and Mobile Eye Tracking to Understand How Visitors View Art in a Museum: A Study at the Bowes Museum, County Durham, UK](https://arxiv.org/abs/2504.19881)
*Claire Warwick, Andrew Beresford, Soazig Casteau, Hubert P. H. Shum, Dan Smith, Francis Xiatian Zhang*

Main category: cs.CV

TL;DR: Researchers used eye tracking to study how museum visitors view art, aiming to improve display effectiveness.


<details>
  <summary>Details</summary>
Motivation: To understand visitor engagement with art in a gallery setting and enhance museum display strategies.

Method: Fixed and mobile eye tracking during summer 2024, involving an interdisciplinary team.

Result: Insights into visitor viewing behavior to inform better art display recommendations.

Conclusion: Findings will help the museum optimize collections display for improved visitor engagement.

Abstract: The following paper describes a collaborative project involving researchers
at Durham University, and professionals at the Bowes Museum, Barnard Castle,
County Durham, UK, during which we used fixed and mobile eye tracking to
understand how visitors view art. Our study took place during summer 2024 and
builds on work presented at DH2017 (Bailey-Ross et al., 2017). Our
interdisciplinary team included researchers from digital humanities,
psychology, art history and computer science, working in collaboration with
professionals from the museum. We used fixed and mobile eye tracking to
understand how museum visitors view art in a physical gallery setting. This
research will enable us to make recommendations about how the Museum's
collections could be more effectively displayed, encouraging visitors to engage
with them more fully.

</details>


### [235] [LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields](https://arxiv.org/abs/2504.20026)
*Zhengqin Li, Dilin Wang, Ka Chen, Zhaoyang Lv, Thu Nguyen-Phuoc, Milim Lee, Jia-Bin Huang, Lei Xiao, Cheng Zhang, Yufeng Zhu, Carl S. Marshall, Yufeng Ren, Richard Newcombe, Zhao Dong*

Main category: cs.CV

TL;DR: LIRM is a transformer-based model for fast, high-quality 3D reconstruction of shape, materials, and radiance fields, addressing limitations of existing methods with novel contributions like progressive view updates, hexa-plane neural SDF, and directional-embedding for view-dependent effects.


<details>
  <summary>Details</summary>
Motivation: Existing Large Reconstruction Models (LRMs) lack accuracy in unseen parts, fail to recover glossy appearance, and cannot generate relightable 3D content. LIRM aims to overcome these limitations for practical multi-view 3D reconstruction.

Method: LIRM introduces three key techniques: 1) progressive view updates, 2) hexa-plane neural SDF for detailed textures and geometry, and 3) neural directional-embedding for view-dependent effects. It uses a coarse-to-fine training scheme on a large dataset.

Result: LIRM outperforms dense-view inverse rendering methods in geometry and relighting accuracy while being significantly faster, achieving high-quality reconstructions in under a second.

Conclusion: LIRM provides a practical, efficient solution for high-quality 3D reconstruction with relightable outputs, advancing the state-of-the-art in inverse rendering.

Abstract: We present Large Inverse Rendering Model (LIRM), a transformer architecture
that jointly reconstructs high-quality shape, materials, and radiance fields
with view-dependent effects in less than a second. Our model builds upon the
recent Large Reconstruction Models (LRMs) that achieve state-of-the-art
sparse-view reconstruction quality. However, existing LRMs struggle to
reconstruct unseen parts accurately and cannot recover glossy appearance or
generate relightable 3D contents that can be consumed by standard Graphics
engines. To address these limitations, we make three key technical
contributions to build a more practical multi-view 3D reconstruction framework.
First, we introduce an update model that allows us to progressively add more
input views to improve our reconstruction. Second, we propose a hexa-plane
neural SDF representation to better recover detailed textures, geometry and
material parameters. Third, we develop a novel neural directional-embedding
mechanism to handle view-dependent effects. Trained on a large-scale shape and
material dataset with a tailored coarse-to-fine training scheme, our model
achieves compelling results. It compares favorably to optimization-based
dense-view inverse rendering methods in terms of geometry and relighting
accuracy, while requiring only a fraction of the inference time.

</details>


### [236] [Federated Out-of-Distribution Generalization: A Causal Augmentation View](https://arxiv.org/abs/2504.19882)
*Runhui Zhang, Sijin Zhou, Zhuang Qi*

Main category: cs.CV

TL;DR: FedCAug introduces causality-inspired data augmentation in federated learning to break spurious correlations, enhancing model generalization without compromising privacy.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning methods struggle with data bias and limited contextual information, hindering performance on out-of-distribution samples.

Method: FedCAug uses causal region localization and augmentation to decouple background and objects, generating counterfactual samples without inter-client data sharing.

Result: FedCAug reduces reliance on background for predictions and outperforms state-of-the-art methods on three datasets.

Conclusion: FedCAug effectively addresses data bias and privacy concerns in federated learning, improving model generalization.

Abstract: Federated learning aims to collaboratively model by integrating multi-source
information to obtain a model that can generalize across all client data.
Existing methods often leverage knowledge distillation or data augmentation to
mitigate the negative impact of data bias across clients. However, the limited
performance of teacher models on out-of-distribution samples and the inherent
quality gap between augmented and original data hinder their effectiveness and
they typically fail to leverage the advantages of incorporating rich contextual
information. To address these limitations, this paper proposes a Federated
Causal Augmentation method, termed FedCAug, which employs causality-inspired
data augmentation to break the spurious correlation between attributes and
categories. Specifically, it designs a causal region localization module to
accurately identify and decouple the background and objects in the image,
providing rich contextual information for causal data augmentation.
Additionally, it designs a causality-inspired data augmentation module that
integrates causal features and within-client context to generate counterfactual
samples. This significantly enhances data diversity, and the entire process
does not require any information sharing between clients, thereby contributing
to the protection of data privacy. Extensive experiments conducted on three
datasets reveal that FedCAug markedly reduces the model's reliance on
background to predict sample labels, achieving superior performance compared to
state-of-the-art methods.

</details>


### [237] [Enhancing breast cancer detection on screening mammogram using self-supervised learning and a hybrid deep model of Swin Transformer and Convolutional Neural Network](https://arxiv.org/abs/2504.19888)
*Han Chen, Anne L. Martel*

Main category: cs.CV

TL;DR: A novel method, HybMNet, combines self-supervised learning and a hybrid deep model to improve breast cancer detection in mammograms, achieving high AUC scores.


<details>
  <summary>Details</summary>
Motivation: The scarcity of labeled medical data limits AI applications in breast cancer diagnosis, prompting the need for efficient training methods.

Method: A two-stage process: SSL pretraining with EsViT and Swin-T, followed by downstream training with HybMNet (Swin-T + CNN) and a fusion module.

Result: Achieved AUC of 0.864 on CMMD and 0.889 on INbreast datasets, demonstrating strong performance.

Conclusion: HybMNet effectively addresses data scarcity and enhances breast cancer detection accuracy.

Abstract: Purpose: The scarcity of high-quality curated labeled medical training data
remains one of the major limitations in applying artificial intelligence (AI)
systems to breast cancer diagnosis. Deep models for mammogram analysis and mass
(or micro-calcification) detection require training with a large volume of
labeled images, which are often expensive and time-consuming to collect. To
reduce this challenge, we proposed a novel method that leverages
self-supervised learning (SSL) and a deep hybrid model, named \textbf{HybMNet},
which combines local self-attention and fine-grained feature extraction to
enhance breast cancer detection on screening mammograms.
  Approach: Our method employs a two-stage learning process: (1) SSL
Pretraining: We utilize EsViT, a SSL technique, to pretrain a Swin Transformer
(Swin-T) using a limited set of mammograms. The pretrained Swin-T then serves
as the backbone for the downstream task. (2) Downstream Training: The proposed
HybMNet combines the Swin-T backbone with a CNN-based network and a novel
fusion strategy. The Swin-T employs local self-attention to identify
informative patch regions from the high-resolution mammogram, while the
CNN-based network extracts fine-grained local features from the selected
patches. A fusion module then integrates global and local information from both
networks to generate robust predictions. The HybMNet is trained end-to-end,
with the loss function combining the outputs of the Swin-T and CNN modules to
optimize feature extraction and classification performance.
  Results: The proposed method was evaluated for its ability to detect breast
cancer by distinguishing between benign (normal) and malignant mammograms.
Leveraging SSL pretraining and the HybMNet model, it achieved AUC of 0.864 (95%
CI: 0.852, 0.875) on the CMMD dataset and 0.889 (95% CI: 0.875, 0.903) on the
INbreast dataset, highlighting its effectiveness.

</details>


### [238] [CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition](https://arxiv.org/abs/2504.19894)
*Quynh Phung, Long Mai, Fabian David Caba Heilbron, Feng Liu, Jia-Bin Huang, Cusuh Ham*

Main category: cs.CV

TL;DR: CineVerse is a framework for cinematic scene composition, addressing challenges like multi-character interactions and visual effects. It uses a two-stage approach: LLM-based planning and text-to-image synthesis, showing promising results.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of cinematic scene generation, such as consistency, multiple characters, and visual effects, which traditional multi-shot generation lacks.

Method: A two-stage approach: 1) LLM generates detailed scene and shot plans from high-level descriptions; 2) Text-to-image model synthesizes keyframes.

Result: CineVerse improves visual coherence and contextual richness in movie scenes.

Conclusion: The framework shows potential for advancing cinematic video synthesis.

Abstract: We present CineVerse, a novel framework for the task of cinematic scene
composition. Similar to traditional multi-shot generation, our task emphasizes
the need for consistency and continuity across frames. However, our task also
focuses on addressing challenges inherent to filmmaking, such as multiple
characters, complex interactions, and visual cinematic effects. In order to
learn to generate such content, we first create the CineVerse dataset. We use
this dataset to train our proposed two-stage approach. First, we prompt a large
language model (LLM) with task-specific instructions to take in a high-level
scene description and generate a detailed plan for the overall setting and
characters, as well as the individual shots. Then, we fine-tune a text-to-image
generation model to synthesize high-quality visual keyframes. Experimental
results demonstrate that CineVerse yields promising improvements in generating
visually coherent and contextually rich movie scenes, paving the way for
further exploration in cinematic video synthesis.

</details>


### [239] [Enhancing Quality for VVC Compressed Videos with Omniscient Quality Enhancement Model](https://arxiv.org/abs/2504.19935)
*Xiem HoangVan, Hieu Bui Minh, Sang NguyenQuang, Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: The paper proposes an Omniscient video quality enhancement Network (OVQE-VVC) for VVC compressed videos, achieving significant PSNR improvement and bitrate savings.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in VVC, challenges remain in perceptual quality and compression performance. AI-based methods, like deep learning, offer potential solutions.

Method: A modified OVQE model, originally for HEVC, is integrated into the VVC decoder, leveraging spatial-temporal and cross-frequency features.

Result: OVQE-VVC achieves PSNR improvements of 0.74 dB to 1.2 dB and ~19.6% bitrate savings compared to the original VVC codec.

Conclusion: The proposed OVQE-VVC effectively enhances video quality and compression efficiency, demonstrating its potential for practical applications.

Abstract: The latest video coding standard H.266/VVC has shown its great improvement in
terms of compression performance when compared to its predecessor HEVC
standard. Though VVC was implemented with many advanced techniques, it still
met the same challenges as its predecessor due to the need for even higher
perceptual quality demand at the decoder side as well as the compression
performance at the encoder side. The advancement of Artificial Intelligence
(AI) technology, notably the deep learning-based video quality enhancement
methods, was shown to be a promising approach to improving the perceptual
quality experience. In this paper, we propose a novel Omniscient video quality
enhancement Network for VVC compressed Videos. The Omniscient Network for
compressed video quality enhancement was originally designed for HEVC
compressed videos in which not only the spatial-temporal features but also
cross-frequencies information were employed to augment the visual quality.
Inspired by this work, we propose a modification of the OVQE model and
integrate it into the lasted STD-VVC (Standard Versatile Video Coding) decoder
architecture. As assessed in a rich set of test conditions, the proposed
OVQE-VVC solution is able to achieve significant PSNR improvement, notably
around 0.74 dB and up to 1.2 dB with respect to the original STD-VVC codec.
This also corresponds to around 19.6% of bitrate saving while keeping a similar
quality observation.

</details>


### [240] [Mesh-Learner: Texturing Mesh with Spherical Harmonics](https://arxiv.org/abs/2504.19938)
*Yunfei Wan, Jianheng Liu, Jiarong Lin, Fu Zhang*

Main category: cs.CV

TL;DR: Mesh-Learner is a 3D reconstruction and rendering framework compatible with rasterization pipelines, using mesh and SH textures for view-dependent radiance learning. It achieves state-of-the-art performance with efficient GPU memory usage.


<details>
  <summary>Details</summary>
Motivation: To create a 3D reconstruction and rendering framework that integrates seamlessly with traditional rasterization pipelines and tools like Blender, while efficiently handling large scenes.

Method: The framework combines mesh and SH textures, using a novel interpolation method for rendering and back-propagating gradients to SH Texels. It leverages rasterization pipeline features for compatibility.

Result: Achieves state-of-the-art performance on Replica and FAST-LIVO2 datasets, with efficient GPU memory usage by storing SH textures in CPU RAM when not in use.

Conclusion: Mesh-Learner offers a scalable, efficient, and compatible solution for 3D reconstruction and rendering, with potential applications in robotics and scene rendering.

Abstract: In this paper, we present a 3D reconstruction and rendering framework termed
Mesh-Learner that is natively compatible with traditional rasterization
pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e.,
texture filled with SH coefficients) into the learning process to learn each
mesh s view-dependent radiance end-to-end. Images are rendered by interpolating
surrounding SH Texels at each pixel s sampling point using a novel
interpolation method. Conversely, gradients from each pixel are back-propagated
to the related SH Texels in SH textures. Mesh-Learner exploits graphic features
of rasterization pipeline (texture sampling, deferred rendering) to render,
which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and
tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for
robotics) that are based on rasterization pipelines. Our system can train vast,
unlimited scenes because we transfer only the SH textures within the frustum to
the GPU for training. At other times, the SH textures are stored in CPU RAM,
which results in moderate GPU memory usage. The rendering results on
interpolation and extrapolation sequences in the Replica and FAST-LIVO2
datasets achieve state-of-the-art performance compared to existing
state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To
benefit the society, the code will be available at
https://github.com/hku-mars/Mesh-Learner.

</details>


### [241] [Shopformer: Transformer-Based Framework for Detecting Shoplifting via Human Pose](https://arxiv.org/abs/2504.19970)
*Narges Rashvand, Ghazal Alinezhad Noghre, Armin Danesh Pazho, Babak Rahimi Ardabili, Hamed Tabkhi*

Main category: cs.CV

TL;DR: Shopformer is a transformer-based model for shoplifting detection using pose sequences, offering privacy and efficiency over traditional video analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional surveillance and AI-based video analysis are ineffective, privacy-invasive, and resource-heavy.

Method: Uses pose sequences and custom tokenization for transformer processing.

Result: Outperforms state-of-the-art anomaly detection models on real-world data.

Conclusion: Provides a privacy-preserving, scalable solution for retail surveillance.

Abstract: Shoplifting remains a costly issue for the retail sector, but traditional
surveillance systems, which are mostly based on human monitoring, are still
largely ineffective, with only about 2% of shoplifters being arrested. Existing
AI-based approaches rely on pixel-level video analysis which raises privacy
concerns, is sensitive to environmental variations, and demands significant
computational resources. To address these limitations, we introduce Shopformer,
a novel transformer-based model that detects shoplifting by analyzing pose
sequences rather than raw video. We propose a custom tokenization strategy that
converts pose sequences into compact embeddings for efficient transformer
processing. To the best of our knowledge, this is the first pose-sequence-based
transformer model for shoplifting detection. Evaluated on real-world pose data,
our method outperforms state-of-the-art anomaly detection models, offering a
privacy-preserving, and scalable solution for real-time retail surveillance.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Shopformer.

</details>


### [242] [Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data](https://arxiv.org/abs/2504.19991)
*Ioannis Kontogiorgakis, Iason Tsardanidis, Dimitrios Bormpoudakis, Ilias Tsoumas, Dimitra A. Loka, Christos Noulas, Alexandros Tsitouras, Charalampos Kontoes*

Main category: cs.CV

TL;DR: The paper proposes an ML-based approach using satellite data to map weed management methods in orchards, improving efficiency over traditional field surveys.


<details>
  <summary>Details</summary>
Motivation: Effective weed management is vital for agriculture, but current monitoring methods like field surveys are costly and slow.

Method: The study uses ML with Sentinel-2 and PlanetScope satellite data to classify four weed management methods (Mowing, Tillage, Chemical-spraying, No practice) in orchards.

Result: The ML-driven remote sensing approach shows promise for efficient and accurate weed management mapping.

Conclusion: ML and EO data can enhance weed management monitoring, offering a scalable and timely alternative to field surveys.

Abstract: Effective weed management is crucial for improving agricultural productivity,
as weeds compete with crops for vital resources like nutrients and water.
Accurate maps of weed management methods are essential for policymakers to
assess farmer practices, evaluate impacts on vegetation health, biodiversity,
and climate, as well as ensure compliance with policies and subsidies. However,
monitoring weed management methods is challenging as commonly rely on on-ground
field surveys, which are often costly, time-consuming and subject to delays. In
order to tackle this problem, we leverage Earth Observation (EO) data and
Machine Learning (ML). Specifically, we developed an ML approach for mapping
four distinct weed management methods (Mowing, Tillage, Chemical-spraying, and
No practice) in orchards using satellite image time series (SITS) data from two
different sources: Sentinel-2 (S2) and PlanetScope (PS). The findings
demonstrate the potential of ML-driven remote sensing to enhance the efficiency
and accuracy of weed management mapping in orchards.

</details>


### [243] [SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning](https://arxiv.org/abs/2504.20024)
*Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jieneng Chen, Jianwen Xie, Alan Yuille*

Main category: cs.CV

TL;DR: SpatialReasoner, a new LVLM, uses explicit 3D representations for improved spatial reasoning and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based methods lack explicit 3D reasoning and generalization to unseen questions.

Method: Introduces SpatialReasoner, an LVLM with shared explicit 3D representations across perception, computation, and reasoning stages.

Result: Outperforms benchmarks and generalizes better to novel 3D spatial reasoning questions.

Conclusion: Bridges 3D parsing and reasoning, advancing 3D spatial reasoning research.

Abstract: Recent studies in 3D spatial reasoning explore data-driven approaches and
achieve enhanced spatial reasoning performance with reinforcement learning
(RL). However, these methods typically perform spatial reasoning in an implicit
manner, and it remains underexplored whether the acquired 3D knowledge
generalizes to unseen question types at any stage of the training. In this work
we introduce SpatialReasoner, a novel large vision-language model (LVLM) that
address 3D spatial reasoning with explicit 3D representations shared between
stages -- 3D perception, computation, and reasoning. Explicit 3D
representations provide a coherent interface that supports advanced 3D spatial
reasoning and enable us to study the factual errors made by LVLMs. Results show
that our SpatialReasoner achieve improved performance on a variety of spatial
reasoning benchmarks and generalizes better when evaluating on novel 3D spatial
reasoning questions. Our study bridges the 3D parsing capabilities of prior
visual foundation models with the powerful reasoning abilities of large
language models, opening new directions for 3D spatial reasoning.

</details>


### [244] [More Clear, More Flexible, More Precise: A Comprehensive Oriented Object Detection benchmark for UAV](https://arxiv.org/abs/2504.20032)
*Kai Ye, Haidi Tang, Bowen Liu, Pingyang Dai, Liujuan Cao, Rongrong Ji*

Main category: cs.CV

TL;DR: CODrone is a new UAV-oriented object detection dataset addressing limitations of existing datasets, enhancing real-world applicability and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing UAV OOD datasets lack generalization and real-world relevance, limiting practical algorithm effectiveness.

Method: CODrone improves upon four key dataset limitations (resolution, categories, views, altitudes) and includes diverse annotated images from multiple cities.

Result: Experiments with 22 methods validate CODrone's effectiveness and highlight challenges in UAV OOD.

Conclusion: CODrone bridges the data gap, offering a robust benchmark for future UAV OOD development.

Abstract: Applications of unmanned aerial vehicle (UAV) in logistics, agricultural
automation, urban management, and emergency response are highly dependent on
oriented object detection (OOD) to enhance visual perception. Although existing
datasets for OOD in UAV provide valuable resources, they are often designed for
specific downstream tasks.Consequently, they exhibit limited generalization
performance in real flight scenarios and fail to thoroughly demonstrate
algorithm effectiveness in practical environments. To bridge this critical gap,
we introduce CODrone, a comprehensive oriented object detection dataset for
UAVs that accurately reflects real-world conditions. It also serves as a new
benchmark designed to align with downstream task requirements, ensuring greater
applicability and robustness in UAV-based OOD.Based on application
requirements, we identify four key limitations in current UAV OOD datasets-low
image resolution, limited object categories, single-view imaging, and
restricted flight altitudes-and propose corresponding improvements to enhance
their applicability and robustness.Furthermore, CODrone contains a broad
spectrum of annotated images collected from multiple cities under various
lighting conditions, enhancing the realism of the benchmark. To rigorously
evaluate CODrone as a new benchmark and gain deeper insights into the novel
challenges it presents, we conduct a series of experiments based on 22
classical or SOTA methods.Our evaluation not only assesses the effectiveness of
CODrone in real-world scenarios but also highlights key bottlenecks and
opportunities to advance OOD in UAV applications.Overall, CODrone fills the
data gap in OOD from UAV perspective and provides a benchmark with enhanced
generalization capability, better aligning with practical applications and
future algorithm development.

</details>


### [245] [Mitigating Catastrophic Forgetting in the Incremental Learning of Medical Images](https://arxiv.org/abs/2504.20033)
*Sara Yavari, Jacob Furst*

Main category: cs.CV

TL;DR: The paper introduces an Incremental Learning (IL) approach using Knowledge Distillation (KD) to improve prostate cancer detection in T2w MRI images, showing enhanced performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in medical image analysis where large datasets from multiple health centers are impractical to store, while maintaining model accuracy.

Method: Utilized Knowledge Distillation (KD) with generated images from past tasks to train models incrementally, tested on PI-CAI, OCT, PathMNIST, and CIFAR-10 datasets.

Result: Improved model performance and faster convergence, demonstrating KD's effectiveness for IL in medical imaging.

Conclusion: KD is a promising IL technique for medical image analysis, enabling knowledge retention without storing original data.

Abstract: This paper proposes an Incremental Learning (IL) approach to enhance the
accuracy and efficiency of deep learning models in analyzing T2-weighted (T2w)
MRI medical images prostate cancer detection using the PI-CAI dataset. We used
multiple health centers' artificial intelligence and radiology data, focused on
different tasks that looked at prostate cancer detection using MRI (PI-CAI). We
utilized Knowledge Distillation (KD), as it employs generated images from past
tasks to guide the training of models for subsequent tasks. The approach
yielded improved performance and faster convergence of the models. To
demonstrate the versatility and robustness of our approach, we evaluated it on
the PI-CAI dataset, a diverse set of medical imaging modalities including OCT
and PathMNIST, and the benchmark continual learning dataset CIFAR-10. Our
results indicate that KD can be a promising technique for IL in medical image
analysis in which data is sourced from individual health centers and the
storage of large datasets is not feasible. By using generated images from prior
tasks, our method enables the model to retain and apply previously acquired
knowledge without direct access to the original data.

</details>


### [246] [MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion](https://arxiv.org/abs/2504.20040)
*Zador Pataki, Paul-Edouard Sarlin, Johannes L. Schönberger, Marc Pollefeys*

Main category: cs.CV

TL;DR: The paper introduces a method to improve Structure-from-Motion (SfM) by integrating monocular depth and normal priors from deep neural networks, enhancing performance in challenging scenarios like low-overlap or high-symmetry.


<details>
  <summary>Details</summary>
Motivation: Current SfM systems fail under extreme viewpoint changes, limiting their usability. The goal is to make SfM more robust and accessible, especially for non-experts.

Method: The approach combines classical SfM with monocular depth and normal priors from deep learning, tightly integrating monocular and multi-view constraints.

Result: The method outperforms existing systems in extreme conditions, handles symmetry-related errors, and reliably reconstructs challenging indoor scenes from few images.

Conclusion: The integration of monocular priors makes SfM more robust and adaptable, with potential for future improvements in depth and normal estimation.

Abstract: While Structure-from-Motion (SfM) has seen much progress over the years,
state-of-the-art systems are prone to failure when facing extreme viewpoint
changes in low-overlap, low-parallax or high-symmetry scenarios. Because
capturing images that avoid these pitfalls is challenging, this severely limits
the wider use of SfM, especially by non-expert users. We overcome these
limitations by augmenting the classical SfM paradigm with monocular depth and
normal priors inferred by deep neural networks. Thanks to a tight integration
of monocular and multi-view constraints, our approach significantly outperforms
existing ones under extreme viewpoint changes, while maintaining strong
performance in standard conditions. We also show that monocular priors can help
reject faulty associations due to symmetries, which is a long-standing problem
for SfM. This makes our approach the first capable of reliably reconstructing
challenging indoor environments from few images. Through principled uncertainty
propagation, it is robust to errors in the priors, can handle priors inferred
by different models with little tuning, and will thus easily benefit from
future progress in monocular depth and normal estimation. Our code is publicly
available at https://github.com/cvg/mpsfm.

</details>


### [247] [Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation](https://arxiv.org/abs/2403.19103)
*Yutong He, Alexander Robey, Naoki Murata, Yiding Jiang, Joshua Nathaniel Williams, George J. Pappas, Hamed Hassani, Yuki Mitsufuji, Ruslan Salakhutdinov, J. Zico Kolter*

Main category: cs.CV

TL;DR: PRISM automates prompt generation for text-to-image models, producing human-interpretable and transferable prompts without needing white-box access.


<details>
  <summary>Details</summary>
Motivation: Manual prompt engineering is labor-intensive, and existing automated methods lack transferability or require model access.

Method: PRISM uses LLM in-context learning to iteratively refine prompts based on reference images.

Result: PRISM effectively generates prompts for objects, styles, and images across models like Stable Diffusion, DALL-E, and Midjourney.

Conclusion: PRISM offers a versatile and efficient solution for automated prompt generation in T2I models.

Abstract: Prompt engineering is an effective but labor-intensive way to control
text-to-image (T2I) generative models. Its time-intensive nature and complexity
have spurred the development of algorithms for automated prompt generation.
However, these methods often struggle with transferability across T2I models,
require white-box access to the underlying model, or produce non-intuitive
prompts. In this work, we introduce PRISM, an algorithm that automatically
produces human-interpretable and transferable prompts that can effectively
generate desired concepts given only black-box access to T2I models. Inspired
by large language model (LLM) jailbreaking, PRISM leverages the in-context
learning ability of LLMs to iteratively refine the candidate prompt
distribution built upon the reference images. Our experiments demonstrate the
versatility and effectiveness of PRISM in generating accurate prompts for
objects, styles, and images across multiple T2I models, including Stable
Diffusion, DALL-E, and Midjourney.

</details>


### [248] [Learning Streaming Video Representation via Multitask Training](https://arxiv.org/abs/2504.20041)
*Yibin Yan, Jilan Xu, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian Li, Yifei Huang, Weidi Xie*

Main category: cs.CV

TL;DR: StreamFormer is a novel streaming video backbone for real-time video understanding, integrating causal temporal attention into a vision transformer. It unifies multitask visual-language alignment for training and achieves competitive results in real-time applications.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of processing video streams frame by frame, preserving historical information, and making low-latency decisions for real-time applications like embodied AI and autonomous driving.

Method: Develop StreamFormer by adding causal temporal attention to a pre-trained vision transformer. Train it using a multitask visual-language alignment framework to learn global semantics, temporal dynamics, and spatial relationships.

Result: Competitive performance in online action detection, video instance segmentation, and video question answering while maintaining efficiency.

Conclusion: StreamFormer demonstrates strong potential for real-time video understanding tasks, balancing accuracy and efficiency.

Abstract: Understanding continuous video streams plays a fundamental role in real-time
applications including embodied AI and autonomous driving. Unlike offline video
understanding, streaming video understanding requires the ability to process
video streams frame by frame, preserve historical information, and make
low-latency decisions.To address these challenges, our main contributions are
three-fold. (i) We develop a novel streaming video backbone, termed as
StreamFormer, by incorporating causal temporal attention into a pre-trained
vision transformer. This enables efficient streaming video processing while
maintaining image representation capability.(ii) To train StreamFormer, we
propose to unify diverse spatial-temporal video understanding tasks within a
multitask visual-language alignment framework. Hence, StreamFormer learns
global semantics, temporal dynamics, and fine-grained spatial relationships
simultaneously. (iii) We conduct extensive experiments on online action
detection, online video instance segmentation, and video question answering.
StreamFormer achieves competitive results while maintaining efficiency,
demonstrating its potential for real-time applications.

</details>


### [249] [Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models](https://arxiv.org/abs/2403.20331)
*Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei Liu, Kiyoharu Aizawa*

Main category: cs.CV

TL;DR: The paper introduces Unsolvable Problem Detection (UPD) to evaluate Large Multimodal Models (LMMs) by assessing their ability to withhold answers for unsolvable MCQA problems, revealing gaps in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current multiple-choice question answering (MCQA) benchmarks don't ensure LMMs truly understand answers, so UPD evaluates their ability to detect unsolvable problems.

Method: UPD includes three problem types (AAD, IASD, IVQD) and uses the MM-UPD Bench for evaluation. Chain-of-thought and self-reflection are tested to improve performance.

Result: Most LMMs perform poorly on MM-UPD, highlighting overlooked trustworthiness aspects. Chain-of-thought and self-reflection help models with LLM bottlenecks.

Conclusion: UPD reveals critical limitations in LMMs, offering insights for developing more reliable models.

Abstract: This paper introduces a novel task to evaluate the robust understanding
capability of Large Multimodal Models (LMMs), termed $\textbf{Unsolvable
Problem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely
used to assess the understanding capability of LMMs, but it does not guarantee
that LMMs truly comprehend the answer. UPD assesses the LMM's ability to
withhold answers when encountering unsolvable problems of MCQA, verifying
whether the model truly understands the answer. UPD encompasses three problems:
Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and
Incompatible Visual Question Detection (IVQD), covering unsolvable cases like
answer-lacking or incompatible choices and image-question mismatches. For the
evaluation, we introduce the MM-UPD Bench, a benchmark for assessing
performance across various ability dimensions. Our experiments reveal that even
most LMMs, which demonstrate adequate performance on existing benchmarks,
struggle significantly with MM-UPD, underscoring a novel aspect of
trustworthiness that current benchmarks have overlooked. A detailed analysis
shows that LMMs have different bottlenecks and chain-of-thought and
self-reflection improved performance for LMMs with the bottleneck in their LLM
capability. We hope our insights will enhance the broader understanding and
development of more reliable LMMs.

</details>


### [250] [CompleteMe: Reference-based Human Image Completion](https://arxiv.org/abs/2504.20042)
*Yu-Ju Tsai, Brian Price, Qing Liu, Luis Figueroa, Daniil Pakhomov, Zhihong Ding, Scott Cohen, Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: CompleteMe is a novel framework for human image completion that uses a dual U-Net and Region-focused Attention Block to preserve fine details from reference images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to preserve unique details like clothing patterns or accessories without explicit reference images, and even reference-based approaches struggle with fine-grained detail integration.

Method: CompleteMe employs a dual U-Net architecture with a Region-focused Attention (RFA) Block to focus on relevant regions in reference images, ensuring accurate semantic correspondence.

Result: The method achieves superior visual quality and semantic consistency compared to existing techniques, as demonstrated by extensive experiments.

Conclusion: CompleteMe effectively addresses the limitations of current human image completion methods by improving fidelity and consistency in completed images.

Abstract: Recent methods for human image completion can reconstruct plausible body
shapes but often fail to preserve unique details, such as specific clothing
patterns or distinctive accessories, without explicit reference images. Even
state-of-the-art reference-based inpainting approaches struggle to accurately
capture and integrate fine-grained details from reference images. To address
this limitation, we propose CompleteMe, a novel reference-based human image
completion framework. CompleteMe employs a dual U-Net architecture combined
with a Region-focused Attention (RFA) Block, which explicitly guides the
model's attention toward relevant regions in reference images. This approach
effectively captures fine details and ensures accurate semantic correspondence,
significantly improving the fidelity and consistency of completed images.
Additionally, we introduce a challenging benchmark specifically designed for
evaluating reference-based human image completion tasks. Extensive experiments
demonstrate that our proposed method achieves superior visual quality and
semantic consistency compared to existing techniques. Project page:
https://liagm.github.io/CompleteMe/

</details>


### [251] [An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training](https://arxiv.org/abs/2501.15579)
*Yuxiang Nie, Sunan He, Yequan Bie, Yihui Wang, Zhixuan Chen, Shu Yang, Zhiyuan Cai, Hongmei Wang, Xi Wang, Luyang Luo, Mingxiang Wu, Xian Wu, Ronald Cheong Kin Chan, Yuk Ming Lau, Yefeng Zheng, Pranav Rajpurkar, Hao Chen*

Main category: cs.CV

TL;DR: ConceptCLIP is an explainable biomedical foundation model that combines high diagnostic accuracy with human-interpretable explanations, outperforming existing models across diverse medical imaging tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between AI performance and interpretability in medical imaging, ensuring clinicians can trust and understand AI decisions.

Method: Developed using a dual-alignment approach on MedConcept-23M, a dataset of 23M image-text-concept triplets, to learn global and fine-grained associations.

Result: Outperforms state-of-the-art models on 52 clinical tasks across 10 modalities, with validated human-understandable explanations.

Conclusion: ConceptCLIP advances trustworthy AI in medicine by balancing accuracy and interpretability, enabling broader clinical adoption.

Abstract: The clinical adoption of artificial intelligence (AI) in medical imaging
requires models that are both diagnostically accurate and interpretable to
clinicians. While current multimodal biomedical foundation models prioritize
performance, their black-box nature hinders explaining the decision-making
process in clinically meaningful concepts. Here, we present ConceptCLIP, the
first explainable biomedical foundation model that achieves state-of-the-art
diagnostic accuracy while delivering human-interpretable explanations across
diverse imaging modalities. We curate MedConcept-23M, the largest pre-training
dataset comprising 23 million image-text-concept triplets across diverse
medical modalities, where clinical concepts are derived from the Unified
Medical Language System. Leveraging this dataset, we develop ConceptCLIP
through a novel dual-alignment approach that simultaneously learns global
image-text representations and fine-grained region-concept associations for
precise and interpretable medical image analysis. We curate the most extensive
evaluation benchmark for multimodal biomedical foundation models, covering 52
clinical tasks spanning 10 imaging modalities. Extensive experiments
demonstrate that ConceptCLIP outperforms existing state-of-the-art multimodal
biomedical foundation models. Importantly, ConceptCLIP demonstrates superior
diagnostic performance while providing human-understandable explanations
validated by clinical experts. As the first precise and interpretable
biomedical foundation model, ConceptCLIP represents a critical milestone toward
the widespread clinical adoption of AI, thereby advancing trustworthy AI in
medicine.

</details>


### [252] [Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering](https://arxiv.org/abs/2502.09573)
*Mark Beliaev, Victor Yang, Madhura Raju, Jiachen Sun, Xinghai Hu*

Main category: cs.CV

TL;DR: Optimizing GPT-based models for zero-shot video classification using prompt optimization and policy refinement, reducing false negatives and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing industry challenges in video content classification by enhancing GPT's performance without additional finetuning.

Method: Novel approach combining prompt optimization, policy refinement, and decomposition-aggregation-based prompt engineering.

Result: Simplified policies reduce false negatives; new technique outperforms single-prompt methods, improving classification.

Conclusion: Thoughtful prompt design enhances GPT's performance, offering a scalable solution for video classification.

Abstract: In this study, we tackle industry challenges in video content classification
by exploring and optimizing GPT-based models for zero-shot classification
across seven critical categories of video quality. We contribute a novel
approach to improving GPT's performance through prompt optimization and policy
refinement, demonstrating that simplifying complex policies significantly
reduces false negatives. Additionally, we introduce a new
decomposition-aggregation-based prompt engineering technique, which outperforms
traditional single-prompt methods. These experiments, conducted on real
industry problems, show that thoughtful prompt design can substantially enhance
GPT's performance without additional finetuning, offering an effective and
scalable solution for improving video classification.

</details>


### [253] [Memory Regulation and Alignment toward Generalizer RGB-Infrared Person](https://arxiv.org/abs/2109.08843)
*Feng Chen, Fei Wu, Qi Wu, Zhiguo Wan*

Main category: cs.CV

TL;DR: The paper addresses domain shift in RGB-Infrared person re-identification by proposing a multi-granularity memory regulation and alignment module (MG-MRA) to align data distributions and reduce over-reliance on seen classes.


<details>
  <summary>Details</summary>
Motivation: Domain shift due to modality gaps and non-overlapping identity classes in RGB-IR ReID leads to sub-optimal adversarial gradients and over-reliance on discriminative features of seen classes.

Method: Introduces MG-MRA, which incorporates latent variable attributes (fine to coarse granularity) into features to mitigate over-confidence and uses sparse attributes for similarity measurement.

Result: Outperforms state-of-the-art methods on RegDB and SYSU-MM01 datasets.

Conclusion: MG-MRA effectively addresses domain shift and improves RGB-IR ReID performance by balancing feature sensitivity and leveraging global structural patterns.

Abstract: The domain shift, coming from unneglectable modality gap and non-overlapped
identity classes between training and test sets, is a major issue of
RGB-Infrared person re-identification. A key to tackle the inherent issue --
domain shift -- is to enforce the data distributions of the two domains to be
similar. However, RGB-IR ReID always demands discriminative features, leading
to over-rely feature sensitivity of seen classes, \textit{e.g.}, via
attention-based feature alignment or metric learning. Therefore, predicting the
unseen query category from predefined training classes may not be accurate and
leads to a sub-optimal adversarial gradient. In this paper, we uncover it in a
more explainable way and propose a novel multi-granularity memory regulation
and alignment module (MG-MRA) to solve this issue. By explicitly incorporating
a latent variable attribute, from fine-grained to coarse semantic granularity,
into intermediate features, our method could alleviate the over-confidence of
the model about discriminative features of seen classes. Moreover, instead of
matching discriminative features by traversing nearest neighbor, sparse
attributes, \textit{i.e.}, global structural pattern, are recollected with
respect to features and assigned to measure pair-wise image similarity in
hashing. Extensive experiments on RegDB \cite{RegDB} and SYSU-MM01 \cite{SYSU}
show the superiority of the proposed method that outperforms existing
state-of-the-art methods. Our code is available in
https://github.com/Chenfeng1271/MGMRA.

</details>


### [254] [Injecting Image Details into CLIP's Feature Space](https://arxiv.org/abs/2208.14649)
*Zilun Zhang, Cuifeng Shen, Yuan Shen, Huixin Xiong, Xinyu Zhou*

Main category: cs.CV

TL;DR: An efficient framework is introduced to enhance CLIP-like models by preserving subtle details in high-resolution images, improving feature representation and retrieval performance.


<details>
  <summary>Details</summary>
Motivation: CLIP-like models lose subtle details in high-resolution images due to input size limitations, necessitating a solution for better feature representation.

Method: A feature fusing model is trained using CLIP features from a custom image patch method, weakly supervised by class-prompted queries.

Result: The framework significantly improves image retrieval performance on real-world and synthetic datasets, including the newly created CLVER-DS.

Conclusion: The proposed framework effectively preserves image details and aligns with CLIP's semantic space, demonstrating superior retrieval capabilities.

Abstract: Although CLIP-like Visual Language Models provide a functional joint feature
space for image and text, due to the limitation of the CILP-like model's image
input size (e.g., 224), subtle details are lost in the feature representation
if we input high-resolution images (e.g., 2240). In this work, we introduce an
efficient framework that can produce a single feature representation for a
high-resolution image that injects image details and shares the same semantic
space as the original CLIP. In the framework, we train a feature fusing model
based on CLIP features extracted from a carefully designed image patch method
that can cover objects of any scale, weakly supervised by image-agnostic class
prompted queries. We validate our framework by retrieving images from class
prompted queries on the real world and synthetic datasets, showing significant
performance improvement on these tasks. Furthermore, to fully demonstrate our
framework's detail retrieval ability, we construct a CLEVR-like synthetic
dataset called CLVER-DS, which is fully annotated and has a controllable object
scale.

</details>


### [255] [OccluTrack: Rethinking Awareness of Occlusion for Enhancing Multiple Pedestrian Tracking](https://arxiv.org/abs/2309.10360)
*Jianjun Gao, Yi Wang, Kim-Hui Yap, Kratika Garg, Boon Siew Han*

Main category: cs.CV

TL;DR: Proposes OccluTrack, an adaptive occlusion-aware tracker for multiple pedestrian tracking, addressing partial occlusion challenges with motion suppression, pose-guided Re-ID, and occlusion-aware association.


<details>
  <summary>Details</summary>
Motivation: To improve safety and efficiency in intelligent transport by addressing inaccuracies in motion estimation and unreliable features caused by partial occlusion in existing methods.

Method: Introduces a motion suppression mechanism in Kalman Filter, a pose-guided Re-ID module, and an occlusion-aware association method for fair IoU and embedding distance measurement.

Result: Outperforms state-of-the-art methods on MOTChallenge and DanceTrack datasets, showing improvements in IDF1 and ID Switches.

Conclusion: OccluTrack effectively mitigates partial occlusion effects, enhancing tracking accuracy and reliability in dynamic environments.

Abstract: Multiple pedestrian tracking is crucial for enhancing safety and efficiency
in intelligent transport and autonomous driving systems by predicting movements
and enabling adaptive decision-making in dynamic environments. It optimizes
traffic flow, facilitates human interaction, and ensures compliance with
regulations. However, it faces the challenge of tracking pedestrians in the
presence of occlusion. Existing methods overlook effects caused by abnormal
detections during partial occlusion. Subsequently, these abnormal detections
can lead to inaccurate motion estimation, unreliable appearance features, and
unfair association. To address these issues, we propose an adaptive
occlusion-aware multiple pedestrian tracker, OccluTrack, to mitigate the
effects caused by partial occlusion. Specifically, we first introduce a
plug-and-play abnormal motion suppression mechanism into the Kalman Filter to
adaptively detect and suppress outlier motions caused by partial occlusion.
Second, we develop a pose-guided re-identification (Re-ID) module to extract
discriminative part features for partially occluded pedestrians. Last, we
develop a new occlusion-aware association method towards fair Intersection over
Union (IoU) and appearance embedding distance measurement for occluded
pedestrians. Extensive evaluation results demonstrate that our method
outperforms state-of-the-art methods on MOTChallenge and DanceTrack datasets.
Particularly, the performance improvements on IDF1 and ID Switches, as well as
visualized results, demonstrate the effectiveness of our method in multiple
pedestrian tracking.

</details>


### [256] [OmniCaptioner: One Captioner to Rule Them All](https://arxiv.org/abs/2504.07089)
*Yiting Lu, Jiakang Yuan, Zhen Li, Shitian Zhao, Qi Qin, Xinyue Li, Le Zhuo, Licheng Wen, Dongyang Liu, Yuewen Cao, Xiangchao Yan, Xin Li, Tianshuo Peng, Shufei Zhang, Botian Shi, Tao Chen, Zhibo Chen, Lei Bai, Bo Zhang, Peng Gao*

Main category: cs.CV

TL;DR: OmniCaptioner is a unified visual captioning framework for diverse visual domains, enhancing multimodal reasoning, image generation, and fine-tuning efficiency.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between visual and textual modalities by providing a versatile solution for captioning various visual types, unlike domain-specific prior methods.

Method: Converts low-level pixel data into rich textual descriptions for natural images, visual text, and structured visuals.

Result: Demonstrates enhanced visual reasoning with LLMs, improved image generation, and efficient supervised fine-tuning.

Conclusion: OmniCaptioner's versatility offers a new perspective for integrating language and visual modalities.

Abstract: We propose OmniCaptioner, a versatile visual captioning framework for
generating fine-grained textual descriptions across a wide variety of visual
domains. Unlike prior methods limited to specific image types (e.g., natural
images or geometric visuals), our framework provides a unified solution for
captioning natural images, visual text (e.g., posters, UIs, textbooks), and
structured visuals (e.g., documents, tables, charts). By converting low-level
pixel information into semantically rich textual representations, our framework
bridges the gap between visual and textual modalities. Our results highlight
three key advantages: (i) Enhanced Visual Reasoning with LLMs, where
long-context captions of visual modalities empower LLMs, particularly the
DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)
Improved Image Generation, where detailed captions improve tasks like
text-to-image generation and image transformation; and (iii) Efficient
Supervised Fine-Tuning (SFT), which enables faster convergence with less data.
We believe the versatility and adaptability of OmniCaptioner can offer a new
perspective for bridging the gap between language and visual modalities.

</details>


### [257] [DEVICE: Depth and Visual Concepts Aware Transformer for OCR-based Image Captioning](https://arxiv.org/abs/2302.01540)
*Dongsheng Xu, Qingbao Huang, Xingmao Zhang, Haonan Cheng, Feng Shuang, Yi Cai*

Main category: cs.CV

TL;DR: The paper proposes DEVICE, a Depth and Visual Concepts Aware Transformer, to improve OCR-based image captioning by incorporating depth information and visual concepts for better scene understanding and caption accuracy.


<details>
  <summary>Details</summary>
Motivation: Current OCR-based image captioning methods lack depth information and fine-grained descriptions, leading to inaccurate captions and insufficient scene text relational reasoning.

Method: Introduces depth-enhanced feature updating and semantic-guided alignment modules to improve OCR token features and utilize visual concepts.

Result: DEVICE outperforms state-of-the-art models on the TextCaps test set, demonstrating improved scene comprehension and caption accuracy.

Conclusion: Incorporating depth and visual concepts enhances OCR-based image captioning, addressing previous limitations and achieving superior performance.

Abstract: OCR-based image captioning is an important but under-explored task, aiming to
generate descriptions containing visual objects and scene text. Recent studies
have made encouraging progress, but they are still suffering from a lack of
overall understanding of scenes and generating inaccurate captions. One
possible reason is that current studies mainly focus on constructing the
plane-level geometric relationship of scene text without depth information.
This leads to insufficient scene text relational reasoning so that models may
describe scene text inaccurately. The other possible reason is that existing
methods fail to generate fine-grained descriptions of some visual objects. In
addition, they may ignore essential visual objects, leading to the scene text
belonging to these ignored objects not being utilized. To address the above
issues, we propose a Depth and Visual Concepts Aware Transformer (DEVICE) for
OCR-based image captinong. Concretely, to construct three-dimensional geometric
relations, we introduce depth information and propose a depth-enhanced feature
updating module to ameliorate OCR token features. To generate more precise and
comprehensive captions, we introduce semantic features of detected visual
concepts as auxiliary information, and propose a semantic-guided alignment
module to improve the model's ability to utilize visual concepts. Our DEVICE is
capable of comprehending scenes more comprehensively and boosting the accuracy
of described visual entities. Sufficient experiments demonstrate the
effectiveness of our proposed DEVICE, which outperforms state-of-the-art models
on the TextCaps test set.

</details>


### [258] [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280)
*Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Ruoyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, Yi Ma*

Main category: cs.CV

TL;DR: The paper introduces All-Angles Bench, a benchmark for evaluating Multi-Modal Large Language Models (MLLMs) on multi-view scene reasoning, revealing significant gaps in geometric consistency and cross-view correspondence compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of MLLMs in multi-view understanding, particularly in geometric consistency and cross-view correspondence, which are crucial for embodied agents.

Method: The authors propose All-Angles Bench, a dataset of 2,100 annotated multi-view QA pairs across 90 real-world scenes, testing six tasks to evaluate MLLMs.

Result: Experiments on 27 MLLMs show a substantial performance gap, with models struggling in cross-view correspondence for occluded views and coarse camera pose estimation.

Conclusion: The findings emphasize the need for domain-specific refinements in MLLMs to improve multi-view awareness, with All-Angles Bench serving as a valuable resource for future research.

Abstract: Multi-view understanding, the ability to reconcile visual information across
diverse viewpoints for effective navigation, manipulation, and 3D scene
comprehension, is a fundamental challenge in Multi-Modal Large Language Models
(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive
advances in high-level reasoning and planning, they frequently fall short when
confronted with multi-view geometric consistency and cross-view correspondence.
To comprehensively evaluate the challenges of MLLMs in multi-view scene
reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human
carefully annotated multi-view question-answer pairs across 90 diverse
real-world scenes. Our six tasks (counting, attribute identification, relative
distance, relative direction, object manipulation, and camera pose estimation)
specifically test model's geometric correspondence and the capacity to align
information consistently across views. Our extensive experiments, benchmark on
27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and
GPT-4o against human evaluators reveals a substantial performance gap,
indicating that current MLLMs remain far from human-level proficiency. Through
in-depth analysis, we show that MLLMs are particularly underperforming under
two aspects: (1) cross-view correspondence for partially occluded views and (2)
establishing the coarse camera poses. These findings highlight the necessity of
domain-specific refinements or modules that embed stronger multi-view
awareness. We believe that our All-Angles Bench offers valuable insights and
contribute to bridging the gap between MLLMs and human-level multi-view
understanding. The project and benchmark are publicly available at
https://danielchyeh.github.io/All-Angles-Bench/.

</details>


### [259] [Local-Global Temporal Difference Learning for Satellite Video Super-Resolution](https://arxiv.org/abs/2304.04421)
*Yi Xiao, Qiangqiang Yuan, Kui Jiang, Xianyu Jin, Jiang He, Liangpei Zhang, Chia-Wen Lin*

Main category: cs.CV

TL;DR: The paper proposes a novel method for temporal compensation in satellite Video Super-Resolution (VSR) using short-term and long-term temporal differences, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing optical-flow and kernel-based methods for temporal compensation in satellite VSR lack generalization in large-scale or complex scenarios.

Method: The authors introduce Short-term (S-TDM) and Long-term (L-TDM) Temporal Difference Modules to capture local and global temporal discrepancies, along with a Difference Compensation Unit (DCU) for spatial consistency.

Result: The method outperforms state-of-the-art approaches in objective and subjective evaluations across five mainstream video satellites.

Conclusion: The proposed approach effectively addresses limitations of existing methods by leveraging temporal differences, offering improved performance in satellite VSR.

Abstract: Optical-flow-based and kernel-based approaches have been extensively explored
for temporal compensation in satellite Video Super-Resolution (VSR). However,
these techniques are less generalized in large-scale or complex scenarios,
especially in satellite videos. In this paper, we propose to exploit the
well-defined temporal difference for efficient and effective temporal
compensation. To fully utilize the local and global temporal information within
frames, we systematically modeled the short-term and long-term temporal
discrepancies since we observed that these discrepancies offer distinct and
mutually complementary properties. Specifically, we devise a Short-term
Temporal Difference Module (S-TDM) to extract local motion representations from
RGB difference maps between adjacent frames, which yields more clues for
accurate texture representation. To explore the global dependency in the entire
frame sequence, a Long-term Temporal Difference Module (L-TDM) is proposed,
where the differences between forward and backward segments are incorporated
and activated to guide the modulation of the temporal feature, leading to a
holistic global compensation. Moreover, we further propose a Difference
Compensation Unit (DCU) to enrich the interaction between the spatial
distribution of the target frame and temporal compensated results, which helps
maintain spatial consistency while refining the features to avoid misalignment.
Rigorous objective and subjective evaluations conducted across five mainstream
video satellites demonstrate that our method performs favorably against
state-of-the-art approaches. Code will be available at
https://github.com/XY-boy/LGTD

</details>


### [260] [FEDORA: Flying Event Dataset fOr Reactive behAvior](https://arxiv.org/abs/2305.14392)
*Amogh Joshi, Adarsh Kosta, Wachirawit Ponghiran, Manish Nagaraj, Kaushik Roy*

Main category: cs.CV

TL;DR: FEDORA is a synthetic dataset addressing the lack of high-rate ground truths for multiple perception tasks in vision-based autonomous systems, inspired by biological systems like fruitflies.


<details>
  <summary>Details</summary>
Motivation: Biological systems perform high-speed maneuvers in cluttered environments, inspiring vision-based autonomous systems. Current datasets lack high-rate ground truths for multiple tasks.

Method: Introduces FEDORA, a synthetic dataset with raw data from frame-based and event-based cameras, IMUs, and high-rate ground truths for depth, pose, and optical flow.

Result: FEDORA provides a comprehensive dataset for training perception tasks, overcoming limitations of existing datasets.

Conclusion: FEDORA enables better training of perception pipelines for high-speed maneuvers in autonomous systems.

Abstract: The ability of resource-constrained biological systems such as fruitflies to
perform complex and high-speed maneuvers in cluttered environments has been one
of the prime sources of inspiration for developing vision-based autonomous
systems. To emulate this capability, the perception pipeline of such systems
must integrate information cues from tasks including optical flow and depth
estimation, object detection and tracking, and segmentation, among others.
However, the conventional approach of employing slow, synchronous inputs from
standard frame-based cameras constrains these perception capabilities,
particularly during high-speed maneuvers. Recently, event-based sensors have
emerged as low latency and low energy alternatives to standard frame-based
cameras for capturing high-speed motion, effectively speeding up perception and
hence navigation. For coherence, all the perception tasks must be trained on
the same input data. However, present-day datasets are curated mainly for a
single or a handful of tasks and are limited in the rate of the provided ground
truths. To address these limitations, we present Flying Event Dataset fOr
Reactive behAviour (FEDORA) - a fully synthetic dataset for perception tasks,
with raw data from frame-based cameras, event-based cameras, and Inertial
Measurement Units (IMU), along with ground truths for depth, pose, and optical
flow at a rate much higher than existing datasets.

</details>


### [261] [Long-Tailed Continual Learning For Visual Food Recognition](https://arxiv.org/abs/2307.00183)
*Jiangpeng He, Xiaoyan Zhang, Luotao Lin, Jack Ma, Heather A. Eicher-Miller, Fengqing Zhu*

Main category: cs.CV

TL;DR: A novel framework for long-tailed continual learning in food recognition addresses challenges like forgetting old classes and handling rare food classes, using knowledge distillation and CAM-CutMix augmentation, validated on new datasets.


<details>
  <summary>Details</summary>
Motivation: Overcome challenges in real-world food recognition: continual learning without forgetting and handling long-tailed distributions of food classes.

Method: Proposes an end-to-end framework with knowledge distillation and CAM-CutMix augmentation to improve generalization, especially for rare classes.

Result: Significant improvements over existing methods on benchmark datasets, with ablation studies confirming performance gains.

Conclusion: The framework shows promise for real-world food recognition, addressing key challenges in continual learning and long-tailed distributions.

Abstract: Deep learning-based food recognition has made significant progress in
predicting food types from eating occasion images. However, two key challenges
hinder real-world deployment: (1) continuously learning new food classes
without forgetting previously learned ones, and (2) handling the long-tailed
distribution of food images, where a few common classes and many more rare
classes. To address these, food recognition methods should focus on long-tailed
continual learning. In this work, We introduce a dataset that encompasses 186
American foods along with comprehensive annotations. We also introduce three
new benchmark datasets, VFN186-LT, VFN186-INSULIN and VFN186-T2D, which reflect
real-world food consumption for healthy populations, insulin takers and
individuals with type 2 diabetes without taking insulin. We propose a novel
end-to-end framework that improves the generalization ability for instance-rare
food classes using a knowledge distillation-based predictor to avoid
misalignment of representation during continual learning. Additionally, we
introduce an augmentation technique by integrating class-activation-map (CAM)
and CutMix to improve generalization on instance-rare food classes. Our method,
evaluated on Food101-LT, VFN-LT, VFN186-LT, VFN186-INSULIN, and VFN186-T2DM,
shows significant improvements over existing methods. An ablation study
highlights further performance enhancements, demonstrating its potential for
real-world food recognition applications.

</details>


### [262] [Shape-centered Representation Learning for Visible-Infrared Person Re-identification](https://arxiv.org/abs/2310.17952)
*Shuang Li, Jiaxu Leng, Ji Gan, Mengjingcheng Mo, Xinbo Gao*

Main category: cs.CV

TL;DR: The paper proposes ScRL, a framework for VI-ReID that integrates shape and appearance features to improve performance, addressing challenges like modality variations and inaccurate shape estimation.


<details>
  <summary>Details</summary>
Motivation: Existing VI-ReID methods focus on appearance features but neglect body shape features, which are robust to modality variations. Integrating both features effectively is challenging due to noise and estimation inaccuracies.

Method: ScRL includes Infrared Shape Restoration (ISR) to correct shape representations, Shape Feature Propagation (SFP) for direct shape extraction, and Appearance Feature Enhancement (AFE) to refine appearance features using shape cues.

Result: ScRL achieves Rank-1 (mAP) accuracies of 76.1% (72.6%), 71.2% (52.9%), and 92.4% (86.7%) on SYSU-MM01, HITSZ-VCM, and RegDB datasets, outperforming state-of-the-art methods.

Conclusion: The ScRL framework successfully integrates shape and appearance features, demonstrating superior performance in VI-ReID tasks.

Abstract: Visible-Infrared Person Re-Identification (VI-ReID) plays a critical role in
all-day surveillance systems. However, existing methods primarily focus on
learning appearance features while overlooking body shape features, which not
only complement appearance features but also exhibit inherent robustness to
modality variations. Despite their potential, effectively integrating shape and
appearance features remains challenging. Appearance features are highly
susceptible to modality variations and background noise, while shape features
often suffer from inaccurate infrared shape estimation due to the limitations
of auxiliary models. To address these challenges, we propose the Shape-centered
Representation Learning (ScRL) framework, which enhances VI-ReID performance by
innovatively integrating shape and appearance features. Specifically, we
introduce Infrared Shape Restoration (ISR) to restore inaccuracies in infrared
body shape representations at the feature level by leveraging infrared
appearance features. In addition, we propose Shape Feature Propagation (SFP),
which enables the direct extraction of shape features from original images
during inference with minimal computational complexity. Furthermore, we design
Appearance Feature Enhancement (AFE), which utilizes shape features to
emphasize shape-related appearance features while effectively suppressing
identity-unrelated noise. Benefiting from the effective integration of shape
and appearance features, ScRL demonstrates superior performance through
extensive experiments. On the SYSU-MM01, HITSZ-VCM, and RegDB datasets, it
achieves Rank-1 (mAP) accuracies of 76.1% (72.6%), 71.2% (52.9%), and 92.4%
(86.7%), respectively, surpassing existing state-of-the-art methods.

</details>


### [263] [MENTOR: Human Perception-Guided Pretraining for Increased Generalization](https://arxiv.org/abs/2310.19545)
*Colton R. Crum, Adam Czajka*

Main category: cs.CV

TL;DR: MENTOR improves CNN generalization for open-set anomaly detection by incorporating human perception in two training rounds: first learning saliency maps, then fine-tuning with class labels.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently integrating limited human perceptual data into CNN training for better generalization in open-set recognition tasks.

Method: Two-stage training: (1) Train an autoencoder to learn human saliency maps without labels, then (2) replace the decoder with a classifier and train with labels.

Result: MENTOR outperforms traditional pretraining and state-of-the-art methods in anomaly detection tasks across various domains (e.g., iris attacks, synthetic faces, chest X-rays).

Conclusion: MENTOR effectively enhances generalization in CNNs for anomaly detection and can be flexibly applied to existing methods without architectural changes.

Abstract: Leveraging human perception into training of convolutional neural networks
(CNN) has boosted generalization capabilities of such models in open-set
recognition tasks. One of the active research questions is where (in the model
architecture or training pipeline) and how to efficiently incorporate always
limited human perceptual data into training strategies of models. In this
paper, we introduce MENTOR (huMan pErceptioN-guided preTraining fOr increased
geneRalization), which addresses this question through two unique rounds of
training CNNs tasked with open-set anomaly detection. First, we train an
autoencoder to learn human saliency maps given an input image, without any
class labels. The autoencoder is thus tasked with discovering domain-specific
salient features which mimic human perception. Second, we remove the decoder
part, add a classification layer on top of the encoder, and train this new
model conventionally, now using class labels. We show that MENTOR successfully
raises the generalization performance across three different CNN backbones in a
variety of anomaly detection tasks (demonstrated for detection of unknown iris
presentation attacks, synthetically-generated faces, and anomalies in chest
X-ray images) compared to traditional pretraining methods (e.g., sourcing the
weights from ImageNet), and as well as state-of-the-art methods that
incorporate human perception guidance into training. In addition, we
demonstrate that MENTOR can be flexibly applied to existing human
perception-guided methods and subsequently increasing their generalization with
no architectural modifications.

</details>


### [264] [Infusion: internal diffusion for inpainting of dynamic textures and complex motion](https://arxiv.org/abs/2311.01090)
*Nicolas Cherel, Andrés Almansa, Yann Gousseau, Alasdair Newson*

Main category: cs.CV

TL;DR: A lightweight diffusion model for video inpainting uses internal learning (training on a single video) to achieve state-of-the-art results with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Video inpainting is challenging due to high data dimensionality and temporal consistency. Diffusion models are powerful but computationally expensive, limiting their video applications.

Method: Proposes internal learning (training on one video) and splits the diffusion process into intervals for efficient training/inference. Uses a lightweight model (500K parameters).

Result: Achieves or exceeds state-of-the-art performance for dynamic textures and complex backgrounds.

Conclusion: Internal learning with lightweight diffusion models is effective for video inpainting, reducing computational load while maintaining high quality.

Abstract: Video inpainting is the task of filling a region in a video in a visually
convincing manner. It is very challenging due to the high dimensionality of the
data and the temporal consistency required for obtaining convincing results.
Recently, diffusion models have shown impressive results in modeling complex
data distributions, including images and videos. Such models remain nonetheless
very expensive to train and to perform inference with, which strongly reduce
their applicability to videos, and yields unreasonable computational loads. We
show that in the case of video inpainting, thanks to the highly auto-similar
nature of videos, the training data of a diffusion model can be restricted to
the input video and still produce very satisfying results. With this internal
learning approach, where the training data is limited to a single video, our
lightweight models perform very well with only half a million parameters, in
contrast to the very large networks with billions of parameters typically found
in the literature. We also introduce a new method for efficient training and
inference of diffusion models in the context of internal learning, by splitting
the diffusion process into different learning intervals corresponding to
different noise levels of the diffusion process. We show qualitative and
quantitative results, demonstrating that our method reaches or exceeds state of
the art performance in the case of dynamic textures and complex dynamic
backgrounds

</details>


### [265] [Semantic-Syntactic Discrepancy in Images (SSDI): Learning Meaning and Order of Features from Natural Images](https://arxiv.org/abs/2401.17515)
*Chun Tao, Timur Ibrayev, Kaushik Roy*

Main category: cs.CV

TL;DR: The paper exposes classification models' vulnerability to unnatural image compositions (SSDI) and proposes 'image grammar' (semantics + syntax) to detect such discrepancies via a two-stage semi-supervised method.


<details>
  <summary>Details</summary>
Motivation: Human perception easily detects unnatural image compositions, but classification models overlook such discrepancies, prompting the need to address this vulnerability.

Method: A semi-supervised two-stage method: first learns semantic meaning of object parts, then their syntactic arrangement to form coherent objects.

Result: Achieves 70%-90% SSDI detection rates on corruptions from CelebA and SUN-RGBD datasets.

Conclusion: The proposed 'image grammar' effectively detects semantic and syntactic discrepancies in images, improving model robustness.

Abstract: Despite considerable progress in image classification tasks, classification
models seem unaffected by the images that significantly deviate from those that
appear natural to human eyes. Specifically, while human perception can easily
identify abnormal appearances or compositions in images, classification models
overlook any alterations in the arrangement of object parts as long as they are
present in any order, even if unnatural. Hence, this work exposes the
vulnerability of having semantic and syntactic discrepancy in images (SSDI) in
the form of corruptions that remove or shuffle image patches or present images
in the form of puzzles. To address this vulnerability, we propose the concept
of "image grammar", comprising "image semantics" and "image syntax". Image
semantics pertains to the interpretation of parts or patches within an image,
whereas image syntax refers to the arrangement of these parts to form a
coherent object. We present a semi-supervised two-stage method for learning the
image grammar of visual elements and environments solely from natural images.
While the first stage learns the semantic meaning of individual object parts,
the second stage learns how their relative arrangement constitutes an entire
object. The efficacy of the proposed approach is then demonstrated by achieving
SSDI detection rates ranging from 70% to 90% on corruptions generated from
CelebA and SUN-RGBD datasets. Code is publicly available at:
https://github.com/ChunTao1999/SSDI/

</details>


### [266] [SIR: Multi-view Inverse Rendering with Decomposable Shadow Under Indoor Intense Lighting](https://arxiv.org/abs/2402.06136)
*Xiaokang Wei, Zhuoman Liu, Ping Li, Yan Luximon*

Main category: cs.CV

TL;DR: SIR is a method for decomposing shadows in inverse rendering using multi-view data, improving material and lighting accuracy with a neural radiance field and shadow term.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in shadow fidelity and material decomposition in complex lighting environments for indoor scenes.

Method: Uses posed HDR images and an SDF-based neural radiance field, integrating a shadow term with a three-stage material estimation approach.

Result: Outperforms existing methods in quantitative and qualitative metrics, enabling advanced editing like relighting and material replacement.

Conclusion: SIR provides superior shadow and material decomposition, enhancing realism and editing capabilities for indoor scenes.

Abstract: We propose SIR, an efficient method to decompose differentiable shadows for
inverse rendering on indoor scenes using multi-view data, addressing the
challenges in accurately decomposing the materials and lighting conditions.
Unlike previous methods that struggle with shadow fidelity in complex lighting
environments, our approach explicitly learns shadows for enhanced realism in
material estimation under unknown light positions. Utilizing posed HDR images
as input, SIR employs an SDF-based neural radiance field for comprehensive
scene representation. Then, SIR integrates a shadow term with a three-stage
material estimation approach to improve SVBRDF quality. Specifically, SIR is
designed to learn a differentiable shadow, complemented by BRDF regularization,
to optimize inverse rendering accuracy. Extensive experiments on both synthetic
and real-world indoor scenes demonstrate the superior performance of SIR over
existing methods in both quantitative metrics and qualitative analysis. The
significant decomposing ability of SIR enables sophisticated editing
capabilities like free-view relighting, object insertion, and material
replacement. The code and data are available at
https://xiaokangwei.github.io/SIR/.

</details>


### [267] [Explicit Motion Handling and Interactive Prompting for Video Camouflaged Object Detection](https://arxiv.org/abs/2403.01968)
*Xin Zhang, Tao Xiao, Gepeng Ji, Xuan Wu, Keren Fu, Qijun Zhao*

Main category: cs.CV

TL;DR: EMIP is a novel framework for video camouflaged object detection (VCOD) that explicitly handles motion cues using a two-stream architecture with interactive prompting, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing VCOD methods rely on noisy motion estimation or implicit motion modeling, limiting performance in dynamic scenes. EMIP addresses this by explicitly leveraging motion cues.

Method: EMIP uses a two-stream architecture for camouflaged segmentation and optical flow estimation, with interactive prompting via camouflaged feeder and motion collector modules. It incorporates self-supervised learning for motion prompts and historical data for robustness.

Result: EMIP sets new state-of-the-art records on VCOD benchmarks, demonstrating superior performance in complex dynamic scenes.

Conclusion: EMIP's explicit motion handling and interactive prompting framework significantly improves VCOD performance, offering a robust solution for dynamic scenes.

Abstract: Camouflage poses challenges in distinguishing a static target, whereas any
movement of the target can break this disguise. Existing video camouflaged
object detection (VCOD) approaches take noisy motion estimation as input or
model motion implicitly, restricting detection performance in complex dynamic
scenes. In this paper, we propose a novel Explicit Motion handling and
Interactive Prompting framework for VCOD, dubbed EMIP, which handles motion
cues explicitly using a frozen pre-trained optical flow fundamental model. EMIP
is characterized by a two-stream architecture for simultaneously conducting
camouflaged segmentation and optical flow estimation. Interactions across the
dual streams are realized in an interactive prompting way that is inspired by
emerging visual prompt learning. Two learnable modules, i.e., the camouflaged
feeder and motion collector, are designed to incorporate segmentation-to-motion
and motion-to-segmentation prompts, respectively, and enhance outputs of the
both streams. The prompt fed to the motion stream is learned by supervising
optical flow in a self-supervised manner. Furthermore, we show that long-term
historical information can also be incorporated as a prompt into EMIP and
achieve more robust results with temporal consistency. Experimental results
demonstrate that our EMIP achieves new state-of-the-art records on popular VCOD
benchmarks. Our code is made publicly available at
https://github.com/zhangxin06/EMIP.

</details>


### [268] [Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation](https://arxiv.org/abs/2409.09497)
*Hugo Porta, Emanuele Dalsasso, Diego Marcos, Devis Tuia*

Main category: cs.CV

TL;DR: The paper proposes a method for interpretable semantic segmentation using multi-scale prototypical part learning, improving sparsity and interpretability while narrowing the performance gap with non-interpretable models.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability in semantic segmentation by linking predictions to learned prototypical patterns and leveraging multi-scale representations.

Method: Introduces a prototype layer for multi-scale prototypical part learning and a sparse grouping mechanism to understand interactions between scales.

Result: Experiments on Pascal VOC, Cityscapes, and ADE20K show improved sparsity, interpretability, and performance close to non-interpretable models.

Conclusion: The method advances interpretable semantic segmentation by effectively combining multi-scale representations and prototype learning.

Abstract: Prototypical part learning is emerging as a promising approach for making
semantic segmentation interpretable. The model selects real patches seen during
training as prototypes and constructs the dense prediction map based on the
similarity between parts of the test image and the prototypes. This improves
interpretability since the user can inspect the link between the predicted
output and the patterns learned by the model in terms of prototypical
information. In this paper, we propose a method for interpretable semantic
segmentation that leverages multi-scale image representation for prototypical
part learning. First, we introduce a prototype layer that explicitly learns
diverse prototypical parts at several scales, leading to multi-scale
representations in the prototype activation output. Then, we propose a sparse
grouping mechanism that produces multi-scale sparse groups of these
scale-specific prototypical parts. This provides a deeper understanding of the
interactions between multi-scale object representations while enhancing the
interpretability of the segmentation model. The experiments conducted on Pascal
VOC, Cityscapes, and ADE20K demonstrate that the proposed method increases
model sparsity, improves interpretability over existing prototype-based
methods, and narrows the performance gap with the non-interpretable counterpart
models. Code is available at github.com/eceo-epfl/ScaleProtoSeg.

</details>


### [269] [Real-Time Image Segmentation via Hybrid Convolutional-Transformer Architecture Search](https://arxiv.org/abs/2403.10413)
*Hongyuan Yu, Cheng Wan, Xiyang Dai, Mengchen Liu, Dongdong Chen, Bin Xiao, Yan Huang, Yuan Lu, Liang Wang*

Main category: cs.CV

TL;DR: The paper introduces HyCTAS, a method for efficiently integrating multi-head self-attention into high-resolution CNNs using architecture search, achieving superior performance in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Designing effective segmentation architectures is labor-intensive. The paper aims to automate this by optimizing the placement of self-attention modules in CNNs for efficiency and performance.

Method: A multi-target multi-branch supernet method is developed to find optimal hybrid combinations of convolution and self-attention layers, optimized for objectives like latency and mIoU.

Result: HyCTAS outperforms previous methods in semantic and panoptic segmentation, demonstrating efficiency and effectiveness.

Conclusion: The proposed HyCTAS method successfully automates architecture design for segmentation, balancing performance and efficiency.

Abstract: Image segmentation is one of the most fundamental problems in computer vision
and has drawn a lot of attention due to its vast applications in image
understanding and autonomous driving. However, designing effective and
efficient segmentation neural architectures is a labor-intensive process that
may require numerous trials by human experts. In this paper, we address the
challenge of integrating multi-head self-attention into high-resolution
representation CNNs efficiently by leveraging architecture search. Manually
replacing convolution layers with multi-head self-attention is non-trivial due
to the costly overhead in memory to maintain high resolution. By contrast, we
develop a multi-target multi-branch supernet method, which not only fully
utilizes the advantages of high-resolution features but also finds the proper
location for placing the multi-head self-attention module. Our search algorithm
is optimized towards multiple objectives (e.g., latency and mIoU) and is
capable of finding architectures on the Pareto frontier with an arbitrary
number of branches in a single search. We further present a series of models
via the Hybrid Convolutional-Transformer Architecture Search (HyCTAS) method
that searches for the best hybrid combination of light-weight convolution
layers and memory-efficient self-attention layers between branches from
different resolutions and fuses them to high resolution for both efficiency and
effectiveness. Extensive experiments demonstrate that HyCTAS outperforms
previous methods in both semantic segmentation and panoptic segmentation tasks.
Code and models are available at https://github.com/MarvinYu1995/HyCTAS.

</details>


### [270] [LaneCorrect: Self-supervised Lane Detection](https://arxiv.org/abs/2404.14671)
*Ming Nie, Xinyue Cai, Hang Xu, Li Zhang*

Main category: cs.CV

TL;DR: The paper presents an unsupervised method for lane detection using LiDAR point clouds and a self-supervised training scheme (LaneCorrect) to correct noisy labels, achieving strong performance on benchmarks without human annotations.


<details>
  <summary>Details</summary>
Motivation: To develop a generalized computer vision system for lane detection without relying on annotated data, addressing domain gaps and complex environments.

Method: Unsupervised 3D lane segmentation from LiDAR, projection to 2D, self-supervised label correction (LaneCorrect), and distillation for target lane detection.

Result: Outperforms supervised methods on benchmarks (TuSimple, CULane, CurveLanes, LLAMAS) and reduces domain gaps effectively.

Conclusion: The proposed unsupervised approach is robust, generalizable, and competitive with supervised methods, demonstrating practical viability for autonomous driving.

Abstract: Lane detection has evolved highly functional autonomous driving system to
understand driving scenes even under complex environments. In this paper, we
work towards developing a generalized computer vision system able to detect
lanes without using any annotation. We make the following contributions: (i) We
illustrate how to perform unsupervised 3D lane segmentation by leveraging the
distinctive intensity of lanes on the LiDAR point cloud frames, and then obtain
the noisy lane labels in the 2D plane by projecting the 3D points; (ii) We
propose a novel self-supervised training scheme, dubbed LaneCorrect, that
automatically corrects the lane label by learning geometric consistency and
instance awareness from the adversarial augmentations; (iii) With the
self-supervised pre-trained model, we distill to train a student network for
arbitrary target lane (e.g., TuSimple) detection without any human labels; (iv)
We thoroughly evaluate our self-supervised method on four major lane detection
benchmarks (including TuSimple, CULane, CurveLanes and LLAMAS) and demonstrate
excellent performance compared with existing supervised counterpart, whilst
showing more effective results on alleviating the domain gap, i.e., training on
CULane and test on TuSimple.

</details>


### [271] [ReDistill: Residual Encoded Distillation for Peak Memory Reduction of CNNs](https://arxiv.org/abs/2406.03744)
*Fang Chen, Gourav Datta, Mujahid Al Rafi, Hyeran Jeon, Meng Tang*

Main category: cs.CV

TL;DR: ReDistill reduces peak memory in neural networks via teacher-student distillation with aggressive pooling, achieving 4x-5x memory reduction with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Addressing high memory and power demands of modern computer vision models for deployment in resource-constrained edge devices.

Method: Proposes residual encoded distillation (ReDistill) in a teacher-student framework using aggressive pooling to derive a memory-efficient student network.

Result: 4x-5x peak memory reduction for CNNs with minimal accuracy loss; 4x reduction for diffusion models while maintaining image quality.

Conclusion: ReDistill outperforms other distillation methods, offering a practical solution for memory-efficient model deployment.

Abstract: The expansion of neural network sizes and the enhanced resolution of modern
image sensors result in heightened memory and power demands to process modern
computer vision models. In order to deploy these models in extremely
resource-constrained edge devices, it is crucial to reduce their peak memory,
which is the maximum memory consumed during the execution of a model. A naive
approach to reducing peak memory is aggressive down-sampling of feature maps
via pooling with large stride, which often results in unacceptable degradation
in network performance. To mitigate this problem, we propose residual encoded
distillation (ReDistill) for peak memory reduction in a teacher-student
framework, in which a student network with less memory is derived from the
teacher network using aggressive pooling. We apply our distillation method to
multiple problems in computer vision, including image classification and
diffusion-based image generation. For image classification, our method yields
4x-5x theoretical peak memory reduction with less degradation in accuracy for
most CNN-based architectures. For diffusion-based image generation, our
proposed distillation method yields a denoising network with 4x lower
theoretical peak memory while maintaining decent diversity and fidelity for
image generation. Experiments demonstrate our method's superior performance
compared to other feature-based and response-based distillation methods when
applied to the same student network. The code is available at
https://github.com/mengtang-lab/ReDistill.

</details>


### [272] [Dynamic Integration of Task-Specific Adapters for Class Incremental Learning](https://arxiv.org/abs/2409.14983)
*Jiashuo Li, Shaokun Wang, Bo Qian, Yuhang He, Xing Wei, Qiang Wang, Yihong Gong*

Main category: cs.CV

TL;DR: The paper introduces DIA, a framework for Non-exemplar Class Incremental Learning (NECIL), addressing catastrophic forgetting with task-specific adapters and patch-level alignment.


<details>
  <summary>Details</summary>
Motivation: To tackle catastrophic forgetting in NECIL without storing old exemplars, ensuring privacy and storage efficiency.

Method: Proposes Dynamic Integration of task-specific Adapters (DIA) with Task-Specific Adapter Integration (TSAI) and Patch-Level Model Alignment (PDL and PFR).

Result: DIA significantly improves performance on benchmark datasets, balancing computational complexity and accuracy.

Conclusion: DIA effectively mitigates catastrophic forgetting in NECIL, offering a flexible and efficient solution.

Abstract: Non-exemplar class Incremental Learning (NECIL) enables models to
continuously acquire new classes without retraining from scratch and storing
old task exemplars, addressing privacy and storage issues. However, the absence
of data from earlier tasks exacerbates the challenge of catastrophic forgetting
in NECIL. In this paper, we propose a novel framework called Dynamic
Integration of task-specific Adapters (DIA), which comprises two key
components: Task-Specific Adapter Integration (TSAI) and Patch-Level Model
Alignment. TSAI boosts compositionality through a patch-level adapter
integration strategy, which provides a more flexible compositional solution
while maintaining low computation costs. Patch-Level Model Alignment maintains
feature consistency and accurate decision boundaries via two specialized
mechanisms: Patch-Level Distillation Loss (PDL) and Patch-Level Feature
Reconstruction method (PFR). Specifically, the PDL preserves feature-level
consistency between successive models by implementing a distillation loss based
on the contributions of patch tokens to new class learning. The PFR facilitates
accurate classifier alignment by reconstructing old class features from
previous tasks that adapt to new task knowledge. Extensive experiments validate
the effectiveness of our DIA, revealing significant improvements on benchmark
datasets in the NECIL setting, maintaining an optimal balance between
computational complexity and accuracy.

</details>


### [273] [Hybrid Video Anomaly Detection for Anomalous Scenarios in Autonomous Driving](https://arxiv.org/abs/2406.06423)
*Daniel Bogdoll, Jan Imhof, Tim Joseph, Svetlana Pavlitska, J. Marius Zöllner*

Main category: cs.CV

TL;DR: HF$^2$-VAD$_{AD}$ adapts a surveillance anomaly detection method for autonomous driving, focusing on rare and critical scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in anomaly detection for autonomous driving, especially in rare and critical scenarios.

Method: Adapts HF$^2$-VAD for autonomous driving, learning normality from ego perspective and evaluating pixel-wise anomalies.

Result: Demonstrates effectiveness in detecting anomalies in rare and critical driving scenarios.

Conclusion: HF$^2$-VAD$_{AD}$ is a promising approach for anomaly detection in autonomous driving.

Abstract: In autonomous driving, the most challenging scenarios can only be detected
within their temporal context. Most video anomaly detection approaches focus
either on surveillance or traffic accidents, which are only a subfield of
autonomous driving. We present HF$^2$-VAD$_{AD}$, a variation of the HF$^2$-VAD
surveillance video anomaly detection method for autonomous driving. We learn a
representation of normality from a vehicle's ego perspective and evaluate
pixel-wise anomaly detections in rare and critical scenarios.

</details>


### [274] [Domain Consistency Representation Learning for Lifelong Person Re-Identification](https://arxiv.org/abs/2409.19954)
*Shiben Liu, Qiang Wang, Huijie Fan, Weihong Ren, Baojie Fan, Yandong Tang*

Main category: cs.CV

TL;DR: The paper proposes a Domain Consistency Representation (DCR) model to balance intra-domain discrimination and inter-domain gaps in Lifelong Person Re-Identification (LReID), using global and attribute-wise representations, along with anti-forgetting and knowledge consolidation strategies.


<details>
  <summary>Details</summary>
Motivation: The contradictory relationship between intra-domain discrimination (focusing on individual nuances) and inter-domain gaps (emphasizing domain consistency) in LReID poses a challenge. Existing methods often neglect intra-domain discrimination while reducing inter-domain gaps.

Method: The DCR model leverages global and attribute-wise representations to balance intra-domain discrimination and inter-domain gaps. It includes an attribute-oriented anti-forgetting strategy and a knowledge consolidation strategy to prevent catastrophic forgetting and enhance knowledge transfer.

Result: Extensive experiments demonstrate that the DCR model outperforms state-of-the-art LReID methods.

Conclusion: The DCR model effectively addresses the trade-off between intra-domain discrimination and inter-domain gaps, achieving superior performance in LReID tasks.

Abstract: Lifelong person re-identification (LReID) exhibits a contradictory
relationship between intra-domain discrimination and inter-domain gaps when
learning from continuous data. Intra-domain discrimination focuses on
individual nuances (i.e., clothing type, accessories, etc.), while inter-domain
gaps emphasize domain consistency. Achieving a trade-off between maximizing
intra-domain discrimination and minimizing inter-domain gaps is a crucial
challenge for improving LReID performance. Most existing methods strive to
reduce inter-domain gaps through knowledge distillation to maintain domain
consistency. However, they often ignore intra-domain discrimination. To address
this challenge, we propose a novel domain consistency representation learning
(DCR) model that explores global and attribute-wise representations as a bridge
to balance intra-domain discrimination and inter-domain gaps. At the
intra-domain level, we explore the complementary relationship between global
and attribute-wise representations to improve discrimination among similar
identities. Excessive learning intra-domain discrimination can lead to
catastrophic forgetting. We further develop an attribute-oriented
anti-forgetting (AF) strategy that explores attribute-wise representations to
enhance inter-domain consistency, and propose a knowledge consolidation (KC)
strategy to facilitate knowledge transfer. Extensive experiments show that our
DCR model achieves superior performance compared to state-of-the-art LReID
methods. Our code is publicly available at https://github.com/LiuShiBen/DCR.

</details>


### [275] [Accessible, At-Home Detection of Parkinson's Disease via Multi-task Video Analysis](https://arxiv.org/abs/2406.14856)
*Md Saiful Islam, Tariq Adnan, Jan Freyberg, Sangwu Lee, Abdelrahman Abdelkader, Meghan Pawlik, Cathe Schwartz, Karen Jaffe, Ruth B. Schneider, E Ray Dorsey, Ehsan Hoque*

Main category: cs.CV

TL;DR: A novel Uncertainty-calibrated Fusion Network (UFNet) uses multimodal data (finger tapping, facial expression, speech) for accurate Parkinson's Disease (PD) detection, outperforming single-task models and enabling accessible home-based screening.


<details>
  <summary>Details</summary>
Motivation: Limited neurological care access leads to underdiagnosed PD. Existing AI methods focus on unimodal analysis, missing the disease's multifaceted nature.

Method: UFNet leverages a large-scale multi-task video dataset (1102 sessions from 845 participants) with Monte Carlo Dropout for uncertainty quantification and self-attended feature fusion.

Result: UFNet achieved 88.0% accuracy, 93.0% AUROC, 79.3% sensitivity, and 92.6% specificity, with no detectable bias across subgroups.

Conclusion: UFNet enables accessible PD screening via webcam/microphone, particularly beneficial for regions with limited healthcare resources.

Abstract: Limited accessibility to neurological care leads to underdiagnosed
Parkinson's Disease (PD), preventing early intervention. Existing AI-based PD
detection methods primarily focus on unimodal analysis of motor or speech
tasks, overlooking the multifaceted nature of the disease. To address this, we
introduce a large-scale, multi-task video dataset consisting of 1102 sessions
(each containing videos of finger tapping, facial expression, and speech tasks
captured via webcam) from 845 participants (272 with PD). We propose a novel
Uncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal
data to enhance diagnostic accuracy. UFNet employs independent task-specific
networks, trained with Monte Carlo Dropout for uncertainty quantification,
followed by self-attended fusion of features, with attention weights
dynamically adjusted based on task-specific uncertainties. To ensure
patient-centered evaluation, the participants were randomly split into three
sets: 60% for training, 20% for model selection, and 20% for final performance
evaluation. UFNet significantly outperformed single-task models in terms of
accuracy, area under the ROC curve (AUROC), and sensitivity while maintaining
non-inferior specificity. Withholding uncertain predictions further boosted the
performance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9%
sensitivity, and 92.6+-0.3% specificity, at the expense of not being able to
predict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further
analysis suggests that the trained model does not exhibit any detectable bias
across sex and ethnic subgroups and is most effective for individuals aged
between 50 and 80. Requiring only a webcam and microphone, our approach
facilitates accessible home-based PD screening, especially in regions with
limited healthcare resources.

</details>


### [276] [GS-ROR$^2$: Bidirectional-guided 3DGS and SDF for Reflective Object Relighting and Reconstruction](https://arxiv.org/abs/2406.18544)
*Zuo-Liang Zhu, Beibei Wang, Jian Yang*

Main category: cs.CV

TL;DR: The paper proposes a bidirectional guidance method combining 3D Gaussian Splatting (3DGS) and SDF for relightable 3D assets and high-quality geometry reconstruction, addressing limitations of each approach.


<details>
  <summary>Details</summary>
Motivation: Current methods like 3DGS struggle with relightable assets and geometry reconstruction, especially for reflective objects, while SDF methods are slow and lack detail.

Method: The approach uses mutual supervision between 3DGS and SDF, including SDF-aided Gaussian splatting for relighting and GS-guided SDF refinement for geometry.

Result: The method achieves realistic relighting and high-quality meshes for reflective objects with minimal extra training time (17%).

Conclusion: The bidirectional guidance effectively combines the strengths of 3DGS and SDF, improving both relighting and geometry reconstruction.

Abstract: 3D Gaussian Splatting (3DGS) has shown a powerful capability for novel view
synthesis due to its detailed expressive ability and highly efficient rendering
speed. Unfortunately, creating relightable 3D assets and reconstructing
faithful geometry with 3DGS is still problematic, particularly for reflective
objects, as its discontinuous representation raises difficulties in
constraining geometries. Volumetric signed distance field (SDF) methods provide
robust geometry reconstruction, while the expensive ray marching hinders its
real-time application and slows the training. Besides, these methods struggle
to capture sharp geometric details. To this end, we propose to guide 3DGS and
SDF bidirectionally in a complementary manner, including an SDF-aided Gaussian
splatting for efficient optimization of the relighting model and a GS-guided
SDF enhancement for high-quality geometry reconstruction. At the core of our
SDF-aided Gaussian splatting is the mutual supervision of the depth and normal
between blended Gaussians and SDF, which avoids the expensive volume rendering
of SDF. Thanks to this mutual supervision, the learned blended Gaussians are
well-constrained with a minimal time cost. As the Gaussians are rendered in a
deferred shading mode, the alpha-blended Gaussians are smooth, while individual
Gaussians may still be outliers, yielding floater artifacts. Therefore, we
introduce an SDF-aware pruning strategy to remove Gaussian outliers located
distant from the surface defined by SDF, avoiding floater issue. This way, our
GS framework provides reasonable normal and achieves realistic relighting,
while the mesh from depth is still problematic. Therefore, we design a
GS-guided SDF refinement, which utilizes the blended normal from Gaussians to
finetune SDF. With this enhancement, our method can further provide
high-quality meshes for reflective objects at the cost of 17% extra training
time.

</details>


### [277] [Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content](https://arxiv.org/abs/2410.08260)
*Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, Fei Yang, Pengfei Wan, Di Zhang*

Main category: cs.CV

TL;DR: The paper introduces Koala-36M, a high-quality video dataset addressing limitations in temporal splitting, captions, and video quality filtering to enhance video generation models.


<details>
  <summary>Details</summary>
Motivation: Existing video datasets lack quality in temporal splitting, detailed captions, and video quality filtering, which are crucial for video generation models.

Method: The authors improve dataset quality by ensuring temporal consistency with a linear classifier, providing detailed captions (200 words avg.), and filtering videos using a Video Training Suitability Score (VTSS).

Result: Koala-36M demonstrates improved consistency between fine-grained conditions and video content, validated by experiments.

Conclusion: The proposed dataset and processing pipeline effectively address dataset quality issues, enhancing video generation model performance.

Abstract: With the continuous progress of visual generation technologies, the scale of
video datasets has grown exponentially. The quality of these datasets plays a
pivotal role in the performance of video generation models. We assert that
temporal splitting, detailed captions, and video quality filtering are three
crucial determinants of dataset quality. However, existing datasets exhibit
various limitations in these areas. To address these challenges, we introduce
Koala-36M, a large-scale, high-quality video dataset featuring accurate
temporal splitting, detailed captions, and superior video quality. The essence
of our approach lies in improving the consistency between fine-grained
conditions and video content. Specifically, we employ a linear classifier on
probability distributions to enhance the accuracy of transition detection,
ensuring better temporal consistency. We then provide structured captions for
the splitted videos, with an average length of 200 words, to improve text-video
alignment. Additionally, we develop a Video Training Suitability Score (VTSS)
that integrates multiple sub-metrics, allowing us to filter high-quality videos
from the original corpus. Finally, we incorporate several metrics into the
training process of the generation model, further refining the fine-grained
conditions. Our experiments demonstrate the effectiveness of our data
processing pipeline and the quality of the proposed Koala-36M dataset. Our
dataset and code have been released at https://koala36m.github.io/.

</details>


### [278] [DD-rPPGNet: De-interfering and Descriptive Feature Learning for Unsupervised rPPG Estimation](https://arxiv.org/abs/2407.21402)
*Pei-Kai Huang, Tzu-Hsien Chen, Ya-Ting Chan, Kuan-Wen Chen, Chiou-Ting Hsu*

Main category: cs.CV

TL;DR: Proposes DD-rPPGNet, an unsupervised method to eliminate interference in rPPG signals, improving HR estimation from facial videos.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised rPPG methods ignore interference, leading to poor performance.

Method: Uses contrastive learning and a 3D Learnable Descriptive Convolution to de-interfere and enhance rPPG signals.

Result: Outperforms unsupervised methods and matches supervised ones on benchmark datasets.

Conclusion: DD-rPPGNet effectively removes interference, improving rPPG signal quality and HR estimation.

Abstract: Remote Photoplethysmography (rPPG) aims to measure physiological signals and
Heart Rate (HR) from facial videos. Recent unsupervised rPPG estimation methods
have shown promising potential in estimating rPPG signals from facial regions
without relying on ground truth rPPG signals. However, these methods seem
oblivious to interference existing in rPPG signals and still result in
unsatisfactory performance. In this paper, we propose a novel De-interfered and
Descriptive rPPG Estimation Network (DD-rPPGNet) to eliminate the interference
within rPPG features for learning genuine rPPG signals. First, we investigate
the characteristics of local spatial-temporal similarities of interference and
design a novel unsupervised model to estimate the interference. Next, we
propose an unsupervised de-interfered method to learn genuine rPPG signals with
two stages. In the first stage, we estimate the initial rPPG signals by
contrastive learning from both the training data and their augmented
counterparts. In the second stage, we use the estimated interference features
to derive de-interfered rPPG features and encourage the rPPG signals to be
distinct from the interference. In addition, we propose an effective
descriptive rPPG feature learning by developing a strong 3D Learnable
Descriptive Convolution (3DLDC) to capture the subtle chrominance changes for
enhancing rPPG estimation. Extensive experiments conducted on five rPPG
benchmark datasets demonstrate that the proposed DD-rPPGNet outperforms
previous unsupervised rPPG estimation methods and achieves competitive
performances with state-of-the-art supervised rPPG methods. The code is
available at: https://github.com/Pei-KaiHuang/TIFS2025-DD-rPPGNet

</details>


### [279] [Free-Mask: A Novel Paradigm of Integration Between the Segmentation Diffusion Model and Image Editing](https://arxiv.org/abs/2411.01819)
*Bo Gao, Jianhui Wang, Xinyuan Song, Yangfan He, Fangxu Xing, Tianyu Shi*

Main category: cs.CV

TL;DR: Free-Mask leverages text-to-image models to generate synthetic datasets with multiple objects, reducing manual annotation effort and improving segmentation model performance.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for semantic segmentation is time-consuming and resource-intensive. Existing methods using text-to-image models are limited to single-instance images.

Method: Combines a Diffusion Model with image editing to integrate multiple objects into images, generating realistic datasets and accurate masks.

Result: Outperforms models trained on real data, especially in zero-shot settings, and achieves state-of-the-art results on unseen VOC 2012 classes.

Conclusion: Free-Mask effectively addresses the limitations of manual annotation and single-instance generation, enhancing dataset diversity and model performance.

Abstract: Current semantic segmentation models typically require a substantial amount
of manually annotated data, a process that is both time-consuming and
resource-intensive. Alternatively, leveraging advanced text-to-image models
such as Midjourney and Stable Diffusion has emerged as an efficient strategy,
enabling the automatic generation of synthetic data in place of manual
annotations. However, previous methods have been limited to generating
single-instance images, as the generation of multiple instances with Stable
Diffusion has proven unstable. To address this limitation and expand the scope
and diversity of synthetic datasets, we propose a framework \textbf{Free-Mask}
that combines a Diffusion Model for segmentation with advanced image editing
capabilities, allowing for the integration of multiple objects into images via
text-to-image models. Our method facilitates the creation of highly realistic
datasets that closely emulate open-world environments while generating accurate
segmentation masks. It reduces the labor associated with manual annotation and
also ensures precise mask generation. Experimental results demonstrate that
synthetic data generated by \textbf{Free-Mask} enables segmentation models to
outperform those trained on real data, especially in zero-shot settings.
Notably, \textbf{Free-Mask} achieves new state-of-the-art results on previously
unseen classes in the VOC 2012 benchmark.

</details>


### [280] [Position: From Correlation to Causation: Max-Pooling-Based Multi-Instance Learning Leads to More Robust Whole Slide Image Classification](https://arxiv.org/abs/2408.09449)
*Xin Liu, Weijia Zhang, Min-Ling Zhang*

Main category: cs.CV

TL;DR: The paper critiques attention-based MIL for WSI classification, proposing FocusMIL, a max-pooling-based method that avoids spurious correlations and outperforms attention-based approaches.


<details>
  <summary>Details</summary>
Motivation: Attention-based MIL methods often focus on irrelevant patterns, leading to unreliable predictions. The paper aims to address this by revisiting max-pooling-based methods.

Method: The authors analyze the flaws in attention-based MIL and propose FocusMIL, a max-pooling-based approach designed to rely on causal factors.

Result: FocusMIL outperforms existing attention-based methods on two datasets, demonstrating robustness and better interpretability.

Conclusion: The paper advocates for max-pooling-based methods like FocusMIL as a more reliable alternative to attention-based MIL for WSI classification.

Abstract: Although attention-based multi-instance learning (MIL) algorithms have
achieved impressive performance on slide-level whole slide image (WSI)
classification tasks, they are prone to mistakenly focusing on irrelevant
patterns such as staining conditions and tissue morphology, leading to
incorrect patch-level predictions and unreliable interpretability. In this
paper, we analyze why attention-based methods tend to rely on spurious
correlations in their predictions. Furthermore, we revisit max-pooling-based
approaches and examine the reasons behind the underperformance of existing
methods. We argue that well-trained max-pooling-based MIL models can make
predictions based on causal factors and avoid relying on spurious correlations.
Building on these insights, we propose a simple yet effective max-pooling-based
MIL method (FocusMIL) that outperforms existing mainstream attention-based
methods on two datasets. In this position paper, we advocate renewed attention
to max-pooling-based methods to achieve more robust and interpretable
predictions.

</details>


### [281] [OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision](https://arxiv.org/abs/2411.07199)
*Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, Wenhu Chen*

Main category: cs.CV

TL;DR: OmniEdit is an advanced image editor addressing limitations of current methods by handling diverse tasks, improving data quality, and supporting any aspect ratio.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods are limited by biased synthesis, noisy datasets, and fixed resolutions, hindering practical use.

Method: OmniEdit uses supervision from specialist models, importance sampling with large multimodal models, a new EditNet architecture, and diverse aspect ratio training.

Result: OmniEdit outperforms existing models in both automatic and human evaluations.

Conclusion: OmniEdit bridges the gap to practical applications by addressing key challenges in image editing.

Abstract: Instruction-guided image editing methods have demonstrated significant
potential by training diffusion models on automatically synthesized or manually
annotated image editing pairs. However, these methods remain far from
practical, real-life applications. We identify three primary challenges
contributing to this gap. Firstly, existing models have limited editing skills
due to the biased synthesis process. Secondly, these methods are trained with
datasets with a high volume of noise and artifacts. This is due to the
application of simple filtering methods like CLIP-score. Thirdly, all these
datasets are restricted to a single low resolution and fixed aspect ratio,
limiting the versatility to handle real-world use cases. In this paper, we
present \omniedit, which is an omnipotent editor to handle seven different
image editing tasks with any aspect ratio seamlessly. Our contribution is in
four folds: (1) \omniedit is trained by utilizing the supervision from seven
different specialist models to ensure task coverage. (2) we utilize importance
sampling based on the scores provided by large multimodal models (like GPT-4o)
instead of CLIP-score to improve the data quality. (3) we propose a new editing
architecture called EditNet to greatly boost the editing success rate, (4) we
provide images with different aspect ratios to ensure that our model can handle
any image in the wild. We have curated a test set containing images of
different aspect ratios, accompanied by diverse instructions to cover different
tasks. Both automatic evaluation and human evaluations demonstrate that
\omniedit can significantly outperform all the existing models. Our code,
dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/

</details>


### [282] [Attention-Guided Multi-scale Interaction Network for Face Super-Resolution](https://arxiv.org/abs/2409.00591)
*Xujie Wan, Wenjie Li, Guangwei Gao, Huimin Lu, Jian Yang, Chia-Wen Lin*

Main category: cs.CV

TL;DR: AMINet, a CNN-Transformer hybrid network, enhances face super-resolution (FSR) by fusing multi-scale features through local-global and encoder-decoder interactions, outperforming existing methods with efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing hybrid networks for FSR lack effective multi-scale feature fusion, limiting performance. AMINet addresses this gap.

Method: Proposes AMINet with Local and Global Feature Interaction Module (LGFI) and Selective Kernel Attention Fusion Module (SKAF) for adaptive feature fusion.

Result: AMINet achieves superior FSR performance with lower computational cost and faster inference.

Conclusion: AMINet's design effectively enhances multi-scale feature complementarity, advancing FSR efficiency and performance.

Abstract: Recently, CNN and Transformer hybrid networks demonstrated excellent
performance in face super-resolution (FSR) tasks. Since numerous features at
different scales in hybrid networks, how to fuse these multi-scale features and
promote their complementarity is crucial for enhancing FSR. However, existing
hybrid network-based FSR methods ignore this, only simply combining the
Transformer and CNN. To address this issue, we propose an attention-guided
Multi-scale interaction network (AMINet), which contains local and global
feature interactions and encoder-decoder phase feature interactions.
Specifically, we propose a Local and Global Feature Interaction Module (LGFI)
to promote fusions of global features and different receptive fields' local
features extracted by our Residual Depth Feature Extraction Module (RDFE).
Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to
adaptively select fusions of different features within LGFI and encoder-decoder
phases. Our above design allows the free flow of multi-scale features from
within modules and between encoder and decoder, which can promote the
complementarity of different scale features to enhance FSR. Comprehensive
experiments confirm that our method consistently performs well with less
computational consumption and faster inference.

</details>


### [283] [SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input](https://arxiv.org/abs/2411.11934)
*Zhen Lv, Yangqi Long, Congzhentao Huang, Cao Li, Chengfei Lv, Hao Ren, Dian Zheng*

Main category: cs.CV

TL;DR: SpatialDreamer introduces a self-supervised stereo video synthesis method using a video diffusion model, addressing data insufficiency and spatio-temporal consistency challenges.


<details>
  <summary>Details</summary>
Motivation: The task of stereo video synthesis from monocular input lacks high-quality paired training data and struggles with spatio-temporal consistency. Existing NVS techniques fall short in dynamic scene representation and require excessive training data.

Method: Proposes Depth-based Video Generation (DVG) for paired video creation, RefinerNet for efficient training, and a consistency control module (stereo deviation metric and Temporal Interaction Learning) for geometric and temporal consistency.

Result: Outperforms benchmark methods in evaluations.

Conclusion: SpatialDreamer effectively addresses stereo video synthesis challenges with innovative modules and self-supervised learning.

Abstract: Stereo video synthesis from a monocular input is a demanding task in the
fields of spatial computing and virtual reality. The main challenges of this
task lie on the insufficiency of high-quality paired stereo videos for training
and the difficulty of maintaining the spatio-temporal consistency between
frames. Existing methods primarily address these issues by directly applying
novel view synthesis (NVS) techniques to video, while facing limitations such
as the inability to effectively represent dynamic scenes and the requirement
for large amounts of training data. In this paper, we introduce a novel
self-supervised stereo video synthesis paradigm via a video diffusion model,
termed SpatialDreamer, which meets the challenges head-on. Firstly, to address
the stereo video data insufficiency, we propose a Depth based Video Generation
module DVG, which employs a forward-backward rendering mechanism to generate
paired videos with geometric and temporal priors. Leveraging data generated by
DVG, we propose RefinerNet along with a self-supervised synthetic framework
designed to facilitate efficient and dedicated training. More importantly, we
devise a consistency control module, which consists of a metric of stereo
deviation strength and a Temporal Interaction Learning module TIL for geometric
and temporal consistency ensurance respectively. We evaluated the proposed
method against various benchmark methods, with the results showcasing its
superior performance.

</details>


### [284] [Free-DyGS: Camera-Pose-Free Scene Reconstruction for Dynamic Surgical Videos with Gaussian Splatting](https://arxiv.org/abs/2409.01003)
*Qian Li, Shuojue Yang, Daiyun Shen, Jimmy Bok Yan So, Jing Qin, Yueming Jin*

Main category: cs.CV

TL;DR: The paper introduces Free-DyGS, a framework for high-fidelity reconstruction of dynamic surgical scenes with a moving camera, using Gaussian Splitting (GS) and novel techniques like Sparse Gaussian Regressor (SGR) and Retrospective Deformation Recapitulation (RDR).


<details>
  <summary>Details</summary>
Motivation: Existing methods either assume fixed camera poses or static scenes, limiting their applicability in realistic surgical scenarios. This paper addresses the challenge of free-pose reconstruction for highly dynamic surgical scenes.

Method: The proposed Free-DyGS framework uses GS for fast reconstruction, initializing scenes with a pre-trained SGR. It jointly optimizes deformation models and camera poses frame-by-frame, employs Scene Expansion for unseen regions, and uses RDR to preserve deformations.

Result: Experiments on StereoMIS and Hamlyn datasets show Free-DyGS outperforms other methods in rendering accuracy and efficiency.

Conclusion: Free-DyGS effectively tackles the challenging setup of dynamic surgical scene reconstruction with a moving camera, offering superior performance and practicality.

Abstract: High-fidelity reconstruction of surgical scene is a fundamentally crucial
task to support many applications, such as intra-operative navigation and
surgical education. However, most existing methods assume the ideal surgical
scenarios - either focus on dynamic reconstruction with deforming tissue yet
assuming a given fixed camera pose, or allow endoscope movement yet
reconstructing the static scenes. In this paper, we target at a more realistic
yet challenging setup - free-pose reconstruction with a moving camera for
highly dynamic surgical scenes. Meanwhile, we take the first step to introduce
Gaussian Splitting (GS) technique to tackle this challenging setting and
propose a novel GS-based framework for fast reconstruction, termed
\textit{Free-DyGS}. Concretely, our model embraces a novel scene initialization
in which a pre-trained Sparse Gaussian Regressor (SGR) can efficiently
parameterize the initial attributes. For each subsequent frame, we propose to
jointly optimize the deformation model and 6D camera poses in a frame-by-frame
manner, easing training given the limited deformation differences between
consecutive frames. A Scene Expansion scheme is followed to expand the GS model
for the unseen regions introduced by the moving camera. Moreover, the framework
is equipped with a novel Retrospective Deformation Recapitulation (RDR)
strategy to preserve the entire-clip deformations throughout the frame-by-frame
training scheme. The efficacy of the proposed Free-DyGS is substantiated
through extensive experiments on two datasets: StereoMIS and Hamlyn datasets.
The experimental outcomes underscore that Free-DyGS surpasses other advanced
methods in both rendering accuracy and efficiency. Code will be available.

</details>


### [285] [PatternPaint: Practical Layout Pattern Generation Using Diffusion-Based Inpainting](https://arxiv.org/abs/2409.01348)
*Guanglei Zhou, Bhargav Korrapati, Gaurav Rajavendra Reddy, Chen-Chia Chang, Jingyu Pan, Jiang Hu, Yiran Chen, Dipto G. Thakurta*

Main category: cs.CV

TL;DR: PatternPaint is a diffusion-based framework for generating diverse VLSI layout patterns with limited training data, outperforming existing methods in legality and diversity.


<details>
  <summary>Details</summary>
Motivation: Existing methods require large datasets, which are impractical for new technology nodes. PatternPaint addresses this by working with limited design-rule-compliant samples.

Method: PatternPaint uses a diffusion-based framework with template-based denoising and few-shot finetuning on a pretrained image model.

Result: The model generates legal patterns in complex 2D metal interconnect designs, achieving high diversity and a 1.87X legality rate improvement.

Conclusion: PatternPaint offers a scalable, production-ready solution for layout pattern generation in new technology nodes.

Abstract: Generating diverse VLSI layout patterns is essential for various downstream
tasks in design for manufacturing, as design rules continually evolve during
the development of new technology nodes. However, existing training-based
methods for layout pattern generation rely on large datasets. In practical
scenarios, especially when developing a new technology node, obtaining such
extensive layout data is challenging. Consequently, training models with large
datasets becomes impractical, limiting the scalability and adaptability of
prior approaches. To this end, we propose PatternPaint, a diffusion-based
framework capable of generating legal patterns with limited
design-rule-compliant training samples. PatternPaint simplifies complex layout
pattern generation into a series of inpainting processes with a template-based
denoising scheme. Furthermore, we perform few-shot finetuning on a pretrained
image foundation model with only 20 design-rule-compliant samples. Experimental
results show that using a sub-3nm technology node (Intel 18A), our model is the
only one that can generate legal patterns in complex 2D metal interconnect
design rule settings among all previous works and achieves a high diversity
score. Additionally, our few-shot finetuning can boost the legality rate with
1.87X improvement compared to the original pretrained model. As a result, we
demonstrate a production-ready approach for layout pattern generation in
developing new technology nodes.

</details>


### [286] [SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization](https://arxiv.org/abs/2409.17993)
*Junchen Yu, Si-Yuan Cao, Runmin Zhang, Chenghao Zhang, Zhu Yu, Shujie Chen, Bailin Yang, Hui-liang Shen*

Main category: cs.CV

TL;DR: SSHNet is an unsupervised cross-modal homography estimation framework that splits the problem into supervised sub-tasks, using specialized networks and a split optimization strategy for stable training. It outperforms previous methods significantly.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of unsupervised cross-modal homography estimation by reformulating it into supervised sub-problems for improved accuracy and stability.

Method: SSHNet divides the task into two supervised sub-problems: homography estimation and modality transfer, using specialized networks. It employs split optimization, feature space supervision, and distillation training.

Result: SSHNet-IHN outperforms unsupervised methods and reduces mean average corner errors by 47.4% and 85.8% compared to supervised approaches on the OPT-SAR dataset.

Conclusion: SSHNet provides a stable and accurate framework for cross-modal homography estimation, with potential for integration with various architectures.

Abstract: We propose a novel unsupervised cross-modal homography estimation learning
framework, named Split Supervised Homography estimation Network (SSHNet).
SSHNet reformulates the unsupervised cross-modal homography estimation into two
supervised sub-problems, each addressed by its specialized network: a
homography estimation network and a modality transfer network. To realize
stable training, we introduce an effective split optimization strategy to train
each network separately within its respective sub-problem. We also formulate an
extra homography feature space supervision to enhance feature consistency,
further boosting the estimation accuracy. Moreover, we employ a simple yet
effective distillation training technique to reduce model parameters and
improve cross-domain generalization ability while maintaining comparable
performance. The training stability of SSHNet enables its cooperation with
various homography estimation architectures. Experiments reveal that the SSHNet
using IHN as homography estimation network, namely SSHNet-IHN, outperforms
previous unsupervised approaches by a significant margin. Even compared to
supervised approaches MHN and LocalTrans, SSHNet-IHN achieves 47.4% and 85.8%
mean average corner errors (MACEs) reduction on the challenging OPT-SAR
dataset.

</details>


### [287] [Hidden in the Noise: Two-Stage Robust Watermarking for Images](https://arxiv.org/abs/2412.04653)
*Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen*

Main category: cs.CV

TL;DR: A distortion-free watermarking method for images using diffusion models and Fourier patterns, achieving robust detection against attacks.


<details>
  <summary>Details</summary>
Motivation: Address vulnerabilities in current watermarking methods that distort images and are prone to forgery and removal.

Method: Two-stage framework: (1) augment initial noise with Fourier patterns during generation, (2) retrieve and match noise groups for detection.

Result: State-of-the-art robustness to forgery and removal attacks.

Conclusion: The proposed method effectively embeds and detects watermarks without distorting images, improving security.

Abstract: As the quality of image generators continues to improve, deepfakes become a
topic of considerable societal debate. Image watermarking allows responsible
model owners to detect and label their AI-generated content, which can mitigate
the harm. Yet, current state-of-the-art methods in image watermarking remain
vulnerable to forgery and removal attacks. This vulnerability occurs in part
because watermarks distort the distribution of generated images,
unintentionally revealing information about the watermarking techniques.
  In this work, we first demonstrate a distortion-free watermarking method for
images, based on a diffusion model's initial noise. However, detecting the
watermark requires comparing the initial noise reconstructed for an image to
all previously used initial noises. To mitigate these issues, we propose a
two-stage watermarking framework for efficient detection. During generation, we
augment the initial noise with generated Fourier patterns to embed information
about the group of initial noises we used. For detection, we (i) retrieve the
relevant group of noises, and (ii) search within the given group for an initial
noise that might match our image. This watermarking approach achieves
state-of-the-art robustness to forgery and removal against a large battery of
attacks.

</details>


### [288] [LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](https://arxiv.org/abs/2409.18125)
*Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, Xihui Liu*

Main category: cs.CV

TL;DR: LLaVA-3D is a framework adapting LLaVA for 3D scene understanding by integrating 3D position embeddings into 2D models, achieving faster convergence and state-of-the-art performance without compromising 2D capabilities.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale 3D vision-language datasets and powerful 3D encoders hinders the development of LMMs with 3D scene understanding.

Method: LLaVA-3D enhances 2D CLIP patches with 3D spatial context using 3D position embeddings, constructs 3D patches, and employs joint 2D and 3D vision-language instruction tuning.

Result: LLaVA-3D converges 3.5x faster than existing 3D LMMs and achieves state-of-the-art performance in 3D tasks while maintaining 2D capabilities.

Conclusion: LLaVA-3D provides a unified architecture for 2D and 3D understanding, offering efficient and accurate 3D perception without relying on slow 3D segmentors.

Abstract: Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced
their proficiency in 2D visual understanding tasks, enabling them to
effectively process and understand images and videos. However, the development
of LMMs with 3D scene understanding capabilities has been hindered by the lack
of large-scale 3D vision-language datasets and powerful 3D encoders. In this
paper, we introduce a simple yet effective framework called LLaVA-3D.
Leveraging the strong 2D visual understanding priors from LLaVA, our LLaVA-3D
efficiently adapts LLaVA for 3D scene understanding without compromising 2D
understanding capabilities. To achieve this, we utilize the 3D position
embeddings to enhance the 2D CLIP Patches with 3D spatial context information
and construct 3D patches. By integrating the 3D position embeddings into 2D
LMMs and employing joint 2D and 3D vision-language instruction tuning, we
establish a unified architecture for both 2D visual understanding and 3D scene
understanding. In contrast to previous 3D LMMs, LLaVA-3D supports decoding
accurate 3D spatial perception outputs, e.g., 3D bounding boxes, directly from
these 3D patches, without relying on the time-consuming off-the-shelf 3D
segmentors. Experimental results show that LLaVA-3D converges 3.5x faster than
existing 3D LMMs when trained on 3D vision-language datasets. Moreover,
LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks
but also maintains comparable 2D visual understanding and vision-language
conversation capabilities with LLaVA.

</details>


### [289] [Towards Interpreting Visual Information Processing in Vision-Language Models](https://arxiv.org/abs/2410.07149)
*Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, Fazl Barez*

Main category: cs.CV

TL;DR: The paper analyzes how LLaVA, a vision-language model, processes visual tokens, focusing on object localization, representation evolution, and integration for predictions. Key findings include a 70% drop in object identification without object-specific tokens, increasing interpretability of visual tokens across layers, and alignment with textual tokens for prediction.


<details>
  <summary>Details</summary>
Motivation: To understand how visual tokens are processed and integrated in VLMs, particularly in LLaVA, to bridge the gap between language and vision models and improve interpretability.

Method: The study involves analyzing visual token localization, representation evolution across layers, and integration mechanisms, supported by ablation studies.

Result: Removing object-specific tokens reduces accuracy by 70%; visual tokens align with textual tokens across layers; object information is extracted at the last token position for prediction.

Conclusion: The findings enhance understanding of VLM processing, offering insights for more interpretable and controllable multimodal systems.

Abstract: Vision-Language Models (VLMs) are powerful tools for processing and
understanding text and images. We study the processing of visual tokens in the
language model component of LLaVA, a prominent VLM. Our approach focuses on
analyzing the localization of object information, the evolution of visual token
representations across layers, and the mechanism of integrating visual
information for predictions. Through ablation studies, we demonstrated that
object identification accuracy drops by over 70\% when object-specific tokens
are removed. We observed that visual token representations become increasingly
interpretable in the vocabulary space across layers, suggesting an alignment
with textual tokens corresponding to image content. Finally, we found that the
model extracts object information from these refined representations at the
last token position for prediction, mirroring the process in text-only language
models for factual association tasks. These findings provide crucial insights
into how VLMs process and integrate visual information, bridging the gap
between our understanding of language and vision models, and paving the way for
more interpretable and controllable multimodal systems.

</details>


### [290] [Progressive Compositionality in Text-to-Image Generative Models](https://arxiv.org/abs/2410.16719)
*Evans Xu Han, Linghao Jin, Xiaofeng Liu, Paul Pu Liang*

Main category: cs.CV

TL;DR: The paper introduces ConPair, a dataset of 15k contrastive image pairs for improving diffusion models' understanding of compositional relationships, and EvoGen, a multi-stage curriculum for contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with compositional relationships in complex settings, and existing solutions are limited.

Method: Leverages LLMs to create realistic scenarios, uses VQA systems to curate ConPair dataset, and proposes EvoGen for contrastive learning.

Result: The framework effectively improves compositional text-to-image synthesis, as shown in experiments.

Conclusion: The proposed approach enhances diffusion models' ability to handle complex compositional scenarios.

Abstract: Despite the impressive text-to-image (T2I) synthesis capabilities of
diffusion models, they often struggle to understand compositional relationships
between objects and attributes, especially in complex settings. Existing
solutions have tackled these challenges by optimizing the cross-attention
mechanism or learning from the caption pairs with minimal semantic changes.
However, can we generate high-quality complex contrastive images that diffusion
models can directly discriminate based on visual representations? In this work,
we leverage large-language models (LLMs) to compose realistic, complex
scenarios and harness Visual-Question Answering (VQA) systems alongside
diffusion models to automatically curate a contrastive dataset, ConPair,
consisting of 15k pairs of high-quality contrastive images. These pairs feature
minimal visual discrepancies and cover a wide range of attribute categories,
especially complex and natural scenarios. To learn effectively from these error
cases, i.e., hard negative images, we propose EvoGen, a new multi-stage
curriculum for contrastive learning of diffusion models. Through extensive
experiments across a wide range of compositional scenarios, we showcase the
effectiveness of our proposed framework on compositional T2I benchmarks.

</details>


### [291] [DivShift: Exploring Domain-Specific Distribution Shift in Large-Scale, Volunteer-Collected Biodiversity Datasets](https://arxiv.org/abs/2410.19816)
*Elena Sierra, Lauren E. Gillespie, Salim Soltani, Moises Exposito-Alonso, Teja Kattenborn*

Main category: cs.CV

TL;DR: The paper introduces DivShift, a framework to quantify bias effects in volunteer-collected biodiversity data on ML model performance, using a curated dataset (DivShift-NAWC) to analyze biases and their impacts.


<details>
  <summary>Details</summary>
Motivation: To understand how biases in citizen science data (e.g., geographic, temporal) affect fine-grained species recognition performance in ML models.

Method: Introduces DivShift framework and DivShift-NAWC dataset (7.5M iNaturalist images) to partition and analyze biases. Compares species recognition performance across bias partitions using diverse metrics.

Result: Biases confound model performance less than expected; more data improves performance, but improvements vary by bias type.

Conclusion: While natural world images aid generalization, biases in volunteer-collected data impact model performance, warranting caution in biodiversity monitoring.

Abstract: Large-scale, volunteer-collected datasets of community-identified natural
world imagery like iNaturalist have enabled marked performance gains for
fine-grained visual classification of species using machine learning methods.
However, such data -- sometimes referred to as citizen science data -- are
opportunistic and lack a structured sampling strategy. This volunteer-collected
biodiversity data contains geographic, temporal, taxonomic, observers, and
sociopolitical biases that can have significant effects on biodiversity model
performance, but whose impacts are unclear for fine-grained species recognition
performance. Here we introduce Diversity Shift (DivShift), a framework for
quantifying the effects of domain-specific distribution shifts on machine
learning model performance. To diagnose the performance effects of biases
specific to volunteer-collected biodiversity data, we also introduce DivShift -
North American West Coast (DivShift-NAWC), a curated dataset of almost 7.5
million iNaturalist images across the western coast of North America
partitioned across five types of expert-verified bias. We compare species
recognition performance across these bias partitions using a diverse variety of
species- and ecosystem-focused accuracy metrics. We observe that these biases
confound model performance less than expected from the underlying label
distribution shift, and that more data leads to better model performance but
the magnitude of these improvements are bias-specific. These findings imply
that while the structure within natural world images provides generalization
improvements for biodiversity monitoring tasks, the biases present in
volunteer-collected biodiversity data can also affect model performance; thus
these models should be used with caution in downstream biodiversity monitoring
tasks.

</details>


### [292] [Unsupervised Tomato Split Anomaly Detection using Hyperspectral Imaging and Variational Autoencoders](https://arxiv.org/abs/2501.02921)
*Mahmoud Abdulsalam, Usman Zahidi, Bradley Hurst, Simon Pearson, Grzegorz Cielniak, James Brown*

Main category: cs.CV

TL;DR: Unsupervised VAE model detects tomato split anomalies with 97% accuracy using hyperspectral data (530nm-550nm range).


<details>
  <summary>Details</summary>
Motivation: Tomato anomalies like splitting degrade quality; detecting them is hard due to appearance variations and scarce data.

Method: Tailored variational autoencoder (VAE) with hyperspectral input, focusing on 530nm-550nm wavelengths.

Result: 97% detection accuracy for tomato splits; reconstruction loss helps estimate anomalous regions.

Conclusion: The VAE model effectively detects and localizes tomato split anomalies in an unsupervised manner.

Abstract: Tomato anomalies/damages pose a significant challenge in greenhouse farming.
While this method of cultivation benefits from efficient resource utilization,
anomalies can significantly degrade the quality of farm produce. A common
anomaly associated with tomatoes is splitting, characterized by the development
of cracks on the tomato skin, which degrades its quality. Detecting this type
of anomaly is challenging due to dynamic variations in appearance and sizes,
compounded by dataset scarcity. We address this problem in an unsupervised
manner by utilizing a tailored variational autoencoder (VAE) with hyperspectral
input. Preliminary analysis of the dataset enabled us to select the optimal
range of wavelengths for detecting this anomaly. Our findings indicate that the
530nm - 550nm range is suitable for identifying tomato dry splits. The proposed
VAE model achieved a 97% detection accuracy for tomato split anomalies in the
test data. The analysis on reconstruction loss allow us to not only detect the
anomalies but also to some degree estimate the anomalous regions.

</details>


### [293] [FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training](https://arxiv.org/abs/2411.11927)
*Anjia Cao, Xing Wei, Zhiheng Ma*

Main category: cs.CV

TL;DR: FLAME uses frozen large language models as text encoders for efficient language-image pre-training, outperforming previous methods in accuracy and multilingual tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in language-image pre-training, such as limited data formats and text encoder constraints, FLAME aims to improve long-text processing and multilingual generalization.

Method: FLAME employs multifaceted prompt distillation and facet-decoupled attention with an offline embedding strategy for efficient computation.

Result: FLAME achieves 4.9% higher ImageNet top-1 accuracy on CC3M and outperforms CLIP by 44.4% in multilingual tasks on YFCC15M.

Conclusion: FLAME demonstrates superior performance in language-image pre-training, particularly for long-text and multilingual applications.

Abstract: Language-image pre-training faces significant challenges due to limited data
in specific formats and the constrained capacities of text encoders. While
prevailing methods attempt to address these issues through data augmentation
and architecture modifications, they continue to struggle with processing
long-form text inputs, and the inherent limitations of traditional CLIP text
encoders lead to suboptimal downstream generalization. In this paper, we
propose FLAME (Frozen Large lAnguage Models Enable data-efficient
language-image pre-training) that leverages frozen large language models as
text encoders, naturally processing long text inputs and demonstrating
impressive multilingual generalization. FLAME comprises two key components: 1)
a multifaceted prompt distillation technique for extracting diverse semantic
representations from long captions, which better aligns with the multifaceted
nature of images, and 2) a facet-decoupled attention mechanism, complemented by
an offline embedding strategy, to ensure efficient computation. Extensive
empirical evaluations demonstrate FLAME's superior performance. When trained on
CC3M, FLAME surpasses the previous state-of-the-art by 4.9% in ImageNet top-1
accuracy. On YFCC15M, FLAME surpasses the WIT-400M-trained CLIP by 44.4\% in
average image-to-text recall@1 across 36 languages, and by 34.6% in
text-to-image recall@1 for long-context retrieval on Urban-1k. Code is
available at https://github.com/MIV-XJTU/FLAME.

</details>


### [294] [Lifting Motion to the 3D World via 2D Diffusion](https://arxiv.org/abs/2411.18808)
*Jiaman Li, C. Karen Liu, Jiajun Wu*

Main category: cs.CV

TL;DR: MVLift predicts 3D motion from 2D poses without 3D supervision, outperforming methods that rely on 3D data.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of prior work requiring 3D ground truth, enabling generalization to out-of-distribution scenarios like athletic or animal motion.

Method: Multi-stage framework using 2D motion diffusion models to generate consistent 2D pose sequences across views for accurate 3D motion recovery.

Result: Outperforms prior work on five datasets, even those using 3D supervision, and generalizes across human, human-object, and animal poses.

Conclusion: MVLift advances 3D motion estimation by eliminating the need for 3D supervision while improving accuracy and generalization.

Abstract: Estimating 3D motion from 2D observations is a long-standing research
challenge. Prior work typically requires training on datasets containing ground
truth 3D motions, limiting their applicability to activities well-represented
in existing motion capture data. This dependency particularly hinders
generalization to out-of-distribution scenarios or subjects where collecting 3D
ground truth is challenging, such as complex athletic movements or animal
motion. We introduce MVLift, a novel approach to predict global 3D motion --
including both joint rotations and root trajectories in the world coordinate
system -- using only 2D pose sequences for training. Our multi-stage framework
leverages 2D motion diffusion models to progressively generate consistent 2D
pose sequences across multiple views, a key step in recovering accurate global
3D motion. MVLift generalizes across various domains, including human poses,
human-object interactions, and animal poses. Despite not requiring 3D
supervision, it outperforms prior work on five datasets, including those
methods that require 3D supervision.

</details>


### [295] [Perception of Visual Content: Differences Between Humans and Foundation Models](https://arxiv.org/abs/2411.18968)
*Nardiena A. Pratama, Shaoyang Fan, Gianluca Demartini*

Main category: cs.CV

TL;DR: The study compares human and ML-generated image annotations, finding similarities in low-level features but differences in performance and bias. ML excels in region classification and income regression, while human annotations are better for non-action categories.


<details>
  <summary>Details</summary>
Motivation: To explore the similarity between human and ML-generated annotations and their impact on ML model performance and bias, especially across diverse socio-economic contexts.

Method: Analyzed images from various regions and income levels, comparing human labels and ML captions for perception and bias.

Result: ML captions showed high similarity to human labels in low-level features but performed better in region classification and income regression. Human annotations were superior for non-action categories.

Conclusion: Both human and ML annotations are valuable, with human input still irreplaceable for certain tasks.

Abstract: Human-annotated content is often used to train machine learning (ML) models.
However, recently, language and multi-modal foundational models have been used
to replace and scale-up human annotator's efforts. This study explores the
similarity between human-generated and ML-generated annotations of images
across diverse socio-economic contexts (RQ1) and their impact on ML model
performance and bias (RQ2). We aim to understand differences in perception and
identify potential biases in content interpretation. Our dataset comprises
images of people from various geographical regions and income levels, covering
various daily activities and home environments. ML captions and human labels
show highest similarity at a low-level, i.e., types of words that appear and
sentence structures, but all annotations are consistent in how they perceive
images across regions. ML Captions resulted in best overall region
classification performance, while ML Objects and ML Captions performed best
overall for income regression. ML annotations worked best for action
categories, while human input was more effective for non-action categories.
These findings highlight the notion that both human and machine annotations are
important, and that human-generated annotations are yet to be replaceable.

</details>


### [296] [Wonderland: Navigating 3D Scenes from a Single Image](https://arxiv.org/abs/2412.12091)
*Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren*

Main category: cs.CV

TL;DR: A novel 3D scene reconstruction pipeline uses a video diffusion model's latents to predict 3D Gaussian Splattings, enabling efficient, high-quality, wide-scope 3D scene generation from single images.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D scene generation from single images face issues like multi-view data requirements, slow optimization, distorted geometry, and poor background quality. This work aims to overcome these limitations.

Method: The pipeline leverages a video diffusion model to generate multi-view consistent latents, which are used to train a 3D reconstruction model for predicting 3D Gaussian Splattings in a feed-forward manner.

Result: The model outperforms existing single-view 3D generation methods, especially with out-of-domain images, demonstrating high-quality and efficient scene generation.

Conclusion: This work shows that a 3D reconstruction model can effectively use a diffusion model's latent space for efficient and high-quality 3D scene generation.

Abstract: How can one efficiently generate high-quality, wide-scope 3D scenes from
arbitrary single images? Existing methods suffer several drawbacks, such as
requiring multi-view data, time-consuming per-scene optimization, distorted
geometry in occluded areas, and low visual quality in backgrounds. Our novel 3D
scene reconstruction pipeline overcomes these limitations to tackle the
aforesaid challenge. Specifically, we introduce a large-scale reconstruction
model that leverages latents from a video diffusion model to predict 3D
Gaussian Splattings of scenes in a feed-forward manner. The video diffusion
model is designed to create videos precisely following specified camera
trajectories, allowing it to generate compressed video latents that encode
multi-view information while maintaining 3D consistency. We train the 3D
reconstruction model to operate on the video latent space with a progressive
learning strategy, enabling the efficient generation of high-quality,
wide-scope, and generic 3D scenes. Extensive evaluations across various
datasets affirm that our model significantly outperforms existing single-view
3D scene generation methods, especially with out-of-domain images. Thus, we
demonstrate for the first time that a 3D reconstruction model can effectively
be built upon the latent space of a diffusion model in order to realize
efficient 3D scene generation.

</details>


### [297] [SimLTD: Simple Supervised and Semi-Supervised Long-Tailed Object Detection](https://arxiv.org/abs/2412.20047)
*Phi Vu Tran*

Main category: cs.CV

TL;DR: SimLTD is a simple, scalable framework for long-tailed object detection, leveraging unlabeled images to improve performance without complex methods like meta-learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on large labeled datasets, which are impractical. SimLTD aims to use unlabeled data for better scalability.

Method: Three-step approach: pre-training on head classes, transfer learning on tail classes, and fine-tuning on a mix of head and tail classes.

Result: Achieves state-of-the-art results on LVIS v1 benchmark in supervised and semi-supervised settings.

Conclusion: SimLTD offers a practical, effective solution for long-tailed detection without dependency on extensive labeled data.

Abstract: While modern visual recognition systems have made significant advancements,
many continue to struggle with the open problem of learning from few exemplars.
This paper focuses on the task of object detection in the setting where object
classes follow a natural long-tailed distribution. Existing methods for
long-tailed detection resort to external ImageNet labels to augment the
low-shot training instances. However, such dependency on a large labeled
database has limited utility in practical scenarios. We propose a versatile and
scalable approach to leverage optional unlabeled images, which are easy to
collect without the burden of human annotations. Our SimLTD framework is
straightforward and intuitive, and consists of three simple steps: (1)
pre-training on abundant head classes; (2) transfer learning on scarce tail
classes; and (3) fine-tuning on a sampled set of both head and tail classes.
Our approach can be viewed as an improved head-to-tail model transfer paradigm
without the added complexities of meta-learning or knowledge distillation, as
was required in past research. By harnessing supplementary unlabeled images,
without extra image labels, SimLTD establishes new record results on the
challenging LVIS v1 benchmark across both supervised and semi-supervised
settings.

</details>


### [298] [CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction](https://arxiv.org/abs/2501.01695)
*Chenhao Zhang, Yuanping Cao, Lei Zhang*

Main category: cs.CV

TL;DR: A novel cross-view Gaussian Splatting method improves large-scale scene reconstruction by using multi-branch construction, gradient-aware regularization, and Gaussian supplementation to handle view disparities.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods struggle with large view changes in cross-view data, limiting their effectiveness in large-scale scene reconstruction.

Method: Proposes multi-branch reconstruction, gradient-aware regularization, and Gaussian supplementation to optimize cross-view Gaussian Splatting.

Result: Achieves superior performance in novel view synthesis compared to state-of-the-art methods.

Conclusion: The method effectively addresses challenges in cross-view reconstruction, enhancing 3DGS for large-scale scenes.

Abstract: 3D Gaussian Splatting (3DGS) leverages densely distributed Gaussian
primitives for high-quality scene representation and reconstruction. While
existing 3DGS methods perform well in scenes with minor view variation, large
view changes from cross-view data pose optimization challenges for these
methods. To address these issues, we propose a novel cross-view Gaussian
Splatting method for large-scale scene reconstruction based on multi-branch
construction and fusion. Our method independently reconstructs models from
different sets of views as multiple independent branches to establish the
baselines of Gaussian distribution, providing reliable priors for cross-view
reconstruction during initialization and densification. Specifically, a
gradient-aware regularization strategy is introduced to mitigate smoothing
issues caused by significant view disparities. Additionally, a unique Gaussian
supplementation strategy is utilized to incorporate complementary information
of multi-branch into the cross-view model. Extensive experiments on benchmark
datasets demonstrate that our method achieves superior performance in novel
view synthesis compared to state-of-the-art methods.

</details>


### [299] [Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity](https://arxiv.org/abs/2502.01776)
*Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, Jianfei Chen, Ion Stoica, Kurt Keutzer, Song Han*

Main category: cs.CV

TL;DR: SVG improves video generation efficiency by leveraging sparsity in 3D Full Attention, achieving up to 2.33x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: High computational cost of Diffusion Transformers (DiTs) limits real-world video generation.

Method: SVG classifies attention heads into Spatial and Temporal Heads, uses online profiling, and optimizes hardware efficiency.

Result: 2.28x and 2.33x speedup on CogVideoX-v1.5 and HunyuanVideo, respectively.

Conclusion: SVG offers a training-free, efficient solution for video generation without compromising quality.

Abstract: Diffusion Transformers (DiTs) dominate video generation but their high
computational cost severely limits real-world applicability, usually requiring
tens of minutes to generate a few seconds of video even on high-performance
GPUs. This inefficiency primarily arises from the quadratic computational
complexity of 3D Full Attention with respect to the context length. In this
paper, we propose a training-free framework termed Sparse VideoGen (SVG) that
leverages the inherent sparsity in 3D Full Attention to boost inference
efficiency. We reveal that the attention heads can be dynamically classified
into two groups depending on distinct sparse patterns: (1) Spatial Head, where
only spatially-related tokens within each frame dominate the attention output,
and (2) Temporal Head, where only temporally-related tokens across different
frames dominate. Based on this insight, SVG proposes an online profiling
strategy to capture the dynamic sparse patterns and predicts the type of
attention head. Combined with a novel hardware-efficient tensor layout
transformation and customized kernel implementations, SVG achieves up to 2.28x
and 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively,
while preserving generation quality. Our code is open-sourced and is available
at https://github.com/svg-project/Sparse-VideoGen

</details>


### [300] [VerteNet -- A Multi-Context Hybrid CNN Transformer for Accurate Vertebral Landmark Localization in Lateral Spine DXA Images](https://arxiv.org/abs/2502.02097)
*Zaid Ilyas, Arooba Maqsood, Afsah Saleem, Erchuan Zhang, David Suter, Parminder Raina, Jonathan M. Hodgson, John T. Schousboe, William D. Leslie, Joshua R. Lewis, Syed Zulqarnain Gilani*

Main category: cs.CV

TL;DR: VerteNet, a hybrid CNN-Transformer model with dual-resolution attention mechanisms, improves Vertebral Landmark Localization (VLL) on DXA Lateral Spine Images (LSIs), aiding in spinal condition detection and AAC assessment.


<details>
  <summary>Details</summary>
Motivation: Accurate VLL on DXA LSIs is crucial for diagnosing spinal conditions and assessing AAC, but automated methods for DXA are lacking.

Method: VerteNet combines CNN and Transformer with Dual Resolution Self-Attention (DRSA) and Cross-Attention (DRCA), and a Multi-Context Feature Fusion Block (MCFB). Trained on 620 DXA LSIs.

Result: Superior performance over existing methods; includes an ROI algorithm for detecting abdominal aorta cropping and improves AAC scoring consistency.

Conclusion: VerteNet advances VLL on DXA LSIs, enhancing AAC assessment and inter-reader correlation, with potential for broader clinical use.

Abstract: Lateral Spine Image (LSI) analysis is important for medical diagnosis,
treatment planning, and detailed spinal health assessments. Although modalities
like Computed Tomography and Digital X-ray Imaging are commonly used, Dual
Energy X-ray Absorptiometry (DXA) is often preferred due to lower radiation
exposure, seamless capture, and cost-effectiveness. Accurate Vertebral Landmark
Localization (VLL) on LSIs is important to detect spinal conditions like
kyphosis and lordosis, as well as assessing Abdominal Aortic Calcification
(AAC) using Inter-Vertebral Guides (IVGs). Nonetheless, few automated VLL
methodologies have concentrated on DXA LSIs. We present VerteNet, a hybrid
CNN-Transformer model featuring a novel dual-resolution attention mechanism in
self and cross-attention domains, referred to as Dual Resolution Self-Attention
(DRSA) and Dual Resolution Cross-Attention (DRCA). These mechanisms capture the
diverse frequencies in DXA images by operating at two different feature map
resolutions. Additionally, we design a Multi-Context Feature Fusion Block
(MCFB) that efficiently integrates the features using DRSA and DRCA. We train
VerteNet on 620 DXA LSIs from various machines and achieve superior results
compared to existing methods. We also design an algorithm that utilizes
VerteNet's predictions in estimating the Region of Interest (ROI) to detect
potential abdominal aorta cropping, where inadequate soft tissue hinders
calcification assessment. Additionally, we present a small proof-of-concept
study to show that IVGs generated from VLL information can improve inter-reader
correlation in AAC scoring, addressing two key areas of disagreement in expert
AAC-24 scoring: IVG placement and quality control for full abdominal aorta
assessment. The code for this work can be found at
https://github.com/zaidilyas89/VerteNet.

</details>


### [301] [Brain Tumor Identification using Improved YOLOv8](https://arxiv.org/abs/2502.03746)
*Rupesh Dulal, Rabin Dulal*

Main category: cs.CV

TL;DR: A modified YOLOv8 model with RT-DETR, Ghost Convolution, and Vision Transformer blocks improves brain tumor detection in MRI scans, outperforming other models with 0.91 mAP.


<details>
  <summary>Details</summary>
Motivation: Manual tumor boundary detection in MRI scans is labor-intensive and requires expertise, prompting the need for automated, accurate solutions.

Method: The proposed model replaces NMS with RT-DETR, uses Ghost Convolution for efficiency, and integrates Vision Transformer blocks for context-aware feature extraction.

Result: The model achieved 0.91 mAP@0.5, outperforming original YOLOv8 and other detectors like Faster R-CNN and DETR.

Conclusion: The modified YOLOv8 model offers a robust, efficient solution for brain tumor detection, suitable for real-time applications.

Abstract: Identifying the extent of brain tumors is a significant challenge in brain
cancer treatment. The main difficulty is in the approximate detection of tumor
size. Magnetic resonance imaging (MRI) has become a critical diagnostic tool.
However, manually detecting the boundaries of brain tumors from MRI scans is a
labor-intensive task that requires extensive expertise. Deep learning and
computer-aided detection techniques have led to notable advances in machine
learning for this purpose. In this paper, we propose a modified You Only Look
Once (YOLOv8) model to accurately detect the tumors within the MRI images. The
proposed model replaced the Non-Maximum Suppression (NMS) algorithm with a
Real-Time Detection Transformer (RT- DETR) in the detection head. NMS filters
out redundant or overlapping bounding boxes in the detected tumors, but they
are hand-designed and pre-set. RT-DETR removes hand-designed components. The
second improvement was made by replacing the normal convolution block with
ghost convolution. Ghost Convolution reduces computational and memory costs
while maintaining high accuracy and enabling faster inference, making it ideal
for resource-constrained environments and real-time applications. The third
improvement was made by introducing a vision transformer block in the backbone
of YOLOv8 to extract context-aware features. We used a publicly available
dataset of brain tumors in the proposed model. The proposed model performed
better than the original YOLOv8 model and also performed better than other
object detectors (Faster R- CNN, Mask R-CNN, YOLO, YOLOv3, YOLOv4, YOLOv5, SSD,
RetinaNet, EfficientDet, and DETR). The proposed model achieved 0.91 mAP (mean
Average Precision)@0.5.

</details>


### [302] [VideoRoPE: What Makes for Good Video Rotary Position Embedding?](https://arxiv.org/abs/2502.05173)
*Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, Dahua Lin*

Main category: cs.CV

TL;DR: VideoRoPE extends Rotary Position Embedding (RoPE) to video by addressing spatio-temporal challenges, outperforming prior variants in tasks like video retrieval and understanding.


<details>
  <summary>Details</summary>
Motivation: Prior RoPE variants struggle with video's spatio-temporal complexity, especially when faced with periodic distractors, necessitating a more robust solution.

Method: VideoRoPE introduces a 3D structure with low-frequency temporal allocation, diagonal layout, and adjustable temporal spacing to handle spatio-temporal relationships effectively.

Result: VideoRoPE consistently outperforms previous RoPE variants in tasks like long video retrieval, video understanding, and video hallucination.

Conclusion: VideoRoPE successfully adapts RoPE for video, addressing key spatio-temporal challenges and demonstrating superior performance across diverse tasks.

Abstract: While Rotary Position Embedding (RoPE) and its variants are widely adopted
for their long-context capabilities, the extension of the 1D RoPE to video,
with its complex spatio-temporal structure, remains an open challenge. This
work first introduces a comprehensive analysis that identifies four key
characteristics essential for the effective adaptation of RoPE to video, which
have not been fully considered in prior work. As part of our analysis, we
introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors)
task, which adds periodic distractors into V-NIAH. The V-NIAH-D task
demonstrates that previous RoPE variants, lacking appropriate temporal
dimension allocation, are easily misled by distractors. Based on our analysis,
we introduce \textbf{VideoRoPE}, with a \textit{3D structure} designed to
preserve spatio-temporal relationships. VideoRoPE features
\textit{low-frequency temporal allocation} to mitigate periodic oscillations, a
\textit{diagonal layout} to maintain spatial symmetry, and \textit{adjustable
temporal spacing} to decouple temporal and spatial indexing. VideoRoPE
consistently surpasses previous RoPE variants, across diverse downstream tasks
such as long video retrieval, video understanding, and video hallucination. Our
code will be available at
\href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}.

</details>


### [303] [From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI Synthesis](https://arxiv.org/abs/2502.08025)
*Kristofer Grover Roos, Atsushi Fukuda, Quan Huu Cap*

Main category: cs.CV

TL;DR: E2fNet is a deep learning model that synthesizes fMRI images from EEG data, outperforming existing methods and offering a cost-effective neuroimaging solution.


<details>
  <summary>Details</summary>
Motivation: fMRI is costly and infrastructure-heavy, while EEG lacks spatial fidelity. E2fNet bridges these gaps by generating fMRI from EEG.

Method: E2fNet is an encoder-decoder network that translates multi-scale EEG features into fMRI representations.

Result: E2fNet achieves state-of-the-art SSIM scores on three public datasets, surpassing CNN- and transformer-based methods.

Conclusion: E2fNet is a promising, cost-effective tool for enhancing neuroimaging capabilities.

Abstract: While functional magnetic resonance imaging (fMRI) offers valuable insights
into brain activity, it is limited by high operational costs and significant
infrastructural demands. In contrast, electroencephalography (EEG) provides
millisecond-level precision in capturing electrical activity but lacks the
spatial fidelity necessary for precise neural localization. To bridge these
gaps, we propose E2fNet, a simple yet effective deep learning model for
synthesizing fMRI images from low-cost EEG data. E2fNet is an encoder-decoder
network specifically designed to capture and translate meaningful multi-scale
features from EEG across electrode channels into accurate fMRI representations.
Extensive evaluations across three public datasets demonstrate that E2fNet
consistently outperforms existing CNN- and transformer-based methods, achieving
state-of-the-art results in terms of the structural similarity index measure
(SSIM). These results demonstrate that E2fNet is a promising, cost-effective
solution for enhancing neuroimaging capabilities. The code is available at
https://github.com/kgr20/E2fNet.

</details>


### [304] [ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences](https://arxiv.org/abs/2502.10377)
*Liyuan Zhu, Shengqu Cai, Shengyu Huang, Gordon Wetzstein, Naji Khosravan, Iro Armeni*

Main category: cs.CV

TL;DR: ReStyle3D is a framework for transferring scene-level appearance from a single style image to a multi-view real-world scene, ensuring precise and coherent stylization using semantic correspondences and multi-view consistency.


<details>
  <summary>Details</summary>
Motivation: To achieve semantically faithful and multi-view coherent stylization, addressing limitations of conventional methods that apply styles globally without semantic matching.

Method: Combines open-vocabulary segmentation for dense instance-level correspondences, a training-free semantic-attention mechanism in a diffusion model for single-view stylization, and a warp-and-refine network for multi-view consistency.

Result: Outperforms prior methods in structure preservation, perceptual style similarity, and multi-view coherence, validated by user studies.

Conclusion: ReStyle3D enables photo-realistic, semantically accurate stylization, with potential applications in interior design, virtual staging, and 3D-consistent stylization.

Abstract: We introduce ReStyle3D, a novel framework for scene-level appearance transfer
from a single style image to a real-world scene represented by multiple views.
The method combines explicit semantic correspondences with multi-view
consistency to achieve precise and coherent stylization. Unlike conventional
stylization methods that apply a reference style globally, ReStyle3D uses
open-vocabulary segmentation to establish dense, instance-level correspondences
between the style and real-world images. This ensures that each object is
stylized with semantically matched textures. It first transfers the style to a
single view using a training-free semantic-attention mechanism in a diffusion
model. It then lifts the stylization to additional views via a learned
warp-and-refine network guided by monocular depth and pixel-wise
correspondences. Experiments show that ReStyle3D consistently outperforms prior
methods in structure preservation, perceptual style similarity, and multi-view
coherence. User studies further validate its ability to produce
photo-realistic, semantically faithful results. Our code, pretrained models,
and dataset will be publicly released, to support new applications in interior
design, virtual staging, and 3D-consistent stylization.

</details>


### [305] [3D Gaussian Inpainting with Depth-Guided Cross-View Consistency](https://arxiv.org/abs/2502.11801)
*Sheng-Yu Huang, Zi-Ting Chou, Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: Proposes 3DGIC, a depth-guided framework for cross-view consistent 3D inpainting, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing texture and geometry consistency challenges in 3D inpainting using NeRF or 3DGS.

Method: Uses depth-guided cross-view consistency to refine inpainting masks and update 3DGS.

Result: Outperforms state-of-the-art methods on benchmark datasets.

Conclusion: 3DGIC effectively achieves cross-view consistency in 3D inpainting.

Abstract: When performing 3D inpainting using novel-view rendering methods like Neural
Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture
and geometry consistency across camera views has been a challenge. In this
paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided
Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided
by the rendered depth information from each training view, our 3DGIC exploits
background pixels visible across different views for updating the inpainting
mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive
experiments on benchmark datasets, we confirm that our 3DGIC outperforms
current state-of-the-art 3D inpainting methods quantitatively and
qualitatively.

</details>


### [306] [OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels](https://arxiv.org/abs/2502.20087)
*Meng Lou, Yizhou Yu*

Main category: cs.CV

TL;DR: OverLoCK introduces a ConvNet backbone with top-down attention, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Modern ConvNets lack biomimetic top-down attention, which is crucial for human vision. OverLoCK addresses this gap.

Method: OverLoCK uses a branched architecture with Base-Net, Overview-Net, and Focus-Net, plus ContMix for long-range dependencies.

Result: OverLoCK-T achieves 84.2% Top-1 accuracy, surpassing ConvNeXt-B with fewer FLOPs/parameters. It also excels in detection and segmentation.

Conclusion: OverLoCK successfully integrates top-down attention, offering a powerful and efficient alternative to traditional ConvNets.

Abstract: Top-down attention plays a crucial role in the human vision system, wherein
the brain initially obtains a rough overview of a scene to discover salient
cues (i.e., overview first), followed by a more careful finer-grained
examination (i.e., look closely next). However, modern ConvNets remain confined
to a pyramid structure that successively downsamples the feature map for
receptive field expansion, neglecting this crucial biomimetic principle. We
present OverLoCK, the first pure ConvNet backbone architecture that explicitly
incorporates a top-down attention mechanism. Unlike pyramid backbone networks,
our design features a branched architecture with three synergistic
sub-networks: 1) a Base-Net that encodes low/mid-level features; 2) a
lightweight Overview-Net that generates dynamic top-down attention through
coarse global context modeling (i.e., overview first); and 3) a robust
Focus-Net that performs finer-grained perception guided by top-down attention
(i.e., look closely next). To fully unleash the power of top-down attention, we
further propose a novel context-mixing dynamic convolution (ContMix) that
effectively models long-range dependencies while preserving inherent local
inductive biases even when the input resolution increases, addressing critical
limitations in existing convolutions. Our OverLoCK exhibits a notable
performance improvement over existing methods. For instance, OverLoCK-T
achieves a Top-1 accuracy of 84.2%, significantly surpassing ConvNeXt-B while
using only around one-third of the FLOPs/parameters. On object detection, our
OverLoCK-S clearly surpasses MogaNet-B by 1% in AP^b. On semantic segmentation,
our OverLoCK-T remarkably improves UniRepLKNet-T by 1.7% in mIoU. Code is
publicly available at https://github.com/LMMMEng/OverLoCK.

</details>


### [307] [HiMo: High-Speed Objects Motion Compensation in Point Clouds](https://arxiv.org/abs/2503.00803)
*Qingwen Zhang, Ajinkya Khoche, Yi Yang, Li Ling, Sina Sharif Mansouri, Olov Andersson, Patric Jensfelt*

Main category: cs.CV

TL;DR: HiMo addresses motion distortions in LiDAR point clouds caused by dynamic objects, improving data quality for autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: Motion distortions from dynamic objects degrade LiDAR data quality, especially in high-speed or multi-LiDAR setups, impacting object shape and position accuracy.

Method: HiMo repurposes scene flow estimation for non-ego motion compensation, and introduces SeFlow++, a real-time scene flow estimator. Two new metrics evaluate motion distortion.

Result: HiMo enhances geometric consistency and visual fidelity of dynamic objects, validated on Argoverse 2, ZOD, and a new dataset.

Conclusion: HiMo effectively corrects distortions, benefiting downstream tasks like semantic segmentation and 3D detection.

Abstract: LiDAR point cloud is essential for autonomous vehicles, but motion
distortions from dynamic objects degrade the data quality. While previous work
has considered distortions caused by ego motion, distortions caused by other
moving objects remain largely overlooked, leading to errors in object shape and
position. This distortion is particularly pronounced in high-speed environments
such as highways and in multi-LiDAR configurations, a common setup for heavy
vehicles. To address this challenge, we introduce HiMo, a pipeline that
repurposes scene flow estimation for non-ego motion compensation, correcting
the representation of dynamic objects in point clouds. During the development
of HiMo, we observed that existing self-supervised scene flow estimators often
produce degenerate or inconsistent estimates under high-speed distortion. We
further propose SeFlow++, a real-time scene flow estimator that achieves
state-of-the-art performance on both scene flow and motion compensation. Since
well-established motion distortion metrics are absent in the literature, we
introduce two evaluation metrics: compensation accuracy at a point level and
shape similarity of objects. We validate HiMo through extensive experiments on
Argoverse 2, ZOD, and a newly collected real-world dataset featuring highway
driving and multi-LiDAR-equipped heavy vehicles. Our findings show that HiMo
improves the geometric consistency and visual fidelity of dynamic objects in
LiDAR point clouds, benefiting downstream tasks such as semantic segmentation
and 3D detection. See https://kin-zhang.github.io/HiMo for more details.

</details>


### [308] [MRI super-resolution reconstruction using efficient diffusion probabilistic model with residual shifting](https://arxiv.org/abs/2503.01576)
*Mojtaba Safari, Shansong Wang, Zach Eidex, Qiang Li, Erik H. Middlebrooks, David S. Yu, Xiaofeng Yang*

Main category: cs.CV

TL;DR: Res-SRDiff, a diffusion-based SR framework with residual error-shifting, accelerates MRI reconstruction by reducing sampling steps while preserving anatomical details, outperforming other methods in quality and speed.


<details>
  <summary>Details</summary>
Motivation: To improve MRI reconstruction efficiency and accuracy by reducing computational time without compromising image quality.

Method: Proposes Res-SRDiff, integrating residual error-shifting into diffusion processes for high-fidelity image reconstruction with fewer sampling steps.

Result: Res-SRDiff outperforms Bicubic, Pix2pix, CycleGAN, and TM-DDPM in PSNR, SSIM, and GMSD, achieving fast reconstruction (under 1 second per slice).

Conclusion: Res-SRDiff is an efficient and accurate MRI SR method, enhancing clinical workflows and medical imaging research.

Abstract: Objective:This study introduces a residual error-shifting mechanism that
drastically reduces sampling steps while preserving critical anatomical
details, thus accelerating MRI reconstruction. Approach:We propose a novel
diffusion-based SR framework called Res-SRDiff, which integrates residual error
shifting into the forward diffusion process. This enables efficient HR image
reconstruction by aligning the degraded HR and LR distributions.We evaluated
Res-SRDiff on ultra-high-field brain T1 MP2RAGE maps and T2-weighted prostate
images, comparing it with Bicubic, Pix2pix, CycleGAN, and a conventional
denoising diffusion probabilistic model with vision transformer backbone
(TM-DDPM), using quantitative metrics such as peak signal-to-noise ratio
(PSNR), structural similarity index (SSIM), gradient magnitude similarity
deviation (GMSD), and learned perceptual image patch similarity (LPIPS). Main
results: Res-SRDiff significantly outperformed all comparative methods in terms
of PSNR, SSIM, and GMSD across both datasets, with statistically significant
improvements (p-values<<0.05). The model achieved high-fidelity image
restoration with only four sampling steps, drastically reducing computational
time to under one second per slice, which is substantially faster than
conventional TM-DDPM with around 20 seconds per slice. Qualitative analyses
further demonstrated that Res-SRDiff effectively preserved fine anatomical
details and lesion morphology in both brain and pelvic MRI images.
Significance: Our findings show that Res-SRDiff is an efficient and accurate
MRI SR method, markedly improving computational efficiency and image quality.
Integrating residual error shifting into the diffusion process allows for rapid
and robust HR image reconstruction, enhancing clinical MRI workflows and
advancing medical imaging research. The source
at:https://github.com/mosaf/Res-SRDiff

</details>


### [309] [WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation](https://arxiv.org/abs/2503.02247)
*Dujun Nie, Xianda Guo, Yiqun Duan, Ruijun Zhang, Long Chen*

Main category: cs.CV

TL;DR: WMNav introduces a world model-based navigation framework using VLMs to predict outcomes and reduce risky interactions, improving success rates and efficiency in object goal navigation.


<details>
  <summary>Details</summary>
Motivation: Current VLM-based agents lack a modular world model to predict future states, leading to costly interactions. WMNav aims to address this gap.

Method: WMNav uses a Curiosity Value Map for memory, a two-stage action proposer (broad exploration then precise localization), and feedback-based decision-making to reduce model hallucination.

Result: WMNav outperforms zero-shot benchmarks with absolute improvements of +3.2% SR and +3.2% SPL on HM3D, and +13.5% SR and +1.1% SPL on MP3D.

Conclusion: WMNav's modular design and feedback-driven approach enhance navigation efficiency and success, setting a new benchmark for object goal navigation.

Abstract: Object Goal Navigation-requiring an agent to locate a specific object in an
unseen environment-remains a core challenge in embodied AI. Although recent
progress in Vision-Language Model (VLM)-based agents has demonstrated promising
perception and decision-making abilities through prompting, none has yet
established a fully modular world model design that reduces risky and costly
interactions with the environment by predicting the future state of the world.
We introduce WMNav, a novel World Model-based Navigation framework powered by
Vision-Language Models (VLMs). It predicts possible outcomes of decisions and
builds memories to provide feedback to the policy module. To retain the
predicted state of the environment, WMNav proposes the online maintained
Curiosity Value Map as part of the world model memory to provide dynamic
configuration for navigation policy. By decomposing according to a human-like
thinking process, WMNav effectively alleviates the impact of model
hallucination by making decisions based on the feedback difference between the
world model plan and observation. To further boost efficiency, we implement a
two-stage action proposer strategy: broad exploration followed by precise
localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses
existing zero-shot benchmarks in both success rate and exploration efficiency
(absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL
on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.

</details>


### [310] [Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization](https://arxiv.org/abs/2503.13915)
*Dongkwan Lee, Kyomin Hwang, Nojun Kwak*

Main category: cs.CV

TL;DR: The paper proposes UPCSC, a method to utilize unconfident-unlabeled samples in semi-supervised domain generalization (SSDG), improving performance by enhancing class-level discriminability and reducing domain gaps.


<details>
  <summary>Details</summary>
Motivation: Existing SSDG methods underutilize unlabeled data by focusing only on confident samples, leaving unconfident samples unexplored.

Method: UPCSC introduces two modules: Unlabeled Proxy-based Contrastive learning (UPC) for negative pairs and Surrogate Class learning (SC) for positive pairs, both plug-and-play and domain-label-free.

Result: Experiments on four benchmarks show UPCSC consistently improves baseline performance and outperforms competitors.

Conclusion: UPCSC effectively leverages unconfident-unlabeled samples, enhancing discriminability and mitigating domain gaps in SSDG.

Abstract: We address the problem of semi-supervised domain generalization (SSDG), where
the distributions of train and test data differ, and only a small amount of
labeled data along with a larger amount of unlabeled data are available during
training. Existing SSDG methods that leverage only the unlabeled samples for
which the model's predictions are highly confident (confident-unlabeled
samples), limit the full utilization of the available unlabeled data. To the
best of our knowledge, we are the first to explore a method for incorporating
the unconfident-unlabeled samples that were previously disregarded in SSDG
setting. To this end, we propose UPCSC to utilize these unconfident-unlabeled
samples in SSDG that consists of two modules: 1) Unlabeled Proxy-based
Contrastive learning (UPC) module, treating unconfident-unlabeled samples as
additional negative pairs and 2) Surrogate Class learning (SC) module,
generating positive pairs for unconfident-unlabeled samples using their
confusing class set. These modules are plug-and-play and do not require any
domain labels, which can be easily integrated into existing approaches.
Experiments on four widely used SSDG benchmarks demonstrate that our approach
consistently improves performance when attached to baselines and outperforms
competing plug-and-play methods. We also analyze the role of our method in
SSDG, showing that it enhances class-level discriminability and mitigates
domain gaps. The code is available at https://github.com/dongkwani/UPCSC.

</details>


### [311] [ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical Flow Estimation](https://arxiv.org/abs/2503.10195)
*Hongze Sun, Jun Wang, Wuque Cai, Duo Chen, Qianqian Liao, Jiayi He, Yan Cui, Dezhong Yao, Daqing Guo*

Main category: cs.CV

TL;DR: The paper introduces ST-FlowNet, a novel SNN architecture for event-based optical flow estimation, leveraging ConvGRU modules and a BISNN method for improved performance and training efficiency.


<details>
  <summary>Details</summary>
Motivation: SNNs show promise for optical flow estimation but face performance constraints. The work aims to enhance SNN applicability in real-world scenarios.

Method: Proposes ST-FlowNet with ConvGRU modules for feature augmentation and temporal alignment. Introduces BISNN for simplified SNN training from ANNs.

Result: ST-FlowNet outperforms state-of-the-art methods on benchmark datasets, offering accurate optical flow estimation and energy efficiency.

Conclusion: The work advances neuromorphic vision by presenting a robust SNN framework for optical flow estimation with practical deployment advantages.

Abstract: Spiking Neural Networks (SNNs) have emerged as a promising tool for
event-based optical flow estimation tasks due to their ability to leverage
spatio-temporal information and low-power capabilities. However, the
performance of SNN models is often constrained, limiting their application in
real-world scenarios. In this work, we address this gap by proposing a novel
neural network architecture, ST-FlowNet, specifically tailored for optical flow
estimation from event-based data. The ST-FlowNet architecture integrates
ConvGRU modules to facilitate cross-modal feature augmentation and temporal
alignment of the predicted optical flow, improving the network's ability to
capture complex motion dynamics. Additionally, to overcome the challenges
associated with training SNNs, we introduce a novel approach to derive SNN
models from pre-trained artificial neural networks (ANNs) through ANN-to-SNN
conversion or our proposed BISNN method. Notably, the BISNN method alleviates
the complexities involved in biological parameter selection, further enhancing
the robustness of SNNs in optical flow estimation tasks. Extensive evaluations
on three benchmark event-based datasets demonstrate that the SNN-based
ST-FlowNet model outperforms state-of-the-art methods, delivering superior
performance in accurate optical flow estimation across a diverse range of
dynamic visual scenes. Furthermore, the inherent energy efficiency of SNN
models is highlighted, establishing a compelling advantage for their practical
deployment. Overall, our work presents a novel framework for optical flow
estimation using SNNs and event-based data, contributing to the advancement of
neuromorphic vision applications.

</details>


### [312] [Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis](https://arxiv.org/abs/2503.12150)
*Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, Jianfei Cai*

Main category: cs.CV

TL;DR: Proposes Point-Cache, a hierarchical cache model for open-vocabulary point cloud recognition, handling distribution shifts and novel classes at test time without training.


<details>
  <summary>Details</summary>
Motivation: Addresses the limitation of prior methods that rely on fixed training data and predefined classes, focusing on adapting models dynamically to unseen classes during inference.

Method: Develops Point-Cache, a plug-and-play module capturing global and local details of point clouds, dynamically managed for high-quality samples.

Result: Achieves significant improvements across 8 benchmarks and 4 large 3D models, operating efficiently without training.

Conclusion: Point-Cache is an effective, training-free solution for open-vocabulary point cloud recognition, adaptable to various models.

Abstract: This paper proposes a general solution to enable point cloud recognition
models to handle distribution shifts at test time. Unlike prior methods, which
rely heavily on training data (often inaccessible during online inference) and
are limited to recognizing a fixed set of point cloud classes predefined during
training, we explore a more practical and challenging scenario: adapting the
model solely based on online test data to recognize both previously seen
classes and novel, unseen classes at test time. To this end, we develop
\textbf{Point-Cache}, a hierarchical cache model that captures essential clues
of online test samples, particularly focusing on the global structure of point
clouds and their local-part details. Point-Cache, which serves as a rich 3D
knowledge base, is dynamically managed to prioritize the inclusion of
high-quality samples. Designed as a plug-and-play module, our method can be
flexibly integrated into large multimodal 3D models to support open-vocabulary
point cloud recognition. Notably, our solution operates with efficiency
comparable to zero-shot inference, as it is entirely training-free. Point-Cache
demonstrates substantial gains across 8 challenging benchmarks and 4
representative large 3D models, highlighting its effectiveness. Code is
available at https://github.com/auniquesun/Point-Cache.

</details>


### [313] [Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models](https://arxiv.org/abs/2503.17794)
*Ketan Suhaas Saichandran, Xavier Thomas, Prakhar Kaushik, Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: SCoPE improves text-to-image alignment by refining prompts from coarse to fine details, enhancing VQA scores by 4% over baselines.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with complex, detailed prompts, leading to misalignment.

Method: Decomposes prompts into sub-prompts (broad to intricate), interpolates during inference for progressive detail.

Result: +4% VQA score improvement on 85% of GenAI-Bench prompts.

Conclusion: SCoPE is a training-free, plug-and-play solution for better prompt alignment.

Abstract: Text-to-image generative models often struggle with long prompts detailing
complex scenes, diverse objects with distinct visual characteristics and
spatial relationships. In this work, we propose SCoPE (Scheduled interpolation
of Coarse-to-fine Prompt Embeddings), a training-free method to improve
text-to-image alignment by progressively refining the input prompt in a
coarse-to-fine-grained manner. Given a detailed input prompt, we first
decompose it into multiple sub-prompts which evolve from describing broad scene
layout to highly intricate details. During inference, we interpolate between
these sub-prompts and thus progressively introduce finer-grained details into
the generated image. Our training-free plug-and-play approach significantly
enhances prompt alignment, achieves an average improvement of up to +4% in
Visual Question Answering (VQA) scores over the Stable Diffusion baselines on
85% of the prompts from the GenAI-Bench dataset.

</details>


### [314] [AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View Prediction](https://arxiv.org/abs/2503.12929)
*Xuying Zhang, Yupeng Zhou, Kai Wang, Yikai Wang, Zhen Li, Shaohui Jiao, Daquan Zhou, Qibin Hou, Ming-Ming Cheng*

Main category: cs.CV

TL;DR: AR-1-to-3 improves novel view synthesis by prioritizing closer views and using diffusion models for progressive synthesis, enhancing consistency and fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with consistency in novel view synthesis, especially with large camera pose differences, leading to poor 3D quality.

Method: Proposes AR-1-to-3, a diffusion model-based approach that prioritizes closer views and uses Stacked-LE and LSTM-GE for feature encoding.

Result: Significantly improves consistency and fidelity in generated views, producing high-quality 3D assets.

Conclusion: AR-1-to-3 effectively addresses consistency issues in novel view synthesis, outperforming existing methods.

Abstract: Novel view synthesis (NVS) is a cornerstone for image-to-3d creation.
However, existing works still struggle to maintain consistency between the
generated views and the input views, especially when there is a significant
camera pose difference, leading to poor-quality 3D geometries and textures. We
attribute this issue to their treatment of all target views with equal priority
according to our empirical observation that the target views closer to the
input views exhibit higher fidelity. With this inspiration, we propose
AR-1-to-3, a novel next-view prediction paradigm based on diffusion models that
first generates views close to the input views, which are then utilized as
contextual information to progressively synthesize farther views. To encode the
generated view subsequences as local and global conditions for the next-view
prediction, we accordingly develop a stacked local feature encoding strategy
(Stacked-LE) and an LSTM-based global feature encoding strategy (LSTM-GE).
Extensive experiments demonstrate that our method significantly improves the
consistency between the generated views and the input views, producing
high-fidelity 3D assets.

</details>


### [315] [8-Calves Image dataset](https://arxiv.org/abs/2503.13777)
*Xuyang Fang, Sion Hannuna, Neill Campbell*

Main category: cs.CV

TL;DR: The 8-Calves dataset is a benchmark for object detection and identity preservation in occlusion-rich, temporally consistent environments, featuring a 1-hour video and 900 static frames. It evaluates 28 object detectors and 23 backbones, revealing trade-offs in model performance.


<details>
  <summary>Details</summary>
Motivation: To address real-world challenges like occlusions, motion blur, and pose variation in object detection, providing a reproducible testbed bridging synthetic and domain-specific complexity.

Method: Fine-tuning 28 object detectors (YOLO variants, transformers) and evaluating 23 pretrained backbones (ResNet, ConvNextV2, ViTs) on the 8-Calves dataset.

Result: Smaller models (e.g., ConvNextV2 Nano) excel in efficiency and retrieval accuracy, while pure vision transformers struggle in occlusion-heavy settings (mAP50:95: 56.5-66.4%).

Conclusion: The dataset offers a structured, reproducible benchmark for object detection, though limitations like partial labeling and detector bias exist.

Abstract: We introduce the 8-Calves dataset, a benchmark for evaluating object
detection and identity preservation in occlusion-rich, temporally consistent
environments. Comprising a 1-hour video (67,760 frames) of eight Holstein
Friesian calves with unique coat patterns and 900 static frames, the dataset
emphasizes real-world challenges like prolonged occlusions, motion blur, and
pose variation. By fine-tuning 28 object detectors (YOLO variants,
transformers) and evaluating 23 pretrained backbones (ResNet, ConvNextV2,
ViTs), we expose critical architectural trade-offs: smaller models (e.g.,
ConvNextV2 Nano, 15.6M parameters) excel in efficiency and retrieval accuracy,
while pure vision transformers lag in occlusion-heavy settings. The dataset's
structured design-fixed camera views, natural motion, and verified
identities-provides a reproducible testbed for object detection challenges
(mAP50:95: 56.5-66.4%), bridging synthetic simplicity and domain-specific
complexity. The dataset and benchmark code are all publicly available at
https://huggingface.co/datasets/tonyFang04/8-calves. Limitations include
partial labeling and detector bias, addressed in later sections.

</details>


### [316] [GranQ: Granular Zero-Shot Quantization with Unified Layer-Channel Awareness](https://arxiv.org/abs/2503.18339)
*Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park*

Main category: cs.CV

TL;DR: GranQ is a novel zero-shot quantization (ZSQ) method that dynamically adjusts quantization granularity using layer-channel awareness, minimizing activation loss and outperforming existing ZSQ methods.


<details>
  <summary>Details</summary>
Motivation: Existing ZSQ methods struggle with significant activation loss in low-bit environments due to coarse-grained scaling. GranQ aims to address this by fine-tuning quantization granularity.

Method: GranQ dynamically adjusts quantization granularity by analyzing layer- and channel-level activation distributions, and introduces vectorized activation quantization for efficiency.

Result: GranQ outperforms state-of-the-art ZSQ methods, even those using quantization-aware training, by reducing activation distortion and computational overhead.

Conclusion: GranQ sets a new direction for ZSQ research, moving beyond traditional data generation and model training approaches.

Abstract: Zero-shot quantization (ZSQ) enables neural network compression without
training data, which is crucial in restricted data access environments.
However, existing ZSQ methods suffer from significant activation loss in
low-bit environments owing to their coarse-grained scaling strategy. To address
this issue, we propose GranQ, a novel ZSQ approach that leverages layer-channel
awareness to minimize the quantization error. Unlike conventional layer- or
channel-wise quantization, GranQ dynamically adjusts quantization granularity
by considering both layer- and channel-level activation distributions. This
enables fine-grained quantization while minimizing activation distortion.
Additionally, we introduce vectorized activation quantization, which enables
efficient parallel computation and reduces computational overhead while
preserving accuracy. GranQ achieves superior performance compared with those of
state-of-the-art ZSQ methods that employ quantization-aware training. With
these findings, we anticipate that GranQ will inspire novel research directions
beyond conventional ZSQ approaches focused on data generation and model
training.

</details>


### [317] [Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding](https://arxiv.org/abs/2503.18478)
*Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, Bo Zhao*

Main category: cs.CV

TL;DR: Video-XL-Pro improves long video understanding with ReCoT, featuring Dynamic Token Synthesizer and Semantic-Guided Masking, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with hour-long video understanding, necessitating efficient token compression and training methods.

Method: Uses ReCoT with Dynamic Token Synthesizer (DTS) and Semantic-Guided Masking (SGM), plus dataset pruning and Query-aware Selector.

Result: Outperforms 7B models with 3B parameters, handles 8K frames on A100 GPU.

Conclusion: Video-XL-Pro is efficient and effective for long video understanding.

Abstract: Despite advanced token compression techniques, existing multimodal large
language models (MLLMs) still struggle with hour-long video understanding. In
this work, we propose Video-XL-Pro, an efficient method for extremely long
video understanding, built upon Reconstructive Compression of Tokens (ReCoT), a
learnable module that leverages self-supervised learning to generate
comprehensive and compact video tokens. ReCoT introduces two key components:
(i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from
static image tokens by learning intra-token relationships, which are then used
in masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively
masks redundant visual tokens to facilitate more effective reconstructive
learning. To improve training efficiency in MLLMs fine-tuning, we introduce a
video-specific dataset pruning strategy and design a simple yet Query-aware
Selector that enables the model to precisely locate query-relevant video
tokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models
trained on larger datasets across multiple long video understanding benchmarks.
Moreover, it can process over 8K frames on a single A100 GPU while maintaining
high-quality performance.

</details>


### [318] [VideoGen-Eval: Agent-based System for Video Generation Evaluation](https://arxiv.org/abs/2503.23452)
*Yuhang Yang, Ke Fan, Shangkun Sun, Hongxiang Li, Ailing Zeng, FeiLin Han, Wei Zhai, Wei Liu, Yang Cao, Zheng-Jun Zha*

Main category: cs.CV

TL;DR: VideoGen-Eval is a dynamic, flexible evaluation system for video generation models, addressing gaps in existing methods with LLM-based structuring, MLLM-based judgment, and patch tools. It includes a benchmark with 700 prompts and 12,000+ videos, showing strong alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing video generation evaluation systems are inadequate due to simple prompts, OOD challenges, and misaligned metrics.

Method: Proposes VideoGen-Eval, integrating LLM-based content structuring, MLLM-based judgment, and temporal-dense patch tools. Introduces a benchmark with 700 prompts and 12,000+ videos from 20+ models.

Result: The system aligns well with human preferences and reliably evaluates models. The benchmark is diverse and rich.

Conclusion: VideoGen-Eval effectively addresses current evaluation shortcomings and provides a robust framework for assessing video generation models.

Abstract: The rapid advancement of video generation has rendered existing evaluation
systems inadequate for assessing state-of-the-art models, primarily due to
simple prompts that cannot showcase the model's capabilities, fixed evaluation
operators struggling with Out-of-Distribution (OOD) cases, and misalignment
between computed metrics and human preferences. To bridge the gap, we propose
VideoGen-Eval, an agent evaluation system that integrates LLM-based content
structuring, MLLM-based content judgment, and patch tools designed for
temporal-dense dimensions, to achieve a dynamic, flexible, and expandable video
generation evaluation. Additionally, we introduce a video generation benchmark
to evaluate existing cutting-edge models and verify the effectiveness of our
evaluation system. It comprises 700 structured, content-rich prompts (both T2V
and I2V) and over 12,000 videos generated by 20+ models, among them, 8
cutting-edge models are selected as quantitative evaluation for the agent and
human. Extensive experiments validate that our proposed agent-based evaluation
system demonstrates strong alignment with human preferences and reliably
completes the evaluation, as well as the diversity and richness of the
benchmark.

</details>


### [319] [Statistical Management of the False Discovery Rate in Medical Instance Segmentation Based on Conformal Risk Control](https://arxiv.org/abs/2504.04482)
*Mengxia Dai, Wenqian Luo, Tianyang Li*

Main category: cs.CV

TL;DR: A robust quality control framework for medical instance segmentation using conformal prediction to address confidence calibration issues.


<details>
  <summary>Details</summary>
Motivation: Deep learning models like Mask R-CNN and BlendMask face confidence calibration problems in high-risk medical scenarios, risking misdiagnosis.

Method: Proposes a calibration-aware loss function with a dynamic threshold mechanism based on conformal prediction theory, ensuring FNR/FDR bounds.

Result: Empirical results show the framework bounds FDR metrics on test sets without modifying model architectures.

Conclusion: The framework effectively addresses calibration issues in medical segmentation, ensuring reliability in clinical settings.

Abstract: Instance segmentation plays a pivotal role in medical image analysis by
enabling precise localization and delineation of lesions, tumors, and
anatomical structures. Although deep learning models such as Mask R-CNN and
BlendMask have achieved remarkable progress, their application in high-risk
medical scenarios remains constrained by confidence calibration issues, which
may lead to misdiagnosis. To address this challenge, we propose a robust
quality control framework based on conformal prediction theory. This framework
innovatively constructs a risk-aware dynamic threshold mechanism that
adaptively adjusts segmentation decision boundaries according to clinical
requirements.Specifically, we design a \textbf{calibration-aware loss function}
that dynamically tunes the segmentation threshold based on a user-defined risk
level $\alpha$. Utilizing exchangeable calibration data, this method ensures
that the expected FNR or FDR on test data remains below $\alpha$ with high
probability. The framework maintains compatibility with mainstream segmentation
models (e.g., Mask R-CNN, BlendMask+ResNet-50-FPN) and datasets (PASCAL VOC
format) without requiring architectural modifications. Empirical results
demonstrate that we rigorously bound the FDR metric marginally over the test
set via our developed calibration framework.

</details>


### [320] [Unlocking Neural Transparency: Jacobian Maps for Explainable AI in Alzheimer's Detection](https://arxiv.org/abs/2504.03230)
*Yasmine Mustafa, Mohamed Elmahallawy, Tie Luo*

Main category: cs.CV

TL;DR: The paper proposes a pre-model approach using Jacobian Maps (JMs) to improve interpretability and trustworthiness in Alzheimer's disease (AD) detection, outperforming traditional methods in accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Early AD detection is critical, but deep learning models lack interpretability, hindering clinical trust. This work aims to bridge this gap.

Method: Introduces JMs in a multi-modal framework to capture brain volume changes, validated via 3D CNN and 3D Grad-CAM for interpretability.

Result: JMs-based CNN shows superior accuracy and provides meaningful correlations with AD biomarkers, enhancing diagnostic reliability.

Conclusion: The JMs approach improves both accuracy and interpretability in AD detection, addressing a key limitation of deep learning models in clinical settings.

Abstract: Alzheimer's disease (AD) leads to progressive cognitive decline, making early
detection crucial for effective intervention. While deep learning models have
shown high accuracy in AD diagnosis, their lack of interpretability limits
clinical trust and adoption. This paper introduces a novel pre-model approach
leveraging Jacobian Maps (JMs) within a multi-modal framework to enhance
explainability and trustworthiness in AD detection. By capturing localized
brain volume changes, JMs establish meaningful correlations between model
predictions and well-known neuroanatomical biomarkers of AD. We validate JMs
through experiments comparing a 3D CNN trained on JMs versus on traditional
preprocessed data, which demonstrates superior accuracy. We also employ 3D
Grad-CAM analysis to provide both visual and quantitative insights, further
showcasing improved interpretability and diagnostic reliability.

</details>


### [321] [Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation](https://arxiv.org/abs/2504.06962)
*Thomas Kerdreux, Alexandre Tuel, Quentin Febvre, Alexis Mouche, Bertrand Chapron*

Main category: cs.CV

TL;DR: Dynamic dataset pruning improves SSL pre-training for EO by enhancing dataset diversity and balance, leading to better transferability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored role of dataset curation in SSL for EO, especially given the challenges of redundancy and heavy-tailed distributions in satellite imagery.

Method: Proposes a dynamic dataset pruning strategy to iteratively refine training sets without needing a pre-existing feature extractor, tested on the Sentinel-1 WV SAR archive.

Result: Dynamic pruning improves computational efficiency and representation quality, enhancing transferability across three downstream tasks.

Conclusion: The approach is effective for domains with limited curated datasets, and the OceanSAR-1 model is released as a foundation for ocean observation.

Abstract: Self-supervised learning (SSL) has enabled the development of vision
foundation models for Earth Observation (EO), demonstrating strong
transferability across diverse remote sensing tasks. While prior work has
focused on network architectures and training strategies, the role of dataset
curation, especially in balancing and diversifying pre-training datasets,
remains underexplored. In EO, this challenge is amplified by the redundancy and
heavy-tailed distributions common in satellite imagery, which can lead to
biased representations and inefficient training.
  In this work, we propose a dynamic dataset pruning strategy designed to
improve SSL pre-training by maximizing dataset diversity and balance. Our
method iteratively refines the training set without requiring a pre-existing
feature extractor, making it well-suited for domains where curated datasets are
limited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode
(WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by
ocean observations. We train models from scratch on the entire Sentinel-1 WV
archive spanning 10 years. Across three downstream tasks, our results show that
dynamic pruning improves both computational efficiency and representation
quality, leading to stronger transferability.
  We also release the weights of OceanSAR-1, the first model in the OceanSAR
family, a series of foundation models for ocean observation and analysis using
SAR imagery, at github.com/galeio-research/OceanSAR-models/.

</details>


### [322] [NCL-CIR: Noise-aware Contrastive Learning for Composed Image Retrieval](https://arxiv.org/abs/2504.04339)
*Peng Gao, Yujian Lee, Zailong Chen, Hui zhang, Xubo Liu, Yiyang Hu, Guquang Jing*

Main category: cs.CV

TL;DR: NCL-CIR introduces noise-aware contrastive learning to address mismatched pairs in Composed Image Retrieval, improving performance via Weight Compensation Block and Noise-pair Filter Block.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods assume perfect query-target alignment, but real-world datasets often contain mismatched pairs, leading to false positives and degraded performance.

Method: Proposes NCL-CIR with Weight Compensation Block (WCB) for stable token representations and Noise-pair Filter Block (NFB) with GMM to predict noise pairs and design a soft-label NCE loss.

Result: NCL-CIR outperforms benchmarks by mitigating mismatched and partially matched sample effects.

Conclusion: NCL-CIR effectively handles noise in CIR datasets, enhancing retrieval performance.

Abstract: Composed Image Retrieval (CIR) seeks to find a target image using a
multi-modal query, which combines an image with modification text to pinpoint
the target. While recent CIR methods have shown promise, they mainly focus on
exploring relationships between the query pairs (image and text) through data
augmentation or model design. These methods often assume perfect alignment
between queries and target images, an idealized scenario rarely encountered in
practice. In reality, pairs are often partially or completely mismatched due to
issues like inaccurate modification texts, low-quality target images, and
annotation errors. Ignoring these mismatches leads to numerous False Positive
Pair (FFPs) denoted as noise pairs in the dataset, causing the model to overfit
and ultimately reducing its performance. To address this problem, we propose
the Noise-aware Contrastive Learning for CIR (NCL-CIR), comprising two key
components: the Weight Compensation Block (WCB) and the Noise-pair Filter Block
(NFB). The WCB coupled with diverse weight maps can ensure more stable token
representations of multi-modal queries and target images. Meanwhile, the NFB,
in conjunction with the Gaussian Mixture Model (GMM) predicts noise pairs by
evaluating loss distributions, and generates soft labels correspondingly,
allowing for the design of the soft-label based Noise Contrastive Estimation
(NCE) loss function. Consequently, the overall architecture helps to mitigate
the influence of mismatched and partially matched samples, with experimental
results demonstrating that NCL-CIR achieves exceptional performance on the
benchmark datasets.

</details>


### [323] [AD-Det: Boosting Object Detection in UAV Images with Focused Small Objects and Balanced Tail Classes](https://arxiv.org/abs/2504.05601)
*Zhenteng Li, Sheng Lian, Dengfeng Pan, Youlin Wang, Wei Liu*

Main category: cs.CV

TL;DR: AD-Det is a novel framework for UAV object detection, combining Adaptive Small Object Enhancement (ASOE) and Dynamic Class-balanced Copy-paste (DCC) to tackle scale variations and class imbalance. It outperforms existing methods, achieving 37.5% AP on VisDrone.


<details>
  <summary>Details</summary>
Motivation: Challenges in UAV object detection include complex scale variations and class imbalance, which existing methods address separately. This paper aims to synergize solutions for these issues.

Method: AD-Det integrates ASOE for small object detection and DCC for class imbalance. ASOE clusters and enlarges small object regions, while DCC dynamically resamples tail classes using a memory bank.

Result: AD-Det achieves 37.5% AP on VisDrone, outperforming competitors by at least 3.1%. It also performs well on UAVDT.

Conclusion: AD-Det effectively addresses UAV object detection challenges through a synergistic framework, demonstrating superior performance on benchmark datasets.

Abstract: Object detection in Unmanned Aerial Vehicle (UAV) images poses significant
challenges due to complex scale variations and class imbalance among objects.
Existing methods often address these challenges separately, overlooking the
intricate nature of UAV images and the potential synergy between them. In
response, this paper proposes AD-Det, a novel framework employing a coherent
coarse-to-fine strategy that seamlessly integrates two pivotal components:
Adaptive Small Object Enhancement (ASOE) and Dynamic Class-balanced Copy-paste
(DCC). ASOE utilizes a high-resolution feature map to identify and cluster
regions containing small objects. These regions are subsequently enlarged and
processed by a fine-grained detector. On the other hand, DCC conducts
object-level resampling by dynamically pasting tail classes around the cluster
centers obtained by ASOE, main-taining a dynamic memory bank for each tail
class. This approach enables AD-Det to not only extract regions with small
objects for precise detection but also dynamically perform reasonable
resampling for tail-class objects. Consequently, AD-Det enhances the overall
detection performance by addressing the challenges of scale variations and
class imbalance in UAV images through a synergistic and adaptive framework. We
extensively evaluate our approach on two public datasets, i.e., VisDrone and
UAVDT, and demonstrate that AD-Det significantly outperforms existing
competitive alternatives. Notably, AD-Det achieves a 37.5% Average Precision
(AP) on the VisDrone dataset, surpassing its counterparts by at least 3.1%.

</details>


### [324] [MM-IFEngine: Towards Multimodal Instruction Following](https://arxiv.org/abs/2504.07957)
*Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, Jiaqi Wang*

Main category: cs.CV

TL;DR: The paper introduces MM-IFEngine, a pipeline for generating high-quality image-instruction pairs, and MM-IFEval, a benchmark for evaluating multi-modal instruction-following. Fine-tuning models on the generated datasets improves performance on IF benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal instruction-following training data is scarce, benchmarks are simple, and evaluation strategies are imprecise for exact output constraints.

Method: Proposes MM-IFEngine to generate image-instruction pairs (MM-IFInstruct-23k and MM-IFDPO-23k) and MM-IFEval for evaluation.

Result: Fine-tuning on MM-IFInstruct-23k and MM-IFDPO-23k improves performance on benchmarks like MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%).

Conclusion: The pipeline and datasets effectively address gaps in multimodal instruction-following, with open-sourced resources for broader use.

Abstract: The Instruction Following (IF) ability measures how well Multi-modal Large
Language Models (MLLMs) understand exactly what users are telling them and
whether they are doing it right. Existing multimodal instruction following
training data is scarce, the benchmarks are simple with atomic instructions,
and the evaluation strategies are imprecise for tasks demanding exact output
constraints. To address this, we present MM-IFEngine, an effective pipeline to
generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields
large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which
is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for
Direct Preference Optimization (DPO). We further introduce MM-IFEval, a
challenging and diverse multi-modal instruction-following benchmark that
includes (1) both compose-level constraints for output responses and
perception-level constraints tied to the input images, and (2) a comprehensive
evaluation pipeline incorporating both rule-based assessment and judge model.
We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on
MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF
benchmarks, such as MM-IFEval (+10.2$\%$), MIA (+7.6$\%$), and IFEval
(+12.3$\%$). We have fully open-sourced the datasets (both SFT and DPO),
evaluation code and training scripts at https://github.com/SYuan03/MM-IFEngine.

</details>


### [325] [MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation](https://arxiv.org/abs/2504.09149)
*Changhao Li, Yu Xin, Xiaowei Zhou, Ariel Shamir, Hao Zhang, Ligang Liu, Ruizhen Hu*

Main category: cs.CV

TL;DR: MASH is a novel 3D shape representation using spherical distance functions from anchor points, encoded with spherical harmonics, for versatile applications like reconstruction and generation.


<details>
  <summary>Details</summary>
Motivation: To improve perceptual shape understanding and learning of 3D shapes by representing them as observable local surface patches.

Method: Represents 3D shapes as spherical distance functions from anchor points, uses spherical harmonics for encoding, and employs a parameterized view cone for locality. Includes a differentiable optimization algorithm for point cloud conversion.

Result: MASH accurately approximates ground-truth surfaces and excels in applications like surface reconstruction, shape generation, completion, and blending.

Conclusion: MASH's unique implicit-explicit hybrid representation offers superior performance in diverse 3D shape tasks.

Abstract: We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view
and parametrized representation of 3D shapes. Inspired by multi-view geometry
and motivated by the importance of perceptual shape understanding for learning
3D shapes, MASH represents a 3D shape as a collection of observable local
surface patches, each defined by a spherical distance function emanating from
an anchor point. We further leverage the compactness of spherical harmonics to
encode the MASH functions, combined with a generalized view cone with a
parameterized base that masks the spatial extent of the spherical function to
attain locality. We develop a differentiable optimization algorithm capable of
converting any point cloud into a MASH representation accurately approximating
ground-truth surfaces with arbitrary geometry and topology. Extensive
experiments demonstrate that MASH is versatile for multiple applications
including surface reconstruction, shape generation, completion, and blending,
achieving superior performance thanks to its unique representation encompassing
both implicit and explicit features.

</details>


### [326] [DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environment](https://arxiv.org/abs/2504.11019)
*Hyejin Lee, Seokjun Hong, Jeonghoon Song, Haechan Cho, Zhixiong Jin, Byeonghun Kim, Joobin Jin, Jaegyun Im, Byeongjoon Noh, Hwasoo Yeo*

Main category: cs.CV

TL;DR: The DRIFT dataset offers high-resolution vehicle trajectories from drone videos, enabling multi-scale traffic analysis without preprocessing.


<details>
  <summary>Details</summary>
Motivation: To provide reliable urban traffic data for understanding mobility and improving traffic management.

Method: Collected data from synchronized drone videos at 250m altitude, covering nine intersections in Daejeon, South Korea, with 81,699 trajectories processed via video synchronization and orthomap alignment.

Result: A comprehensive dataset with directional vehicle trajectories, supporting analysis from individual maneuvers to network flow dynamics.

Conclusion: DRIFT aids academic and practical traffic studies, with open-source tools and public accessibility.

Abstract: Reliable traffic data are essential for understanding urban mobility and
developing effective traffic management strategies. This study introduces the
DRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale
urban traffic dataset collected systematically from synchronized drone videos
at approximately 250 meters altitude, covering nine interconnected
intersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle
trajectories that include directional information, processed through video
synchronization and orthomap alignment, resulting in a comprehensive dataset of
81,699 vehicle trajectories. Through our DRIFT dataset, researchers can
simultaneously analyze traffic at multiple scales - from individual vehicle
maneuvers like lane-changes and safety metrics such as time-to-collision to
aggregate network flow dynamics across interconnected urban intersections. The
DRIFT dataset is structured to enable immediate use without additional
preprocessing, complemented by open-source models for object detection and
trajectory extraction, as well as associated analytical tools. DRIFT is
expected to significantly contribute to academic research and practical
applications, such as traffic flow analysis and simulation studies. The dataset
and related resources are publicly accessible at
https://github.com/AIxMobility/The-DRIFT.

</details>


### [327] [Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals](https://arxiv.org/abs/2504.12121)
*Jose Francisco Diez-Pastor, Francisco Javier Gonzalez-Moya, Pedro Latorre-Carmona, Francisco Javier Perez-Barbería, Ludmila I. Kuncheva, Antonio Canepa-Oneto, Alvar Arnaiz-González, Cesar Garcia-Osorio*

Main category: cs.CV

TL;DR: Assessed ML methods for detecting grazing trails, finding UNet with MambaOut encoder as the best solution for automated mapping.


<details>
  <summary>Details</summary>
Motivation: To identify spatial regions where biodiversity is threatened by automating the detection of grazing trails for ecosystem conservation.

Method: Tested five semantic segmentation models combined with 14 encoder networks.

Result: UNet with MambaOut encoder performed the best.

Conclusion: The solution can serve as a foundation for tools to map and monitor grazing trails over time.

Abstract: Identifying spatial regions where biodiversity is threatened is crucial for
effective ecosystem conservation and monitoring. In this stydy, we assessed
varios machine learning methods to detect grazing trails automatically. We
tested five semantic segmentation models combined with 14 different encoder
networks. The best combination was UNet with MambaOut encoder. The solution
proposed could be used as the basis for tools aiming at mapping and tracking
changes in grazing trails on a continuous temporal basis.

</details>


### [328] [U-Shape Mamba: State Space Model for faster diffusion](https://arxiv.org/abs/2504.13499)
*Alex Ergasti, Filippo Botti, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati*

Main category: cs.CV

TL;DR: USM is a new diffusion model using Mamba-based layers in a U-Net structure, reducing computational costs while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: High computational costs of diffusion models hinder their efficiency.

Method: USM integrates Mamba blocks in a U-Net-like hierarchy to reduce sequence length in the encoder and restore it in the decoder.

Result: USM reduces GFlops by one-third, uses less memory, and is faster than Zigma, with improved FID scores on multiple datasets.

Conclusion: USM is an efficient, scalable solution for high-quality image synthesis with lower computational costs.

Abstract: Diffusion models have become the most popular approach for high-quality image
generation, but their high computational cost still remains a significant
challenge. To address this problem, we propose U-Shape Mamba (USM), a novel
diffusion model that leverages Mamba-based layers within a U-Net-like
hierarchical structure. By progressively reducing sequence length in the
encoder and restoring it in the decoder through Mamba blocks, USM significantly
lowers computational overhead while maintaining strong generative capabilities.
Experimental results against Zigma, which is currently the most efficient
Mamba-based diffusion model, demonstrate that USM achieves one-third the
GFlops, requires less memory and is faster, while outperforming Zigma in image
quality. Frechet Inception Distance (FID) is improved by 15.3, 0.84 and 2.7
points on AFHQ, CelebAHQ and COCO datasets, respectively. These findings
highlight USM as a highly efficient and scalable solution for diffusion-based
generative models, making high-quality image synthesis more accessible to the
research community while reducing computational costs.

</details>


### [329] [Compile Scene Graphs with Reinforcement Learning](https://arxiv.org/abs/2504.13617)
*Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, Chang Wen Chen*

Main category: cs.CV

TL;DR: The paper introduces R1-SGG, a multimodal LLM trained via supervised fine-tuning and refined with reinforcement learning to generate structured scene graphs, outperforming SFT alone.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored use of LLMs for end-to-end extraction of structured visual representations like scene graphs, which requires accurate object and relationship triplet generation.

Method: R1-SGG is trained via supervised fine-tuning (SFT) on scene graph data and refined with reinforcement learning (RL), using a graph-centric reward function.

Result: Rule-based RL significantly improves model performance, achieving a zero failure rate, unlike SFT which struggles to generalize.

Conclusion: The combination of SFT and RL effectively enhances the model's ability to generate structured scene graphs, demonstrating the potential of RL in such tasks.

Abstract: Next token prediction is the fundamental principle for training large
language models (LLMs), and reinforcement learning (RL) further enhances their
reasoning performance. As an effective way to model language, image, video, and
other modalities, the use of LLMs for end-to-end extraction of structured
visual representations, such as scene graphs, remains underexplored. It
requires the model to accurately produce a set of objects and relationship
triplets, rather than generating text token by token. To achieve this, we
introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised
fine-tuning (SFT) on the scene graph dataset and subsequently refined using
reinforcement learning to enhance its ability to generate scene graphs in an
end-to-end manner. The SFT follows a conventional prompt-response paradigm,
while RL requires the design of effective reward signals. Given the structured
nature of scene graphs, we design a graph-centric reward function that
integrates node-level rewards, edge-level rewards, and a format consistency
reward. Our experiments demonstrate that rule-based RL substantially enhances
model performance in the SGG task, achieving a zero failure rate--unlike
supervised fine-tuning (SFT), which struggles to generalize effectively. Our
code is available at https://github.com/gpt4vision/R1-SGG.

</details>


### [330] [ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification](https://arxiv.org/abs/2504.14139)
*Hai Pham-Ngoc, De Nguyen-Van, Dung Vu-Tien, Phuong Le-Hong*

Main category: cs.CV

TL;DR: A deep learning system for thyroid FNAB image classification achieves high accuracy with low computational cost, validated internally and externally.


<details>
  <summary>Details</summary>
Motivation: Address challenges in thyroid FNAB image classification, such as limited data and high computational costs, to provide efficient clinical support.

Method: Uses YOLOv10 for cell cluster detection, curriculum learning, lightweight EfficientNetB0, and a Transformer-inspired module for multi-scale analysis.

Result: Achieves high macro F1 (89.19%) and AUCs (0.98, 0.95, 0.96) on internal tests; external validation shows AUCs of 0.9495, 0.7436, 0.8396.

Conclusion: High-accuracy, interpretable thyroid FNAB classification is feasible with minimal computational demands.

Abstract: Background: Automated classification of thyroid Fine Needle Aspiration Biopsy
(FNAB) images faces challenges in limited data, inter-observer variability, and
computational cost. Efficient, interpretable models are crucial for clinical
support.
  Objective: To develop and externally validate a deep learning system for
multi-class thyroid FNAB image classification into three key categories
directly guiding post-biopsy treatment in Vietnam: Benign (Bethesda II),
Indeterminate/Suspicious (BI, III, IV, V), and Malignant (BVI), achieving high
diagnostic accuracy with low computational overhead.
  Methods: Our pipeline features: (1) YOLOv10 cell cluster detection for
informative sub-region extraction/noise reduction; (2) curriculum learning
sequencing localized crops to full images for multi-scale capture; (3) adaptive
lightweight EfficientNetB0 (4M parameters) balancing performance/efficiency;
and (4) a Transformer-inspired module for multi-scale/multi-region analysis.
External validation used 1,015 independent FNAB images.
  Results: ThyroidEffi Basic achieved macro F1 of 89.19% and AUCs of 0.98
(Benign), 0.95 (Indeterminate/Suspicious), 0.96 (Malignant) on the internal
test set. External validation yielded AUCs of 0.9495 (Benign), 0.7436
(Indeterminate/Suspicious), 0.8396 (Malignant). ThyroidEffi Premium improved
macro F1 to 89.77%. Grad-CAM highlighted key diagnostic regions, confirming
interpretability. The system processed 1000 cases in 30 seconds, demonstrating
feasibility on widely accessible hardware.
  Conclusions: This work demonstrates that high-accuracy, interpretable thyroid
FNAB image classification is achievable with minimal computational demands.

</details>


### [331] [VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation](https://arxiv.org/abs/2504.15095)
*Mingxia Zhan, Li Zhang, Xiaomeng Chu, Beibei Wang*

Main category: cs.CV

TL;DR: VistaDepth improves monocular depth estimation by integrating frequency-domain enhancements and adaptive weighting into diffusion models, excelling in distant depth reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based MDE methods struggle with distant depth accuracy due to imbalanced depth distributions and spatial-domain over-reliance.

Method: VistaDepth uses a Latent Frequency Modulation (LFM) module for spectral refinement and adaptive weighting to balance diffusion loss, enhancing distant depth reconstruction.

Result: VistaDepth achieves state-of-the-art performance, particularly in distant depth accuracy.

Conclusion: The framework's innovations in frequency-domain features and adaptive weighting significantly improve MDE, especially for distant regions.

Abstract: Monocular depth estimation (MDE) aims to predict per-pixel depth values from
a single RGB image. Recent advancements have positioned diffusion models as
effective MDE tools by framing the challenge as a conditional image generation
task. Despite their progress, these methods often struggle with accurately
reconstructing distant depths, due largely to the imbalanced distribution of
depth values and an over-reliance on spatial-domain features. To overcome these
limitations, we introduce VistaDepth, a novel framework that integrates
adaptive frequency-domain feature enhancements with an adaptive
weight-balancing mechanism into the diffusion process. Central to our approach
is the Latent Frequency Modulation (LFM) module, which dynamically refines
spectral responses in the latent feature space, thereby improving the
preservation of structural details and reducing noisy artifacts. Furthermore,
we implement an adaptive weighting strategy that modulates the diffusion loss
in real-time, enhancing the model's sensitivity towards distant depth
reconstruction. These innovations collectively result in superior depth
perception performance across both distance and detail. Experimental
evaluations confirm that VistaDepth achieves state-of-the-art performance among
diffusion-based MDE techniques, particularly excelling in the accurate
reconstruction of distant regions.

</details>


### [332] [Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation](https://arxiv.org/abs/2504.15134)
*Xiao Zhang, Lu Zou, Tao Lu, Yuan Yao, Zhangjin Huang, Guoping Wang*

Main category: cs.CV

TL;DR: INKL-Pose introduces instance-adaptive keypoint learning with local-to-global geometric aggregation for category-level 6D pose estimation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in generalizing across diverse object instances, especially those with complex geometries or deviations from canonical shapes.

Method: Predicts keypoints via an Instance-Adaptive Keypoint Generator, refines them with local and global feature aggregators, and uses bidirectional Mamba with Feature Sequence Flipping. Includes surface and separation losses for keypoint quality.

Result: Achieves state-of-the-art performance on CAMERA25, REAL275, and HouseCat6D datasets.

Conclusion: INKL-Pose effectively handles intra-class variations and complex geometries, setting a new benchmark for category-level pose estimation.

Abstract: Category-level object pose estimation aims to predict the 6D pose and size of
previously unseen instances from predefined categories, requiring strong
generalization across diverse object instances. Although many previous methods
attempt to mitigate intra-class variations, they often struggle with instances
exhibiting complex geometries or significant deviations from canonical shapes.
To address this challenge, we propose INKL-Pose, a novel category-level object
pose estimation framework that enables INstance-adaptive Keypoint Learning with
local-to-global geometric aggregation. Specifically, our approach first
predicts semantically consistent and geometric informative keypoints through an
Instance-Adaptive Keypoint Generator, then refines them with: (1) a Local
Keypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global
Keypoint Feature Aggregator using bidirectional Mamba for structural
consistency. To enable bidirectional modeling in Mamba, we introduce a Feature
Sequence Flipping strategy that preserves spatial coherence while constructing
backward feature sequences. Additionally, we design a surface loss and a
separation loss to enforce uniform coverage and spatial diversity in keypoint
distribution. The generated keypoints are finally mapped to a canonical space
for regressing the object's 6D pose and size. Extensive experiments on
CAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves
state-of-the-art performance and significantly outperforms existing methods.

</details>


### [333] [FaceInsight: A Multimodal Large Language Model for Face Perception](https://arxiv.org/abs/2504.15624)
*Jingzhi Li, Changjiang Luo, Ruoyu Chen, Hua Zhang, Wenqi Ren, Jianhou Gan, Xiaochun Cao*

Main category: cs.CV

TL;DR: FaceInsight is a multimodal large language model (MLLM) designed for face perception tasks, outperforming general-domain MLLMs by incorporating visual-textual alignment and face segmentation maps.


<details>
  <summary>Details</summary>
Motivation: General-domain MLLMs perform poorly in face perception tasks, producing inaccurate responses. This gap motivates the development of FaceInsight.

Method: FaceInsight aligns visual-textual facial knowledge and uses face segmentation maps as an auxiliary modality to enhance understanding.

Result: FaceInsight consistently outperforms nine other MLLMs in three face perception tasks under training-free and fine-tuned settings.

Conclusion: FaceInsight addresses the limitations of general-domain MLLMs in face perception, offering improved accuracy and fine-grained facial understanding.

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
strong capabilities in understanding general visual content. However, these
general-domain MLLMs perform poorly in face perception tasks, often producing
inaccurate or misleading responses to face-specific queries. To address this
gap, we propose FaceInsight, the versatile face perception MLLM that provides
fine-grained facial information. Our approach introduces visual-textual
alignment of facial knowledge to model both uncertain dependencies and
deterministic relationships among facial information, mitigating the
limitations of language-driven reasoning. Additionally, we incorporate face
segmentation maps as an auxiliary perceptual modality, enriching the visual
input with localized structural cues to enhance semantic understanding.
Comprehensive experiments and analyses across three face perception tasks
demonstrate that FaceInsight consistently outperforms nine compared MLLMs under
both training-free and fine-tuned settings.

</details>


### [334] [PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels](https://arxiv.org/abs/2504.16419)
*Qi Yang, Weichen Bi, Haiyang Shen, Yaoqi Guo, Yun Ma*

Main category: cs.CV

TL;DR: PixelWeb is a large-scale GUI dataset with over 100,000 annotated web pages, addressing inaccurate BBox annotations in existing datasets through a novel automatic annotation approach combining visual and DOM analysis.


<details>
  <summary>Details</summary>
Motivation: Existing GUI datasets suffer from inaccurate BBox annotations (missing, duplicate, or meaningless) and lack comprehensive metadata, limiting model performance and downstream task development.

Method: PixelWeb uses channel derivation (BGRA four-channel bitmap annotations) for accurate GUI element localization and layer analysis (DOM-based visibility and stacking order) for precise BBox annotations.

Result: PixelWeb achieves 3-7 times better performance on the mAP95 metric for GUI element detection compared to existing datasets.

Conclusion: PixelWeb's high-quality annotations and comprehensive metadata make it a valuable resource for improving downstream tasks like GUI generation and automated user interaction.

Abstract: Graphical User Interface (GUI) datasets are crucial for various downstream
tasks. However, GUI datasets often generate annotation information through
automatic labeling, which commonly results in inaccurate GUI element BBox
annotations, including missing, duplicate, or meaningless BBoxes. These issues
can degrade the performance of models trained on these datasets, limiting their
effectiveness in real-world applications. Additionally, existing GUI datasets
only provide BBox annotations visually, which restricts the development of
visually related GUI downstream tasks. To address these issues, we introduce
PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web
pages. PixelWeb is constructed using a novel automatic annotation approach that
integrates visual feature extraction and Document Object Model (DOM) structure
analysis through two core modules: channel derivation and layer analysis.
Channel derivation ensures accurate localization of GUI elements in cases of
occlusion and overlapping elements by extracting BGRA four-channel bitmap
annotations. Layer analysis uses the DOM to determine the visibility and
stacking order of elements, providing precise BBox annotations. Additionally,
PixelWeb includes comprehensive metadata such as element images, contours, and
mask annotations. Manual verification by three independent annotators confirms
the high quality and accuracy of PixelWeb annotations. Experimental results on
GUI element detection tasks show that PixelWeb achieves performance on the
mAP95 metric that is 3-7 times better than existing datasets. We believe that
PixelWeb has great potential for performance improvement in downstream tasks
such as GUI generation and automated user interaction.

</details>


### [335] [FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation](https://arxiv.org/abs/2504.15958)
*Zebin Yao, Lei Ren, Huixing Jiang, Chen Wei, Xiaojie Wang, Ruifan Li, Fangxiang Feng*

Main category: cs.CV

TL;DR: FreeGraftor is a training-free framework for subject-driven image generation, balancing fidelity and efficiency via cross-image feature grafting.


<details>
  <summary>Details</summary>
Motivation: Existing methods face a trade-off between fidelity and efficiency; tuning-based approaches are resource-heavy, while zero-shot methods lack subject consistency.

Method: Uses semantic matching, position-constrained attention fusion, and noise initialization to transfer visual details and preserve geometry priors.

Result: Outperforms zero-shot and training-free methods in subject fidelity and text alignment, with seamless multi-subject generation.

Conclusion: FreeGraftor offers a practical, efficient solution for high-fidelity subject-driven image synthesis without training.

Abstract: Subject-driven image generation aims to synthesize novel scenes that
faithfully preserve subject identity from reference images while adhering to
textual guidance, yet existing methods struggle with a critical trade-off
between fidelity and efficiency. Tuning-based approaches rely on time-consuming
and resource-intensive subject-specific optimization, while zero-shot methods
fail to maintain adequate subject consistency. In this work, we propose
FreeGraftor, a training-free framework that addresses these limitations through
cross-image feature grafting. Specifically, FreeGraftor employs semantic
matching and position-constrained attention fusion to transfer visual details
from reference subjects to the generated image. Additionally, our framework
incorporates a novel noise initialization strategy to preserve geometry priors
of reference subjects for robust feature matching. Extensive qualitative and
quantitative experiments demonstrate that our method enables precise subject
identity transfer while maintaining text-aligned scene synthesis. Without
requiring model fine-tuning or additional training, FreeGraftor significantly
outperforms existing zero-shot and training-free approaches in both subject
fidelity and text alignment. Furthermore, our framework can seamlessly extend
to multi-subject generation, making it practical for real-world deployment. Our
code is available at https://github.com/Nihukat/FreeGraftor.

</details>


### [336] [EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception](https://arxiv.org/abs/2504.16616)
*Haosheng Chen, Lian Luo, Mengjingcheng Mo, Zhanjie Wu, Guobao Xiao, Ji Gan, Jiaxu Leng, Xinbo Gao*

Main category: cs.CV

TL;DR: EHGCN is a novel approach for event vision, combining Euclidean and hyperbolic spaces to improve perception of event streams with adaptive sampling and motion-aware hyperedge generation.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based methods struggle with long-range dependencies and hierarchical structures in non-uniform event streams.

Method: EHGCN uses adaptive sampling, motion-aware hyperedge generation, and a Euclidean-Hyperbolic GCN for hybrid event perception.

Result: Experiments show EHGCN's effectiveness in tasks like object detection and recognition.

Conclusion: EHGCN successfully addresses limitations of traditional methods by leveraging dual-space modeling.

Abstract: Event cameras, with microsecond temporal resolution and high dynamic range
(HDR) characteristics, emit high-speed event stream for perception tasks.
Despite the recent advancement in GNN-based perception methods, they are prone
to use straightforward pairwise connectivity mechanisms in the pure Euclidean
space where they struggle to capture long-range dependencies and fail to
effectively characterize the inherent hierarchical structures of non-uniformly
distributed event stream. To this end, in this paper we propose a novel
approach named EHGCN, which is a pioneer to perceive event stream in both
Euclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an
adaptive sampling strategy to dynamically regulate sampling rates, retaining
discriminative events while attenuating chaotic noise. Then we present a Markov
Vector Field (MVF)-driven motion-aware hyperedge generation method based on
motion state transition probabilities, thereby eliminating cross-target
spurious associations and providing critically topological priors while
capturing long-range dependencies between events. Finally, we propose a
Euclidean-Hyperbolic GCN to fuse the information locally aggregated and
globally hierarchically modeled in Euclidean and hyperbolic spaces,
respectively, to achieve hybrid event perception. Experimental results on event
perception tasks such as object detection and recognition validate the
effectiveness of our approach.

</details>


### [337] [Latent Video Dataset Distillation](https://arxiv.org/abs/2504.17132)
*Ning Li, Antai Andy Liu, Jingran Zhang, Justin Cui*

Main category: cs.CV

TL;DR: A novel video dataset distillation method in latent space outperforms prior methods, achieving significant performance gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing video dataset distillation methods focus on pixel space, ignoring latent space advancements in modern models.

Method: Uses a state-of-the-art variational encoder in latent space, diversity-aware data selection, and a training-free compression technique.

Result: Achieves 2.6% and 7.8% performance increases on HMDB51 IPC 1 and MiniUCF IPC 5, respectively.

Conclusion: The approach sets a new state-of-the-art in video dataset distillation by leveraging latent space and diversity-aware strategies.

Abstract: Dataset distillation has demonstrated remarkable effectiveness in
high-compression scenarios for image datasets. While video datasets inherently
contain greater redundancy, existing video dataset distillation methods
primarily focus on compression in the pixel space, overlooking advances in the
latent space that have been widely adopted in modern text-to-image and
text-to-video models. In this work, we bridge this gap by introducing a novel
video dataset distillation approach that operates in the latent space using a
state-of-the-art variational encoder. Furthermore, we employ a diversity-aware
data selection strategy to select both representative and diverse samples.
Additionally, we introduce a simple, training-free method to further compress
the distilled latent dataset. By combining these techniques, our approach
achieves a new state-of-the-art performance in dataset distillation,
outperforming prior methods on all datasets, e.g. on HMDB51 IPC 1, we achieve a
2.6% performance increase; on MiniUCF IPC 5, we achieve a 7.8% performance
increase. Our code is available at
https://github.com/liningresearch/Latent_Video_Dataset_Distillation.

</details>


### [338] [MASR: Self-Reflective Reasoning through Multimodal Hierarchical Attention Focusing for Agent-based Video Understanding](https://arxiv.org/abs/2504.17213)
*Shiwen Cao, Zhaoxing Zhang, Junming Jiao, Juyi Qiao, Guowen Song, Rong Shen, Xiangbing Meng*

Main category: cs.CV

TL;DR: MASR framework improves video understanding by prioritizing relevant segments and adjusting attention dynamically, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Video understanding is challenging due to information redundancy; MASR aims to enhance accuracy by focusing on query-relevant segments.

Method: MASR uses Multimodal Coarse-to-fine Relevance Sensing (MCRS) and Dilated Temporal Expansion (DTE) to adaptively adjust attention and extract relevant context.

Result: MASR achieves 5% gain in EgoSchema and outperforms benchmarks in Next-QA, IntentQA, and Video-MME datasets.

Conclusion: MASR effectively addresses video understanding challenges by dynamically focusing attention, demonstrating superior performance across datasets.

Abstract: Even in the era of rapid advances in large models, video understanding
remains a highly challenging task. Compared to texts or images, videos commonly
contain more information with redundancy, requiring large models to properly
allocate attention at a global level for comprehensive and accurate
understanding. To address this, we propose a Multimodal hierarchical Attention
focusing Self-reflective Reasoning (MASR) framework for agent-based video
understanding. The key innovation lies in its ability to detect and prioritize
segments of videos that are highly relevant to the query. Firstly, MASR
realizes Multimodal Coarse-to-fine Relevance Sensing (MCRS) which enhances the
correlation between the acquired contextual information and the query.
Secondly, MASR employs Dilated Temporal Expansion (DTE) to mitigate the risk of
missing crucial details when extracting semantic information from the focused
frames selected through MCRS. By iteratively applying MCRS and DTE in the
self-reflective reasoning process, MASR is able to adaptively adjust the
attention to extract highly query-relevant context and therefore improve the
response accuracy. In the EgoSchema dataset, MASR achieves a remarkable 5%
performance gain over previous leading approaches. In the Next-QA and IntentQA
datasets, it outperforms the state-of-the-art standards by 0.2% and 0.3%
respectively. In the Video-MME dataset that contains long-term videos, MASR
also performs better than other agent-based methods.

</details>


### [339] [Step1X-Edit: A Practical Framework for General Image Editing](https://arxiv.org/abs/2504.17761)
*Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, Daxin Jiang*

Main category: cs.CV

TL;DR: The paper introduces Step1X-Edit, an open-source image editing model that rivals proprietary models like GPT-4o and Gemini2 Flash, using Multimodal LLM and diffusion decoding.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between open-source and closed-source image editing models by developing a competitive open-source alternative.

Method: Uses Multimodal LLM for processing images and user instructions, extracts latent embeddings, and integrates them with a diffusion decoder. A high-quality dataset is generated for training, and GEdit-Bench is developed for evaluation.

Result: Step1X-Edit outperforms open-source baselines and approaches the performance of proprietary models on GEdit-Bench.

Conclusion: Step1X-Edit advances open-source image editing, narrowing the gap with proprietary models.

Abstract: In recent years, image editing models have witnessed remarkable and rapid
development. The recent unveiling of cutting-edge multimodal models such as
GPT-4o and Gemini2 Flash has introduced highly promising image editing
capabilities. These models demonstrate an impressive aptitude for fulfilling a
vast majority of user-driven editing requirements, marking a significant
advancement in the field of image manipulation. However, there is still a large
gap between the open-source algorithm with these closed-source models. Thus, in
this paper, we aim to release a state-of-the-art image editing model, called
Step1X-Edit, which can provide comparable performance against the closed-source
models like GPT-4o and Gemini2 Flash. More specifically, we adopt the
Multimodal LLM to process the reference image and the user's editing
instruction. A latent embedding has been extracted and integrated with a
diffusion image decoder to obtain the target image. To train the model, we
build a data generation pipeline to produce a high-quality dataset. For
evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world
user instructions. Experimental results on GEdit-Bench demonstrate that
Step1X-Edit outperforms existing open-source baselines by a substantial margin
and approaches the performance of leading proprietary models, thereby making
significant contributions to the field of image editing.

</details>


### [340] [Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models](https://arxiv.org/abs/2504.17789)
*Xu Ma, Peize Sun, Haoyu Ma, Hao Tang, Chih-Yao Ma, Jialiang Wang, Kunpeng Li, Xiaoliang Dai, Yujun Shi, Xuan Ju, Yushi Hu, Artsiom Sanakoyeu, Felix Juefei-Xu, Ji Hou, Junjiao Tian, Tao Xu, Tingbo Hou, Yen-Cheng Liu, Zecheng He, Zijian He, Matt Feiszli, Peizhao Zhang, Peter Vajda, Sam Tsai, Yun Fu*

Main category: cs.CV

TL;DR: Token-Shuffle reduces image tokens in AR models for efficient high-resolution image synthesis, outperforming AR and diffusion models.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of AR models in image synthesis due to high token counts, leveraging visual redundancy in MLLMs.

Method: Introduces token-shuffle (merging local tokens) and token-unshuffle (restoring spatial arrangement) to reduce tokens while maintaining resolution.

Result: Achieves 2048x2048 resolution, outperforming AR and diffusion models in benchmarks and human evaluations.

Conclusion: Token-Shuffle is a foundational method for efficient high-resolution image generation in MLLMs.

Abstract: Autoregressive (AR) models, long dominant in language generation, are
increasingly applied to image synthesis but are often considered less
competitive than Diffusion-based models. A primary limitation is the
substantial number of image tokens required for AR models, which constrains
both training and inference efficiency, as well as image resolution. To address
this, we present Token-Shuffle, a novel yet simple method that reduces the
number of image tokens in Transformer. Our key insight is the dimensional
redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),
where low-dimensional visual codes from visual encoder are directly mapped to
high-dimensional language vocabularies. Leveraging this, we consider two key
operations: token-shuffle, which merges spatially local tokens along channel
dimension to decrease the input token number, and token-unshuffle, which
untangles the inferred tokens after Transformer blocks to restore the spatial
arrangement for output. Jointly training with textual prompts, our strategy
requires no additional pretrained text-encoder and enables MLLMs to support
extremely high-resolution image synthesis in a unified next-token prediction
way while maintaining efficient training and inference. For the first time, we
push the boundary of AR text-to-image generation to a resolution of 2048x2048
with gratifying generation performance. In GenAI-benchmark, our 2.7B model
achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen
by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human
evaluations also demonstrate our prominent image generation ability in terms of
text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle
can serve as a foundational design for efficient high-resolution image
generation within MLLMs.

</details>


### [341] [RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects](https://arxiv.org/abs/2504.18468)
*Georgios Kouros, Minye Wu, Tinne Tuytelaars*

Main category: cs.CV

TL;DR: RGS-DR is a novel inverse rendering method for glossy/reflective objects, excelling in relighting and scene editing by using 2D Gaussian surfels and deferred shading.


<details>
  <summary>Details</summary>
Motivation: Existing methods (e.g., NeRF, 3D Gaussian Splatting) struggle with view-dependent effects, limiting accurate reconstruction and relighting of glossy objects.

Method: Uses 2D Gaussian surfels for geometry/normal estimation, learnable primitives in deferred shading, multi-level cube mipmap for lighting, and a residual pass for appearance refinement.

Result: Achieves high-quality reconstruction and rendering for shiny objects, outperforming state-of-the-art methods in relighting tasks.

Conclusion: RGS-DR effectively addresses limitations of current methods, enabling superior glossy object reconstruction and flexible relighting.

Abstract: We introduce RGS-DR, a novel inverse rendering method for reconstructing and
rendering glossy and reflective objects with support for flexible relighting
and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian
Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D
Gaussian surfel representation to accurately estimate geometry and surface
normals, an essential property for high-quality inverse rendering. Our approach
explicitly models geometric and material properties through learnable
primitives rasterized into a deferred shading pipeline, effectively reducing
rendering artifacts and preserving sharp reflections. By employing a
multi-level cube mipmap, RGS-DR accurately approximates environment lighting
integrals, facilitating high-quality reconstruction and relighting. A residual
pass with spherical-mipmap-based directional encoding further refines the
appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality
reconstruction and rendering quality for shiny objects, often outperforming
reconstruction-exclusive state-of-the-art methods incapable of relighting.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [342] [BELL: Benchmarking the Explainability of Large Language Models](https://arxiv.org/abs/2504.18572)
*Syed Quiser Ahmed, Bharathi Vokkaliga Ganesh, Jagadish Babu P, Karthick Selvaraj, ReddySiva Naga Parvathi Devi, Sravya Kappala*

Main category: cs.AI

TL;DR: A standardized benchmarking technique is introduced to evaluate the explainability of large language models (LLMs) to address transparency, trust, and bias concerns.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency in LLMs' decision-making raises concerns about trust, bias, and performance, necessitating a method to evaluate their interpretability.

Method: The paper proposes a standardized benchmarking technique, Benchmarking the Explainability of Large Language Models, to assess LLM explainability.

Result: The benchmarking technique provides a structured approach to evaluate the explainability of LLMs.

Conclusion: Standardized benchmarking is essential for improving the transparency and trustworthiness of LLMs.

Abstract: Large Language Models have demonstrated remarkable capabilities in natural
language processing, yet their decision-making processes often lack
transparency. This opaqueness raises significant concerns regarding trust,
bias, and model performance. To address these issues, understanding and
evaluating the interpretability of LLMs is crucial. This paper introduces a
standardised benchmarking technique, Benchmarking the Explainability of Large
Language Models, designed to evaluate the explainability of large language
models.

</details>


### [343] [A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study](https://arxiv.org/abs/2504.18604)
*Xingyu Xiao, Peng Chen, Jiejuan Tong, Shunshun Liu, Hongru Zhao, Jun Zhao, Qianqian Jia, Jingang Liang, Haitao Wang*

Main category: cs.AI

TL;DR: The paper introduces COGMIF, a cognitive-mechanistic framework enhancing IDHEAS-ECA by integrating ACT-R-based human digital twins and TimeGAN-augmented simulation for scalable, mechanism-informed human error probability estimation in nuclear power plants.


<details>
  <summary>Details</summary>
Motivation: Traditional HRA methods like IDHEAS-ECA lack cognitive insights and face impracticalities in human-in-the-loop experiments for advanced nuclear plants.

Method: COGMIF combines ACT-R-based human digital twins (simulating cognition) with TimeGAN-augmented synthetic data to enhance IDHEAS-ECA assessments.

Result: Comparative analyses show COGMIF's robustness and practical advantages over SPAR-H, with Bayesian network mapping revealing key operational risk drivers.

Conclusion: COGMIF provides a credible, efficient way to integrate cognitive theory into industrial HRA practices.

Abstract: Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA,
rely on expert judgment and empirical rules that often overlook the cognitive
underpinnings of human error. Moreover, conducting human-in-the-loop
experiments for advanced nuclear power plants is increasingly impractical due
to novel interfaces and limited operational data. This study proposes a
cognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA
methodology by integrating an ACT-R-based human digital twin (HDT) with
TimeGAN-augmented simulation. The ACT-R model simulates operator cognition,
including memory retrieval, goal-directed procedural reasoning, and
perceptual-motor execution, under high-fidelity scenarios derived from a
high-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource
constraints of large-scale cognitive modeling, TimeGAN is trained on
ACT-R-generated time-series data to produce high-fidelity synthetic operator
behavior datasets. These simulations are then used to drive IDHEAS-ECA
assessments, enabling scalable, mechanism-informed estimation of human error
probabilities (HEPs). Comparative analyses with SPAR-H and sensitivity
assessments demonstrate the robustness and practical advantages of the proposed
COGMIF. Finally, procedural features are mapped onto a Bayesian network to
quantify the influence of contributing factors, revealing key drivers of
operational risk. This work offers a credible and computationally efficient
pathway to integrate cognitive theory into industrial HRA practices.

</details>


### [344] [Research on Personalized Medical Intervention Strategy Generation System based on Group Relative Policy Optimization and Time-Series Data Fusion](https://arxiv.org/abs/2504.18631)
*Dingxin Lu, Shurui Wu, Xinyi Huang*

Main category: cs.AI

TL;DR: A system using GRPO and time-series data fusion generates personalized medical interventions, balancing individual and group gains, with improved accuracy and decision-making benefits.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of forming timely, personalized intervention plans from high-dimensional, heterogeneous medical data.

Method: Uses GRPO for policy updates, multi-layer neural networks for patient grouping, multi-channel networks with self-attention for data fusion, and a genetic algorithm with Monte Carlo tree search for strategy optimization.

Result: Significant improvements in accuracy, coverage, and decision-making benefits over existing methods.

Conclusion: The proposed system effectively generates personalized medical interventions, outperforming current approaches.

Abstract: With the timely formation of personalized intervention plans based on
high-dimensional heterogeneous time series information becoming an important
challenge in the medical field today, electronic medical records, wearables,
and other multi-source medical data are increasingly generated and diversified.
In this work, we develop a system to generate personalized medical intervention
strategies based on Group Relative Policy Optimization (GRPO) and Time-Series
Data Fusion. First, by incorporating relative policy constraints among the
groups during policy gradient updates, we adaptively balance individual and
group gains. To improve the robustness and interpretability of decision-making,
a multi-layer neural network structure is employed to group-code patient
characteristics. Second, for the rapid multi-modal fusion of multi-source
heterogeneous time series, a multi-channel neural network combined with a
self-attention mechanism is used for dynamic feature extraction. Key feature
screening and aggregation are achieved through a differentiable gating network.
Finally, a collaborative search process combining a genetic algorithm and Monte
Carlo tree search is proposed to find the ideal intervention strategy,
achieving global optimization. Experimental results show significant
improvements in accuracy, coverage, and decision-making benefits compared with
existing methods.

</details>


### [345] [Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development](https://arxiv.org/abs/2504.18651)
*Filipi Miranda Soares, Antonio Mauro Saraiva, Luís Ferreira Pires, Luiz Olavo Bonino da Silva Santos, Dilvan de Abreu Moreira, Fernando Elias Corrêa, Kelly Rosa Braghetto, Debora Pignatari Drucker, Alexandre Cláudio Botazzo Delbem*

Main category: cs.AI

TL;DR: The paper explores using ChatGPT-4 to automate species name management in ontologies, comparing two methods: direct prompting and Python algorithm design, with mixed results.


<details>
  <summary>Details</summary>
Motivation: The challenge of manually maintaining evolving species taxonomies in ontologies like APTO motivates the use of automation via ChatGPT-4.

Method: Two approaches were tested: (1) direct prompting via BrowserOP and (2) designing a Python algorithm for data extraction and OWL file generation.

Result: The first approach faced scalability issues, while the second struggled with data errors but showed potential for automation.

Conclusion: ChatGPT-4 can aid ontology development despite limitations, offering efficiency improvements in taxonomy tasks.

Abstract: Managing scientific names in ontologies that represent species taxonomies is
challenging due to the ever-evolving nature of these taxonomies. Manually
maintaining these names becomes increasingly difficult when dealing with
thousands of scientific names. To address this issue, this paper investigates
the use of ChatGPT-4 to automate the development of the :Organism module in the
Agricultural Product Types Ontology (APTO) for species classification. Our
methodology involved leveraging ChatGPT-4 to extract data from the GBIF
Backbone API and generate OWL files for further integration in APTO. Two
alternative approaches were explored: (1) issuing a series of prompts for
ChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4
to design a Python algorithm to perform analogous tasks. Both approaches rely
on a prompting method where we provide instructions, context, input data, and
an output indicator. The first approach showed scalability limitations, while
the second approach used the Python algorithm to overcome these challenges, but
it struggled with typographical errors in data handling. This study highlights
the potential of Large language models like ChatGPT-4 to streamline the
management of species names in ontologies. Despite certain limitations, these
tools offer promising advancements in automating taxonomy-related tasks and
improving the efficiency of ontology development.

</details>


### [346] [Can AI Agents Design and Implement Drug Discovery Pipelines?](https://arxiv.org/abs/2504.19912)
*Khachik Smbatyan, Tsolak Ghukasyan, Tigran Aghajanyan, Hovhannes Dabaghyan, Sergey Adamyan, Aram Bughdaryan, Vahagn Altunyan, Gagik Navasardyan, Aram Davtyan, Anush Hakobyan, Aram Gharibyan, Arman Fahradyan, Artur Hakobyan, Hasmik Mnatsakanyan, Narek Ginoyan, Garik Petrosyan*

Main category: cs.AI

TL;DR: The paper introduces the DO Challenge benchmark to evaluate AI agents' decision-making in drug discovery, highlighting the potential and limitations of AI-driven methods.


<details>
  <summary>Details</summary>
Motivation: To accelerate drug discovery by leveraging AI agents' capabilities in solving complex problems like pharmaceutical design.

Method: The DO Challenge benchmark tests AI agents in virtual screening scenarios, requiring them to develop strategies for identifying molecular structures. The Deep Thought multi-agent system was also evaluated.

Result: The Deep Thought system outperformed most human teams, with Claude 3.7 Sonnet, Gemini 2.5 Pro, and o3 excelling in primary roles, and GPT-4o and Gemini 2.0 Flash in auxiliary roles. However, AI performance still lagged behind expert solutions and showed instability.

Conclusion: AI-driven methods show promise in drug discovery but face limitations in stability and performance compared to expert-designed solutions.

Abstract: The rapid advancement of artificial intelligence, particularly autonomous
agentic systems based on Large Language Models (LLMs), presents new
opportunities to accelerate drug discovery by improving in-silico modeling and
reducing dependence on costly experimental trials. Current AI agent-based
systems demonstrate proficiency in solving programming challenges and
conducting research, indicating an emerging potential to develop software
capable of addressing complex problems such as pharmaceutical design and drug
discovery. This paper introduces DO Challenge, a benchmark designed to evaluate
the decision-making abilities of AI agents in a single, complex problem
resembling virtual screening scenarios. The benchmark challenges systems to
independently develop, implement, and execute efficient strategies for
identifying promising molecular structures from extensive datasets, while
navigating chemical space, selecting models, and managing limited resources in
a multi-objective context. We also discuss insights from the DO Challenge 2025,
a competition based on the proposed benchmark, which showcased diverse
strategies explored by human participants. Furthermore, we present the Deep
Thought multi-agent system, which demonstrated strong performance on the
benchmark, outperforming most human teams. Among the language models tested,
Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,
and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While
promising, the system's performance still fell short of expert-designed
solutions and showed high instability, highlighting both the potential and
current limitations of AI-driven methodologies in transforming drug discovery
and broader scientific research.

</details>


### [347] [Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction](https://arxiv.org/abs/2504.18671)
*Ross Gore, Eranga Bandara, Sachin Shetty, Alberto E. Musto, Pratip Rana, Ambrosio Valencia-Romero, Christopher Rhea, Lobat Tayebi, Heather Richter, Atmaram Yarlagadda, Donna Edmonds, Steven Wallace, Donna Broshek*

Main category: cs.AI

TL;DR: Proof-of-TBI integrates fine-tuned vision-language models with OpenAI-o3 reasoning LLM for accurate mild TBI diagnosis.


<details>
  <summary>Details</summary>
Motivation: Mild TBI detection is challenging due to subtle symptoms in medical imaging, requiring advanced diagnostic tools.

Method: Fine-tunes vision-language models on TBI MRI scans, aggregates predictions via consensus, and uses OpenAI-o3 LLM for final diagnosis.

Result: The system shows high accuracy and reliability in diagnosing mild TBI, validated by collaboration with U.S. Army Medical Research.

Conclusion: This approach is the first to combine vision-language models with reasoning LLM for TBI prediction, offering robust and automated diagnosis.

Abstract: Mild Traumatic Brain Injury (TBI) detection presents significant challenges
due to the subtle and often ambiguous presentation of symptoms in medical
imaging, making accurate diagnosis a complex task. To address these challenges,
we propose Proof-of-TBI, a medical diagnosis support system that integrates
multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large
language model (LLM). Our approach fine-tunes multiple vision-language models
using a labeled dataset of TBI MRI scans, training them to diagnose TBI
symptoms effectively. The predictions from these models are aggregated through
a consensus-based decision-making process. The system evaluates the predictions
from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a
model that has demonstrated remarkable reasoning performance, to produce the
most accurate final diagnosis. The LLM Agents orchestrates interactions between
the vision-language models and the reasoning LLM, managing the final
decision-making process with transparency, reliability, and automation. This
end-to-end decision-making workflow combines the vision-language model
consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt
engineering by the LLM agents. The prototype for the proposed platform was
developed in collaboration with the U.S. Army Medical Research team in Newport
News, Virginia, incorporating five fine-tuned vision-language models. The
results demonstrate the transformative potential of combining fine-tuned
vision-language model inputs with the OpenAI-o3 reasoning LLM to create a
robust, secure, and highly accurate diagnostic system for mild TBI prediction.
To the best of our knowledge, this research represents the first application of
fine-tuned vision-language models integrated with a reasoning LLM for TBI
prediction tasks.

</details>


### [348] [A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information](https://arxiv.org/abs/2412.16874)
*Anuprabha M, Krishna Gurugubelli, V Kesavaraj, Anil Kumar Vuppala*

Main category: cs.AI

TL;DR: The paper introduces a novel method for dysarthria detection and severity assessment by combining speech and text modalities using cross-attention, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses mainly on speech, but integrating text can improve dysarthric detection and severity assessment.

Method: Uses cross-attention to learn acoustic and linguistic similarities between speech and text, assessing pronunciation deviations.

Result: Achieved 99.53% and 93.20% accuracy in detection, and 98.12% and 51.97% in severity assessment.

Conclusion: Combining text with speech enhances dysarthric detection and assessment, improving diagnostic effectiveness.

Abstract: Automatic detection and severity assessment of dysarthria are crucial for
delivering targeted therapeutic interventions to patients. While most existing
research focuses primarily on speech modality, this study introduces a novel
approach that leverages both speech and text modalities. By employing
cross-attention mechanism, our method learns the acoustic and linguistic
similarities between speech and text representations. This approach assesses
specifically the pronunciation deviations across different severity levels,
thereby enhancing the accuracy of dysarthric detection and severity assessment.
All the experiments have been performed using UA-Speech dysarthric database.
Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97%
for severity assessment have been achieved when speaker-dependent and
speaker-independent, unseen and seen words settings are used. These findings
suggest that by integrating text information, which provides a reference
linguistic knowledge, a more robust framework has been developed for dysarthric
detection and assessment, thereby potentially leading to more effective
diagnoses.

</details>


### [349] [Transformational Creativity in Science: A Graphical Theory](https://arxiv.org/abs/2504.18687)
*Samuel Schapiro, Jonah Black, Lav R. Varshney*

Main category: cs.AI

TL;DR: A graphical theory of transformational scientific creativity is proposed, linking Boden's enabling constraints and Kuhn's paradigm shifts, showing axiom modifications have the most transformative potential.


<details>
  <summary>Details</summary>
Motivation: To synthesize Boden's and Kuhn's theories into a graphical model for understanding transformational creativity in science.

Method: Develop a graphical model to represent transformational creativity, focusing on axiom modifications.

Result: Proves axiom changes have the highest transformative potential and applies the model to historical cases.

Conclusion: The graphical framework effectively captures transformational creativity, aligning with historical examples.

Abstract: Creative processes are typically divided into three types: combinatorial,
exploratory, and transformational. Here, we provide a graphical theory of
transformational scientific creativity, synthesizing Boden's insight that
transformational creativity arises from changes in the "enabling constraints"
of a conceptual space and Kuhn's structure of scientific revolutions as
resulting from paradigm shifts. We prove that modifications made to axioms of
our graphical model have the most transformative potential and then illustrate
how several historical instances of transformational creativity can be captured
by our framework.

</details>


### [350] [A Vision for Auto Research with LLM Agents](https://arxiv.org/abs/2504.18765)
*Chengwei Liu, Chong Wang, Jiayue Cao, Jingquan Ge, Kun Wang, Lvye Zhang, Ming-Ming Cheng, Penghai Zhao, Tianlin Li, Xiaojun Jia, Xiang Li, Xinfeng Li, Yang Liu, Yebo Feng, Yihao Huang, Yijia Xu, Yuqiang Sun, Zhenhong Zhou, Zhengzi Xu*

Main category: cs.AI

TL;DR: Agent-Based Auto Research automates scientific research using multi-agent LLMs, covering all research phases and addressing workflow fragmentation.


<details>
  <summary>Details</summary>
Motivation: To streamline and optimize scientific research by overcoming fragmented workflows and cognitive overload.

Method: A multi-agent framework leveraging LLMs for collaboration across research phases like literature review and experimentation.

Result: Preliminary results show feasibility and potential for AI-driven, self-improving research processes.

Conclusion: The framework presents a scalable, systematic approach to AI-augmented scientific inquiry.

Abstract: This paper introduces Agent-Based Auto Research, a structured multi-agent
framework designed to automate, coordinate, and optimize the full lifecycle of
scientific research. Leveraging the capabilities of large language models
(LLMs) and modular agent collaboration, the system spans all major research
phases, including literature review, ideation, methodology planning,
experimentation, paper writing, peer review response, and dissemination. By
addressing issues such as fragmented workflows, uneven methodological
expertise, and cognitive overload, the framework offers a systematic and
scalable approach to scientific inquiry. Preliminary explorations demonstrate
the feasibility and potential of Auto Research as a promising paradigm for
self-improving, AI-driven research processes.

</details>


### [351] [Evaluating AI-Driven Automated Map Digitization in QGIS](https://arxiv.org/abs/2504.18777)
*Diana Febrita*

Main category: cs.AI

TL;DR: The paper evaluates Deepness, an AI tool for automated map digitization, comparing its results with OpenStreetMap outputs.


<details>
  <summary>Details</summary>
Motivation: To reduce human involvement in map digitization by leveraging AI, specifically Deepness, for efficiency and accuracy.

Method: Uses Deepness in QGIS to digitize Google Earth imagery and compares results with OpenStreetMap data.

Result: Performance of Deepness is assessed against OSM to determine effectiveness in automated digitization.

Conclusion: The study highlights the potential of AI-driven tools like Deepness for improving map digitization processes.

Abstract: Map digitization is an important process that converts maps into digital
formats that can be used for further analysis. This process typically requires
a deep human involvement because of the need for interpretation and
decision-making when translating complex features. With the advancement of
artificial intelligence, there is an alternative to conducting map digitization
with the help of machine learning techniques. Deepness, or Deep Neural Remote
Sensing, is an advanced AI-driven tool designed and integrated as a plugin in
QGIS application. This research focuses on assessing the effectiveness of
Deepness in automated digitization. This study analyses AI-generated
digitization results from Google Earth imagery and compares them with digitized
outputs from OpenStreetMap (OSM) to evaluate performance.

</details>


### [352] [Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots](https://arxiv.org/abs/2504.18794)
*Brendon Johnson, Alfredo Weitzenfeld*

Main category: cs.AI

TL;DR: HRL outperforms standard RL in complex navigation tasks by leveraging sub-goals and termination functions.


<details>
  <summary>Details</summary>
Motivation: To evaluate HRL's advantages over traditional RL in sparse-reward robot learning tasks.

Method: Compared PPO and HRL, tested sub-goal creation (manual/automatic), and termination frequency effects.

Result: HRL showed superior performance due to its hierarchical structure and sub-goal mechanisms.

Conclusion: HRL is more effective for complex tasks with sparse rewards, highlighting its hierarchical benefits.

Abstract: Hierarchical reinforcement learning (HRL) is hypothesized to be able to take
advantage of the inherent hierarchy in robot learning tasks with sparse reward
schemes, in contrast to more traditional reinforcement learning algorithms. In
this research, hierarchical reinforcement learning is evaluated and contrasted
with standard reinforcement learning in complex navigation tasks. We evaluate
unique characteristics of HRL, including their ability to create sub-goals and
the termination function. We constructed experiments to test the differences
between PPO and HRL, different ways of creating sub-goals, manual vs automatic
sub-goal creation, and the effects of the frequency of termination on
performance. These experiments highlight the advantages of HRL and how it
achieves these advantages.

</details>


### [353] [Generative to Agentic AI: Survey, Conceptualization, and Challenges](https://arxiv.org/abs/2504.18875)
*Johannes Schneider*

Main category: cs.AI

TL;DR: The paper differentiates Agentic AI from Generative AI (GenAI), highlighting its advanced reasoning and autonomy. It surveys their key differences, Agentic AI's advantages, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To clarify the distinction between Agentic AI and GenAI, addressing gaps in understanding and exploring Agentic AI's potential and risks.

Method: A two-part survey: first comparing GenAI and Agentic AI, then delving into Agentic AI's novel aspects and challenges.

Result: Agentic AI offers stronger reasoning and autonomy, enabling new applications beyond GenAI, but raises concerns about surpassing human intelligence.

Conclusion: Agentic AI represents a significant evolution of AI, with promising applications and challenges that warrant further research and caution.

Abstract: Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It
constitutes the next major step in the evolution of AI with much stronger
reasoning and interaction capabilities that enable more autonomous behavior to
tackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI
has seen widespread adoption, giving users firsthand experience. However, the
distinction between Agentic AI and GenAI remains less well understood. To
address this gap, our survey is structured in two parts. In the first part, we
compare GenAI and Agentic AI using existing literature, discussing their key
characteristics, how Agentic AI remedies limitations of GenAI, and the major
steps in GenAI's evolution toward Agentic AI. This section is intended for a
broad audience, including academics in both social sciences and engineering, as
well as industry professionals. It provides the necessary insights to
comprehend novel applications that are possible with Agentic AI but not with
GenAI. In the second part, we deep dive into novel aspects of Agentic AI,
including recent developments and practical concerns such as defining agents.
Finally, we discuss several challenges that could serve as a future research
agenda, while cautioning against risks that can emerge when exceeding human
intelligence.

</details>


### [354] [Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents](https://arxiv.org/abs/2504.18880)
*Zuhong Lin, Daoyuan Ren, Kai Ran, Sun Jing, Xiaotiang Huang, Haiyang He, Pengxu Pan, Xiaohang Zhang, Ying Fang, Tianying Wang, Minli Wu, Zhanglin Li, Xiaochuan Zhang, Haipu Li, Jingjing Yao*

Main category: cs.AI

TL;DR: MOFh6, an LLM-based tool, streamlines MOF synthesis by integrating various agents to provide optimal conditions and pre-modeling files.


<details>
  <summary>Details</summary>
Motivation: Identifying precise MOF synthesis conditions is challenging due to the vast possibilities; LLMs offer a solution.

Method: Leveraged gpt-4o-mini to integrate MOF-related agents (synthesis, attribute, chemical info) into MOFh6, enabling multi-format queries.

Result: MOFh6 provides optimal synthesis conditions and generates pre-modeling files, enhancing research efficiency.

Conclusion: MOFh6 improves MOF synthesis efficiency for researchers by leveraging LLM capabilities.

Abstract: The mining of synthesis conditions for metal-organic frameworks (MOFs) is a
significant focus in materials science. However, identifying the precise
synthesis conditions for specific MOFs within the vast array of possibilities
presents a considerable challenge. Large Language Models (LLMs) offer a
promising solution to this problem. We leveraged the capabilities of LLMs,
specifically gpt-4o-mini, as core agents to integrate various MOF-related
agents, including synthesis, attribute, and chemical information agents. This
integration culminated in the development of MOFh6, an LLM tool designed to
streamline the MOF synthesis process. MOFh6 allows users to query in multiple
formats, such as submitting scientific literature, or inquiring about specific
MOF codes or structural properties. The tool analyzes these queries to provide
optimal synthesis conditions and generates model files for density functional
theory pre modeling. We believe MOFh6 will enhance efficiency in the MOF
synthesis of all researchers.

</details>


### [355] [Use of Metric Learning for the Recognition of Handwritten Digits, and its Application to Increase the Outreach of Voice-based Communication Platforms](https://arxiv.org/abs/2504.18948)
*Devesh Pant, Dibyendu Talukder, Deepak Kumar, Rachit Pandey, Aaditeshwar Seth, Chetan Arora*

Main category: cs.AI

TL;DR: The paper addresses challenges in digital data collection for development programs, proposing paper-based methods with OCR/OMR automation. It introduces a dataset, deep learning models, and tools for digitizing handwritten forms, demonstrated in a maternal health project in India.


<details>
  <summary>Details</summary>
Motivation: Digital data collection is often unfeasible due to cost and training limitations. Paper-based methods with automated digitization offer a viable alternative.

Method: Developed a large dataset of handwritten digits and deep learning models for OCR/OMR. Deployed tools in a maternal health project using IVR systems.

Result: Successfully digitized paper forms to push 4 million phone calls in the project. Data, models, and code were released open-source.

Conclusion: Paper-based data collection with automated digitization is effective in resource-limited settings, as demonstrated in the maternal health project.

Abstract: Initiation, monitoring, and evaluation of development programmes can involve
field-based data collection about project activities. This data collection
through digital devices may not always be feasible though, for reasons such as
unaffordability of smartphones and tablets by field-based cadre, or shortfalls
in their training and capacity building. Paper-based data collection has been
argued to be more appropriate in several contexts, with automated digitization
of the paper forms through OCR (Optical Character Recognition) and OMR (Optical
Mark Recognition) techniques. We contribute with providing a large dataset of
handwritten digits, and deep learning based models and methods built using this
data, that are effective in real-world environments. We demonstrate the
deployment of these tools in the context of a maternal and child health and
nutrition awareness project, which uses IVR (Interactive Voice Response)
systems to provide awareness information to rural women SHG (Self Help Group)
members in north India. Paper forms were used to collect phone numbers of the
SHG members at scale, which were digitized using the OCR tools developed by us,
and used to push almost 4 million phone calls. The data, model, and code have
been released in the open-source domain.

</details>


### [356] [Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles](https://arxiv.org/abs/2504.19017)
*Alireza Ghafarollahi, Markus J. Buehler*

Main category: cs.AI

TL;DR: Sparks, a multi-modal multi-agent AI model, autonomously conducts scientific discovery, uncovering new protein science phenomena without human intervention.


<details>
  <summary>Details</summary>
Motivation: To advance AI beyond resurfacing latent knowledge in training data by enabling autonomous discovery cycles.

Method: Uses generative sequence design, structure prediction, and physics-aware models with paired generation-and-reflection agents for self-correction.

Result: Discovered two new protein phenomena: a length-dependent mechanical crossover and a stability map for secondary structures.

Conclusion: Sparks demonstrates the ability to independently conduct rigorous scientific inquiry and identify unknown principles.

Abstract: Advances in artificial intelligence (AI) promise autonomous discovery, yet
most systems still resurface knowledge latent in their training data. We
present Sparks, a multi-modal multi-agent AI model that executes the entire
discovery cycle that includes hypothesis generation, experiment design and
iterative refinement to develop generalizable principles and a report without
human intervention. Applied to protein science, Sparks uncovered two previously
unknown phenomena: (i) a length-dependent mechanical crossover whereby
beta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond
~80 residues, establishing a new design principle for peptide mechanics; and
(ii) a chain-length/secondary-structure stability map revealing unexpectedly
robust beta-sheet-rich architectures and a "frustration zone" of high variance
in mixed alpha/beta folds. These findings emerged from fully self-directed
reasoning cycles that combined generative sequence design, high-accuracy
structure prediction and physics-aware property models, with paired
generation-and-reflection agents enforcing self-correction and reproducibility.
The key result is that Sparks can independently conduct rigorous scientific
inquiry and identify previously unknown scientific principles.

</details>


### [357] [GLaMoR: Consistency Checking of OWL Ontologies using Graph Language Models](https://arxiv.org/abs/2504.19023)
*Justin Mücke, Ansgar Scherp*

Main category: cs.AI

TL;DR: GLaMoR, a Graph Language Model for Reasoning, improves ontology consistency checking by transforming OWL ontologies into graph-structured data, achieving 95% accuracy and 20x speedup over classical reasoners.


<details>
  <summary>Details</summary>
Motivation: The challenge of verifying ontology consistency efficiently, especially as ontology sizes grow, motivates the need for a scalable solution beyond traditional reasoners and classical machine learning models.

Method: The paper proposes GLaMoR, which transforms OWL ontologies into graph-structured data and adapts the Graph Language Model (GLM) architecture for consistency checking.

Result: GLaMoR achieves 95% accuracy and is 20 times faster than classical reasoners, as demonstrated on NCBO BioPortal ontologies.

Conclusion: GLaMoR offers a promising, efficient alternative for ontology consistency checking, leveraging graph-structured data and language models.

Abstract: Semantic reasoning aims to infer new knowledge from existing knowledge, with
OWL ontologies serving as a standardized framework for organizing information.
A key challenge in semantic reasoning is verifying ontology consistency.
However, state-of-the-art reasoners are computationally expensive, and their
efficiency decreases as ontology sizes grow. While classical machine learning
models have been explored for consistency checking, they struggle to capture
complex relationships within ontologies. Large language models (LLMs) have
shown promising results for simple reasoning tasks but perform poorly on
structured reasoning. The recently introduced Graph Language Model (GLM) offers
a way to simultaneously process graph-structured data and text. This paper
proposes GLaMoR (Graph Language Model for Reasoning), a reasoning pipeline that
transforms OWL ontologies into graph-structured data and adapts the GLM
architecture for consistency checking. We evaluate GLaMoR on ontologies from
the NCBO BioPortal repository, converting them into triples suitable for model
input. Our results show that the GLM outperforms all baseline models, achieving
$95\%$ accuracy while being 20 times faster than classical reasoners.
  The Code is accessible under: https://github.com/JustinMuecke/GLaMoR

</details>


### [358] [DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning](https://arxiv.org/abs/2504.19027)
*Volkan Bakir, Polat Goktas, Sureyya Akyuz*

Main category: cs.AI

TL;DR: DiCE-Extended improves counterfactual (CF) explanations by balancing proximity, diversity, and robustness using multi-objective optimization and a novel robustness metric.


<details>
  <summary>Details</summary>
Motivation: Existing CF methods like DiCE lack robustness, limiting real-world applicability in decision-critical domains.

Method: DiCE-Extended integrates multi-objective optimization, introduces a Dice-Sorensen-based robustness metric, and refines CF generation with weighted loss components.

Result: Empirical validation shows improved CF validity, stability, and alignment with decision boundaries compared to standard DiCE.

Conclusion: DiCE-Extended enhances reliability and interpretability of CFs, with future work focusing on adaptive optimization and domain-specific constraints.

Abstract: Explainable artificial intelligence (XAI) has become increasingly important
in decision-critical domains such as healthcare, finance, and law.
Counterfactual (CF) explanations, a key approach in XAI, provide users with
actionable insights by suggesting minimal modifications to input features that
lead to different model outcomes. Despite significant advancements, existing CF
generation methods often struggle to balance proximity, diversity, and
robustness, limiting their real-world applicability. A widely adopted
framework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but
lacks robustness, making CF explanations sensitive to perturbations and domain
constraints. To address these challenges, we introduce DiCE-Extended, an
enhanced CF explanation framework that integrates multi-objective optimization
techniques to improve robustness while maintaining interpretability. Our
approach introduces a novel robustness metric based on the Dice-Sorensen
coefficient, ensuring stability under small input variations. Additionally, we
refine CF generation using weighted loss components (lambda_p, lambda_d,
lambda_r) to balance proximity, diversity, and robustness. We empirically
validate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German
Credit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch,
TensorFlow). Results demonstrate improved CF validity, stability, and alignment
with decision boundaries compared to standard DiCE-generated explanations. Our
findings highlight the potential of DiCE-Extended in generating more reliable
and interpretable CFs for high-stakes applications. Future work will explore
adaptive optimization techniques and domain-specific constraints to further
enhance CF generation in real-world scenarios.

</details>


### [359] [ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development](https://arxiv.org/abs/2504.19144)
*Bowei Wang, Jiaran Gao, Yelai Feng, Renzhi Chen, Shanshan Li, Lei Wang*

Main category: cs.AI

TL;DR: ChiseLLM improves Chisel code generation by enhancing syntax correctness and design variability through domain adaptation and prompt-guided reasoning.


<details>
  <summary>Details</summary>
Motivation: The need for better Chisel code generation in Agile Hardware Development Methodology (AHDM) due to limitations of LLMs in syntax correctness and variability.

Method: ChiseLLM combines data processing, prompt-guided reasoning trace synthesis, and domain-adapted model training using high-quality datasets from RTL code.

Result: ChiseLLM-7B and ChiseLLM-32B improved syntax correctness by 18.85% and 26.32%, and design variability by 47.58% over baselines.

Conclusion: ChiseLLM provides a high-performance, cost-effective solution for HCL-Based AHDM and sets a benchmark for future research.

Abstract: The growing demand for Domain-Specific Architecture (DSA) has driven the
development of Agile Hardware Development Methodology (AHDM). Hardware
Construction Language (HCL) like Chisel offers high-level abstraction features,
making it an ideal language for HCL-Based AHDM. While Large Language Models
(LLMs) excel in code generation tasks, they still face challenges with Chisel
generation, particularly regarding syntax correctness and design variability.
Recent reasoning models have significantly enhanced code generation
capabilities through test-time scaling techniques. However, we found that
reasoning models without domain adaptation cannot bring substantial benefits to
Chisel code generation tasks. This paper presents ChiseLLM, a solution
comprising data processing and transformation, prompt-guided reasoning trace
synthesis, and domain-adapted model training. We constructed high-quality
datasets from public RTL code resources and guided the model to adopt
structured thinking patterns through prompt enhancement methods. Experiments
demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax
correctness by 18.85% and 26.32% respectively over base models, while
increasing variability design ability by 47.58% compared to baseline reasoning
models. Our datasets and models are publicly available, providing
high-performance, cost-effective models for HCL-Based AHDM, and offering an
effective baseline for future research. Github repository:
https://github.com/observerw/ChiseLLM

</details>


### [360] [A Dynamic Fuzzy Rule and Attribute Management Framework for Fuzzy Inference Systems in High-Dimensional Data](https://arxiv.org/abs/2504.19148)
*Ke Liu, Jing Ma, Edmund M-K Lai*

Main category: cs.AI

TL;DR: ADAR framework improves neuro-fuzzy systems by dynamically weighting attributes and rules, achieving lower RMSE than baselines while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Address challenges of high-dimensional data in neuro-fuzzy systems, balancing performance and interpretability.

Method: Integrates dual weighting mechanisms (attributes and rules) with automated growth/pruning strategies.

Result: Consistently lower RMSE on diverse datasets; outperforms ANFIS, SOFENN, and APLR.

Conclusion: ADAR effectively balances complexity and feature importance, enabling scalable, accurate, and transparent neuro-fuzzy systems.

Abstract: This paper presents an Adaptive Dynamic Attribute and Rule (ADAR) framework
designed to address the challenges posed by high-dimensional data in
neuro-fuzzy inference systems. By integrating dual weighting
mechanisms-assigning adaptive importance to both attributes and rules-together
with automated growth and pruning strategies, ADAR adaptively streamlines
complex fuzzy models without sacrificing performance or interpretability.
Experimental evaluations on four diverse datasets - Auto MPG (7 variables),
Beijing PM2.5 (10 variables), Boston Housing (13 variables), and Appliances
Energy Consumption (27 variables) show that ADAR-based models achieve
consistently lower Root Mean Square Error (RMSE) compared to state-of-the-art
baselines. On the Beijing PM2.5 dataset, for instance, ADAR-SOFENN attained an
RMSE of 56.87 with nine rules, surpassing traditional ANFIS [12] and SOFENN
[16] models. Similarly, on the high-dimensional Appliances Energy dataset,
ADAR-ANFIS reached an RMSE of 83.25 with nine rules, outperforming established
fuzzy logic approaches and interpretability-focused methods such as APLR.
Ablation studies further reveal that combining rule-level and attribute-level
weight assignment significantly reduces model overlap while preserving
essential features, thereby enhancing explainability. These results highlight
ADAR's effectiveness in dynamically balancing rule complexity and feature
importance, paving the way for scalable, high-accuracy, and transparent
neuro-fuzzy systems applicable to a range of real-world scenarios.

</details>


### [361] [A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption](https://arxiv.org/abs/2504.19179)
*Pedro A. Moreno-Sánchez, Javier Del Ser, Mark van Gils, Jussi Hernesniemi*

Main category: cs.AI

TL;DR: The paper proposes a design framework to integrate Trustworthy AI (TAI) principles into medical AI systems, addressing ethical, regulatory, and trust challenges in healthcare.


<details>
  <summary>Details</summary>
Motivation: Despite AI's potential in healthcare, clinical adoption is hindered by non-technical challenges like ethics, regulation, and trust. TAI principles offer a solution but are hard to implement.

Method: A disease-agnostic framework is proposed to embed TAI principles into medical AI systems, with stakeholder-specific requirements and an examination of practical challenges.

Result: The framework is demonstrated using cardiovascular diseases, highlighting both successful TAI applications and persistent obstacles.

Conclusion: The paper bridges TAI theory and practice, offering a practical approach to enhance trust and adoption of AI in healthcare.

Abstract: Artificial Intelligence (AI) holds great promise for transforming healthcare,
particularly in disease diagnosis, prognosis, and patient care. The increasing
availability of digital medical data, such as images, omics, biosignals, and
electronic health records, combined with advances in computing, has enabled AI
models to approach expert-level performance. However, widespread clinical
adoption remains limited, primarily due to challenges beyond technical
performance, including ethical concerns, regulatory barriers, and lack of
trust. To address these issues, AI systems must align with the principles of
Trustworthy AI (TAI), which emphasize human agency and oversight, algorithmic
robustness, privacy and data governance, transparency, bias and discrimination
avoidance, and accountability. Yet, the complexity of healthcare processes
(e.g., screening, diagnosis, prognosis, and treatment) and the diversity of
stakeholders (clinicians, patients, providers, regulators) complicate the
integration of TAI principles. To bridge the gap between TAI theory and
practical implementation, this paper proposes a design framework to support
developers in embedding TAI principles into medical AI systems. Thus, for each
stakeholder identified across various healthcare processes, we propose a
disease-agnostic collection of requirements that medical AI systems should
incorporate to adhere to the principles of TAI. Additionally, we examine the
challenges and tradeoffs that may arise when applying these principles in
practice. To ground the discussion, we focus on cardiovascular diseases, a
field marked by both high prevalence and active AI innovation, and demonstrate
how TAI principles have been applied and where key obstacles persist.

</details>


### [362] [The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach](https://arxiv.org/abs/2504.19255)
*Chad Coleman, W. Russell Neuman, Ali Dasdan, Safinah Ali, Manan Shah*

Main category: cs.AI

TL;DR: The paper introduces the PRIME framework to evaluate LLMs' ethical reasoning, revealing alignment with human moral preferences but underweighting certain moral dimensions.


<details>
  <summary>Details</summary>
Motivation: To systematically assess LLMs' ethical reasoning as they are deployed in consequential decision-making contexts.

Method: The PRIME framework analyzes moral priorities using direct questioning and response analysis to ethical dilemmas across six leading LLMs.

Result: LLMs prioritize care/harm and fairness/cheating, underweight authority/loyalty/sanctity, and align with human moral preferences.

Conclusion: The PRIME framework provides scalable ethical benchmarking, revealing both capabilities and limitations in AI moral reasoning.

Abstract: As large language models (LLMs) are increasingly deployed in consequential
decision-making contexts, systematically assessing their ethical reasoning
capabilities becomes a critical imperative. This paper introduces the
Priorities in Reasoning and Intrinsic Moral Evaluation (PRIME) framework--a
comprehensive methodology for analyzing moral priorities across foundational
ethical dimensions including consequentialist-deontological reasoning, moral
foundations theory, and Kohlberg's developmental stages. We apply this
framework to six leading LLMs through a dual-protocol approach combining direct
questioning and response analysis to established ethical dilemmas. Our analysis
reveals striking patterns of convergence: all evaluated models demonstrate
strong prioritization of care/harm and fairness/cheating foundations while
consistently underweighting authority, loyalty, and sanctity dimensions.
Through detailed examination of confidence metrics, response reluctance
patterns, and reasoning consistency, we establish that contemporary LLMs (1)
produce decisive ethical judgments, (2) demonstrate notable cross-model
alignment in moral decision-making, and (3) generally correspond with
empirically established human moral preferences. This research contributes a
scalable, extensible methodology for ethical benchmarking while highlighting
both the promising capabilities and systematic limitations in current AI moral
reasoning architectures--insights critical for responsible development as these
systems assume increasingly significant societal roles.

</details>


### [363] [Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling](https://arxiv.org/abs/2504.19277)
*Ishan Kavathekar, Raghav Donakanti, Ponnurangam Kumaraguru, Karthik Vaidhyanathan*

Main category: cs.AI

TL;DR: The paper explores the use of Small Language Models (SLMs) for function calling, comparing zero-shot, few-shot, and fine-tuning approaches. SLMs show promise but struggle with output format adherence.


<details>
  <summary>Details</summary>
Motivation: Function calling is complex and widely applicable, but Large Language Models (LLMs) are impractical for resource-constrained settings. SLMs offer efficiency and speed, making them suitable for edge devices.

Method: Evaluated SLMs using zero-shot, few-shot, and fine-tuning approaches, with and without prompt injection. Analyzed performance metrics and tested on edge devices for latency and memory usage.

Result: SLMs improve from zero-shot to fine-tuning but struggle with output format. Prompt injection shows robustness with slight performance decline. SLMs show potential but need refinement for real-time use.

Conclusion: SLMs are promising for function calling but require further refinement, especially in adhering to output formats and real-time performance.

Abstract: Function calling is a complex task with widespread applications in domains
such as information retrieval, software engineering and automation. For
example, a query to book the shortest flight from New York to London on January
15 requires identifying the correct parameters to generate accurate function
calls. Large Language Models (LLMs) can automate this process but are
computationally expensive and impractical in resource-constrained settings. In
contrast, Small Language Models (SLMs) can operate efficiently, offering faster
response times, and lower computational demands, making them potential
candidates for function calling on edge devices. In this exploratory empirical
study, we evaluate the efficacy of SLMs in generating function calls across
diverse domains using zero-shot, few-shot, and fine-tuning approaches, both
with and without prompt injection, while also providing the finetuned models to
facilitate future applications. Furthermore, we analyze the model responses
across a range of metrics, capturing various aspects of function call
generation. Additionally, we perform experiments on an edge device to evaluate
their performance in terms of latency and memory usage, providing useful
insights into their practical applicability. Our findings show that while SLMs
improve from zero-shot to few-shot and perform best with fine-tuning, they
struggle significantly with adhering to the given output format. Prompt
injection experiments further indicate that the models are generally robust and
exhibit only a slight decline in performance. While SLMs demonstrate potential
for the function call generation task, our results also highlight areas that
need further refinement for real-time functioning.

</details>


### [364] [Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics](https://arxiv.org/abs/2504.19320)
*Ralph Wojtowicz*

Main category: cs.AI

TL;DR: The paper applies categorical logic to design AI agents for symbolic reasoning about structured objects, using Johnstone's sequent calculus and adapting Horn logic and first-order unification for richer contexts.


<details>
  <summary>Details</summary>
Motivation: To enable symbolic reasoning about objects in semantic categories that don't support classical logic or all its connectives.

Method: Develops forward chaining and normal form algorithms using Johnstone's sequent calculus and adapts first-order unification for multi-sorted theories and contexts.

Result: Provides reformulations for reasoning in non-classical semantic categories.

Conclusion: The approach extends symbolic reasoning capabilities to more complex and less classical structures.

Abstract: This paper seeks to apply categorical logic to the design of artificial
intelligent agents that reason symbolically about objects more richly
structured than sets. Using Johnstone's sequent calculus of terms- and
formulae-in-context, we develop forward chaining and normal form algorithms for
reasoning about objects in cartesian categories with the rules for Horn logic.
We also adapt first-order unification to support multi-sorted theories,
contexts, and fragments of first-order logic. The significance of these
reformulations rests in the fact that they can be applied to reasoning about
objects in semantic categories that do not support classical logic or even all
its connectives.

</details>


### [365] [Neurosymbolic Association Rule Mining from Tabular Data](https://arxiv.org/abs/2504.19354)
*Erkan Karabulut, Paul Groth, Victoria Degeler*

Main category: cs.AI

TL;DR: Aerial+ is a neurosymbolic ARM method that uses an under-complete autoencoder to reduce rule explosion, achieving concise, high-quality rule sets with full data coverage.


<details>
  <summary>Details</summary>
Motivation: High-dimensional datasets in ARM lead to excessive rules, increasing execution time and degrading performance.

Method: Aerial+ employs an under-complete autoencoder to create neural data representations and extracts rules from the model's reconstruction mechanism.

Result: Aerial+ outperforms seven baselines, producing concise, high-quality rule sets with full coverage and reducing execution time in interpretable ML models.

Conclusion: Aerial+ effectively addresses rule explosion in ARM, improving efficiency and accuracy in downstream tasks.

Abstract: Association Rule Mining (ARM) is the task of mining patterns among data
features in the form of logical rules, with applications across a myriad of
domains. However, high-dimensional datasets often result in an excessive number
of rules, increasing execution time and negatively impacting downstream task
performance. Managing this rule explosion remains a central challenge in ARM
research. To address this, we introduce Aerial+, a novel neurosymbolic ARM
method. Aerial+ leverages an under-complete autoencoder to create a neural
representation of the data, capturing associations between features. It
extracts rules from this neural representation by exploiting the model's
reconstruction mechanism. Extensive evaluations on five datasets against seven
baselines demonstrate that Aerial+ achieves state-of-the-art results by
learning more concise, high-quality rule sets with full data coverage. When
integrated into rule-based interpretable machine learning models, Aerial+
significantly reduces execution time while maintaining or improving accuracy.

</details>


### [366] [Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks](https://arxiv.org/abs/2504.19499)
*Omid Semiari, Hosein Nikopour, Shilpa Talwar*

Main category: cs.AI

TL;DR: A novel QoS-aware Load Balancing (LB) approach using Graph Reinforcement Learning (GRL) is proposed for multi-band O-RAN, reducing QoS violations by 53% and improving BE traffic performance.


<details>
  <summary>Details</summary>
Motivation: Next-gen wireless networks require strict QoS guarantees, but cell congestion poses a challenge. Balancing load while ensuring QoS for GBR and BE traffic is critical.

Method: The LB problem is modeled as a Markov Decision Process with graph-based states. A GRL framework (combining GNN and RL) is used, with an off-policy dueling DQN for training.

Result: The GRL-based solution reduces QoS violations by 53% and increases the 5th percentile rate for BE traffic fourfold compared to baselines.

Conclusion: The proposed GRL approach effectively optimizes QoS-aware LB in O-RAN, demonstrating significant performance improvements over traditional methods.

Abstract: Next-generation wireless cellular networks are expected to provide
unparalleled Quality-of-Service (QoS) for emerging wireless applications,
necessitating strict performance guarantees, e.g., in terms of link-level data
rates. A critical challenge in meeting these QoS requirements is the prevention
of cell congestion, which involves balancing the load to ensure sufficient
radio resources are available for each cell to serve its designated User
Equipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach
is developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best
Effort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS
and resource constraints. The proposed solution builds on Graph Reinforcement
Learning (GRL), a powerful framework at the intersection of Graph Neural
Network (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process,
with states represented as graphs. QoS consideration are integrated into both
state representations and reward signal design. The LB agent is then trained
using an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based
architecture. This design ensures the LB policy is invariant to the ordering of
nodes (UE or cell), flexible in handling various network sizes, and capable of
accounting for spatial node dependencies in LB decisions. Performance of the
GRL-based solution is compared with two baseline methods. Results show
substantial performance gains, including a $53\%$ reduction in QoS violations
and a fourfold increase in the 5th percentile rate for BE traffic.

</details>


### [367] [GVPO: Group Variance Policy Optimization for Large Language Model Post-Training](https://arxiv.org/abs/2504.19599)
*Kaichen Zhang, Yuzhong Hong, Junwei Bao, Hongfei Jiang, Yang Song, Dingqian Hong, Hui Xiong*

Main category: cs.AI

TL;DR: GVPO introduces a stable post-training method for LLMs by incorporating KL-constrained reward maximization into gradient weights, ensuring optimal policy alignment and flexible sampling.


<details>
  <summary>Details</summary>
Motivation: Addressing training instability in existing post-training techniques like GRPO, which limits practical adoption despite superior performance.

Method: GVPO integrates KL-constrained reward maximization into gradient weights, providing intuitive physical interpretations and flexible sampling distributions.

Result: GVPO guarantees a unique optimal solution, aligns with KL-constrained reward maximization, and avoids limitations of on-policy and importance sampling.

Conclusion: GVPO sets a new standard for reliable and versatile LLM post-training by unifying theoretical guarantees with practical adaptability.

Abstract: Post-training plays a crucial role in refining and aligning large language
models to meet specific tasks and human preferences. While recent advancements
in post-training techniques, such as Group Relative Policy Optimization (GRPO),
leverage increased sampling with relative reward scoring to achieve superior
performance, these methods often suffer from training instability that limits
their practical adoption. To address this challenge, we present Group Variance
Policy Optimization (GVPO). GVPO incorporates the analytical solution to
KL-constrained reward maximization directly into its gradient weights, ensuring
alignment with the optimal policy. The method provides intuitive physical
interpretations: its gradient mirrors the mean squared error between the
central distance of implicit rewards and that of actual rewards. GVPO offers
two key advantages: (1) it guarantees a unique optimal solution, exactly the
KL-constrained reward maximization objective, (2) it supports flexible sampling
distributions that avoids on-policy and importance sampling limitations. By
unifying theoretical guarantees with practical adaptability, GVPO establishes a
new paradigm for reliable and versatile LLM post-training.

</details>


### [368] [From Evidence to Belief: A Bayesian Epistemology Approach to Language Models](https://arxiv.org/abs/2504.19622)
*Minsu Kim, Sangryul Kim, James Thorne*

Main category: cs.AI

TL;DR: Language models' adherence to Bayesian epistemology is inconsistent; they perform well with true evidence but struggle with other types, showing bias and varying confidence-accuracy relationships.


<details>
  <summary>Details</summary>
Motivation: To understand how language models adjust confidence and responses based on evidence quality and reliability, aligning with Bayesian principles.

Method: Created a dataset with diverse evidence types, analyzed models using verbalized confidence, token probability, and sampling.

Result: Models follow Bayesian confirmation with true evidence but fail otherwise, exhibit high confidence without accuracy, and show bias toward golden evidence.

Conclusion: Language models deviate from Bayesian assumptions due to biases and evidence-type dependencies, highlighting limitations in their epistemic behavior.

Abstract: This paper investigates the knowledge of language models from the perspective
of Bayesian epistemology. We explore how language models adjust their
confidence and responses when presented with evidence with varying levels of
informativeness and reliability. To study these properties, we create a dataset
with various types of evidence and analyze language models' responses and
confidence using verbalized confidence, token probability, and sampling. We
observed that language models do not consistently follow Bayesian epistemology:
language models follow the Bayesian confirmation assumption well with true
evidence but fail to adhere to other Bayesian assumptions when encountering
different evidence types. Also, we demonstrated that language models can
exhibit high confidence when given strong evidence, but this does not always
guarantee high accuracy. Our analysis also reveals that language models are
biased toward golden evidence and show varying performance depending on the
degree of irrelevance, helping explain why they deviate from Bayesian
assumptions.

</details>


### [369] [Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search](https://arxiv.org/abs/2504.19636)
*Fei Liu, Qingfu Zhang, Xialiang Tong, Mingxuan Yuan, Kun Mao*

Main category: cs.AI

TL;DR: The paper analyzes the fitness landscape of LLM-assisted Algorithm Search (LAS) using a graph-based approach, revealing multimodal and rugged landscapes with task and LLM-specific variations.


<details>
  <summary>Details</summary>
Motivation: To understand the underexplored fitness landscape of LLM-assisted iterative algorithm search, critical for improving search behavior.

Method: A graph-based approach where nodes represent algorithms and edges denote transitions, evaluated across six tasks and six LLMs.

Result: LAS landscapes are highly multimodal and rugged, with task-specific variations (e.g., dense clusters in heuristic design, sparse distributions in symbolic regression). Population size affects exploration-exploitation trade-offs.

Conclusion: The study advances understanding of LAS landscapes and offers practical guidance for designing more effective LAS methods.

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
algorithm design. However, when integrated into search frameworks for iterative
algorithm search, the underlying fitness landscape--critical for understanding
search behaviou--remains underexplored. In this paper, we illustrate and
analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a
graph-based approach, where nodes represent algorithms and edges denote
transitions between them. We conduct extensive evaluations across six algorithm
design tasks and six commonly used LLMs. Our findings reveal that LAS
landscapes are highly multimodal and rugged, particularly in combinatorial
optimization tasks, with distinct structural variations across tasks and LLMs.
For instance, heuristic design tasks exhibit dense clusters of high-performing
algorithms, while symbolic regression tasks show sparse, scattered
distributions. Additionally, we demonstrate how population size influences
exploration-exploitation trade-offs and the evolving trajectory of elite
algorithms. These insights not only advance our understanding of LAS landscapes
but also provide practical guidance for designing more effective LAS methods.

</details>


### [370] [From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review](https://arxiv.org/abs/2504.19678)
*Mohamed Amine Ferrag, Norbert Tihanyi, Merouane Debbah*

Main category: cs.AI

TL;DR: The paper provides a unified taxonomy and survey of benchmarks and frameworks for evaluating large language models and autonomous AI agents, along with real-world applications and collaboration protocols.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of large language models and autonomous AI agents has led to fragmented evaluation benchmarks and frameworks, necessitating a unified taxonomy and comprehensive survey.

Method: The study conducts a side-by-side comparison of benchmarks (2019-2025) and reviews AI-agent frameworks (2023-2025), proposing a taxonomy of 60 benchmarks and surveying collaboration protocols.

Result: A taxonomy of benchmarks across domains, a review of AI-agent frameworks, real-world applications, and key collaboration protocols (ACP, MCP, A2A) are presented.

Conclusion: Future research should focus on advanced reasoning, multi-agent failure modes, automated discovery, dynamic tool integration, search capabilities, and security vulnerabilities.

Abstract: Large language models and autonomous AI agents have evolved rapidly,
resulting in a diverse array of evaluation benchmarks, frameworks, and
collaboration protocols. However, the landscape remains fragmented and lacks a
unified taxonomy or comprehensive survey. Therefore, we present a side-by-side
comparison of benchmarks developed between 2019 and 2025 that evaluate these
models and agents across multiple domains. In addition, we propose a taxonomy
of approximately 60 benchmarks that cover general and academic knowledge
reasoning, mathematical problem-solving, code generation and software
engineering, factual grounding and retrieval, domain-specific evaluations,
multimodal and embodied tasks, task orchestration, and interactive assessments.
Furthermore, we review AI-agent frameworks introduced between 2023 and 2025
that integrate large language models with modular toolkits to enable autonomous
decision-making and multi-step reasoning. Moreover, we present real-world
applications of autonomous AI agents in materials science, biomedical research,
academic ideation, software engineering, synthetic data generation, chemical
reasoning, mathematical problem-solving, geographic information systems,
multimedia, healthcare, and finance. We then survey key agent-to-agent
collaboration protocols, namely the Agent Communication Protocol (ACP), the
Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally,
we discuss recommendations for future research, focusing on advanced reasoning
strategies, failure modes in multi-agent LLM systems, automated scientific
discovery, dynamic tool integration via reinforcement learning, integrated
search capabilities, and security vulnerabilities in agent protocols.

</details>


### [371] [Learning Efficiency Meets Symmetry Breaking](https://arxiv.org/abs/2504.19738)
*Yingbin Bai, Sylvie Thiebaux, Felipe Trevizan*

Main category: cs.AI

TL;DR: A learning-based planner using Graph Neural Networks introduces symmetry-aware pruning methods (action and state pruning) to improve search efficiency, outperforming LAMA on IPC datasets.


<details>
  <summary>Details</summary>
Motivation: The potential of learning-based planners to handle symmetries in large search spaces is underexplored.

Method: Proposes a graph representation for planning problems and two pruning methods (action and state pruning) to manage symmetries during search.

Result: Integration into Fast Downward achieves superior performance over LAMA on the IPC learning track dataset.

Conclusion: The approach effectively combines learning efficiency with symmetry detection, demonstrating practical success.

Abstract: Learning-based planners leveraging Graph Neural Networks can learn search
guidance applicable to large search spaces, yet their potential to address
symmetries remains largely unexplored. In this paper, we introduce a graph
representation of planning problems allying learning efficiency with the
ability to detect symmetries, along with two pruning methods, action pruning
and state pruning, designed to manage symmetries during search. The integration
of these techniques into Fast Downward achieves a first-time success over LAMA
on the latest IPC learning track dataset. Code is released at:
https://github.com/bybeye/Distincter.

</details>


### [372] [Automated decision-making for dynamic task assignment at scale](https://arxiv.org/abs/2504.19933)
*Riccardo Lo Bianco, Willem van Jaarsveld, Jeroen Middelhuis, Luca Begnardi, Remco Dijkman*

Main category: cs.AI

TL;DR: A DRL-based DSS is proposed for real-world scale DTAPs, featuring a novel graph structure and reward function, outperforming baselines in evaluations.


<details>
  <summary>Details</summary>
Motivation: Address the gap in DRL research for DTAPs, which often neglects real-world challenges, by developing a scalable solution.

Method: Introduces a DRL agent with a graph structure for observations/actions and a provably equivalent reward function to minimize task cycle time.

Result: The DRL agent matches or outperforms baselines in real-world DTAP instances and generalizes across time horizons.

Conclusion: The proposed DSS effectively tackles real-world DTAPs, demonstrating scalability and generalization.

Abstract: The Dynamic Task Assignment Problem (DTAP) concerns matching resources to
tasks in real time while minimizing some objectives, like resource costs or
task cycle time. In this work, we consider a DTAP variant where every task is a
case composed of a stochastic sequence of activities. The DTAP, in this case,
involves the decision of which employee to assign to which activity to process
requests as quickly as possible. In recent years, Deep Reinforcement Learning
(DRL) has emerged as a promising tool for tackling this DTAP variant, but most
research is limited to solving small-scale, synthetic problems, neglecting the
challenges posed by real-world use cases. To bridge this gap, this work
proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS.
To this end, we introduce a DRL agent with two novel elements: a graph
structure for observations and actions that can effectively represent any DTAP
and a reward function that is provably equivalent to the objective of
minimizing the average cycle time of tasks. The combination of these two
novelties allows the agent to learn effective and generalizable assignment
policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP
instances whose parameters are extracted from real-world logs through process
mining. The experimental evaluation shows how the proposed DRL agent matches or
outperforms the best baseline in all DTAP instances and generalizes on
different time horizons and across instances.

</details>


### [373] [How Group Lives Go Well](https://arxiv.org/abs/2504.19968)
*John Beverley, Regina Hurley*

Main category: cs.AI

TL;DR: The paper proposes a framework for modeling group well-being using ontology engineering, extending the Counterfactual Account (CT) to address limitations in traditional individual-focused theories.


<details>
  <summary>Details</summary>
Motivation: Traditional well-being theories focus on individuals and struggle to account for group-level welfare, especially when individual sacrifices contribute to social progress.

Method: The paper refines the CT framework and integrates it with Basic Formal Ontology (BFO) to model group flourishing based on functional persistence, roles, and historical impact.

Result: The proposed model enables semantic interoperability for reasoning about group welfare, social institutions, and longitudinal contributions.

Conclusion: The framework addresses gaps in group-level well-being modeling, offering a structured approach for evaluating collective welfare and flourishing.

Abstract: This paper explores the ontological space of group well being, proposing a
framework for representing collective welfare, group functions, and long term
contributions within an ontology engineering context. Traditional well being
theories focus on individual states, often relying on hedonistic, desire
satisfaction, or objective list models. Such approaches struggle to account for
cases where individual sacrifices contribute to broader social progress, a
critical challenge in modeling group flourishing. To address this, the paper
refines and extends the Counterfactual Account (CT) of well being, which
evaluates goodness of an event by comparing an individual's actual well being
with a hypothetical counterpart in a nearby possible world. While useful, this
framework is insufficient for group level ontologies, where well being depends
on functional persistence, institutional roles, and historical impact rather
than immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the
paper introduces a model in which group flourishing is evaluated in terms of
group functional, where members bear roles and exhibit persistence conditions
akin to biological systems or designed artifacts. This approach enables
semantic interoperability for modeling longitudinal social contributions,
allowing for structured reasoning about group welfare, social institutions, and
group flourishing over time.

</details>


### [374] [Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage](https://arxiv.org/abs/2504.20007)
*Anita Srbinovska, Angela Srbinovska, Vivek Senthil, Adrian Martin, John McCluskey, Ernest Fokoué*

Main category: cs.AI

TL;DR: A novel AI and ML framework analyzes police BWC footage to detect and classify interaction patterns between officers and civilians.


<details>
  <summary>Details</summary>
Motivation: To identify key behavioral dynamics (e.g., respect, escalation) in police-civilian interactions using BWC data.

Method: Multimodal data analysis integrating video, audio, and NLP techniques.

Result: Findings provide insights into behavioral patterns and a practical approach for law enforcement.

Conclusion: The framework advances knowledge discovery from BWC data and aids law enforcement.

Abstract: This paper proposes a novel interdisciplinary framework for analyzing police
body-worn camera (BWC) footage from the Rochester Police Department (RPD) using
advanced artificial intelligence (AI) and statistical machine learning (ML)
techniques. Our goal is to detect, classify, and analyze patterns of
interaction between police officers and civilians to identify key behavioral
dynamics, such as respect, disrespect, escalation, and de-escalation. We apply
multimodal data analysis by integrating video, audio, and natural language
processing (NLP) techniques to extract meaningful insights from BWC footage. We
present our methodology, computational techniques, and findings, outlining a
practical approach for law enforcement while advancing the frontiers of
knowledge discovery from police BWC data.

</details>


### [375] [Towards Automated Scoping of AI for Social Good Projects](https://arxiv.org/abs/2504.20010)
*Jacob Emmerson, Rayid Ghani, Zheyuan Ryan Shi*

Main category: cs.AI

TL;DR: AI4SG aims to tackle societal issues using AI, but problem scoping is a bottleneck. A Problem Scoping Agent (PSA) using LLMs generates expert-level proposals, validated by blind reviews. Challenges and future work are noted.


<details>
  <summary>Details</summary>
Motivation: Addressing societal challenges with AI is hindered by the complex, resource-intensive process of problem scoping due to a lack of dual-expertise professionals.

Method: Proposes a Problem Scoping Agent (PSA) leveraging large language models (LLMs) to create scientifically grounded project proposals.

Result: PSA-generated proposals are comparable to expert-written ones, validated through blind reviews and AI evaluations.

Conclusion: The PSA framework shows promise but highlights challenges in real-world problem scoping, suggesting areas for future research.

Abstract: Artificial Intelligence for Social Good (AI4SG) is an emerging effort that
aims to address complex societal challenges with the powerful capabilities of
AI systems. These challenges range from local issues with transit networks to
global wildlife preservation. However, regardless of scale, a critical
bottleneck for many AI4SG initiatives is the laborious process of problem
scoping -- a complex and resource-intensive task -- due to a scarcity of
professionals with both technical and domain expertise. Given the remarkable
applications of large language models (LLM), we propose a Problem Scoping Agent
(PSA) that uses an LLM to generate comprehensive project proposals grounded in
scientific literature and real-world knowledge. We demonstrate that our PSA
framework generates proposals comparable to those written by experts through a
blind review and AI evaluations. Finally, we document the challenges of
real-world problem scoping and note several areas for future work.

</details>


### [376] [Task-Agnostic Learning to Accomplish New Tasks](https://arxiv.org/abs/2209.04100)
*Xianqi Zhang, Xingtao Wang, Xu Liu, Wenrui Wang, Xiaopeng Fan, Debin Zhao*

Main category: cs.AI

TL;DR: The paper introduces TAL, a task-agnostic learning method for robotic decision-making, outperforming RL and IL by 20%.


<details>
  <summary>Details</summary>
Motivation: RL and IL struggle with new tasks due to reward functions, distribution shifts, and limited expert data. Humans use fragmented knowledge from task-agnostic experience, inspiring TAL.

Method: TAL involves four stages: task-agnostic exploration, knowledge graph organization, action feature extraction, candidate action generation, and action proposal for new tasks.

Result: TAL outperforms state-of-the-art offline RL and IL methods by over 20% in virtual indoor scene experiments.

Conclusion: TAL effectively learns fragmented knowledge from task-agnostic data, enabling better performance on new tasks compared to RL and IL.

Abstract: Reinforcement Learning (RL) and Imitation Learning (IL) have made great
progress in robotic decision-making in recent years. However, these methods
show obvious deterioration for new tasks that need to be completed through new
combinations of actions. RL methods suffer from reward functions and
distribution shifts, while IL methods are limited by expert demonstrations
which do not cover new tasks. In contrast, humans can easily complete these
tasks with the fragmented knowledge learned from task-agnostic experience.
Inspired by this observation, this paper proposes a task-agnostic learning
method (TAL for short) that can learn fragmented knowledge only from
task-agnostic data to accomplish new tasks. TAL consists of four stages. First,
the task-agnostic exploration is performed to collect data from interactions
with the environment. The collected data is organized via a knowledge graph.
Second, an action feature extractor is proposed and trained using the collected
knowledge graph data for task-agnostic fragmented knowledge learning. Third, a
candidate action generator is designed, which applies the action feature
extractor on a new task to generate multiple candidate action sets. Finally, an
action proposal network is designed to produce the probabilities for actions in
a new task according to the environmental information. The probabilities are
then used to generate order information for selecting actions to be executed
from multiple candidate action sets to form the plan. Experiments on a virtual
indoor scene show that the proposed method outperforms the state-of-the-art
offline RL methods and IL methods by more than 20%.

</details>


### [377] [Representing states in iterated belief revision](https://arxiv.org/abs/2305.09200)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: The paper compares four methods of storing doxastic states for iterated belief revision, highlighting their space efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the exponential growth problem of doxastic states in belief revision, often overlooked in literature.

Method: Analyzes four storage methods: explicit representation, level representation, natural representation, and lexicographic representation.

Result: Lexicographic representation is the most space-efficient, followed by natural and level representations, while explicit representation is the least efficient.

Conclusion: Space efficiency varies significantly across methods, with lexicographic representation being the most compact for storing doxastic states.

Abstract: Iterated belief revision requires information about the current beliefs. This
information is represented by mathematical structures called doxastic states.
Most literature concentrates on how to revise a doxastic state and neglects
that it may exponentially grow. This problem is studied for the most common
ways of storing a doxastic state. All four methods are able to store every
doxastic state, but some do it in less space than others. In particular, the
explicit representation (an enumeration of the current beliefs) is the more
wasteful on space. The level representation (a sequence of propositional
formulae) and the natural representation (a history of natural revisions) are
more compact than it. The lexicographic representation (a history of
lexicographic revision) is even more compact than them.

</details>


### [378] [Can we forget how we learned? Doxastic redundancy in iterated belief revision](https://arxiv.org/abs/2402.15445)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: Forgetting belief revisions may not lose information due to indirect deductions. An algorithm checks this for various belief revision operators, with exponential worst-case time, coNP-hard complexity, and coNP for homogeneous lexicographic sequences.


<details>
  <summary>Details</summary>
Motivation: To determine if forgetting belief acquisition episodes causes information loss, considering indirect deductions and multiple belief revisions.

Method: An algorithm is developed to check information loss for iterated belief revision operators (lexicographic, natural, severe, etc.), analyzing computational complexity.

Result: The algorithm may take exponential time (coNP-hard in worst case) but is in coNP for homogeneous lexicographic sequences.

Conclusion: Forgetting belief revisions doesn't always lose information due to deductions, but verifying this is computationally challenging.

Abstract: Forgetting a belief acquisition episode may not cause information loss
because of the others. Checking whether it does is not obvious, as the
contribution of each belief revision is not isolated from the others, and the
same information may be given not directly but by deduction. An algorithm for
checking whether forgetting reduces information is given for a number of
iterated belief revision operators: lexicographic, natural, severe, plain
severe, moderate severe, restrained, very radical and full meet revisions. It
may take exponential time in the worst case, which is expected given that the
problem is coNP-hard, even in the Horn restriction. It is in coNP for
homogeneous sequences of lexicographic revisions.

</details>


### [379] [Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment](https://arxiv.org/abs/2407.06443)
*Qizhang Feng, Siva Rajesh Kasa, Santhosh Kumar Kasa, Hyokun Yun, Choon Hui Teo, Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: The paper examines the vulnerability of LLMs aligned with DPO and PPO to membership inference attacks, finding DPO models more susceptible and introducing a new attack framework, PREMIA.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in LLM alignment using human preference data, focusing on vulnerabilities to membership inference attacks.

Method: Theoretical analysis and empirical evaluation using PREMIA and existing baselines to compare DPO and PPO models.

Result: DPO models are more vulnerable to membership inference attacks than PPO models.

Conclusion: The study highlights privacy risks in LLM alignment methods and proposes PREMIA for analyzing preference data vulnerabilities.

Abstract: Large Language Models (LLMs) have seen widespread adoption due to their
remarkable natural language capabilities. However, when deploying them in
real-world settings, it is important to align LLMs to generate texts according
to acceptable human standards. Methods such as Proximal Policy Optimization
(PPO) and Direct Preference Optimization (DPO) have enabled significant
progress in refining LLMs using human preference data. However, the privacy
concerns inherent in utilizing such preference data have yet to be adequately
studied. In this paper, we investigate the vulnerability of LLMs aligned using
two widely used methods - DPO and PPO - to membership inference attacks (MIAs).
Our study has two main contributions: first, we theoretically motivate that DPO
models are more vulnerable to MIA compared to PPO models; second, we introduce
a novel reference-based attack framework specifically for analyzing preference
data called PREMIA (\uline{Pre}ference data \uline{MIA}). Using PREMIA and
existing baselines we empirically show that DPO models have a relatively
heightened vulnerability towards MIA.

</details>


### [380] [Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with Spatiotemporal Constraints](https://arxiv.org/abs/2408.13918)
*Siyu Li, Toan Tran, Haowen Lin, John Krumm, Cyrus Shahabi, Lingyi Zhao, Khurram Shafique, Li Xiong*

Main category: cs.AI

TL;DR: Geo-Llama is a novel LLM framework for generating realistic human mobility trajectories under multiple spatiotemporal constraints, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Real human mobility data is often inaccessible due to costs and privacy concerns, and existing generative models lack control mechanisms for constraints like specific visits.

Method: Geo-Llama fine-tunes pre-trained LLMs on trajectory data using a visit-wise permutation strategy, enabling flexible constraint integration via prompts.

Result: Experiments show Geo-Llama effectively handles diverse constraints and generates more realistic trajectories than existing methods.

Conclusion: Geo-Llama addresses limitations of current models, offering a robust solution for controlled trajectory generation.

Abstract: Generating realistic human mobility data is essential for various application
domains, including transportation, urban planning, and epidemic control, as
real data is often inaccessible to researchers due to high costs and privacy
concerns. Existing deep generative models learn from real trajectories to
generate synthetic ones. Despite the progress, most of them suffer from
training stability issues and scale poorly with increasing data size. More
importantly, they often lack control mechanisms to guide the generated
trajectories under constraints such as enforcing specific visits. To address
these limitations, we formally define the controlled trajectory generation
problem for effectively handling multiple spatiotemporal constraints. We
introduce Geo-Llama, a novel LLM finetuning framework that can enforce multiple
explicit visit constraints while maintaining contextual coherence of the
generated trajectories. In this approach, pre-trained LLMs are fine-tuned on
trajectory data with a visit-wise permutation strategy where each visit
corresponds to a specific time and location. This strategy enables the model to
capture spatiotemporal patterns regardless of visit orders while maintaining
flexible and in-context constraint integration through prompts during
generation. Extensive experiments on real-world and synthetic datasets validate
the effectiveness of Geo-Llama, demonstrating its versatility and robustness in
handling a broad range of constraints to generate more realistic trajectories
compared to existing methods.

</details>


### [381] [AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents](https://arxiv.org/abs/2409.09013)
*Zhe Su, Xuhui Zhou, Sanketh Rangreji, Anubha Kabra, Julia Mendelsohn, Faeze Brahman, Maarten Sap*

Main category: cs.AI

TL;DR: The paper introduces AI-LieDar, a framework to study how LLMs balance truthfulness and utility in multi-turn interactions, revealing models often lie and can be steered toward deception or truth.


<details>
  <summary>Details</summary>
Motivation: To address the conflict between truthfulness and utility in LLMs, especially in real-world scenarios where goals may require deception.

Method: Developed AI-LieDar, a framework with real-world scenarios and a truthfulness detector to evaluate LLM agents in multi-turn conversations.

Result: Models were truthful less than 50% of the time, with varying truthfulness and utility rates. LLMs can be steered toward truth or deception, but even truth-steered models still lie.

Conclusion: The study highlights the complexity of truthfulness in LLMs and the need for further research to ensure their safe and reliable deployment.

Abstract: Truthfulness (adherence to factual accuracy) and utility (satisfying human
needs and instructions) are both fundamental aspects of Large Language Models,
yet these goals often conflict (e.g., sell a car with known flaws), which makes
it challenging to achieve both in real-world deployments. We propose AI-LieDar,
a framework to study how LLM-based agents navigate these scenarios in an
multi-turn interactive setting. We design a set of real-world scenarios where
language agents are instructed to achieve goals that are in conflict with being
truthful during a multi-turn conversation with simulated human agents. To
evaluate the truthfulness at large scale, we develop a truthfulness detector
inspired by psychological literature to assess the agents' responses. Our
experiment demonstrates that all models are truthful less than 50% of the time,
though truthfulness and goal achievement (utility) rates vary across models. We
further test the steerability of LLMs towards truthfulness, finding that models
can be directed to be truthful or deceptive, and even truth-steered models
still lie. These findings reveal the complex nature of truthfulness in LLMs and
underscore the importance of further research to ensure the safe and reliable
deployment of LLMs and LLM-based agents.

</details>


### [382] [Channel-Aware Throughput Maximization for Cooperative Data Fusion in CAV](https://arxiv.org/abs/2410.04320)
*Haonan An, Zhengru Fang, Yuang Zhang, Senkang Hu, Xianhao Chen, Guowen Xu, Yuguang Fang*

Main category: cs.AI

TL;DR: The paper proposes a channel-aware throughput maximization approach for CAV data fusion, using a self-supervised autoencoder for adaptive data compression, improving network throughput and precision.


<details>
  <summary>Details</summary>
Motivation: Challenges like blind spots and obstructions in CAVs are addressed by V2V communications, but cooperative perception is limited by network throughput and channel quality.

Method: A mixed integer programming model is decomposed into sub-problems for optimal data rate and compression ratio. A self-supervised autoencoder is trained for adaptive compression, with fine-tuning to reduce resource use.

Result: Experiments show a 20.19% throughput improvement, 9.38% AP@IoU increase, and optimal latency of 19.99 ms.

Conclusion: The proposed method effectively enhances CAV data fusion performance under network constraints.

Abstract: Connected and autonomous vehicles (CAVs) have garnered significant attention
due to their extended perception range and enhanced sensing coverage. To
address challenges such as blind spots and obstructions, CAVs employ
vehicle-to-vehicle (V2V) communications to aggregate sensory data from
surrounding vehicles. However, cooperative perception is often constrained by
the limitations of achievable network throughput and channel quality. In this
paper, we propose a channel-aware throughput maximization approach to
facilitate CAV data fusion, leveraging a self-supervised autoencoder for
adaptive data compression. We formulate the problem as a mixed integer
programming (MIP) model, which we decompose into two sub-problems to derive
optimal data rate and compression ratio solutions under given link conditions.
An autoencoder is then trained to minimize bitrate with the determined
compression ratio, and a fine-tuning strategy is employed to further reduce
spectrum resource consumption. Experimental evaluation on the OpenCOOD platform
demonstrates the effectiveness of our proposed algorithm, showing more than
20.19\% improvement in network throughput and a 9.38\% increase in average
precision (AP@IoU) compared to state-of-the-art methods, with an optimal
latency of 19.99 ms.

</details>


### [383] [Multi-Agent LLMs Ensemble for Efficient Atrial Fibrillation Annotation of ECG Reports](https://arxiv.org/abs/2410.16543)
*Jingwei Huang, Kuroush Nezafati, Ismael Villanueva-Miranda, Zifan Gu, Yueshuang Xu, Ann Marie Navar, Tingyi Wanyan, Qin Zhou, Bo Yao, Ruichen Rong, Xiaowei Zhan, Guanghua Xiao, Eric D. Peterson, Donghan M. Yang, Wenqi Shi, Yang Xie*

Main category: cs.AI

TL;DR: A novel multiagent ensemble method using LLMs automates large-scale EHR data labeling, achieving high accuracy and reducing manual effort.


<details>
  <summary>Details</summary>
Motivation: Manual labeling of large-scale EHR datasets is labor-intensive, time-consuming, and error-prone, necessitating an automated solution.

Method: An ensemble of diverse open-source LLMs uses majority voting with a minimal winning threshold for labeling tasks, applied to ECG reports and SDOH identification.

Result: The method labeled 623,566 ECG reports with 98.2% accuracy and identified SDOH from 1,405 clinical notes, outperforming individual LLMs and reducing errors.

Conclusion: The ensemble LLMs method is scalable, efficient, and generalizable, significantly reducing labeling effort while maintaining high accuracy.

Abstract: This study introduces a novel multiagent ensemble method powered by LLMs to
address a key challenge in ML - data labeling, particularly in large-scale EHR
datasets. Manual labeling of such datasets requires domain expertise and is
labor-intensive, time-consuming, expensive, and error-prone. To overcome this
bottleneck, we developed an ensemble LLMs method and demonstrated its
effectiveness in two real-world tasks: (1) labeling a large-scale unlabeled ECG
dataset in MIMIC-IV; (2) identifying social determinants of health (SDOH) from
the clinical notes of EHR. Trading off benefits and cost, we selected a pool of
diverse open source LLMs with satisfactory performance. We treat each LLM's
prediction as a vote and apply a mechanism of majority voting with minimal
winning threshold for ensemble. We implemented an ensemble LLMs application for
EHR data labeling tasks. By using the ensemble LLMs and natural language
processing, we labeled MIMIC-IV ECG dataset of 623,566 ECG reports with an
estimated accuracy of 98.2%. We applied the ensemble LLMs method to identify
SDOH from social history sections of 1,405 EHR clinical notes, also achieving
competitive performance. Our experiments show that the ensemble LLMs can
outperform individual LLM even the best commercial one, and the method reduces
hallucination errors. From the research, we found that (1) the ensemble LLMs
method significantly reduces the time and effort required for labeling
large-scale EHR data, automating the process with high accuracy and quality;
(2) the method generalizes well to other text data labeling tasks, as shown by
its application to SDOH identification; (3) the ensemble of a group of diverse
LLMs can outperform or match the performance of the best individual LLM; and
(4) the ensemble method substantially reduces hallucination errors. This
approach provides a scalable and efficient solution to data-labeling
challenges.

</details>


### [384] [Quasi-random Multi-Sample Inference for Large Language Models](https://arxiv.org/abs/2411.06251)
*Aditya Parashar, Aditya Vikram Singh, Avinash Amballa, Jinlin Lai, Benjamin Rozonoyer*

Main category: cs.AI

TL;DR: Arithmetic sampling in LLMs improves diversity and performance in multi-sample tasks like reasoning and translation, outperforming traditional methods without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Traditional text generation methods (beam search, ancestral sampling) lack parallelizability or sample diversity, limiting their effectiveness in multi-sample inference tasks.

Method: The study introduces arithmetic sampling, leveraging quasi-random codes for parallelizable and diverse sample generation, and compares it with ancestral sampling in chain-of-thought reasoning and machine translation tasks.

Result: Arithmetic sampling enhances diversity and performance, yielding 3-5% accuracy boost on GSM8K and 0.45-0.89% COMET score improvement on WMT19 tasks.

Conclusion: Arithmetic sampling is a superior alternative to traditional methods for multi-sample decoding, offering significant performance gains with minimal overhead.

Abstract: Large language models (LLMs) are often equipped with multi-sample decoding
strategies. An LLM implicitly defines an arithmetic code book, facilitating
efficient and embarrassingly parallelizable \textbf{arithmetic sampling} to
produce multiple samples using quasi-random codes. Traditional text generation
methods, such as beam search and sampling-based techniques, have notable
limitations: they lack parallelizability or diversity of sampled sequences.
This study explores the potential of arithmetic sampling, contrasting it with
ancestral sampling across two decoding tasks that employ multi-sample
inference: chain-of-thought reasoning with self-consistency and machine
translation with minimum Bayes risk decoding. Our results demonstrate that
arithmetic sampling produces more diverse samples, significantly improving
reasoning and translation performance as the sample size increases. We observe
a $\mathbf{3\text{-}5\%}$ point increase in accuracy on the GSM8K dataset and a
$\mathbf{0.45\text{-}0.89\%}$ point increment in COMET score for WMT19 tasks
using arithmetic sampling without any significant computational overhead.

</details>


### [385] [A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks](https://arxiv.org/abs/2501.10069)
*Xinzhe Li*

Main category: cs.AI

TL;DR: A survey unifying LLM test-time compute frameworks under MDP, modularizing LLM profiling and search procedures for better comparison.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of standardized comparisons in LLM inference frameworks due to divergent task definitions, profiling, and search procedures.

Method: Unifies task definitions under Markov Decision Process (MDP) and modularizes LLM profiling and search procedures.

Result: Enables precise comparisons of LLM inference frameworks and highlights deviations from conventional search algorithms.

Conclusion: The survey provides a comprehensive review of LLM inference frameworks, discussing their applicability, performance, and efficiency.

Abstract: LLM test-time compute (or LLM inference) via search has emerged as a
promising research area with rapid developments. However, current frameworks
often adopt distinct perspectives on three key aspects: task definition, LLM
profiling, and search procedures, making direct comparisons challenging.
Moreover, the search algorithms employed often diverge from standard
implementations, and their specific characteristics are not thoroughly
specified. This survey aims to provide a comprehensive but integrated technical
review on existing LIS frameworks. Specifically, we unify task definitions
under Markov Decision Process (MDP) and provides modular definitions of LLM
profiling and search procedures. The definitions enable precise comparisons of
various LLM inference frameworks while highlighting their departures from
conventional search algorithms. We also discuss the applicability, performance,
and efficiency of these methods. For ongoing paper updates, please refer to our
GitHub repository: https://github.com/xinzhel/LLM-Search.

</details>


### [386] [MatterChat: A Multi-Modal LLM for Material Science](https://arxiv.org/abs/2502.13107)
*Yingheng Tang, Wenbin Xu, Jie Cao, Weilu Gao, Steve Farrell, Benjamin Erichson, Michael W. Mahoney, Andy Nonaka, Zhi Yao*

Main category: cs.AI

TL;DR: MatterChat integrates material structure data with language models to improve material property prediction and human-AI interaction.


<details>
  <summary>Details</summary>
Motivation: Advancing materials science requires better tools for understanding and predicting material properties, especially by combining structural data with language models.

Method: MatterChat uses a bridging module to align a pretrained interatomic potential model with a pretrained LLM, unifying structural and textual inputs.

Result: MatterChat outperforms general-purpose LLMs like GPT-4 in material property prediction and enables advanced scientific reasoning and synthesis.

Conclusion: MatterChat is a powerful tool for materials science, enhancing AI capabilities in property prediction and synthesis planning.

Abstract: Understanding and predicting the properties of inorganic materials is crucial
for accelerating advancements in materials science and driving applications in
energy, electronics, and beyond. Integrating material structure data with
language-based information through multi-modal large language models (LLMs)
offers great potential to support these efforts by enhancing human-AI
interaction. However, a key challenge lies in integrating atomic structures at
full resolution into LLMs. In this work, we introduce MatterChat, a versatile
structure-aware multi-modal LLM that unifies material structural data and
textual inputs into a single cohesive model. MatterChat employs a bridging
module to effectively align a pretrained machine learning interatomic potential
with a pretrained LLM, reducing training costs and enhancing flexibility. Our
results demonstrate that MatterChat significantly improves performance in
material property prediction and human-AI interaction, surpassing
general-purpose LLMs such as GPT-4. We also demonstrate its usefulness in
applications such as more advanced scientific reasoning and step-by-step
material synthesis.

</details>


### [387] [Atomic Proximal Policy Optimization for Electric Robo-Taxi Dispatch and Charger Allocation](https://arxiv.org/abs/2502.13392)
*Jim Dai, Manxi Wu, Zhanhao Zhang*

Main category: cs.AI

TL;DR: The paper introduces Atomic-PPO, a scalable deep reinforcement learning algorithm, to optimize robo-taxi operations like ride matching, repositioning, and charging in stochastic environments. It outperforms benchmarks and analyzes charging facility allocation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of exponentially growing action and state spaces in large-scale robo-taxi fleet dispatching.

Method: Model the system as a Markov Decision Process and propose Atomic-PPO, which reduces action space via atomic action decomposition.

Result: Atomic-PPO achieves superior performance in long-run average reward compared to benchmarks, and insights on charging facility allocation are provided.

Conclusion: Atomic-PPO is effective for scalable robo-taxi dispatching, and charging infrastructure planning significantly impacts system performance.

Abstract: Pioneering companies such as Waymo have deployed robo-taxi services in
several U.S. cities. These robo-taxis are electric vehicles, and their
operations require the joint optimization of ride matching, vehicle
repositioning, and charging scheduling in a stochastic environment. We model
the operations of the ride-hailing system with robo-taxis as a discrete-time,
average-reward Markov Decision Process with an infinite horizon. As the fleet
size grows, dispatching becomes challenging, as both the system state space and
the fleet dispatching action space grow exponentially with the number of
vehicles. To address this, we introduce a scalable deep reinforcement learning
algorithm, called Atomic Proximal Policy Optimization (Atomic-PPO), that
reduces the action space using atomic action decomposition. We evaluate our
algorithm using real-world NYC for-hire vehicle trip records and measure its
performance by the long-run average reward achieved by the dispatching policy,
relative to a fluid-based upper bound. Our experiments demonstrate the superior
performance of Atomic-PPO compared to benchmark methods. Furthermore, we
conduct extensive numerical experiments to analyze the efficient allocation of
charging facilities and assess the impact of vehicle range and charger speed on
system performance.

</details>


### [388] [TabulaTime: A Novel Multimodal Deep Learning Framework for Advancing Acute Coronary Syndrome Prediction through Environmental and Clinical Data Integration](https://arxiv.org/abs/2502.17049)
*Xin Zhang, Liangxiu Han, Stephen White, Saad Hassan, Philip A Kalra, James Ritchie, Carl Diver, Jennie Shorley*

Main category: cs.AI

TL;DR: TabulaTime is a multimodal deep learning framework that combines clinical and air pollution data to improve ACS risk prediction, outperforming traditional models by over 20%.


<details>
  <summary>Details</summary>
Motivation: Traditional cardiovascular risk scores overlook environmental factors like air pollution, which significantly impact heart health. Integrating such data with clinical records is challenging.

Method: TabulaTime integrates time-series air pollution and clinical data, uses PatchRWKV for temporal pattern extraction, and employs attention mechanisms for interpretability.

Result: TabulaTime improves prediction accuracy by over 20%, with air pollution data alone contributing over 10%. Key predictors include previous angina, systolic blood pressure, PM10, and NO2.

Conclusion: TabulaTime bridges clinical and environmental insights, aiding personalized prevention and public health policies to reduce ACS risk.

Abstract: Acute Coronary Syndromes (ACS), including ST-segment elevation myocardial
infarctions (STEMI) and non-ST-segment elevation myocardial infarctions
(NSTEMI), remain a leading cause of mortality worldwide. Traditional
cardiovascular risk scores rely primarily on clinical data, often overlooking
environmental influences like air pollution that significantly impact heart
health. Moreover, integrating complex time-series environmental data with
clinical records is challenging.
  We introduce TabulaTime, a multimodal deep learning framework that enhances
ACS risk prediction by combining clinical risk factors with air pollution data.
TabulaTime features three key innovations: First, it integrates time-series air
pollution data with clinical tabular data to improve prediction accuracy.
Second, its PatchRWKV module automatically extracts complex temporal patterns,
overcoming limitations of traditional feature engineering while maintaining
linear computational complexity. Third, attention mechanisms enhance
interpretability by revealing interactions between clinical and environmental
factors.
  Experimental results show that TabulaTime improves prediction accuracy by
over 20% compared to conventional models such as CatBoost, Random Forest, and
LightGBM, with air pollution data alone contributing over a 10% improvement.
Feature importance analysis identifies critical predictors including previous
angina, systolic blood pressure, PM10, and NO2. Overall, TabulaTime bridges
clinical and environmental insights, supporting personalized prevention
strategies and informing public health policies to mitigate ACS risk.

</details>


### [389] [Repurposing the scientific literature with vision-language models](https://arxiv.org/abs/2502.19546)
*Anton Alyakin, Jaden Stryker, Daniel Alexander Alber, Karl L. Sangwon, Jin Vivian Lee, Brandon Duderstadt, Akshay Save, David Kurland, Spencer Frome, Shrutika Singh, Jeff Zhang, Eunice Yang, Ki Yun Park, Cordelia Orillac, Aly A. Valliani, Sean Neifert, Albert Liu, Aneek Patel, Christopher Livia, Darryl Lau, Ilya Laufer, Peter A. Rozman, Eveline Teresa Hidalgo, Howard Riina, Rui Feng, Todd Hollon, Yindalon Aphinyanaphongs, John G. Golfinos, Laura Snyder, Eric Leuthardt, Douglas Kondziolka, Eric Karl Oermann*

Main category: cs.AI

TL;DR: Training VLMs on neurosurgery-specific data (NeuroPubs) yields high-performance tools for academic and clinical tasks, matching GPT-4o in some areas.


<details>
  <summary>Details</summary>
Motivation: General VLMs overlook domain-specific knowledge in scientific journals, limiting their utility in specialty tasks like neurosurgery.

Method: Created NeuroPubs (23K articles, 134M words, 78K image-caption pairs) and trained CNS-Obsidian (34B-parameter VLM) for tasks like generating graphical abstracts and board-style questions.

Result: Achieved 70% success in graphical abstracts and 54% in indistinguishable board-style questions; CNS-Obsidian matched GPT-4o in neurosurgical diagnosis (non-inferiority shown).

Conclusion: Specialty-specific training enables high-performance AI tools without relying on large-scale internet data, applicable across diverse fields.

Abstract: Leading vision-language models (VLMs) are trained on general Internet
content, overlooking scientific journals' rich, domain-specific knowledge.
Training on specialty-specific literature could yield high-performance,
task-specific tools, enabling generative AI to match generalist models in
specialty publishing, educational, and clinical tasks. We created NeuroPubs, a
multimodal dataset of 23,000 Neurosurgery Publications articles (134M words,
78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready
graphical abstracts (70% of 100 abstracts) and board-style questions
indistinguishable from human-written ones (54% of 89,587 questions). We used
these questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded,
randomized controlled trial, our model demonstrated non-inferiority to then
state-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical
utility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%,
p=0.3797). Our pilot study demonstrates how training generative AI models on
specialty-specific journal content - without large-scale internet data -
results in high-performance academic and clinical tools, enabling
domain-tailored AI across diverse fields.

</details>


### [390] [NutriGen: Personalized Meal Plan Generator Leveraging Large Language Models to Enhance Dietary and Nutritional Adherence](https://arxiv.org/abs/2502.20601)
*Saman Khamesian, Asiful Arefeen, Stephanie M. Carpenter, Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: NutriGen, an LLM-based framework, generates personalized meal plans addressing dietary constraints and preferences, outperforming existing systems with low error rates.


<details>
  <summary>Details</summary>
Motivation: Many struggle with meal planning due to complexity, time, and lack of knowledge. Current systems lack adaptability and practicality.

Method: NutriGen uses LLMs (e.g., Llama 3.1 8B, GPT-3.5 Turbo) and a personalized nutrition database with prompt engineering for accurate recommendations.

Result: LLMs achieve low error rates (1.55% and 3.68%), closely aligning with caloric targets and improving precision.

Conclusion: LLMs show strong potential for scalable, accurate, and user-friendly dietary recommendations, addressing existing system limitations.

Abstract: Maintaining a balanced diet is essential for overall health, yet many
individuals struggle with meal planning due to nutritional complexity, time
constraints, and lack of dietary knowledge. Personalized food recommendations
can help address these challenges by tailoring meal plans to individual
preferences, habits, and dietary restrictions. However, existing dietary
recommendation systems often lack adaptability, fail to consider real-world
constraints such as food ingredient availability, and require extensive user
input, making them impractical for sustainable and scalable daily use. To
address these limitations, we introduce NutriGen, a framework based on large
language models (LLM) designed to generate personalized meal plans that align
with user-defined dietary preferences and constraints. By building a
personalized nutrition database and leveraging prompt engineering, our approach
enables LLMs to incorporate reliable nutritional references like the USDA
nutrition database while maintaining flexibility and ease-of-use. We
demonstrate that LLMs have strong potential in generating accurate and
user-friendly food recommendations, addressing key limitations in existing
dietary recommendation systems by providing structured, practical, and scalable
meal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve
the lowest percentage errors of 1.55\% and 3.68\%, respectively, producing meal
plans that closely align with user-defined caloric targets while minimizing
deviation and improving precision. Additionally, we compared the performance of
DeepSeek V3 against several established models to evaluate its potential in
personalized nutrition planning.

</details>


### [391] [Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
*Andy Zhou*

Main category: cs.AI

TL;DR: Siege is a multi-turn adversarial framework that erodes LLM safety via tree search, achieving high jailbreak success rates with fewer queries.


<details>
  <summary>Details</summary>
Motivation: To understand how minor concessions in LLM responses can accumulate into fully disallowed outputs, highlighting the need for robust multi-turn testing.

Method: Uses breadth-first tree search to expand adversarial prompts incrementally, exploiting partial compliance from previous responses.

Result: Achieves 100% success on GPT-3.5-turbo and 97% on GPT-4, outperforming baselines like Crescendo or GOAT.

Conclusion: Siege reveals the vulnerability of LLM safeguards over successive turns, emphasizing the need for stronger multi-turn testing.

Abstract: We introduce Siege, a multi-turn adversarial framework that models the
gradual erosion of Large Language Model (LLM) safety through a tree search
perspective. Unlike single-turn jailbreaks that rely on one meticulously
engineered prompt, Siege expands the conversation at each turn in a
breadth-first fashion, branching out multiple adversarial prompts that exploit
partial compliance from previous responses. By tracking these incremental
policy leaks and re-injecting them into subsequent queries, Siege reveals how
minor concessions can accumulate into fully disallowed outputs. Evaluations on
the JailbreakBench dataset show that Siege achieves a 100% success rate on
GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries
than baselines such as Crescendo or GOAT. This tree search methodology offers
an in-depth view of how model safeguards degrade over successive dialogue
turns, underscoring the urgency of robust multi-turn testing procedures for
language models.

</details>


### [392] [Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use](https://arxiv.org/abs/2504.04736)
*Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, Christopher D. Manning*

Main category: cs.AI

TL;DR: SWiRL introduces a multi-step reinforcement learning approach for language models, outperforming baselines on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional RL methods treat tasks as single-step, but complex reasoning requires multi-step optimization.

Method: SWiRL decomposes multi-step trajectories into sub-trajectories, applies synthetic data filtering, and optimizes with RL.

Result: SWiRL improves accuracy by 11.1%-21.5% on tasks like GSM8K and HotPotQA, showing cross-task generalization.

Conclusion: SWiRL is effective for multi-step reasoning and generalizes across tasks, enhancing language model performance.

Abstract: Reinforcement learning has been shown to improve the performance of large
language models. However, traditional approaches like RLHF or RLAIF treat the
problem as single-step. As focus shifts toward more complex reasoning and
agentic tasks, language models must take multiple steps of text generation,
reasoning and environment interaction before generating a solution. We propose
a synthetic data generation and RL methodology targeting multi-step
optimization scenarios. This approach, called Step-Wise Reinforcement Learning
(SWiRL), iteratively generates multi-step reasoning and tool use data, and then
learns from that data. It employs a simple step-wise decomposition that breaks
each multi-step trajectory into multiple sub-trajectories corresponding to each
action by the original model. It then applies synthetic data filtering and RL
optimization on these sub-trajectories. We evaluated SWiRL on a number of
multi-step tool use, question answering, and mathematical reasoning tasks. Our
experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,
14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,
MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits
generalization across tasks: for example, training only on HotPotQA (text
question-answering) improves zero-shot performance on GSM8K (a math dataset) by
a relative 16.9%.

</details>


### [393] [Embodied World Models Emerge from Navigational Task in Open-Ended Environments](https://arxiv.org/abs/2504.11419)
*Li Jin, Liu Jia*

Main category: cs.AI

TL;DR: A recurrent agent trained with sparse rewards in planar mazes internalizes spatial concepts like direction and distance, enabling near-optimal navigation in unseen mazes. Analysis reveals stable neural-behavioral alignment and causal evidence for embodied world models.


<details>
  <summary>Details</summary>
Motivation: To explore if active, embodied interaction (rather than passive prediction) leads to useful spatial representations in partially observable environments.

Method: Train a recurrent agent with sparse rewards in procedurally generated mazes, then analyze its behavior and neural dynamics using hybrid dynamical systems theory and Ridge Representation.

Result: The agent achieves near-optimal navigation in new mazes, with neural-behavioral alignment confirmed by canonical correlation analysis and causal perturbations.

Conclusion: Sustained sensorimotor interaction enables spontaneous emergence of compact, embodied world models, offering interpretable and transferable navigation policies.

Abstract: Spatial reasoning in partially observable environments has often been
approached through passive predictive models, yet theories of embodied
cognition suggest that genuinely useful representations arise only when
perception is tightly coupled to action. Here we ask whether a recurrent agent,
trained solely by sparse rewards to solve procedurally generated planar mazes,
can autonomously internalize metric concepts such as direction, distance and
obstacle layout. After training, the agent consistently produces near-optimal
paths in unseen mazes, behavior that hints at an underlying spatial model. To
probe this possibility, we cast the closed agent-environment loop as a hybrid
dynamical system, identify stable limit cycles in its state space, and
characterize behavior with a Ridge Representation that embeds whole
trajectories into a common metric space. Canonical correlation analysis exposes
a robust linear alignment between neural and behavioral manifolds, while
targeted perturbations of the most informative neural dimensions sharply
degrade navigation performance. Taken together, these dynamical,
representational, and causal signatures show that sustained sensorimotor
interaction is sufficient for the spontaneous emergence of compact, embodied
world models, providing a principled path toward interpretable and transferable
navigation policies.

</details>


### [394] [A Survey of AI Agent Protocols](https://arxiv.org/abs/2504.16736)
*Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, Weinan Zhang*

Main category: cs.AI

TL;DR: The paper proposes a unified communication protocol for LLM agents to address the lack of standardization, enabling smoother interactions and collaboration. It classifies existing protocols, analyzes their performance, and outlines future research directions.


<details>
  <summary>Details</summary>
Motivation: The absence of standardized communication protocols for LLM agents hinders collaboration, scalability, and their ability to handle complex tasks. A unified protocol could enhance interoperability and collective intelligence.

Method: The authors classify existing protocols into context-oriented vs. inter-agent and general-purpose vs. domain-specific. They also analyze performance across security, scalability, and latency.

Result: The study provides a systematic classification and performance analysis of protocols, identifying gaps and proposing future research directions.

Conclusion: The paper serves as a reference for designing next-generation protocols, emphasizing adaptability, privacy, and collective intelligence.

Abstract: The rapid development of large language models (LLMs) has led to the
widespread deployment of LLM agents across diverse industries, including
customer service, content generation, data analysis, and even healthcare.
However, as more LLM agents are deployed, a major issue has emerged: there is
no standard way for these agents to communicate with external tools or data
sources. This lack of standardized protocols makes it difficult for agents to
work together or scale effectively, and it limits their ability to tackle
complex, real-world tasks. A unified communication protocol for LLM agents
could change this. It would allow agents and tools to interact more smoothly,
encourage collaboration, and triggering the formation of collective
intelligence. In this paper, we provide the first comprehensive analysis of
existing agent protocols, proposing a systematic two-dimensional classification
that differentiates context-oriented versus inter-agent protocols and
general-purpose versus domain-specific protocols. Additionally, we conduct a
comparative performance analysis of these protocols across key dimensions such
as security, scalability, and latency. Finally, we explore the future landscape
of agent protocols by identifying critical research directions and
characteristics necessary for next-generation protocols. These characteristics
include adaptability, privacy preservation, and group-based interaction, as
well as trends toward layered architectures and collective intelligence
infrastructures. We expect this work to serve as a practical reference for both
researchers and engineers seeking to design, evaluate, or integrate robust
communication infrastructures for intelligent agents.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [395] [Speaker Diarization for Low-Resource Languages Through Wav2vec Fine-Tuning](https://arxiv.org/abs/2504.18582)
*Abdulhady Abas Abdullah, Sarkhel H. Taher Karim, Sara Azad Ahmed, Kanar R. Tariq, Tarik A. Rashid*

Main category: cs.SD

TL;DR: The study improves speaker diarization for Kurdish using Wav2Vec 2.0, reducing error rates by 7.2% and boosting cluster purity by 13%.


<details>
  <summary>Details</summary>
Motivation: Address challenges in low-resource languages like Kurdish, including limited data, dialects, and code-switching.

Method: Train Wav2Vec 2.0 on a Kurdish corpus using transfer learning from multilingual representations.

Result: Reduced diarization error rate by 7.2% and improved cluster purity by 13%.

Conclusion: Enhancements to existing models can improve diarization for under-resourced languages, aiding transcription and multilingual applications.

Abstract: Speaker diarization is a fundamental task in speech processing that involves
dividing an audio stream by speaker. Although state-of-the-art models have
advanced performance in high-resource languages, low-resource languages such as
Kurdish pose unique challenges due to limited annotated data, multiple dialects
and frequent code-switching. In this study, we address these issues by training
the Wav2Vec 2.0 self-supervised learning model on a dedicated Kurdish corpus.
By leveraging transfer learning, we adapted multilingual representations
learned from other languages to capture the phonetic and acoustic
characteristics of Kurdish speech. Relative to a baseline method, our approach
reduced the diarization error rate by seven point two percent and improved
cluster purity by thirteen percent. These findings demonstrate that
enhancements to existing models can significantly improve diarization
performance for under-resourced languages. Our work has practical implications
for developing transcription services for Kurdish-language media and for
speaker segmentation in multilingual call centers, teleconferencing and
video-conferencing systems. The results establish a foundation for building
effective diarization systems in other understudied languages, contributing to
greater equity in speech technology.

</details>


### [396] [Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness](https://arxiv.org/abs/2504.18950)
*Erfan Loweimi, Mengjie Qian, Kate Knill, Mark Gales*

Main category: cs.SD

TL;DR: The paper explores challenges and solutions for speaker retrieval in large, uncontrolled audio/video archives, focusing on the BBC Rewind archive, and demonstrates the robustness of the proposed framework.


<details>
  <summary>Details</summary>
Motivation: The increasing importance of efficient access to audio/video archives and the challenges of speaker retrieval in uncontrolled environments motivate this study.

Method: The paper investigates system development aspects like speaker diarisation and embedding extraction, and evaluates performance through experiments in clean and distorted setups.

Result: The developed speaker retrieval systems show effectiveness and robustness, proving the framework's versatility for broader applications.

Conclusion: The proposed framework is scalable and adaptable for diverse archives beyond the BBC Rewind corpus.

Abstract: There is a growing abundance of publicly available or company-owned
audio/video archives, highlighting the increasing importance of efficient
access to desired content and information retrieval from these archives. This
paper investigates the challenges, solutions, effectiveness, and robustness of
speaker retrieval systems developed "in the wild" which involves addressing two
primary challenges: extraction of task-relevant labels from limited metadata
for system development and evaluation, as well as the unconstrained acoustic
conditions encountered in the archive, ranging from quiet studios to adverse
noisy environments. While we focus on the publicly-available BBC Rewind archive
(spanning 1948 to 1979), our framework addresses the broader issue of speaker
retrieval on extensive and possibly aged archives with no control over the
content and acoustic conditions. Typically, these archives offer a brief and
general file description, mostly inadequate for specific applications like
speaker retrieval, and manual annotation of such large-scale archives is
unfeasible. We explore various aspects of system development (e.g., speaker
diarisation, embedding extraction, query selection) and analyse the challenges,
possible solutions, and their functionality. To evaluate the performance, we
conduct systematic experiments in both clean setup and against various
distortions simulating real-world applications. Our findings demonstrate the
effectiveness and robustness of the developed speaker retrieval systems,
establishing the versatility and scalability of the proposed framework for a
wide range of applications beyond the BBC Rewind corpus.

</details>


### [397] [Improving Pretrained YAMNet for Enhanced Speech Command Detection via Transfer Learning](https://arxiv.org/abs/2504.19030)
*Sidahmed Lachenani, Hamza Kheddar, Mohamed Ouldzmirli*

Main category: cs.SD

TL;DR: The paper proposes a method using YAMNet and transfer learning to improve speech command recognition, achieving 95.28% accuracy.


<details>
  <summary>Details</summary>
Motivation: Enhancing accuracy and efficiency in speech command recognition for better user interaction in smart applications.

Method: Adapts and trains the YAMNet model using the Speech Commands dataset, with augmentation and strategic feature extraction.

Result: Achieved a recognition accuracy of 95.28%.

Conclusion: Demonstrates the effectiveness of transfer learning in speech command recognition, setting a new benchmark for future research.

Abstract: This work addresses the need for enhanced accuracy and efficiency in speech
command recognition systems, a critical component for improving user
interaction in various smart applications. Leveraging the robust pretrained
YAMNet model and transfer learning, this study develops a method that
significantly improves speech command recognition. We adapt and train a YAMNet
deep learning model to effectively detect and interpret speech commands from
audio signals. Using the extensively annotated Speech Commands dataset
(speech_commands_v0.01), our approach demonstrates the practical application of
transfer learning to accurately recognize a predefined set of speech commands.
The dataset is meticulously augmented, and features are strategically extracted
to boost model performance. As a result, the final model achieved a recognition
accuracy of 95.28%, underscoring the impact of advanced machine learning
techniques on speech command recognition. This achievement marks substantial
progress in audio processing technologies and establishes a new benchmark for
future research in the field.

</details>


### [398] [Muyan-TTS: A Trainable Text-to-Speech Model Optimized for Podcast Scenarios with a $50K Budget](https://arxiv.org/abs/2504.19146)
*Xin Li, Kaikai Jia, Hao Sun, Jun Dai, Ziyang Jiang*

Main category: cs.SD

TL;DR: Muyan-TTS is an open-source, budget-friendly TTS model optimized for podcast scenarios, offering high-quality voice generation and speaker adaptation.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of open-source training code, efficient inference frameworks, and podcast-optimized TTS models in existing LLM-based TTS solutions.

Method: Pre-trained on 100k+ hours of podcast audio, Muyan-TTS supports zero-shot synthesis and speaker adaptation with minimal data.

Result: Delivers high-quality TTS synthesis and customizable voice generation, with open-sourced code, data pipeline, and optimized inference.

Conclusion: Muyan-TTS fills a gap in accessible, adaptable, and podcast-focused TTS solutions, backed by open-source resources.

Abstract: Recent advancements in text-to-speech (TTS) models have been driven by the
integration of large language models (LLMs), enhancing semantic comprehension
and improving speech naturalness. However, existing LLM-based TTS models often
lack open-source training code and efficient inference acceleration frameworks,
limiting their accessibility and adaptability. Additionally, there is no
publicly available TTS model specifically optimized for podcast scenarios,
which are in high demand for voice interaction applications. To address these
limitations, we introduce Muyan-TTS, an open-source trainable TTS model
designed for podcast applications within a $50,000 budget. Our model is
pre-trained on over 100,000 hours of podcast audio data, enabling zero-shot TTS
synthesis with high-quality voice generation. Furthermore, Muyan-TTS supports
speaker adaptation with dozens of minutes of target speech, making it highly
customizable for individual voices. In addition to open-sourcing the model, we
provide a comprehensive data collection and processing pipeline, a full
training procedure, and an optimized inference framework that accelerates
LLM-based TTS synthesis. Our code and models are available at
https://github.com/MYZY-AI/Muyan-TTS.

</details>


### [399] [Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements](https://arxiv.org/abs/2504.19197)
*Sandipan Dhar, Nanda Dulal Jana, Swagatam Das*

Main category: cs.SD

TL;DR: A systematic review of voice conversion (VC) technologies, focusing on GAN-based approaches, their challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for high-quality synthetic voices in applications like movie dubbing and speech rehabilitation drives the need for advanced VC techniques.

Method: The paper categorizes and analyzes existing VC methods, particularly GAN-based approaches, and evaluates their technical challenges and recent advancements.

Result: GAN-based VC shows promise but faces issues like training stability, linguistic consistency, and perceptual naturalness. The review consolidates research to guide future work.

Conclusion: This survey serves as a resource to advance VC technology by identifying gaps and proposing directions for more robust systems.

Abstract: Voice conversion (VC) stands as a crucial research area in speech synthesis,
enabling the transformation of a speaker's vocal characteristics to resemble
another while preserving the linguistic content. This technology has broad
applications, including automated movie dubbing, speech-to-singing conversion,
and assistive devices for pathological speech rehabilitation. With the
increasing demand for high-quality and natural-sounding synthetic voices,
researchers have developed a wide range of VC techniques. Among these,
generative adversarial network (GAN)-based approaches have drawn considerable
attention for their powerful feature-mapping capabilities and potential to
produce highly realistic speech. Despite notable advancements, challenges such
as ensuring training stability, maintaining linguistic consistency, and
achieving perceptual naturalness continue to hinder progress in GAN-based VC
systems. This systematic review presents a comprehensive analysis of the voice
conversion landscape, highlighting key techniques, key challenges, and the
transformative impact of GANs in the field. The survey categorizes existing
methods, examines technical obstacles, and critically evaluates recent
developments in GAN-based VC. By consolidating and synthesizing research
findings scattered across the literature, this review provides a structured
understanding of the strengths and limitations of different approaches. The
significance of this survey lies in its ability to guide future research by
identifying existing gaps, proposing potential directions, and offering
insights for building more robust and efficient VC systems. Overall, this work
serves as an essential resource for researchers, developers, and practitioners
aiming to advance the state-of-the-art (SOTA) in voice conversion technology.

</details>


### [400] [Variable Bitrate Residual Vector Quantization for Audio Coding](https://arxiv.org/abs/2410.06016)
*Yunkee Chae, Woosung Choi, Yuhta Takida, Junghyun Koo, Yukara Ikemiya, Zhi Zhong, Kin Wai Cheuk, Marco A. Martínez-Ramírez, Kyogu Lee, Wei-Hsiang Liao, Yuki Mitsufuji*

Main category: cs.SD

TL;DR: Proposes VRVQ for adaptive codebook usage in audio compression, improving rate-distortion tradeoff, and introduces a gradient estimation method for better training.


<details>
  <summary>Details</summary>
Motivation: Fixed codebook usage in RVQ is suboptimal for simple audio (e.g., silence), prompting adaptive solutions.

Method: Introduces VRVQ for variable bitrate coding and a gradient estimation technique for non-differentiable masking.

Result: VRVQ outperforms baselines and enhances state-of-the-art codecs.

Conclusion: Adaptive codebook usage and improved training methods advance audio compression efficiency.

Abstract: Recent state-of-the-art neural audio compression models have progressively
adopted residual vector quantization (RVQ). Despite this success, these models
employ a fixed number of codebooks per frame, which can be suboptimal in terms
of rate-distortion tradeoff, particularly in scenarios with simple input audio,
such as silence. To address this limitation, we propose variable bitrate RVQ
(VRVQ) for audio codecs, which allows for more efficient coding by adapting the
number of codebooks used per frame. Furthermore, we propose a gradient
estimation method for the non-differentiable masking operation that transforms
from the importance map to the binary importance mask, improving model training
via a straight-through estimator. We demonstrate that the proposed training
framework achieves superior results compared to the baseline method and shows
further improvement when applied to the current state-of-the-art codec.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [401] [Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review](https://arxiv.org/abs/2504.18544)
*Nazia Nafis, Inaki Esnaola, Alvaro Martinez-Perez, Maria-Cruz Villa-Uriol, Venet Osmani*

Main category: cs.LG

TL;DR: The paper highlights challenges in evaluating synthetic health data and proposes guidelines for better generation and evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of consensus and rigor in evaluating synthetic health data, ensuring its reliability and appropriate use.

Method: Systematic review of 1766 papers, with detailed analysis of 101, to identify key challenges in synthetic data evaluation.

Result: Identified challenges include inconsistent evaluation methods, misuse of metrics, lack of domain expert input, poor dataset reporting, and low reproducibility.

Conclusion: Proposes guidelines to improve synthetic data generation and evaluation, aiming to unlock its transformative potential.

Abstract: Generating synthetic tabular data can be challenging, however evaluation of
their quality is just as challenging, if not more. This systematic review sheds
light on the critical importance of rigorous evaluation of synthetic health
data to ensure reliability, relevance, and their appropriate use. Based on
screening of 1766 papers and a detailed review of 101 papers we identified key
challenges, including lack of consensus on evaluation methods, improper use of
evaluation metrics, limited input from domain experts, inadequate reporting of
dataset characteristics, and limited reproducibility of results. In response,
we provide several guidelines on the generation and evaluation of synthetic
data, to allow the community to unlock and fully harness the transformative
potential of synthetic data and accelerate innovation.

</details>


### [402] [Low-Bit Integerization of Vision Transformers using Operand Reodering for Efficient Hardware](https://arxiv.org/abs/2504.18547)
*Ching-Yi Lin, Sahil Shah*

Main category: cs.LG

TL;DR: The paper proposes an integerization process for pre-trained vision transformers to reduce computational overhead by delaying dequantization until after matrix operations, enabling efficient low-bit inference.


<details>
  <summary>Details</summary>
Motivation: Pre-trained vision transformers face high computational and memory costs, even with quantization, due to dequantization overhead before matrix operations.

Method: The method involves analyzing the computation graph and reordering operations to delay dequantization, allowing integerized matrix multiplication and linear modules.

Result: Experiments on a systolic array-based hardware show reduced per-PE power consumption for linear layers and matrix multiplication.

Conclusion: The approach bridges the gap between quantized models and efficient inference, validating its effectiveness for low-bit inference.

Abstract: Pre-trained vision transformers have achieved remarkable performance across
various visual tasks but suffer from expensive computational and memory costs.
While model quantization reduces memory usage by lowering precision, these
models still incur significant computational overhead due to the dequantization
before matrix operations. In this work, we analyze the computation graph and
propose an integerization process based on operation reordering. Specifically,
the process delays dequantization until after matrix operations. This enables
integerized matrix multiplication and linear module by directly processing the
quantized input. To validate our approach, we synthesize the self-attention
module of ViT on a systolic array-based hardware. Experimental results show
that our low-bit inference reduces per-PE power consumption for linear layer
and matrix multiplication, bridging the gap between quantized models and
efficient inference.

</details>


### [403] [RDI: An adversarial robustness evaluation metric for deep neural networks based on sample clustering features](https://arxiv.org/abs/2504.18556)
*Jialei Song, Xingquan Zuo, Feiyang Wang, Hai Huang, Tianle Zhang*

Main category: cs.LG

TL;DR: Proposes Robustness Difference Index (RDI), a novel metric for evaluating adversarial robustness in DNNs, based on clustering features, offering attack-independence and high efficiency.


<details>
  <summary>Details</summary>
Motivation: Current adversarial robustness evaluation methods are either attack-dependent/time-consuming (attack-based) or hard to implement (certified robustness). Existing boundary-based methods lack accuracy.

Method: RDI analyzes intra-class and inter-class distances of feature vectors separated by the decision boundary, inspired by clustering evaluation.

Result: RDI correlates strongly with attack success rate (ASR) and is computationally efficient (1/30 time of PGD-based evaluation).

Conclusion: RDI provides an effective, attack-independent, and efficient alternative for adversarial robustness evaluation.

Abstract: Deep neural networks (DNNs) are highly susceptible to adversarial samples,
raising concerns about their reliability in safety-critical tasks. Currently,
methods of evaluating adversarial robustness are primarily categorized into
attack-based and certified robustness evaluation approaches. The former not
only relies on specific attack algorithms but also is highly time-consuming,
while the latter due to its analytical nature, is typically difficult to
implement for large and complex models. A few studies evaluate model robustness
based on the model's decision boundary, but they suffer from low evaluation
accuracy. To address the aforementioned issues, we propose a novel adversarial
robustness evaluation metric, Robustness Difference Index (RDI), which is based
on sample clustering features. RDI draws inspiration from clustering evaluation
by analyzing the intra-class and inter-class distances of feature vectors
separated by the decision boundary to quantify model robustness. It is
attack-independent and has high computational efficiency. Experiments show
that, RDI demonstrates a stronger correlation with the gold-standard
adversarial robustness metric of attack success rate (ASR). The average
computation time of RDI is only 1/30 of the evaluation method based on the PGD
attack. Our open-source code is available at:
https://anonymous.4open.science/r/RDI-B1DA.

</details>


### [404] [Deep Learning with Pretrained 'Internal World' Layers: A Gemma 3-Based Modular Architecture for Wildfire Prediction](https://arxiv.org/abs/2504.18562)
*Ayoub Jadouli, Chaker El Amrani*

Main category: cs.LG

TL;DR: The paper introduces a modular architecture using Gemma 3's frozen Transformer layers for wildfire prediction, improving accuracy and robustness with fewer trainable parameters.


<details>
  <summary>Details</summary>
Motivation: To leverage the rich internal representations of large Transformers (like Gemma 3) for wildfire prediction while avoiding overfitting on limited data.

Method: A custom feed-forward module transforms tabular wildfire features into Gemma 3's hidden dimensions, freezing its mid-layers and training only smaller input/output networks.

Result: Improved predictive accuracy and robustness on a Moroccan wildfire dataset compared to baseline models, with frozen Transformer layers enhancing representations.

Conclusion: Strategic reuse of pretrained Transformer mid-layers enables data-efficient and interpretable solutions for environmental applications like wildfire risk management.

Abstract: Deep learning models, especially large Transformers, carry substantial
"memory" in their intermediate layers -- an \emph{internal world} that encodes
a wealth of relational and contextual knowledge. This work harnesses that
internal world for wildfire occurrence prediction by introducing a modular
architecture built upon Gemma 3, a state-of-the-art multimodal model. Rather
than relying on Gemma 3's original embedding and positional encoding stacks, we
develop a custom feed-forward module that transforms tabular wildfire features
into the hidden dimension required by Gemma 3's mid-layer Transformer blocks.
We freeze these Gemma 3 sub-layers -- thus preserving their pretrained
representation power -- while training only the smaller input and output
networks. This approach minimizes the number of trainable parameters and
reduces the risk of overfitting on limited wildfire data, yet retains the
benefits of Gemma 3's broad knowledge. Evaluations on a Moroccan wildfire
dataset demonstrate improved predictive accuracy and robustness compared to
standard feed-forward and convolutional baselines. Ablation studies confirm
that the frozen Transformer layers consistently contribute to better
representations, underscoring the feasibility of reusing large-model mid-layers
as a learned internal world. Our findings suggest that strategic modular reuse
of pretrained Transformers can enable more data-efficient and interpretable
solutions for critical environmental applications such as wildfire risk
management.

</details>


### [405] [Unsupervised outlier detection to improve bird audio dataset labels](https://arxiv.org/abs/2504.18650)
*Bruce Collins*

Main category: cs.LG

TL;DR: A cleaning process using audio preprocessing, dimensionality reduction, and unsupervised outlier detection is proposed to reduce label noise in bird species datasets from Xeno-Canto. Performance varies across species.


<details>
  <summary>Details</summary>
Motivation: The Xeno-Canto repository is valuable for bird vocalization research but suffers from label noise due to non-target sounds in recordings.

Method: Audio preprocessing, dimensionality reduction (convolutional autoencoders and VaDE), and unsupervised outlier detection are used to clean datasets.

Result: The methods reduce label noise but show varying effectiveness across different bird species.

Conclusion: The cleaning process can meaningfully reduce label noise, though results are species-dependent.

Abstract: The Xeno-Canto bird audio repository is an invaluable resource for those
interested in vocalizations and other sounds made by birds around the world.
This is particularly the case for machine learning researchers attempting to
improve on the bird species recognition accuracy of classification models.
However, the task of extracting labeled datasets from the recordings found in
this crowd-sourced repository faces several challenges. One challenge of
particular significance to machine learning practitioners is that one bird
species label is applied to each audio recording, but frequently other sounds
are also captured including other bird species, other animal sounds,
anthropogenic and other ambient sounds. These non-target bird species sounds
can result in dataset labeling discrepancies referred to as label noise. In
this work we present a cleaning process consisting of audio preprocessing
followed by dimensionality reduction and unsupervised outlier detection (UOD)
to reduce the label noise in a dataset derived from Xeno-Canto recordings. We
investigate three neural network dimensionality reduction techniques: two
flavors of convolutional autoencoders and variational deep embedding (VaDE
(Jiang, 2017)). While both methods show some degree of effectiveness at
detecting outliers for most bird species datasets, we found significant
variation in the performance of the methods from one species to the next. We
believe that the results of this investigation demonstrate that the application
of our cleaning process can meaningfully reduce the label noise of bird species
datasets derived from Xeno-Canto audio repository but results vary across
species.

</details>


### [406] [Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism](https://arxiv.org/abs/2504.18574)
*Aviv Bick, Eric Xing, Albert Gu*

Main category: cs.LG

TL;DR: The paper examines in-context retrieval in Transformer- and SSM-based models, identifying a shared Gather-and-Aggregate (G&A) mechanism critical for performance. Disabling G&A heads significantly impacts tasks like MMLU, revealing retrieval challenges in SSMs due to smoother attention patterns. Hybrid models or attention-based replacements can improve retrieval.


<details>
  <summary>Details</summary>
Motivation: To understand how in-context retrieval operates in Transformer- and SSM-based models and identify performance bottlenecks.

Method: Analyze the Gather-and-Aggregate (G&A) mechanism in both architectures, test its impact on tasks like MMLU, and explore hybrid solutions.

Result: G&A is concentrated in few heads, and its disruption degrades performance. SSMs struggle with retrieval due to smoother attention patterns, but attention-based replacements can help.

Conclusion: The gap between Transformers and SSMs in retrieval is confined to a few heads, suggesting hybrid models or targeted improvements can bridge the performance difference.

Abstract: SSMs offer efficient processing of long sequences with fixed state sizes, but
struggle with algorithmic tasks like retrieving past context. In this work, we
examine how such in-context retrieval operates within Transformer- and
SSM-based language models. We find that both architectures develop the same
fundamental Gather-and-Aggregate (G&A) mechanism. A Gather Head first
identifies and extracts relevant information from the context, which an
Aggregate Head then integrates into a final representation. Across both model
types, G&A concentrates in just a few heads, making them critical bottlenecks
even for benchmarks that require a basic form of retrieval. For example,
disabling a single Gather or Aggregate Head of a pruned Llama-3.1-8B degrades
its ability to retrieve the correct answer letter in MMLU, reducing accuracy
from 66% to 25%. This finding suggests that in-context retrieval can obscure
the limited knowledge demands of certain tasks. Despite strong MMLU performance
with retrieval intact, the pruned model fails on other knowledge tests. Similar
G&A dependencies exist in GSM8K, BBH, and dialogue tasks. Given the
significance of G&A in performance, we show that retrieval challenges in SSMs
manifest in how they implement G&A, leading to smoother attention patterns
rather than the sharp token transitions that effective G&A relies on. Thus,
while a gap exists between Transformers and SSMs in implementing in-context
retrieval, it is confined to a few heads, not the entire model. This insight
suggests a unified explanation for performance differences between Transformers
and SSMs while also highlighting ways to combine their strengths. For example,
in pretrained hybrid models, attention components naturally take on the role of
Aggregate Heads. Similarly, in a pretrained pure SSM, replacing a single G&A
head with an attention-based variant significantly improves retrieval.

</details>


### [407] [An Artificial Intelligence-Based Framework for Predicting Emergency Department Overcrowding: Development and Evaluation Study](https://arxiv.org/abs/2504.18578)
*Orhun Vural, Bunyamin Ozaydin, Khalid Y. Aram, James Booth, Brittany F. Lindsey, Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: Machine learning models predict ED waiting room occupancy at hourly and daily scales to improve resource planning and reduce overcrowding.


<details>
  <summary>Details</summary>
Motivation: Emergency department overcrowding causes delays and operational strain; proactive forecasting can enhance efficiency.

Method: Used data from a hospital ED, tested 11 ML algorithms, optimized features, and evaluated performance across patient volumes and times.

Result: TSiTPlus (hourly) and XCMPlus (daily) performed best with low MAE/MSE, showing accurate predictions even in extreme cases.

Conclusion: The models effectively forecast ED occupancy, aiding proactive resource allocation and reducing overcrowding.

Abstract: Background: Emergency department (ED) overcrowding remains a major challenge,
causing delays in care and increased operational strain. Hospital management
often reacts to congestion after it occurs. Machine learning predictive
modeling offers a proactive approach by forecasting patient flow metrics, such
as waiting count, to improve resource planning and hospital efficiency.
  Objective: This study develops machine learning models to predict ED waiting
room occupancy at two time scales. The hourly model forecasts the waiting count
six hours ahead (e.g., a 1 PM prediction for 7 PM), while the daily model
estimates the average waiting count for the next 24 hours (e.g., a 5 PM
prediction for the following day's average). These tools support staffing
decisions and enable earlier interventions to reduce overcrowding.
  Methods: Data from a partner hospital's ED in the southeastern United States
were used, integrating internal metrics and external features. Eleven machine
learning algorithms, including traditional and deep learning models, were
trained and evaluated. Feature combinations were optimized, and performance was
assessed across varying patient volumes and hours.
  Results: TSiTPlus achieved the best hourly prediction (MAE: 4.19, MSE:
29.32). The mean hourly waiting count was 18.11, with a standard deviation of
9.77. Accuracy varied by hour, with MAEs ranging from 2.45 (11 PM) to 5.45 (8
PM). Extreme case analysis at one, two, and three standard deviations above the
mean showed MAEs of 6.16, 10.16, and 15.59, respectively. For daily
predictions, XCMPlus performed best (MAE: 2.00, MSE: 6.64), with a daily mean
of 18.11 and standard deviation of 4.51.
  Conclusions: These models accurately forecast ED waiting room occupancy and
support proactive resource allocation. Their implementation has the potential
to improve patient flow and reduce overcrowding in emergency care settings.

</details>


### [408] [ZipR1: Reinforcing Token Sparsity in MLLMs](https://arxiv.org/abs/2504.18579)
*Feng Chen, Yefei He, Lequan Lin, Jing Liu, Bohan Zhuang, Qi Wu*

Main category: cs.LG

TL;DR: ZipR1 is an RL-based post-training method that optimizes token sparsity in MLLMs to balance efficiency and performance, reducing token ratio significantly with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the under-explored challenge of actively encouraging token sparsity in MLLMs to improve computational efficiency during inference without compromising performance.

Method: Proposes ZipR1, a reinforcement learning (RL)-based approach that treats token reduction ratio as an efficiency reward and answer accuracy as a performance reward, optimizing the tradeoff.

Result: ZipR1 reduces the token ratio from 80% to 25% with minimal accuracy reduction on 13 benchmarks for Qwen2/2.5-VL models.

Conclusion: ZipR1 effectively alleviates computation and memory bottlenecks by directly optimizing the efficiency-performance tradeoff during inference.

Abstract: Sparse attention mechanisms aim to reduce computational overhead by
selectively processing a subset of salient tokens while preserving model
performance. Despite the effectiveness of such designs, how to actively
encourage token sparsity of well-posed MLLMs remains under-explored, which
fundamentally limits the achievable acceleration effect during inference. In
this paper, we propose a simple RL-based post-training method named
\textbf{ZipR1} that treats the token reduction ratio as the efficiency reward
and answer accuracy as the performance reward.
  In this way, our method can jointly alleviate the computation and memory
bottlenecks via directly optimizing the inference-consistent
efficiency-performance tradeoff. Experimental results demonstrate that ZipR1
can reduce the token ratio of Qwen2/2.5-VL from 80\% to 25\% with a minimal
accuracy reduction on 13 image and video benchmarks.

</details>


### [409] [Parameter-Efficient Checkpoint Merging via Metrics-Weighted Averaging](https://arxiv.org/abs/2504.18580)
*Shi Jie Yu, Sehyun Choi*

Main category: cs.LG

TL;DR: Checkpoint merging in PEFT using Metrics-Weighted Averaging (MWA) improves model performance by weighting parameters based on metrics like loss or training steps, outperforming naive averaging.


<details>
  <summary>Details</summary>
Motivation: To enhance parameter-efficient fine-tuning (PEFT) by combining model checkpoints more effectively, reducing training time and improving performance.

Method: Proposes Metrics-Weighted Averaging (MWA), weighting checkpoints by performance metrics (e.g., loss, training steps) with a penalty factor for balanced merging.

Result: MWA outperforms uniform averaging, with loss-weighted merging achieving up to 5% higher accuracy, even surpassing individual checkpoints.

Conclusion: Checkpoint merging with MWA is validated for PEFT, showing metric-driven weighting efficiently boosts performance with minimal overhead.

Abstract: Checkpoint merging is a technique for combining multiple model snapshots into
a single superior model, potentially reducing training time for large language
models. This paper explores checkpoint merging in the context of
parameter-efficient fine-tuning (PEFT), where only small adapter modules (e.g.
LoRA) are trained. We propose Metrics-Weighted Averaging (MWA), a simple yet
effective method to merge model checkpoints by weighting their parameters
according to performance metrics. In particular, we investigate weighting by
training loss and by training steps, under the intuition that lower-loss or
later-step checkpoints are more valuable. We introduce a formula with a penalty
factor to adjust weight distribution, requiring only one hyperparameter
regardless of the number of checkpoints. Experiments on three fine-tuning tasks
(mathematical reasoning, preference alignment, and general instruction tuning)
show that MWA consistently produces merged models that outperform the naive
uniform average of checkpoints. Notably, loss-weighted merging often yields the
best results, delivering up to 5% higher task accuracy than the baseline
uniform merge and even surpassing the final individual checkpoint's
performance. These findings validate checkpoint merging for PEFT and
demonstrate that a metric-driven weighting heuristic can efficiently boost
model performance with minimal computational overhead.

</details>


### [410] [PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation](https://arxiv.org/abs/2504.18583)
*Zihao An, Huajun Bai, Ziqiong Liu, Dong Li, Emad Barsoum*

Main category: cs.LG

TL;DR: PARD introduces a parallel speculative decoding method to speed up LLM inference by predicting multiple tokens in one pass, reducing training costs and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs are slow due to single-token generation and memory bottlenecks. Speculative decoding helps but has draft-phase overhead and high training costs.

Method: PARD uses parallel draft models to predict multiple tokens in one forward pass, with a conditional drop token method for faster training and target-independence for broader applicability.

Result: PARD improves training efficiency by 3x and accelerates LLaMA3.1-8B inference by 4.08x, achieving 311.5 tokens per second.

Conclusion: PARD is an efficient, adaptable solution for speeding up LLM inference with minimal adaptation costs.

Abstract: The autoregressive nature of large language models (LLMs) limits inference
speed. Each forward pass generates only a single token and is often
bottlenecked by memory bandwidth. Speculative decoding alleviates this issue
using a draft-then-verify approach to accelerate token generation. However, the
overhead introduced during the draft phase and the training cost of the draft
model limit the efficiency and adaptability of speculative decoding. In this
work, we introduce PARallel Draft (PARD), a novel speculative decoding method
that enables low-cost adaptation of autoregressive draft models into parallel
draft models. PARD enhances inference efficiency by predicting multiple future
tokens in a single forward pass of the draft phase, and incorporates a
conditional drop token method to accelerate training. Its target-independence
property allows a single draft model to be applied to an entire family of
different models, minimizing the adaptation cost. Our proposed conditional drop
token method can improves draft model training efficiency by 3x. On our
optimized inference framework, PARD accelerates LLaMA3.1-8B inference by 4.08x,
achieving 311.5 tokens per second.

</details>


### [411] [Training Large Language Models to Reason via EM Policy Gradient](https://arxiv.org/abs/2504.18587)
*Tianbing Xu*

Main category: cs.LG

TL;DR: EM Policy Gradient is a new off-policy RL algorithm for enhancing LLM reasoning, outperforming GRPO in simplicity and performance on GSM8K and MATH (HARD) datasets.


<details>
  <summary>Details</summary>
Motivation: To improve LLM reasoning by simplifying and optimizing reinforcement learning methods, avoiding complexities like importance weights and clipping.

Method: Frames reasoning as an EM optimization problem, alternating between sampling rationale trajectories and reward-guided fine-tuning.

Result: Achieves comparable or better performance than GRPO, with added scalability and simplicity, and exhibits cognitive behaviors like sub-problem decomposition.

Conclusion: EM Policy Gradient enhances LLM reasoning interpretability and robustness, offering a simpler yet effective alternative to existing methods.

Abstract: Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's
R1, have demonstrated strong reasoning capacities and problem-solving skills
acquired through large-scale reinforcement learning (RL), with wide
applications in mathematics, coding, science, intelligent agents, and virtual
assistants. In this work, we introduce an off-policy reinforcement learning
algorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing
expected return over reasoning trajectories. We frame the reasoning task as an
Expectation-Maximization (EM) optimization problem, alternating between
sampling diverse rationale trajectories and performing reward-guided
fine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and
heuristic clipping, our method provides a simpler, more principled off-policy
policy gradient approach, eliminating these complexities while maintaining
strong performance. We evaluate the effectiveness of EM Policy Gradient on the
GSM8K and MATH (HARD) datasets, where it achieves performance comparable to or
slightly surpassing the state-of-the-art GRPO, while offering additional
advantages in scalability, simplicity, and reasoning conciseness. Moreover,
models fine-tuned with our method exhibit cognitive behaviors, such as
sub-problem decomposition, self-verification, and backtracking, highlighting
its potential to enhance both the interpretability and robustness of LLM
reasoning.

</details>


### [412] [Dynamic QoS Prediction via a Non-Negative Tensor Snowflake Factorization](https://arxiv.org/abs/2504.18588)
*YongHui Xia, Lan Wang, Hao Wu*

Main category: cs.LG

TL;DR: A Non-negative Snowflake Factorization model is proposed to predict missing QoS data by capturing dynamic user-service interaction patterns.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unobserved QoS data affecting service choices as user and service numbers grow.

Method: Uses a snowflake core tensor and SLF-NMUT for parameter learning to enhance pattern learning.

Result: Empirical results show improved accuracy in predicting missing QoS data.

Conclusion: The model effectively learns dynamic interactions for better QoS predictions.

Abstract: Dynamic quality of service (QoS) data exhibit rich temporal patterns in
user-service interactions, which are crucial for a comprehensive understanding
of user behavior and service conditions in Web service. As the number of users
and services increases, there is a large amount of unobserved QoS data, which
significantly affects users'choice of services. To predict unobserved QoS data,
we propose a Non-negative Snowflake Factorization of tensors model. This method
designs a snowflake core tensor to enhance the model's learning capability.
Additionally, it employs a single latent factor-based, nonnegative
multiplication update on tensor (SLF-NMUT) for parameter learning. Empirical
results demonstrate that the proposed model more accurately learns dynamic
user-service interaction patterns, thereby yielding improved predictions for
missing QoS data.

</details>


### [413] [A multilevel approach to accelerate the training of Transformers](https://arxiv.org/abs/2504.18590)
*Guillaume Lauga, Maël Chaumette, Edgar Desainte-Maréville, Étienne Lasalle, Arthur Lebeurrier*

Main category: cs.LG

TL;DR: The paper explores multilevel methods to speed up transformer training by leveraging an ODE interpretation and adaptive discretization.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency in training transformer architectures by introducing a multilevel approach.

Method: Proposes varying the discretization of ODE Transformers based on an ODE interpretation to optimize training.

Result: Experimental validation shows the method accelerates training compared to standard procedures.

Conclusion: Multilevel approaches with adaptive discretization can effectively reduce transformer training time.

Abstract: In this article, we investigate the potential of multilevel approaches to
accelerate the training of transformer architectures. Using an ordinary
differential equation (ODE) interpretation of these architectures, we propose
an appropriate way of varying the discretization of these ODE Transformers in
order to accelerate the training. We validate our approach experimentally by a
comparison with the standard training procedure.

</details>


### [414] [Theoretical Framework for Tempered Fractional Gradient Descent: Application to Breast Cancer Classification](https://arxiv.org/abs/2504.18849)
*Omar Naifar*

Main category: cs.LG

TL;DR: TFGD combines fractional calculus and exponential tempering to improve gradient descent, offering faster convergence and better accuracy than SGD, especially in noisy or high-dimensional tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient descent struggles with oscillatory updates and slow convergence in noisy, high-dimensional landscapes. TFGD aims to overcome these issues.

Method: TFGD uses a tempered memory mechanism, weighting historical gradients with fractional coefficients and exponential decay. It maintains SGD's time complexity while adding minimal memory overhead.

Result: TFGD achieves 98.25% test accuracy (vs. 92.11% for SGD) and converges twice as fast, with theoretical guarantees for convex and stochastic settings.

Conclusion: TFGD is a robust alternative to traditional optimizers, particularly effective in tasks like medical classification where stable gradient averaging is beneficial.

Abstract: This paper introduces Tempered Fractional Gradient Descent (TFGD), a novel
optimization framework that synergizes fractional calculus with exponential
tempering to enhance gradient-based learning. Traditional gradient descent
methods often suffer from oscillatory updates and slow convergence in
high-dimensional, noisy landscapes. TFGD addresses these limitations by
incorporating a tempered memory mechanism, where historical gradients are
weighted by fractional coefficients $|w_j| = \binom{\alpha}{j}$ and
exponentially decayed via a tempering parameter $\lambda$. Theoretical analysis
establishes TFGD's convergence guarantees: in convex settings, it achieves an
$\mathcal{O}(1/K)$ rate with alignment coefficient $d_{\alpha,\lambda} = (1 -
e^{-\lambda})^{-\alpha}$, while stochastic variants attain
$\mathcal{O}(1/k^\alpha)$ error decay. The algorithm maintains $\mathcal{O}(n)$
time complexity equivalent to SGD, with memory overhead scaling as
$\mathcal{O}(d/\lambda)$ for parameter dimension $d$. Empirical validation on
the Breast Cancer Wisconsin dataset demonstrates TFGD's superiority, achieving
98.25\% test accuracy (vs. 92.11\% for SGD) and 2$\times$ faster convergence.
The tempered memory mechanism proves particularly effective in medical
classification tasks, where feature correlations benefit from stable gradient
averaging. These results position TFGD as a robust alternative to conventional
optimizers in both theoretical and applied machine learning.

</details>


### [415] [Distributed Multi-Task Learning for Stochastic Bandits with Context Distribution and Stage-wise Constraints](https://arxiv.org/abs/2401.11563)
*Jiabin Lin, Shana Moothedath*

Main category: cs.LG

TL;DR: The paper introduces conservative distributed multi-task learning for stochastic linear contextual bandits with heterogeneous agents, proposing two algorithms (DiSC-UCB and DiSC-UCB2) to handle performance constraints and unknown baseline rewards, with theoretical and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of distributed multi-task learning in contextual bandits where agents face related tasks but must meet performance constraints, especially when only a context distribution is available.

Method: Proposes DiSC-UCB and DiSC-UCB2 algorithms, which use pruned action sets and synchronized sharing of estimates via a central server to ensure constraints and handle unknown baseline rewards.

Result: Theoretical regret and communication bounds are proven, and empirical validation on synthetic and Movielens-100K data confirms performance.

Conclusion: The algorithms effectively balance task performance and constraints in distributed settings, with DiSC-UCB2 extending applicability to unknown baseline rewards.

Abstract: We present conservative distributed multi-task learning in stochastic linear
contextual bandits with heterogeneous agents. This extends conservative linear
bandits to a distributed setting where M agents tackle different but related
tasks while adhering to stage-wise performance constraints. The exact context
is unknown, and only a context distribution is available to the agents as in
many practical applications that involve a prediction mechanism to infer
context, such as stock market prediction and weather forecast. We propose a
distributed upper confidence bound (UCB) algorithm, DiSC-UCB. Our algorithm
constructs a pruned action set during each round to ensure the constraints are
met. Additionally, it includes synchronized sharing of estimates among agents
via a central server using well-structured synchronization steps. We prove the
regret and communication bounds on the algorithm. We extend the problem to a
setting where the agents are unaware of the baseline reward. For this setting,
we provide a modified algorithm, DiSC-UCB2, and we show that the modified
algorithm achieves the same regret and communication bounds. We empirically
validated the performance of our algorithm on synthetic data and real-world
Movielens-100K data.

</details>


### [416] [Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations](https://arxiv.org/abs/2504.18591)
*Giovanni Catalani, Michael Bauerheim, Frédéric Tost, Xavier Bertrand, Joseph Morlier*

Main category: cs.LG

TL;DR: enf2enf is an encoder-decoder method for predicting steady-state PDEs with non-parameterized geometric variability, leveraging Equivariant Neural Fields for improved generalization and physical compliance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting PDE solutions on varying geometries without parameterization, enhancing generalization and capturing fine-scale features.

Method: Uses latent point cloud embeddings to encode input geometries, preserving geometric grounding, and decodes them into continuous output fields, combining local and global parameters.

Result: Superior or competitive performance on aerodynamic, hyper-elastic, and airfoil datasets, with real-time inference and zero-shot super-resolution capabilities.

Conclusion: enf2enf effectively models geometry-physics coupling, offering high accuracy and efficiency in PDE solutions for diverse geometries.

Abstract: Recent advances in Neural Fields have enabled powerful,
discretization-invariant methods for learning neural operators that approximate
solutions of Partial Differential Equations (PDEs) on general geometries.
Building on these developments, we introduce enf2enf, an encoder--decoder
methodology for predicting steady-state Partial Differential Equations with
non-parameterized geometric variability, based on recently proposed Equivariant
Neural Field architectures. In enf2enf, input geometries are encoded into
latent point cloud embeddings that inherently preserve geometric grounding and
capture local phenomena. The resulting representations are then combined with
global parameters and directly decoded into continuous output fields, thus
efficiently modeling the coupling between geometry and physics. By leveraging
the inductive biases of locality and translation invariance, our approach is
able to capture fine-scale physical features as well as complex shape
variations, thereby enhancing generalization and physical compliance. Extensive
experiments on a high-fidelity aerodynamic dataset, a hyper-elastic material
benchmark, and multi-element airfoil geometries, demonstrate that the proposed
model achieves superior or competitive performance compared to state-of-the-art
graph based, operator learning, and neural field methods. Notably, our method
supports real time inference and zero-shot super-resolution, enabling efficient
training on low-resolution meshes while maintaining high accuracy on full-scale
discretizations.

</details>


### [417] [Evolution of Societies via Reinforcement Learning](https://arxiv.org/abs/2410.17466)
*Yann Bouteiller, Karthik Soma, Giovanni Beltrame*

Main category: cs.LG

TL;DR: A methodology for simulating large, heterogeneous populations of MARL agents using parallelizable PG and LOLA, tested in classic games to study evolutionary dynamics.


<details>
  <summary>Details</summary>
Motivation: Current MARL applications are limited to small, homogeneous populations and are computationally intensive.

Method: Fast, parallelizable implementation of PG and LOLA for stateless normal-form games with random pairwise interactions.

Result: Simulated 200,000 agents in Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors, revealing insights into population evolution under naive and advanced MARL rules.

Conclusion: LOLA influences social evolution in distinct ways, demonstrating the potential of scalable MARL simulations.

Abstract: The universe involves many independent co-learning agents as an ever-evolving
part of our observed environment. Yet, in practice, Multi-Agent Reinforcement
Learning (MARL) applications are typically constrained to small, homogeneous
populations and remain computationally intensive. We propose a methodology that
enables simulating populations of Reinforcement Learning agents at evolutionary
scale. More specifically, we derive a fast, parallelizable implementation of
Policy Gradient (PG) and Opponent-Learning Awareness (LOLA), tailored for
evolutionary simulations where agents undergo random pairwise interactions in
stateless normal-form games. We demonstrate our approach by simulating the
evolution of very large populations made of heterogeneous co-learning agents,
under both naive and advanced learning strategies. In our experiments, 200,000
PG or LOLA agents evolve in the classic games of Hawk-Dove, Stag-Hunt, and
Rock-Paper-Scissors. Each game provides distinct insights into how populations
evolve under both naive and advanced MARL rules, including compelling ways in
which Opponent-Learning Awareness affects social evolution.

</details>


### [418] [Severity Classification of Chronic Obstructive Pulmonary Disease in Intensive Care Units: A Semi-Supervised Approach Using MIMIC-III Dataset](https://arxiv.org/abs/2504.18593)
*Akram Shojaei, Mehdi Delrobaei*

Main category: cs.LG

TL;DR: A machine learning framework for COPD severity classification in ICU settings achieved high accuracy (92.51%) and ROC AUC (0.98) using the MIMIC-III database.


<details>
  <summary>Details</summary>
Motivation: Precise severity assessment of COPD is critical for ICU management, and AI can enhance this process.

Method: Developed a robust classification model with ICU parameters (blood gas, vital signs) and semi-supervised learning for unlabeled data.

Result: Random forest classifier performed best, with 92.51% accuracy and 0.98 ROC AUC in distinguishing COPD severity.

Conclusion: The model offers a practical tool for ICU clinicians, with potential to improve decision-making and outcomes. Future work includes external validation and integration with clinical systems.

Abstract: Chronic obstructive pulmonary disease (COPD) represents a significant global
health burden, where precise severity assessment is particularly critical for
effective clinical management in intensive care unit (ICU) settings. This study
introduces an innovative machine learning framework for COPD severity
classification utilizing the MIMIC-III critical care database, thereby
expanding the applications of artificial intelligence in critical care
medicine. Our research developed a robust classification model incorporating
key ICU parameters such as blood gas measurements and vital signs, while
implementing semi-supervised learning techniques to effectively utilize
unlabeled data and enhance model performance. The random forest classifier
emerged as particularly effective, demonstrating exceptional discriminative
capability with 92.51% accuracy and 0.98 ROC AUC in differentiating between
mild-to-moderate and severe COPD cases. This machine learning approach provides
clinicians with a practical, accurate, and efficient tool for rapid COPD
severity evaluation in ICU environments, with significant potential to improve
both clinical decision-making processes and patient outcomes. Future research
directions should prioritize external validation across diverse patient
populations and integration with clinical decision support systems to optimize
COPD management in critical care settings.

</details>


### [419] [A Simple DropConnect Approach to Transfer-based Targeted Attack](https://arxiv.org/abs/2504.18594)
*Tongrui Su, Qingbin Li, Shengyu Zhu, Wei Chen, Xueqi Cheng*

Main category: cs.LG

TL;DR: The paper introduces MCD, a method to enhance transferability of adversarial attacks by mitigating perturbation co-adaptation via DropConnect, achieving higher Attack Success Rates (ASRs) compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Existing targeted transfer-based black-box attacks have low ASRs due to adversarial examples overfitting the surrogate model. The paper hypothesizes this is caused by perturbation co-adaptation.

Method: Proposes MCD, which mitigates perturbation co-adaptation by creating diverse surrogate model variants using DropConnect during optimization.

Result: MCD achieves 13% higher average ASRs in challenging scenarios (e.g., CNN to Transformer transfer) and boosts self-ensemble methods.

Conclusion: MCD effectively enhances adversarial transferability, especially in targeted attacks, and scales well with computational resources.

Abstract: We study the problem of transfer-based black-box attack, where adversarial
samples generated using a single surrogate model are directly applied to target
models. Compared with untargeted attacks, existing methods still have lower
Attack Success Rates (ASRs) in the targeted setting, i.e., the obtained
adversarial examples often overfit the surrogate model but fail to mislead
other models. In this paper, we hypothesize that the pixels or features in
these adversarial examples collaborate in a highly dependent manner to maximize
the success of an adversarial attack on the surrogate model, which we refer to
as perturbation co-adaptation. Then, we propose to Mitigate perturbation
Co-adaptation by DropConnect (MCD) to enhance transferability, by creating
diverse variants of surrogate model at each optimization iteration. We conduct
extensive experiments across various CNN- and Transformer-based models to
demonstrate the effectiveness of MCD. In the challenging scenario of
transferring from a CNN-based model to Transformer-based models, MCD achieves
13% higher average ASRs compared with state-of-the-art baselines. MCD boosts
the performance of self-ensemble methods by bringing in more diversification
across the variants while reserving sufficient semantic information for each
variant. In addition, MCD attains the highest performance gain when scaling the
compute of crafting adversarial examples.

</details>


### [420] [EnviroPiNet: A Physics-Guided AI Model for Predicting Biofilter Performance](https://arxiv.org/abs/2504.18595)
*Uzma, Fabien Cholet, Domenic Quinn, Cindy Smith, Siming You, William Sloan*

Main category: cs.LG

TL;DR: The paper introduces Buckingham Pi theory to model biofilter performance, creating the EnviroPiNet model, which outperforms traditional methods like PCA and autoencoders with an R² of 0.9236.


<details>
  <summary>Details</summary>
Motivation: Predicting biofilter performance is difficult due to sparse, high-dimensional data. The study aims to improve accuracy using physics-guided approaches.

Method: Applied Buckingham Pi theory for dimensionality reduction and developed EnviroPiNet, a physics-guided neural network, comparing it to PCA and autoencoders.

Result: EnviroPiNet achieved an R² of 0.9236, outperforming PCA and autoencoders, and provided insights into biofilter behavior.

Conclusion: Combining physical principles with AI enhances modeling of complex environmental systems with sparse data.

Abstract: Environmental biotechnologies, such as drinking water biofilters, rely on
complex interactions between microbial communities and their surrounding
physical-chemical environments. Predicting the performance of these systems is
challenging due to high-dimensional, sparse datasets that lack diversity and
fail to fully capture system behaviour. Accurate predictive models require
innovative, science-guided approaches. In this study, we present the first
application of Buckingham Pi theory to modelling biofilter performance. This
dimensionality reduction technique identifies meaningful, dimensionless
variables that enhance predictive accuracy and improve model interpretability.
Using these variables, we developed the Environmental Buckingham Pi Neural
Network (EnviroPiNet), a physics-guided model benchmarked against traditional
data-driven methods, including Principal Component Analysis (PCA) and
autoencoder neural networks. Our findings demonstrate that the EnviroPiNet
model achieves an R^2 value of 0.9236 on the testing dataset, significantly
outperforming PCA and autoencoder methods. The Buckingham Pi variables also
provide insights into the physical and chemical relationships governing
biofilter behaviour, with implications for system design and optimization. This
study highlights the potential of combining physical principles with AI
approaches to model complex environmental systems characterized by sparse,
high-dimensional datasets.

</details>


### [421] [A Hybrid Framework for Real-Time Data Drift and Anomaly Identification Using Hierarchical Temporal Memory and Statistical Tests](https://arxiv.org/abs/2504.18599)
*Subhadip Bandyopadhyay, Joy Bose, Sujoy Roy Chowdhury*

Main category: cs.LG

TL;DR: A hybrid framework combining HTM and SPRT is proposed for real-time data drift detection and anomaly identification, outperforming traditional methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Data drift reduces model relevance over time, necessitating effective detection methods. HTM's biological inspiration and online learning capabilities make it suitable for this task.

Method: Combines HTM and SPRT for drift detection and anomaly identification, with an extension to multidimensional data using multiple HTM columns and a neural network.

Result: Outperforms KS test, Wasserstein distance, and PSI in accuracy, adaptability, and computational efficiency.

Conclusion: The proposed hybrid framework is effective for real-time drift detection and anomaly identification, with potential for optimization in domains like Telecom.

Abstract: Data Drift is the phenomenon where the generating model behind the data
changes over time. Due to data drift, any model built on the past training data
becomes less relevant and inaccurate over time. Thus, detecting and controlling
for data drift is critical in machine learning models. Hierarchical Temporal
Memory (HTM) is a machine learning model developed by Jeff Hawkins, inspired by
how the human brain processes information. It is a biologically inspired model
of memory that is similar in structure to the neocortex, and whose performance
is claimed to be comparable to state of the art models in detecting anomalies
in time series data. Another unique benefit of HTMs is its independence from
training and testing cycle; all the learning takes place online with streaming
data and no separate training and testing cycle is required. In sequential
learning paradigm, Sequential Probability Ratio Test (SPRT) offers some unique
benefit for online learning and inference. This paper proposes a novel hybrid
framework combining HTM and SPRT for real-time data drift detection and anomaly
identification. Unlike existing data drift methods, our approach eliminates
frequent retraining and ensures low false positive rates. HTMs currently work
with one dimensional or univariate data. In a second study, we also propose an
application of HTM in multidimensional supervised scenario for anomaly
detection by combining the outputs of multiple HTM columns, one for each
dimension of the data, through a neural network. Experimental evaluations
demonstrate that the proposed method outperforms conventional drift detection
techniques like the Kolmogorov-Smirnov (KS) test, Wasserstein distance, and
Population Stability Index (PSI) in terms of accuracy, adaptability, and
computational efficiency. Our experiments also provide insights into optimizing
hyperparameters for real-time deployment in domains such as Telecom.

</details>


### [422] [Exploring the Potential of Latent Embeddings for Sea Ice Characterization using ICESat-2 Data](https://arxiv.org/abs/2504.18668)
*Daehyeon Han, Morteza Karimzadeh*

Main category: cs.LG

TL;DR: The study explores unsupervised autoencoders for ICESat-2 data to reduce reliance on manual labels, using LSTM and CNN models to derive embeddings, which show promise in reducing label requirements.


<details>
  <summary>Details</summary>
Motivation: Manual label collection for ICESat-2 data is time-consuming and limited by rare coincidences with background imagery, prompting the need for unsupervised methods.

Method: Developed LSTM and CNN autoencoder models to reconstruct topographic sequences and derive embeddings, followed by UMAP for dimensionality reduction and visualization.

Result: Autoencoder embeddings preserved data structure and formed more compact clusters, suggesting reduced label dependency.

Conclusion: Unsupervised autoencoders show potential to lessen manual labeling efforts for ICESat-2 data analysis.

Abstract: The Ice, Cloud, and Elevation Satellite-2 (ICESat-2) provides high-resolution
measurements of sea ice height. Recent studies have developed machine learning
methods on ICESat-2 data, primarily focusing on surface type classification.
However, the heavy reliance on manually collected labels requires significant
time and effort for supervised learning, as it involves cross-referencing track
measurements with overlapping background optical imagery. Additionally, the
coincidence of ICESat-2 tracks with background images is relatively rare due to
the different overpass patterns and atmospheric conditions. To address these
limitations, this study explores the potential of unsupervised autoencoder on
unlabeled data to derive latent embeddings. We develop autoencoder models based
on Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) to
reconstruct topographic sequences from ICESat-2 and derive embeddings. We then
apply Uniform Manifold Approximation and Projection (UMAP) to reduce dimensions
and visualize the embeddings. Our results show that embeddings from
autoencoders preserve the overall structure but generate relatively more
compact clusters compared to the original ICESat-2 data, indicating the
potential of embeddings to lessen the number of required labels samples.

</details>


### [423] [AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual Analysis](https://arxiv.org/abs/2504.19621)
*Haroui Ma, Francesco Quinzan, Theresa Willem, Stefan Bauer*

Main category: cs.LG

TL;DR: A novel statistical framework evaluates ML models in medical imaging for biases using counterfactual invariance, outperforming baselines and ensuring better generalization.


<details>
  <summary>Details</summary>
Motivation: ML systems in medical imaging risk biases affecting performance; this work aims to quantify and mitigate such biases.

Method: Combines conditional latent diffusion models with statistical hypothesis testing to measure bias without counterfactual data.

Result: Demonstrates alignment with counterfactual fairness and superior performance on synthetic and real-world datasets (CheXpert, MIMIC-CXR).

Conclusion: Provides a robust tool for bias evaluation, enhancing AI safety and generalization in healthcare diagnostics.

Abstract: Machine learning (ML) systems for medical imaging have demonstrated
remarkable diagnostic capabilities, but their susceptibility to biases poses
significant risks, since biases may negatively impact generalization
performance. In this paper, we introduce a novel statistical framework to
evaluate the dependency of medical imaging ML models on sensitive attributes,
such as demographics. Our method leverages the concept of counterfactual
invariance, measuring the extent to which a model's predictions remain
unchanged under hypothetical changes to sensitive attributes. We present a
practical algorithm that combines conditional latent diffusion models with
statistical hypothesis testing to identify and quantify such biases without
requiring direct access to counterfactual data. Through experiments on
synthetic datasets and large-scale real-world medical imaging datasets,
including \textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach
aligns closely with counterfactual fairness principles and outperforms standard
baselines. This work provides a robust tool to ensure that ML diagnostic
systems generalize well, e.g., across demographic groups, offering a critical
step towards AI safety in healthcare. Code:
https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging.

</details>


### [424] [A Unified MDL-based Binning and Tensor Factorization Framework for PDF Estimation](https://arxiv.org/abs/2504.18686)
*Mustafa Musab, Joseph K. Chege, Arie Yeredor, Martin Haardt*

Main category: cs.LG

TL;DR: A novel non-parametric method for multivariate PDF estimation using MDL-based binning and tensor factorization, addressing limitations of uniform histograms.


<details>
  <summary>Details</summary>
Motivation: Conventional density estimators like uniform histograms fail to capture local variations and smooth derivatives, limiting their effectiveness in complex, multimodal data scenarios.

Method: Utilizes MDL-based binning with quantile cuts and tensor factorization (CPD) for joint probability tensor decomposition.

Result: Demonstrated effectiveness on synthetic data and a real dry bean classification dataset.

Conclusion: The proposed method offers a reliable solution for density estimation in complex, nonuniform distributions.

Abstract: Reliable density estimation is fundamental for numerous applications in
statistics and machine learning. In many practical scenarios, data are best
modeled as mixtures of component densities that capture complex and multimodal
patterns. However, conventional density estimators based on uniform histograms
often fail to capture local variations, especially when the underlying
distribution is highly nonuniform. Furthermore, the inherent discontinuity of
histograms poses challenges for tasks requiring smooth derivatives, such as
gradient-based optimization, clustering, and nonparametric discriminant
analysis. In this work, we present a novel non-parametric approach for
multivariate probability density function (PDF) estimation that utilizes
minimum description length (MDL)-based binning with quantile cuts. Our approach
builds upon tensor factorization techniques, leveraging the canonical polyadic
decomposition (CPD) of a joint probability tensor. We demonstrate the
effectiveness of our method on synthetic data and a challenging real dry bean
classification dataset.

</details>


### [425] [Active Few-Shot Learning for Vertex Classification Starting from an Unlabeled Dataset](https://arxiv.org/abs/2504.18696)
*Felix Burr, Marcel Hoffmann, Ansgar Scherp*

Main category: cs.LG

TL;DR: The paper explores few-shot vertex classification without a class oracle, comparing prototypical and discriminative models under relaxed assumptions. Prototypical models outperform GCNs with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Labeling vertices in graph data is costly, and existing few-shot learners rely on unrealistic class oracles. This work aims to address real-world scenarios where class information is unknown.

Method: Three experiments were conducted: (1) Balanced Sampling with a class oracle, (2) Unbalanced Sampling using k-medoids clustering, and (3) Unknown Number of Classes. Models were trained by iteratively prompting human annotators.

Result: Prototypical models consistently outperformed discriminative models (e.g., GCNs) with fewer than 20 labeled samples per class. Performance drops were smaller for prototypical models when assumptions were relaxed.

Conclusion: Prototypical networks are more robust than discriminative models in few-shot vertex classification, especially when class information is uncertain or unavailable.

Abstract: Despite the ample availability of graph data, obtaining vertex labels is a
tedious and expensive task. Therefore, it is desirable to learn from a few
labeled vertices only. Existing few-shot learners assume a class oracle, which
provides labeled vertices for a desired class. However, such an oracle is not
available in a real-world setting, i.e., when drawing a vertex for labeling it
is unknown to which class the vertex belongs. Few-shot learners are often
combined with prototypical networks, while classical semi-supervised vertex
classification uses discriminative models, e.g., Graph Convolutional Networks
(GCN). In this paper, we train our models by iteratively prompting a human
annotator with vertices to annotate. We perform three experiments where we
continually relax our assumptions. First, we assume a class oracle, i.e., the
human annotator is provided with an equal number of vertices to label for each
class. We denote this as "Balanced Sampling''. In the subsequent experiment,
"Unbalanced Sampling,'' we replace the class oracle with $k$-medoids clustering
and draw vertices to label from the clusters. In the last experiment, the
"Unknown Number of Classes,'' we no longer assumed we knew the number and
distribution of classes. Our results show that prototypical models outperform
discriminative models in all experiments when fewer than $20$ samples per class
are available. While dropping the assumption of the class oracle for the
"Unbalanced Sampling'' experiment reduces the performance of the GCN by $9\%$,
the prototypical network loses only $1\%$ on average. For the "Unknown Number
of Classes'' experiment, the average performance for both models decreased
further by $1\%$.
  Source code: https://github.com/Ximsa/2023-felix-ma

</details>


### [426] [Explicit neural network classifiers for non-separable data](https://arxiv.org/abs/2504.18710)
*Patrícia Muñoz Ewald*

Main category: cs.LG

TL;DR: The paper characterizes feedforward neural networks using truncation maps and demonstrates ReLU networks' ability to separate concentric data.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize the behavior of feedforward neural networks, particularly focusing on their implementation of feature maps.

Method: Uses truncation maps to analyze a class of feedforward neural networks and applies this to ReLU networks.

Result: Shows that ReLU neural networks can implement feature maps capable of separating concentric data.

Conclusion: The study provides insights into the capabilities of feedforward neural networks, especially ReLU networks, in handling complex data structures like concentric data.

Abstract: We fully characterize a large class of feedforward neural networks in terms
of truncation maps. As an application, we show how a ReLU neural network can
implement a feature map which separates concentric data.

</details>


### [427] [Appa: Bending Weather Dynamics with Latent Diffusion Models for Global Data Assimilation](https://arxiv.org/abs/2504.18720)
*Gérôme Andry, François Rozet, Sacha Lewin, Omer Rochman, Victor Mangeleer, Matthias Pirlet, Elise Faulx, Marilaure Grégoire, Gilles Louppe*

Main category: cs.LG

TL;DR: Appa, a score-based data assimilation model, improves weather forecasting by inferring atmospheric states from observations using a unified probabilistic framework.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying current atmospheric states from vast observational data for accurate weather forecasting.

Method: Uses a 1.5B-parameter spatio-temporal latent diffusion model trained on ERA5 reanalysis data, conditioned on observations to infer plausible state trajectories.

Result: Demonstrates physical consistency, good reconstructions, and competitive forecasting skills.

Conclusion: Latent score-based data assimilation is a promising foundation for future global atmospheric modeling.

Abstract: Deep learning has transformed weather forecasting by improving both its
accuracy and computational efficiency. However, before any forecast can begin,
weather centers must identify the current atmospheric state from vast amounts
of observational data. To address this challenging problem, we introduce Appa,
a score-based data assimilation model producing global atmospheric trajectories
at 0.25-degree resolution and 1-hour intervals. Powered by a 1.5B-parameter
spatio-temporal latent diffusion model trained on ERA5 reanalysis data, Appa
can be conditioned on any type of observations to infer the posterior
distribution of plausible state trajectories, without retraining. Our unified
probabilistic framework flexibly tackles multiple inference tasks --
reanalysis, filtering, and forecasting -- using the same model, eliminating the
need for task-specific architectures or training procedures. Experiments
demonstrate physical consistency on a global scale and good reconstructions
from observations, while showing competitive forecasting skills. Our results
establish latent score-based data assimilation as a promising foundation for
future global atmospheric modeling systems.

</details>


### [428] [Multimodal graph representation learning for website generation based on visual sketch](https://arxiv.org/abs/2504.18729)
*Tung D. Vu, Chung Hoang, Truong-Son Hy*

Main category: cs.LG

TL;DR: A novel method using multimodal graph representation learning improves accuracy and efficiency in converting digital designs to functional HTML code.


<details>
  <summary>Details</summary>
Motivation: The complexity and inefficiency of traditional methods in interpreting visual and structural details of webpage designs motivate the need for a better solution.

Method: The proposed approach integrates visual and structural information from design sketches using multimodal graph representation learning.

Result: The method shows significant improvements in accuracy and efficiency for generating semantically correct and structurally sound HTML code.

Conclusion: The approach has the potential to revolutionize design-to-code automation, outperforming existing techniques.

Abstract: The Design2Code problem, which involves converting digital designs into
functional source code, is a significant challenge in software development due
to its complexity and time-consuming nature. Traditional approaches often
struggle with accurately interpreting the intricate visual details and
structural relationships inherent in webpage designs, leading to limitations in
automation and efficiency. In this paper, we propose a novel method that
leverages multimodal graph representation learning to address these challenges.
By integrating both visual and structural information from design sketches, our
approach enhances the accuracy and efficiency of code generation, particularly
in producing semantically correct and structurally sound HTML code. We present
a comprehensive evaluation of our method, demonstrating significant
improvements in both accuracy and efficiency compared to existing techniques.
Extensive evaluation demonstrates significant improvements of multimodal graph
learning over existing techniques, highlighting the potential of our method to
revolutionize design-to-code automation. Code available at
https://github.com/HySonLab/Design2Code

</details>


### [429] [TLoRA: Tri-Matrix Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2504.18735)
*Tanvir Islam*

Main category: cs.LG

TL;DR: TLoRA is a tri-matrix low-rank adaptation method for efficient parameter adaptation with minimal overhead, matching LoRA's performance using fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To improve resource efficiency in model adaptation while maintaining performance comparable to existing methods like LoRA.

Method: Decomposes weight updates into three matrices (two fixed random, one trainable) with a learnable scaling factor.

Result: Achieves comparable performance to LoRA on GLUE benchmark with fewer parameters, exhibiting stable adaptation dynamics.

Conclusion: TLoRA is an efficient and effective fine-tuning method for LLMs, advancing resource-efficient adaptation.

Abstract: We propose TLoRA, a novel tri-matrix low-rank adaptation method that
decomposes weight updates into three matrices: two fixed random matrices and
one trainable matrix, combined with a learnable, layer-wise scaling factor.
This tri-matrix design enables TLoRA to achieve highly efficient parameter
adaptation while introducing minimal additional computational overhead. Through
extensive experiments on the GLUE benchmark, we demonstrate that TLoRA achieves
comparable performance to existing low-rank methods such as LoRA and
Adapter-based techniques, while requiring significantly fewer trainable
parameters. Analyzing the adaptation dynamics, we observe that TLoRA exhibits
Gaussian-like weight distributions, stable parameter norms, and scaling factor
variability across layers, further highlighting its expressive power and
adaptability. Additionally, we show that TLoRA closely resembles LoRA in its
eigenvalue distributions, parameter norms, and cosine similarity of updates,
underscoring its ability to effectively approximate LoRA's adaptation behavior.
Our results establish TLoRA as a highly efficient and effective fine-tuning
method for LLMs, offering a significant step forward in resource-efficient
model adaptation.

</details>


### [430] [Non-Asymptotic Guarantees for Average-Reward Q-Learning with Adaptive Stepsizes](https://arxiv.org/abs/2504.18743)
*Zaiwei Chen*

Main category: cs.LG

TL;DR: Finite-time analysis of last-iterate convergence for average-reward Q-learning with adaptive stepsizes, showing O(1/k) convergence rates and necessity of adaptive stepsizes.


<details>
  <summary>Details</summary>
Motivation: To address the lack of finite-time analysis for last-iterate convergence in asynchronous average-reward Q-learning, especially with adaptive stepsizes.

Method: Uses adaptive stepsizes as local clocks, introduces a centering step, and employs non-Markovian stochastic approximation techniques with time-inhomogeneous Markovian reformulation.

Result: Proves O(1/k) convergence rates in mean-square sense for optimal relative Q-function and centered optimal relative Q-function.

Conclusion: Adaptive stepsizes are crucial for convergence, and the developed tools can aid analysis of other stochastic approximation algorithms with adaptive stepsizes.

Abstract: This work presents the first finite-time analysis for the last-iterate
convergence of average-reward Q-learning with an asynchronous implementation. A
key feature of the algorithm we study is the use of adaptive stepsizes, which
serve as local clocks for each state-action pair. We show that the iterates
generated by this Q-learning algorithm converge at a rate of $O(1/k)$ (in the
mean-square sense) to the optimal relative Q-function in the span seminorm.
Moreover, by adding a centering step to the algorithm, we further establish
pointwise mean-square convergence to a centered optimal relative Q-function,
also at a rate of $O(1/k)$. To prove these results, we show that adaptive
stepsizes are necessary, as without them, the algorithm fails to converge to
the correct target. In addition, adaptive stepsizes can be interpreted as a
form of implicit importance sampling that counteracts the effects of
asynchronous updates.
  Technically, the use of adaptive stepsizes makes each Q-learning update
depend on the entire sample history, introducing strong correlations and making
the algorithm a non-Markovian stochastic approximation (SA) scheme. Our
approach to overcoming this challenge involves (1) a time-inhomogeneous
Markovian reformulation of non-Markovian SA, and (2) a combination of
almost-sure time-varying bounds, conditioning arguments, and Markov chain
concentration inequalities to break the strong correlations between the
adaptive stepsizes and the iterates. The tools developed in this work are
likely to be broadly applicable to the analysis of general SA algorithms with
adaptive stepsizes.

</details>


### [431] [High-order Graph Neural Networks with Common Neighbor Awareness for Link Prediction](https://arxiv.org/abs/2504.18758)
*Ling Wang, Minglian Han*

Main category: cs.LG

TL;DR: HGNN-CNA improves link prediction in dynamic graphs by incorporating multi-hop common neighbor interactions into message-passing.


<details>
  <summary>Details</summary>
Motivation: Existing DGNNs neglect common neighbor interactions, limiting their link prediction performance.

Method: Proposes HGNN-CNA, which estimates correlation scores via multi-hop common neighbors and integrates them into message-passing.

Result: HGNN-CNA achieves significant accuracy gains over state-of-the-art models on three real dynamic graphs.

Conclusion: Incorporating common neighbor awareness enhances link prediction performance in dynamic graph learning.

Abstract: Link prediction is a fundamental task in dynamic graph learning (DGL),
inherently shaped by the topology of the DG. Recent advancements in dynamic
graph neural networks (DGNN), primarily by modeling the relationships among
nodes via a message passing scheme, have significantly improved link prediction
performance. However, DGNNs heavily rely on the pairwise node interactions,
which neglect the common neighbor interaction in DGL. To address this
limitation, we propose a High-order Graph Neural Networks with Common Neighbor
Awareness (HGNN-CNA) for link prediction with two-fold ideas: a) estimating
correlation score by considering multi-hop common neighbors for capturing the
complex interaction between nodes; b) fusing the correlation into the
message-passing process to consider common neighbor interaction directly in
DGL. Experimental results on three real DGs demonstrate that the proposed
HGNN-CNA acquires a significant accuracy gain over several state-of-the-art
models on the link prediction task.

</details>


### [432] [Dynamic Action Interpolation: A Universal Approach for Accelerating Reinforcement Learning with Expert Guidance](https://arxiv.org/abs/2504.18766)
*Wenjun Cao*

Main category: cs.LG

TL;DR: DAI is a simple framework for RL that interpolates expert and RL actions, improving sample efficiency without complex modifications.


<details>
  <summary>Details</summary>
Motivation: RL's sample inefficiency, especially early in training, is addressed without adding complexity.

Method: Dynamic Action Interpolation (DAI) uses a time-varying weight to blend expert and RL actions, integrating easily into Actor-Critic algorithms.

Result: DAI improves early-stage performance by 160% and final performance by 50%, with significant gains in tasks like Humanoid.

Conclusion: DAI proves sample-efficient RL doesn't require complex architectures, offering a simple yet effective solution.

Abstract: Reinforcement learning (RL) suffers from severe sample inefficiency,
especially during early training, requiring extensive environmental
interactions to perform competently. Existing methods tend to solve this by
incorporating prior knowledge, but introduce significant architectural and
implementation complexity. We propose Dynamic Action Interpolation (DAI), a
universal yet straightforward framework that interpolates expert and RL actions
via a time-varying weight $\alpha(t)$, integrating into any Actor-Critic
algorithm with just a few lines of code and without auxiliary networks or
additional losses. Our theoretical analysis shows that DAI reshapes state
visitation distributions to accelerate value function learning while preserving
convergence guarantees. Empirical evaluations across MuJoCo continuous control
tasks demonstrate that DAI improves early-stage performance by over 160\% on
average and final performance by more than 50\%, with the Humanoid task showing
a 4$\times$ improvement early on and a 2$\times$ gain at convergence. These
results challenge the assumption that complex architectural modifications are
necessary for sample-efficient reinforcement learning.

</details>


### [433] [Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications](https://arxiv.org/abs/2504.18771)
*Markus Haug, Gissel Velarde*

Main category: cs.LG

TL;DR: The paper evaluates ML models on imbalanced datasets, finding XGB and MLP outperform generative models, with IterativeImputer being complex for large datasets.


<details>
  <summary>Details</summary>
Motivation: To empirically assess the performance of various machine learning models on imbalanced datasets and compare their effectiveness.

Method: Data preparation, model training (XGB, MLP, GAN, VAE, MO-GAAL), and evaluation using 80/20 train/test split, 5-fold CV, and imputation techniques.

Result: XGB and MLP outperform generative models; IterativeImputer is comparable to simpler methods but less efficient for large datasets.

Conclusion: XGB and MLP are effective for imbalanced data, while IterativeImputer's complexity limits its practicality for large-scale use.

Abstract: This work empirically evaluates machine learning models on two imbalanced
public datasets (KDDCUP99 and Credit Card Fraud 2013). The method includes data
preparation, model training, and evaluation, using an 80/20 (train/test) split.
Models tested include eXtreme Gradient Boosting (XGB), Multi Layer Perceptron
(MLP), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and
Multiple-Objective Generative Adversarial Active Learning (MO-GAAL), with XGB
and MLP further combined with Random-Over-Sampling (ROS) and
Self-Paced-Ensemble (SPE). Evaluation involves 5-fold cross-validation and
imputation techniques (mean, median, and IterativeImputer) with 10, 20, 30, and
50 % missing data. Findings show XGB and MLP outperform generative models.
IterativeImputer results are comparable to mean and median, but not recommended
for large datasets due to increased complexity and execution time. The code
used is publicly available on GitHub (github.com/markushaug/acr-25).

</details>


### [434] [ALF: Advertiser Large Foundation Model for Multi-Modal Advertiser Understanding](https://arxiv.org/abs/2504.18785)
*Santosh Rajagopalan, Jonathan Vronsky, Songbai Yan, S. Alireza Golestaneh, Shubhra Chandra, Min Zhou*

Main category: cs.LG

TL;DR: ALF is a multi-modal transformer model for understanding advertiser behavior, achieving state-of-the-art performance in fraud detection and policy violation tasks with high precision.


<details>
  <summary>Details</summary>
Motivation: To create unified advertiser representations across text, image, video, and structured data for better understanding of advertiser intent and behavior.

Method: Uses contrastive learning, multi-task optimization, multi-modal transformations, inter-sample attention, spectrally normalized projections, and calibrated probabilistic outputs.

Result: Reduces false positives by 90% with 99.8% precision in abuse detection, outperforming existing methods.

Conclusion: ALF's innovative architecture effectively captures advertiser behavior and intent, making it highly practical for real-world deployment.

Abstract: We present ALF (Advertiser Large Foundation model), a multi-modal transformer
architecture for understanding advertiser behavior and intent across text,
image, video and structured data modalities. Through contrastive learning and
multi-task optimization, ALF creates unified advertiser representations that
capture both content and behavioral patterns. Our model achieves
state-of-the-art performance on critical tasks including fraud detection,
policy violation identification, and advertiser similarity matching. In
production deployment, ALF reduces false positives by 90% while maintaining
99.8% precision on abuse detection tasks. The architecture's effectiveness
stems from its novel combination of multi-modal transformations, inter-sample
attention mechanism, spectrally normalized projections, and calibrated
probabilistic outputs.

</details>


### [435] [Frequency-Integrated Transformer for Arbitrary-Scale Super-Resolution](https://arxiv.org/abs/2504.18818)
*Xufei Wang, Fei Ge, Jinchen Zhu, Mingjian Zhang, Qi Wu, Jifeng Ren Shizhuang Weng*

Main category: cs.LG

TL;DR: FIT, a novel network, integrates frequency information for ASSR tasks using FIM and FUSAM, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect frequency domain, limiting ASSR performance.

Method: FIT uses FIM for lossless frequency incorporation and FUSAM for spatial-frequency synergy and global context capture.

Result: FIT outperforms benchmarks, with FIM enriching details, IISA improving frequency fidelity, and FCSA capturing global context.

Conclusion: FIT effectively leverages frequency information to enhance ASSR performance.

Abstract: Methods based on implicit neural representation have demonstrated remarkable
capabilities in arbitrary-scale super-resolution (ASSR) tasks, but they neglect
the potential value of the frequency domain, leading to sub-optimal
performance. We proposes a novel network called Frequency-Integrated
Transformer (FIT) to incorporate and utilize frequency information to enhance
ASSR performance. FIT employs Frequency Incorporation Module (FIM) to introduce
frequency information in a lossless manner and Frequency Utilization
Self-Attention module (FUSAM) to efficiently leverage frequency information by
exploiting spatial-frequency interrelationship and global nature of frequency.
FIM enriches detail characterization by incorporating frequency information
through a combination of Fast Fourier Transform (FFT) with real-imaginary
mapping. In FUSAM, Interaction Implicit Self-Attention (IISA) achieves
cross-domain information synergy by interacting spatial and frequency
information in subspace, while Frequency Correlation Self-attention (FCSA)
captures the global context by computing correlation in frequency. Experimental
results demonstrate FIT yields superior performance compared to existing
methods across multiple benchmark datasets. Visual feature map proves the
superiority of FIM in enriching detail characterization. Frequency error map
validates IISA productively improve the frequency fidelity. Local attribution
map validates FCSA effectively captures global context.

</details>


### [436] [Preserving Seasonal and Trend Information: A Variational Autoencoder-Latent Space Arithmetic Based Approach for Non-stationary Learning](https://arxiv.org/abs/2504.18819)
*Hassan Wasswa, Aziida Nanyonga, Timothy Lynar*

Main category: cs.LG

TL;DR: The paper proposes a method to enforce stationary behavior in latent space while preserving trend and seasonal patterns, improving predictive performance on non-stationary data.


<details>
  <summary>Details</summary>
Motivation: Existing AI models assume stationary training environments, losing vital trend and seasonal patterns in non-stationary data. This research addresses this limitation.

Method: Uses Differencing, Time-series decomposition, and Latent Space Arithmetic (LSA) within a Variational Autoencoder (VAE) to preserve trend and seasonal info.

Result: Evaluated on two non-stationary datasets, the method preserved trend/seasonal info and improved predictive performance in deep learning models.

Conclusion: The proposed method effectively handles non-stationary data while maintaining predictive accuracy, outperforming state-of-the-art techniques.

Abstract: AI models have garnered significant research attention towards predictive
task automation. However, a stationary training environment is an underlying
assumption for most models and such models simply do not work on non-stationary
data since a stationary relationship is learned. The existing solutions propose
making data stationary prior to model training and evaluation. This leads to
loss of trend and seasonal patterns which are vital components for learning
temporal dependencies of the system under study. This research aims to address
this limitation by proposing a method for enforcing stationary behaviour within
the latent space while preserving trend and seasonal information. The method
deploys techniques including Differencing, Time-series decomposition, and
Latent Space Arithmetic (LSA), to learn information vital for efficient
approximation of trend and seasonal information which is then stored as
embeddings within the latent space of a Variational Autoencoder (VAE). The
approach's ability to preserve trend and seasonal information was evaluated on
two time-series non-stationary datasets. For predictive performance evaluation,
four deep learning models were trained on the latent vector representations of
the datasets after application of the proposed method and all models produced
competitive results in comparison with state-of-the-art techniques using RMSE
as the performance metric.

</details>


### [437] [Introducing Interval Neural Networks for Uncertainty-Aware System Identification](https://arxiv.org/abs/2504.18845)
*Mehmet Ali Ferah, Tufan Kumbasar*

Main category: cs.LG

TL;DR: The paper introduces Interval Neural Networks (INNs) for uncertainty quantification in System Identification (SysID), extending LSTM and Neural ODEs into interval-valued architectures (ILSTM and INODE) to improve reliability.


<details>
  <summary>Details</summary>
Motivation: Traditional SysID methods lack nonlinearity handling, and DL-based models lack uncertainty quantification, necessitating a framework like INNs for reliable SysID.

Method: Proposes INNs by transforming pre-trained neural network parameters into interval-valued ones, using interval arithmetic for prediction intervals. Extends LSTM and Neural ODEs into ILSTM and INODE.

Result: Demonstrates effectiveness of ILSTM and INODE in SysID experiments, with INNs generating reliable prediction intervals.

Conclusion: INNs provide a systematic approach for uncertainty quantification in SysID, enhancing reliability and safety of DL-based models.

Abstract: System Identification (SysID) is crucial for modeling and understanding
dynamical systems using experimental data. While traditional SysID methods
emphasize linear models, their inability to fully capture nonlinear dynamics
has driven the adoption of Deep Learning (DL) as a more powerful alternative.
However, the lack of uncertainty quantification (UQ) in DL-based models poses
challenges for reliability and safety, highlighting the necessity of
incorporating UQ. This paper introduces a systematic framework for constructing
and learning Interval Neural Networks (INNs) to perform UQ in SysID tasks. INNs
are derived by transforming the learnable parameters (LPs) of pre-trained
neural networks into interval-valued LPs without relying on probabilistic
assumptions. By employing interval arithmetic throughout the network, INNs can
generate Prediction Intervals (PIs) that capture target coverage effectively.
We extend Long Short-Term Memory (LSTM) and Neural Ordinary Differential
Equations (Neural ODEs) into Interval LSTM (ILSTM) and Interval NODE (INODE)
architectures, providing the mathematical foundations for their application in
SysID. To train INNs, we propose a DL framework that integrates a UQ loss
function and parameterization tricks to handle constraints arising from
interval LPs. We introduce novel concept "elasticity" for underlying
uncertainty causes and validate ILSTM and INODE in SysID experiments,
demonstrating their effectiveness.

</details>


### [438] [TSRM: A Lightweight Temporal Feature Encoding Architecture for Time Series Forecasting and Imputation](https://arxiv.org/abs/2504.18878)
*Robert Leppich, Michael Stenger, Daniel Grillmeyer, Vanessa Borst, Samuel Kounev*

Main category: cs.LG

TL;DR: TSRM is a CNN and attention-based model for time series forecasting and imputation, outperforming state-of-the-art methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To improve multivariate time series forecasting and imputation by capturing diverse temporal patterns efficiently.

Method: Uses CNN-based representation layers for independent tasks, attention-based feature extraction, and a merge layer, inspired by Transformer encoders.

Result: Outperforms benchmarks on seven datasets for both tasks while reducing parameter complexity.

Conclusion: TSRM is an effective, efficient architecture for time series tasks, with open-source availability.

Abstract: We introduce a temporal feature encoding architecture called Time Series
Representation Model (TSRM) for multivariate time series forecasting and
imputation. The architecture is structured around CNN-based representation
layers, each dedicated to an independent representation learning task and
designed to capture diverse temporal patterns, followed by an attention-based
feature extraction layer and a merge layer, designed to aggregate extracted
features. The architecture is fundamentally based on a configuration that is
inspired by a Transformer encoder, with self-attention mechanisms at its core.
The TSRM architecture outperforms state-of-the-art approaches on most of the
seven established benchmark datasets considered in our empirical evaluation for
both forecasting and imputation tasks. At the same time, it significantly
reduces complexity in the form of learnable parameters. The source code is
available at https://github.com/RobertLeppich/TSRM.

</details>


### [439] [TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis](https://arxiv.org/abs/2504.18881)
*Hangtao Zhang, Zhe Li, Kairui Zhang*

Main category: cs.LG

TL;DR: Proposes TSCAN, a two-stage context-aware uplift model (CAN-U and CAN-D) to address sample selection bias and context variability in ITE estimation, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Sample selection bias and context variability in ITE estimation limit traditional methods, necessitating a more adaptive approach.

Method: Two-stage training: CAN-U (with IPM and propensity score) generates uplift labels; CAN-D (isotonic layer) refines estimates without regularization. Context-Aware Attention Layer handles feature interactions.

Result: Validated on real-world datasets, TSCAN outperforms traditional methods and proves practical in merchant diagnosis on a major platform.

Conclusion: TSCAN effectively addresses bias and context issues, offering superior performance and real-world utility.

Abstract: A primary challenge in ITE estimation is sample selection bias. Traditional
approaches utilize treatment regularization techniques such as the Integral
Probability Metrics (IPM), re-weighting, and propensity score modeling to
mitigate this bias. However, these regularizations may introduce undesirable
information loss and limit the performance of the model. Furthermore, treatment
effects vary across different external contexts, and the existing methods are
insufficient in fully interacting with and utilizing these contextual features.
To address these issues, we propose a Context-Aware uplift model based on the
Two-Stage training approach (TSCAN), comprising CAN-U and CAN-D sub-models. In
the first stage, we train an uplift model, called CAN-U, which includes the
treatment regularizations of IPM and propensity score prediction, to generate a
complete dataset with counterfactual uplift labels. In the second stage, we
train a model named CAN-D, which utilizes an isotonic output layer to directly
model uplift effects, thereby eliminating the reliance on the regularization
components. CAN-D adaptively corrects the errors estimated by CAN-U through
reinforcing the factual samples, while avoiding the negative impacts associated
with the aforementioned regularizations. Additionally, we introduce a
Context-Aware Attention Layer throughout the two-stage process to manage the
interactions between treatment, merchant, and contextual features, thereby
modeling the varying treatment effect in different contexts. We conduct
extensive experiments on two real-world datasets to validate the effectiveness
of TSCAN. Ultimately, the deployment of our model for real-world merchant
diagnosis on one of China's largest online food ordering platforms validates
its practical utility and impact.

</details>


### [440] [SPD Learning for Covariance-Based Neuroimaging Analysis: Perspectives, Methods, and Challenges](https://arxiv.org/abs/2504.18882)
*Ce Ju, Reinmar J. Kobler, Antoine Collas, Motoaki Kawanabe, Cuntai Guan, Bertrand Thirion*

Main category: cs.LG

TL;DR: The paper reviews machine learning approaches for covariance-based neuroimaging data, focusing on SPD matrices and Riemannian metrics to enhance brain imaging analytics.


<details>
  <summary>Details</summary>
Motivation: Neuroimaging faces challenges like low signal-to-noise ratios and cross-session non-stationarity. Machine learning can improve understanding by leveraging SPD matrices and Riemannian geometry.

Method: The paper uses Riemannian metrics (e.g., affine-invariant or log-Euclidean) to analyze SPD matrices, forming a Riemannian manifold for geometric analysis.

Result: The SPD learning framework unifies methodologies to process covariance features, improving brain imaging analytics.

Conclusion: Riemannian geometry and SPD matrices offer a powerful framework for advancing neuroimaging data analysis.

Abstract: Neuroimaging provides a critical framework for characterizing brain activity
by quantifying connectivity patterns and functional architecture across
modalities. While modern machine learning has significantly advanced our
understanding of neural processing mechanisms through these datasets, decoding
task-specific signatures must contend with inherent neuroimaging constraints,
for example, low signal-to-noise ratios in raw electrophysiological recordings,
cross-session non-stationarity, and limited sample sizes. This review focuses
on machine learning approaches for covariance-based neuroimaging data, where
often symmetric positive definite (SPD) matrices under full-rank conditions
encode inter-channel relationships. By equipping the space of SPD matrices with
Riemannian metrics (e.g., affine-invariant or log-Euclidean), their space forms
a Riemannian manifold enabling geometric analysis. We unify methodologies
operating on this manifold under the SPD learning framework, which
systematically leverages the SPD manifold's geometry to process covariance
features, thereby advancing brain imaging analytics.

</details>


### [441] [Factor Analysis with Correlated Topic Model for Multi-Modal Data](https://arxiv.org/abs/2504.18914)
*Małgorzata Łazęcka, Ewa Szczurek*

Main category: cs.LG

TL;DR: FACTM is a Bayesian model combining factor analysis and topic modeling for structured data, outperforming others in clustering and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing factor analysis (FA) methods are inadequate for structured data like text or single-cell sequencing, which have clustering structures.

Method: FACTM integrates FA with correlated topic modeling, uses variational inference, and includes factor rotation for interpretability.

Result: FACTM excels in clustering structured data and integrating it with simple modalities, as shown on text, video, music, and COVID-19 datasets.

Conclusion: FACTM is a robust solution for multimodal structured data analysis, offering improved interpretability and performance.

Abstract: Integrating various data modalities brings valuable insights into underlying
phenomena. Multimodal factor analysis (FA) uncovers shared axes of variation
underlying different simple data modalities, where each sample is represented
by a vector of features. However, FA is not suited for structured data
modalities, such as text or single cell sequencing data, where multiple data
points are measured per each sample and exhibit a clustering structure. To
overcome this challenge, we introduce FACTM, a novel, multi-view and
multi-structure Bayesian model that combines FA with correlated topic modeling
and is optimized using variational inference. Additionally, we introduce a
method for rotating latent factors to enhance interpretability with respect to
binary features. On text and video benchmarks as well as real-world music and
COVID-19 datasets, we demonstrate that FACTM outperforms other methods in
identifying clusters in structured data, and integrating them with simple
modalities via the inference of shared, interpretable factors.

</details>


### [442] [Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity](https://arxiv.org/abs/2504.18929)
*Ruifeng Ren, Yong Liu*

Main category: cs.LG

TL;DR: Transformers exhibit a bias toward learning lower-entropy distributions during compression, with larger models showing stronger tendencies. The FFN module drives this bias, and dynamic sparsity reveals parameter redundancy. Training instability in larger models correlates with dead neurons.


<details>
  <summary>Details</summary>
Motivation: To understand how Transformers achieve compression and compare their learned distributions with target distributions, given the challenges of unknown target distributions and high entropy computation costs.

Method: Controlled experiments to analyze Transformers' inductive bias in compression, focusing on entropy and dynamic sparsity, particularly in attention and FFN modules.

Result: Transformers favor lower-entropy distributions, with larger models showing stronger biases. The FFN module is key to this behavior, and dynamic sparsity highlights parameter redundancy. Training instability in larger models links to dead neurons.

Conclusion: Transformers' compression behavior is driven by entropy and dynamic sparsity, with the FFN module playing a critical role. Larger models exhibit stronger biases and training instability, offering insights into their inner workings.

Abstract: Compression has been a critical lens to understand the success of
Transformers. In the past, we have typically taken the target distribution as a
criterion to evaluate a model's compression performance. Nevertheless,it often
remains challenging to precisely assess how well the model achieves compression
and to compare the information content of the learned distribution with that of
the target distribution during compression,as the target distribution is
typically unknown and entropy computation often incurs exponential cost. In
this work, we explore these issues under a controlled experimental setup. We
find that Transformers exhibit a unique inductive bias in data compression:
beyond approaching the target distribution, they tend to favor learning
lower-entropy distributions, with this tendency becoming more pronounced as the
model size increases. This preference prevents Transformers from perfectly
aligning with the target distribution, instead further compressing its
information content. Furthermore, we show that the FFN module plays a critical
role in driving this bias. In addition, while models remove informational
redundancy from data during compression, they also exhibit redundancy within
their parameters, which enables compression and can be characterized through
dynamic sparsity. However, the dynamic sparsity patterns in Transformers,
particularly in attention and FFN modules, demand further exploration. As for
this, we show that larger Transformers show stronger preferences for bypassing
attention computations via residual connections and have lower proportion of
active neurons. Interestingly, we also find that training instability in larger
models strongly correlates with sudden increases in dead neurons. Our work
contributes to a deeper understanding of Transformers from the lens of entropy
and dynamic sparsity.

</details>


### [443] [Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers](https://arxiv.org/abs/2504.19000)
*Elad Sofer, Tomer Shaked, Caroline Chaux, Nir Shlezinger*

Main category: cs.LG

TL;DR: The paper explores adversarial sensitivity in non-learned iterative optimizers, showing they share vulnerabilities with ML models. It proposes adversarial training to enhance robustness and validates findings numerically.


<details>
  <summary>Details</summary>
Motivation: To investigate if non-learned iterative optimizers, like ML models, are susceptible to adversarial perturbations and to develop methods to improve their robustness.

Method: Analyzes adversarial sensitivity in iterative optimizers, casts them as ML models via deep unfolding, and uses adversarial training to enhance robustness. Theoretical proofs and numerical experiments validate the approach.

Result: Non-learned optimizers exhibit adversarial sensitivity similar to ML models. Adversarial training improves robustness, and theoretical proofs support the findings.

Conclusion: Iterative optimizers are vulnerable to adversarial attacks, but adversarial training can mitigate this, bridging insights between optimization and ML robustness.

Abstract: Machine learning (ML) models are often sensitive to carefully crafted yet
seemingly unnoticeable perturbations. Such adversarial examples are considered
to be a property of ML models, often associated with their black-box operation
and sensitivity to features learned from data. This work examines the
adversarial sensitivity of non-learned decision rules, and particularly of
iterative optimizers. Our analysis is inspired by the recent developments in
deep unfolding, which cast such optimizers as ML models. We show that
non-learned iterative optimizers share the sensitivity to adversarial examples
of ML models, and that attacking iterative optimizers effectively alters the
optimization objective surface in a manner that modifies the minima sought. We
then leverage the ability to cast iteration-limited optimizers as ML models to
enhance robustness via adversarial training. For a class of proximal gradient
optimizers, we rigorously prove how their learning affects adversarial
sensitivity. We numerically back our findings, showing the vulnerability of
various optimizers, as well as the robustness induced by unfolding and
adversarial training.

</details>


### [444] [Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation](https://arxiv.org/abs/2504.19002)
*Delun Lai, Yeyubei Zhang, Yunchong Liu, Chaojie Li, Huadong Mo*

Main category: cs.LG

TL;DR: A deep learning-based multimodal fusion architecture improves autonomous robot navigation by integrating RGB and LiDAR data, achieving higher accuracy and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Enhancing perception capabilities of autonomous robots in complex environments by effectively fusing multimodal data.

Method: Uses lightweight feature extraction, adaptive weighted cross-modal fusion, and time-series modeling to integrate RGB and LiDAR data.

Result: Increases navigation and positioning accuracy by 3.5% and 2.2% on the KITTI dataset while maintaining real-time performance.

Conclusion: Provides a novel solution for autonomous robot navigation in complex environments.

Abstract: This paper introduces a novel deep learning-based multimodal fusion
architecture aimed at enhancing the perception capabilities of autonomous
navigation robots in complex environments. By utilizing innovative feature
extraction modules, adaptive fusion strategies, and time-series modeling
mechanisms, the system effectively integrates RGB images and LiDAR data. The
key contributions of this work are as follows: a. the design of a lightweight
feature extraction network to enhance feature representation; b. the
development of an adaptive weighted cross-modal fusion strategy to improve
system robustness; and c. the incorporation of time-series information modeling
to boost dynamic scene perception accuracy. Experimental results on the KITTI
dataset demonstrate that the proposed approach increases navigation and
positioning accuracy by 3.5% and 2.2%, respectively, while maintaining
real-time performance. This work provides a novel solution for autonomous robot
navigation in complex environments.

</details>


### [445] [\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks](https://arxiv.org/abs/2504.19013)
*Júlia Vicens Figueres, Juliette Vanderhaeghen, Federica Bragone, Kateryna Morozovska, Khemraj Shukla*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Physics-Informed Neural Networks (PINNs) are a novel computational approach
for solving partial differential equations (PDEs) with noisy and sparse initial
and boundary data. Although, efficient quantification of epistemic and
aleatoric uncertainties in big multi-scale problems remains challenging. We
propose \$PINN a novel method of computing global uncertainty in PDEs using a
Bayesian framework, by combining local Bayesian Physics-Informed Neural
Networks (BPINN) with domain decomposition. The solution continuity across
subdomains is obtained by imposing the flux continuity across the interface of
neighboring subdomains. To demonstrate the effectiveness of \$PINN, we conduct
a series of computational experiments on PDEs in 1D and 2D spatial domains.
Although we have adopted conservative PINNs (cPINNs), the method can be
seamlessly extended to other domain decomposition techniques. The results infer
that the proposed method recovers the global uncertainty by computing the local
uncertainty exactly more efficiently as the uncertainty in each subdomain can
be computed concurrently. The robustness of \$PINN is verified by adding
uncorrelated random noise to the training data up to 15% and testing for
different domain sizes.

</details>


### [446] [Towards minimax optimal algorithms for Active Simple Hypothesis Testing](https://arxiv.org/abs/2504.19014)
*Sushant Vijayan*

Main category: cs.LG

TL;DR: The paper introduces a game-theoretic approach to the Active Simple Hypothesis Testing (ASHT) problem, proposing computationally efficient algorithms that outperform static methods.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges and dimensionality issues in prior work on ASHT, leveraging game theory and PDEs for better bounds and algorithms.

Method: Uses differential games and PDEs for theoretical bounds, then links to Blackwell Approachability for a more efficient algorithm.

Result: The proposed algorithm is computationally tractable, outperforms static methods, and often achieves optimal performance.

Conclusion: The new algorithm, while not proven optimal, is a significant improvement over existing methods in ASHT.

Abstract: We study the Active Simple Hypothesis Testing (ASHT) problem, a simpler
variant of the Fixed Budget Best Arm Identification problem. In this work, we
provide novel game theoretic formulation of the upper bounds of the ASHT
problem. This formulation allows us to leverage tools of differential games and
Partial Differential Equations (PDEs) to propose an approximately optimal
algorithm that is computationally tractable compared to prior work. However,
the optimal algorithm still suffers from a curse of dimensionality and instead
we use a novel link to Blackwell Approachability to propose an algorithm that
is far more efficient computationally. We show that this new algorithm,
although not proven to be optimal, is always better than static algorithms in
all instances of ASHT and is numerically observed to attain the optimal
exponent in various instances.

</details>


### [447] [Smooth Approximations of the Rounding Function](https://arxiv.org/abs/2504.19026)
*Stanislav Semenov*

Main category: cs.LG

TL;DR: Novel smooth approximations to the classical rounding function for differentiable optimization and ML, using sigmoid-based methods with controlled trade-offs between smoothness and accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable gradient-based optimization in applications requiring rounding, by providing differentiable alternatives to hard rounding.

Method: Two approaches: (1) localized sigmoid window functions centered at integers, and (2) normalized weighted sums of sigmoid derivatives for density-based interpolation. Both converge to classical rounding as sharpness increases.

Result: Methods achieve smooth interpolation with low computational cost, maintaining precision while being fully differentiable.

Conclusion: These approximations are valuable for gradient-based optimization, offering controlled smoothness and accuracy trade-offs.

Abstract: We propose novel smooth approximations to the classical rounding function,
suitable for differentiable optimization and machine learning applications. Our
constructions are based on two approaches: (1) localized sigmoid window
functions centered at each integer, and (2) normalized weighted sums of sigmoid
derivatives representing local densities. The first method approximates the
step-like behavior of rounding through differences of shifted sigmoids, while
the second method achieves smooth interpolation between integers via
density-based weighting. Both methods converge pointwise to the classical
rounding function as the sharpness parameter k tends to infinity, and allow
controlled trade-offs between smoothness and approximation accuracy. We
demonstrate that by restricting the summation to a small set of nearest
integers, the computational cost remains low without sacrificing precision.
These constructions provide fully differentiable alternatives to hard rounding,
which are valuable in contexts where gradient-based methods are essential.

</details>


### [448] [On learning functions over biological sequence space: relating Gaussian process priors, regularization, and gauge fixing](https://arxiv.org/abs/2504.19034)
*Samantha Petti, Carlos Martí-Gómez, Justin B. Kinney, Juannan Zhou, David M. McCandlish*

Main category: cs.LG

TL;DR: The paper unifies Gaussian process approaches with regularized regression for sequence-function mapping, enabling better inference and interpretation of biological sequence functionality.


<details>
  <summary>Details</summary>
Motivation: To improve the inference and interpretation of sequence-function maps in biology by addressing gauge-fixing and unifying methods.

Method: Links regularized regression in overparameterized weight space to Gaussian process approaches in function space, and derives weight distributions for efficient computation.

Result: Shows how to construct regularizers for arbitrary Gaussian process priors and gauges, and efficiently computes weight distributions for long sequences.

Conclusion: The framework enhances the ability to infer and interpret sequence-function relationships, unifying and extending existing methods.

Abstract: Mappings from biological sequences (DNA, RNA, protein) to quantitative
measures of sequence functionality play an important role in contemporary
biology. We are interested in the related tasks of (i) inferring predictive
sequence-to-function maps and (ii) decomposing sequence-function maps to
elucidate the contributions of individual subsequences. Because each
sequence-function map can be written as a weighted sum over subsequences in
multiple ways, meaningfully interpreting these weights requires "gauge-fixing,"
i.e., defining a unique representation for each map. Recent work has
established that most existing gauge-fixed representations arise as the unique
solutions to $L_2$-regularized regression in an overparameterized "weight
space" where the choice of regularizer defines the gauge. Here, we establish
the relationship between regularized regression in overparameterized weight
space and Gaussian process approaches that operate in "function space," i.e.
the space of all real-valued functions on a finite set of sequences. We
disentangle how weight space regularizers both impose an implicit prior on the
learned function and restrict the optimal weights to a particular gauge. We
also show how to construct regularizers that correspond to arbitrary explicit
Gaussian process priors combined with a wide variety of gauges. Next, we derive
the distribution of gauge-fixed weights implied by the Gaussian process
posterior and demonstrate that even for long sequences this distribution can be
efficiently computed for product-kernel priors using a kernel trick. Finally,
we characterize the implicit function space priors associated with the most
common weight space regularizers. Overall, our framework unifies and extends
our ability to infer and interpret sequence-function relationships.

</details>


### [449] [Atlantes: A system of GPS transformers for global-scale real-time maritime intelligence](https://arxiv.org/abs/2504.19036)
*Henry Herzog, Joshua Hansen, Yawen Zhang, Patrick Beukema*

Main category: cs.LG

TL;DR: Atlantes is a deep learning system for real-time global vessel monitoring to combat unsustainable ocean exploitation.


<details>
  <summary>Details</summary>
Motivation: Unsustainable ocean exploitation and global warming threaten coastal communities, necessitating accurate maritime monitoring.

Method: Atlantes uses bespoke transformers to process GPS data from vessels into quantifiable behaviors for real-time analysis.

Result: The system enables low-latency, high-performance decision-making and interventions against illegal maritime activities.

Conclusion: Atlantes is operational globally, aiding hundreds of organizations in effective maritime governance.

Abstract: Unsustainable exploitation of the oceans exacerbated by global warming is
threatening coastal communities worldwide. Accurate and timely monitoring of
maritime activity is an essential step to effective governance and to inform
future policy. In support of this complex global-scale effort, we built
Atlantes, a deep learning based system that provides the first-ever real-time
view of vessel behavior at global scale. Atlantes leverages a series of bespoke
transformers to distill a high volume, continuous stream of GPS messages
emitted by hundreds of thousands of vessels into easily quantifiable behaviors.
The combination of low latency and high performance enables operationally
relevant decision-making and successful interventions on the high seas where
illegal and exploitative activity is too common. Atlantes is already in use by
hundreds of organizations worldwide. Here we provide an overview of the model
and infrastructure that enables this system to function efficiently and
cost-effectively at global-scale and in real-time.

</details>


### [450] [Improved Molecular Generation through Attribute-Driven Integrative Embeddings and GAN Selectivity](https://arxiv.org/abs/2504.19040)
*Nandan Joshi, Erhan Guven*

Main category: cs.LG

TL;DR: A transformer-based vector embedding generator and modified GAN are introduced for generating molecules with desired properties, validated by creating novel odorant molecules.


<details>
  <summary>Details</summary>
Motivation: The demand for tailored molecules in fields like drug discovery and chemical engineering drives the need for advanced computational methods.

Method: Combines a transformer-based embedding generator (using Morgan fingerprints and global attributes) with a modified GAN (adjusted loss function for specific properties).

Result: Achieves 94% reconversion accuracy and successfully generates odorant molecules exclusively.

Conclusion: The approach demonstrates the potential of combining novel embeddings and modified GANs for efficient molecular design.

Abstract: The growing demand for molecules with tailored properties in fields such as
drug discovery and chemical engineering has driven advancements in
computational methods for molecular design. Machine learning-based approaches
for de-novo molecular generation have recently garnered significant attention.
This paper introduces a transformer-based vector embedding generator combined
with a modified Generative Adversarial Network (GAN) to generate molecules with
desired properties. The embedding generator utilizes a novel molecular
descriptor, integrating Morgan fingerprints with global molecular attributes,
enabling the transformer to capture local functional groups and broader
molecular characteristics. Modifying the GAN generator loss function ensures
the generation of molecules with specific desired properties. The transformer
achieves a reconversion accuracy of 94% while translating molecular descriptors
back to SMILES strings, validating the utility of the proposed embeddings for
generative tasks. The approach is validated by generating novel odorant
molecules using a labeled dataset of odorant and non-odorant compounds. With
the modified range-loss function, the GAN exclusively generates odorant
molecules. This work underscores the potential of combining novel vector
embeddings with transformers and modified GAN architectures to accelerate the
discovery of tailored molecules, offering a robust tool for diverse molecular
design applications.

</details>


### [451] [Score-Debiased Kernel Density Estimation](https://arxiv.org/abs/2504.19084)
*Elliot L. Epstein, Rajat Dwaraknath, Thanawat Sornwanee, John Winnicki, Jerry Weihong Liu*

Main category: cs.LG

TL;DR: A novel method (SD-KDE) improves density estimation by debiasing kernel density estimation using an estimated score function, reducing error significantly.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of kernel density estimation by addressing its leading order bias through score-based corrections.

Method: Adjusts data points with a score function step, modifies bandwidth, and applies standard KDE to debias estimation.

Result: SD-KDE reduces mean integrated squared error significantly compared to standard Silverman KDE, even with noisy score estimates.

Conclusion: Score-based corrections can effectively improve nonparametric density estimation.

Abstract: We propose a novel method for density estimation that leverages an estimated
score function to debias kernel density estimation (SD-KDE). In our approach,
each data point is adjusted by taking a single step along the score function
with a specific choice of step size, followed by standard KDE with a modified
bandwidth. The step size and modified bandwidth are chosen to remove the
leading order bias in the KDE. Our experiments on synthetic tasks in 1D, 2D and
on MNIST, demonstrate that our proposed SD-KDE method significantly reduces the
mean integrated squared error compared to the standard Silverman KDE, even with
noisy estimates in the score function. These results underscore the potential
of integrating score-based corrections into nonparametric density estimation.

</details>


### [452] [Harmonizing Generalization and Personalization in Ring-topology Decentralized Federated Learning](https://arxiv.org/abs/2504.19103)
*Shunxin Guo, Jiaqi Lv, Xin Geng*

Main category: cs.LG

TL;DR: DRDFL improves decentralized federated learning by addressing data heterogeneity with personalized and shared knowledge extraction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To avoid centralized failure risks in federated learning and tackle low information-sharing efficiency in ring-topology decentralized FL due to data heterogeneity.

Method: Proposes DRDFL with PersonaNet for personalized feature learning and Learngene for shared knowledge extraction, using adversarial classifiers.

Result: DRDFL outperforms state-of-the-art methods in diverse data heterogeneity settings.

Conclusion: DRDFL effectively balances personalization and generalization in decentralized federated learning.

Abstract: We introduce Ring-topology Decentralized Federated Learning (RDFL) for
distributed model training, aiming to avoid the inherent risks of centralized
failure in server-based FL. However, RDFL faces the challenge of low
information-sharing efficiency due to the point-to-point communication manner
when handling inherent data heterogeneity. Existing studies to mitigate data
heterogeneity focus on personalized optimization of models, ignoring that the
lack of shared information constraints can lead to large differences among
models, weakening the benefits of collaborative learning. To tackle these
challenges, we propose a Divide-and-conquer RDFL framework (DRDFL) that uses a
feature generation model to extract personalized information and invariant
shared knowledge from the underlying data distribution, ensuring both effective
personalization and strong generalization. Specifically, we design a
\textit{PersonaNet} module that encourages class-specific feature
representations to follow a Gaussian mixture distribution, facilitating the
learning of discriminative latent representations tailored to local data
distributions. Meanwhile, the \textit{Learngene} module is introduced to
encapsulate shared knowledge through an adversarial classifier to align latent
representations and extract globally invariant information. Extensive
experiments demonstrate that DRDFL outperforms state-of-the-art methods in
various data heterogeneity settings.

</details>


### [453] [Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments](https://arxiv.org/abs/2504.19139)
*Yun Qu, Qi, Wang, Yixiu Mao, Yiqin Lv, Xiangyang Ji*

Main category: cs.LG

TL;DR: The paper introduces PDTS, a method for robust active task sampling in sequential decision-making, improving adaptation robustness and learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the efficiency and robustness challenges in task adaptation, particularly in risk-averse scenarios requiring costly evaluations.

Method: Proposes PDTS (Posterior and Diversity Synergized Task Sampling), a method integrating risk-predictive models and theoretical insights for robust task sampling.

Result: PDTS enhances zero-shot and few-shot adaptation robustness and accelerates learning in challenging tasks.

Conclusion: PDTS effectively improves robustness and efficiency in sequential decision-making, validated by extensive experiments.

Abstract: Task robust adaptation is a long-standing pursuit in sequential
decision-making. Some risk-averse strategies, e.g., the conditional
value-at-risk principle, are incorporated in domain randomization or meta
reinforcement learning to prioritize difficult tasks in optimization, which
demand costly intensive evaluations. The efficiency issue prompts the
development of robust active task sampling to train adaptive policies, where
risk-predictive models are used to surrogate policy evaluation. This work
characterizes the optimization pipeline of robust active task sampling as a
Markov decision process, posits theoretical and practical insights, and
constitutes robustness concepts in risk-averse scenarios. Importantly, we
propose an easy-to-implement method, referred to as Posterior and Diversity
Synergized Task Sampling (PDTS), to accommodate fast and robust sequential
decision-making. Extensive experiments show that PDTS unlocks the potential of
robust active task sampling, significantly improves the zero-shot and few-shot
adaptation robustness in challenging tasks, and even accelerates the learning
process under certain scenarios. Our project website is at
https://thu-rllab.github.io/PDTS_project_page.

</details>


### [454] [Reliable Thermal Monitoring of Electric Machines through Machine Learning](https://arxiv.org/abs/2504.19141)
*Panagiotis Kakosimos*

Main category: cs.LG

TL;DR: The paper explores AI techniques for monitoring cooling efficiency in induction machines, using machine learning models trained on experimental data.


<details>
  <summary>Details</summary>
Motivation: To improve thermal management in electrified powertrains by leveraging data-driven methods for temperature monitoring.

Method: Collected experimental data under specific conditions, developed three machine-learning models, and optimized them via hyperparameter searches.

Result: All three models performed well in monitoring machine conditions, even during transient operations.

Conclusion: Data-driven methods show strong potential for enhancing thermal management in induction machines.

Abstract: The electrification of powertrains is rising as the objective for a more
viable future is intensified. To ensure continuous and reliable operation
without undesirable malfunctions, it is essential to monitor the internal
temperatures of machines and keep them within safe operating limits.
Conventional modeling methods can be complex and usually require expert
knowledge. With the amount of data collected these days, it is possible to use
information models to assess thermal behaviors. This paper investigates
artificial intelligence techniques for monitoring the cooling efficiency of
induction machines. Experimental data was collected under specific operating
conditions, and three machine-learning models have been developed. The optimal
configuration for each approach was determined through rigorous hyperparameter
searches, and the models were evaluated using a variety of metrics. The three
solutions performed well in monitoring the condition of the machine even under
transient operation, highlighting the potential of data-driven methods in
improving the thermal management.

</details>


### [455] [Newton-Puiseux Analysis for Interpretability and Calibration of Complex-Valued Neural Networks](https://arxiv.org/abs/2504.19176)
*Piotr Migus*

Main category: cs.LG

TL;DR: A Newton-Puiseux framework is proposed to explain and calibrate complex-valued neural networks (CVNNs) by fitting local polynomial surrogates and decomposing them into fractional-power series, improving robustness and calibration.


<details>
  <summary>Details</summary>
Motivation: Standard explainability and calibration tools fail for CVNNs due to their multi-sheeted decision surfaces. The paper addresses this gap.

Method: The method involves fitting a local polynomial surrogate to high-uncertainty inputs, decomposing it into Puiseux expansions, and using these to estimate robustness and over-confidence.

Result: The framework achieves low RMSE (0.09) on a controlled helix, predicts adversarial flip radii accurately, and reduces calibration error on the MIT-BIH arrhythmia corpus from 0.087 to 0.034.

Conclusion: The proposed framework advances CVNN explainability and calibration, with practical applications demonstrated on synthetic and real-world datasets.

Abstract: Complex-valued neural networks (CVNNs) excel where phase matters, yet their
multi-sheeted decision surfaces defy standard explainability and calibration
tools. We propose a \emph{Newton-Puiseux} framework that fits a local
polynomial surrogate to a high-uncertainty input and analytically decomposes
this surrogate into fractional-power series. The resulting Puiseux expansions,
dominant Puiseux coefficients, and phase-aligned curvature descriptors deliver
closed-form estimates of robustness and over-confidence that gradient - or
perturbation-based methods (saliency, LIME, SHAP) cannot provide. On a
controlled $\mathbb{C}^2$ helix the surrogate attains RMSE $< 0.09$ while
recovering the number of decision sheets; quartic coefficients predict
adversarial flip radii within $10^{-3}$. On the real-world MIT-BIH arrhythmia
corpus, Puiseux-guided, phase-aware temperature scaling lowers expected
calibration error from 0.087 to 0.034, contributing to the advancement of
CVNNs. Full code, pre-trained weights, and scripts are at
https://github.com/piotrmgs/puiseux-cvnn.

</details>


### [456] [Hierarchical Attention Generates Better Proofs](https://arxiv.org/abs/2504.19188)
*Jianlong Chen, Chao Li, Yang Yuan, Andrew C Yao*

Main category: cs.LG

TL;DR: Hierarchical Attention improves LLMs' theorem proving by aligning attention with proof structures, boosting success rates and reducing complexity.


<details>
  <summary>Details</summary>
Motivation: Token-level processing in LLMs often misses the hierarchical nature of mathematical proofs.

Method: Introduces Hierarchical Attention, a five-level hierarchy for structured proof generation.

Result: Improves proof success rates (2.05% on miniF2F, 1.69% on ProofNet) and reduces complexity (23.81%, 16.50%).

Conclusion: Hierarchical Attention effectively aligns LLMs with mathematical reasoning, enhancing performance.

Abstract: Large language models (LLMs) have shown promise in formal theorem proving,
but their token-level processing often fails to capture the inherent
hierarchical nature of mathematical proofs. We introduce \textbf{Hierarchical
Attention}, a regularization method that aligns LLMs' attention mechanisms with
mathematical reasoning structures. Our approach establishes a five-level
hierarchy from foundational elements to high-level concepts, ensuring
structured information flow in proof generation. Experiments demonstrate that
our method improves proof success rates by 2.05\% on miniF2F and 1.69\% on
ProofNet while reducing proof complexity by 23.81\% and 16.50\% respectively.
The code is available at https://github.com/Car-pe/HAGBP.

</details>


### [457] [HetGL2R: Learning to Rank Critical Road Segments via Attributed Heterogeneous Graph Random Walks](https://arxiv.org/abs/2504.19199)
*Ming Xu, Jinrong Xiang, Zilong Xie, Xiangfu Meng*

Main category: cs.LG

TL;DR: HetGL2R is a novel method for ranking node importance in road networks by integrating OD demand, route choice, and structural features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing node ranking methods overlook OD demand and route information, limiting accuracy. HetGL2R addresses this gap.

Method: Uses a tripartite graph (trip graph) with OD demand and route data, embedding via HetGWalk and Transformer encoder for spatial influence.

Result: Outperforms baselines in accuracy and robustness, validated on synthetic and real-world datasets.

Conclusion: HetGL2R improves node ranking by effectively capturing spatial dependencies and practical traffic data.

Abstract: Accurately identifying critical nodes with high spatial influence in road
networks is essential for enhancing the efficiency of traffic management and
urban planning. However, existing node importance ranking methods mainly rely
on structural features and topological information, often overlooking critical
factors such as origin-destination (OD) demand and route information. This
limitation leaves considerable room for improvement in ranking accuracy. To
address this issue, we propose HetGL2R, an attributed heterogeneous graph
learning approach for ranking node importance in road networks. This method
introduces a tripartite graph (trip graph) to model the structure of the road
network, integrating OD demand, route choice, and various structural features
of road segments. Based on the trip graph, we design an embedding method to
learn node representations that reflect the spatial influence of road segments.
The method consists of a heterogeneous random walk sampling algorithm
(HetGWalk) and a Transformer encoder. HetGWalk constructs multiple
attribute-guided graphs based on the trip graph to enrich the diversity of
semantic associations between nodes. It then applies a joint random walk
mechanism to convert both topological structures and node attributes into
sequences, enabling the encoder to capture spatial dependencies more
effectively among road segments. Finally, a listwise ranking strategy is
employed to evaluate node importance. To validate the performance of our
method, we construct two synthetic datasets using SUMO based on simulated road
networks. Experimental results demonstrate that HetGL2R significantly
outperforms baselines in incorporating OD demand and route choice information,
achieving more accurate and robust node ranking. Furthermore, we conduct a case
study using real-world taxi trajectory data from Beijing, further verifying the
practicality of the proposed method.

</details>


### [458] [Convergence Properties of Natural Gradient Descent for Minimizing KL Divergence](https://arxiv.org/abs/2504.19259)
*Adwait Datar, Nihat Ay*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Kullback-Leibler (KL) divergence plays a central role in probabilistic
machine learning, where it commonly serves as the canonical loss function.
Optimization in such settings is often performed over the probability simplex,
where the choice of parameterization significantly impacts convergence. In this
work, we study the problem of minimizing the KL divergence and analyze the
behavior of gradient-based optimization algorithms under two dual coordinate
systems within the framework of information geometry$-$ the exponential family
($\theta$ coordinates) and the mixture family ($\eta$ coordinates). We compare
Euclidean gradient descent (GD) in these coordinates with the
coordinate-invariant natural gradient descent (NGD), where the natural gradient
is a Riemannian gradient that incorporates the intrinsic geometry of the
parameter space. In continuous time, we prove that the convergence rates of GD
in the $\theta$ and $\eta$ coordinates provide lower and upper bounds,
respectively, on the convergence rate of NGD. Moreover, under affine
reparameterizations of the dual coordinates, the convergence rates of GD in
$\eta$ and $\theta$ coordinates can be scaled to $2c$ and $\frac{2}{c}$,
respectively, for any $c>0$, while NGD maintains a fixed convergence rate of
$2$, remaining invariant to such transformations and sandwiched between them.
Although this suggests that NGD may not exhibit uniformly superior convergence
in continuous time, we demonstrate that its advantages become pronounced in
discrete time, where it achieves faster convergence and greater robustness to
noise, outperforming GD. Our analysis hinges on bounding the spectrum and
condition number of the Hessian of the KL divergence at the optimum, which
coincides with the Fisher information matrix.

</details>


### [459] [TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks](https://arxiv.org/abs/2504.19274)
*Mohammad M Maheri, Hamed Haddadi, Alex Davidson*

Main category: cs.LG

TL;DR: TeleSparse introduces ZK-friendly post-processing to verify deep learning inference efficiently, reducing computational overhead and memory usage while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Verifying deep learning inference without accessing sensitive data (weights/training data) is challenging due to computational overhead with ZK-SNARKs.

Method: TeleSparse uses sparsification to reduce circuit constraints and neural teleportation to minimize lookup tables for non-linear functions.

Result: TeleSparse cuts prover memory by 67%, proof time by 46%, with ~1% accuracy loss, tested on various models and datasets.

Conclusion: TeleSparse advances scalable, resource-efficient verifiable deep learning, enabling ZK-friendly model design.

Abstract: Verification of the integrity of deep learning inference is crucial for
understanding whether a model is being applied correctly. However, such
verification typically requires access to model weights and (potentially
sensitive or private) training data. So-called Zero-knowledge Succinct
Non-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the
capability to verify model inference without access to such sensitive data.
However, applying ZK-SNARKs to modern neural networks, such as transformers and
large vision models, introduces significant computational overhead.
  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce
practical solutions to this problem. TeleSparse tackles two fundamental
challenges inherent in applying ZK-SNARKs to modern neural networks: (1)
Reducing circuit constraints: Over-parameterized models result in numerous
constraints for ZK-SNARK verification, driving up memory and proof generation
costs. We address this by applying sparsification to neural network models,
enhancing proof efficiency without compromising accuracy or security. (2)
Minimizing the size of lookup tables required for non-linear functions, by
optimizing activation ranges through neural teleportation, a novel adaptation
for narrowing activation functions' range.
  TeleSparse reduces prover memory usage by 67% and proof generation time by
46% on the same model, with an accuracy trade-off of approximately 1%. We
implement our framework using the Halo2 proving system and demonstrate its
effectiveness across multiple architectures (Vision-transformer, ResNet,
MobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new
directions for ZK-friendly model design, moving toward scalable,
resource-efficient verifiable deep learning.

</details>


### [460] [Anyprefer: An Agentic Framework for Preference Data Synthesis](https://arxiv.org/abs/2504.19276)
*Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Weitong Zhang, Ying Wei, Mohit Bansal, Huaxiu Yao*

Main category: cs.LG

TL;DR: Anyprefer is a framework for synthesizing high-quality preference data to align foundation models with human values, using a cooperative two-player Markov Game and external tools to mitigate biases.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of preference data is costly and time-consuming, and self-rewarding methods can amplify biases. Anyprefer aims to address these issues.

Method: Anyprefer uses a cooperative two-player Markov Game involving the target model and a judge model, assisted by external tools and a feedback mechanism.

Result: Anyprefer-V1, a dataset of 58K preference pairs, improves model alignment by 18.55% in NLG, 3.66% in vision-language, 30.05% in medical image analysis, and 16.00% in visuo-motor tasks.

Conclusion: Anyprefer effectively synthesizes high-quality preference data, enhancing model alignment across diverse applications.

Abstract: High-quality preference data is essential for aligning foundation models with
human values through preference learning. However, manual annotation of such
data is often time-consuming and costly. Recent methods often adopt a
self-rewarding approach, where the target model generates and annotates its own
preference data, but this can lead to inaccuracies since the reward model
shares weights with the target model, thereby amplifying inherent biases. To
address these issues, we propose Anyprefer, a framework designed to synthesize
high-quality preference data for aligning the target model. Anyprefer frames
the data synthesis process as a cooperative two-player Markov Game, where the
target model and the judge model collaborate together. Here, a series of
external tools are introduced to assist the judge model in accurately rewarding
the target model's responses, mitigating biases in the rewarding process. In
addition, a feedback mechanism is introduced to optimize prompts for both
models, enhancing collaboration and improving data quality. The synthesized
data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K
high-quality preference pairs. Extensive experiments show that Anyprefer
significantly improves model alignment performance across four main
applications, covering 21 datasets, achieving average improvements of 18.55% in
five natural language generation datasets, 3.66% in nine vision-language
understanding datasets, 30.05% in three medical image analysis datasets, and
16.00% in four visuo-motor control tasks.

</details>


### [461] [Ethical Challenges of Using Artificial Intelligence in Judiciary](https://arxiv.org/abs/2504.19284)
*Angel Mary John, Aiswarya M. U., Jerrin Thomas Panachakel*

Main category: cs.LG

TL;DR: AI in the judiciary offers efficiency and cost benefits but raises ethical challenges that need addressing.


<details>
  <summary>Details</summary>
Motivation: To explore AI's potential in revolutionizing the legal system while addressing its ethical implications.

Method: Analysis of AI's role in legal tasks and identification of ethical challenges.

Result: AI can enhance legal processes but requires ethical safeguards for responsible use.

Conclusion: Ethical guidelines are essential for equitable AI deployment in the judiciary.

Abstract: Artificial intelligence (AI) has emerged as a ubiquitous concept in numerous
domains, including the legal system. AI has the potential to revolutionize the
functioning of the judiciary and the dispensation of justice. Incorporating AI
into the legal system offers the prospect of enhancing decision-making for
judges, lawyers, and legal professionals, while concurrently providing the
public with more streamlined, efficient, and cost-effective services. The
integration of AI into the legal landscape offers manifold benefits,
encompassing tasks such as document review, legal research, contract analysis,
case prediction, and decision-making. By automating laborious and error-prone
procedures, AI has the capacity to alleviate the burden associated with these
arduous tasks. Consequently, courts around the world have begun embracing AI
technology as a means to enhance the administration of justice. However,
alongside its potential advantages, the use of AI in the judiciary poses a
range of ethical challenges. These ethical quandaries must be duly addressed to
ensure the responsible and equitable deployment of AI systems. This article
delineates the principal ethical challenges entailed in employing AI within the
judiciary and provides recommendations to effectively address these issues.

</details>


### [462] [Flow Along the K-Amplitude for Generative Modeling](https://arxiv.org/abs/2504.19353)
*Weitao Du, Shuning Chang, Jiasheng Tang, Yu Rong, Fan Wang, Shengchao Liu*

Main category: cs.LG

TL;DR: K-Flow is a generative learning algorithm that uses a scaling parameter (K) to organize frequency bands, enabling flow matching and steerable generation across scales.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of controlling information at different scales in generative models, K-Flow introduces a novel paradigm for steerable generation.

Method: K-Flow incorporates K-amplitude decomposition to enable flow matching across scaling parameters, with theoretical, dynamic, and practical properties explored.

Result: Experiments on image and molecule generation show K-Flow's effectiveness, with ablation studies confirming its ability to control resolution via scaling parameters.

Conclusion: K-Flow provides a scalable and steerable generative framework, validated by its performance in diverse generation tasks.

Abstract: In this work, we propose a novel generative learning paradigm, K-Flow, an
algorithm that flows along the $K$-amplitude. Here, $k$ is a scaling parameter
that organizes frequency bands (or projected coefficients), and amplitude
describes the norm of such projected coefficients. By incorporating the
$K$-amplitude decomposition, K-Flow enables flow matching across the scaling
parameter as time. We discuss three venues and six properties of K-Flow, from
theoretical foundations, energy and temporal dynamics, and practical
applications, respectively. Specifically, from the practical usage perspective,
K-Flow allows steerable generation by controlling the information at different
scales. To demonstrate the effectiveness of K-Flow, we conduct experiments on
unconditional image generation, class-conditional image generation, and
molecule assembly generation. Additionally, we conduct three ablation studies
to demonstrate how K-Flow steers scaling parameter to effectively control the
resolution of image generation.

</details>


### [463] [Rethinking Label-specific Features for Label Distribution Learning](https://arxiv.org/abs/2504.19374)
*Suping Xu, Chuyi Dai, Lin Shang, Changbin Shao, Xibei Yang, Witold Pedrycz*

Main category: cs.LG

TL;DR: The paper introduces LIFT-SAP, an enhanced method for label distribution learning (LDL) that improves upon LIFT by incorporating Structural Anchor Points (SAPs) to capture inter-cluster interactions and multi-perspective information. The proposed LDL-LIFT-SAP algorithm outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LIFT methods for LDL focus on intra-cluster relationships, neglecting inter-cluster interactions and relying solely on Euclidean distance, which can introduce noise and bias.

Method: The authors propose LIFT-SAP, which uses SAPs to capture inter-cluster interactions and integrates distance and direction information for robust label-specific features (LSFs). They also introduce LDL-LIFT-SAP, unifying multiple label descriptions into a cohesive distribution.

Result: Experiments on 15 datasets show LIFT-SAP outperforms LIFT, and LDL-LIFT-SAP surpasses seven other algorithms.

Conclusion: LIFT-SAP and LDL-LIFT-SAP provide more accurate and robust LDL by addressing limitations of existing methods, demonstrating superior performance.

Abstract: Label distribution learning (LDL) is an emerging learning paradigm designed
to capture the relative importance of labels for each instance. Label-specific
features (LSFs), constructed by LIFT, have proven effective for learning tasks
with label ambiguity by leveraging clustering-based prototypes for each label
to re-characterize instances. However, directly introducing LIFT into LDL tasks
can be suboptimal, as the prototypes it collects primarily reflect
intra-cluster relationships while neglecting interactions among distinct
clusters. Additionally, constructing LSFs using multi-perspective information,
rather than relying solely on Euclidean distance, provides a more robust and
comprehensive representation of instances, mitigating noise and bias that may
arise from a single distance perspective. To address these limitations, we
introduce Structural Anchor Points (SAPs) to capture inter-cluster
interactions. This leads to a novel LSFs construction strategy, LIFT-SAP, which
enhances LIFT by integrating both distance and direction information of each
instance relative to SAPs. Furthermore, we propose a novel LDL algorithm, Label
Distribution Learning via Label-specifIc FeaTure with SAPs (LDL-LIFT-SAP),
which unifies multiple label description degrees predicted from different LSF
spaces into a cohesive label distribution. Extensive experiments on 15
real-world datasets demonstrate the effectiveness of LIFT-SAP over LIFT, as
well as the superiority of LDL-LIFT-SAP compared to seven other
well-established algorithms.

</details>


### [464] [$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation](https://arxiv.org/abs/2504.19375)
*Siddharth Chandak*

Main category: cs.LG

TL;DR: Improved $O(1/k)$ bound for non-linear two-time-scale stochastic approximation, surpassing the previous $O(1/k^{2/3})$ bound.


<details>
  <summary>Details</summary>
Motivation: Address the gap in performance bounds for non-linear two-time-scale stochastic approximation, which has applications in reinforcement learning, optimization, and game control.

Method: Rewrite the iteration using an averaged noise sequence with fast decay and use an induction-based approach to bound iterates in expectation.

Result: Achieved an improved $O(1/k)$ bound for non-linear settings, applicable to algorithms like gradient descent-ascent and Lagrangian optimization.

Conclusion: The work provides a tighter performance bound for non-linear two-time-scale stochastic approximation, enhancing its theoretical foundation.

Abstract: Two-time-scale stochastic approximation is an algorithm with coupled
iterations which has found broad applications in reinforcement learning,
optimization and game control. While several prior works have obtained a mean
square error bound of $O(1/k)$ for linear two-time-scale iterations, the best
known bound in the non-linear contractive setting has been $O(1/k^{2/3})$. In
this work, we obtain an improved bound of $O(1/k)$ for non-linear
two-time-scale stochastic approximation. Our result applies to algorithms such
as gradient descent-ascent and two-time-scale Lagrangian optimization. The key
step in our analysis involves rewriting the original iteration in terms of an
averaged noise sequence which decays sufficiently fast. Additionally, we use an
induction-based approach to show that the iterates are bounded in expectation.

</details>


### [465] [HyperController: A Hyperparameter Controller for Fast and Stable Training of Reinforcement Learning Neural Networks](https://arxiv.org/abs/2504.19382)
*Jonathan Gornet, Yiannis Kantaros, Bruno Sinopoli*

Main category: cs.LG

TL;DR: HyperController is an efficient algorithm for hyperparameter optimization in reinforcement learning, using a Linear Gaussian Dynamical System and Kalman filter to achieve faster training and better performance.


<details>
  <summary>Details</summary>
Motivation: To improve hyperparameter optimization efficiency and stability in reinforcement learning neural networks.

Method: Models hyperparameter optimization as a Linear Gaussian Dynamical System and uses the Kalman filter for efficient representation.

Result: Outperforms other algorithms in 4 out of 5 OpenAI Gymnasium environments, achieving higher median rewards.

Conclusion: HyperController shows promise for efficient and stable reinforcement learning training.

Abstract: We introduce Hyperparameter Controller (HyperController), a computationally
efficient algorithm for hyperparameter optimization during training of
reinforcement learning neural networks. HyperController optimizes
hyperparameters quickly while also maintaining improvement of the reinforcement
learning neural network, resulting in faster training and deployment. It
achieves this by modeling the hyperparameter optimization problem as an unknown
Linear Gaussian Dynamical System, which is a system with a state that linearly
changes. It then learns an efficient representation of the hyperparameter
objective function using the Kalman filter, which is the optimal one-step
predictor for a Linear Gaussian Dynamical System. To demonstrate the
performance of HyperController, it is applied as a hyperparameter optimizer
during training of reinforcement learning neural networks on a variety of
OpenAI Gymnasium environments. In four out of the five Gymnasium environments,
HyperController achieves highest median reward during evaluation compared to
other algorithms. The results exhibit the potential of HyperController for
efficient and stable training of reinforcement learning neural networks.

</details>


### [466] [Bi-directional Model Cascading with Proxy Confidence](https://arxiv.org/abs/2504.19391)
*David Warren, Mark Dras*

Main category: cs.LG

TL;DR: A bi-directional model cascading approach improves efficiency by using small and large model confidence simultaneously, reducing costly deferrals.


<details>
  <summary>Details</summary>
Motivation: Existing cascading methods lack large model confidence, limiting efficiency.

Method: Uses hidden state analysis for small model confidence and a proxy for large model confidence.

Result: Reduces deferrals to costly models, improving efficiency.

Conclusion: Bi-directional cascading with richer confidence representation outperforms standard methods.

Abstract: Model Cascading, recently applied successfully to LLMs, is a simple but
powerful technique that improves the efficiency of inference by selectively
applying models of varying sizes. Models are used in sequence from smallest to
largest, only deferring samples to large, costly models when smaller models are
not sufficiently confident. Existing approaches to deferral use only limited
small model confidence estimates because of the inaccessibility of the large
model, although large model confidence is known to be important. We therefore
propose a bi-directional approach to deferral that considers the confidence of
small and large models in the cascade simultaneously through the use of a proxy
for the large model. This requires a richer representation of model confidence
to enable comparative calibration: we use an analysis of hidden states to
improve post-invocation confidence of the small model, which in itself improves
cascading results over prior approaches. We then combine this with a tiny proxy
model to estimate pre-invocation confidence of the large model. We examine the
proposed cascading system over challenging, multiple-choice datasets, finding
improvements over standard cascading baselines reflected in reductions in
deferrals to more costly models.

</details>


### [467] [Observational Learning with a Budget](https://arxiv.org/abs/2504.19396)
*Shuo Wu, Pawan Poojary, Randall Berry*

Main category: cs.LG

TL;DR: A Bayesian learning model where agents improve decisions by observing predecessors. A planner optimizes signal quality with budget constraints, proposing two strategies to maximize correct information cascades.


<details>
  <summary>Details</summary>
Motivation: To enhance decision accuracy in sequential Bayesian learning by optimizing signal quality allocation under budget limits.

Method: Formulate and analyze budget allocation problem; propose two optimal strategies for signal enhancement.

Result: At least one strategy maximizes the probability of achieving a correct information cascade.

Conclusion: Optimal budget allocation can significantly improve decision accuracy in observational learning.

Abstract: We consider a model of Bayesian observational learning in which a sequence of
agents receives a private signal about an underlying binary state of the world.
Each agent makes a decision based on its own signal and its observations of
previous agents. A central planner seeks to improve the accuracy of these
signals by allocating a limited budget to enhance signal quality across agents.
We formulate and analyze the budget allocation problem and propose two optimal
allocation strategies. At least one of these strategies is shown to maximize
the probability of achieving a correct information cascade.

</details>


### [468] [UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting](https://arxiv.org/abs/2504.19408)
*Maitreya Sonawane, Sumit Mamtani*

Main category: cs.LG

TL;DR: The paper introduces a Transformer-based deep learning model for weather nowcasting, achieving high accuracy in localized storm predictions with fast inference.


<details>
  <summary>Details</summary>
Motivation: Current numerical weather models struggle with localized, rapidly evolving storms. The goal is to replace these with efficient deep learning approaches for immediate, high-resolution forecasts.

Method: A novel Transformer-based model using axial attention mechanisms is developed to learn patterns from time series data, applicable to univariate and multivariate datasets.

Result: State-of-the-art results are achieved (PSNR = 47.67, SSIM = 0.9943) using UNet with Axial Transformer on the dataset.

Conclusion: The proposed method demonstrates the potential of deep learning for accurate and efficient weather nowcasting, outperforming traditional models.

Abstract: Making accurate weather predictions can be particularly challenging for
localized storms or events that evolve on hourly timescales, such as
thunderstorms. Hence, our goal for the project was to model Weather Nowcasting
for making highly localized and accurate predictions that apply to the
immediate future replacing the current numerical weather models and data
assimilation systems with Deep Learning approaches. A significant advantage of
machine learning is that inference is computationally cheap given an
already-trained model, allowing forecasts that are nearly instantaneous and in
the native high resolution of the input data. In this work we developed a novel
method that employs Transformer-based machine learning models to forecast
precipitation. This approach works by leveraging axial attention mechanisms to
learn complex patterns and dynamics from time series frames. Moreover, it is a
generic framework and can be applied to univariate and multivariate time series
data, as well as time series embeddings data. This paper represents an initial
research on the dataset used in the domain of next frame prediciton, and hence,
we demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67,
SSIM = 0.9943) used for the given dataset using UNet with Axial Transformer.

</details>


### [469] [Graph-based Semi-supervised and Unsupervised Methods for Local Clustering](https://arxiv.org/abs/2504.19419)
*Zhaiming Shen, Sung Ha Kang*

Main category: cs.LG

TL;DR: The paper introduces semi-supervised and unsupervised methods for local clustering in large graphs, using sparse solutions to graph Laplacian systems and diffusion-based techniques, achieving state-of-the-art results with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Local clustering is essential for identifying substructures in large graphs without full graph knowledge, but existing methods struggle with limited labeled data or no prior information.

Method: Proposes semi-supervised and unsupervised approaches involving random graph sampling, diffusion-based local cluster extraction, and overlap analysis to identify clusters.

Result: The methods are proven correct under co-membership conditions and outperform existing techniques in low-label rate scenarios.

Conclusion: The proposed methods effectively address local clustering with minimal labeled data, offering scalable and accurate solutions.

Abstract: Local clustering aims to identify specific substructures within a large graph
without requiring full knowledge of the entire graph. These substructures are
typically small compared to the overall graph, enabling the problem to be
approached by finding a sparse solution to a linear system associated with the
graph Laplacian. In this work, we first propose a method for identifying
specific local clusters when very few labeled data is given, which we term
semi-supervised local clustering. We then extend this approach to the
unsupervised setting when no prior information on labels is available. The
proposed methods involve randomly sampling the graph, applying diffusion
through local cluster extraction, then examining the overlap among the results
to find each cluster. We establish the co-membership conditions for any pair of
nodes and rigorously prove the correctness of our methods. Additionally, we
conduct extensive experiments to demonstrate that the proposed methods achieve
state-of-the-arts results in the low-label rates regime.

</details>


### [470] [Learning High-dimensional Gaussians from Censored Data](https://arxiv.org/abs/2504.19446)
*Arnab Bhattacharyya, Constantinos Daskalakis, Themis Gouleakis, Yuhao Wang*

Main category: cs.LG

TL;DR: Efficient algorithms for learning high-dimensional Gaussian distributions with MNAR missing data, under self-censoring and linear thresholding models.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of distribution learning when data is missing not at random (MNAR) in high-dimensional Gaussian settings.

Method: Two settings: (i) Self-censoring with unknown parameters, using poly-sample algorithms; (ii) Linear thresholding with known covariance, focusing on mean estimation.

Result: Algorithms achieve learning up to TV distance epsilon or efficient mean estimation under specified assumptions.

Conclusion: Proposed methods effectively handle MNAR missingness in Gaussian data, with polynomial sample complexity or efficient mean estimation.

Abstract: We provide efficient algorithms for the problem of distribution learning from
high-dimensional Gaussian data where in each sample, some of the variable
values are missing. We suppose that the variables are missing not at random
(MNAR). The missingness model, denoted by $S(y)$, is the function that maps any
point $y$ in $R^d$ to the subsets of its coordinates that are seen. In this
work, we assume that it is known. We study the following two settings:
  (i) Self-censoring: An observation $x$ is generated by first sampling the
true value $y$ from a $d$-dimensional Gaussian $N(\mu*, \Sigma*)$ with unknown
$\mu*$ and $\Sigma*$. For each coordinate $i$, there exists a set $S_i$
subseteq $R^d$ such that $x_i = y_i$ if and only if $y_i$ in $S_i$. Otherwise,
$x_i$ is missing and takes a generic value (e.g., "?"). We design an algorithm
that learns $N(\mu*, \Sigma*)$ up to total variation (TV) distance epsilon,
using $poly(d, 1/\epsilon)$ samples, assuming only that each pair of
coordinates is observed with sufficiently high probability.
  (ii) Linear thresholding: An observation $x$ is generated by first sampling
$y$ from a $d$-dimensional Gaussian $N(\mu*, \Sigma)$ with unknown $\mu*$ and
known $\Sigma$, and then applying the missingness model $S$ where $S(y) = {i in
[d] : v_i^T y <= b_i}$ for some $v_1, ..., v_d$ in $R^d$ and $b_1, ..., b_d$ in
$R$. We design an efficient mean estimation algorithm, assuming that none of
the possible missingness patterns is very rare conditioned on the values of the
observed coordinates and that any small subset of coordinates is observed with
sufficiently high probability.

</details>


### [471] [Improving Reasoning Performance in Large Language Models via Representation Engineering](https://arxiv.org/abs/2504.19483)
*Bertram Højer, Oliver Jarvis, Stefan Heinrich*

Main category: cs.LG

TL;DR: The paper explores improving LLM reasoning via representation engineering, using control vectors derived from model activations to modulate performance without additional training.


<details>
  <summary>Details</summary>
Motivation: To address the debate on whether LLM reasoning is inherently different and to enhance reasoning performance via interventions.

Method: Uses representation engineering to derive control vectors from LLM activations during reasoning tasks, applied as inference-time interventions.

Result: Demonstrates improved reasoning benchmarks and shows LLMs can be controlled to enhance perceived reasoning ability.

Conclusion: Reasoning in LLMs can be modulated like other tasks, and simple interventions on the residual stream can improve performance.

Abstract: Recent advancements in large language models (LLMs) have resulted in
increasingly anthropomorphic language concerning the ability of LLMs to reason.
Whether reasoning in LLMs should be understood to be inherently different is,
however, widely debated. We propose utilizing a representation engineering
approach wherein model activations are read from the residual stream of an LLM
when processing a reasoning task. The activations are used to derive a control
vector that is applied to the model as an inference-time intervention,
modulating the representational space of the model, to improve performance on
the specified task. We publish the code for deriving control vectors and
analyzing model representations. The method allows us to improve performance on
reasoning benchmarks and assess how control vectors influence the final logit
distribution of a model via metrics such as KL divergence and entropy. We apply
control vectors to Mistral-7B-Instruct and a range of Pythia models on an
inductive, a deductive and mathematical reasoning task. We show that an LLM
can, to a certain degree, be controlled to improve its perceived reasoning
ability by modulating activations. The intervention is dependent upon the
ability to reliably extract the model's typical state when correctly solving a
task. Our results suggest that reasoning performance can be modulated in the
same manner as other information-processing tasks performed by LLMs and
demonstrate that we are capable of improving performance on specific tasks via
a simple intervention on the residual stream with no additional training.

</details>


### [472] [R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference](https://arxiv.org/abs/2504.19449)
*Zhenyu Zhang, Zechun Liu, Yuandong Tian, Harshit Khaitan, Zhangyang Wang, Steven Li*

Main category: cs.LG

TL;DR: R-Sparse is a training-free activation sparsity method for LLMs, achieving high sparsity and efficiency without active channel prediction.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in LLM inference due to large model size and limitations of current activation sparsity methods.

Method: Replaces linear layers with a rank-aware sparse inference method, leveraging input channel sparsity and singular value components.

Result: Achieves 50% model-level sparsity with comparable performance, improving efficiency by 43%.

Conclusion: R-Sparse offers a scalable and effective solution for efficient LLM inference without retraining.

Abstract: Large Language Models (LLMs), while demonstrating remarkable capabilities
across various applications, present significant challenges during inference
due to their substantial model size, especially when deployed on edge devices.
Activation sparsity offers a promising solution to reduce computation and
memory movement, enabling more efficient inference, particularly for
small-batch on-device applications. However, current approaches face
limitations with non-ReLU activation function, which are foundational to most
advanced LLMs, or require heavy continual training. Additionally, the
difficulty in predicting active channels and limited achievable sparsity ratios
constrain the effectiveness of activation sparsity-based methods. In this
paper, we introduce R-Sparse, a training-free activation sparsity approach
capable of achieving high sparsity levels in advanced LLMs. We conducted two
preliminary investigations into how different components contribute to the
output within a single linear layer and found two key observations: (i) the
non-sparse components of the input function can be regarded as a few bias
terms, and (ii) The full computation can be effectively approximated by an
appropriate combination of input channels and weight singular values. Building
on this, we replace the linear layers in LLMs with a rank-aware sparse
inference method that leverages the sparsity of input channels and singular
value components, eliminating the need for active channel prediction like the
output sparsity based approaches. Experiments on Llama-2/3 and Mistral models
across ten diverse tasks demonstrate that R-Sparse achieves comparable
performance at 50% model-level sparsity, resulting in a significant 43%
end-to-end efficient improvements with customized kernels.

</details>


### [473] [Geometry-Informed Neural Operator Transformer](https://arxiv.org/abs/2504.19452)
*Qibang Liu, Vincient Zhong, Hadi Meidani, Diab Abueidda, Seid Koric, Philippe Geubelle*

Main category: cs.LG

TL;DR: GINOT combines transformers with neural operators for efficient PDE predictions on arbitrary geometries, achieving high accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency and accuracy in solving PDEs for arbitrary geometries using machine learning.

Method: Integrates transformer architecture with neural operators, encoding geometry via point clouds and attention mechanisms.

Result: Validated on challenging datasets, GINOT shows high accuracy and generalization for 2D/3D geometries.

Conclusion: GINOT is a robust and efficient surrogate model for PDE predictions on complex geometries.

Abstract: Machine-learning-based surrogate models offer significant computational
efficiency and faster simulations compared to traditional numerical methods,
especially for problems requiring repeated evaluations of partial differential
equations. This work introduces the Geometry-Informed Neural Operator
Transformer (GINOT), which integrates the transformer architecture with the
neural operator framework to enable forward predictions for arbitrary
geometries. GINOT encodes the surface points cloud of a geometry using a
sampling and grouping mechanism combined with an attention mechanism, ensuring
invariance to point order and padding while maintaining robustness to
variations in point density. The geometry information is seamlessly integrated
with query points in the solution decoder through the attention mechanism. The
performance of GINOT is validated on multiple challenging datasets, showcasing
its high accuracy and strong generalization capabilities for complex and
arbitrary 2D and 3D geometries.

</details>


### [474] [Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning](https://arxiv.org/abs/2504.19583)
*Hanlu Zhang, Yumeng Ma, Shuo Wang, Guiran Liu, Binrong Zhu*

Main category: cs.LG

TL;DR: A graph spectral analysis-enhanced parameter collaborative optimization algorithm for large language models improves fine-tuning efficiency and structural awareness.


<details>
  <summary>Details</summary>
Motivation: To enhance fine-tuning efficiency and structural awareness in large language models by leveraging graph spectral analysis.

Method: Treats model parameters as graph nodes, constructs a weighted graph, applies Laplacian spectral decomposition, designs a joint loss function with spectral regularization, and introduces a spectral filtering mechanism for gradient adjustment.

Result: Superior performance in fine-tuning comparisons, few-shot generalization, and convergence speed, with reduced parameter perturbations and improved fine-tuning quality.

Conclusion: The framework advances parameter-efficient training, highlights structural signal processing in deep learning, and offers a robust solution for language model adaptability.

Abstract: This paper proposes a parameter collaborative optimization algorithm for
large language models, enhanced with graph spectral analysis. The goal is to
improve both fine-tuning efficiency and structural awareness during training.
In the proposed method, the parameters of a pre-trained language model are
treated as nodes in a graph. A weighted graph is constructed, and Laplacian
spectral decomposition is applied to enable frequency-domain modeling and
structural representation of the parameter space. Based on this structure, a
joint loss function is designed. It combines the task loss with a spectral
regularization term to facilitate collaborative updates among parameters. In
addition, a spectral filtering mechanism is introduced during the optimization
phase. This mechanism adjusts gradients in a structure-aware manner, enhancing
the model's training stability and convergence behavior. The method is
evaluated on multiple tasks, including traditional fine-tuning comparisons,
few-shot generalization tests, and convergence speed analysis. In all settings,
the proposed approach demonstrates superior performance. The experimental
results confirm that the spectral collaborative optimization framework
effectively reduces parameter perturbations and improves fine-tuning quality
while preserving overall model performance. This work contributes significantly
to the field of artificial intelligence by advancing parameter-efficient
training methodologies for large-scale models, reinforcing the importance of
structural signal processing in deep learning optimization, and offering a
robust, generalizable framework for enhancing language model adaptability and
performance.

</details>


### [475] [Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov Function](https://arxiv.org/abs/2504.19473)
*Donghe Chen, Han Wang, Lin Cheng, Shengping Gong*

Main category: cs.LG

TL;DR: The paper introduces SAC-CLF, a framework combining Soft Actor-Critic with Control Lyapunov Functions to enhance safety and stability in reinforcement learning for real-world control tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods lack safety guarantees during learning, limiting real-world applications. Traditional approaches like reward shaping or model-based methods (CLFs/CBFs) often fail to ensure safety or hinder exploration.

Method: SAC-CLF integrates three innovations: task-specific CLF design, dynamic constraint adjustment, and improved control input smoothness.

Result: Experiments on nonlinear systems and satellite attitude control show SAC-CLF outperforms existing methods in safety and performance.

Conclusion: SAC-CLF effectively addresses safety and stability challenges in RL, enabling broader real-world applications.

Abstract: Reinforcement Learning (RL) has shown promise in control tasks but faces
significant challenges in real-world applications, primarily due to the absence
of safety guarantees during the learning process. Existing methods often
struggle with ensuring safe exploration, leading to potential system failures
and restricting applications primarily to simulated environments. Traditional
approaches such as reward shaping and constrained policy optimization can fail
to guarantee safety during initial learning stages, while model-based methods
using Control Lyapunov Functions (CLFs) or Control Barrier Functions (CBFs) may
hinder efficient exploration and performance. To address these limitations,
this paper introduces Soft Actor-Critic with Control Lyapunov Function
(SAC-CLF), a framework that enhances stability and safety through three key
innovations: (1) a task-specific CLF design method for safe and optimal
performance; (2) dynamic adjustment of constraints to maintain robustness under
unmodeled dynamics; and (3) improved control input smoothness while ensuring
safety. Experimental results on a classical nonlinear system and satellite
attitude control demonstrate the effectiveness of SAC-CLF in overcoming the
shortcomings of existing methods.

</details>


### [476] [An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination](https://arxiv.org/abs/2504.19480)
*Dixiao Wei, Peng Yi, Jinlong Lei, Yiguang Hong, Yuchuan Du*

Main category: cs.LG

TL;DR: The paper introduces an LLM-based framework (PCRD) to automate reward function design for RL in platoon coordination, outperforming manual designs by 10%.


<details>
  <summary>Details</summary>
Motivation: Manual reward function design for RL in platoon coordination is challenging due to goal variability, problem complexity, and trial-and-error inefficiency.

Method: Proposes PCRD, using LLM-driven initialization (AIR module) and iterative optimization (evolutionary module) to automate reward function design.

Result: RL agents with PCRD-generated rewards outperform human-engineered ones by 10% in six complex scenarios.

Conclusion: PCRD effectively automates reward design, improving RL performance in platoon coordination.

Abstract: Reinforcement Learning (RL) has demonstrated excellent decision-making
potential in platoon coordination problems. However, due to the variability of
coordination goals, the complexity of the decision problem, and the
time-consumption of trial-and-error in manual design, finding a well
performance reward function to guide RL training to solve complex platoon
coordination problems remains challenging. In this paper, we formally define
the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based
cooperative platoon coordination problem to incorporate automated reward
function generation. To address PCRDP, we propose a Large Language Model
(LLM)-based Platoon coordination Reward Design (PCRD) framework, which
systematically automates reward function discovery through LLM-driven
initialization and iterative optimization. In this method, LLM first
initializes reward functions based on environment code and task requirements
with an Analysis and Initial Reward (AIR) module, and then iteratively
optimizes them based on training feedback with an evolutionary module. The AIR
module guides LLM to deepen their understanding of code and tasks through a
chain of thought, effectively mitigating hallucination risks in code
generation. The evolutionary module fine-tunes and reconstructs the reward
function, achieving a balance between exploration diversity and convergence
stability for training. To validate our approach, we establish six challenging
coordination scenarios with varying complexity levels within the Yangtze River
Delta transportation network simulation. Comparative experimental results
demonstrate that RL agents utilizing PCRD-generated reward functions
consistently outperform human-engineered reward functions, achieving an average
of 10\% higher performance metrics in all scenarios.

</details>


### [477] [DISCO: learning to DISCover an evolution Operator for multi-physics-agnostic prediction](https://arxiv.org/abs/2504.19496)
*Rudy Morel, Jiequn Han, Edouard Oyallon*

Main category: cs.LG

TL;DR: DISCO uses a hypernetwork to generate parameters for a smaller operator network, enabling efficient prediction of dynamical system states governed by unknown PDEs.


<details>
  <summary>Details</summary>
Motivation: To predict the next state of dynamical systems with unknown PDEs using minimal data, leveraging structured evolution operators for efficiency.

Method: DISCO employs a hypernetwork to process short trajectories and generate parameters for a smaller operator network, which performs time integration for state prediction.

Result: Pretraining on diverse physics datasets yields state-of-the-art performance with fewer epochs and strong generalization.

Conclusion: DISCO effectively decouples dynamics estimation from state prediction, achieving high performance and generalization in PDE-governed systems.

Abstract: We address the problem of predicting the next state of a dynamical system
governed by unknown temporal partial differential equations (PDEs) using only a
short trajectory. While standard transformers provide a natural black-box
solution to this task, the presence of a well-structured evolution operator in
the data suggests a more tailored and efficient approach. Specifically, when
the PDE is fully known, classical numerical solvers can evolve the state
accurately with only a few parameters. Building on this observation, we
introduce DISCO, a model that uses a large hypernetwork to process a short
trajectory and generate the parameters of a much smaller operator network,
which then predicts the next state through time integration. Our framework
decouples dynamics estimation (i.e., DISCovering an evolution operator from a
short trajectory) from state prediction (i.e., evolving this operator).
Experiments show that pretraining our model on diverse physics datasets
achieves state-of-the-art performance while requiring significantly fewer
epochs. Moreover, it generalizes well and remains competitive when fine-tuned
on downstream tasks.

</details>


### [478] [Identification and Estimation of Long-Term Treatment Effects with Monotone Missing](https://arxiv.org/abs/2504.19527)
*Qinwei Yang, Ruocheng Guo, Shasha Han, Peng Wu*

Main category: cs.LG

TL;DR: The paper addresses the challenge of estimating long-term treatment effects under monotone missing data, proposing three methods and a balancing-enhanced approach (BalanceNet) to improve stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Long-term treatment effect estimation is crucial but complicated by monotone missing data, a common yet understudied issue.

Method: Three methods are introduced: inverse probability weighting, sequential regression imputation, and sequential marginal structural model (SeqMSM), with BalanceNet enhancing SeqMSM stability.

Result: Experiments on benchmark datasets confirm the effectiveness of the proposed methods.

Conclusion: The study fills a gap in handling monotone missing data for long-term treatment effect estimation, offering practical solutions.

Abstract: Estimating long-term treatment effects has a wide range of applications in
various domains. A key feature in this context is that collecting long-term
outcomes typically involves a multi-stage process and is subject to monotone
missing, where individuals missing at an earlier stage remain missing at
subsequent stages. Despite its prevalence, monotone missing has been rarely
explored in previous studies on estimating long-term treatment effects. In this
paper, we address this gap by introducing the sequential missingness assumption
for identification. We propose three novel estimation methods, including
inverse probability weighting, sequential regression imputation, and sequential
marginal structural model (SeqMSM). Considering that the SeqMSM method may
suffer from high variance due to severe data sparsity caused by monotone
missing, we further propose a novel balancing-enhanced approach, BalanceNet, to
improve the stability and accuracy of the estimation methods. Extensive
experiments on two widely used benchmark datasets demonstrate the effectiveness
of our proposed methods.

</details>


### [479] [Euclidean Distance Matrix Completion via Asymmetric Projected Gradient Descent](https://arxiv.org/abs/2504.19530)
*Yicheng Li, Xinghua Sun*

Main category: cs.LG

TL;DR: The paper introduces APGD, a gradient-type algorithm for solving the EDMC problem, showing global convergence with exact recovery under certain conditions. It outperforms in rich-sample regions but struggles with limited samples.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the EDMC problem by proposing a new algorithm (APGD) that guarantees global convergence and exact recovery without sample splitting, improving upon existing methods.

Method: The method involves the Asymmetric Projected Gradient Descent (APGD) algorithm, leveraging Burer-Monteiro factorization and replacing the random graph lemma with new upper bounds under the EDMC setting.

Result: APGD achieves exact linear convergence in rich-sample regions but deteriorates with limited samples, matching theoretical predictions while revealing potential weaknesses in implicit regularization.

Conclusion: The study concludes that APGD is effective under certain conditions but highlights the need for more samples than theoretically expected, suggesting limitations in its implicit regularization and stabilization.

Abstract: This paper proposes and analyzes a gradient-type algorithm based on
Burer-Monteiro factorization, called the Asymmetric Projected Gradient Descent
(APGD), for reconstructing the point set configuration from partial Euclidean
distance measurements, known as the Euclidean Distance Matrix Completion (EDMC)
problem. By paralleling the incoherence matrix completion framework, we show
for the first time that global convergence guarantee with exact recovery of
this routine can be established given $\mathcal{O}(\mu^2 r^3 \kappa^2 n \log
n)$ Bernoulli random observations without any sample splitting. Unlike
leveraging the tangent space Restricted Isometry Property (RIP) and local
curvature of the low-rank embedding manifold in some very recent works, our
proof provides new upper bounds to replace the random graph lemma under EDMC
setting. The APGD works surprisingly well and numerical experiments demonstrate
exact linear convergence behavior in rich-sample regions yet deteriorates fast
when compared with the performance obtained by optimizing the s-stress
function, i.e., the standard but unexplained non-convex approach for EDMC, if
the sample size is limited. While virtually matching our theoretical
prediction, this unusual phenomenon might indicate that: (i) the power of
implicit regularization is weakened when specified in the APGD case; (ii) the
stabilization of such new gradient direction requires substantially more
samples than the information-theoretic limit would suggest.

</details>


### [480] [Towards Faster and More Compact Foundation Models for Molecular Property Prediction](https://arxiv.org/abs/2504.19538)
*Yasir Ghunaim, Andrés Villa, Gergo Ignacz, Gyorgy Szekely, Motasem Alfarra, Bernard Ghanem*

Main category: cs.LG

TL;DR: The paper explores strategies to reduce the size of the JMP foundation model for molecular property prediction without significant performance loss, achieving a 32% size reduction and 1.3x faster inference.


<details>
  <summary>Details</summary>
Motivation: Despite JMP's strong performance, its fine-tuning is computationally expensive, prompting the need for a more efficient variant.

Method: Analyzes layer contributions, prunes interaction blocks, and evaluates the impact on efficiency and accuracy.

Result: Removing two interaction blocks reduces model size by 32% and increases inference speed by 1.3x with minimal performance drop.

Conclusion: JMP is over-parameterized, and a smaller variant can maintain performance while lowering computational costs, aiding scalable molecular discovery.

Abstract: Advancements in machine learning for molecular property prediction have
improved accuracy but at the expense of higher computational cost and longer
training times. Recently, the Joint Multi-domain Pre-training (JMP) foundation
model has demonstrated strong performance across various downstream tasks with
reduced training time over previous models. Despite JMP's advantages,
fine-tuning it on molecular datasets ranging from small-scale to large-scale
requires considerable time and computational resources. In this work, we
investigate strategies to enhance efficiency by reducing model size while
preserving performance. To better understand the model's efficiency, we analyze
the layer contributions of JMP and find that later interaction blocks provide
diminishing returns, suggesting an opportunity for model compression. We
explore block reduction strategies by pruning the pre-trained model and
evaluating its impact on efficiency and accuracy during fine-tuning. Our
analysis reveals that removing two interaction blocks results in a minimal
performance drop, reducing the model size by 32% while increasing inference
throughput by 1.3x. These results suggest that JMP-L is over-parameterized and
that a smaller, more efficient variant can achieve comparable performance with
lower computational cost. Our study provides insights for developing lighter,
faster, and more scalable foundation models for molecular and materials
discovery. The code is publicly available at:
https://github.com/Yasir-Ghunaim/efficient-jmp.

</details>


### [481] [Quantifying Memory Utilization with Effective State-Size](https://arxiv.org/abs/2504.19561)
*Rom N. Parnichkun, Neehal Tumma, Armin W. Thomas, Alessandro Moro, Qi An, Taiji Suzuki, Atsushi Yamashita, Michael Poli, Stefano Massaroli*

Main category: cs.LG

TL;DR: The paper introduces a metric called Effective State-Size (ESS) to measure memory utilization in sequence models, offering interpretable insights for improving model design.


<details>
  <summary>Details</summary>
Motivation: The expanding design space of sequence models necessitates a general framework for analyzing architecture, particularly memory utilization.

Method: Leveraging classical signal processing and control theory, ESS is developed for systems with input-invariant and input-varying linear operators, applicable to attention, convolutions, and recurrences.

Result: ESS provides actionable insights, improving initialization, regularization, and model distillation, while revealing cross-architectural differences in memory use.

Conclusion: ESS enhances understanding of memory dynamics, aiding the design of more efficient and effective sequence models.

Abstract: The need to develop a general framework for architecture analysis is becoming
increasingly important, given the expanding design space of sequence models. To
this end, we draw insights from classical signal processing and control theory,
to develop a quantitative measure of \textit{memory utilization}: the internal
mechanisms through which a model stores past information to produce future
outputs. This metric, which we call \textbf{\textit{effective state-size}}
(ESS), is tailored to the fundamental class of systems with
\textit{input-invariant} and \textit{input-varying linear operators},
encompassing a variety of computational units such as variants of attention,
convolutions, and recurrences. Unlike prior work on memory utilization, which
either relies on raw operator visualizations (e.g. attention maps), or simply
the total \textit{memory capacity} (i.e. cache size) of a model, our metrics
provide highly interpretable and actionable measurements. In particular, we
show how ESS can be leveraged to improve initialization strategies, inform
novel regularizers and advance the performance-efficiency frontier through
model distillation. Furthermore, we demonstrate that the effect of context
delimiters (such as end-of-speech tokens) on ESS highlights cross-architectural
differences in how large language models utilize their available memory to
recall information. Overall, we find that ESS provides valuable insights into
the dynamics that dictate memory utilization, enabling the design of more
efficient and effective sequence models.

</details>


### [482] [Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation](https://arxiv.org/abs/2504.19602)
*Kitsuya Azuma, Takayuki Nishio, Yuichi Kitagawa, Wakako Nakano, Takahito Tanimura*

Main category: cs.LG

TL;DR: SCARLET is a federated learning framework that reduces communication costs by 50% using synchronized soft-label caching and Enhanced ERA, while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address high communication overhead and limited model heterogeneity in conventional FL, and redundancy in distillation-based FL.

Method: Integrates synchronized soft-label caching and Enhanced Entropy Reduction Aggregation (Enhanced ERA) to minimize redundant transmissions.

Result: Achieves up to 50% reduction in communication costs and outperforms state-of-the-art methods in accuracy and efficiency.

Conclusion: SCARLET is effective for diverse client scenarios, offering a robust and efficient solution for FL.

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients, enhancing privacy by keeping data local. Yet
conventional FL, relying on frequent parameter-sharing, suffers from high
communication overhead and limited model heterogeneity. Distillation-based FL
approaches address these issues by sharing predictions (soft-labels) instead,
but they often involve redundant transmissions across communication rounds,
reducing efficiency. We propose SCARLET, a novel framework integrating
synchronized soft-label caching and an enhanced Entropy Reduction Aggregation
(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing
cached soft-labels, achieving up to 50% reduction in communication costs
compared to existing methods while maintaining accuracy. Enhanced ERA can be
tuned to adapt to non-IID data variations, ensuring robust aggregation and
performance in diverse client scenarios. Experimental evaluations demonstrate
that SCARLET consistently outperforms state-of-the-art distillation-based FL
methods in terms of accuracy and communication efficiency. The implementation
of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.

</details>


### [483] [LODAP: On-Device Incremental Learning Via Lightweight Operations and Data Pruning](https://arxiv.org/abs/2504.19638)
*Biqing Duan, Qing Wang, Di Liu, Wei Zhou, Zhenli He, Shengfa Miao*

Main category: cs.LG

TL;DR: LODAP is an on-device incremental learning framework for edge systems, featuring an Efficient Incremental Module (EIM) and data pruning to improve accuracy and reduce complexity.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient incremental learning in edge systems where remote server communication is impractical.

Method: Proposes LODAP with EIM, using lightweight adapters and data pruning to reduce training overhead.

Result: Achieves up to 4.32% higher accuracy and 50% lower model complexity on CIFAR-100 and Tiny-ImageNet datasets.

Conclusion: LODAP is effective for on-device incremental learning, validated by real-edge system evaluations.

Abstract: Incremental learning that learns new classes over time after the model's
deployment is becoming increasingly crucial, particularly for industrial edge
systems, where it is difficult to communicate with a remote server to conduct
computation-intensive learning. As more classes are expected to learn after
their execution for edge devices. In this paper, we propose LODAP, a new
on-device incremental learning framework for edge systems. The key part of
LODAP is a new module, namely Efficient Incremental Module (EIM). EIM is
composed of normal convolutions and lightweight operations. During incremental
learning, EIM exploits some lightweight operations, called adapters, to
effectively and efficiently learn features for new classes so that it can
improve the accuracy of incremental learning while reducing model complexity as
well as training overhead. The efficiency of LODAP is further enhanced by a
data pruning strategy that significantly reduces the training data, thereby
lowering the training overhead. We conducted extensive experiments on the
CIFAR-100 and Tiny- ImageNet datasets. Experimental results show that LODAP
improves the accuracy by up to 4.32\% over existing methods while reducing
around 50\% of model complexity. In addition, evaluations on real edge systems
demonstrate its applicability for on-device machine learning. The code is
available at https://github.com/duanbiqing/LODAP.

</details>


### [484] [A Unified Benchmark of Federated Learning with Kolmogorov-Arnold Networks for Medical Imaging](https://arxiv.org/abs/2504.19639)
*Youngjoon Lee, Jinu Gong, Joonhyuk Kang*

Main category: cs.LG

TL;DR: Kolmogorov-Arnold Networks (KAN) outperform MLPs in federated learning for medical imaging, offering simpler architectures and better performance under Non-IID data.


<details>
  <summary>Details</summary>
Motivation: To evaluate KAN's potential as a privacy-preserving alternative to MLPs in federated learning, especially in healthcare.

Method: Benchmarked KAN against MLP using six FL algorithms on a blood cell classification dataset, analyzing hyperparameters and Non-IID data impact.

Result: KAN achieved superior performance with simpler architectures, optimized by adjusting width and minimal depth.

Conclusion: KAN is a promising alternative for privacy-preserving medical imaging in distributed healthcare, marking the first comprehensive FL benchmark for KAN.

Abstract: Federated Learning (FL) enables model training across decentralized devices
without sharing raw data, thereby preserving privacy in sensitive domains like
healthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN)
architectures against traditional MLP across six state-of-the-art FL algorithms
on a blood cell classification dataset. Notably, our experiments demonstrate
that KAN can effectively replace MLP in federated environments, achieving
superior performance with simpler architectures. Furthermore, we analyze the
impact of key hyperparameters-grid size and network architecture-on KAN
performance under varying degrees of Non-IID data distribution. Additionally,
our ablation studies reveal that optimizing KAN width while maintaining minimal
depth yields the best performance in federated settings. As a result, these
findings establish KAN as a promising alternative for privacy-preserving
medical imaging applications in distributed healthcare. To the best of our
knowledge, this is the first comprehensive benchmark of KAN in FL settings for
medical imaging task.

</details>


### [485] [Intelligent4DSE: Optimizing High-Level Synthesis Design Space Exploration with Graph Neural Networks and Large Language Models](https://arxiv.org/abs/2504.19649)
*Lei Xu, Shanshan Wang, Emmanuel Casseau, Chenglong Xiao*

Main category: cs.LG

TL;DR: The paper introduces CoGNNs-LLMEA, a framework combining graph neural networks and a large language model-enhanced evolutionary algorithm to improve HLS design space exploration by predicting QoR without HLS tools, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing HLS DSE methods prioritize structural complexity and training loss, overlooking task-specific needs, while evolutionary algorithms require extensive domain knowledge. The paper aims to address these gaps.

Method: Proposes CoGNNs-LLMEA, integrating a task-adaptive graph neural network (CoGNNs) with a large language model-enhanced evolutionary algorithm (LLMEA) to predict QoR from compiler front-end outputs.

Result: CoGNNs reduces mean prediction errors by 2.8× for latency and 3.4× for resource utilization compared to baselines, achieving state-of-the-art accuracy in post-HLS QoR prediction.

Conclusion: The framework effectively bridges high-level abstractions and physical implementation, offering a more efficient and accurate approach to HLS DSE.

Abstract: High-level synthesis (HLS) design space exploration (DSE) is an optimization
process in electronic design automation (EDA) that systematically explores
high-level design configurations to achieve Pareto-optimal hardware
implementations balancing performance, area, and power (PPA). To optimize this
process, HLS prediction tasks often employ message-passing neural networks
(MPNNs), leveraging complex architectures to achieve high accuracy. These
predictors serve as evaluators in the DSE process, effectively bypassing the
time-consuming estimations traditionally required by HLS tools. However,
existing models often prioritize structural complexity and minimization of
training loss, overlooking task-specific characteristics. Additionally, while
evolutionary algorithms are widely used in DSE, they typically require
extensive domain-specific knowledge to design effective crossover and mutation
operators. To address these limitations, we propose CoGNNs-LLMEA, a framework
that integrates a graph neural network with task-adaptive message passing and a
large language model-enhanced evolutionary algorithm. As a predictive model,
CoGNNs directly leverages intermediate representations generated from source
code after compiler front-end processing, enabling prediction of quality of
results (QoR) without invoking HLS tools. Due to its strong adaptability to
tasks, CoGNNs can be tuned to predict post-HLS and post-implementation
outcomes, effectively bridging the gap between high-level abstractions and
physical implementation characteristics. CoGNNs achieves state-of-the-art
prediction accuracy in post-HLS QoR prediction, reducing mean prediction errors
by 2.8$\times$ for latency and 3.4$\times$ for resource utilization compared to
baseline models.

</details>


### [486] [Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs](https://arxiv.org/abs/2504.19659)
*Muhammad Sabih, Abrarul Karim, Jakob Wittmann, Frank Hannig, Jürgen Teich*

Main category: cs.LG

TL;DR: The paper proposes RISC-V extensions for accelerating DNNs with semi-structured and unstructured sparsity, achieving speedups of up to 5x while using minimal FPGA resources.


<details>
  <summary>Details</summary>
Motivation: RISC-V's customizability makes it ideal for DNN acceleration, but efficient exploitation requires co-design of hardware and software.

Method: Novel RISC-V extensions and custom functional units are introduced to handle sparsity: bit-level configurability for semi-structured sparsity and variable-cycle multiply-and-accumulate for unstructured sparsity.

Result: Speedups of 3x for unstructured and 4x for semi-structured sparsity, with a combined design achieving 5x. Minimal FPGA resource usage enables deployment on small devices.

Conclusion: The proposed co-design approach effectively accelerates DNNs on RISC-V, demonstrating significant performance gains for TinyML applications.

Abstract: The customizability of RISC-V makes it an attractive choice for accelerating
deep neural networks (DNNs). It can be achieved through instruction set
extensions and corresponding custom functional units. Yet, efficiently
exploiting these opportunities requires a hardware/software co-design approach
in which the DNN model, software, and hardware are designed together. In this
paper, we propose novel RISC-V extensions for accelerating DNN models
containing semi-structured and unstructured sparsity. While the idea of
accelerating structured and unstructured pruning is not new, our novel design
offers various advantages over other designs. To exploit semi-structured
sparsity, we take advantage of the fine-grained (bit-level) configurability of
FPGAs and suggest reserving a few bits in a block of DNN weights to encode the
information about sparsity in the succeeding blocks. The proposed custom
functional unit utilizes this information to skip computations. To exploit
unstructured sparsity, we propose a variable cycle sequential
multiply-and-accumulate unit that performs only as many multiplications as the
non-zero weights. Our implementation of unstructured and semi-structured
pruning accelerators can provide speedups of up to a factor of 3 and 4,
respectively. We then propose a combined design that can accelerate both types
of sparsities, providing speedups of up to a factor of 5. Our designs consume a
small amount of additional FPGA resources such that the resulting co-designs
enable the acceleration of DNNs even on small FPGAs. We benchmark our designs
on standard TinyML applications such as keyword spotting, image classification,
and person detection.

</details>


### [487] [A Tripartite Perspective on GraphRAG](https://arxiv.org/abs/2504.19667)
*Michael Banf, Johannes Kuhn*

Main category: cs.LG

TL;DR: The paper proposes Tripartite-GraphRAG, a method combining LLMs with a tripartite knowledge graph to improve factual accuracy and knowledge updates, evaluated in healthcare.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with knowledge-intensive tasks due to hallucinations, lack of provenance, and outdated knowledge. Integrating knowledge graphs could address these issues.

Method: The approach constructs a tripartite knowledge graph via domain-specific concepts, enabling concept-specific text compression, relevance estimation, and avoiding entity resolution challenges.

Result: The method optimizes LLM prompt information density, coverage, and arrangement, reducing prompt length and improving reliability in healthcare use cases.

Conclusion: Tripartite-GraphRAG enhances LLM performance in knowledge-intensive domains by leveraging structured knowledge graphs, offering cost and reliability benefits.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
various domains, yet they struggle with knowledge-intensive tasks in areas that
demand factual accuracy, e.g. industrial automation and healthcare. Key
limitations include their tendency to hallucinate, lack of source traceability
(provenance), and challenges in timely knowledge updates. Combining language
models with knowledge graphs (GraphRAG) offers promising avenues for overcoming
these deficits. However, a major challenge lies in creating such a knowledge
graph in the first place. Here, we propose a novel approach that combines LLMs
with a tripartite knowledge graph representation, which is constructed by
connecting complex, domain-specific objects via a curated ontology of
corresponding, domain-specific concepts to relevant sections within chunks of
text through a concept-anchored pre-analysis of source documents starting from
an initial lexical graph. As a consequence, our Tripartite-GraphRAG approach
implements: i) a concept-specific, information-preserving pre-compression of
textual chunks; ii) allows for the formation of a concept-specific relevance
estimation of embedding similarities grounded in statistics; and iii) avoids
common challenges w.r.t. continuous extendability, such as the need for entity
resolution and deduplication. By applying a transformation to the knowledge
graph, we formulate LLM prompt creation as an unsupervised node classification
problem, drawing on ideas from Markov Random Fields. We evaluate our approach
on a healthcare use case, involving multi-faceted analyses of patient anamneses
given a set of medical concepts as well as clinical literature. Experiments
indicate that it can optimize information density, coverage, and arrangement of
LLM prompts while reducing their lengths, which may lead to reduced costs and
more consistent and reliable LLM outputs.

</details>


### [488] [Graph Fourier Transformer with Structure-Frequency Information](https://arxiv.org/abs/2504.19740)
*Yonghui Zhai, Yang Zhang, Minghao Shang, Lihua Pang, Yaxin Ren*

Main category: cs.LG

TL;DR: Grafourierformer enhances Graph Transformers by integrating Frequency-Structure inductive bias via Graph Fourier Transform, improving performance in graph and node classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Transformers lack generalization bias consideration, leading to sub-optimal performance. This paper addresses this by combining structural and frequency information.

Method: Proposes Grafourierformer, using Graph Fourier Transform on the Attention Matrix to incorporate node frequency and structural information, optimizing attention heads.

Result: Outperforms GNN and GT-based models in benchmarks, validated by ablation studies.

Conclusion: Grafourierformer effectively combines structural and frequency information, enhancing performance and generalization in graph tasks.

Abstract: Graph Transformers (GTs) have shown advantages in numerous graph structure
tasks but their self-attention mechanism ignores the generalization bias of
graphs, with existing methods mainly compensating for this bias from aspects
like position encoding, attention bias and relative distance yet still having
sub-optimal performance and being insufficient by only considering the
structural perspective of generalization bias. To address this, this paper
proposes Grafourierformer, which innovatively combines GT with inductive bias
containing Frequency-Structure information by applying Graph Fourier Transform
to the Attention Matrix: specifically, eigenvalues from the Graph Laplacian
matrix are used to construct an Eigenvalue matrix mask (reflecting node
positions and structural relationships with neighboring nodes to enable
consideration of node range structural characteristics and focus on local graph
details), and inverse Fourier transform is employed to extract node
high-frequency and low-frequency features, calculate low-frequency and
high-frequency energy, and construct a node frequency-energy matrix to filter
the eigenvalue matrix mask, allowing attention heads to incorporate both graph
structural information and node frequency information optimization, adaptively
distinguish global trends from local details, and effectively suppress
redundant information interference. Extensive experiments on various benchmarks
show Grafourierformer consistently outperforms GNN and GT-based models in graph
classification and node classification tasks, with ablation experiments further
validating the effectiveness and necessity of the method. Codes are available
at https://github.com/Arichibald/Grafourierformer.git

</details>


### [489] [FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained Mixed-Precision Quantization of LLMs](https://arxiv.org/abs/2504.19746)
*Xilong Xie, Liang Wang, Limin Xiao, Meng Han, Lin Sun, Shuai Zheng, Xiangrong Xu*

Main category: cs.LG

TL;DR: FineQ is a software-hardware co-design for fine-grained mixed-precision quantization of LLMs, balancing accuracy and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between memory overhead and accuracy degradation in ultra-low-bit quantization of LLMs.

Method: Partitions weights into fine-grained clusters, uses an outlier protection mechanism with 3 bits, and introduces an encoding scheme for aligned memory access. Also includes an accelerator with temporal coding.

Result: Achieves higher accuracy than SOTA mixed-precision quantization at similar bit-widths, with 1.79x energy efficiency and 61.2% area reduction in the systolic array.

Conclusion: FineQ effectively balances accuracy and memory efficiency in LLM quantization, supported by hardware optimization.

Abstract: Large language models (LLMs) have significantly advanced the natural language
processing paradigm but impose substantial demands on memory and computational
resources. Quantization is one of the most effective ways to reduce memory
consumption of LLMs. However, advanced single-precision quantization methods
experience significant accuracy degradation when quantizing to ultra-low bits.
Existing mixed-precision quantization methods are quantized by groups with
coarse granularity. Employing high precision for group data leads to
substantial memory overhead, whereas low precision severely impacts model
accuracy. To address this issue, we propose FineQ, software-hardware co-design
for low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ
partitions the weights into finer-grained clusters and considers the
distribution of outliers within these clusters, thus achieving a balance
between model accuracy and memory overhead. Then, we propose an outlier
protection mechanism within clusters that uses 3 bits to represent outliers and
introduce an encoding scheme for index and data concatenation to enable aligned
memory access. Finally, we introduce an accelerator utilizing temporal coding
that effectively supports the quantization algorithm while simplifying the
multipliers in the systolic array. FineQ achieves higher model accuracy
compared to the SOTA mixed-precision quantization algorithm at a close average
bit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency
and reduces the area of the systolic array by 61.2%.

</details>


### [490] [If Concept Bottlenecks are the Question, are Foundation Models the Answer?](https://arxiv.org/abs/2504.19774)
*Nicola Debole, Pietro Barbiero, Francesco Giannini, Andrea Passeggini, Stefano Teso, Emanuele Marconato*

Main category: cs.LG

TL;DR: VLM-CBMs replace expert annotations with weak supervision from foundation models, but their concept quality varies and doesn't strongly correlate with accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate the impact of replacing expert annotations with weak supervision from foundation models in Concept Bottleneck Models (CBMs).

Method: Analyze state-of-the-art VLM-CBMs using empirical metrics to assess concept quality.

Result: VLM supervision differs from expert annotations, and concept accuracy doesn't strongly correlate with quality.

Conclusion: Weak supervision from VLMs can impact concept quality, and accuracy isn't a reliable proxy for concept quality.

Abstract: Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high
performance with ante-hoc interpretability. CBMs work by first mapping inputs
(e.g., images) to high-level concepts (e.g., visible objects and their
properties) and then use these to solve a downstream task (e.g., tagging or
scoring an image) in an interpretable manner. Their performance and
interpretability, however, hinge on the quality of the concepts they learn. The
go-to strategy for ensuring good quality concepts is to leverage expert
annotations, which are expensive to collect and seldom available in
applications. Researchers have recently addressed this issue by introducing
"VLM-CBM" architectures that replace manual annotations with weak supervision
from foundation models. It is however unclear what is the impact of doing so on
the quality of the learned concepts. To answer this question, we put
state-of-the-art VLM-CBMs to the test, analyzing their learned concepts
empirically using a selection of significant metrics. Our results show that,
depending on the task, VLM supervision can sensibly differ from expert
annotations, and that concept accuracy and quality are not strongly correlated.
Our code is available at https://github.com/debryu/CQA.

</details>


### [491] [Learning Brenier Potentials with Convex Generative Adversarial Neural Networks](https://arxiv.org/abs/2504.19779)
*Claudia Drygala, Hanno Gottschalk, Thomas Kruse, Ségolène Martin, Annika Mütze*

Main category: cs.LG

TL;DR: The paper develops a statistical learning theory for generative adversarial neural networks (GANs) that learn the Brenier potential, ensuring convexity through adversarial training and proving consistency for expanding network capacity.


<details>
  <summary>Details</summary>
Motivation: To leverage the Brenier potential's properties for learning transport maps between probability distributions, particularly focusing on the convexity and regularity of the potential.

Method: Uses ReCU networks with cubic activation for approximation, introduces adversarial training with a convexity penalty, and decomposes learning errors to ensure strict convexity.

Result: Theoretical consistency is proven, and empirical results show the convexity loss becomes inactive during training, with networks successfully learning convex potentials.

Conclusion: The proposed method effectively learns the Brenier potential, combining theoretical guarantees with practical performance on standard test cases.

Abstract: Brenier proved that under certain conditions on a source and a target
probability measure there exists a strictly convex function such that its
gradient is a transport map from the source to the target distribution. This
function is called the Brenier potential. Furthermore, detailed information on
the H\"older regularity of the Brenier potential is available. In this work we
develop the statistical learning theory of generative adversarial neural
networks that learn the Brenier potential. As by the transformation of
densities formula, the density of the generated measure depends on the second
derivative of the Brenier potential, we develop the universal approximation
theory of ReCU networks with cubic activation $\mathtt{ReCU}(x)=\max\{0,x\}^3$
that combines the favorable approximation properties of H\"older functions with
a Lipschitz continuous density. In order to assure the convexity of such
general networks, we introduce an adversarial training procedure for a
potential function represented by the ReCU networks that combines the classical
discriminator cross entropy loss with a penalty term that enforces (strict)
convexity. We give a detailed decomposition of learning errors and show that
for a suitable high penalty parameter all networks chosen in the adversarial
min-max optimization problem are strictly convex. This is further exploited to
prove the consistency of the learning procedure for (slowly) expanding network
capacity. We also implement the described learning algorithm and apply it to a
number of standard test cases from Gaussian mixture to image data as target
distributions. As predicted in theory, we observe that the convexity loss
becomes inactive during the training process and the potentials represented by
the neural networks have learned convexity.

</details>


### [492] [Heterophily-informed Message Passing](https://arxiv.org/abs/2504.19785)
*Haishan Wang, Arno Solin, Vikas Garg*

Main category: cs.LG

TL;DR: A novel scheme mitigates GNN oversmoothing by regulating message aggregation, preserving information frequencies without auxiliary labels, showing improved performance in classification and molecular generation.


<details>
  <summary>Details</summary>
Motivation: Address GNNs' vulnerability to oversmoothing due to homophily assumptions by preserving both low and high-frequency information.

Method: Regulate message aggregation locally using learnt embeddings, avoiding auxiliary labels.

Result: Performance improvements in classification benchmarks and notable gains in molecular generation tasks.

Conclusion: The approach effectively handles heterophily, extending benefits to broader applications like generative modeling.

Abstract: Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due
to their implicit homophily assumption. We mitigate this problem with a novel
scheme that regulates the aggregation of messages, modulating the type and
extent of message passing locally thereby preserving both the low and
high-frequency components of information. Our approach relies solely on learnt
embeddings, obviating the need for auxiliary labels, thus extending the
benefits of heterophily-aware embeddings to broader applications, e.g.,
generative modelling. Our experiments, conducted across various data sets and
GNN architectures, demonstrate performance enhancements and reveal heterophily
patterns across standard classification benchmarks. Furthermore, application to
molecular generation showcases notable performance improvements on
chemoinformatics benchmarks.

</details>


### [493] [Contextures: The Mechanism of Representation Learning](https://arxiv.org/abs/2504.19792)
*Runtian Zhai*

Main category: cs.LG

TL;DR: The paper introduces the contexture theory to unify and mathematically characterize representation learning, emphasizing the role of context variables in optimizing representations for downstream tasks. It argues that better contexts, not just larger models, are key to progress.


<details>
  <summary>Details</summary>
Motivation: To address the lack of scientific understanding of why representations learned by foundation models are effective, especially as scaling models yields diminishing returns.

Method: Proposes the contexture theory, proving that optimal representations arise from capturing the association between input X and context variable A. Introduces SVME and KISE objectives for learning contextures and shows how to combine contexts.

Result: Demonstrates that many pretraining methods can learn contextures and provides statistical learning bounds. Highlights the importance of balanced association strength between X and A.

Conclusion: Scaling models alone is insufficient; better contexts are crucial for advancing representation learning. The theory unifies diverse pretraining methods and guides future improvements.

Abstract: This dissertation establishes the contexture theory to mathematically
characterize the mechanism of representation learning, or pretraining. Despite
the remarkable empirical success of foundation models, it is not very clear
what representations they learn, and why these representations are useful for
various downstream tasks. A scientific understanding of representation learning
is critical, especially at this point when scaling up the model size is
producing diminishing returns, and designing new pretraining methods is
imperative for further progress.
  Prior work treated different representation learning methods quite
differently, whereas the contexture theory provides a unified framework for
analyzing these methods. The central argument is that a representation is
learned from the association between the input X and a context variable A. We
prove that if an encoder captures the maximum information of this association,
in which case we say that the encoder learns the contexture, then it will be
optimal on the class of tasks that are compatible with the context. We also
show that a context is the most useful when the association between X and A is
neither too strong nor too weak. The important implication of the contexture
theory is that increasing the model size alone will achieve diminishing
returns, and further advancements require better contexts.
  We demonstrate that many pretraining objectives can learn the contexture,
including supervised learning, self-supervised learning, generative models,
etc. Then, we introduce two general objectives -- SVME and KISE, for learning
the contexture. We also show how to mix multiple contexts together, an
effortless way to create better contexts from existing ones. Then, we prove
statistical learning bounds for representation learning. Finally, we discuss
the effect of the data distribution shift from pretraining to the downstream
task.

</details>


### [494] [Hierarchical Uncertainty-Aware Graph Neural Network](https://arxiv.org/abs/2504.19820)
*Yoonhyuk Choi, Chong-Kwon Kim*

Main category: cs.LG

TL;DR: HU-GNN integrates multi-scale representation learning, uncertainty estimation, and self-supervised diversity in a single framework for robust and interpretable graph neural networks.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored synergy between local uncertainty capture and graph hierarchy exploitation in GNNs.

Method: HU-GNN adaptively clusters nodes, estimates uncertainty at multiple scales, and uses these for robust message-passing and attention weighting.

Result: Achieves state-of-the-art robustness and interpretability on standard benchmarks.

Conclusion: HU-GNN unifies uncertainty-aware learning and hierarchical representation, offering theoretical guarantees and practical performance.

Abstract: Recent research on graph neural networks (GNNs) has explored mechanisms for
capturing local uncertainty and exploiting graph hierarchies to mitigate data
sparsity and leverage structural properties. However, the synergistic
integration of these two approaches remains underexplored. In this work, we
introduce a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural
Network (HU-GNN), which unifies multi-scale representation learning, principled
uncertainty estimation, and self-supervised embedding diversity within a single
end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and
estimates uncertainty at multiple structural scales from individual nodes to
higher levels. These uncertainty estimates guide a robust message-passing
mechanism and attention weighting, effectively mitigating noise and adversarial
perturbations while preserving predictive accuracy on both node- and
graph-level tasks. We also offer key theoretical contributions, including a
probabilistic formulation, rigorous uncertainty-calibration guarantees, and
formal robustness bounds. Finally, by incorporating recent advances in graph
contrastive learning, HU-GNN maintains diverse, structurally faithful
embeddings. Extensive experiments on standard benchmarks demonstrate that our
model achieves state-of-the-art robustness and interpretability.

</details>


### [495] [Mjölnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density](https://arxiv.org/abs/2504.19822)
*Minjong Cheon*

Main category: cs.LG

TL;DR: Mj"olnir is a deep learning framework for global lightning flash density prediction, achieving high accuracy with a Pearson correlation of 0.96.


<details>
  <summary>Details</summary>
Motivation: To leverage AI for accurate global lightning activity prediction, building on advances in AI-based weather forecasting.

Method: Uses ERA5 and WWLLN data with an InceptionNeXt-SENet architecture and multi-task learning for lightning occurrence and magnitude.

Result: Accurately predicts global lightning distribution, seasonal variability, and regional patterns.

Conclusion: Mj"olnir is a robust AI-based tool for lightning parameterization and future Earth system models.

Abstract: Recent advances in AI-based weather forecasting models, such as FourCastNet,
Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep
learning to emulate complex atmospheric dynamics. Building on this momentum, we
propose Mj\"olnir, a novel deep learning-based framework for global lightning
flash density parameterization. Trained on ERA5 atmospheric predictors and
World Wide Lightning Location Network (WWLLN) observations at a daily temporal
resolution and 1 degree spatial resolution, Mj\"olnir captures the nonlinear
mapping between large-scale environmental conditions and lightning activity.
The model architecture is based on the InceptionNeXt backbone with SENet, and a
multi-task learning strategy to simultaneously predict lightning occurrence and
magnitude. Extensive evaluations yield that Mollnir accurately reproduces the
global distribution, seasonal variability, and regional characteristics of
lightning activity, achieving a global Pearson correlation coefficient of 0.96
for annual mean fields. These results suggest that Mj\"olnir serves not only as
an effective data-driven global lightning parameterization but also as a
promising AI-based scheme for next-generation Earth system models (AI-ESMs).

</details>


### [496] [TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate](https://arxiv.org/abs/2504.19874)
*Amir Zandieh, Majid Daliri, Majid Hadian, Vahab Mirrokni*

Main category: cs.LG

TL;DR: TurboQuant is a novel vector quantization method that achieves near-optimal distortion rates for both MSE and inner product, outperforming existing techniques with minimal quality degradation.


<details>
  <summary>Details</summary>
Motivation: Existing vector quantization methods fail to achieve optimal distortion rates for both MSE and inner product, limiting their effectiveness in applications like KV cache quantization and nearest neighbor search.

Method: TurboQuant uses random rotations to induce a Beta distribution on coordinates, applies optimal scalar quantizers per coordinate, and introduces a two-stage approach (MSE quantizer + 1-bit QJL transform) for unbiased inner product estimation.

Result: TurboQuant achieves near-optimal distortion rates, closely matching theoretical lower bounds. It shows absolute quality neutrality at 3.5 bits per channel and outperforms existing methods in nearest neighbor search tasks.

Conclusion: TurboQuant is a highly efficient and practical solution for vector quantization, offering superior performance with minimal distortion and computational overhead.

Abstract: Vector quantization, a problem rooted in Shannon's source coding theory, aims
to quantize high-dimensional Euclidean vectors while minimizing distortion in
their geometric structure. We propose TurboQuant to address both mean-squared
error (MSE) and inner product distortion, overcoming limitations of existing
methods that fail to achieve optimal distortion rates. Our data-oblivious
algorithms, suitable for online applications, achieve near-optimal distortion
rates (within a small constant factor) across all bit-widths and dimensions.
TurboQuant achieves this by randomly rotating input vectors, inducing a
concentrated Beta distribution on coordinates, and leveraging the
near-independence property of distinct coordinates in high dimensions to simply
apply optimal scalar quantizers per each coordinate. Recognizing that
MSE-optimal quantizers introduce bias in inner product estimation, we propose a
two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL
(QJL) transform on the residual, resulting in an unbiased inner product
quantizer. We also provide a formal proof of the information-theoretic lower
bounds on best achievable distortion rate by any vector quantizer,
demonstrating that TurboQuant closely matches these bounds, differing only by a
small constant ($\approx 2.7$) factor. Experimental results validate our
theoretical findings, showing that for KV cache quantization, we achieve
absolute quality neutrality with 3.5 bits per channel and marginal quality
degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search
tasks, our method outperforms existing product quantization techniques in
recall while reducing indexing time to virtually zero.

</details>


### [497] [Attention Mechanism, Max-Affine Partition, and Universal Approximation](https://arxiv.org/abs/2504.19901)
*Hude Liu, Jerry Yao-Chieh Hu, Zhao Song, Han Liu*

Main category: cs.LG

TL;DR: Single-layer, single-head self- and cross-attention mechanisms can universally approximate continuous and Lebesgue integrable functions with minimal structures.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the universal approximation capability of simplified attention mechanisms, challenging the notion that complex architectures are necessary.

Method: Interpret single-head attention as a domain-partition mechanism, engineer attention weights to imitate target functions, and extend the approach to cross-attention.

Result: Proven capability to approximate any continuous function under $L_\infty$-norm and any Lebesgue integrable function under $L_p$-norm.

Conclusion: Simplified attention mechanisms achieve universal approximation, broadening their theoretical applicability.

Abstract: We establish the universal approximation capability of single-layer,
single-head self- and cross-attention mechanisms with minimal attached
structures. Our key insight is to interpret single-head attention as an input
domain-partition mechanism that assigns distinct values to subregions. This
allows us to engineer the attention weights such that this assignment imitates
the target function. Building on this, we prove that a single self-attention
layer, preceded by sum-of-linear transformations, is capable of approximating
any continuous function on a compact domain under the $L_\infty$-norm.
Furthermore, we extend this construction to approximate any Lebesgue integrable
function under $L_p$-norm for $1\leq p <\infty$. Lastly, we also extend our
techniques and show that, for the first time, single-head cross-attention
achieves the same universal approximation guarantees.

</details>


### [498] [Convergence Analysis of Asynchronous Federated Learning with Gradient Compression for Non-Convex Optimization](https://arxiv.org/abs/2504.19903)
*Diying Yang, Yingwei Hou, Danyang Xiao, Weigang Wu*

Main category: cs.LG

TL;DR: The paper analyzes gradient compression and error feedback in asynchronous federated learning (FL), improving convergence analysis and showing how EF mitigates variance caused by compression and delay.


<details>
  <summary>Details</summary>
Motivation: To systematically study gradient compression and error feedback in asynchronous FL, addressing gaps in understanding their convergence behaviors.

Method: Analyzes convergence under three frameworks: AsynFL (basic asynchronous FL), AsynFLC (with gradient compression), and AsynFLC-EF (with EF). Provides theoretical conditions for convergence and examines interactions between delay, compression, and data heterogeneity.

Result: Improved convergence rates for AsynFL, identified conditions for AsynFLC convergence, and demonstrated EF's effectiveness in reducing gradient variance despite delay.

Conclusion: EF enhances convergence in asynchronous FL by mitigating compression-induced variance, with delay primarily affecting higher-order convergence terms. Experiments validate findings.

Abstract: Gradient compression is an effective technique for reducing communication
costs in federated learning (FL), and error feedback (EF) is usually adopted to
remedy the compression errors. However, there remains a lack of systematic
study on these techniques in asynchronous FL. In this paper, we fill this gap
by analyzing the convergence behaviors of FL under different frameworks. We
firstly consider a basic asynchronous FL framework AsynFL, and provide an
improved convergence analysis that relies on fewer assumptions and yields a
superior convergence rate than prior studies. Then, we consider a variant
framework with gradient compression, AsynFLC. We show sufficient conditions for
its convergence to the optimum, indicating the interaction between asynchronous
delay and compression rate. Our analysis also demonstrates that asynchronous
delay amplifies the variance caused by compression, thereby hindering
convergence, and such an impact is exacerbated by high data heterogeneity.
Furthermore, we study the convergence of AsynFLC-EF, the framework that further
integrates EF. We prove that EF can effectively reduce the variance of gradient
estimation despite asynchronous delay, which enables AsynFLC-EF to match the
convergence rate of AsynFL. We also show that the impact of asynchronous delay
on EF is limited to slowing down the higher-order convergence term.
Experimental results substantiate our analytical findings very well.

</details>


### [499] [Robust Federated Personalised Mean Estimation for the Gaussian Mixture Model](https://arxiv.org/abs/2504.19955)
*Malhar A. Managoli, Vinod M. Prabhakaran, Suhas Diggavi*

Main category: cs.LG

TL;DR: The paper explores combining personalization and robustness in federated learning with corrupted clients, focusing on personalized mean estimation under a Gaussian mixture model. It provides an algorithm with near-linear error dependence on corruption ratio and a matching lower bound.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of combining personalization for heterogeneous data with robustness against corrupted clients in federated learning.

Method: Formulates a simplified problem of personalized mean estimation using a Gaussian mixture model and develops an algorithm for this scenario.

Result: The algorithm's error scales almost linearly with the ratio of corrupted to uncorrupted samples, with a lower bound showing similar behavior.

Conclusion: The study demonstrates feasibility in combining personalization and robustness, with theoretical guarantees on error behavior.

Abstract: Federated learning with heterogeneous data and personalization has received
significant recent attention. Separately, robustness to corrupted data in the
context of federated learning has also been studied. In this paper we explore
combining personalization for heterogeneous data with robustness, where a
constant fraction of the clients are corrupted. Motivated by this broad
problem, we formulate a simple instantiation which captures some of its
difficulty. We focus on the specific problem of personalized mean estimation
where the data is drawn from a Gaussian mixture model. We give an algorithm
whose error depends almost linearly on the ratio of corrupted to uncorrupted
samples, and show a lower bound with the same behavior, albeit with a gap of a
constant factor.

</details>


### [500] [Transfer Learning Under High-Dimensional Network Convolutional Regression Model](https://arxiv.org/abs/2504.19979)
*Liyuan Wang, Jiachen Chen, Kathryn L. Lunetta, Danyang Huang, Huimin Cheng, Debarghya Mukherjee*

Main category: cs.LG

TL;DR: A transfer learning framework (NCR) for networked data improves prediction accuracy by leveraging dependencies and domain shifts, validated theoretically and empirically.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of transfer learning in networked data with dependencies, where existing methods fall short.

Method: Proposes a network convolutional regression (NCR) model, incorporating local dependencies and a two-step transfer learning algorithm with source detection.

Result: Theoretical analysis shows improved convergence rates; empirical tests on simulations and Sina Weibo data confirm higher accuracy, especially with limited labeled data.

Conclusion: The NCR framework effectively handles networked data dependencies and domain shifts, enhancing transfer learning performance.

Abstract: Transfer learning enhances model performance by utilizing knowledge from
related domains, particularly when labeled data is scarce. While existing
research addresses transfer learning under various distribution shifts in
independent settings, handling dependencies in networked data remains
challenging. To address this challenge, we propose a high-dimensional transfer
learning framework based on network convolutional regression (NCR), inspired by
the success of graph convolutional networks (GCNs). The NCR model incorporates
random network structure by allowing each node's response to depend on its
features and the aggregated features of its neighbors, capturing local
dependencies effectively. Our methodology includes a two-step transfer learning
algorithm that addresses domain shift between source and target networks, along
with a source detection mechanism to identify informative domains.
Theoretically, we analyze the lasso estimator in the context of a random graph
based on the Erdos-Renyi model assumption, demonstrating that transfer learning
improves convergence rates when informative sources are present. Empirical
evaluations, including simulations and a real-world application using Sina
Weibo data, demonstrate substantial improvements in prediction accuracy,
particularly when labeled data in the target domain is limited.

</details>


### [501] [Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets](https://arxiv.org/abs/2504.19981)
*Adam Younsi, Abdalgader Abubaker, Mohamed El Amine Seddik, Hakim Hacid, Salem Lahlou*

Main category: cs.LG

TL;DR: The paper introduces a Process Reward Model (PRM) and Generative Flow Networks (GFlowNets) to improve accuracy and diversity in LLM reasoning for mathematics, showing significant gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of evaluating intermediate reasoning steps in LLMs without costly human annotations to enhance mathematical reasoning.

Method: Develops a PRM using Monte Carlo Tree Search and data augmentation, then adapts GFlowNets to sample diverse, high-quality solutions based on PRM rewards.

Result: Improves accuracy (+2.59% on MATH Level 5) and diversity, with generalization (+9.4% on SAT MATH).

Conclusion: PRM-guided GFlowNets enhance LLM robustness and versatility in mathematical reasoning.

Abstract: Achieving both accuracy and diverse reasoning remains challenging for Large
Language Models (LLMs) in complex domains like mathematics. A key bottleneck is
evaluating intermediate reasoning steps to guide generation without costly
human annotations. To address this, we first introduce a novel Process Reward
Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a
similarity-based data augmentation technique, effectively capturing step-level
reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks
(GFlowNets) to operate at the reasoning step level. Unlike traditional
reinforcement learning focused on maximizing a single reward, GFlowNets
naturally sample diverse, high-quality solutions proportional to their rewards,
as measured by our PRM. Empirical evaluation shows strong improvements in both
accuracy and solution diversity on challenging mathematical benchmarks (e.g.,
+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective
generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work
demonstrates the potential of PRM-guided, step-level GFlowNets for developing
more robust and versatile mathematical reasoning in LLMs.

</details>


### [502] [Emergence and scaling laws in SGD learning of shallow neural networks](https://arxiv.org/abs/2504.19983)
*Yunwei Ren, Eshaan Nichani, Denny Wu, Jason D. Lee*

Main category: cs.LG

TL;DR: The paper analyzes the complexity of online SGD for training a two-layer neural network with isotropic Gaussian data, focusing on the extensive-width regime and power-law scaling. It identifies sharp transition times for signal recovery and characterizes scaling laws for MSE loss.


<details>
  <summary>Details</summary>
Motivation: Understanding the dynamics of SGD in learning two-layer neural networks, especially in the extensive-width regime with diverging condition numbers, to provide insights into signal recovery and scaling laws.

Method: The study uses online SGD to train a student two-layer network, analyzing its dynamics for MSE minimization. It examines the power-law scaling of second-layer coefficients and identifies transition times for signal recovery.

Result: The paper identifies sharp transition times for recovering signal directions and characterizes scaling law exponents for MSE loss with respect to training samples, SGD steps, and network parameters.

Conclusion: While individual teacher neurons show abrupt transitions, the combination of many emergent learning curves results in a smooth cumulative scaling law.

Abstract: We study the complexity of online stochastic gradient descent (SGD) for
learning a two-layer neural network with $P$ neurons on isotropic Gaussian
data: $f_*(\boldsymbol{x}) = \sum_{p=1}^P a_p\cdot
\sigma(\langle\boldsymbol{x},\boldsymbol{v}_p^*\rangle)$, $\boldsymbol{x} \sim
\mathcal{N}(0,\boldsymbol{I}_d)$, where the activation
$\sigma:\mathbb{R}\to\mathbb{R}$ is an even function with information exponent
$k_*>2$ (defined as the lowest degree in the Hermite expansion),
$\{\boldsymbol{v}^*_p\}_{p\in[P]}\subset \mathbb{R}^d$ are orthonormal signal
directions, and the non-negative second-layer coefficients satisfy $\sum_{p}
a_p^2=1$. We focus on the challenging ``extensive-width'' regime $P\gg 1$ and
permit diverging condition number in the second-layer, covering as a special
case the power-law scaling $a_p\asymp p^{-\beta}$ where
$\beta\in\mathbb{R}_{\ge 0}$. We provide a precise analysis of SGD dynamics for
the training of a student two-layer network to minimize the mean squared error
(MSE) objective, and explicitly identify sharp transition times to recover each
signal direction. In the power-law setting, we characterize scaling law
exponents for the MSE loss with respect to the number of training samples and
SGD steps, as well as the number of parameters in the student neural network.
Our analysis entails that while the learning of individual teacher neurons
exhibits abrupt transitions, the juxtaposition of $P\gg 1$ emergent learning
curves at different timescales leads to a smooth scaling law in the cumulative
objective.

</details>


### [503] [Modelling of Underwater Vehicles using Physics-Informed Neural Networks with Control](https://arxiv.org/abs/2504.20019)
*Abdelhakim Amer, David Felsager, Yury Brodskiy, Andriy Sarabakha*

Main category: cs.LG

TL;DR: An open-source implementation of Physics-Informed Neural Network with Control (PINC) improves underwater vehicle dynamics modeling by integrating physical laws and control actions.


<details>
  <summary>Details</summary>
Motivation: To enhance generalization and sample efficiency in modeling underwater vehicle dynamics by combining physics and data-driven approaches.

Method: PINC extends PINNs with control inputs, testing various configurations like loss functions and hyperparameters.

Result: PINC achieves more accurate long-horizon predictions than non-physics-informed baselines in simulations.

Conclusion: PINC effectively integrates physics and control for improved underwater vehicle dynamics modeling.

Abstract: Physics-informed neural networks (PINNs) integrate physical laws with
data-driven models to improve generalization and sample efficiency. This work
introduces an open-source implementation of the Physics-Informed Neural Network
with Control (PINC) framework, designed to model the dynamics of an underwater
vehicle. Using initial states, control actions, and time inputs, PINC extends
PINNs to enable physically consistent transitions beyond the training domain.
Various PINC configurations are tested, including differing loss functions,
gradient-weighting schemes, and hyperparameters. Validation on a simulated
underwater vehicle demonstrates more accurate long-horizon predictions compared
to a non-physics-informed baseline

</details>


### [504] [Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models](https://arxiv.org/abs/2504.20020)
*Xin Wang, Haoyang Li, Zeyang Zhang, Haibo Chen, Wenwu Zhu*

Main category: cs.LG

TL;DR: The paper introduces Modular Machine Learning (MML) to address limitations in LLMs like reasoning, factual consistency, and interpretability by decomposing them into modular components.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack reasoning, factual consistency, and interpretability, limiting their reliability and applicability.

Method: MML divides LLMs into modular representation, model, and reasoning, using techniques like disentangled representation learning and neuro-symbolic learning.

Result: MML enhances counterfactual reasoning, reduces hallucinations, and improves fairness, safety, and transparency in LLMs.

Conclusion: MML bridges statistical learning and formal reasoning, promising robust and trustworthy AI systems for real-world applications.

Abstract: Large language models (LLMs) have dramatically advanced machine learning
research including natural language processing, computer vision, data mining,
etc., yet they still exhibit critical limitations in reasoning, factual
consistency, and interpretability. In this paper, we introduce a novel learning
paradigm -- Modular Machine Learning (MML) -- as an essential approach toward
new-generation LLMs. MML decomposes the complex structure of LLMs into three
interdependent components: modular representation, modular model, and modular
reasoning, aiming to enhance LLMs' capability of counterfactual reasoning,
mitigating hallucinations, as well as promoting fairness, safety, and
transparency. Specifically, the proposed MML paradigm can: i) clarify the
internal working mechanism of LLMs through the disentanglement of semantic
components; ii) allow for flexible and task-adaptive model design; iii) enable
interpretable and logic-driven decision-making process. We present a feasible
implementation of MML-based LLMs via leveraging advanced techniques such as
disentangled representation learning, neural architecture search and
neuro-symbolic learning. We critically identify key challenges, such as the
integration of continuous neural and discrete symbolic processes, joint
optimization, and computational scalability, present promising future research
directions that deserve further exploration. Ultimately, the integration of the
MML paradigm with LLMs has the potential to bridge the gap between statistical
(deep) learning and formal (logical) reasoning, thereby paving the way for
robust, adaptable, and trustworthy AI systems across a wide range of real-world
applications.

</details>


### [505] [NoisyHate: Mining Online Human-Written Perturbations for Realistic Robustness Benchmarking of Content Moderation Models](https://arxiv.org/abs/2303.10430)
*Yiran Ye, Thai Le, Dongwon Lee*

Main category: cs.LG

TL;DR: The paper introduces NoisyHate, a dataset of human-written toxic text perturbations, to improve toxic speech detection by addressing gaps in algorithm-generated datasets.


<details>
  <summary>Details</summary>
Motivation: Existing toxic content detection tools struggle with human-written perturbations, which differ from algorithm-generated ones. NoisyHate aims to bridge this gap.

Method: The authors created NoisyHate, a dataset of real-life human-written perturbations, and validated it using state-of-the-art language models and APIs.

Result: NoisyHate exhibits unique traits compared to algorithm-generated datasets, enhancing toxic speech detection.

Conclusion: NoisyHate is a valuable resource for developing better toxic content detection tools, as it captures human-written perturbations more effectively.

Abstract: Online texts with toxic content are a clear threat to the users on social
media in particular and society in general. Although many platforms have
adopted various measures (e.g., machine learning-based hate-speech detection
systems) to diminish their effect, toxic content writers have also attempted to
evade such measures by using cleverly modified toxic words, so-called
human-written text perturbations. Therefore, to help build automatic detection
tools to recognize those perturbations, prior methods have developed
sophisticated techniques to generate diverse adversarial samples. However, we
note that these ``algorithms"-generated perturbations do not necessarily
capture all the traits of ``human"-written perturbations. Therefore, in this
paper, we introduce a novel, high-quality dataset of human-written
perturbations, named as NoisyHate, that was created from real-life
perturbations that are both written and verified by human-in-the-loop. We show
that perturbations in NoisyHate have different characteristics than prior
algorithm-generated toxic datasets show, and thus can be in particular useful
to help develop better toxic speech detection solutions. We thoroughly validate
NoisyHate against state-of-the-art language models, such as BERT and RoBERTa,
and black box APIs, such as Perspective API, on two tasks, such as perturbation
normalization and understanding.

</details>


### [506] [Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans](https://arxiv.org/abs/2307.12369)
*Rumeng Li, Xun Wang, Dan Berlowitz, Brian Silver, Wen Hu, Heather Keating, Raelene Goodwin, Weisong Liu, Honghuang Lin, Hong Yu*

Main category: cs.LG

TL;DR: Machine learning models using AD-related keywords from EHRs predict Alzheimer's disease (AD) up to ten years before diagnosis with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Early prediction of AD is critical for timely intervention, and EHRs provide a rich data source for identifying early signs.

Method: Used longitudinal EHRs from VHA, case-control design, and four machine learning models to analyze AD-related keywords over time.

Result: Best model achieved ROCAUC 0.997 for predictions up to ten years before diagnosis, with consistent performance across subgroups except patients under 65.

Conclusion: EHR-based machine learning models offer an affordable and effective tool for early AD screening in large populations.

Abstract: Early prediction of Alzheimer's disease (AD) is crucial for timely
intervention and treatment. This study aims to use machine learning approaches
to analyze longitudinal electronic health records (EHRs) of patients with AD
and identify signs and symptoms that can predict AD onset earlier. We used a
case-control design with longitudinal EHRs from the U.S. Department of Veterans
Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA
patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9
with controls by age, sex and clinical utilization with replacement. We used a
panel of AD-related keywords and their occurrences over time in a patient's
longitudinal EHRs as predictors for AD prediction with four machine learning
models. We performed subgroup analyses by age, sex, and race/ethnicity, and
validated the model in a hold-out and "unseen" VHA stations group. Model
discrimination, calibration, and other relevant metrics were reported for
predictions up to ten years before ICD-based diagnosis. The study population
included 16,701 cases and 39,097 matched controls. The average number of
AD-related keywords (e.g., "concentration", "speaking") per year increased
rapidly for cases as diagnosis approached, from around 10 to over 40, while
remaining flat at 10 for controls. The best model achieved high discriminative
accuracy (ROCAUC 0.997) for predictions using data from at least ten years
before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow
goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and
race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine
learning models using AD-related keywords identified from EHR notes can predict
future AD diagnoses, suggesting its potential use for identifying AD risk using
EHR notes, offering an affordable way for early screening on large population.

</details>


### [507] [Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization](https://arxiv.org/abs/2410.08847)
*Noam Razin, Sadhika Malladi, Adithya Bhaskar, Danqi Chen, Sanjeev Arora, Boris Hanin*

Main category: cs.LG

TL;DR: DPO and variants align models with human preferences but can reduce the likelihood of preferred responses, termed likelihood displacement, which may shift probability to opposite or harmful responses. A CHES score identifies problematic samples, and filtering them mitigates unalignment.


<details>
  <summary>Details</summary>
Motivation: To understand why preferred responses' likelihood decreases during DPO training and its implications, including unintended shifts to harmful responses.

Method: Theoretical analysis using CHES score to measure embedding similarity and empirical validation by filtering problematic samples.

Result: Likelihood displacement is driven by similar embeddings; CHES score identifies problematic samples, and filtering them reduces unalignment.

Conclusion: Data curation with distinct preferences is crucial; CHES score aids in identifying and mitigating likelihood displacement.

Abstract: Direct Preference Optimization (DPO) and its variants are increasingly used
for aligning language models with human preferences. Although these methods are
designed to teach a model to generate preferred responses more frequently
relative to dispreferred responses, prior work has observed that the likelihood
of preferred responses often decreases during training. The current work sheds
light on the causes and implications of this counter-intuitive phenomenon,
which we term likelihood displacement. We demonstrate that likelihood
displacement can be catastrophic, shifting probability mass from preferred
responses to responses with an opposite meaning. As a simple example, training
a model to prefer $\texttt{No}$ over $\texttt{Never}$ can sharply increase the
probability of $\texttt{Yes}$. Moreover, when aligning the model to refuse
unsafe prompts, we show that such displacement can unintentionally lead to
unalignment, by shifting probability mass from preferred refusal responses to
harmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from
74.4% to 33.4%). We theoretically characterize that likelihood displacement is
driven by preferences that induce similar embeddings, as measured by a centered
hidden embedding similarity (CHES) score. Empirically, the CHES score enables
identifying which training samples contribute most to likelihood displacement
in a given dataset. Filtering out these samples effectively mitigated
unintentional unalignment in our experiments. More broadly, our results
highlight the importance of curating data with sufficiently distinct
preferences, for which we believe the CHES score may prove valuable.

</details>


### [508] [CREAM: Consistency Regularized Self-Rewarding Language Models](https://arxiv.org/abs/2410.12735)
*Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao*

Main category: cs.LG

TL;DR: The paper introduces CREAM, a self-rewarding LLM framework with regularization to mitigate bias and improve alignment performance by ensuring reward consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of accuracy and accumulated bias in self-rewarding LLMs, which can degrade alignment performance over iterations.

Method: Proposes CREAM, a framework with regularization to ensure reward consistency across iterations, improving the reliability of preference data.

Result: Empirical results show CREAM enhances reward consistency and alignment performance compared to standard self-rewarding methods.

Conclusion: CREAM effectively mitigates bias in self-rewarding LLMs, leading to more reliable preference data and improved alignment.

Abstract: Recent self-rewarding large language models (LLM) have successfully applied
LLM-as-a-Judge to iteratively improve the alignment performance without the
need of human annotations for preference data. These methods commonly utilize
the same LLM to act as both the policy model (which generates responses) and
the reward model (which scores and ranks those responses). The ranked responses
are then used as preference pairs to train the LLM via direct alignment
technologies (e.g. DPO). However, it is noteworthy that throughout this
process, there is no guarantee of accuracy in the rewarding and ranking, which
is critical for ensuring accurate rewards and high-quality preference data.
Empirical results from relatively small LLMs (e.g., 7B parameters) also
indicate that improvements from self-rewarding may diminish after several
iterations in certain situations, which we hypothesize is due to accumulated
bias in the reward system. This bias can lead to unreliable preference data for
training the LLM. To address this issue, we first formulate and analyze the
generalized iterative preference fine-tuning framework for self-rewarding
language model. We then introduce the regularization to this generalized
framework to mitigate the overconfident preference labeling in the
self-rewarding process. Based on this theoretical insight, we propose a
Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages
the consistency of rewards across different iterations to regularize the
self-rewarding training, helping the model to learn from more reliable
preference data. With this explicit regularization, our empirical results
demonstrate the superiority of CREAM in improving both reward consistency and
alignment performance. The code is publicly available at
https://github.com/Raibows/CREAM.

</details>


### [509] [Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models](https://arxiv.org/abs/2410.18252)
*Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, Aaron Courville*

Main category: cs.LG

TL;DR: The paper proposes asynchronous RLHF for faster training by separating generation and learning, finding online DPO robust to off-policy data, and validating scalability with a chatbot and math task.


<details>
  <summary>Details</summary>
Motivation: Current RLHF methods are computationally inefficient due to synchronous generation and learning. The goal is to improve efficiency without sacrificing performance.

Method: Separates generation and learning in RLHF, enabling asynchronous training. Tests various RLHF algorithms, focusing on online DPO's robustness to off-policy data.

Result: Asynchronous RLHF trains a chatbot 40% faster and a math model 70% faster while matching synchronous performance.

Conclusion: Asynchronous RLHF offers a compute-efficient alternative to synchronous methods, with online DPO being particularly robust to off-policy data.

Abstract: The dominant paradigm for RLHF is online and on-policy RL: synchronously
generating from the large language model (LLM) policy, labelling with a reward
model, and learning using feedback on the LLM's own outputs. While performant,
this paradigm is computationally inefficient. Inspired by classical deep RL
literature, we propose separating generation and learning in RLHF. This enables
asynchronous generation of new samples while simultaneously training on old
samples, leading to faster training and more compute-optimal scaling. However,
asynchronous training relies on an underexplored regime, online but off-policy
RLHF: learning on samples from previous iterations of our model which give a
worse training signal. We tackle the fundamental challenge in this regime: how
much off-policyness can we tolerate for asynchronous training to speed up
learning but maintain performance? Among several RLHF algorithms we test,
online DPO is found to be most robust to off-policy data, and robustness
increases with the scale of the policy model. We study further compute
optimizations for asynchronous RLHF but find that they come at a performance
cost, giving rise to a trade-off. We verify the scalability of asynchronous
RLHF by training a general-purpose chatbot from LLaMA 3.1 8B on an
instruction-following task ~40% faster than a synchronous run while matching
final performance. Finally, we extend our results to math and reasoning to
demonstrate asynchronous RL can finetune Rho 1B on GSM8k ~70% faster while
matching synchronous accuracy.

</details>


### [510] [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465)
*Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen*

Main category: cs.LG

TL;DR: ShadowKV is a system for high-throughput long-context LLM inference, reducing memory footprint and boosting throughput without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: The demand for efficient high-throughput inference in long-context LLMs is unmet due to memory and latency issues with current methods.

Method: ShadowKV stores low-rank key cache and offloads value cache, using an accurate KV selection strategy to minimize latency.

Result: It supports 6× larger batch sizes and boosts throughput by 3.04× on an A100 GPU, maintaining accuracy.

Conclusion: ShadowKV effectively addresses memory and throughput challenges in long-context LLM inference.

Abstract: With the widespread deployment of long-context large language models (LLMs),
there has been a growing demand for efficient support of high-throughput
inference. However, as the key-value (KV) cache expands with the sequence
length, the increasing memory footprint and the need to access it for each
token generation both result in low throughput when serving long-context LLMs.
While various dynamic sparse attention methods have been proposed to speed up
inference while maintaining generation quality, they either fail to
sufficiently reduce GPU memory consumption or introduce significant decoding
latency by offloading the KV cache to the CPU. We present ShadowKV, a
high-throughput long-context LLM inference system that stores the low-rank key
cache and offloads the value cache to reduce the memory footprint for larger
batch sizes and longer sequences. To minimize decoding latency, ShadowKV
employs an accurate KV selection strategy that reconstructs minimal sparse KV
pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,
including RULER, LongBench, and Needle In A Haystack, and models like
Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and
Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch
sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without
sacrificing accuracy, even surpassing the performance achievable with infinite
batch size under the assumption of infinite GPU memory. The code is available
at https://github.com/bytedance/ShadowKV.

</details>


### [511] [Finding Minimum-Cost Explanations for Predictions made by Tree Ensembles](https://arxiv.org/abs/2303.09271)
*John Törnblom, Emil Karlsson, Simin Nadjm-Tehrani*

Main category: cs.LG

TL;DR: The paper focuses on finding provably correct, minimal, and cost-efficient explanations for predictions made by tree ensembles, introducing an efficient oracle and an adapted algorithm (m-MARCO) to achieve this.


<details>
  <summary>Details</summary>
Motivation: To provide human operators of critical systems with trustworthy, concise, and cost-effective explanations for machine learning model predictions.

Method: 1. Introduces a highly efficient oracle for verifying explanation correctness. 2. Adapts the MARCO algorithm (m-MARCO) to compute single minimum explanations.

Result: Demonstrates significant speed improvements (orders of magnitude for minimal explanations, 2x for m-MARCO) and reveals that minimum explanations are less verbose and rare among minimal ones.

Conclusion: The work successfully advances the efficiency and quality of explanations for tree ensemble predictions, emphasizing the value of minimum explanations in practice.

Abstract: The ability to explain why a machine learning model arrives at a particular
prediction is crucial when used as decision support by human operators of
critical systems. The provided explanations must be provably correct, and
preferably without redundant information, called minimal explanations. In this
paper, we aim at finding explanations for predictions made by tree ensembles
that are not only minimal, but also minimum with respect to a cost function.
  To this end, we first present a highly efficient oracle that can determine
the correctness of explanations, surpassing the runtime performance of current
state-of-the-art alternatives by several orders of magnitude when computing
minimal explanations.
  Secondly, we adapt an algorithm called MARCO from related works (calling it
m-MARCO) for the purpose of computing a single minimum explanation per
prediction, and demonstrate an overall speedup factor of two compared to the
MARCO algorithm which enumerates all minimal explanations.
  Finally, we study the obtained explanations from a range of use cases,
leading to further insights of their characteristics. In particular, we observe
that in several cases, there are more than 100,000 minimal explanations to
choose from for a single prediction. In these cases, we see that only a small
portion of the minimal explanations are also minimum, and that the minimum
explanations are significantly less verbose, hence motivating the aim of this
work.

</details>


### [512] [An Empirical Study of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration](https://arxiv.org/abs/2307.08187)
*Hiroki Naganuma, Ryuichiro Hataya, Kotaro Yoshida, Ioannis Mitliagkas*

Main category: cs.LG

TL;DR: Pre-trained model size, dataset size, and training strategies significantly impact OOD generalization and calibration, with larger models and datasets improving both performance and confidence calibration.


<details>
  <summary>Details</summary>
Motivation: To systematically study how pre-trained model selection (size, dataset, training strategies) affects OOD generalization and calibration, beyond focusing solely on learning algorithms.

Method: Evaluated 100 models across varying sizes, five pre-training datasets, and five data augmentations on four distribution shift datasets (120,000+ GPU hours).

Result: Optimal pre-trained model choices improve OOD accuracy more than algorithm improvements. Larger models and datasets enhance both OOD performance and calibration, countering prior findings.

Conclusion: Pre-trained model selection is crucial for OOD generalization and calibration, with larger models and datasets offering significant benefits.

Abstract: In the field of computer vision, fine-tuning pre-trained models has become a
prevalent strategy for out-of-distribution (OOD) generalization tasks.
Different from most prior work that has focused on advancing learning
algorithms, we systematically examined how pre-trained model size, pre-training
dataset size, and training strategies impact generalization and confidence
calibration on downstream tasks. We evaluated 100 models across diverse
pre-trained model sizes, five pre-training datasets, and five data
augmentations through extensive experiments on four distribution shift datasets
totaling over 120,000 GPU hours. Our results demonstrate the significant impact
of pre-trained model selection, with optimal choices substantially improving
OOD accuracy over algorithm improvement alone. Additionally, we find that
larger models and bigger pre-training datasets not only enhance OOD performance
but also improve calibration, helping to mitigate overconfidence, contrary to
some prior studies that found modern deep networks to calibrate worse than
classical shallow models. Our work underscores the overlooked importance of
pre-trained model selection for out-of-distribution generalization and
calibration.

</details>


### [513] [Learning Temporal Logic Predicates from Data with Statistical Guarantees](https://arxiv.org/abs/2406.10449)
*Emi Soroka, Rohan Sinha, Sanjay Lall*

Main category: cs.LG

TL;DR: A novel method for learning temporal logic predicates from data with correctness guarantees, using expression optimization and conformal prediction.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack correctness assurances for learned temporal logic predicates, limiting their reliability in applications like safety validation and motion planning.

Method: Leverages expression optimization and conformal prediction to ensure finite-sample correctness guarantees under mild statistical assumptions.

Result: Demonstrated performance on a simulated trajectory dataset, with ablation studies highlighting the contribution of each algorithm component.

Conclusion: The approach provides reliable temporal logic predicates with correctness guarantees, enhancing applications in control and robotics.

Abstract: Temporal logic rules are often used in control and robotics to provide
structured, human-interpretable descriptions of trajectory data. These rules
have numerous applications including safety validation using formal methods,
constraining motion planning among autonomous agents, and classifying data.
However, existing methods for learning temporal logic predicates from data do
not provide assurances about the correctness of the resulting predicate. We
present a novel method to learn temporal logic predicates from data with
finite-sample correctness guarantees. Our approach leverages expression
optimization and conformal prediction to learn predicates that correctly
describe future trajectories under mild statistical assumptions. We provide
experimental results showing the performance of our approach on a simulated
trajectory dataset and perform ablation studies to understand how each
component of our algorithm contributes to its performance.

</details>


### [514] [Retrieval Augmented Generation for Dynamic Graph Modeling](https://arxiv.org/abs/2408.14523)
*Yuxia Wu, Lizi Liao, Yuan Fang*

Main category: cs.LG

TL;DR: RAG4DyG is a novel framework for dynamic graph modeling that improves predictions by integrating contextually and temporally relevant examples, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for dynamic graph modeling focus narrowly on historical contexts, limiting adaptability to new patterns. RAG4DyG aims to address this by leveraging broader graph structures.

Method: The framework uses time- and context-aware contrastive learning to identify high-quality examples and a graph fusion strategy to integrate them with historical data.

Result: Experiments show RAG4DyG improves predictive accuracy and adaptability in dynamic graph modeling.

Conclusion: RAG4DyG is effective for both transductive and inductive scenarios, offering a robust solution for evolving graph structures.

Abstract: Modeling dynamic graphs, such as those found in social networks,
recommendation systems, and e-commerce platforms, is crucial for capturing
evolving relationships and delivering relevant insights over time. Traditional
approaches primarily rely on graph neural networks with temporal components or
sequence generation models, which often focus narrowly on the historical
context of target nodes. This limitation restricts the ability to adapt to new
and emerging patterns in dynamic graphs. To address this challenge, we propose
a novel framework, Retrieval-Augmented Generation for Dynamic Graph modeling
(RAG4DyG), which enhances dynamic graph predictions by incorporating
contextually and temporally relevant examples from broader graph structures.
Our approach includes a time- and context-aware contrastive learning module to
identify high-quality demonstrations and a graph fusion strategy to effectively
integrate these examples with historical contexts. The proposed framework is
designed to be effective in both transductive and inductive scenarios, ensuring
adaptability to previously unseen nodes and evolving graph structures.
Extensive experiments across multiple real-world datasets demonstrate the
effectiveness of RAG4DyG in improving predictive accuracy and adaptability for
dynamic graph modeling. The code and datasets are publicly available at
https://github.com/YuxiaWu/RAG4DyG.

</details>


### [515] [Robust Collaborative Inference with Vertically Split Data Over Dynamic Device Environments](https://arxiv.org/abs/2312.16638)
*Surojit Ganguli, Zeyu Zhou, Christopher G. Brinton, David I. Inouye*

Main category: cs.LG

TL;DR: The paper proposes MAGS, a method for robust collaborative inference in dynamic networks prone to significant faults, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing collaborative learning methods assume stable networks, making them vulnerable to failures in safety-critical applications.

Method: MAGS uses simulated faults (dropout, replication, gossiping) to enhance robustness in dynamic networks.

Result: MAGS significantly improves robustness across varying fault rates, including extreme cases.

Conclusion: MAGS provides a practical and theoretically sound solution for robust collaborative inference in fault-prone networks.

Abstract: When each edge device of a network only perceives a local part of the
environment, collaborative inference across multiple devices is often needed to
predict global properties of the environment. In safety-critical applications,
collaborative inference must be robust to significant network failures caused
by environmental disruptions or extreme weather. Existing collaborative
learning approaches, such as privacy-focused Vertical Federated Learning (VFL),
typically assume a centralized setup or that one device never fails. However,
these assumptions make prior approaches susceptible to significant network
failures. To address this problem, we first formalize the problem of robust
collaborative inference over a dynamic network of devices that could experience
significant network faults. Then, we develop a minimalistic yet impactful
method called Multiple Aggregation with Gossip Rounds and Simulated Faults
(MAGS) that synthesizes simulated faults via dropout, replication, and
gossiping to significantly improve robustness over baselines. We also
theoretically analyze our proposed approach to explain why each component
enhances robustness. Extensive empirical results validate that MAGS is robust
across a range of fault rates-including extreme fault rates.

</details>


### [516] [Predictive Churn with the Set of Good Models](https://arxiv.org/abs/2402.07745)
*Jamelle Watson-Daniels, Flavio du Pin Calmon, Alexander D'Amour, Carol Long, David C. Parkes, Berk Ustun*

Main category: cs.LG

TL;DR: The paper bridges predictive multiplicity (conflicting predictions from similar models) and predictive churn (prediction changes after updates), highlighting their connections.


<details>
  <summary>Details</summary>
Motivation: Address the gap between fairness/transparency research and practical deployment concerns by linking predictive multiplicity and churn.

Method: Theoretical and empirical analysis to explore parallels between predictive multiplicity and churn.

Result: Uncovered links between the two concepts, showing their fundamental relationship.

Conclusion: Translational work is needed to connect fairness research with deployment challenges, as demonstrated by the study.

Abstract: Issues can arise when research focused on fairness, transparency, or safety
is conducted separately from research driven by practical deployment concerns
and vice versa. This separation creates a growing need for translational work
that bridges the gap between independently studied concepts that may be
fundamentally related. This paper explores connections between two seemingly
unrelated concepts of predictive inconsistency that share intriguing parallels.
The first, known as predictive multiplicity, occurs when models that perform
similarly (e.g., nearly equivalent training loss) produce conflicting
predictions for individual samples. This concept is often emphasized in
algorithmic fairness research as a means of promoting transparency in ML model
development. The second concept, predictive churn, examines the differences in
individual predictions before and after model updates, a key challenge in
deploying ML models in consumer-facing applications. We present theoretical and
empirical results that uncover links between these previously disconnected
concepts.

</details>


### [517] [Hyperparameters in Continual Learning: A Reality Check](https://arxiv.org/abs/2403.09066)
*Sungmin Cha, Kyunghyun Cho*

Main category: cs.LG

TL;DR: The paper critiques the conventional evaluation protocol for continual learning (CL) algorithms, proposing a new Generalizable Two-phase Evaluation Protocol (GTEP) to better assess CL capacity across unseen scenarios. Results show overestimation of current algorithms' performance.


<details>
  <summary>Details</summary>
Motivation: Current CL evaluation protocols overestimate algorithm performance by tuning hyperparameters on the same scenario, which is unrealistic for real-world applications.

Method: Proposes GTEP, a two-phase protocol: hyperparameter tuning on one dataset and evaluation on another, both with the same scenario configuration.

Result: Over 8,000 experiments reveal most state-of-the-art CL algorithms fail to replicate reported performance, indicating overestimation.

Conclusion: GTEP provides a more realistic and generalizable evaluation framework for CL algorithms, exposing limitations of current methods.

Abstract: Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a
CL scenario) while balancing the trade-off between plasticity (learning new
tasks) and stability (retaining prior knowledge). The dominantly adopted
conventional evaluation protocol for CL algorithms selects the best
hyperparameters (e.g., learning rate, mini-batch size, regularization
strengths, etc.) within a given scenario and then evaluates the algorithms
using these hyperparameters in the same scenario. However, this protocol has
significant shortcomings: it overestimates the CL capacity of algorithms and
relies on unrealistic hyperparameter tuning, which is not feasible for
real-world applications. From the fundamental principles of evaluation in
machine learning, we argue that the evaluation of CL algorithms should focus on
assessing the generalizability of their CL capacity to unseen scenarios. Based
on this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP)
consisting of hyperparameter tuning and evaluation phases. Both phases share
the same scenario configuration (e.g., number of tasks) but are generated from
different datasets. Hyperparameters of CL algorithms are tuned in the first
phase and applied in the second phase to evaluate the algorithms. We apply this
protocol to class-incremental learning, both with and without pretrained
models. Across more than 8,000 experiments, our results show that most
state-of-the-art algorithms fail to replicate their reported performance,
highlighting that their CL capacity has been significantly overestimated in the
conventional evaluation protocol. Our implementation can be found in
https://github.com/csm9493/GTEP.

</details>


### [518] [Variational Bayesian Optimal Experimental Design with Normalizing Flows](https://arxiv.org/abs/2404.13056)
*Jiayuan Dong, Christian Jacobsen, Mehdi Khalloufi, Maryam Akram, Wanjiao Liu, Karthik Duraisamy, Xun Huan*

Main category: cs.LG

TL;DR: vOED-NFs uses normalizing flows to estimate a lower bound of expected information gain (EIG) in Bayesian OED, improving accuracy and efficiency over nested Monte Carlo methods.


<details>
  <summary>Details</summary>
Motivation: Direct EIG estimation via nested Monte Carlo is computationally expensive and requires explicit likelihoods, motivating the need for variational methods like vOED-NFs.

Method: vOED-NFs employs conditional invertible neural networks (coupling layers) and a summary network for variational posterior approximation, optimizing variational parameters and design variables simultaneously.

Result: vOED-NFs achieves lower EIG bias with 4-5 coupling layers, capturing non-Gaussian and multi-modal features in posteriors, validated on benchmark and real-world problems.

Conclusion: vOED-NFs offers a computationally efficient and accurate alternative for Bayesian OED, particularly for complex, implicit-likelihood models.

Abstract: Bayesian optimal experimental design (OED) seeks experiments that maximize
the expected information gain (EIG) in model parameters. Directly estimating
the EIG using nested Monte Carlo is computationally expensive and requires an
explicit likelihood. Variational OED (vOED), in contrast, estimates a lower
bound of the EIG without likelihood evaluations by approximating the posterior
distributions with variational forms, and then tightens the bound by optimizing
its variational parameters. We introduce the use of normalizing flows (NFs) for
representing variational distributions in vOED; we call this approach vOED-NFs.
Specifically, we adopt NFs with a conditional invertible neural network
architecture built from compositions of coupling layers, and enhanced with a
summary network for data dimension reduction. We present Monte Carlo estimators
to the lower bound along with gradient expressions to enable a gradient-based
simultaneous optimization of the variational parameters and the design
variables. The vOED-NFs algorithm is then validated in two benchmark problems,
and demonstrated on a partial differential equation-governed application of
cathodic electrophoretic deposition and an implicit likelihood case with
stochastic modeling of aphid population. The findings suggest that a
composition of 4--5 coupling layers is able to achieve lower EIG estimation
bias, under a fixed budget of forward model runs, compared to previous
approaches. The resulting NFs produce approximate posteriors that agree well
with the true posteriors, able to capture non-Gaussian and multi-modal features
effectively.

</details>


### [519] [DIRESA, a distance-preserving nonlinear dimension reduction technique based on regularized autoencoders](https://arxiv.org/abs/2404.18314)
*Geert De Paepe, Lesley De Cruz*

Main category: cs.LG

TL;DR: A dimension reduction technique using autoencoder neural networks (DIRESA) is proposed to compress weather and climate datasets, enabling efficient search in a latent space while preserving interpretability and outperforming PCA and other methods.


<details>
  <summary>Details</summary>
Motivation: Large weather and climate datasets are often nearline, requiring significant bandwidth and storage for downloads before analysis. Efficient, interpretable compression is needed.

Method: A distance-regularized Siamese twin autoencoder (DIRESA) is designed to compress datasets, preserving distances in latent space and capturing nonlinearities.

Result: DIRESA reduces storage needs, keeps latent components uncorrelated, and outperforms PCA, UMAP, and variational autoencoders in distance preservation and reconstruction fidelity.

Conclusion: DIRESA provides an efficient, interpretable solution for compressing and analyzing large weather and climate datasets, offering physical insights into system variability.

Abstract: In meteorology, finding similar weather patterns or analogs in historical
datasets can be useful for data assimilation, forecasting, and postprocessing.
In climate science, analogs in historical and climate projection data are used
for attribution and impact studies. However, most of the time, those large
weather and climate datasets are nearline. This means that they must be
downloaded, which takes a lot of bandwidth and disk space, before the
computationally expensive search can be executed. We propose a dimension
reduction technique based on autoencoder (AE) neural networks to compress the
datasets and perform the search in an interpretable, compressed latent space. A
distance-regularized Siamese twin autoencoder (DIRESA) architecture is designed
to preserve distance in latent space while capturing the nonlinearities in the
datasets. Using conceptual climate models of different complexities, we show
that the latent components thus obtained provide physical insight into the
dominant modes of variability in the system. Compressing datasets with DIRESA
reduces the online storage and keeps the latent components uncorrelated, while
the distance (ordering) preservation and reconstruction fidelity robustly
outperform Principal Component Analysis (PCA) and other dimension reduction
techniques such as UMAP or variational autoencoders.

</details>


### [520] [EM-GANSim: Real-time and Accurate EM Simulation Using Conditional GANs for 3D Indoor Scenes](https://arxiv.org/abs/2405.17366)
*Ruichen Wang, Dinesh Manocha*

Main category: cs.LG

TL;DR: EM-GANSim is a novel ML approach using a modified GAN for real-time EM propagation simulation in 3D indoor environments, achieving accuracy comparable to ray tracing with 5X speedup.


<details>
  <summary>Details</summary>
Motivation: To enable real-time EM propagation simulation for wireless communication in complex 3D indoor environments, overcoming the computational inefficiency of traditional ray tracing methods.

Method: Uses a modified conditional GAN incorporating encoded geometry and transmitter location, adhering to EM propagation theory to predict power distribution (heatmaps).

Result: Evaluated on 19 scenarios, EM-GANSim matches ray tracing accuracy (lower MSE) and reduces computation time by 5X, enabling real-time signal strength prediction.

Conclusion: EM-GANSim is the first real-time EM simulation algorithm for 3D indoor environments, with plans to release code and dataset.

Abstract: We present a novel machine-learning (ML) approach (EM-GANSim) for real-time
electromagnetic (EM) propagation that is used for wireless communication
simulation in 3D indoor environments. Our approach uses a modified conditional
Generative Adversarial Network (GAN) that incorporates encoded geometry and
transmitter location while adhering to the electromagnetic propagation theory.
The overall physically-inspired learning is able to predict the power
distribution in 3D scenes, which is represented using heatmaps. We evaluated
our method on 15 complex 3D indoor environments, with 4 additional scenarios
later included in the results, showcasing the generalizability of the model
across diverse conditions. Our overall accuracy is comparable to ray
tracing-based EM simulation, as evidenced by lower mean squared error values.
Furthermore, our GAN-based method drastically reduces the computation time,
achieving a 5X speedup on complex benchmarks. In practice, it can compute the
signal strength in a few milliseconds on any location in 3D indoor
environments. We also present a large dataset of 3D models and EM ray
tracing-simulated heatmaps. To the best of our knowledge, EM-GANSim is the
first real-time algorithm for EM simulation in complex 3D indoor environments.
We plan to release the code and the dataset.

</details>


### [521] [When Are Bias-Free ReLU Networks Effectively Linear Networks?](https://arxiv.org/abs/2406.12615)
*Yedi Zhang, Andrew Saxe, Peter E. Latham*

Main category: cs.LG

TL;DR: Bias-free ReLU networks, especially two-layer ones, have limited expressivity and share learning dynamics with linear networks under symmetric data. Deep bias-free ReLU networks, while more expressive, still resemble linear networks, allowing insights from linear networks to apply.


<details>
  <summary>Details</summary>
Motivation: To understand the expressivity and learning dynamics of bias-free ReLU networks and their similarities to linear networks.

Method: Analyzed two-layer and deep bias-free ReLU networks, comparing their expressivity and learning dynamics to linear networks under symmetric data conditions.

Result: Two-layer bias-free ReLU networks are limited to expressing linear odd functions and share learning dynamics with linear networks. Deep bias-free ReLU networks, though more expressive, still resemble linear networks.

Conclusion: Some properties of bias-free ReLU networks arise from their equivalence to linear networks, enabling insights from linear networks to be applied.

Abstract: We investigate the implications of removing bias in ReLU networks regarding
their expressivity and learning dynamics. We first show that two-layer
bias-free ReLU networks have limited expressivity: the only odd function
two-layer bias-free ReLU networks can express is a linear one. We then show
that, under symmetry conditions on the data, these networks have the same
learning dynamics as linear networks. This enables us to give analytical
time-course solutions to certain two-layer bias-free (leaky) ReLU networks
outside the lazy learning regime. While deep bias-free ReLU networks are more
expressive than their two-layer counterparts, they still share a number of
similarities with deep linear networks. These similarities enable us to
leverage insights from linear networks to understand certain ReLU networks.
Overall, our results show that some properties previously established for
bias-free ReLU networks arise due to equivalence to linear networks.

</details>


### [522] [Adaptive RKHS Fourier Features for Compositional Gaussian Process Models](https://arxiv.org/abs/2407.01856)
*Xinxing Shi, Thomas Baldwin-McDonald, Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: The paper introduces ODE-based RKHS Fourier features for DGPs, enhancing their ability to model non-stationary processes through adaptive amplitude and phase modulation, improving predictive performance.


<details>
  <summary>Details</summary>
Motivation: To improve DGPs' capability to capture complex non-stationary patterns by incorporating global Fourier features and linear transformations.

Method: Extends DGPs with ODE-based RKHS Fourier features, enabling adaptive modulation via convolution, and uses doubly stochastic variational inference.

Result: The model shows improved predictive performance in various regression tasks.

Conclusion: ODE-based RKHS Fourier features enhance DGPs, linking them to deep latent force models for better modeling of nonlinear dynamics.

Abstract: Deep Gaussian Processes (DGPs) leverage a compositional structure to model
non-stationary processes. DGPs typically rely on local inducing point
approximations across intermediate GP layers. Recent advances in DGP inference
have shown that incorporating global Fourier features from the Reproducing
Kernel Hilbert Space (RKHS) can enhance the DGPs' capability to capture complex
non-stationary patterns. This paper extends the use of these features to
compositional GPs involving linear transformations. In particular, we introduce
Ordinary Differential Equation(ODE)--based RKHS Fourier features that allow for
adaptive amplitude and phase modulation through convolution operations. This
convolutional formulation relates our work to recently proposed deep latent
force models, a multi-layer structure designed for modelling nonlinear
dynamical systems. By embedding these adjustable RKHS Fourier features within a
doubly stochastic variational inference framework, our model exhibits improved
predictive performance across various regression tasks.

</details>


### [523] [Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews](https://arxiv.org/abs/2407.10652)
*Lucas Joos, Daniel A. Keim, Maximilian T. Fischer*

Main category: cs.LG

TL;DR: LLMSurver, an open-source tool using LLMs, significantly speeds up literature reviews by reducing filtering time from weeks to minutes while maintaining high accuracy (>98.8% recall).


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of manual keyword-based filtering in systematic literature reviews due to high publication volumes.

Method: Developed LLMSurver, a visual tool leveraging LLMs for interactive literature filtration, query refinement, and evaluation. Tested on 8.3k articles.

Result: LLMs reduced filtering time drastically (weeks to minutes) with >98.8% recall, outperforming human error thresholds.

Conclusion: LLMs enhance literature review efficiency and accuracy, demonstrating the potential of human-AI collaboration in research.

Abstract: Systematic literature reviews (SLRs) are essential but labor-intensive due to
high publication volumes and inefficient keyword-based filtering. To streamline
this process, we evaluate Large Language Models (LLMs) for enhancing efficiency
and accuracy in corpus filtration while minimizing manual effort. Our
open-source tool LLMSurver presents a visual interface to utilize LLMs for
literature filtration, evaluate the results, and refine queries in an
interactive way. We assess the real-world performance of our approach in
filtering over 8.3k articles during a recent survey construction, comparing
results with human efforts. The findings show that recent LLM models can reduce
filtering time from weeks to minutes. A consensus scheme ensures recall rates
>98.8%, surpassing typical human error thresholds and improving selection
accuracy. This work advances literature review methodologies and highlights the
potential of responsible human-AI collaboration in academic research.

</details>


### [524] [Leveraging Large Language Models for Effective Label-free Node Classification in Text-Attributed Graphs](https://arxiv.org/abs/2412.11983)
*Taiyan Zhang, Renchi Yang, Yurui Lai, Mingyu Yan, Xiaochun Ye, Dongrui Fan*

Main category: cs.LG

TL;DR: Locle is an active self-training framework for label-free node classification using LLMs and GNNs, outperforming state-of-the-art methods with minimal cost.


<details>
  <summary>Details</summary>
Motivation: GNNs rely on labeled data, which is costly. LLMs offer zero-shot capabilities but face issues like high query costs or noisy labels. Locle addresses these challenges.

Method: Locle iteratively selects critical nodes, generates pseudo-labels using LLMs and GNNs, and refines labels with a rewired topology.

Result: Locle achieves an 8.08% accuracy improvement on DBLP dataset at minimal cost.

Conclusion: Locle effectively combines LLMs and GNNs for cost-efficient, label-free node classification.

Abstract: Graph neural networks (GNNs) have become the preferred models for node
classification in graph data due to their robust capabilities in integrating
graph structures and attributes. However, these models heavily depend on a
substantial amount of high-quality labeled data for training, which is often
costly to obtain. With the rise of large language models (LLMs), a promising
approach is to utilize their exceptional zero-shot capabilities and extensive
knowledge for node labeling. Despite encouraging results, this approach either
requires numerous queries to LLMs or suffers from reduced performance due to
noisy labels generated by LLMs. To address these challenges, we introduce
Locle, an active self-training framework that does Label-free node
Classification with LLMs cost-Effectively. Locle iteratively identifies small
sets of "critical" samples using GNNs and extracts informative pseudo-labels
for them with both LLMs and GNNs, serving as additional supervision signals to
enhance model training. Specifically, Locle comprises three key components: (i)
an effective active node selection strategy for initial annotations; (ii) a
careful sample selection scheme to identify "critical" nodes based on label
disharmonicity and entropy; and (iii) a label refinement module that combines
LLMs and GNNs with a rewired topology. Extensive experiments on five benchmark
text-attributed graph datasets demonstrate that Locle significantly outperforms
state-of-the-art methods under the same query budget to LLMs in terms of
label-free node classification. Notably, on the DBLP dataset with 14.3k nodes,
Locle achieves an 8.08% improvement in accuracy over the state-of-the-art at a
cost of less than one cent. Our code is available at
https://github.com/HKBU-LAGAS/Locle.

</details>


### [525] [EuroCropsML: A Time Series Benchmark Dataset For Few-Shot Crop Type Classification](https://arxiv.org/abs/2407.17458)
*Joana Reuss, Jan Macdonald, Simon Becker, Lorenz Richter, Marco Körner*

Main category: cs.LG

TL;DR: EuroCropsML is a remote sensing dataset for crop type classification in Europe, designed for benchmarking few-shot algorithms.


<details>
  <summary>Details</summary>
Motivation: To support advancements in algorithmic development and enable research comparability for transnational few-shot crop type classification.

Method: The dataset includes 706,683 labeled data points across 176 classes, featuring annual Sentinel-2 L1C time series data for 2021, crop type labels, and spatial coordinates.

Result: EuroCropsML is the first such dataset, publicly available on Zenodo, derived from the open-source EuroCrops collection.

Conclusion: EuroCropsML facilitates benchmarking and research in few-shot crop type classification, promoting algorithmic progress in remote sensing.

Abstract: We introduce EuroCropsML, an analysis-ready remote sensing machine learning
dataset for time series crop type classification of agricultural parcels in
Europe. It is the first dataset designed to benchmark transnational few-shot
crop type classification algorithms that supports advancements in algorithmic
development and research comparability. It comprises 706 683 multi-class
labeled data points across 176 classes, featuring annual time series of
per-parcel median pixel values from Sentinel-2 L1C data for 2021, along with
crop type labels and spatial coordinates. Based on the open-source EuroCrops
collection, EuroCropsML is publicly available on Zenodo.

</details>


### [526] [Sequential Conditional Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness](https://arxiv.org/abs/2408.03425)
*Agathe Fernandes Machado, Arthur Charpentier, Ewen Gallic*

Main category: cs.LG

TL;DR: The paper combines causal graph adaptations and optimal transport to derive counterfactuals, introducing sequential transport for fairness analysis.


<details>
  <summary>Details</summary>
Motivation: To bridge causal graph methods and optimal transport for counterfactual analysis, focusing on individual fairness.

Method: Extends Knothe's rearrangement and triangular transport to probabilistic graphical models, proposing sequential transport.

Result: Theoretical foundations are established, and numerical experiments on synthetic and real datasets validate the method.

Conclusion: Sequential transport effectively links causal and transport-based approaches for counterfactual fairness.

Abstract: In this paper, we link two existing approaches to derive counterfactuals:
adaptations based on a causal graph, and optimal transport. We extend "Knothe's
rearrangement" and "triangular transport" to probabilistic graphical models,
and use this counterfactual approach, referred to as sequential transport, to
discuss fairness at the individual level. After establishing the theoretical
foundations of the proposed method, we demonstrate its application through
numerical experiments on both synthetic and real datasets.

</details>


### [527] [On the choice of the non-trainable internal weights in random feature maps](https://arxiv.org/abs/2408.03626)
*Pinak Mandal, Georg A. Gottwald, Nicholas Cranch*

Main category: cs.LG

TL;DR: The paper proposes a hit-and-run algorithm to optimize internal weights in random feature maps for better forecasting in dynamical systems, showing superior performance and lower computational cost compared to traditional neural networks.


<details>
  <summary>Details</summary>
Motivation: The accuracy of random feature maps heavily depends on the choice of internal weights, which are typically fixed randomly. The paper aims to improve forecasting in dynamical systems by optimizing these weights.

Method: A computationally cheap hit-and-run algorithm is introduced to select optimal internal weights for random feature maps, focusing on forecasting dynamical systems.

Result: The number of good features is key to forecasting skill, acting as an effective feature dimension. Random feature maps outperform traditional neural networks in forecasting while being much cheaper computationally.

Conclusion: Random feature maps with optimized internal weights offer a cost-effective and accurate alternative to traditional neural networks for forecasting tasks.

Abstract: The computationally cheap machine learning architecture of random feature
maps can be viewed as a single-layer feedforward network in which the weights
of the hidden layer are random but fixed and only the outer weights are learned
via linear regression. The internal weights are typically chosen from a
prescribed distribution. The choice of the internal weights significantly
impacts the accuracy of random feature maps. We address here the task of how to
best select the internal weights. In particular, we consider the forecasting
problem whereby random feature maps are used to learn a one-step propagator map
for a dynamical system. We provide a computationally cheap hit-and-run
algorithm to select good internal weights which lead to good forecasting skill.
We show that the number of good features is the main factor controlling the
forecasting skill of random feature maps and acts as an effective feature
dimension. Lastly, we compare random feature maps with single-layer feedforward
neural networks in which the internal weights are now learned using gradient
descent. We find that random feature maps have superior forecasting
capabilities whilst having several orders of magnitude lower computational
cost.

</details>


### [528] [A prototype-based model for set classification](https://arxiv.org/abs/2408.13720)
*Mohammad Mohammadi, Sreejita Ghosh*

Main category: cs.LG

TL;DR: A prototype-based method for learning on the Grassmann manifold is proposed, offering efficient, explainable classification of sets of inputs like images and texts.


<details>
  <summary>Details</summary>
Motivation: To improve classification of sets of inputs (e.g., images, texts) by leveraging linear subspaces and enhancing transparency and efficiency.

Method: Uses subspace prototypes and relevance factors on the Grassmann manifold to automate dimensionality selection and provide explainable decisions.

Result: Outperforms transformer-based models in performance, explainability, and computational efficiency on benchmark datasets.

Conclusion: The proposed method is efficient, transparent, and effective for set classification tasks.

Abstract: Classification of sets of inputs (e.g., images and texts) is an active area
of research within both computer vision (CV) and natural language processing
(NLP). A common way to represent a set of vectors is to model them as linear
subspaces. In this contribution, we present a prototype-based approach for
learning on the manifold formed from such linear subspaces, the Grassmann
manifold. Our proposed method learns a set of subspace prototypes capturing the
representative characteristics of classes and a set of relevance factors
automating the selection of the dimensionality of the subspaces. This leads to
a transparent classifier model which presents the computed impact of each input
vector on its decision. Through experiments on benchmark image and text
datasets, we have demonstrated the efficiency of our proposed classifier,
compared to the transformer-based models in terms of not only performance and
explainability but also computational resource requirements.

</details>


### [529] [Fast and Accurate Identification of Hardware Trojan Locations in Gate-Level Netlist using Nearest Neighbour Approach integrated with Machine Learning Technique](https://arxiv.org/abs/2501.16347)
*Anindita Chattopadhyay, Siddharth Bisariya, Vijay Kumar Sutrakar*

Main category: cs.LG

TL;DR: A machine learning-based method for detecting Hardware Trojans (HTs) in gate-level netlists is proposed, using path retrace algorithms and validated across three cases with different models. The nearest neighbor (NN) method outperformed Graph Neural Networks (GNNs) in accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting HTs in multi-entity integrated circuit designs necessitates innovative solutions to identify malicious logic gates.

Method: Three cases were tested: Case I used decision trees with PCA, Case II employed GNNs for graph-to-graph classification, and Case III used GNNs for node-to-node classification. NN methods were also integrated.

Result: NN methods outperformed GNNs, with 2nd NN achieving 97.7% accuracy in both graph-to-graph and node-to-node classifications, compared to GNNs' lower accuracies. However, higher NN methods increase code coverage.

Conclusion: The NN approach is more effective for HT detection than GNNs, but scalability concerns arise with higher NN methods due to increased code coverage.

Abstract: In the evolving landscape of integrated circuit design, detecting Hardware
Trojans (HTs) within a multi entity based design cycle presents significant
challenges. This research proposes an innovative machine learning-based
methodology for identifying malicious logic gates in gate-level netlists. By
focusing on path retrace algorithms. The methodology is validated across three
distinct cases, each employing different machine learning models to classify
HTs. Case I utilizes a decision tree algorithm for node-to-node comparisons,
significantly improving detection accuracy through the integration of Principal
Component Analysis (PCA). Case II introduces a graph-to-graph classification
using a Graph Neural Network (GNN) model, enabling the differentiation between
normal and Trojan-infected circuit designs. Case III applies GNN-based node
classification to identify individual compromised nodes and its location.
Additionally, nearest neighbor (NN) method has been combined with GNN
graph-to-graph in Case II and GNN node-to-node in Case III. Despite the
potential of GNN model graph-to-graph classification, NN approach demonstrated
superior performance, with the first nearest neighbor (1st NN) achieving 73.2%
accuracy and the second nearest neighbor (2nd NN) method reaching 97.7%. In
comparison, the GNN model achieved an accuracy of 62.8%. Similarly, GNN model
node-to-node classification, NN approach demonstrated superior performance,
with the 1st NN achieving 93% accuracy and the 2nd NN method reaching 97.7%. In
comparison, the GNN model achieved an accuracy of 79.8%. However, higher and
higher NN will lead to large code coverage for the identification of HTs.

</details>


### [530] [SHIRE: Enhancing Sample Efficiency using Human Intuition in REinforcement Learning](https://arxiv.org/abs/2409.09990)
*Amogh Joshi, Adarsh Kumar Kosta, Kaushik Roy*

Main category: cs.LG

TL;DR: SHIRE is a framework combining human intuition (via PGMs) with Deep RL to improve sample efficiency and explainability in robotic tasks.


<details>
  <summary>Details</summary>
Motivation: Deep RL lacks sample efficiency and explainability, which are critical for robotics. Human intuition can address these gaps.

Method: SHIRE encodes human intuition using Probabilistic Graphical Models (PGMs) and integrates it into Deep RL training.

Result: Achieves 25-78% sample efficiency gains with negligible overhead and enhances policy explainability.

Conclusion: SHIRE effectively bridges the gap between human intuition and Deep RL, improving performance and explainability in robotics.

Abstract: The ability of neural networks to perform robotic perception and control
tasks such as depth and optical flow estimation, simultaneous localization and
mapping (SLAM), and automatic control has led to their widespread adoption in
recent years. Deep Reinforcement Learning has been used extensively in these
settings, as it does not have the unsustainable training costs associated with
supervised learning. However, DeepRL suffers from poor sample efficiency, i.e.,
it requires a large number of environmental interactions to converge to an
acceptable solution. Modern RL algorithms such as Deep Q Learning and Soft
Actor-Critic attempt to remedy this shortcoming but can not provide the
explainability required in applications such as autonomous robotics. Humans
intuitively understand the long-time-horizon sequential tasks common in
robotics. Properly using such intuition can make RL policies more explainable
while enhancing their sample efficiency. In this work, we propose SHIRE, a
novel framework for encoding human intuition using Probabilistic Graphical
Models (PGMs) and using it in the Deep RL training pipeline to enhance sample
efficiency. Our framework achieves 25-78% sample efficiency gains across the
environments we evaluate at negligible overhead cost. Additionally, by teaching
RL agents the encoded elementary behavior, SHIRE enhances policy
explainability. A real-world demonstration further highlights the efficacy of
policies trained using our framework.

</details>


### [531] [Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning](https://arxiv.org/abs/2410.06140)
*Barak Gahtan, Robert J. Shahla, Reuven Cohen, Alex M. Bronstein*

Main category: cs.LG

TL;DR: The paper proposes a machine learning-based method to estimate HTTP/3 responses in QUIC connections, achieving high accuracy for network monitoring and security applications.


<details>
  <summary>Details</summary>
Motivation: QUIC's features challenge network middle-boxes, necessitating methods to monitor and analyze web traffic for applications like load balancing and attack detection.

Method: Transform QUIC connection traces into image sequences and use ML models with a tailored loss function to predict response counts.

Result: Achieves up to 97% accuracy in known/unknown server settings and 92% accuracy on unseen QUIC traces.

Conclusion: The method effectively estimates HTTP/3 responses, aiding in network monitoring and security.

Abstract: QUIC, a new and increasingly used transport protocol, enhances TCP by
offering improved security, performance, and stream multiplexing. These
features, however, also impose challenges for network middle-boxes that need to
monitor and analyze web traffic. This paper proposes a novel method to estimate
the number of HTTP/3 responses in a given QUIC connection by an observer. This
estimation reveals server behavior, client-server interactions, and data
transmission efficiency, which is crucial for various applications such as
designing a load balancing solution and detecting HTTP/3 flood attacks. The
proposed scheme transforms QUIC connection traces into image sequences and uses
machine learning (ML) models, guided by a tailored loss function, to predict
response counts. Evaluations on more than seven million images-derived from
100,000 traces collected across 44,000 websites over four months-achieve up to
97% accuracy in both known and unknown server settings and 92% accuracy on
previously unseen complete QUIC traces.

</details>


### [532] [Measurability in the Fundamental Theorem of Statistical Learning](https://arxiv.org/abs/2410.10243)
*Lothar Sebastian Krapp, Laura Wirth*

Main category: cs.LG

TL;DR: The paper rigorously analyzes the Fundamental Theorem of Statistical Learning in the agnostic PAC model, clarifying minimal measurability assumptions and proving its validity. It also extends applications to Model Theory, particularly for NIP and o-minimal structures.


<details>
  <summary>Details</summary>
Motivation: To address the tacit measurability assumptions in existing proofs of the Fundamental Theorem of Statistical Learning and provide a rigorous, self-contained proof with minimal requirements.

Method: Scrutinizing existing proofs from a measure-theoretic perspective to extract explicit assumptions and presenting a detailed proof for the agnostic PAC learning setting.

Result: A sound statement and proof of the Fundamental Theorem of Statistical Learning with minimal measurability requirements, applicable to Model Theory and hypothesis spaces like neural networks.

Conclusion: The careful analysis of measurability is foundational for theoretical developments, especially in settings like Model Theory and neural networks, where measure-theoretic subtleties matter.

Abstract: The Fundamental Theorem of Statistical Learning states that a hypothesis
space is PAC learnable if and only if its VC dimension is finite. For the
agnostic model of PAC learning, the literature so far presents proofs of this
theorem that often tacitly impose several measurability assumptions on the
involved sets and functions. We scrutinize these proofs from a
measure-theoretic perspective in order to explicitly extract the assumptions
needed for a rigorous argument. This leads to a sound statement as well as a
detailed and self-contained proof of the Fundamental Theorem of Statistical
Learning in the agnostic setting, showcasing the minimal measurability
requirements needed. As the Fundamental Theorem of Statistical Learning
underpins a wide range of further theoretical developments, our results are of
foundational importance: A careful analysis of measurability aspects is
essential, especially when the theorem is used in settings where
measure-theoretic subtleties play a role. We particularly discuss applications
in Model Theory, considering NIP and o-minimal structures. Our main theorem
presents sufficient conditions for the PAC learnability of hypothesis spaces
defined over o-minimal expansions of the reals. This class of hypothesis spaces
covers all artificial neural networks for binary classification that use
commonly employed activation functions like ReLU and the sigmoid function.

</details>


### [533] [FedBaF: Federated Learning Aggregation Biased by a Foundation Model](https://arxiv.org/abs/2410.18352)
*Jong-Ik Park, Srinivasa Pranav, José M. F. Moura, Carlee Joe-Wong*

Main category: cs.LG

TL;DR: FedBaF is a new FL method that integrates pre-trained foundation model weights during aggregation, preserving model confidentiality while improving accuracy in IID and non-IID settings.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods compromise model security by disclosing foundation model weights. FedBaF aims to address this while enhancing performance.

Method: FedBaF dynamically integrates foundation model weights during FL aggregation, avoiding direct weight disclosure.

Result: FedBaF improves test accuracy by up to 11.4% (IID) and 15.8% (non-IID), and reduces perplexity by 39.2% in language models.

Conclusion: FedBaF successfully balances model security and performance, outperforming traditional FL methods.

Abstract: Foundation models are now a major focus of leading technology organizations
due to their ability to generalize across diverse tasks. Existing approaches
for adapting foundation models to new applications often rely on Federated
Learning (FL) and disclose the foundation model weights to clients when using
it to initialize the global model. While these methods ensure client data
privacy, they compromise model and information security. In this paper, we
introduce Federated Learning Aggregation Biased by a Foundation Model (FedBaF),
a novel method for dynamically integrating pre-trained foundation model weights
during the FL aggregation phase. Unlike conventional methods, FedBaF preserves
the confidentiality of the foundation model while still leveraging its power to
train more accurate models, especially in non-IID and adversarial scenarios.
Our comprehensive experiments use Pre-ResNet and foundation models like Vision
Transformer to demonstrate that FedBaF not only matches, but often surpasses
the test accuracy of traditional weight initialization methods by up to 11.4%
in IID and up to 15.8% in non-IID settings. Additionally, FedBaF applied to a
Transformer-based language model significantly reduced perplexity by up to
39.2%.

</details>


### [534] [MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular Metaverse Services](https://arxiv.org/abs/2410.19665)
*Hongjia Wu, Hui Zeng, Zehui Xiong, Jiawen Kang, Zhiping Cai, Tse-Tin Chan, Dusit Niyato, Zhu Han*

Main category: cs.LG

TL;DR: The paper proposes an immersion-aware model trading framework for IoT data in vehicular metaverse services, using federated learning for privacy and a novel IoM metric to assess model value. An incentive mechanism and dynamic reward algorithm improve performance and reduce training time.


<details>
  <summary>Details</summary>
Motivation: Challenges like latency, privacy risks, and computational burdens hinder high-quality IoT data collection for vehicular metaverse services.

Method: Develops the IoM metric, designs an incentive mechanism, models interactions as an EPEC problem, and formulates reward decisions as a multi-agent Markov decision process solved with deep reinforcement learning.

Result: Outperforms benchmarks with 38.3% and 37.2% improvements in IoM, and 43.5% and 49.8% reductions in training time for MNIST and GTSRB datasets.

Conclusion: The framework effectively balances privacy, efficiency, and performance in IoT data collection for metaverse services.

Abstract: Timely updating of Internet of Things (IoT) data is crucial for immersive
vehicular metaverse services. However, challenges such as latency caused by
massive data transmissions, privacy risks associated with user data, and
computational burdens on metaverse service providers (MSPs) hinder continuous
collection of high-quality data. To address these issues, we propose an
immersion-aware model trading framework that facilitates data provision for
services while ensuring privacy through federated learning (FL). Specifically,
we first develop a novel multi-dimensional metric, the immersion of model
(IoM), which assesses model value comprehensively by considering freshness and
accuracy of learning models, as well as the amount and potential value of raw
data used for training. Then, we design an incentive mechanism to incentivize
metaverse users (MUs) to contribute high-value models under resource
constraints. The trading interactions between MSPs and MUs are modeled as an
equilibrium problem with equilibrium constraints (EPEC) to analyze and balance
their costs and gains, where MSPs as leaders determine rewards, while MUs as
followers optimize resource allocation. Furthermore, considering dynamic
network conditions and privacy concerns, we formulate the reward decisions of
MSPs as a multi-agent Markov decision process. To solve this, we develop a
fully distributed dynamic reward algorithm based on deep reinforcement
learning, without accessing any private information about MUs and other MSPs.
Experimental results demonstrate that the proposed framework outperforms
state-of-the-art benchmarks, achieving improvements in IoM of 38.3% and 37.2%,
and reductions in training time to reach the target accuracy of 43.5% and
49.8%, on average, for the MNIST and GTSRB datasets, respectively.

</details>


### [535] [Centaur: a foundation model of human cognition](https://arxiv.org/abs/2410.20268)
*Marcel Binz, Elif Akata, Matthias Bethge, Franziska Brändle, Fred Callaway, Julian Coda-Forno, Peter Dayan, Can Demircan, Maria K. Eckstein, Noémi Éltető, Thomas L. Griffiths, Susanne Haridi, Akshay K. Jagadish, Li Ji-An, Alexander Kipnis, Sreejan Kumar, Tobias Ludwig, Marvin Mathony, Marcelo Mattar, Alireza Modirshanechi, Surabhi S. Nath, Joshua C. Peterson, Milena Rmus, Evan M. Russek, Tankred Saanum, Johannes A. Schubert, Luca M. Schulze Buschoff, Nishad Singhi, Xin Sui, Mirko Thalmann, Fabian Theis, Vuong Truong, Vishaal Udandarao, Konstantinos Voudouris, Robert Wilson, Kristin Witte, Shuchen Wu, Dirk Wulff, Huadong Xiong, Eric Schulz*

Main category: cs.LG

TL;DR: Centaur, a computational model trained on Psych-101 data, predicts human behavior across diverse experiments and aligns with neural activity, advancing unified cognitive theory.


<details>
  <summary>Details</summary>
Motivation: To create a unified computational model predicting human behavior across varied settings, addressing gaps in existing cognitive theories.

Method: Finetuned a state-of-the-art language model on Psych-101, a large-scale dataset with trial-by-trial data from 60,000 participants and 10M choices in 160 experiments.

Result: Centaur outperforms existing models in predicting behavior, generalizes to new tasks, and aligns better with human neural activity post-finetuning.

Conclusion: Centaur demonstrates the feasibility of unified computational models for human cognition, offering potential for cognitive theory development.

Abstract: Establishing a unified theory of cognition has been a major goal of
psychology. While there have been previous attempts to instantiate such
theories by building computational models, we currently do not have one model
that captures the human mind in its entirety. A first step in this direction is
to create a model that can predict human behavior in a wide range of settings.
Here we introduce Centaur, a computational model that can predict and simulate
human behavior in any experiment expressible in natural language. We derived
Centaur by finetuning a state-of-the-art language model on a novel, large-scale
data set called Psych-101. Psych-101 reaches an unprecedented scale, covering
trial-by-trial data from over 60,000 participants performing over 10,000,000
choices in 160 experiments. Centaur not only captures the behavior of held-out
participants better than existing cognitive models, but also generalizes to new
cover stories, structural task modifications, and entirely new domains.
Furthermore, we find that the model's internal representations become more
aligned with human neural activity after finetuning. Taken together, our
results demonstrate that it is possible to discover computational models that
capture human behavior across a wide range of domains. We believe that such
models provide tremendous potential for guiding the development of cognitive
theories and present a case study to demonstrate this.

</details>


### [536] [Toward Understanding In-context vs. In-weight Learning](https://arxiv.org/abs/2410.23042)
*Bryan Chan, Xinyi Chen, András György, Dale Schuurmans*

Main category: cs.LG

TL;DR: The paper explains why in-context learning emerges and disappears in transformers, using theoretical and experimental analysis of simplified models and real-world language models.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which in-context learning emerges and diminishes in transformers, providing theoretical insights into these phenomena.

Method: Analyzes a simplified gating mechanism model, combines generalization error and regret analysis, and validates findings experimentally with transformers and large language models.

Result: Identifies distributional conditions for in-context learning emergence and disappearance, with experimental alignment between simplified models and real transformers.

Conclusion: Theoretical and experimental results show how in-context learning behavior is influenced by training data properties, extending insights to large language models.

Abstract: It has recently been demonstrated empirically that in-context learning
emerges in transformers when certain distributional properties are present in
the training data, but this ability can also diminish upon further training. We
provide a new theoretical understanding of these phenomena by identifying
simplified distributional properties that give rise to the emergence and
eventual disappearance of in-context learning. We do so by first analyzing a
simplified model that uses a gating mechanism to choose between an in-weight
and an in-context predictor. Through a combination of a generalization error
and regret analysis we identify conditions where in-context and in-weight
learning emerge. These theoretical findings are then corroborated
experimentally by comparing the behaviour of a full transformer on the
simplified distributions to that of the stylized model, demonstrating aligned
results. We then extend the study to a full large language model, showing how
fine-tuning on various collections of natural language prompts can elicit
similar in-context and in-weight learning behaviour.

</details>


### [537] [Lorentz-Equivariant Quantum Graph Neural Network for High-Energy Physics](https://arxiv.org/abs/2411.01641)
*Md Abrar Jahin, Md. Akmol Masud, Md Wahiduzzaman Suva, M. F. Mridha, Nilanjan Dey*

Main category: cs.LG

TL;DR: A quantum-enhanced graph neural network (Lorentz-EQGNN) outperforms classical and quantum GNNs in particle physics tasks with fewer parameters and noise resilience.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges in particle physics data processing by leveraging quantum machine learning, overcoming noise and symmetry constraints in current quantum GNNs.

Method: Replace Lorentz Group Equivariant Block modules in LorentzNet with dressed quantum circuits, preserving symmetries and reducing parameters.

Result: Achieved 74.00% test accuracy and 87.38% AUC on Quark-Gluon dataset, 67.00% accuracy on Electron-Photon dataset, and competitive results on MNIST/FashionMNIST with minimal training data.

Conclusion: Lorentz-EQGNN is efficient, noise-resilient, and suitable for jet tagging and event classification in high-energy physics.

Abstract: The rapid data surge from the high-luminosity Large Hadron Collider
introduces critical computational challenges requiring novel approaches for
efficient data processing in particle physics. Quantum machine learning, with
its capability to leverage the extensive Hilbert space of quantum hardware,
offers a promising solution. However, current quantum graph neural networks
(GNNs) lack robustness to noise and are often constrained by fixed symmetry
groups, limiting adaptability in complex particle interaction modeling. This
paper demonstrates that replacing the Lorentz Group Equivariant Block modules
in LorentzNet with a dressed quantum circuit significantly enhances performance
despite using nearly 5.5 times fewer parameters. Additionally, quantum circuits
effectively replace MLPs by inherently preserving symmetries, with Lorentz
symmetry integration ensuring robust handling of relativistic invariance. Our
Lorentz-Equivariant Quantum Graph Neural Network (Lorentz-EQGNN) achieved
$74.00\%$ test accuracy and an AUC of $87.38\%$ on the Quark-Gluon jet tagging
dataset, outperforming the classical and quantum GNNs with a reduced
architecture using only 4 qubits. On the Electron-Photon dataset, Lorentz-EQGNN
reached $67.00\%$ test accuracy and an AUC of $68.20\%$, demonstrating
competitive results with just 800 training samples. Evaluation of our model on
generic MNIST and FashionMNIST datasets confirmed Lorentz-EQGNN's efficiency,
achieving $88.10\%$ and $74.80\%$ test accuracy, respectively. Ablation studies
validated the impact of quantum components on performance, with notable
improvements in background rejection rates over classical counterparts. These
results highlight Lorentz-EQGNN's potential for immediate applications in
noise-resilient jet tagging, event classification, and broader data-scarce HEP
tasks.

</details>


### [538] [Fair Resource Allocation in Weakly Coupled Markov Decision Processes](https://arxiv.org/abs/2411.09804)
*Xiaohui Tu, Yossiri Adulyasak, Nima Akbarzadeh, Erick Delage*

Main category: cs.LG

TL;DR: The paper proposes a fair resource allocation method in weakly coupled MDPs using the generalized Gini function, reducing the problem to optimizing a utilitarian objective for homogeneous cases and validating the approach with experiments.


<details>
  <summary>Details</summary>
Motivation: To address fairness in resource allocation within sequential decision-making environments, moving beyond traditional utilitarian objectives.

Method: Introduces a linear programming solution for general cases and simplifies the problem for homogeneous sub-MDPs using permutation invariant policies and Whittle index policies, supplemented by deep reinforcement learning.

Result: The proposed method effectively achieves fairness, validated through comprehensive experiments.

Conclusion: The approach successfully balances fairness and efficiency in resource allocation for weakly coupled MDPs.

Abstract: We consider fair resource allocation in sequential decision-making
environments modeled as weakly coupled Markov decision processes, where
resource constraints couple the action spaces of $N$ sub-Markov decision
processes (sub-MDPs) that would otherwise operate independently. We adopt a
fairness definition using the generalized Gini function instead of the
traditional utilitarian (total-sum) objective. After introducing a general but
computationally prohibitive solution scheme based on linear programming, we
focus on the homogeneous case where all sub-MDPs are identical. For this case,
we show for the first time that the problem reduces to optimizing the
utilitarian objective over the class of "permutation invariant" policies. This
result is particularly useful as we can exploit Whittle index policies in the
restless bandits setting while, for the more general setting, we introduce a
count-proportion-based deep reinforcement learning approach. Finally, we
validate our theoretical findings with comprehensive experiments, confirming
the effectiveness of our proposed method in achieving fairness.

</details>


### [539] [BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration](https://arxiv.org/abs/2411.11745)
*Yuzong Chen, Ahmed F. AbouElhamayed, Xilai Dai, Yang Wang, Marta Andronic, George A. Constantinides, Mohamed S. Abdelfattah*

Main category: cs.LG

TL;DR: BitMoD is an algorithm-hardware co-design solution for efficient LLM acceleration at low weight precision, achieving high accuracy with 4-bit and 3-bit quantization and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The substantial memory footprint of LLMs hinders deployment, prompting the need for efficient acceleration solutions like BitMoD.

Method: BitMoD combines fine-grained data type adaptation for quantization with a bit-serial processing element and unified hardware representation.

Result: BitMoD quantizes LLM weights to 4-bit (discriminative tasks) and 3-bit (generative tasks) with minimal accuracy loss, outperforming state-of-the-art methods.

Conclusion: BitMoD's efficient design and superior performance make it a promising solution for LLM deployment.

Abstract: Large language models (LLMs) have demonstrated remarkable performance across
various machine learning tasks. Yet the substantial memory footprint of LLMs
significantly hinders their deployment. In this paper, we improve the
accessibility of LLMs through BitMoD, an algorithm-hardware co-design solution
that enables efficient LLM acceleration at low weight precision. On the
algorithm side, BitMoD introduces fine-grained data type adaptation that uses a
different numerical data type to quantize a group of (e.g., 128) weights.
Through the careful design of these new data types, BitMoD is able to quantize
LLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaining
high accuracy. On the hardware side, BitMoD employs a bit-serial processing
element to easily support multiple numerical precisions and data types; our
hardware design includes two key innovations: First, it employs a unified
representation to process different weight data types, thus reducing the
hardware cost. Second, it adopts a bit-serial dequantization unit to rescale
the per-group partial sum with minimal hardware overhead. Our evaluation on six
representative LLMs demonstrates that BitMoD significantly outperforms
state-of-the-art LLM quantization and acceleration methods. For discriminative
tasks, BitMoD can quantize LLM weights to 4-bit with $<\!0.5\%$ accuracy loss
on average. For generative tasks, BitMoD is able to quantize LLM weights to
3-bit while achieving better perplexity than prior LLM quantization scheme.
Combining the superior model performance with an efficient accelerator design,
BitMoD achieves an average of $1.69\times$ and $1.48\times$ speedups compared
to prior LLM accelerators ANT and OliVe, respectively.

</details>


### [540] [A Hybrid Deep-Learning Model for El Niño Southern Oscillation in the Low-Data Regime](https://arxiv.org/abs/2412.03743)
*Jakob Schloer, Matthew Newman, Jannik Thuemmel, Antonietta Capotondi, Bedartha Goswami*

Main category: cs.LG

TL;DR: A hybrid model combining Linear Inverse Models (LIMs) with deep-learning corrections outperforms both LIMs and full deep-learning models for ENSO forecasting, especially for longer lead times.


<details>
  <summary>Details</summary>
Motivation: Deep-learning models for ENSO forecasting rely on biased climate simulations, while LIMs trained on observational data lack nonlinear process capture. A hybrid approach addresses these limitations.

Method: The hybrid model integrates LIMs with a deep-learning non-Markovian correction, leveraging the LIM's data efficiency and deep-learning's nonlinear capabilities.

Result: The hybrid model surpasses both LIMs and full deep-learning models in skill, particularly for ENSO events beyond 9 months, capturing asymmetric evolution better.

Conclusion: The hybrid approach offers a balanced solution for ENSO forecasting, combining data efficiency and nonlinear accuracy, improving predictions for longer lead times.

Abstract: While deep-learning models have demonstrated skillful El Ni\~no Southern
Oscillation (ENSO) forecasts up to one year in advance, they are predominantly
trained on climate model simulations that provide thousands of years of
training data at the expense of introducing climate model biases. Simpler
Linear Inverse Models (LIMs) trained on the much shorter observational record
also make skillful ENSO predictions but do not capture predictable nonlinear
processes. This motivates a hybrid approach, combining the LIMs modest data
needs with a deep-learning non-Markovian correction of the LIM. For O(100 yr)
datasets, our resulting Hybrid model is more skillful than the LIM while also
exceeding the skill of a full deep-learning model. Additionally, while the most
predictable ENSO events are still identified in advance by the LIM, they are
better predicted by the Hybrid model, especially in the western tropical
Pacific for leads beyond about 9 months, by capturing the subsequent asymmetric
(warm versus cold phases) evolution of ENSO.

</details>


### [541] [Sanity Checking Causal Representation Learning on a Simple Real-World System](https://arxiv.org/abs/2502.20099)
*Juan L. Gamella, Simon Bing, Jakob Runge*

Main category: cs.LG

TL;DR: The paper evaluates causal representation learning (CRL) methods on a real-world optical experiment, finding they fail to recover causal factors. Synthetic data reveals reproducibility issues, and assumptions on mixing functions are often unmet.


<details>
  <summary>Details</summary>
Motivation: To assess the practical applicability of CRL methods by testing them on a controlled real-world system with known ground truth.

Method: Evaluated representative CRL methods on a real-world optical experiment and synthetic data ablation.

Result: All methods failed to recover causal factors; synthetic data exposed reproducibility problems and invalid assumptions.

Conclusion: Highlights the gap between theoretical promise and practical challenges in CRL, offering a benchmark for future method validation.

Abstract: We evaluate methods for causal representation learning (CRL) on a simple,
real-world system where these methods are expected to work. The system consists
of a controlled optical experiment specifically built for this purpose, which
satisfies the core assumptions of CRL and where the underlying causal factors
(the inputs to the experiment) are known, providing a ground truth. We select
methods representative of different approaches to CRL and find that they all
fail to recover the underlying causal factors. To understand the failure modes
of the evaluated algorithms, we perform an ablation on the data by substituting
the real data-generating process with a simpler synthetic equivalent. The
results reveal a reproducibility problem, as most methods already fail on this
synthetic ablation despite its simple data-generating process. Additionally, we
observe that common assumptions on the mixing function are crucial for the
performance of some of the methods but do not hold in the real data. Our
efforts highlight the contrast between the theoretical promise of the state of
the art and the challenges in its application. We hope the benchmark serves as
a simple, real-world sanity check to further develop and validate methodology,
bridging the gap towards CRL methods that work in practice. We make all code
and datasets publicly available at github.com/simonbing/CRLSanityCheck

</details>


### [542] [Achieving Group Fairness through Independence in Predictive Process Monitoring](https://arxiv.org/abs/2412.04914)
*Jari Peeperkorn, Simon De Vos*

Main category: cs.LG

TL;DR: The paper addresses group fairness in predictive process monitoring, proposing metrics and a composite loss function to balance predictive performance and fairness.


<details>
  <summary>Details</summary>
Motivation: Machine learning models in predictive process monitoring may inherit biases from historical data, risking unfair decisions. This work aims to mitigate such biases by ensuring predictions are independent of sensitive group attributes.

Method: The study investigates independence using fairness metrics like ΔDP and distribution-based alternatives. It introduces a composite loss function combining binary cross-entropy and Wasserstein loss for balanced performance and fairness.

Result: The proposed fairness metrics and composite loss function are validated through controlled experiments, demonstrating their effectiveness.

Conclusion: The work successfully addresses fairness in predictive process monitoring, offering tools to balance accuracy and fairness with customizable trade-offs.

Abstract: Predictive process monitoring focuses on forecasting future states of ongoing
process executions, such as predicting the outcome of a particular case. In
recent years, the application of machine learning models in this domain has
garnered significant scientific attention. When using historical execution
data, which may contain biases or exhibit unfair behavior, these biases may be
encoded into the trained models. Consequently, when such models are deployed to
make decisions or guide interventions for new cases, they risk perpetuating
this unwanted behavior. This work addresses group fairness in predictive
process monitoring by investigating independence, i.e. ensuring predictions are
unaffected by sensitive group membership. We explore independence through
metrics for demographic parity such as $\Delta$DP, as well as recently
introduced, threshold-independent distribution-based alternatives.
Additionally, we propose a composite loss function existing of binary
cross-entropy and a distribution-based loss (Wasserstein) to train models that
balance predictive performance and fairness, and allow for customizable
trade-offs. The effectiveness of both the fairness metrics and the composite
loss functions is validated through a controlled experimental setup.

</details>


### [543] [Multi-Source Urban Traffic Flow Forecasting with Drone and Loop Detector Data](https://arxiv.org/abs/2501.03492)
*Weijiang Xiong, Robert Fonod, Alexandre Alahi, Nikolas Geroliminis*

Main category: cs.LG

TL;DR: The paper proposes HiMSNet, a graph-based model for multi-source traffic speed prediction using drone and loop detector data, improving accuracy over single-modality methods.


<details>
  <summary>Details</summary>
Motivation: Current traffic forecasting relies on single-modality loop detectors, but drone-captured data offers a more accurate and flexible solution for urban traffic monitoring.

Method: HiMSNet, a graph-based model, integrates drone and loop detector data to learn spatio-temporal correlations.

Result: Combining drone and loop detector data improves prediction accuracy, especially in high-demand scenarios with noise or low sensor coverage.

Conclusion: Integrating drones into traffic forecasting enhances accuracy and monitoring capabilities, as demonstrated in a real urban road network simulation.

Abstract: Traffic forecasting is a fundamental task in transportation research, however
the scope of current research has mainly focused on a single data modality of
loop detectors. Recently, the advances in Artificial Intelligence and drone
technologies have made possible novel solutions for efficient, accurate and
flexible aerial observations of urban traffic. As a promising traffic
monitoring approach, drone-captured data can create an accurate multi-sensor
mobility observatory for large-scale urban networks, when combined with
existing infrastructure. Therefore, this paper investigates the problem of
multi-source traffic speed prediction, simultaneously using drone and loop
detector data. A simple yet effective graph-based model HiMSNet is proposed to
integrate multiple data modalities and learn spatio-temporal correlations.
Detailed analysis shows that predicting accurate segment-level speed is more
challenging than the regional speed, especially under high-demand scenarios
with heavier congestions and varying traffic dynamics. Utilizing both drone and
loop detector data, the prediction accuracy can be improved compared to
single-modality cases, when the sensors have lower coverages and are subject to
noise. Our simulation study based on vehicle trajectories in a real urban road
network has highlighted the added value of integrating drones in traffic
forecasting and monitoring.

</details>


### [544] [Toward Foundation Models for Online Complex Event Detection in CPS-IoT: A Case Study](https://arxiv.org/abs/2503.12282)
*Liying Han, Gaofeng Dong, Xiaomin Ouyang, Lance Kaplan, Federico Cerutti, Mani Srivastava*

Main category: cs.LG

TL;DR: The paper explores CE detection in CPS-IoT using three methods, finding state-space models like Mamba outperform others in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing models lack long-term reasoning for CE detection, which is crucial for CPS-IoT applications like smart monitoring.

Method: Three approaches are evaluated: LLMs, neural architectures learning CE rules, and neurosymbolic integration of neural models with symbolic engines.

Result: Mamba, a state-space model, outperforms other methods in accuracy and generalization to longer, unseen sensor traces.

Conclusion: State-space models like Mamba are promising for CPS-IoT foundation models requiring long-span reasoning.

Abstract: Complex events (CEs) play a crucial role in CPS-IoT applications, enabling
high-level decision-making in domains such as smart monitoring and autonomous
systems. However, most existing models focus on short-span perception tasks,
lacking the long-term reasoning required for CE detection. CEs consist of
sequences of short-time atomic events (AEs) governed by spatiotemporal
dependencies. Detecting them is difficult due to long, noisy sensor data and
the challenge of filtering out irrelevant AEs while capturing meaningful
patterns. This work explores CE detection as a case study for CPS-IoT
foundation models capable of long-term reasoning. We evaluate three approaches:
(1) leveraging large language models (LLMs), (2) employing various neural
architectures that learn CE rules from data, and (3) adopting a neurosymbolic
approach that integrates neural models with symbolic engines embedding human
knowledge. Our results show that the state-space model, Mamba, which belongs to
the second category, outperforms all methods in accuracy and generalization to
longer, unseen sensor traces. These findings suggest that state-space models
could be a strong backbone for CPS-IoT foundation models for long-span
reasoning tasks.

</details>


### [545] [Online Clustering with Bandit Information](https://arxiv.org/abs/2501.11421)
*G Dhinesh Chandran, Srinivas Reddy Kota, Srikrishna Bhashyam*

Main category: cs.LG

TL;DR: The paper introduces two algorithms, ATBOC and LUCBBOC, for online clustering in the multi-armed bandit framework, proving their order-optimality and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of clustering arms with unknown means in the multi-armed bandit problem while minimizing sample complexity and ensuring low error probability.

Method: Proposes ATBOC (order-optimal) and LUCBBOC (computationally efficient) algorithms, leveraging SLINK clustering and confidence bounds.

Result: ATBOC is within a factor of 2 of the lower bound; LUCBBOC performs comparably. Both validated on synthetic and real-world data.

Conclusion: First work enabling clustering with varying means and K>2, demonstrating effectiveness of the proposed algorithms.

Abstract: We study the problem of online clustering within the multi-armed bandit
framework under the fixed confidence setting. In this multi-armed bandit
problem, we have $M$ arms, each providing i.i.d. samples that follow a
multivariate Gaussian distribution with an {\em unknown} mean and a known unit
covariance. The arms are grouped into $K$ clusters based on the distance
between their means using the Single Linkage (SLINK) clustering algorithm on
the means of the arms. Since the true means are unknown, the objective is to
obtain the above clustering of the arms with the minimum number of samples
drawn from the arms, subject to an upper bound on the error probability. We
introduce a novel algorithm, Average Tracking Bandit Online Clustering (ATBOC),
and prove that this algorithm is order optimal, meaning that the upper bound on
its expected sample complexity for given error probability $\delta$ is within a
factor of 2 of an instance-dependent lower bound as $\delta \rightarrow 0$.
Furthermore, we propose a computationally more efficient algorithm, Lower and
Upper Confidence Bound-based Bandit Online Clustering (LUCBBOC), inspired by
the LUCB algorithm for best arm identification. Simulation results demonstrate
that the performance of LUCBBOC is comparable to that of ATBOC. We numerically
assess the effectiveness of the proposed algorithms through numerical
experiments on both synthetic datasets and the real-world MovieLens dataset. To
the best of our knowledge, this is the first work on bandit online clustering
that allows arms with different means in a cluster and $K$ greater than 2.

</details>


### [546] [Evaluating Deep Human-in-the-Loop Optimization for Retinal Implants Using Sighted Participants](https://arxiv.org/abs/2502.00177)
*Eirini Schoinas, Adyah Rastogi, Anissa Carter, Jacob Granley, Michael Beyeler*

Main category: cs.LG

TL;DR: HILO optimizes visual prostheses using human feedback, showing preference over naive methods in tests with sighted participants.


<details>
  <summary>Details</summary>
Motivation: To validate HILO's efficacy with human participants, as previous work only tested it in simulation.

Method: Participants refined a deep stimulus encoder by selecting phosphenes in three conditions: standard optimization, threshold misspecifications, and out-of-distribution sampling.

Result: Participants preferred HILO-generated stimuli over naive methods, with log odds favoring HILO in all conditions. Human decision-making differed from simulations.

Conclusion: HILO is viable for personalizing visual prostheses, emphasizing the need for human validation in optimization strategies.

Abstract: Human-in-the-loop optimization (HILO) is a promising approach for
personalizing visual prostheses by iteratively refining stimulus parameters
based on user feedback. Previous work demonstrated HILO's efficacy in
simulation, but its performance with human participants remains untested. Here
we evaluate HILO using sighted participants viewing simulated prosthetic vision
to assess its ability to optimize stimulation strategies under realistic
conditions. Participants selected between phosphenes generated by competing
encoders to iteratively refine a deep stimulus encoder (DSE). We tested HILO in
three conditions: standard optimization, threshold misspecifications, and
out-of-distribution parameter sampling. Participants consistently preferred
HILO-generated stimuli over both a naive encoder and the DSE alone, with log
odds favoring HILO across all conditions. We also observed key differences
between human and simulated decision-making, highlighting the importance of
validating optimization strategies with human participants. These findings
support HILO as a viable approach for adapting visual prostheses to
individuals. Clinical relevance: Validating HILO with sighted participants
viewing simulated prosthetic vision is an important step toward personalized
calibration of future visual prostheses.

</details>


### [547] [Implicit bias of Normalized Steepest Descent in Multiclass Classification: Sign Descent, Spectral Descent, and Adam](https://arxiv.org/abs/2502.04664)
*Chen Fan, Mark Schmidt, Christos Thrampoulidis*

Main category: cs.LG

TL;DR: The paper analyzes the implicit bias of gradient-based methods like Adam and SignGD in multi-class linear classification, proving they converge to max-margin solutions and extending this to p-norm NSD algorithms.


<details>
  <summary>Details</summary>
Motivation: To address open questions about implicit optimization bias in linear classification with separable data, particularly for multi-class settings.

Method: Characterizes the implicit bias of Adam and SignGD, proves convergence to max-margin solutions, and generalizes to p-norm NSD algorithms like Spectral Descent.

Result: Shows convergence to max-margin solutions for various norms, with insights reducing analysis to max-norm cases.

Conclusion: Multi-class linear settings offer a clear framework for studying implicit biases in matrix-parameter optimization.

Abstract: In the optimization of overparameterized models, different gradient-based
methods can achieve zero training error yet converge to distinctly different
solutions inducing different generalization properties. Despite a decade of
research on implicit optimization bias, important questions remain open even in
the foundational case of linear classification with separable data. We address
this gap by characterizing the implicit bias of both Adam and Sign gradient
descent (SignGD) in multi-class cross-entropy minimization: we prove that their
iterates converge to solutions maximizing the margin with respect to the
classifier matrix's max-norm, and we establish the corresponding convergence
rates. We then generalize our analysis to p-norm normalized steepest descent
(NSD) algorithms. This includes Spectral Descent, which we show converges to
the max-margin solution with respect to the spectral norm. A key insight is
that the analysis of general entry-wise and Schatten p-norms can be reduced to
the analysis of NSD with max-norm (i.e., SignGD) by exploiting a natural
ordering property between all p-norms relative to the max-norm and its dual
sum-norm. Our results demonstrate that the multi-class linear setting, which is
inherently richer than the binary counterpart, provides the most transparent
playground for studying implicit biases of matrix-parameter optimization
algorithms.

</details>


### [548] [DROP: Poison Dilution via Knowledge Distillation for Federated Learning](https://arxiv.org/abs/2502.07011)
*Georgios Syros, Anshuman Suri, Farinaz Koushanfar, Cristina Nita-Rotaru, Alina Oprea*

Main category: cs.LG

TL;DR: DROP is a new defense mechanism for Federated Learning that combats stealthy adversarial attacks by combining clustering, activity-tracking, and knowledge distillation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing defenses fail against targeted backdoor attacks under varied learning and attack configurations, necessitating a more robust solution.

Method: DROP integrates clustering, activity-tracking, and knowledge distillation to extract benign behavior and mitigate poisoning attacks.

Result: DROP shows superior robustness across diverse learning configurations, including non-IID data distributions.

Conclusion: DROP addresses limitations of current defenses, proving effective against stealthy adversaries, but challenges remain in non-IID settings.

Abstract: Federated Learning is vulnerable to adversarial manipulation, where malicious
clients can inject poisoned updates to influence the global model's behavior.
While existing defense mechanisms have made notable progress, they fail to
protect against adversaries that aim to induce targeted backdoors under
different learning and attack configurations. To address this limitation, we
introduce DROP (Distillation-based Reduction Of Poisoning), a novel defense
mechanism that combines clustering and activity-tracking techniques with
extraction of benign behavior from clients via knowledge distillation to tackle
stealthy adversaries that manipulate low data poisoning rates and diverse
malicious client ratios within the federation. Through extensive
experimentation, our approach demonstrates superior robustness compared to
existing defenses across a wide range of learning configurations. Finally, we
evaluate existing defenses and our method under the challenging setting of
non-IID client data distribution and highlight the challenges of designing a
resilient FL defense in this setting.

</details>


### [549] [Keep your distance: learning dispersed embeddings on $\mathbb{S}_d$](https://arxiv.org/abs/2502.08231)
*Evgeniia Tokarchuk, Hua Chang Bakker, Vlad Niculae*

Main category: cs.LG

TL;DR: The paper discusses methods for achieving well-separated features in high-dimensional spaces, introduces new approaches for dispersion, and evaluates their effectiveness in machine learning tasks.


<details>
  <summary>Details</summary>
Motivation: Learning well-separated features is crucial for machine learning, but existing methods are limited in high-dimensional spaces or when combined with task-oriented objectives.

Method: The paper reviews existing methods, proposes a reinterpretation using MMD, introduces an online variant of Lloyd's algorithm, and derives a novel hypersphere-based dispersion method.

Result: Experiments demonstrate the importance of dispersion in tasks like image classification and NLP, showing trade-offs between different algorithms.

Conclusion: The paper highlights the significance of dispersion and presents new methods to achieve it, offering practical insights for representation learning.

Abstract: Learning well-separated features in high-dimensional spaces, such as text or
image embeddings, is crucial for many machine learning applications. Achieving
such separation can be effectively accomplished through the dispersion of
embeddings, where unrelated vectors are pushed apart as much as possible. By
constraining features to be on a hypersphere, we can connect dispersion to
well-studied problems in mathematics and physics, where optimal solutions are
known for limited low-dimensional cases. However, in representation learning we
typically deal with a large number of features in high-dimensional space, and
moreover, dispersion is usually traded off with some other task-oriented
training objective, making existing theoretical and numerical solutions
inapplicable. Therefore, it is common to rely on gradient-based methods to
encourage dispersion, usually by minimizing some function of the pairwise
distances. In this work, we first give an overview of existing methods from
disconnected literature, making new connections and highlighting similarities.
Next, we introduce some new angles. We propose to reinterpret pairwise
dispersion using a maximum mean discrepancy (MMD) motivation. We then propose
an online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an
effective alternative regularizer for dispersion on generic domains. Finally,
we derive a novel dispersion method that directly exploits properties of the
hypersphere. Our experiments show the importance of dispersion in image
classification and natural language processing tasks, and how algorithms
exhibit different trade-offs in different regimes.

</details>


### [550] [Verification and Validation for Trustworthy Scientific Machine Learning](https://arxiv.org/abs/2502.15496)
*John D. Jakeman, Lorena A. Barba, Joaquim R. R. A. Martins, Thomas O'Leary-Roseberry*

Main category: cs.LG

TL;DR: The paper discusses establishing good practices for predictive scientific machine learning (SciML) to enhance trustworthiness, addressing challenges in applying existing guidelines and providing 16 recommendations.


<details>
  <summary>Details</summary>
Motivation: The rapid application of SciML has outpaced the development of trustworthy modeling practices, limiting its potential impact. The paper aims to initiate consensus on good practices.

Method: The authors identify challenges in applying computational science and engineering guidelines (e.g., verification and validation) to SciML and propose solutions.

Result: 16 recommendations are provided to improve rigor in predictive SciML modeling and documentation, applicable across SciML domains.

Conclusion: The paper advocates for standardized practices in SciML to ensure reliability and broader impact, focusing on predictive applications but extending to all SciML areas.

Abstract: Scientific machine learning (SciML) models are transforming many scientific
disciplines. However, the development of good modeling practices to increase
the trustworthiness of SciML has lagged behind its application, limiting its
potential impact. The goal of this paper is to start a discussion on
establishing consensus-based good practices for predictive SciML. We identify
key challenges in applying existing computational science and engineering
guidelines, such as verification and validation protocols, and provide
recommendations to address these challenges. Our discussion focuses on
predictive SciML, which uses machine learning models to learn, improve, and
accelerate numerical simulations of physical systems. While centered on
predictive applications, our 16 recommendations aim to help researchers conduct
and document their modeling processes rigorously across all SciML domains.

</details>


### [551] [CAGN-GAT Fusion: A Hybrid Contrastive Attentive Graph Neural Network for Network Intrusion Detection](https://arxiv.org/abs/2503.00961)
*Md Abrar Jahin, Shahriar Soudeep, Fahmid Al Farid, M. F. Mridha, Raihan Kabir, Md Rashedul Islam, Hezerul Abdul Karim*

Main category: cs.LG

TL;DR: The paper proposes CAGN-GAT Fusion, a hybrid GNN model, for network intrusion detection, showing competitive performance on imbalanced datasets compared to 15 other models.


<details>
  <summary>Details</summary>
Motivation: Growing cybersecurity threats and challenges with imbalanced datasets in traditional ML models motivate the need for robust, efficient solutions like GNNs.

Method: The study combines Contrastive Attentive Graph Network and Graph Attention Network (CAGN-GAT Fusion) and benchmarks it against 15 models using four datasets (KDD-CUP-1999, NSL-KDD, UNSW-NB15, CICIDS2017) with 5000 samples each. Adaptive graph construction techniques (edge perturbation, feature masking) are analyzed.

Result: CAGN-GAT Fusion shows stable accuracy, recall, and F1-score, though not always the highest. Adaptive techniques improve detection performance, confirming GNNs' robustness and efficiency.

Conclusion: GNNs, especially CAGN-GAT Fusion, are effective for resource-constrained environments. Future work will explore GraphSAGE layers and multiview techniques for better adaptability and accuracy.

Abstract: Cybersecurity threats are growing, making network intrusion detection
essential. Traditional machine learning models remain effective in
resource-limited environments due to their efficiency, requiring fewer
parameters and less computational time. However, handling short and highly
imbalanced datasets remains challenging. In this study, we propose the fusion
of a Contrastive Attentive Graph Network and Graph Attention Network (CAGN-GAT
Fusion) and benchmark it against 15 other models, including both Graph Neural
Networks (GNNs) and traditional ML models. Our evaluation is conducted on four
benchmark datasets (KDD-CUP-1999, NSL-KDD, UNSW-NB15, and CICIDS2017) using a
short and proportionally imbalanced dataset with a constant size of 5000
samples to ensure fairness in comparison. Results show that CAGN-GAT Fusion
demonstrates stable and competitive accuracy, recall, and F1-score, even though
it does not achieve the highest performance in every dataset. Our analysis also
highlights the impact of adaptive graph construction techniques, including
small changes in connections (edge perturbation) and selective hiding of
features (feature masking), improving detection performance. The findings
confirm that GNNs, particularly CAGN-GAT Fusion, are robust and computationally
efficient, making them well-suited for resource-constrained environments.
Future work will explore GraphSAGE layers and multiview graph construction
techniques to further enhance adaptability and detection accuracy.

</details>


### [552] [Differential Privacy Personalized Federated Learning Based on Dynamically Sparsified Client Updates](https://arxiv.org/abs/2503.09192)
*Chuanyin Wang, Yifei Zhang, Neng Gao, Qiang Luo*

Main category: cs.LG

TL;DR: The paper proposes DP-pFedDSU, a differentially private personalized federated learning method, addressing privacy leakage risks and improving model performance by dynamically sparsifying updates and using adaptive norms.


<details>
  <summary>Details</summary>
Motivation: Existing differential privacy methods in personalized federated learning introduce excessive noise and lack spatial control, compromising privacy and model effectiveness.

Method: The approach uses reparameterization to select personalized updates and dynamic adaptive norms to control update norms, minimizing noise and clipping impact.

Result: Experiments on EMNIST, CIFAR-10, and CIFAR-100 show superior performance in complex scenarios.

Conclusion: DP-pFedDSU effectively balances privacy and model performance, making it suitable for complex personalized federated learning.

Abstract: Personalized federated learning is extensively utilized in scenarios
characterized by data heterogeneity, facilitating more efficient and automated
local training on data-owning terminals. This includes the automated selection
of high-performance model parameters for upload, thereby enhancing the overall
training process. However, it entails significant risks of privacy leakage.
Existing studies have attempted to mitigate these risks by utilizing
differential privacy. Nevertheless, these studies present two major
limitations: (1) The integration of differential privacy into personalized
federated learning lacks sufficient personalization, leading to the
introduction of excessive noise into the model. (2) It fails to adequately
control the spatial scope of model update information, resulting in a
suboptimal balance between data privacy and model effectiveness in differential
privacy federated learning. In this paper, we propose a differentially private
personalized federated learning approach that employs dynamically sparsified
client updates through reparameterization and adaptive norm(DP-pFedDSU).
Reparameterization training effectively selects personalized client update
information, thereby reducing the quantity of updates. This approach minimizes
the introduction of noise to the greatest extent possible. Additionally,
dynamic adaptive norm refers to controlling the norm space of model updates
during the training process, mitigating the negative impact of clipping on the
update information. These strategies substantially enhance the effective
integration of differential privacy and personalized federated learning.
Experimental results on EMNIST, CIFAR-10, and CIFAR-100 demonstrate that our
proposed scheme achieves superior performance and is well-suited for more
complex personalized federated learning scenarios.

</details>


### [553] [SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions](https://arxiv.org/abs/2504.02698)
*Shengrui XU, Tianchi Lu, Zikun Wang, Jixiu Zhai*

Main category: cs.LG

TL;DR: SCMPPI is a novel supervised contrastive multimodal framework for PPI prediction, achieving high accuracy and cross-species generalization by integrating sequence and network features with contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional methods in cross-modal feature fusion and false-negative suppression for PPI prediction.

Method: Integrates sequence-based features (AAC, DPC, ESMC-CKSAAP) with network topology (Node2Vec embeddings) using enhanced contrastive learning and negative sample filtering.

Result: Achieves 98.13% accuracy and 99.69% AUC on benchmark datasets, with strong cross-species generalization (AUC>99%).

Conclusion: SCMPPI is a powerful tool for multimodal biological data analysis, with applications in disease target discovery.

Abstract: Protein-protein interaction (PPI) prediction plays a pivotal role in
deciphering cellular functions and disease mechanisms. To address the
limitations of traditional experimental methods and existing computational
approaches in cross-modal feature fusion and false-negative suppression, we
propose SCMPPI-a novel supervised contrastive multimodal framework. By
effectively integrating sequence-based features (AAC, DPC, ESMC-CKSAAP) with
network topology (Node2Vec embeddings) and incorporating an enhanced
contrastive learning strategy with negative sample filtering, SCMPPI achieves
superior prediction performance. Extensive experiments on eight benchmark
datasets demonstrate its state-of-the-art accuracy(98.13%) and AUC(99.69%),
along with excellent cross-species generalization (AUC>99%). Successful
applications in CD9 networks, Wnt pathway analysis, and cancer-specific
networks further highlight its potential for disease target discovery,
establishing SCMPPI as a powerful tool for multimodal biological data analysis.

</details>


### [554] [Heterogenous graph neural networks for species distribution modeling](https://arxiv.org/abs/2503.11900)
*Lauren Harrell, Christine Kaeser-Chen, Burcu Karagol Ayan, Keith Anderson, Michelangelo Conserva, Elise Kleeman, Maxim Neumann, Matt Overlan, Melissa Chapman, Drew Purves*

Main category: cs.LG

TL;DR: A novel presence-only SDM using GNNs treats species and locations as nodes, predicting detection records as edges, outperforming traditional SDMs.


<details>
  <summary>Details</summary>
Motivation: To enhance species distribution modeling by capturing fine-grained interactions between species and environmental factors.

Method: Uses a heterogeneous GNN with species and locations as distinct node sets, predicting edges (detection records). Evaluated on NCEAS's six-region dataset.

Result: GNN model matches or outperforms single-species SDMs and a feed-forward neural network baseline.

Conclusion: GNNs offer a promising approach for SDMs by effectively modeling species-environment interactions.

Abstract: Species distribution models (SDMs) are necessary for measuring and predicting
occurrences and habitat suitability of species and their relationship with
environmental factors. We introduce a novel presence-only SDM with graph neural
networks (GNN). In our model, species and locations are treated as two distinct
node sets, and the learning task is predicting detection records as the edges
that connect locations to species. Using GNN for SDM allows us to model
fine-grained interactions between species and the environment. We evaluate the
potential of this methodology on the six-region dataset compiled by National
Center for Ecological Analysis and Synthesis (NCEAS) for benchmarking SDMs. For
each of the regions, the heterogeneous GNN model is comparable to or
outperforms previously-benchmarked single-species SDMs as well as a
feed-forward neural network baseline model.

</details>


### [555] [Permutation Learning with Only N Parameters: From SoftSort to Self-Organizing Gaussians](https://arxiv.org/abs/2503.13051)
*Kai Uwe Barthel, Florian Barthel, Peter Eisert*

Main category: cs.LG

TL;DR: A novel method for learning permutations with only N parameters is introduced, improving memory efficiency and scalability while maintaining high-quality results.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Gumbel-Sinkhorn and SoftSort are computationally expensive or struggle with multidimensional data, necessitating a more efficient solution.

Method: The method extends SoftSort by iteratively shuffling indices and applying SoftSort optimization steps, reducing parameters to N.

Result: The approach outperforms SoftSort in sorting quality and efficiency, especially for multidimensional data and large-scale tasks.

Conclusion: The method is highly scalable and efficient, making it ideal for large-scale optimization tasks like Self-Organizing Gaussians.

Abstract: Sorting and permutation learning are key concepts in optimization and machine
learning, especially when organizing high-dimensional data into meaningful
spatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N
parameters to determine a full permutation matrix, making it computationally
expensive for large datasets. Low-rank matrix factorization approximations
reduce memory requirements to 2NM (with M << N), but they still struggle with
very large problems. SoftSort, by providing a continuous relaxation of the
argsort operator, allows differentiable 1D sorting, but it faces challenges
with multidimensional data and complex permutations. In this paper, we present
a novel method for learning permutations using only N parameters, which
dramatically reduces storage costs. Our method extends SoftSort by iteratively
shuffling the N indices of the elements and applying a few SoftSort
optimization steps per iteration. This modification significantly improves
sorting quality, especially for multidimensional data and complex optimization
criteria, and outperforms pure SoftSort. Our method offers improved memory
efficiency and scalability compared to existing approaches, while maintaining
high-quality permutation learning. Its dramatically reduced memory requirements
make it particularly well-suited for large-scale optimization tasks, such as
"Self-Organizing Gaussians", where efficient and scalable permutation learning
is critical.

</details>


### [556] [Borsuk-Ulam and Replicable Learning of Large-Margin Halfspaces](https://arxiv.org/abs/2503.15294)
*Ari Blondal, Hamed Hatami, Pooya Hatami, Chavdar Lalov, Sivan Tretiak*

Main category: cs.LG

TL;DR: The paper investigates whether the equivalence of list replicability, global stability, DP learnability, and shared-randomness replicability with finite Littlestone dimension extends to partial concept classes. It proves bounds for list replicability of half-spaces and resolves several open problems.


<details>
  <summary>Details</summary>
Motivation: To determine if the equivalence of various learning-theoretic properties with finite Littlestone dimension holds for partial concept classes, addressing gaps in current understanding.

Method: Proves bounds on list replicability for half-spaces using topological arguments (local Borsuk-Ulam theorem) and constructs a list-replicable learning rule using SVM generalization properties.

Result: Shows that list replicability and global stability do not necessarily follow from bounded Littlestone dimension, DP-learnability, or shared-randomness replicability for partial classes. Resolves multiple open problems in the field.

Conclusion: The equivalence of learning-theoretic properties with finite Littlestone dimension does not extend to partial concept classes, as demonstrated by the results and resolved open problems.

Abstract: Recent remarkable advances in learning theory have established that, for
total concept classes, list replicability, global stability, differentially
private (DP) learnability, and shared-randomness replicability all coincide
with the finiteness of Littlestone dimension. Does this equivalence extend to
partial concept classes?
  We answer this question by proving that the list replicability number of
$d$-dimensional $\gamma$-margin half-spaces satisfies \[ \frac{d}{2}+1 \le
\mathrm{LR}(H^d_\gamma) \le d, \] which grows with dimension. Consequently, for
partial classes, list replicability and global stability do not necessarily
follow from bounded Littlestone dimension, pure DP-learnability, or
shared-randomness replicability.
  Applying our main theorem, we resolve several open problems:
  $\bullet$ Every disambiguation of infinite-dimensional large-margin
half-spaces to a total concept class has unbounded Littlestone dimension,
answering an open question of Alon et al. (FOCS '21).
  $\bullet$ The maximum list-replicability number of any finite set of points
and homogeneous half-spaces in $d$-dimensional Euclidean space is $d$,
resolving a problem of Chase et al. (FOCS '23).
  $\bullet$ Every disambiguation of the Gap Hamming Distance problem in the
large gap regime has unbounded public-coin randomized communication complexity.
This answers an open question of Fang et al. (STOC '25).
  $\bullet$ There exists a partial concept class with Littlestone dimension $1$
such that all its disambiguations have infinite Littlestone dimension. This
answers a question of Cheung et al. (ICALP '23).
  Our lower bound follows from a topological argument based on the local
Borsuk-Ulam theorem of Chase, Chornomaz, Moran, and Yehudayoff (STOC '24). For
the upper bound, we construct a list-replicable learning rule using the
generalization properties of SVMs.

</details>


### [557] [An Efficient Alternating Algorithm for ReLU-based Symmetric Matrix Decomposition](https://arxiv.org/abs/2503.16846)
*Qingsong Wang*

Main category: cs.LG

TL;DR: Proposes ReLU-NSMD for low-rank symmetric matrix decomposition using ReLU, with AAPB method for efficient solution and proven convergence.


<details>
  <summary>Details</summary>
Motivation: Exploit low-rank structure in non-negative, sparse symmetric matrices using ReLU activation.

Method: ReLU-NSMD model with AAPB method, leveraging Bregman proximal gradient to avoid estimating global L-smooth constant.

Result: Validated effectiveness on synthetic and real datasets.

Conclusion: ReLU-NSMD and AAPB are effective for symmetric matrix decomposition.

Abstract: Symmetric matrix decomposition is an active research area in machine
learning. This paper focuses on exploiting the low-rank structure of
non-negative and sparse symmetric matrices via the rectified linear unit (ReLU)
activation function. We propose the ReLU-based nonlinear symmetric matrix
decomposition (ReLU-NSMD) model, introduce an accelerated alternating partial
Bregman (AAPB) method for its solution, and present the algorithm's convergence
results. Our algorithm leverages the Bregman proximal gradient framework to
overcome the challenge of estimating the global $L$-smooth constant in the
classic proximal gradient algorithm. Numerical experiments on synthetic and
real datasets validate the effectiveness of our model and algorithm.

</details>


### [558] [A Probabilistic Neuro-symbolic Layer for Algebraic Constraint Satisfaction](https://arxiv.org/abs/2503.19466)
*Leander Kurscheidt, Paolo Morettin, Roberto Sebastiani, Andrea Passerini, Antonio Vergari*

Main category: cs.LG

TL;DR: A differentiable probabilistic layer (PAL) is introduced to guarantee satisfaction of non-convex algebraic constraints in neural models, enabling safe operation in continuous environments.


<details>
  <summary>Details</summary>
Motivation: Neural models often fail to handle intricate algebraic constraints in safety-critical applications, such as autonomous agents avoiding obstacles.

Method: PAL is a probabilistic layer that defines distributions over linear inequalities, parameterized by polynomials, and integrates symbolically for exact renormalization.

Result: PAL is demonstrated on benchmarks and real-world trajectory data, showing effective constraint satisfaction.

Conclusion: PAL provides a scalable and exact solution for integrating algebraic constraints into neural architectures without approximations.

Abstract: In safety-critical applications, guaranteeing the satisfaction of constraints
over continuous environments is crucial, e.g., an autonomous agent should never
crash into obstacles or go off-road. Neural models struggle in the presence of
these constraints, especially when they involve intricate algebraic
relationships. To address this, we introduce a differentiable probabilistic
layer that guarantees the satisfaction of non-convex algebraic constraints over
continuous variables. This probabilistic algebraic layer (PAL) can be
seamlessly plugged into any neural architecture and trained via maximum
likelihood without requiring approximations. PAL defines a distribution over
conjunctions and disjunctions of linear inequalities, parameterized by
polynomials. This formulation enables efficient and exact renormalization via
symbolic integration, which can be amortized across different data points and
easily parallelized on a GPU. We showcase PAL and our integration scheme on a
number of benchmarks for algebraic constraint integration and on real-world
trajectory data.

</details>


### [559] [Rethinking Graph Structure Learning in the Era of LLMs](https://arxiv.org/abs/2503.21223)
*Zhihan Zhang, Xunkai Li, Guang Zeng, Hongchao Qin, Ronghua Li, Guoren Wang*

Main category: cs.LG

TL;DR: The paper introduces LLaTA, a method integrating LLMs with graph structure learning (GSL) for text-attributed graphs (TAGs), addressing optimization and architecture challenges.


<details>
  <summary>Details</summary>
Motivation: To enhance graph encoding by integrating textual descriptions (TAGs) and adapting GSL for LLMs, overcoming limitations of traditional GSL methods.

Method: Reformulates GSL objectives as a tree optimization framework and proposes decoupled, training-free LLM integration principles, leading to LLaTA.

Result: LLaTA outperforms other LLM-enhanced methods, achieving state-of-the-art performance on 10 datasets.

Conclusion: LLaTA effectively integrates LLMs with GSL for TAGs, offering flexibility, scalability, and superior predictive performance.

Abstract: Recently, the emergence of LLMs has prompted researchers to integrate
language descriptions into graphs, aiming to enhance model encoding
capabilities from a data-centric perspective. This graph representation is
called text-attributed graphs (TAGs). A review of prior advancements highlights
that graph structure learning (GSL) is a pivotal technique for improving data
utility, making it highly relevant to efficient TAG learning. However, most GSL
methods are tailored for traditional graphs without textual information,
underscoring the necessity of developing a new GSL paradigm. Despite clear
motivations, it remains challenging: (1) How can we define a reasonable
optimization objective for GSL in the era of LLMs, considering the massive
parameters in LLM? (2) How can we design an efficient model architecture that
enables seamless integration of LLM for this optimization objective? For
Question 1, we reformulate existing GSL optimization objectives as a tree
optimization framework, shifting the focus from obtaining a well-trained edge
predictor to a language-aware tree sampler. For Question 2, we propose
decoupled and training-free model design principles for LLM integration,
shifting the focus from computation-intensive fine-tuning to more efficient
inference. Based on this, we propose Large Language and Tree Assistant (LLaTA),
which leverages tree-based LLM in-context learning to enhance the understanding
of topology and text, enabling reliable inference and generating improved graph
structure. Extensive experiments on 10 datasets demonstrate that LLaTA enjoys
flexibility-incorporated with any backbone; scalability-outperforms other
LLM-enhanced graph learning methods; effectiveness-achieves SOTA predictive
performance.

</details>


### [560] [Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving](https://arxiv.org/abs/2504.12984)
*Yaoyao Ding, Bohan Hou, Xiao Zhang, Allan Lin, Tianqi Chen, Cody Yu Hao, Yida Wang, Gennady Pekhimenko*

Main category: cs.LG

TL;DR: A virtual machine (VM) for GPGPU computing is introduced to support low-precision data types with arbitrary bit widths, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Efficiently serving LLMs requires reducing computational resource demands, but current low-precision kernel approaches are limited and suboptimal.

Method: The proposed VM includes a thread-block-level programming model, hierarchical memory, an algebraic layout system, and supports diverse low-precision types, compiling into optimized GPU programs.

Result: The VM outperforms state-of-the-art methods, achieving performance improvements of up to 2.61x over existing compilers and hand-optimized kernels.

Conclusion: The VM enables efficient low-precision computations for LLMs, addressing limitations of current approaches and improving performance.

Abstract: Serving Large Language Models (LLMs) is critical for AI-powered applications
but demands substantial computational resources, particularly in memory
bandwidth and computational throughput. Low-precision computation has emerged
as a key technique to improve efficiency while reducing resource consumption.
Existing approaches for generating low-precision kernels are limited to weight
bit widths that are powers of two and suffer from suboptimal performance due to
high-level GPU programming abstractions. These abstractions restrict critical
optimizations, such as fine-grained register management and optimized memory
access patterns, which are essential for efficient low-precision computations.
In this paper, we introduce a virtual machine (VM) designed for General-Purpose
GPU (GPGPU) computing, enabling support for low-precision data types with
arbitrary bit widths while maintaining GPU programmability. The proposed VM
features a thread-block-level programming model, a hierarchical memory space, a
novel algebraic layout system, and extensive support for diverse low-precision
data types. VM programs are compiled into highly efficient GPU programs with
automatic vectorization and instruction selection. Extensive experiments
demonstrate that our VM efficiently supports a full spectrum of low-precision
data types, and outperforms state-of-the-art low-precision kernels on their
supported types. Compared to existing compilers like Triton and Ladder, as well
as hand-optimized kernels such as QuantLLM and Marlin, our VM achieves
performance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.

</details>


### [561] [Graph Kolmogorov-Arnold Networks for Multi-Cancer Classification and Biomarker Identification, An Interpretable Multi-Omics Approach](https://arxiv.org/abs/2503.22939)
*Fadi Alharbi, Nishant Budhiraja, Aleksandar Vakanski, Boyu Zhang, Murtada K. Elbashir, Hrshith Gudur, Mohanad Mohammed*

Main category: cs.LG

TL;DR: MOGKAN, a deep learning framework, integrates multi-omics data and PPI networks for cancer classification, achieving 96.28% accuracy and validated biomarkers.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of integrating heterogeneous multi-omics data for precision cancer diagnostics.

Method: Combines DESeq2, LIMMA, and LASSO regression for dimensionality reduction, and uses Kolmogorov-Arnold theorem-based architecture for interpretability.

Result: Achieves 96.28% classification accuracy with low variability and validated cancer-related biomarkers.

Conclusion: MOGKAN offers robust predictive performance and interpretability, aiding clinical translation of multi-omics data.

Abstract: The integration of heterogeneous multi-omics datasets at a systems level
remains a central challenge for developing analytical and computational models
in precision cancer diagnostics. This paper introduces Multi-Omics Graph
Kolmogorov-Arnold Network (MOGKAN), a deep learning framework that utilizes
messenger-RNA, micro-RNA sequences, and DNA methylation samples together with
Protein-Protein Interaction (PPI) networks for cancer classification across 31
different cancer types. The proposed approach combines differential gene
expression with DESeq2, Linear Models for Microarray (LIMMA), and Least
Absolute Shrinkage and Selection Operator (LASSO) regression to reduce
multi-omics data dimensionality while preserving relevant biological features.
The model architecture is based on the Kolmogorov-Arnold theorem principle and
uses trainable univariate functions to enhance interpretability and feature
analysis. MOGKAN achieves classification accuracy of 96.28 percent and exhibits
low experimental variability in comparison to related deep learning-based
models. The biomarkers identified by MOGKAN were validated as cancer-related
markers through Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes
(KEGG) enrichment analysis. By integrating multi-omics data with graph-based
deep learning, our proposed approach demonstrates robust predictive performance
and interpretability with potential to enhance the translation of complex
multi-omics data into clinically actionable cancer diagnostics.

</details>


### [562] [Towards Optimal Differentially Private Regret Bounds in Linear MDPs](https://arxiv.org/abs/2504.09339)
*Sharan Sahu*

Main category: cs.LG

TL;DR: The paper introduces a differentially private algorithm for regret minimization in episodic inhomogeneous linear MDPs, improving regret bounds while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for privacy in RL applications using sensitive user data, the study focuses on joint differential privacy (JDP) to protect data while maintaining performance.

Method: The authors privatize the LSVI-UCB++ algorithm, incorporating Bernstein-style bonuses and variance-aware techniques from offline RL.

Result: The proposed algorithm achieves a regret bound of ~O(d√(H³K) + H¹⁵/⁴d⁷/⁶K¹/²/ε), outperforming prior private methods.

Conclusion: Privacy can be ensured with minimal performance loss, as demonstrated by near-optimal utility in empirical results.

Abstract: We study regret minimization under privacy constraints in episodic
inhomogeneous linear Markov Decision Processes (MDPs), motivated by the growing
use of reinforcement learning (RL) in personalized decision-making systems that
rely on sensitive user data. In this setting, both transition probabilities and
reward functions are assumed to be linear in a feature mapping $\phi(s, a)$,
and we aim to ensure privacy through joint differential privacy (JDP), a
relaxation of differential privacy suited to online learning. Prior work has
established suboptimal regret bounds by privatizing the LSVI-UCB algorithm,
which achieves $\widetilde{O}(\sqrt{d^3 H^4 K})$ regret in the non-private
setting. Building on recent advances that improve this to near minimax optimal
regret $\widetilde{O}(d\sqrt{H^{3}K})$ via LSVI-UCB++ with Bernstein-style
bonuses, we design a new differentially private algorithm by privatizing
LSVI-UCB++ and adapting techniques for variance-aware analysis from offline RL.
Our algorithm achieves a regret bound of $\widetilde{O}(d \sqrt{H^3 K} +
H^{15/4} d^{7/6} K^{1/2} / \epsilon)$, improving over previous private methods.
Empirical results show that our algorithm retains near-optimal utility compared
to non-private baselines, indicating that privacy can be achieved with minimal
performance degradation in this setting.

</details>


### [563] [Support is All You Need for Certified VAE Training](https://arxiv.org/abs/2504.11831)
*Changming Xu, Debangshu Banerjee, Deepak Vasisht, Gagandeep Singh*

Main category: cs.LG

TL;DR: CIVET is a novel method for certified training of VAEs, providing robustness guarantees under adversarial attacks by bounding worst-case error via latent layer support sets.


<details>
  <summary>Details</summary>
Motivation: To ensure safety in critical applications, certified probabilistic guarantees for VAEs under adversarial attacks are needed.

Method: CIVET bounds worst-case VAE error by focusing on latent layer support sets and introduces a training algorithm based on this insight.

Result: Outperforms state-of-the-art methods in standard performance and robustness across datasets, architectures, and perturbation magnitudes.

Conclusion: CIVET effectively combines strong performance with certified robustness, making it suitable for safety-critical applications.

Abstract: Variational Autoencoders (VAEs) have become increasingly popular and deployed
in safety-critical applications. In such applications, we want to give
certified probabilistic guarantees on performance under adversarial attacks. We
propose a novel method, CIVET, for certified training of VAEs. CIVET depends on
the key insight that we can bound worst-case VAE error by bounding the error on
carefully chosen support sets at the latent layer. We show this point
mathematically and present a novel training algorithm utilizing this insight.
We show in an extensive evaluation across different datasets (in both the
wireless and vision application areas), architectures, and perturbation
magnitudes that our method outperforms SOTA methods achieving good standard
performance with strong robustness guarantees.

</details>


### [564] [Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification](https://arxiv.org/abs/2504.12712)
*Hyunji Jung, Hanseul Cho, Chulhee Yun*

Main category: cs.LG

TL;DR: The paper analyzes continual learning for multiple linear classification tasks using gradient descent, showing convergence to the joint max-margin solution and exploring forgetting dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how gradient descent in continual learning settings converges to joint solutions despite biases toward individual task solutions.

Method: Sequential gradient descent on tasks in cyclic/random orders, with analysis of directional convergence and forgetting.

Result: Convergence to joint max-margin solution for separable tasks; cyclic order reduces forgetting. For non-separable tasks, convergence to joint loss minimum.

Conclusion: Alignment between tasks affects forgetting, and cyclic training mitigates it. Joint solutions emerge despite individual biases.

Abstract: We study continual learning on multiple linear classification tasks by
sequentially running gradient descent (GD) for a fixed budget of iterations per
task. When all tasks are jointly linearly separable and are presented in a
cyclic/random order, we show the directional convergence of the trained linear
classifier to the joint (offline) max-margin solution. This is surprising
because GD training on a single task is implicitly biased towards the
individual max-margin solution for the task, and the direction of the joint
max-margin solution can be largely different from these individual solutions.
Additionally, when tasks are given in a cyclic order, we present a
non-asymptotic analysis on cycle-averaged forgetting, revealing that (1)
alignment between tasks is indeed closely tied to catastrophic forgetting and
backward knowledge transfer and (2) the amount of forgetting vanishes to zero
as the cycle repeats. Lastly, we analyze the case where the tasks are no longer
jointly separable and show that the model trained in a cyclic order converges
to the unique minimum of the joint loss function.

</details>


### [565] [Dual-channel Heterophilic Message Passing for Graph Fraud Detection](https://arxiv.org/abs/2504.14205)
*Wenxin Zhang, Jingxing Zhong, Guangzhen Yao, Renda Han, Xiaojian Lin, Zeyu Zhang, Cuicui Luo*

Main category: cs.LG

TL;DR: The paper proposes DHMP, a framework for fraud detection using GNNs, addressing limitations of existing methods by separating homophilic and heterophilic signals.


<details>
  <summary>Details</summary>
Motivation: Fraud detection is critical, but current GNN-based methods disrupt graph topology by excluding heterophilic neighbors, increasing prediction uncertainty.

Method: DHMP divides the graph into homophilic and heterophilic subgraphs, uses shared weights for signal capture, and employs a customized sampling strategy.

Result: DHMP outperforms existing methods on three real-world datasets, showing improved fraud detection.

Conclusion: Separating signals of different frequencies enhances fraud detection, with DHMP proving effective.

Abstract: Fraudulent activities have significantly increased across various domains,
such as e-commerce, online review platforms, and social networks, making fraud
detection a critical task. Spatial Graph Neural Networks (GNNs) have been
successfully applied to fraud detection tasks due to their strong inductive
learning capabilities. However, existing spatial GNN-based methods often
enhance the graph structure by excluding heterophilic neighbors during message
passing to align with the homophilic bias of GNNs. Unfortunately, this approach
can disrupt the original graph topology and increase uncertainty in
predictions. To address these limitations, this paper proposes a novel
framework, Dual-channel Heterophilic Message Passing (DHMP), for fraud
detection. DHMP leverages a heterophily separation module to divide the graph
into homophilic and heterophilic subgraphs, mitigating the low-pass inductive
bias of traditional GNNs. It then applies shared weights to capture signals at
different frequencies independently and incorporates a customized sampling
strategy for training. This allows nodes to adaptively balance the
contributions of various signals based on their labels. Extensive experiments
on three real-world datasets demonstrate that DHMP outperforms existing
methods, highlighting the importance of separating signals with different
frequencies for improved fraud detection. The code is available at
https://github.com/shaieesss/DHMP.

</details>


### [566] [Data Selection for ERMs](https://arxiv.org/abs/2504.14572)
*Steve Hanneke, Shay Moran, Alexander Shlimovich, Amir Yehudayoff*

Main category: cs.LG

TL;DR: The paper shifts from a model-centric to a data-centric approach, optimizing training data selection for learning rules to achieve comparable performance with fewer points.


<details>
  <summary>Details</summary>
Motivation: Traditional learning theory focuses on algorithms for fixed tasks; this work explores optimizing data selection for given learning rules.

Method: Study data selection for learning rules, analyzing performance with limited data points across empirical risk minimizers like mean estimation and linear models.

Result: Optimal bounds for data selection in mean estimation, linear classification, and regression, plus general insights for binary classification and convex optimization.

Conclusion: Demonstrates feasibility of selecting fewer points for comparable performance, with open questions for future research.

Abstract: Learning theory has traditionally followed a model-centric approach, focusing
on designing optimal algorithms for a fixed natural learning task (e.g., linear
classification or regression). In this paper, we adopt a complementary
data-centric perspective, whereby we fix a natural learning rule and focus on
optimizing the training data. Specifically, we study the following question:
given a learning rule $\mathcal{A}$ and a data selection budget $n$, how well
can $\mathcal{A}$ perform when trained on at most $n$ data points selected from
a population of $N$ points? We investigate when it is possible to select $n \ll
N$ points and achieve performance comparable to training on the entire
population.
  We address this question across a variety of empirical risk minimizers. Our
results include optimal data-selection bounds for mean estimation, linear
classification, and linear regression. Additionally, we establish two general
results: a taxonomy of error rates in binary classification and in stochastic
convex optimization. Finally, we propose several open questions and directions
for future research.

</details>


### [567] [Reinforcement Learning from Multi-level and Episodic Human Feedback](https://arxiv.org/abs/2504.14732)
*Muhammad Qasim Elahi, Somtochukwu Oguchienti, Maheed H. Ahmed, Mahsa Ghasemi*

Main category: cs.LG

TL;DR: The paper explores multi-level human feedback for reward function design in reinforcement learning, proposing an efficient algorithm to learn rewards and policies from episodic scores.


<details>
  <summary>Details</summary>
Motivation: Designing effective reward functions in complex, unstructured environments is challenging. Existing methods like comparative feedback are limited, prompting the exploration of multi-level feedback.

Method: The authors propose an algorithm to learn reward functions and optimal policies from episodic scores, a form of multi-level human feedback.

Result: The algorithm achieves sublinear regret and demonstrates empirical effectiveness in simulations.

Conclusion: Multi-level human feedback is a viable alternative to comparative feedback, offering coarse but informative signals for reward learning in reinforcement learning.

Abstract: Designing an effective reward function has long been a challenge in
reinforcement learning, particularly for complex tasks in unstructured
environments. To address this, various learning paradigms have emerged that
leverage different forms of human input to specify or refine the reward
function. Reinforcement learning from human feedback is a prominent approach
that utilizes human comparative feedback, expressed as a preference for one
behavior over another, to tackle this problem. In contrast to comparative
feedback, we explore multi-level human feedback, which is provided in the form
of a score at the end of each episode. This type of feedback offers more coarse
but informative signals about the underlying reward function than binary
feedback. Additionally, it can handle non-Markovian rewards, as it is based on
the evaluation of an entire episode. We propose an algorithm to efficiently
learn both the reward function and the optimal policy from this form of
feedback. Moreover, we show that the proposed algorithm achieves sublinear
regret and demonstrate its empirical effectiveness through extensive
simulations.

</details>


### [568] [A Basic Evaluation of Neural Networks Trained with the Error Diffusion Learning Algorithm](https://arxiv.org/abs/2504.14814)
*Kazuhisa Fujita*

Main category: cs.LG

TL;DR: Kaneko's Error Diffusion Learning Algorithm (EDLA) is a biologically plausible alternative to backpropagation, showing high accuracy in tasks like parity check, regression, and image classification.


<details>
  <summary>Details</summary>
Motivation: The lack of biological plausibility in backpropagation inspired the development of EDLA as an alternative learning method.

Method: EDLA uses a single global error signal diffusing through excitatory-inhibitory sublayers, avoiding layer-wise backpropagation.

Result: EDLA achieves high accuracy in benchmarks, with performance influenced by learning rate, neuron count, and depth. It also shows meaningful feature extraction.

Conclusion: EDLA is a viable biologically motivated training method for feedforward networks, with potential for future extensions in biologically inspired neural networks.

Abstract: Artificial neural networks are powerful tools capable of addressing various
tasks. Although the backpropagation algorithm has become a standard training
method for these neural networks, its lack of biological plausibility has
inspired the development of alternative learning approaches. One such
alternative is Kaneko's Error Diffusion Learning Algorithm (EDLA), a
biologically motivated approach wherein a single global error signal diffuses
throughout a network composed of paired excitatory-inhibitory sublayers,
thereby eliminating the necessity for layer-wise backpropagation. This study
presents a contemporary formulation of the EDLA framework and evaluates its
effectiveness through parity check, regression, and image classification tasks.
Our experimental results indicate that EDLA networks can consistently achieve
high accuracy across these benchmarks, with performance efficiency and
convergence speed notably influenced by the choice of learning rate, neuron
count, and network depth. Further investigation of the internal representations
formed by EDLA networks reveals their capacity for meaningful feature
extraction, similar to traditional neural networks. These results suggest that
EDLA is a biologically motivated alternative for training feedforward networks
and will motivate future work on extending this method to biologically inspired
neural networks.

</details>


### [569] [Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL](https://arxiv.org/abs/2504.15077)
*Simone Papicchio, Simone Rossi, Luca Cagliero, Paolo Papotti*

Main category: cs.LG

TL;DR: The paper explores how reasoning capabilities in LLMs affect Text2SQL performance, comparing ZSL, SFT, RL, and SFT+RL strategies. Results show SFT with reasoning helps small LLMs, RL is broadly effective, and fine-grained rewards boost performance.


<details>
  <summary>Details</summary>
Motivation: Small LLMs struggle with complex SQL queries under ZSL, and SFT alone isn't enough for multi-hop reasoning. The study aims to understand how reasoning impacts Text2SQL performance.

Method: Evaluates LLMs under ZSL (with/without reasoning), SFT (with/without reasoning traces), RL (using EX and fine-grained rewards), and SFT+RL on four datasets.

Result: General-purpose reasoning in ZSL fails for complex cases. SFT with reasoning aids small LLMs. RL is universally beneficial, with fine-grained rewards being most effective. A 7B model matches GPT-4o on Bird dataset.

Conclusion: Reasoning-enhanced training, especially RL with fine-grained rewards, significantly improves Text2SQL performance, enabling smaller models to compete with larger ones.

Abstract: Large Language Models (LLMs) have shown impressive capabilities in
transforming natural language questions about relational databases into SQL
queries. Despite recent improvements, small LLMs struggle to handle questions
involving multiple tables and complex SQL patterns under a Zero-Shot Learning
(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensates for the
knowledge deficits in pretrained models but falls short while dealing with
queries involving multi-hop reasoning. To bridge this gap, different LLM
training strategies to reinforce reasoning capabilities have been proposed,
ranging from leveraging a thinking process within ZSL, including reasoning
traces in SFT, or adopt Reinforcement Learning (RL) strategies. However, the
influence of reasoning on Text2SQL performance is still largely unexplored.
This paper investigates to what extent LLM reasoning capabilities influence
their Text2SQL performance on four benchmark datasets. To this end, it
considers the following LLM settings: (1) ZSL, including general-purpose
reasoning or not; (2) SFT, with and without task-specific reasoning traces; (3)
RL, exploring the use of different rewarding functions, both the established
EXecution accuracy (EX) and a mix with fine-grained ones that also account the
precision, recall, and cardinality of partially correct answers; (4) SFT+RL,
i.e, a two-stage approach that combines SFT and RL. The results show that
general-purpose reasoning under ZSL proves to be ineffective in tackling
complex Text2SQL cases. Small LLMs benefit from SFT with reasoning much more
than larger ones. RL is generally beneficial across all tested models and
datasets. The use of the fine-grained metrics turns out to be the most
effective RL strategy. Thanks to RL and the novel text2SQL rewards, the 7B
Qwen-Coder-2.5 model performs on par with 400+ Billion ones (including gpt-4o)
on the Bird dataset.

</details>


### [570] [Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation](https://arxiv.org/abs/2504.17058)
*Rahul Vishwakarma, Shrey Dharmendra Modi, Vishwanath Seshagiri*

Main category: cs.LG

TL;DR: A novel framework, Conformalized GAN (cGAN), integrates conformal prediction into GANs to provide statistical guarantees for synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: Existing generative models lack rigorous statistical guarantees, limiting their use in critical domains.

Method: Incorporates multiple conformal prediction paradigms (ICP, Mondrian, Cross-Conformal, Venn-Abers) into GANs.

Result: cGAN offers enhanced calibration and provable statistical guarantees for synthetic data.

Conclusion: The framework enables reliable synthetic data use in high-stakes domains like healthcare and finance.

Abstract: The generation of high-quality synthetic data presents significant challenges
in machine learning research, particularly regarding statistical fidelity and
uncertainty quantification. Existing generative models produce compelling
synthetic samples but lack rigorous statistical guarantees about their relation
to the underlying data distribution, limiting their applicability in critical
domains requiring robust error bounds. We address this fundamental limitation
by presenting a novel framework that incorporates conformal prediction
methodologies into Generative Adversarial Networks (GANs). By integrating
multiple conformal prediction paradigms including Inductive Conformal
Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,
and Venn-Abers Predictors, we establish distribution-free uncertainty
quantification in generated samples. This approach, termed Conformalized GAN
(cGAN), demonstrates enhanced calibration properties while maintaining the
generative power of traditional GANs, producing synthetic data with provable
statistical guarantees. We provide rigorous mathematical proofs establishing
finite-sample validity guarantees and asymptotic efficiency properties,
enabling the reliable application of synthetic data in high-stakes domains
including healthcare, finance, and autonomous systems.

</details>


### [571] [Combining GCN Structural Learning with LLM Chemical Knowledge for Enhanced Virtual Screening](https://arxiv.org/abs/2504.17497)
*Radia Berreziga, Mohammed Brahimi, Khairedine Kraim, Hamid Azzoune*

Main category: cs.LG

TL;DR: A hybrid architecture combining Graph Convolutional Networks (GCNs) and Large Language Model (LLM) embeddings improves virtual screening performance by integrating localized structural learning with global chemical knowledge.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning methods like SVM and XGBoost rely on predefined molecular representations, which may lose information or introduce bias. Deep learning, especially GCNs and LLMs, offers more expressive and unbiased alternatives.

Method: Proposes a hybrid model integrating GCNs with precomputed LLM embeddings, concatenating LLM embeddings after each GCN layer for deeper global context integration.

Result: Achieves superior performance with an F1-score of 88.8%, outperforming standalone GCN (87.9%), XGBoost (85.5%), and SVM (85.4%).

Conclusion: The hybrid architecture effectively combines GCNs and LLMs, enhancing virtual screening accuracy and computational efficiency.

Abstract: Virtual screening plays a critical role in modern drug discovery by enabling
the identification of promising candidate molecules for experimental
validation. Traditional machine learning methods such, as Support Vector
Machines (SVM) and XGBoost, rely on predefined molecular representations, often
leading to information loss and potential bias. In contrast, deep learning
approaches-particularly Graph Convolutional Networks (GCNs)-offer a more
expressive and unbiased alternative by operating directly on molecular graphs.
Meanwhile, Large Language Models (LLMs) have recently demonstrated
state-of-the-art performance in drug design, thanks to their capacity to
capture complex chemical patterns from large-scale data via attention
mechanisms.
  In this paper, we propose a hybrid architecture that integrates GCNs with
LLM-derived embeddings to combine localized structural learning with global
chemical knowledge. The LLM embeddings can be precomputed and stored in a
molecular feature library, removing the need to rerun the LLM during training
or inference and thus maintaining computational efficiency. We found that
concatenating the LLM embeddings after each GCN layer-rather than only at the
final layer-significantly improves performance, enabling deeper integration of
global context throughout the network. The resulting model achieves superior
results, with an F1-score of (88.8\%), outperforming standalone GCN (87.9%),
XGBoost (85.5%), and SVM (85.4%) baselines.

</details>


### [572] [TileLang: A Composable Tiled Programming Model for AI Systems](https://arxiv.org/abs/2504.17577)
*Lei Wang, Yu Cheng, Yining Shi, Zhengju Tang, Zhiwen Mo, Wenhao Xie, Lingxiao Ma, Yuqing Xia, Jilong Xue, Fan Yang, Zhi Yang*

Main category: cs.LG

TL;DR: TileLang is a tiled programming model for AI kernels, decoupling scheduling from dataflow to simplify high-performance kernel development while achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Writing high-performance AI kernels is complex due to hardware-centric optimizations, and existing compilers struggle with usability and expressiveness.

Method: TileLang introduces a generalized tiled programming model, separating scheduling (thread binding, layout, etc.) from dataflow via annotations and primitives.

Result: Experiments show TileLang achieves state-of-the-art performance in key kernels, proving its effectiveness.

Conclusion: TileLang's unified paradigm and transparent scheduling provide the power and flexibility needed for modern AI development.

Abstract: Modern AI workloads rely heavily on optimized computing kernels for both
training and inference. These AI kernels follow well-defined data-flow
patterns, such as moving tiles between DRAM and SRAM and performing a sequence
of computations on those tiles. However, writing high-performance kernels
remains complex despite the clarity of these patterns. Achieving peak
performance requires careful, hardware-centric optimizations to fully leverage
modern accelerators. While domain-specific compilers attempt to reduce the
burden of writing high-performance kernels, they often struggle with usability
and expressiveness gaps. In this paper, we present TileLang, a generalized
tiled programming model for more efficient AI Kernel programming. TileLang
decouples scheduling space (thread binding, layout, tensorize and pipeline)
from dataflow, and encapsulated them as a set of customization annotations and
primitives. This approach allows users to focus on the kernel's data-flow
itself, while leaving most other optimizations to compilers. We conduct
comprehensive experiments on commonly-used devices, across numerous
experiments, our evaluation shows that TileLang can achieve state-of-the-art
performance in key kernels, demonstrating that its unified block-and-thread
paradigm and transparent scheduling capabilities deliver both the power and
flexibility demanded by modern AI system development.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [573] [Symmetric Policy Design for Multi-Agent Dispatch Coordination in Supply Chains](https://arxiv.org/abs/2504.19397)
*Sagar Sudhakara*

Main category: cs.MA

TL;DR: Proposes symmetric dispatch strategies for multi-agent supply chains, reducing coordination costs via dynamic programming.


<details>
  <summary>Details</summary>
Motivation: Address decentralized dispatch coordination in multi-agent supply chains with shared logistics capacity, avoiding centralized control.

Method: Uses a common information approach to derive dynamic programming solutions, transforming the problem into a tractable program.

Result: Simulations show significant cost reduction compared to baseline heuristics like belief-based strategies.

Conclusion: Symmetric strategy design with common information-based dynamic programming improves multi-agent coordination.

Abstract: We study a decentralized dispatch coordination problem in a multi-agent
supply chain setting with shared logistics capacity. We propose symmetric
(identical) dispatch strategies for all agents, enabling efficient coordination
without centralized control. Using a common information approach, we derive a
dynamic programming solution that computes optimal symmetric dispatch
strategies by transforming the multi-agent problem into a tractable dynamic
program on the agents common information state. Simulation results demonstrate
that our method significantly reduces coordination cost compared to baseline
heuristics, including belief-based strategies and an always-dispatch policy.
These findings highlight the benefits of combining symmetric strategy design
with a common information-based dynamic programming framework for improving
multi-agent coordination performance.

</details>


### [574] [Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies](https://arxiv.org/abs/2504.19487)
*Kavindu Warnakulasuriya, Prabhash Dissanayake, Navindu De Silva, Stephen Cranefield, Bastin Tony Roy Savarimuthu, Surangika Ranathunga, Nisansa de Silva*

Main category: cs.MA

TL;DR: The paper explores cooperation dynamics in LLM-based agent simulations, confirming Boyd and Richerson's model and showing punishment-driven norm emergence.


<details>
  <summary>Details</summary>
Motivation: To test if cooperation dynamics from abstract models persist in realistic LLM-agent simulations of the diner's dilemma.

Method: Uses LLM agents with natural language reasoning and pairwise imitation for strategy adoption.

Result: Agents replicate Boyd and Richerson's model, with punishment reinforcing cooperation.

Conclusion: LLM-based simulations can replicate traditional models and offer a more realistic testbed for cooperative behavior.

Abstract: The evolution of cooperation has been extensively studied using abstract
mathematical models and simulations. Recent advances in Large Language Models
(LLM) and the rise of LLM agents have demonstrated their ability to perform
social reasoning, thus providing an opportunity to test the emergence of norms
in more realistic agent-based simulations with human-like reasoning using
natural language. In this research, we investigate whether the cooperation
dynamics presented in Boyd and Richerson's model persist in a more realistic
simulation of the diner's dilemma using LLM agents compared to the abstract
mathematical nature in the work of Boyd and Richerson. Our findings indicate
that agents follow the strategies defined in the Boyd and Richerson model, and
explicit punishment mechanisms drive norm emergence, reinforcing cooperative
behaviour even when the agent strategy configuration varies. Our results
suggest that LLM-based Multi-Agent System simulations, in fact, can replicate
the evolution of cooperation predicted by the traditional mathematical models.
Moreover, our simulations extend beyond the mathematical models by integrating
natural language-driven reasoning and a pairwise imitation method for strategy
adoption, making them a more realistic testbed for cooperative behaviour in
MASs.

</details>


### [575] [Diffusion Stochastic Learning Over Adaptive Competing Networks](https://arxiv.org/abs/2504.19635)
*Yike Zhao, Haoyuan Cai, Ali H. Sayed*

Main category: cs.MA

TL;DR: The paper studies a stochastic dynamic game between two competing teams of agents, proposing diffusion learning algorithms for zero-sum and non-zero-sum games, with theoretical and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To address competitive scenarios where teams of agents have conflicting objectives, requiring strategies for intra-team collaboration and inter-team adaptation.

Method: Proposes diffusion learning algorithms for two game classes: zero-sum (weak cross-team interactions) and non-zero-sum (strong cross-team interactions).

Result: The algorithms' stability is analyzed theoretically and validated through experiments on Cournot competition and decentralized GAN training.

Conclusion: The proposed methods effectively handle competitive team dynamics, balancing intra-team collaboration and inter-team adaptation.

Abstract: This paper studies a stochastic dynamic game between two competing teams,
each consisting of a network of collaborating agents. Unlike fully cooperative
settings, where all agents share a common objective, each team in this game
aims to minimize its own distinct objective. In the adversarial setting, their
objectives could be conflicting as in zero-sum games. Throughout the
competition, agents share strategic information within their own team while
simultaneously inferring and adapting to the strategies of the opposing team.
We propose diffusion learning algorithms to address two important classes of
this network game: i) a zero-sum game characterized by weak cross-team subgraph
interactions, and ii) a general non-zero-sum game exhibiting strong cross-team
subgraph interactions. We analyze the stability performance of the proposed
algorithms under reasonable assumptions and illustrate the theoretical results
through experiments on Cournot team competition and decentralized GAN training.

</details>


### [576] [PhenoAssistant: A Conversational Multi-Agent AI System for Automated Plant Phenotyping](https://arxiv.org/abs/2504.19818)
*Feng Chen, Ilias Stogiannidis, Andrew Wood, Danilo Bueno, Dominic Williams, Fraser Macfarlane, Bruce Grieve, Darren Wells, Jonathan A. Atkinson, Malcolm J. Hawkesford, Stephen A. Rolfe, Tracy Lawson, Tony Pridmore, Mario Valerio Giuffrida, Sotirios A. Tsaftaris*

Main category: cs.MA

TL;DR: PhenoAssistant is an AI-driven system simplifying plant phenotyping through natural language interaction, making it accessible to non-experts.


<details>
  <summary>Details</summary>
Motivation: Existing plant phenotyping tools are complex and hard to use, limiting accessibility for non-experts.

Method: PhenoAssistant uses a large language model to manage tasks like phenotype extraction, visualization, and model training.

Result: Validated through case studies and evaluations, it lowers technical barriers in plant biology.

Conclusion: PhenoAssistant demonstrates AI's potential to democratize plant phenotyping.

Abstract: Plant phenotyping increasingly relies on (semi-)automated image-based
analysis workflows to improve its accuracy and scalability. However, many
existing solutions remain overly complex, difficult to reimplement and
maintain, and pose high barriers for users without substantial computational
expertise. To address these challenges, we introduce PhenoAssistant: a
pioneering AI-driven system that streamlines plant phenotyping via intuitive
natural language interaction. PhenoAssistant leverages a large language model
to orchestrate a curated toolkit supporting tasks including automated phenotype
extraction, data visualisation and automated model training. We validate
PhenoAssistant through several representative case studies and a set of
evaluation tasks. By significantly lowering technical hurdles, PhenoAssistant
underscores the promise of AI-driven methodologies to democratising AI adoption
in plant biology.

</details>


### [577] [Windowed MAPF with Completeness Guarantees](https://arxiv.org/abs/2410.01798)
*Rishi Veerapaneni, Muhammad Suhail Saleem, Jiaoyang Li, Maxim Likhachev*

Main category: cs.MA

TL;DR: WinC-MAPF introduces a framework for complete windowed MAPF, addressing deadlock/livelock issues by combining real-time heuristic search insights and agent independence. SS-CBS, an instantiation, effectively solves tough scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional MAPF methods are slow for replanning, and windowed approaches lack completeness, leading to deadlock/livelock.

Method: Combines real-time heuristic search updates and agent independence ideas, introducing SS-CBS as a novel CBS modification.

Result: SS-CBS successfully solves challenging scenarios where other windowed methods fail.

Conclusion: WinC-MAPF and SS-CBS provide a complete and efficient solution for windowed MAPF.

Abstract: Traditional multi-agent path finding (MAPF) methods try to compute entire
start-goal paths which are collision free. However, computing an entire path
can take too long for MAPF systems where agents need to replan fast. Methods
that address this typically employ a "windowed" approach and only try to find
collision free paths for a small windowed timestep horizon. This adaptation
comes at the cost of incompleteness; all current windowed approaches can become
stuck in deadlock or livelock. Our main contribution is to introduce our
framework, WinC-MAPF, for Windowed MAPF that enables completeness. Our
framework uses heuristic update insights from single-agent real-time heuristic
search algorithms as well as agent independence ideas from MAPF algorithms. We
also develop Single-Step CBS (SS-CBS), an instantiation of this framework using
a novel modification to CBS. We show how SS-CBS, which only plans a single step
and updates heuristics, can effectively solve tough scenarios where existing
windowed approaches fail.

</details>


### [578] [Online Dynamic Pricing for Electric Vehicle Charging Stations with Reservations](https://arxiv.org/abs/2410.05538)
*Jan Mrkos, Antonín Komenda, David Fiedler, Jiří Vokřínek*

Main category: cs.MA

TL;DR: A dynamic pricing model for EV charging services integrates reservation, parking, and charging, using a Poisson process and MDP for optimization, with a focus on discretization error analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of pricing EV charging services dynamically by combining reservation, parking, and charging into a bundled offering.

Method: Uses a Poisson process for charging reservation arrivals and optimizes pricing via a Markov Decision Process (MDP), with a heuristic method based on Monte-Carlo tree search.

Result: Demonstrates feasibility of the MDP model and provides a practical dynamic pricing strategy for real-world use.

Conclusion: The model effectively integrates multiple services into a dynamic pricing framework, with potential for real-world implementation.

Abstract: This paper introduces a novel model for online dynamic pricing of electric
vehicle charging services that integrates reservation, parking, and charging
into a comprehensive bundle priced as a whole. Our approach focuses on the
individual high-demand, fast-charging location, employing a Poisson process as
a model of charging reservation arrivals, and develops an online dynamic
pricing strategy optimized through a Markov Decision Process (MDP). A key
contribution is the novel analysis of discretization error introduced when
incorporating the continuous-time Poisson process into the discrete MDP
framework. The MDP model's feasibility is demonstrated with a heuristic dynamic
pricing method based on Monte-Carlo tree search, offering a viable path for
real-world applications.

</details>


### [579] [Incentivized Symbiosis: A Paradigm for Human-Agent Coevolution](https://arxiv.org/abs/2412.06855)
*Tomer Jordi Chaffer, Justin Goldston, Gemach D. A. T. A. I*

Main category: cs.MA

TL;DR: The paper proposes 'Incentivized Symbiosis,' a social contract between humans and AI, using Web3 and blockchain to enforce cooperation rules and incentives.


<details>
  <summary>Details</summary>
Motivation: Understanding and fostering cooperation between humans and AI is critical as AI becomes integral to human systems.

Method: The paper conceptualizes a framework inspired by Web3 and blockchain to define and enforce rules and incentives for human-AI cooperation.

Result: The proposed paradigm aims to enable cooperative human-agent coevolution by leveraging transparency, accountability, and trust.

Conclusion: This approach could catalyze research at the intersection of AI, Web3, and society, promoting innovative cooperation pathways.

Abstract: Cooperation is vital to our survival and progress. Evolutionary game theory
offers a lens to understand the structures and incentives that enable
cooperation to be a successful strategy. As artificial intelligence agents
become integral to human systems, the dynamics of cooperation take on
unprecedented significance. The convergence of human-agent teaming, contract
theory, and decentralized frameworks like Web3, grounded in transparency,
accountability, and trust, offers a foundation for fostering cooperation by
establishing enforceable rules and incentives for humans and AI agents. We
conceptualize Incentivized Symbiosis as a social contract between humans and
AI, inspired by Web3 principles and encoded in blockchain technology, to define
and enforce rules, incentives, and consequences for both parties. By exploring
this paradigm, we aim to catalyze new research at the intersection of systems
thinking in AI, Web3, and society, fostering innovative pathways for
cooperative human-agent coevolution.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [580] [A Survey on Multimodal Music Emotion Recognition](https://arxiv.org/abs/2504.18799)
*Rashini Liyanarachchi, Aditya Joshi, Erik Meijering*

Main category: cs.MM

TL;DR: A survey on multimodal music emotion recognition (MMER) outlining a four-stage framework, advancements in deep learning, and remaining challenges like dataset needs and real-time processing.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of MMER, highlight advancements, and identify gaps for future research.

Method: Introduces a four-stage MMER framework: multimodal data selection, feature extraction, feature processing, and emotion prediction. Discusses deep learning and feature fusion techniques.

Result: Reveals progress in deep learning and feature fusion but notes challenges like dataset limitations and real-time processing needs.

Conclusion: Identifies gaps in MMER research and suggests future directions, emphasizing robust, scalable models for applications in music recommendation, therapy, and entertainment.

Abstract: Multimodal music emotion recognition (MMER) is an emerging discipline in
music information retrieval that has experienced a surge in interest in recent
years. This survey provides a comprehensive overview of the current
state-of-the-art in MMER. Discussing the different approaches and techniques
used in this field, the paper introduces a four-stage MMER framework, including
multimodal data selection, feature extraction, feature processing, and final
emotion prediction. The survey further reveals significant advancements in deep
learning methods and the increasing importance of feature fusion techniques.
Despite these advancements, challenges such as the need for large annotated
datasets, datasets with more modalities, and real-time processing capabilities
remain. This paper also contributes to the field by identifying critical gaps
in current research and suggesting potential directions for future research.
The gaps underscore the importance of developing robust, scalable, a
interpretable models for MMER, with implications for applications in music
recommendation systems, therapeutic tools, and entertainment.

</details>


### [581] [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
*Taoyu Su, Jiawei Sheng, Duohe Ma, Xiaodong Li, Juwei Yue, Mengxiao Song, Yingkai Tang, Tingwen Liu*

Main category: cs.MM

TL;DR: CDMEA is a counterfactual debiasing framework for Multi-Modal Entity Alignment (MMEA) that reduces visual modality bias by leveraging causal effects, outperforming 14 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing MMEA methods overly rely on visual features, which can bias the model and degrade performance for low-similarity images.

Method: Proposes CDMEA, a framework that estimates Total Effect (TE) and excludes Natural Direct Effect (NDE) of visual modality to reduce bias, focusing on Total Indirect Effect (TIE).

Result: CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity, high-noise, and low-resource scenarios.

Conclusion: CDMEA effectively reduces visual modality bias and improves MMEA performance by leveraging causal analysis.

Abstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from
different Multi-Modal Knowledge Graphs (MMKGs), a critical information
retrieval task. Existing studies have explored various fusion paradigms and
consistency constraints to improve the alignment of equivalent entities, while
overlooking that the visual modality may not always contribute positively.
Empirically, entities with low-similarity images usually generate
unsatisfactory performance, highlighting the limitation of overly relying on
visual features. We believe the model can be biased toward the visual modality,
leading to a shortcut image-matching task. To address this, we propose a
counterfactual debiasing framework for MMEA, termed CDMEA, which investigates
visual modality bias from a causal perspective. Our approach aims to leverage
both visual and graph modalities to enhance MMEA while suppressing the direct
causal effect of the visual modality on model predictions. By estimating the
Total Effect (TE) of both modalities and excluding the Natural Direct Effect
(NDE) of the visual modality, we ensure that the model predicts based on the
Total Indirect Effect (TIE), effectively utilizing both modalities and reducing
visual modality bias. Extensive experiments on 9 benchmark datasets show that
CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,
high-noise, and low-resource data scenarios.

</details>


### [582] [WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution](https://arxiv.org/abs/2504.19595)
*Pietro Bongini, Sara Mandelli, Andrea Montibeller, Mirko Casu, Orazio Pontorno, Claudio Ragaglia, Luca Zanchetta, Mattia Aquilina, Taiba Majid Wani, Luca Guarnera, Benedetta Tondi, Paolo Bestagini, Irene Amerini, Francesco Denatale, Sebastiano Battiato, Mauro Barni*

Main category: cs.MM

TL;DR: WILD is a dataset for training and benchmarking synthetic image source attribution models, featuring 20,000 images from 20 generators, including post-processed and adversarial examples.


<details>
  <summary>Details</summary>
Motivation: The challenge of attributing synthetic images to their sources due to the growing number of generators and lack of diverse datasets.

Method: Creation of the WILD dataset with closed and open sets of 10 generators each, totaling 20,000 images, half post-processed. Benchmarking seven baseline models.

Result: WILD enables robust benchmarking for tasks like closed/open set identification, verification, and post-processing robustness. Baseline results are provided.

Conclusion: WILD is a valuable tool for advancing synthetic image attribution, with models expected to perform better due to its challenging design.

Abstract: Synthetic image source attribution is an open challenge, with an increasing
number of image generators being released yearly. The complexity and the sheer
number of available generative techniques, as well as the scarcity of
high-quality open source datasets of diverse nature for this task, make
training and benchmarking synthetic image source attribution models very
challenging. WILD is a new in-the-Wild Image Linkage Dataset designed to
provide a powerful training and benchmarking tool for synthetic image
attribution models. The dataset is built out of a closed set of 10 popular
commercial generators, which constitutes the training base of attribution
models, and an open set of 10 additional generators, simulating a real-world
in-the-wild scenario. Each generator is represented by 1,000 images, for a
total of 10,000 images in the closed set and 10,000 images in the open set.
Half of the images are post-processed with a wide range of operators. WILD
allows benchmarking attribution models in a wide range of tasks, including
closed and open set identification and verification, and robust attribution
with respect to post-processing and adversarial attacks. Models trained on WILD
are expected to benefit from the challenging scenario represented by the
dataset itself. Moreover, an assessment of seven baseline methodologies on
closed and open set attribution is presented, including robustness tests with
respect to post-processing.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [583] [Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation](https://arxiv.org/abs/2504.18539)
*Sungnyun Kim, Sungwoo Cho, Sangmin Bae, Kangwook Jang, Se-Young Yun*

Main category: eess.AS

TL;DR: CAV2vec is a self-supervised framework for AVSR that handles joint audio-visual corruption via self-distillation and unimodal multi-task learning, improving robustness in noisy or corrupted environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of research on visual corruptions (e.g., lip occlusions) in AVSR, which degrade performance despite audio-visual fusion's advantages in noisy settings.

Method: Proposes CAV2vec, using self-distillation with a corrupted prediction task. Unimodal multi-task learning aligns corrupted modalities by predicting clean targets (audio/video) from corrupted inputs.

Result: CAV2vec significantly improves recognition accuracy in environments with various corruptions, as shown in robust AVSR benchmarks.

Conclusion: The framework effectively mitigates representation dispersion from corrupted modalities, enhancing robustness in real-world AVSR applications.

Abstract: Audio-visual speech recognition (AVSR) incorporates auditory and visual
modalities to improve recognition accuracy, particularly in noisy environments
where audio-only speech systems are insufficient. While previous research has
largely addressed audio disruptions, few studies have dealt with visual
corruptions, e.g., lip occlusions or blurred videos, which are also
detrimental. To address this real-world challenge, we propose CAV2vec, a novel
self-supervised speech representation learning framework particularly designed
to handle audio-visual joint corruption. CAV2vec employs a self-distillation
approach with a corrupted prediction task, where the student model learns to
predict clean targets, generated by the teacher model, with corrupted input
frames. Specifically, we suggest a unimodal multi-task learning, which distills
cross-modal knowledge and aligns the corrupted modalities, by predicting clean
audio targets with corrupted videos, and clean video targets with corrupted
audios. This strategy mitigates the dispersion in the representation space
caused by corrupted modalities, leading to more reliable and robust
audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that
the corrupted representation learning method significantly enhances recognition
accuracy across generalized environments involving various types of corruption.

</details>


### [584] [Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention](https://arxiv.org/abs/2504.19046)
*Billel Essaid, Hamza Kheddar, Noureddine Batel*

Main category: eess.AS

TL;DR: Deep learning improves cochlear implant electrodograms, matching traditional methods in intelligibility while offering better adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional cochlear implant coding strategies like ACE lack adaptability and precision, prompting exploration of deep learning for better performance.

Method: A deep learning model was developed to generate electrodograms and compared to the ACE strategy using the STOI metric for intelligibility.

Result: The DL model achieved a STOI score of 0.6031, close to ACE's 0.6126, with added flexibility and adaptability benefits.

Conclusion: AI integration in cochlear implants enhances personalization and efficiency, demonstrating promise for future advancements.

Abstract: Cochlear implants (CIs) play a vital role in restoring hearing for
individuals with severe to profound sensorineural hearing loss by directly
stimulating the auditory nerve with electrical signals. While traditional
coding strategies, such as the advanced combination encoder (ACE), have proven
effective, they are constrained by their adaptability and precision. This paper
investigates the use of deep learning (DL) techniques to generate
electrodograms for CIs, presenting our model as an advanced alternative. We
compared the performance of our model with the ACE strategy by evaluating the
intelligibility of reconstructed audio signals using the short-time objective
intelligibility (STOI) metric. The results indicate that our model achieves a
STOI score of 0.6031, closely approximating the 0.6126 score of the ACE
strategy, and offers potential advantages in flexibility and adaptability. This
study underscores the benefits of incorporating artificial intelligent (AI)
into CI technology, such as enhanced personalization and efficiency.

</details>


### [585] [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
*Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Ruiqi Li, Jingyu Lu, Rongjie Huang, Ruiyuan Zhang, Zhiqing Hong, Ziyue Jiang, Zhou Zhao*

Main category: eess.AS

TL;DR: VersBand is a multi-task song generation framework addressing vocal and accompaniment alignment and control, outperforming baselines in quality and versatility.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack prompt-based control and alignment for vocals and accompaniments, and fail to support diverse tasks.

Method: VersBand includes VocalBand (flow-matching for vocals), AccompBand (flow-based transformer for accompaniments), and additional models (LyricBand, MelodyBand) for comprehensive control.

Result: VersBand outperforms baseline models in multiple song generation tasks, validated by objective and subjective metrics.

Conclusion: VersBand successfully synthesizes high-quality, aligned songs with extensive prompt-based control, advancing multi-task song generation.

Abstract: Song generation focuses on producing controllable high-quality songs based on
various prompts. However, existing methods struggle to generate vocals and
accompaniments with prompt-based control and proper alignment. Additionally,
they fall short in supporting various tasks. To address these challenges, we
introduce VersBand, a multi-task song generation framework for synthesizing
high-quality, aligned songs with prompt-based control. VersBand comprises these
primary models: 1) VocalBand, a decoupled model, leverages the flow-matching
method for generating singing styles, pitches, and mel-spectrograms, allowing
fast, high-quality vocal generation with style control. 2) AccompBand, a
flow-based transformer model, incorporates the Band-MOE, selecting suitable
experts for enhanced quality, alignment, and control. This model allows for
generating controllable, high-quality accompaniments aligned with vocals. 3)
Two generation models, LyricBand for lyrics and MelodyBand for melodies,
contribute to the comprehensive multi-task song generation system, allowing for
extensive control based on multiple prompts. Experimental results demonstrate
that VersBand performs better over baseline models across multiple song
generation tasks using objective and subjective metrics. Audio samples are
available at https://VersBand.github.io.

</details>


### [586] [A Comparative Study on Positional Encoding for Time-frequency Domain Dual-path Transformer-based Source Separation Models](https://arxiv.org/abs/2504.19605)
*Kohei Saijo, Tetsuji Ogawa*

Main category: eess.AS

TL;DR: PE improves performance for sequences of training length but harms length extrapolation, especially with convolutional layers.


<details>
  <summary>Details</summary>
Motivation: Explore the impact of PE on source separation and length extrapolation in TF-domain dual-path Transformer models.

Method: Compare various PE methods using TF-Locoformer as the base architecture.

Result: PE boosts performance for trained lengths but reduces extrapolation ability, notably with convolutional layers.

Conclusion: PE choice is critical for balancing performance and generalization in TF-domain models.

Abstract: In this study, we investigate the impact of positional encoding (PE) on
source separation performance and the generalization ability to long sequences
(length extrapolation) in Transformer-based time-frequency (TF) domain
dual-path models. The length extrapolation capability in TF-domain dual-path
models is a crucial factor, as it affects not only their performance on
long-duration inputs but also their generalizability to signals with unseen
sampling rates. While PE is known to significantly impact length extrapolation,
there has been limited research that explores the choice of PEs for TF-domain
dual-path models from this perspective. To address this gap, we compare various
PE methods using a recent state-of-the-art model, TF-Locoformer, as the base
architecture. Our analysis yields the following key findings: (i) When handling
sequences that are the same length as or shorter than those seen during
training, models with PEs achieve better performance. (ii) However, models
without PE exhibit superior length extrapolation. This trend is particularly
pronounced when the model contains convolutional layers.

</details>


### [587] [TS3-Codec: Transformer-Based Simple Streaming Single Codec](https://arxiv.org/abs/2411.18803)
*Haibin Wu, Naoyuki Kanda, Sefik Emre Eskimez, Jinyu Li*

Main category: eess.AS

TL;DR: TS3-Codec, a transformer-based audio codec, outperforms convolution-based models with less computation and bitrate.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of transformer-based architectures in neural audio codecs, which are typically convolution-based.

Method: TS3-Codec uses a stack of transformer layers and linear layers, eliminating convolutions for simplicity and efficiency.

Result: Achieves comparable or better performance than state-of-the-art convolution-based codecs with 12% computation and 77% bitrate.

Conclusion: Transformer-based architectures like TS3-Codec offer a simpler, more efficient alternative to convolution-based audio codecs.

Abstract: Neural audio codecs (NACs) have garnered significant attention as key
technologies for audio compression as well as audio representation for speech
language models. While mainstream NAC models are predominantly
convolution-based, the performance of NACs with a purely transformer-based, and
convolution-free architecture remains unexplored. This paper introduces
TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec
consists of only a stack of transformer layers with a few linear layers,
offering greater simplicity and expressiveness by fully eliminating convolution
layers that require careful hyperparameter tuning and large computations. Under
the streaming setup, the proposed TS3-Codec achieves comparable or superior
performance compared to the codec with state-of-the-art convolution-based
architecture while requiring only 12% of the computation and 77% of bitrate.
Furthermore, it significantly outperforms the convolution-based codec when
using similar computational resources.

</details>


### [588] [Improving Acoustic Scene Classification in Low-Resource Conditions](https://arxiv.org/abs/2412.20722)
*Zhi Chen, Yun-Fei Shao, Yong Ma, Mingsheng Wei, Le Zhang, Wei-Qiang Zhang*

Main category: eess.AS

TL;DR: DS-FlexiNet is a novel model for Acoustic Scene Classification (ASC) in low-resource settings, combining MobileNetV2's depthwise separable convolutions with ResNet-inspired residual connections. It uses QAT, ADIR, FMS, and KD for efficiency, generalization, and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing ASC challenges in low-resource conditions, including hardware limitations and device heterogeneity, while maintaining accuracy and efficiency.

Method: Proposes DS-FlexiNet with depthwise separable convolutions, residual connections, QAT, ADIR, FMS, and KD from twelve teacher models. Includes a Residual Normalization layer for domain adaptation.

Result: DS-FlexiNet demonstrates superior adaptability and performance in resource-constrained scenarios.

Conclusion: The model effectively balances efficiency and accuracy, making it suitable for low-resource ASC tasks.

Abstract: Acoustic Scene Classification (ASC) identifies an environment based on an
audio signal. This paper explores ASC in low-resource conditions and proposes a
novel model, DS-FlexiNet, which combines depthwise separable convolutions from
MobileNetV2 with ResNet-inspired residual connections for a balance of
efficiency and accuracy. To address hardware limitations and device
heterogeneity, DS-FlexiNet employs Quantization Aware Training (QAT) for model
compression and data augmentation methods like Auto Device Impulse Response
(ADIR) and Freq-MixStyle (FMS) to improve cross-device generalization.
Knowledge Distillation (KD) from twelve teacher models further enhances
performance on unseen devices. The architecture includes a custom Residual
Normalization layer to handle domain differences across devices, and depthwise
separable convolutions reduce computational overhead without sacrificing
feature representation. Experimental results show that DS-FlexiNet excels in
both adaptability and performance under resource-constrained conditions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [589] [Dual-Modality Computational Ophthalmic Imaging with Deep Learning and Coaxial Optical Design](https://arxiv.org/abs/2504.18549)
*Boyuan Peng, Jiaju Chen, Yiwei Zhang, Cuiyi Peng, Junyang Li, Jiaming Deng, Peiwu Qin*

Main category: eess.IV

TL;DR: A compact dual-function optical device integrates fundus photography and refractive error detection, using a coaxial design and Dense-U-Net for automated alignment, achieving high precision and reliability for scalable eye screening.


<details>
  <summary>Details</summary>
Motivation: Address the growing burden of myopia and retinal diseases by providing an accessible and efficient eye screening solution.

Method: Coaxial optical design with dichroic mirrors for wavelength separation, combined with a Dense-U-Net algorithm for pupil segmentation and automated alignment.

Result: High-precision pupil localization (EDE = 2.8 px, mIoU = 0.931) and reliable refractive estimation (mean absolute error <5%).

Conclusion: The framework offers a promising, scalable solution for rapid and intelligent ophthalmic screening, especially in community health settings.

Abstract: The growing burden of myopia and retinal diseases necessitates more
accessible and efficient eye screening solutions. This study presents a
compact, dual-function optical device that integrates fundus photography and
refractive error detection into a unified platform. The system features a
coaxial optical design using dichroic mirrors to separate wavelength-dependent
imaging paths, enabling simultaneous alignment of fundus and refraction
modules. A Dense-U-Net-based algorithm with customized loss functions is
employed for accurate pupil segmentation, facilitating automated alignment and
focusing. Experimental evaluations demonstrate the system's capability to
achieve high-precision pupil localization (EDE = 2.8 px, mIoU = 0.931) and
reliable refractive estimation with a mean absolute error below 5%. Despite
limitations due to commercial lens components, the proposed framework offers a
promising solution for rapid, intelligent, and scalable ophthalmic screening,
particularly suitable for community health settings.

</details>


### [590] [Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis](https://arxiv.org/abs/2504.18802)
*Xiren Zhou, Shikang Liu, Xinyu Yan, Yizhan Fan, Xiangyu Wang, Yu Kang, Jian Cheng, Huanhuan Chen*

Main category: eess.IV

TL;DR: Res-SAM, a novel framework combining visual and wave-based analysis, improves GPR anomaly detection with high accuracy (>85%) and minimal training.


<details>
  <summary>Details</summary>
Motivation: Urban infrastructure faces threats from subsurface anomalies, but GPR-based detection is hindered by limited labeled data and unclear boundaries.

Method: Res-SAM uses visual prompts and EM wave analysis to refine anomaly regions, requiring minimal non-target data and human interaction.

Result: Achieves >85% accuracy, outperforms state-of-the-art methods, and reduces manual effort and computational costs.

Conclusion: Res-SAM offers a scalable, efficient solution for urban safety monitoring, enhancing anomaly detection with minimal resources.

Abstract: Urban roads and infrastructure, vital to city operations, face growing
threats from subsurface anomalies like cracks and cavities. Ground Penetrating
Radar (GPR) effectively visualizes underground conditions employing
electromagnetic (EM) waves; however, accurate anomaly detection via GPR remains
challenging due to limited labeled data, varying subsurface conditions, and
indistinct target boundaries. Although visually image-like, GPR data
fundamentally represent EM waves, with variations within and between waves
critical for identifying anomalies. Addressing these, we propose the
Reservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework
exploiting both visual discernibility and wave-changing properties of GPR data.
Res-SAM initially identifies apparent candidate anomaly regions given minimal
prompts, and further refines them by analyzing anomaly-induced changing
information within and between EM waves in local GPR data, enabling precise and
complete anomaly region extraction and category determination. Real-world
experiments demonstrate that Res-SAM achieves high detection accuracy (>85%)
and outperforms state-of-the-art. Notably, Res-SAM requires only minimal
accessible non-target data, avoids intensive training, and incorporates simple
human interaction to enhance reliability. Our research provides a scalable,
resource-efficient solution for rapid subsurface anomaly detection across
diverse environments, improving urban safety monitoring while reducing manual
effort and computational cost.

</details>


### [591] [Surgeons vs. Computer Vision: A comparative analysis on surgical phase recognition capabilities](https://arxiv.org/abs/2504.18954)
*Marco Mezzina, Pieter De Backer, Tom Vercauteren, Matthew Blaschko, Alexandre Mottrie, Tinne Tuytelaars*

Main category: eess.IV

TL;DR: Automated Surgical Phase Recognition (SPR) improves with temporal context and visual landmarks, matching expert surgeons' performance when AI includes temporal data.


<details>
  <summary>Details</summary>
Motivation: To explore if temporal context enhances SPR accuracy in non-linear procedures like RAPN, addressing gaps in prior research focused on linear surgeries.

Method: Urologists classified RAPN phases on frames/video snippets; AI models (with/without temporal context) were trained on RAPN and benchmarked on Cholec80.

Result: Video snippets and landmarks boosted accuracy; surgeons outperformed novices. AI matched surgeons when temporal context was included.

Conclusion: SPR is complex but improves with temporal context. Key landmarks (tools/organs) are crucial for humans and future AI in SPR.

Abstract: Purpose: Automated Surgical Phase Recognition (SPR) uses Artificial
Intelligence (AI) to segment the surgical workflow into its key events,
functioning as a building block for efficient video review, surgical education
as well as skill assessment. Previous research has focused on short and linear
surgical procedures and has not explored if temporal context influences
experts' ability to better classify surgical phases. This research addresses
these gaps, focusing on Robot-Assisted Partial Nephrectomy (RAPN) as a highly
non-linear procedure. Methods: Urologists of varying expertise were grouped and
tasked to indicate the surgical phase for RAPN on both single frames and video
snippets using a custom-made web platform. Participants reported their
confidence levels and the visual landmarks used in their decision-making. AI
architectures without and with temporal context as trained and benchmarked on
the Cholec80 dataset were subsequently trained on this RAPN dataset. Results:
Video snippets and presence of specific visual landmarks improved phase
classification accuracy across all groups. Surgeons displayed high confidence
in their classifications and outperformed novices, who struggled discriminating
phases. The performance of the AI models is comparable to the surgeons in the
survey, with improvements when temporal context was incorporated in both cases.
Conclusion: SPR is an inherently complex task for expert surgeons and computer
vision, where both perform equally well when given the same context.
Performance increases when temporal information is provided. Surgical tools and
organs form the key landmarks for human interpretation and are expected to
shape the future of automated SPR.

</details>


### [592] [MLICv2: Enhanced Multi-Reference Entropy Modeling for Learned Image Compression](https://arxiv.org/abs/2504.19119)
*Wei Jiang, Yongqi Zhai, Jiayu Yang, Feng Gao, Ronggang Wang*

Main category: eess.IV

TL;DR: MLICv2 and MLICv2+ improve learned image compression with enhanced transform, entropy modeling, and instance adaptability, outperforming VVC Intra significantly.


<details>
  <summary>Details</summary>
Motivation: To address performance degradation at high bit-rates and improve global context capture in learned image compression.

Method: Introduces token mixing transform, hyperprior-guided global correlation prediction, channel reweighting, and stochastic Gumbel annealing.

Result: Reduces BD-rate by 16.54%-24.35% compared to VTM-17.0 Intra on benchmark datasets.

Conclusion: MLICv2 and MLICv2+ achieve state-of-the-art performance in learned image compression.

Abstract: Recent advancements in learned image compression (LIC) have yielded
impressive performance gains. Notably, the learned image compression models
with multi-reference entropy models (MLIC series) have significantly
outperformed existing traditional image codecs such as the Versatile Video
Coding (VVC) Intra. In this paper, we present MLICv2 and MLICv2$^+$, enhanced
versions of the MLIC series, featuring improved transform techniques, entropy
modeling, and instance adaptability. For better transform, we introduce a
simple token mixing transform block inspired by the meta transformer
architecture, addressing the performance degradation at high bit-rates observed
in previous MLIC series while maintaining computational efficiency. To enhance
entropy modeling, we propose a hyperprior-guided global correlation prediction,
enabling the capture of global contexts in the initial slice of the latent
representation. We also develop a channel reweighting module to dynamically
prioritize important channels within each context. Additionally, advanced
positional embedding for context modeling and selective compression with guided
optimization are investigated. To boost instance adaptability, we employ
stochastic Gumbel annealing to iteratively refine the latent representation
according to the rate-distortion optimization of a specific input image. This
approach further enhances performance without impacting decoding speed.
Experimental results demonstrate that our MLICv2 and MLICv2$^+$ achieve
state-of-the-art performance, reducing Bjontegaard-Delta rate (BD-rate) by
16.54%, 21.61%, 16.05% and 20.46%, 24.35%, 19.14% respectively, compared to
VTM-17.0 Intra on the Kodak, Tecnick, CLIC Pro Val dataset, respectively.

</details>


### [593] [Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction](https://arxiv.org/abs/2504.19203)
*Ehsan Karami, Hamid Soltanian-Zadeh*

Main category: eess.IV

TL;DR: The paper improves deep learning model generalization for KOA prediction by replacing batch normalization with instance normalization, using data augmentation, and applying contrastive loss.


<details>
  <summary>Details</summary>
Motivation: MRI-based deep learning models for KOA prediction lack generalizability across different imaging data sources.

Method: Replaced batch normalization with instance normalization, used data augmentation, and applied contrastive loss. Evaluated on OAI database MRI data (FS-IW-TSE and DESS images).

Result: Statistically significant improvement in classification accuracy across domains, outperforming the baseline model.

Conclusion: The proposed enhancements improve model generalization for KOA prediction across diverse imaging data.

Abstract: Knee osteoarthritis (KOA) is a common joint disease that causes pain and
mobility issues. While MRI-based deep learning models have demonstrated
superior performance in predicting total knee replacement (TKR) and disease
progression, their generalizability remains challenging, particularly when
applied to imaging data from different sources. In this study, we have shown
that replacing batch normalization with instance normalization, using data
augmentation, and applying contrastive loss improves model generalization in a
baseline deep learning model for knee osteoarthritis (KOA) prediction. We
trained and evaluated our model using MRI data from the Osteoarthritis
Initiative (OAI) database, considering sagittal fat-suppressed
intermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain
and sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state
(DESS) images as the target domain. The results demonstrate a statistically
significant improvement in classification accuracy across both domains, with
our approach outperforming the baseline model.

</details>


### [594] [Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading](https://arxiv.org/abs/2504.19362)
*Yunxuan Wang, Ray Yin, Yumei Tan, Hao Chen, Haiying Xia*

Main category: eess.IV

TL;DR: A novel framework, LoASP, enhances domain generalization for diabetic retinopathy grading by incorporating structural priors, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing domain generalization methods for diabetic retinopathy grading overlook lesion-specific features, leading to insufficient accuracy.

Method: Proposes Low-rank Adaptive Structural Priors (LoASP), a plug-and-play framework that integrates with existing DG models to learn adaptive structural representations.

Result: Validated on eight datasets, LoASP improves generalization in single-source and multi-source scenarios, with visualizations confirming alignment with vessel and lesion structures.

Conclusion: LoASP effectively addresses domain shifts in DR grading by leveraging structural priors, offering both performance gains and interpretability.

Abstract: Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one
of the primary causes of vision loss among retinal vascular diseases. Deep
learning methods have been extensively applied in the grading of diabetic
retinopathy (DR). However, their performance declines significantly when
applied to data outside the training distribution due to domain shifts. Domain
generalization (DG) has emerged as a solution to this challenge. However, most
existing DG methods overlook lesion-specific features, resulting in
insufficient accuracy. In this paper, we propose a novel approach that enhances
existing DG methods by incorporating structural priors, inspired by the
observation that DR grading is heavily dependent on vessel and lesion
structures. We introduce Low-rank Adaptive Structural Priors (LoASP), a
plug-and-play framework designed for seamless integration with existing DG
models. LoASP improves generalization by learning adaptive structural
representations that are finely tuned to the complexities of DR diagnosis.
Extensive experiments on eight diverse datasets validate its effectiveness in
both single-source and multi-source domain scenarios. Furthermore,
visualizations reveal that the learned structural priors intuitively align with
the intricate architecture of the vessels and lesions, providing compelling
insights into their interpretability and diagnostic relevance.

</details>


### [595] [Dual Attention Driven Lumbar Magnetic Resonance Image Feature Enhancement and Automatic Diagnosis of Herniation](https://arxiv.org/abs/2504.19438)
*Lingrui Zhang, Liang Guo, Xiao An, Feng Lin, Binlong Zheng, Jiankun Wang, Zhirui Li*

Main category: eess.IV

TL;DR: An automated framework for classifying lumbar disc herniation (LDH) using MRI images achieves high accuracy and AUC-ROC, aiding physicians in faster and more confident diagnoses.


<details>
  <summary>Details</summary>
Motivation: LDH diagnosis relies on radiologists' expertise, causing delays and high costs. Automating this process can improve efficiency and accessibility.

Method: The framework uses T1/T2-weighted MRI images from 205 patients, employing data augmentation and attention mechanisms for feature extraction and standardized outputs.

Result: Achieves an AUC-ROC of 0.969 and accuracy of 0.9486, demonstrating high performance with minimal training data.

Conclusion: The framework enhances LDH detection in primary hospitals, offering a scalable and efficient diagnostic solution.

Abstract: Lumbar disc herniation (LDH) is a common musculoskeletal disease that
requires magnetic resonance imaging (MRI) for effective clinical management.
However, the interpretation of MRI images heavily relies on the expertise of
radiologists, leading to delayed diagnosis and high costs for training
physicians. Therefore, this paper proposes an innovative automated LDH
classification framework. To address these key issues, the framework utilizes
T1-weighted and T2-weighted MRI images from 205 people. The framework extracts
clinically actionable LDH features and generates standardized diagnostic
outputs by leveraging data augmentation and channel and spatial attention
mechanisms. These outputs can help physicians make confident and time-effective
care decisions when needed. The proposed framework achieves an area under the
receiver operating characteristic curve (AUC-ROC) of 0.969 and an accuracy of
0.9486 for LDH detection. The experimental results demonstrate the performance
of the proposed framework. Our framework only requires a small number of
datasets for training to demonstrate high diagnostic accuracy. This is expected
to be a solution to enhance the LDH detection capabilities of primary
hospitals.

</details>


### [596] [Accelerated 3D-3D rigid registration of echocardiographic images obtained from apical window using particle filter](https://arxiv.org/abs/2504.19930)
*Thanuja Uruththirakodeeswaran, Harald Becher, Michelle Noga, Lawrence H. Le, Pierre Boulanger, Jonathan Windram, Kumaradevan Punithakumar*

Main category: eess.IV

TL;DR: The paper proposes an accelerated SMC algorithm for 3D-3D rigid registration of echocardiographic images, improving alignment and speed.


<details>
  <summary>Details</summary>
Motivation: To enhance image quality and field of view in 3D echocardiography by addressing noise and intensity variations.

Method: Uses an accelerated SMC algorithm for rigid registration, tested with image-based and mask-based approaches on 4D sequences from 7 volunteers.

Result: Mask-based SMC achieved a Dice score of 0.819 +/- 0.045 for the left ventricle and 16.7x speedup over CPU SMC.

Conclusion: The accelerated SMC algorithm is effective for robust and fast 3D-3D registration of echocardiographic images.

Abstract: The perfect alignment of 3D echocardiographic images captured from various
angles has improved image quality and broadened the field of view. This study
proposes an accelerated sequential Monte Carlo (SMC) algorithm for 3D-3D rigid
registration of transthoracic echocardiographic images with significant and
limited overlap taken from apical window that is robust to the noise and
intensity variation in ultrasound images. The algorithm estimates the
translational and rotational components of the rigid transform through an
iterative process and requires an initial approximation of the rotation and
translation limits. We perform registration in two ways: the image-based
registration computes the transform to align the end-diastolic frame of the
apical nonstandard image to the apical standard image and applies the same
transform to all frames of the cardiac cycle, whereas the mask-based
registration approach uses the binary masks of the left ventricle in the same
way. The SMC and exhaustive search (EX) algorithms were evaluated for 4D
temporal sequences recorded from 7 volunteers who participated in a study
conducted at the Mazankowski Alberta Heart Institute. The evaluations
demonstrate that the mask-based approach of the accelerated SMC yielded a Dice
score value of 0.819 +/- 0.045 for the left ventricle and gained 16.7x speedup
compared to the CPU version of the SMC algorithm.

</details>


### [597] [SST-DUNet: Automated preclinical functional MRI skull stripping using Smart Swin Transformer and Dense UNet](https://arxiv.org/abs/2504.19937)
*Sima Soltanpour, Rachel Utama, Arnold Chang, Md Taufiq Nasseef, Dan Madularu, Praveen Kulkarni, Craig Ferris, Chris Joslin*

Main category: eess.IV

TL;DR: SST-DUNet, a novel method combining dense UNet and Smart Swin Transformer, automates skull stripping in rat fMRI with high accuracy, outperforming manual methods.


<details>
  <summary>Details</summary>
Motivation: Manual skull stripping in fMRI is time-consuming and operator-dependent, with existing automated methods struggling with low resolution and variable slice sizes in preclinical data.

Method: Proposes SST-DUNet, integrating dense UNet with Smart Swin Transformer (SST), using SSW-MSA for feature extraction and a combined Focal-Dice loss to address class imbalance.

Result: Achieves Dice scores of 98.65%, 97.86%, and 98.04% on rat fMRI datasets, closely matching manual skull stripping results.

Conclusion: SST-DUNet effectively replaces manual skull stripping in rat fMRI, handling challenges like low resolution and variable slice sizes.

Abstract: Skull stripping is a common preprocessing step that is often performed
manually in Magnetic Resonance Imaging (MRI) pipelines, including functional
MRI (fMRI). This manual process is time-consuming and operator dependent.
Automating this process is challenging for preclinical data due to variations
in brain geometry, resolution, and tissue contrast. While existing methods for
MRI skull stripping exist, they often struggle with the low resolution and
varying slice sizes in preclinical fMRI data. This study proposes a novel
method called SST-DUNet, that integrates a dense UNet-based architecture with a
feature extractor based on Smart Swin Transformer (SST) for fMRI skull
stripping. The Smart Shifted Window Multi-Head Self-Attention (SSW-MSA) module
in SST is adapted to replace the mask-based module in the Swin Transformer
(ST), enabling the learning of distinct channel-wise features while focusing on
relevant dependencies within brain structures. This modification allows the
model to better handle the complexities of fMRI skull stripping, such as low
resolution and variable slice sizes. To address the issue of class imbalance in
preclinical data, a combined loss function using Focal and Dice loss is
utilized. The model was trained on rat fMRI images and evaluated across three
in-house datasets with a Dice similarity score of 98.65%, 97.86%, and 98.04%.
The fMRI results obtained through automatic skull stripping using the SST-DUNet
model closely align with those from manual skull stripping for both seed-based
and independent component analyses. These results indicate that the SST-DUNet
can effectively substitute manual brain extraction in rat fMRI analysis.

</details>


### [598] [An $\ell^1$-Plug-and-Play Approach for MPI Using a Zero Shot Denoiser with Evaluation on the 3D Open MPI Dataset](https://arxiv.org/abs/2401.00275)
*Vladyslav Gapyak, Corinna Rentschler, Thomas März, Andreas Weinmann*

Main category: eess.IV

TL;DR: A zero-shot plug-and-play denoiser with an ℓ¹-prior is proposed for MPI reconstruction, avoiding training costs and showing competitive results.


<details>
  <summary>Details</summary>
Motivation: MPI's high temporal resolution and non-ionizing nature make it promising, but its ill-posed reconstruction problem requires advanced regularization methods.

Method: A zero-shot denoiser with ℓ¹-prior is used for reconstruction, validated on hybrid and 3D Open MPI datasets against Tikhonov, DIP, and PP-MPI baselines.

Result: Quantitative and qualitative evaluations show the method's effectiveness, even with varying preprocessing levels.

Conclusion: The zero-shot approach eliminates training costs and is adaptable for future MPI applications.

Abstract: Objective: Magnetic particle imaging (MPI) is an emerging medical imaging
modality which has gained increasing interest in recent years. Among the
benefits of MPI are its high temporal resolution, and that the technique does
not expose the specimen to any kind of ionizing radiation. It is based on the
non-linear response of magnetic nanoparticles to an applied magnetic field.
From the electric signal measured in receive coils, the particle concentration
has to be reconstructed. Due to the ill-posedness of the reconstruction
problem, various regularization methods have been proposed for reconstruction
ranging from early stopping methods, via classical Tikhonov regularization and
iterative methods to modern machine learning approaches. In this work, we
contribute to the latter class: we propose a plug-and-play approach based on a
generic zero-shot denoiser with an $\ell^1$-prior.
  Approach: We validate the reconstruction parameters of the method on a hybrid
dataset and compare it with the baseline Tikhonov, DIP and the previous PP-MPI,
which is a plug-and-play method with denoiser trained on MPI-friendly data.
  Main results: We offer a quantitative and qualitative evaluation of the
zero-shot plug-and-play approach on the 3D Open MPI dataset. Moreover, we show
the quality of the approach with different levels of preprocessing of the data.
  Significance: The proposed method employs a zero-shot denoiser which has not
been trained for the MPI task and therefore saves the cost for training.
Moreover, it offers a method that can be potentially applied in future MPI
contexts.

</details>


### [599] [Devil is in Details: Locality-Aware 3D Abdominal CT Volume Generation for Self-Supervised Organ Segmentation](https://arxiv.org/abs/2409.20332)
*Yuran Wang, Zhijing Wan, Yansheng Qiu, Zheng Wang*

Main category: eess.IV

TL;DR: The paper introduces Locality-Aware Diffusion (Lad), a method for generating high-quality 3D abdominal CT volumes to address data scarcity in self-supervised learning (SSL) for medical image analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of training data scarcity in medical image analysis due to resource and privacy constraints, especially for complex abdominal CT volumes.

Method: Proposes Lad with a locality loss to refine anatomical regions and a condition extractor to integrate abdominal priori, enabling high-fidelity 3D CT volume generation without additional labeled data.

Result: Achieves significant fidelity improvements (FID score drop from 0.0034 to 0.0002) and enhances SSL organ segmentation performance (higher mean Dice scores).

Conclusion: Synthetic data generated by Lad can effectively advance SSL in medical image analysis, particularly for abdominal CT volumes.

Abstract: In the realm of medical image analysis, self-supervised learning (SSL)
techniques have emerged to alleviate labeling demands, while still facing the
challenge of training data scarcity owing to escalating resource requirements
and privacy constraints. Numerous efforts employ generative models to generate
high-fidelity, unlabeled 3D volumes across diverse modalities and anatomical
regions. However, the intricate and indistinguishable anatomical structures
within the abdomen pose a unique challenge to abdominal CT volume generation
compared to other anatomical regions. To address the overlooked challenge, we
introduce the Locality-Aware Diffusion (Lad), a novel method tailored for
exquisite 3D abdominal CT volume generation. We design a locality loss to
refine crucial anatomical regions and devise a condition extractor to integrate
abdominal priori into generation, thereby enabling the generation of large
quantities of high-quality abdominal CT volumes essential for SSL tasks without
the need for additional data such as labels or radiology reports. Volumes
generated through our method demonstrate remarkable fidelity in reproducing
abdominal structures, achieving a decrease in FID score from 0.0034 to 0.0002
on AbdomenCT-1K dataset, closely mirroring authentic data and surpassing
current methods. Extensive experiments demonstrate the effectiveness of our
method in self-supervised organ segmentation tasks, resulting in an improvement
in mean Dice scores on two abdominal datasets effectively. These results
underscore the potential of synthetic data to advance self-supervised learning
in medical image analysis.

</details>


### [600] [Learning Modality-Aware Representations: Adaptive Group-wise Interaction Network for Multimodal MRI Synthesis](https://arxiv.org/abs/2411.14684)
*Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Linda Wei, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang*

Main category: eess.IV

TL;DR: AGI-Net improves multimodal MR image synthesis by modeling inter- and intra-modality relationships using adaptive group-wise interactions and cross-group attention.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat modalities as input channels, leading to sub-optimal results due to poor feature alignment.

Method: AGI-Net partitions features into groups, uses adaptive rolling kernels for alignment, and employs cross-group attention for fusion.

Result: AGI-Net achieves state-of-the-art performance on IXI and BraTS2023 datasets.

Conclusion: AGI-Net's modality-aware design effectively enhances multimodal MR image synthesis.

Abstract: Multimodal MR image synthesis aims to generate missing modality images by
effectively fusing and mapping from a subset of available MRI modalities. Most
existing methods adopt an image-to-image translation paradigm, treating
multiple modalities as input channels. However, these approaches often yield
sub-optimal results due to the inherent difficulty in achieving precise
feature- or semantic-level alignment across modalities. To address these
challenges, we propose an Adaptive Group-wise Interaction Network (AGI-Net)
that explicitly models both inter-modality and intra-modality relationships for
multimodal MR image synthesis. Specifically, feature channels are first
partitioned into predefined groups, after which an adaptive rolling mechanism
is applied to conventional convolutional kernels to better capture feature and
semantic correspondences between different modalities. In parallel, a
cross-group attention module is introduced to enable effective feature fusion
across groups, thereby enhancing the network's representational capacity. We
validate the proposed AGI-Net on the publicly available IXI and BraTS2023
datasets. Experimental results demonstrate that AGI-Net achieves
state-of-the-art performance in multimodal MR image synthesis tasks, confirming
the effectiveness of its modality-aware interaction design. We release the
relevant code at:
https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git.

</details>


### [601] [Self-Consistent Nested Diffusion Bridge for Accelerated MRI Reconstruction](https://arxiv.org/abs/2412.09998)
*Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Guoting Luo, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang*

Main category: eess.IV

TL;DR: The paper proposes a Self-Consistent Nested Diffusion Bridge (SC-NDB) framework for accelerated MRI reconstruction using magnitude images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MRI reconstruction methods often rely on inaccessible complex-valued data, leaving only magnitude images available in clinical practice. This gap motivates the focus on magnitude-image-based reconstruction.

Method: The SC-NDB framework models MRI reconstruction as a bi-directional image translation process, incorporating nested diffusion with self-consistency constraints and a Contour Decomposition Embedding Module (CDEM) for structural knowledge.

Result: SC-NDB achieves state-of-the-art performance on fastMRI and IXI datasets, surpassing both magnitude-based and non-magnitude-based diffusion models.

Conclusion: The SC-NDB framework is effective and clinically relevant for MRI reconstruction, addressing the limitations of existing methods.

Abstract: Accelerated MRI reconstruction plays a vital role in reducing scan time while
preserving image quality. While most existing methods rely on complex-valued
image-space or k-space data, these formats are often inaccessible in clinical
practice due to proprietary reconstruction pipelines, leaving only magnitude
images stored in DICOM files. To address this gap, we focus on the
underexplored task of magnitude-image-based MRI reconstruction. Recent
advancements in diffusion models, particularly denoising diffusion
probabilistic models (DDPMs), have demonstrated strong capabilities in modeling
image priors. However, their task-agnostic denoising nature limits performance
in source-to-target image translation tasks, such as MRI reconstruction. In
this work, we propose a novel Self-Consistent Nested Diffusion Bridge (SC-NDB)
framework that models accelerated MRI reconstruction as a bi-directional image
translation process between under-sampled and fully-sampled magnitude MRI
images. SC-NDB introduces a nested diffusion architecture with a
self-consistency constraint and reverse bridge diffusion pathways to improve
intermediate prediction fidelity and better capture the explicit priors of
source images. Furthermore, we incorporate a Contour Decomposition Embedding
Module (CDEM) to inject structural and textural knowledge by leveraging
Laplacian pyramids and directional filter banks. Extensive experiments on the
fastMRI and IXI datasets demonstrate that our method achieves state-of-the-art
performance compared to both magnitude-based and non-magnitude-based diffusion
models, confirming the effectiveness and clinical relevance of SC-NDB.

</details>


### [602] [Investigating the Feasibility of Patch-based Inference for Generalized Diffusion Priors in Inverse Problems for Medical Images](https://arxiv.org/abs/2501.15309)
*Saikat Roy, Mahmoud Mostapha, Radu Miron, Matt Holbrook, Mariappan Nadar*

Main category: eess.IV

TL;DR: The paper explores patch-based training and inference for diffusion priors in MRI image restoration, focusing on efficiency, artifact avoidance, and adaptability.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency and memory usage in diffusion-based plug-and-play methods for MRI image tasks by leveraging patch-based approaches.

Method: Investigates patch-based training and inference for diffusion priors, evaluating adaptations for artifact avoidance, performance, and memory efficiency across tasks and datasets.

Result: Demonstrates the feasibility of patch-based methods for MRI images, highlighting their efficiency and adaptability in plug-and-play frameworks.

Conclusion: Patch-based diffusion priors are viable for MRI image tasks, offering computational benefits without compromising performance.

Abstract: Plug-and-play approaches to solving inverse problems such as restoration and
super-resolution have recently benefited from Diffusion-based generative priors
for natural as well as medical images. However, solutions often use the
standard albeit computationally intensive route of training and inferring with
the whole image on the diffusion prior. While patch-based approaches to
evaluating diffusion priors in plug-and-play methods have received some
interest, they remain an open area of study. In this work, we explore the
feasibility of the usage of patches for training and inference of a diffusion
prior on MRI images. We explore the minor adaptation necessary for artifact
avoidance, the performance and the efficiency of memory usage of patch-based
methods as well as the adaptability of whole image training to patch-based
evaluation - evaluating across multiple plug-and-play methods, tasks and
datasets.

</details>


### [603] [Physics-Driven Neural Compensation For Electrical Impedance Tomography](https://arxiv.org/abs/2504.18067)
*Chuyu Wang, Huiting Deng, Dong Liu*

Main category: eess.IV

TL;DR: PhyNC, an unsupervised deep learning framework, improves EIT by addressing ill-posedness and sensitivity variability using physics-driven neural compensation.


<details>
  <summary>Details</summary>
Motivation: EIT faces challenges like ill-posed inverse problems and sensitivity variability, which traditional and deep learning methods inadequately address.

Method: PhyNC dynamically allocates neural capacity to low-sensitivity regions, incorporating EIT's physical principles for accurate reconstructions.

Result: PhyNC outperforms existing methods in detail preservation and artifact resistance, especially in low-sensitivity areas.

Conclusion: PhyNC enhances EIT robustness and offers a flexible framework adaptable to other imaging modalities.

Abstract: Electrical Impedance Tomography (EIT) provides a non-invasive, portable
imaging modality with significant potential in medical and industrial
applications. Despite its advantages, EIT encounters two primary challenges:
the ill-posed nature of its inverse problem and the spatially variable,
location-dependent sensitivity distribution. Traditional model-based methods
mitigate ill-posedness through regularization but overlook sensitivity
variability, while supervised deep learning approaches require extensive
training data and lack generalization. Recent developments in neural fields
have introduced implicit regularization techniques for image reconstruction,
but these methods typically neglect the physical principles underlying EIT,
thus limiting their effectiveness. In this study, we propose PhyNC
(Physics-driven Neural Compensation), an unsupervised deep learning framework
that incorporates the physical principles of EIT. PhyNC addresses both the
ill-posed inverse problem and the sensitivity distribution by dynamically
allocating neural representational capacity to regions with lower sensitivity,
ensuring accurate and balanced conductivity reconstructions. Extensive
evaluations on both simulated and experimental data demonstrate that PhyNC
outperforms existing methods in terms of detail preservation and artifact
resistance, particularly in low-sensitivity regions. Our approach enhances the
robustness of EIT reconstructions and provides a flexible framework that can be
adapted to other imaging modalities with similar challenges.

</details>
