<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.CV](#cs.CV) [Total: 103]
- [cs.AI](#cs.AI) [Total: 35]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.LG](#cs.LG) [Total: 96]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 0]
- [eess.AS](#eess.AS) [Total: 3]
- [eess.IV](#eess.IV) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)](https://arxiv.org/abs/2504.15349)
*William Bruns*

Main category: cs.CL

TL;DR: The paper demonstrates that a Transformer encoder-decoder, using RASP, can achieve 100% semantic exact match on ReCOGS_pos, proving it doesn't require hierarchical solutions.


<details>
  <summary>Details</summary>
Motivation: To address the 0% accuracy of Transformer models on certain structural generalizations in COGS, showing systematic and compositional capabilities.

Method: Uses RASP to construct a Transformer-equivalent model with flat pattern-matching rules, handling prepositional phrases and sentential complements without recursion.

Result: Achieves 100% semantic exact match on ReCOGS_pos, except for one split (92%), and handles recursion via decoder loop.

Conclusion: Transformers can perform compositional generalization without hierarchical rules, as shown by the RASP model's success.

Abstract: Humans understand new combinations of words encountered if they are
combinations of words recognized from different contexts, an ability called
Compositional Generalization. The COGS benchmark (Kim and Linzen, 2020)
arXiv:2010.05465 reports 0% accuracy for Transformer models on some structural
generalizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted
Access Sequence Processing (RASP), a Transformer-equivalent programming
language, to prove by construction that a Transformer encoder-decoder can
perform the semantically equivalent ReCOGS_pos (Wu et al., 2024)
arXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP
model attains 100% semantic exact match on the ReCOGS test set and 100% SEM on
all generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore,
our RASP model shows the ReCOGS_pos task does not require a hierarchical or
tree-structured solution: we use word-level tokens with an "embedding" layer
that tags with possible parts of speech, applying just once per encoder pass 19
attention-head compatible flat pattern-matching rules, shown using grammar
coverage (Zeller et al., 2023) to be learnable from the training data, plus
general prepositional phrase (pp) handling and sentential complement (cp)
handling logic, and output the next logical form (LF) token (repeating until
the LF is complete). The model does not apply recursive, tree-structured rules
like 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact
match on pp recursion, cp recursion using the decoder loop.

</details>


### [2] [Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection](https://arxiv.org/abs/2504.15392)
*Myrthe Reuver,Indira Sen,Matteo Melis,Gabriella Lapesa*

Main category: cs.CL

TL;DR: The paper explores hybrid intelligence by collaborating sexism researchers with LLMs, testing their knowledge and co-creating definitions of sexism, then evaluating classification performance.


<details>
  <summary>Details</summary>
Motivation: To investigate how human experts and LLMs can collaborate to improve understanding and detection of sexism.

Method: A four-component pipeline: expert surveys, two interactive experiments with LLMs (knowledge assessment and definition creation), and zero-shot classification tests on sexism benchmarks.

Result: LLM-generated definitions were longer and more complex, outperforming expert-written ones in classification. Some co-created definitions improved performance, even for LLM-inexperienced experts.

Conclusion: Hybrid intelligence can enhance sexism detection, with LLMs aiding experts, though individual expertise still plays a role.

Abstract: This paper investigates hybrid intelligence and collaboration between
researchers of sexism and Large Language Models (LLMs), with a four-component
pipeline. First, nine sexism researchers answer questions about their knowledge
of sexism and of LLMs. They then participate in two interactive experiments
involving an LLM (GPT3.5). The first experiment has experts assessing the
model's knowledge about sexism and suitability for use in research. The second
experiment tasks them with creating three different definitions of sexism: an
expert-written definition, an LLM-written one, and a co-created definition.
Lastly, zero-shot classification experiments use the three definitions from
each expert in a prompt template for sexism detection, evaluating GPT4o on
2.500 texts sampled from five sexism benchmarks. We then analyze the resulting
67.500 classification decisions. The LLM interactions lead to longer and more
complex definitions of sexism. Expert-written definitions on average perform
poorly compared to LLM-generated definitions. However, some experts do improve
classification performance with their co-created definitions of sexism, also
experts who are inexperienced in using LLMs.

</details>


### [3] [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431)
*Sungjun Han,Juyoung Suk,Suyeong An,Hyungguk Kim,Kyuseok Kim,Wonsuk Yang,Seungtaek Choi,Jamin Shin*

Main category: cs.CL

TL;DR: Trillion-7B is a token-efficient Korean-centric multilingual LLM using XLDA for cross-lingual knowledge transfer, achieving strong performance with minimal multilingual data and training costs.


<details>
  <summary>Details</summary>
Motivation: To create a highly efficient multilingual LLM focused on Korean, leveraging cross-lingual knowledge transfer while minimizing resource usage.

Method: Uses Cross-lingual Document Attention (XLDA), optimized data mixtures, language-specific filtering, and tailored tokenizer construction.

Result: Competitive performance across 27 benchmarks in four languages, with only 10% multilingual data and low training costs ($148K).

Conclusion: Trillion-7B is a robust, efficient multilingual model with exceptional cross-lingual consistency.

Abstract: We introduce Trillion-7B, the most token-efficient Korean-centric
multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)
mechanism enables highly efficient and effective knowledge transfer from
English to target languages like Korean and Japanese. Combined with optimized
data mixtures, language-specific filtering, and tailored tokenizer
construction, Trillion-7B achieves competitive performance while dedicating
only 10\% of its 2T training tokens to multilingual data and requiring just
59.4K H100 GPU hours (\$148K) for full training. Comprehensive evaluations
across 27 benchmarks in four languages demonstrate Trillion-7B's robust
multilingual performance and exceptional cross-lingual consistency.

</details>


### [4] [Feeding LLM Annotations to BERT Classifiers at Your Own Risk](https://arxiv.org/abs/2504.15432)
*Yucheng Lu,Kazimier Smith*

Main category: cs.CL

TL;DR: Using LLM-generated labels to fine-tune smaller models for text classification shows performance degradation, instability, and premature plateaus compared to gold labels, raising reliability concerns. Mitigation strategies like entropy filtering help but don't eliminate risks.


<details>
  <summary>Details</summary>
Motivation: To investigate the reliability of using LLM-generated labels for fine-tuning smaller models in text classification, especially in high-stakes applications.

Method: Empirical analysis comparing models trained on LLM-generated labels versus gold labels, measuring accuracy, F1 score, stability, and performance plateaus.

Result: Degraded performance, increased instability, and premature plateaus in models trained on synthetic data. Mitigation strategies like entropy-based filtering and ensembles offer partial relief.

Conclusion: Caution is needed when using LLM-generated labels for high-stakes tasks, as inherent risks of error propagation remain unresolved.

Abstract: Using LLM-generated labels to fine-tune smaller encoder-only models for text
classification has gained popularity in various settings. While this approach
may be justified in simple and low-stakes applications, we conduct empirical
analysis to demonstrate how the perennial curse of training on synthetic data
manifests itself in this specific setup. Compared to models trained on gold
labels, we observe not only the expected performance degradation in accuracy
and F1 score, but also increased instability across training runs and premature
performance plateaus. These findings cast doubts on the reliability of such
approaches in real-world applications. We contextualize the observed phenomena
through the lens of error propagation and offer several practical mitigation
strategies, including entropy-based filtering and ensemble techniques. Although
these heuristics offer partial relief, they do not fully resolve the inherent
risks of propagating non-random errors from LLM annotations to smaller
classifiers, underscoring the need for caution when applying this workflow in
high-stakes text classification tasks.

</details>


### [5] [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)
*Tyler A. Chang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: Bigram subnetworks in Transformer models are minimal, critical for performance, and concentrated in the first MLP layer, aiding next token predictions.


<details>
  <summary>Details</summary>
Motivation: To isolate and understand the minimal transformation from current token embeddings to next token predictions in language models.

Method: Identify and analyze bigram subnetworks in fully trained language models up to 1B parameters.

Result: Bigram subnetworks are critical for performance, concentrated in the first MLP layer, and overlap with optimal pruning subnetworks.

Conclusion: Bigram subnetworks are necessary and sufficient for basic next token predictions, providing a foundation for studying model circuits.

Abstract: In Transformer language models, activation vectors transform from current
token embeddings to next token predictions as they pass through the model. To
isolate a minimal form of this transformation, we identify language model
subnetworks that make bigram predictions, naive next token predictions based
only on the current token. We find that bigram subnetworks can be found in
fully trained language models up to 1B parameters, and these subnetworks are
critical for model performance even when they consist of less than 0.2% of
model parameters. Bigram subnetworks are concentrated in the first Transformer
MLP layer, and they overlap significantly with subnetworks trained to optimally
prune a given model. Mechanistically, the bigram subnetworks often recreate a
pattern from the full models where the first layer induces a sharp change that
aligns activations with next token predictions rather than current token
representations. Our results demonstrate that bigram subnetworks comprise a
minimal subset of parameters that are both necessary and sufficient for basic
next token predictions in language models, and they help drive the
transformation from current to next token activations in the residual stream.
These subnetworks can lay a foundation for studying language model circuits by
building up from a minimal circuit rather than the traditional approach of
ablating circuits from a full model.

</details>


### [6] [SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation](https://arxiv.org/abs/2504.15509)
*Keqi Deng,Wenxi Chen,Xie Chen,Philip C. Woodland*

Main category: cs.CL

TL;DR: SimulS2S-LLM enables simultaneous speech-to-speech translation using LLMs by training offline and guiding inference with boundary-aware prompts, improving quality-latency trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of streaming speech translation with LLMs, which struggle with prepended speech prompts during generation.

Method: Proposes SimulS2S-LLM, which trains speech LLMs offline, uses boundary-aware prompts for alignment, and employs incremental beam search for token prediction.

Result: Achieves better translation quality-latency trade-offs, improving ASR-BLEU scores by 3 points at similar latency.

Conclusion: SimulS2S-LLM effectively bridges the gap between training and inference for simultaneous speech translation, outperforming existing methods.

Abstract: Simultaneous speech translation (SST) outputs translations in parallel with
streaming speech input, balancing translation quality and latency. While large
language models (LLMs) have been extended to handle the speech modality,
streaming remains challenging as speech is prepended as a prompt for the entire
generation process. To unlock LLM streaming capability, this paper proposes
SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy
to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between
training and inference by extracting boundary-aware speech prompts that allows
it to be better matched with text input data. SimulS2S-LLM achieves
simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete
output speech tokens and then synthesising output speech using a pre-trained
vocoder. An incremental beam search is designed to expand the search space of
speech token prediction without increasing latency. Experiments on the CVSS
speech data show that SimulS2S-LLM offers a better translation quality-latency
trade-off than existing methods that use the same training data, such as
improving ASR-BLEU scores by 3 points at similar latency.

</details>


### [7] [Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)
*Szymon Kobus,Deniz Gündüz*

Main category: cs.CL

TL;DR: Speculative decoding connects to channel simulation for speed-up analysis, proposing ERSD for state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To analyze and improve the speed-up of speculative decoding in large language model inference.

Method: Information-theoretic analysis linking speculative decoding to channel simulation, and proposing ERSD.

Result: Derived an explicit relation between speed-up and draft tokens, with ERSD matching top performance.

Conclusion: Speculative decoding's speed-up is theoretically bounded, and ERSD offers competitive performance.

Abstract: Speculative decoding accelerates large language model inference using a
smaller draft model. In this paper, we establish a surprising connection
between speculative decoding and channel simulation, which aims at simulating a
noisy channel using as few bits as possible. This connection allows us to
provide an information-theoretic analysis of the speed up that can be achieved
by speculative decoding. Leveraging this link, we derive an explicit relation
between generation speed-up and the number of tokens $k$ generated by the draft
model for large $k$, which serves as an upper bound for all $k$. We also
propose a novel speculative decoding method via exponential race ERSD that
matches state-of-the-art performance.

</details>


### [8] [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521)
*Minghao Wu,Weixuan Wang,Sinuo Liu,Huifeng Yin,Xintong Wang,Yu Zhao,Chenyang Lyu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: The paper critiques multilingual benchmarking practices, revealing English overrepresentation and the inadequacy of translations, advocating for culturally tailored benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address inequities in multilingual evaluation and promote equitable progress in large language models (LLMs).

Method: Analysis of over 2,000 multilingual benchmarks from 148 countries (2021-2024), comparing performance with human judgments.

Result: English is overrepresented; localized benchmarks outperform translations (0.68 vs. 0.47 correlation with human judgments). STEM tasks align better with humans than NLP tasks.

Conclusion: Proposes principles for effective multilingual benchmarking and calls for global collaboration to develop human-aligned benchmarks.

Abstract: As large language models (LLMs) continue to advance in linguistic
capabilities, robust multilingual evaluation has become essential for promoting
equitable technological progress. This position paper examines over 2,000
multilingual (non-English) benchmarks from 148 countries, published between
2021 and 2024, to evaluate past, present, and future practices in multilingual
benchmarking. Our findings reveal that, despite significant investments
amounting to tens of millions of dollars, English remains significantly
overrepresented in these benchmarks. Additionally, most benchmarks rely on
original language content rather than translations, with the majority sourced
from high-resource countries such as China, India, Germany, the UK, and the
USA. Furthermore, a comparison of benchmark performance with human judgments
highlights notable disparities. STEM-related tasks exhibit strong correlations
with human evaluations (0.70 to 0.85), while traditional NLP tasks like
question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).
Moreover, translating English benchmarks into other languages proves
insufficient, as localized benchmarks demonstrate significantly higher
alignment with local human judgments (0.68) than their translated counterparts
(0.47). This underscores the importance of creating culturally and
linguistically tailored benchmarks rather than relying solely on translations.
Through this comprehensive analysis, we highlight six key limitations in
current multilingual evaluation practices, propose the guiding principles
accordingly for effective multilingual benchmarking, and outline five critical
research directions to drive progress in the field. Finally, we call for a
global collaborative effort to develop human-aligned benchmarks that prioritize
real-world applications.

</details>


### [9] [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)
*Qiyao Wang,Guhong Chen,Hongbo Wang,Huaren Liu,Minghui Zhu,Zhifei Qin,Linwei Li,Yilin Yue,Shiqiang Wang,Jiayan Li,Yihang Wu,Ziqiang Liu,Longze Chen,Run Luo,Liyang Fan,Jiaming Li,Lei Zhang,Kan Xu,Hongfei Lin,Hamid Alinejad-Rokny,Shiwen Ni,Yuan Lin,Min Yang*

Main category: cs.CL

TL;DR: The paper introduces IPBench, a comprehensive bilingual benchmark for evaluating LLMs in real-world IP tasks, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: The complexity of IP tasks and the lack of aligned datasets necessitate a robust benchmark to assess LLM capabilities in this domain.

Method: Developed a taxonomy of 8 IP mechanisms and 20 tasks, benchmarked 16 LLMs (general and domain-specific) using IPBench.

Result: Best-performing model achieved 75.8% accuracy, with open-source and law-oriented models underperforming compared to closed-source general models.

Conclusion: IPBench addresses the gap in IP task evaluation, highlighting the need for further LLM improvement in this domain.

Abstract: Intellectual Property (IP) is a unique domain that integrates technical and
legal knowledge, making it inherently complex and knowledge-intensive. As large
language models (LLMs) continue to advance, they show great potential for
processing IP tasks, enabling more efficient analysis, understanding, and
generation of IP-related content. However, existing datasets and benchmarks
either focus narrowly on patents or cover limited aspects of the IP field,
lacking alignment with real-world scenarios. To bridge this gap, we introduce
the first comprehensive IP task taxonomy and a large, diverse bilingual
benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is
designed to evaluate LLMs in real-world intellectual property applications,
encompassing both understanding and generation. We benchmark 16 LLMs, ranging
from general-purpose to domain-specific models, and find that even the
best-performing model achieves only 75.8% accuracy, revealing substantial room
for improvement. Notably, open-source IP and law-oriented models lag behind
closed-source general-purpose models. We publicly release all data and code of
IPBench and will continue to update it with additional IP-related tasks to
better reflect real-world challenges in the intellectual property domain.

</details>


### [10] [Compass-V2 Technical Report](https://arxiv.org/abs/2504.15527)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v2 is a lightweight MoE model for SEA languages and e-commerce, balancing performance and cost with 30B total parameters. It uses a hybrid reasoning model and curated datasets to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address underrepresentation of SEA languages and lack of e-commerce focus in existing LLMs.

Method: Lightweight MoE design (30B total, 5B active parameters), hybrid reasoning, and high-quality SEA/e-commerce datasets.

Result: State-of-the-art performance in SEA multilingual and e-commerce tasks among sub-30B models, with lower inference cost.

Conclusion: Compass-v2 effectively bridges gaps in multilingual and domain-specific LLM performance for SEA and e-commerce.

Abstract: Predominant LLMs focus on high-resource languages while leaving low-resource
languages, particularly those in Southeast Asia (SEA), underrepresented. In
addition, those models are general-purpose and pay limited attention to the
e-commerce domain. To overcome these limitations, we introduce Compass-v2, a
lightweight Mixture-of-Experts (MoE) model specifically designed for Southeast
Asian languages and e-commerce applications. To balance model performance and
inference cost, the model is designed with 30B total parameters and 5B active
parameters, incorporating both fine-grained and shared expert modules. To
enhance multilingual performance, we curated and constructed a high-quality,
industry-leading SEA dataset, to the best of our knowledge. To boost
performance in the e-commerce domain, we built a dataset comprising hundreds of
billions of tokens, sourced through external data mining and internal platform
collection. Besides, we pioneered a hybrid reasoning model that supports both
fast thinking and deep thinking within a unified framework to enhance the
reasoning capabilities, diverging from the conventional industry practice of
deploying two separate models. Through extensive experimental evaluations, our
model demonstrates state-of-the-art SEA multilingual and e-commerce performance
among sub-30B models, while maintaining significantly lower inference cost.

</details>


### [11] [Fine-tuning Whisper on Low-Resource Languages for Real-World Applications](https://arxiv.org/abs/2412.15726)
*Vincenzo Timmel,Claudio Paonessa,Reza Kakooee,Manfred Vogel,Daniel Perruchoud*

Main category: cs.CL

TL;DR: A novel method for fine-tuning Whisper for low-resource languages by converting sentence-level data into long-form corpus, demonstrated with Swiss German, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of non-sentence-level data for low-resource languages, which is crucial for long-form audio performance but often restricted by copyright.

Method: Transform sentence-level data into a long-form corpus to preserve Whisper's long-form audio handling and segmentation capabilities without needing non-sentence-level data.

Result: Improved performance in real-world applications, higher BLEU scores compared to non-fine-tuned Whisper and previous models, and adaptability to other low-resource languages.

Conclusion: The method successfully bridges the data gap for low-resource languages, enabling high-quality long-form transcription using only sentence-level data.

Abstract: This paper presents a new approach to fine-tuning OpenAI's Whisper model for
low-resource languages by introducing a novel data generation method that
converts sentence-level data into a long-form corpus, using Swiss German as a
case study. Non-sentence-level data, which could improve the performance of
long-form audio, is difficult to obtain and often restricted by copyright laws.
Our method bridges this gap by transforming more accessible sentence-level data
into a format that preserves the model's ability to handle long-form audio and
perform segmentation without requiring non-sentence-level data. Our data
generation process improves performance in several real-world applications and
leads to the development of a new state-of-the-art speech-to-text (STT) model
for Swiss German. We compare our model with a non-fine-tuned Whisper and our
previous state-of-the-art Swiss German STT models, where our new model achieves
higher BLEU scores. Our results also indicate that the proposed method is
adaptable to other low-resource languages, supported by written guidance and
code that allows the creation of fine-tuned Whisper models, which keep
segmentation capabilities and allow the transcription of longer audio files
using only sentence-level data with high quality.

</details>


### [12] [llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length](https://arxiv.org/abs/2504.15544)
*Issa Sugiura,Kouta Nakayama,Yusuke Oda*

Main category: cs.CL

TL;DR: A ModernBERT model (llm-jp-modernbert) trained on a large Japanese corpus with 8192-token contexts achieves good fill-mask results but doesn't surpass baselines in downstream tasks. The study includes analysis of context length effects and sentence embeddings.


<details>
  <summary>Details</summary>
Motivation: To explore pretraining encoder models with large-scale corpora and long contexts, an area less studied compared to decoder-only transformers.

Method: Train a ModernBERT model on a massive Japanese corpus with 8192-token contexts, evaluate on fill-mask tasks, and analyze context length and sentence embeddings.

Result: Good fill-mask performance but no improvement over baselines in downstream tasks; insights into context length and sentence embeddings.

Conclusion: The model supports reproducibility and advances long-context BERT research, though further improvements are needed for downstream tasks.

Abstract: Encoder-only transformer models like BERT are widely adopted as a pre-trained
backbone for tasks like sentence classification and retrieval. However,
pretraining of encoder models with large-scale corpora and long contexts has
been relatively underexplored compared to decoder-only transformers. In this
work, we present llm-jp-modernbert, a ModernBERT model trained on a publicly
available, massive Japanese corpus with a context length of 8192 tokens. While
our model does not surpass existing baselines on downstream tasks, it achieves
good results on fill-mask test evaluations. We also analyze the effect of
context length expansion through pseudo-perplexity experiments. Furthermore, we
investigate sentence embeddings in detail, analyzing their transitions during
training and comparing them with those from other existing models, confirming
similar trends with models sharing the same architecture. To support
reproducibility and foster the development of long-context BERT, we release our
model, along with the training and evaluation code.

</details>


### [13] [LLM-based Semantic Augmentation for Harmful Content Detection](https://arxiv.org/abs/2504.15548)
*Elyas Meguellati,Assaad Zeghina,Shazia Sadiq,Gianluca Demartini*

Main category: cs.CL

TL;DR: LLMs perform poorly on complex social media tasks like propaganda detection, but LLM-based text preprocessing and semantic augmentation can improve performance without large data increases.


<details>
  <summary>Details</summary>
Motivation: Address the decline in LLM efficacy for high-context social media tasks by leveraging LLMs for text cleaning and semantic augmentation.

Method: Use LLMs to preprocess noisy text and provide context-rich explanations, enhancing training sets. Evaluated on SemEval 2024, Google Jigsaw, and Facebook datasets.

Result: Zero-shot LLM classification underperforms, but LLM-based augmentation matches human-annotated data performance at lower cost.

Conclusion: Strategic LLM integration into ML pipelines is crucial for social media classification, offering cost-effective solutions for harmful content detection.

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
performance on simple text classification tasks, frequently under zero-shot
settings. However, their efficacy declines when tackling complex social media
challenges such as propaganda detection, hateful meme classification, and
toxicity identification. Much of the existing work has focused on using LLMs to
generate synthetic training data, overlooking the potential of LLM-based text
preprocessing and semantic augmentation. In this paper, we introduce an
approach that prompts LLMs to clean noisy text and provide context-rich
explanations, thereby enhancing training sets without substantial increases in
data volume. We systematically evaluate on the SemEval 2024 multi-label
Persuasive Meme dataset and further validate on the Google Jigsaw toxic
comments and Facebook hateful memes datasets to assess generalizability. Our
results reveal that zero-shot LLM classification underperforms on these
high-context tasks compared to supervised models. In contrast, integrating
LLM-based semantic augmentation yields performance on par with approaches that
rely on human-annotated data, at a fraction of the cost. These findings
underscore the importance of strategically incorporating LLMs into machine
learning (ML) pipeline for social media classification tasks, offering broad
implications for combating harmful content online.

</details>


### [14] [VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation](https://arxiv.org/abs/2504.04060)
*Yuhao Wang,Heyang Liu,Ziyang Cheng,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: VocalNet-1B and VocalNet-8B are high-performance, low-latency speech LLMs using multi-token prediction (MTP) for faster, better speech generation, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: To advance speech LLMs by improving generation speed and quality for real-time voice interaction.

Method: Introduced multi-token prediction (MTP) as a paradigm shift from next-token prediction (NTP), with a scalable, model-agnostic training framework.

Result: VocalNet matches mainstream Omni LLMs with limited data and surpasses open-source speech LLMs.

Conclusion: MTP significantly enhances speech LLMs; VocalNet's open-source release aims to foster community progress.

Abstract: Speech large language models (LLMs) have emerged as a prominent research
focus in speech processing. We introduce VocalNet-1B and VocalNet-8B, a series
of high-performance, low-latency speech LLMs enabled by a scalable and
model-agnostic training framework designed for real-time voice interaction.
Central to our contribution is the first application of multi-token prediction
(MTP) to speech LLMs. This approach represents a paradigm shift from standard
next-token prediction (NTP), offering simultaneous improvements in generation
speed and quality. Informed by analysis of MTP's effect on speech generation
and experimental comparisons, we designed a straightforward and highly
effective MTP implementation. Experiments demonstrate that VocalNet performs on
par with mainstream Omni LLMs even with limited training data, and
significantly surpasses existing open-source speech LLMs. To foster
reproducibility and community advancement, all model weights, inference code,
training data, and framework implementations have been made publicly available
at https://github.com/SJTU-OmniAgent/VocalNet

</details>


### [15] [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
*Yuxin Jiang,Yufei Wang,Chuhan Wu,Xinyi Dai,Yan Xu,Weinan Gan,Yasheng Wang,Xin Jiang,Lifeng Shang,Ruiming Tang,Wei Wang*

Main category: cs.CL

TL;DR: WebR is an automated framework for synthesizing high-quality instruction-tuning data from raw web documents, outperforming existing methods by up to 16.65% in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on seed data quality or strong assumptions about web documents, limiting their effectiveness.

Method: WebR uses a dual-perspective paradigm (Web as Instruction and Web as Response) to reconstruct web documents into instruction-response pairs.

Result: WebR outperforms state-of-the-art baselines by up to 16.65% in benchmarks, showing superior compatibility, data efficiency, and scalability.

Conclusion: WebR enables enhanced domain adaptation with minimal effort, offering a scalable solution for high-quality instruction-tuning data.

Abstract: The improvement of LLMs' instruction-following capabilities depends
critically on the availability of high-quality instruction-response pairs.
While existing automatic data synthetic methods alleviate the burden of manual
curation, they often rely heavily on either the quality of seed data or strong
assumptions about the structure and content of web documents. To tackle these
challenges, we propose Web Reconstruction (WebR), a fully automated framework
for synthesizing high-quality instruction-tuning (IT) data directly from raw
web documents with minimal assumptions. Leveraging the inherent diversity of
raw web content, we conceptualize web reconstruction as an instruction-tuning
data synthesis task via a novel dual-perspective paradigm--Web as Instruction
and Web as Response--where each web document is designated as either an
instruction or a response to trigger the reconstruction process. Comprehensive
experiments show that datasets generated by WebR outperform state-of-the-art
baselines by up to 16.65% across four instruction-following benchmarks.
Notably, WebR demonstrates superior compatibility, data efficiency, and
scalability, enabling enhanced domain adaptation with minimal effort. The data
and code are publicly available at https://github.com/YJiangcm/WebR.

</details>


### [16] [Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models](https://arxiv.org/abs/2504.15604)
*Pavan Yadav,Nikhil Khandalkar,Krishna Shinde,Lokesh B. Ramegowda,Rajarshi Das*

Main category: cs.CL

TL;DR: The study compares GPT-2 and Llama-2-7b-chat-hf on Theory of Mind tasks, finding Llama-2 outperforms GPT-2, especially at lower temperatures, with added context reducing accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate how model architecture, temperature, and contextual complexity affect next-token prediction in Theory of Mind tasks.

Method: Built a dataset from 10 short stories, enhanced with GPT-4 infills, and tested models under four temperature settings across three reasoning levels.

Result: Llama-2 outperforms GPT-2, especially at lower temperatures. Added context reduces accuracy, and higher reasoning complexity increases prediction variability.

Conclusion: The study highlights the impact of model architecture, temperature, and context on next-token prediction, revealing strengths and limitations of current language models.

Abstract: Language models have made significant progress in generating coherent text
and predicting next tokens based on input prompts. This study compares the
next-token prediction performance of two well-known models: OpenAI's GPT-2 and
Meta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their
capabilities, we built a dataset from 10 short stories sourced from the Explore
ToM Dataset. We enhanced these stories by programmatically inserting additional
sentences (infills) using GPT-4, creating variations that introduce different
levels of contextual complexity. This setup enables analysis of how increasing
context affects model performance. We tested both models under four temperature
settings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next
token across three reasoning levels. Zero-order reasoning involves tracking the
state, either current (ground truth) or past (memory). First-order reasoning
concerns understanding another's mental state (e.g., "Does Anne know the apple
is salted?"). Second-order reasoning adds recursion (e.g., "Does Anne think
that Charles knows the apple is salted?").
  Our results show that adding more infill sentences slightly reduces
prediction accuracy, as added context increases complexity and ambiguity.
Llama-2 consistently outperforms GPT-2 in prediction accuracy, especially at
lower temperatures, demonstrating greater confidence in selecting the most
probable token. As reasoning complexity rises, model responses diverge more.
Notably, GPT-2 and Llama-2 display greater variability in predictions during
first- and second-order reasoning tasks. These findings illustrate how model
architecture, temperature, and contextual complexity influence next-token
prediction, contributing to a better understanding of the strengths and
limitations of current language models.

</details>


### [17] [Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement](https://arxiv.org/abs/2504.15630)
*Xiaowei Yuan,Zhao Yang,Ziyang Huang,Yequan Wang,Siqi Fan,Yiming Ju,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: The paper introduces Context-aware Layer Enhancement (CaLE), a method to improve LLMs' context-faithful generation by optimizing internal contextual information processing.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with context-faithful generation despite their capabilities. Existing methods overlook internal contextual processing mechanisms.

Method: Proposes CaLE, using V-usable information analysis to amplify contextual information at an optimal layer, enhancing final representations.

Result: CaLE improves context-faithful generation in Question-Answering tasks, especially with unknown or conflicting contexts.

Conclusion: CaLE effectively addresses LLMs' limitations in leveraging contextual knowledge, enhancing their performance in context-dependent tasks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, yet they often struggle with context-faithfulness generations
that properly reflect contextual knowledge. While existing approaches focus on
enhancing the decoding strategies, they ignore the fundamental mechanism of how
contextual information is processed within LLMs' internal states. As a result,
LLMs remain limited in their ability to fully leverage contextual knowledge. In
this paper, we propose Context-aware Layer Enhancement (CaLE), a novel
intervention method that enhances the utilization of contextual knowledge
within LLMs' internal representations. By employing V-usable information
analysis, CaLE strategically amplifies the growth of contextual information at
an optimal layer, thereby enriching representations in the final layer. Our
experiments demonstrate that CaLE effectively improves context-faithful
generation in Question-Answering tasks, particularly in scenarios involving
unknown or conflicting contextual knowledge.

</details>


### [18] [Cost-Effective Text Clustering with Large Language Models](https://arxiv.org/abs/2504.15640)
*Hongtao Wang,Taiyan Zhang,Renchi Yang,Jianliang Xu*

Main category: cs.CL

TL;DR: TECL is a cost-effective framework for text clustering using LLMs, reducing computational/financial overhead while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the high computational and financial costs of using LLMs for text clustering by optimizing query usage.

Method: Uses EdgeLLM/TriangleLLM to generate must-link/cannot-link constraints, then applies weighted constrained clustering.

Result: Outperforms existing methods in unsupervised text clustering under the same query budget.

Conclusion: TECL offers a practical solution for leveraging LLMs in text clustering efficiently.

Abstract: Text clustering aims to automatically partition a collection of text
documents into distinct clusters based on linguistic features. In the
literature, this task is usually framed as metric clustering based on text
embeddings from pre-trained encoders or a graph clustering problem upon
pairwise similarities from an oracle, e.g., a large ML model. Recently, large
language models (LLMs) bring significant advancement in this field by offering
contextualized text embeddings and highly accurate similarity scores, but
meanwhile, present grand challenges to cope with substantial computational
and/or financial overhead caused by numerous API-based queries or inference
calls to the models.
  In response, this paper proposes TECL, a cost-effective framework that taps
into the feedback from LLMs for accurate text clustering within a limited
budget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or
TriangleLLM to construct must-link/cannot-link constraints for text pairs, and
further leverages such constraints as supervision signals input to our weighted
constrained clustering approach to generate clusters. Particularly, EdgeLLM
(resp. TriangleLLM) enables the identification of informative text pairs (resp.
triplets) for querying LLMs via well-thought-out greedy algorithms and accurate
extraction of pairwise constraints through carefully-crafted prompting
techniques. Our experiments on multiple benchmark datasets exhibit that TECL
consistently and considerably outperforms existing solutions in unsupervised
text clustering under the same query cost for LLMs.

</details>


### [19] [Computational Typology](https://arxiv.org/abs/2504.15642)
*Gerhard Jäger*

Main category: cs.CL

TL;DR: Computational methods enhance typological linguistics by analyzing large-scale data and testing structural hypotheses.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the advantages of computational statistical modeling in understanding language diversity and universals.

Method: Utilizes computational statistical modeling for large-scale linguistic data analysis.

Result: Improved ability to identify language universals and test structural hypotheses.

Conclusion: Computational approaches significantly advance typological research by enabling robust data analysis.

Abstract: Typology is a subfield of linguistics that focuses on the study and
classification of languages based on their structural features. Unlike
genealogical classification, which examines the historical relationships
between languages, typology seeks to understand the diversity of human
languages by identifying common properties and patterns, known as universals.
In recent years, computational methods have played an increasingly important
role in typological research, enabling the analysis of large-scale linguistic
data and the testing of hypotheses about language structure and evolution. This
article provides an illustration of the benefits of computational statistical
modeling in typology.

</details>


### [20] [FinTextSim: Enhancing Financial Text Analysis with BERTopic](https://arxiv.org/abs/2504.15683)
*Simon Jehnen,Joaquín Ordieres-Meré,Javier Villalba-Díez*

Main category: cs.CL

TL;DR: The study evaluates BERTopic's effectiveness for analyzing 10-K filings and introduces FinTextSim, a finetuned sentence-transformer model, which outperforms all-MiniLM-L6-v2 in financial text analysis.


<details>
  <summary>Details</summary>
Motivation: To enhance financial text analysis by integrating contextual embeddings and improving topic modeling for better insights from annual reports.

Method: Uses BERTopic with FinTextSim and all-MiniLM-L6-v2 embeddings to analyze 10-K filings (2016-2022), comparing performance.

Result: FinTextSim improves intratopic similarity by 81% and reduces intertopic similarity by 100%, enabling clear economic topic clusters with BERTopic.

Conclusion: FinTextSim is essential for advancing financial text analysis, improving research quality, and aiding decision-making and business valuation.

Abstract: Recent advancements in information availability and computational
capabilities have transformed the analysis of annual reports, integrating
traditional financial metrics with insights from textual data. To extract
valuable insights from this wealth of textual data, automated review processes,
such as topic modeling, are crucial. This study examines the effectiveness of
BERTopic, a state-of-the-art topic model relying on contextual embeddings, for
analyzing Item 7 and Item 7A of 10-K filings from S&P 500 companies
(2016-2022). Moreover, we introduce FinTextSim, a finetuned
sentence-transformer model optimized for clustering and semantic search in
financial contexts. Compared to all-MiniLM-L6-v2, the most widely used
sentence-transformer, FinTextSim increases intratopic similarity by 81% and
reduces intertopic similarity by 100%, significantly enhancing organizational
clarity. We assess BERTopic's performance using embeddings from both FinTextSim
and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and
distinct economic topic clusters when paired with FinTextSim's embeddings.
Without FinTextSim, BERTopic struggles with misclassification and overlapping
topics. Thus, FinTextSim is pivotal for advancing financial text analysis.
FinTextSim's enhanced contextual embeddings, tailored for the financial domain,
elevate the quality of future research and financial information. This improved
quality of financial information will enable stakeholders to gain a competitive
advantage, streamlining resource allocation and decision-making processes.
Moreover, the improved insights have the potential to leverage business
valuation and stock price prediction models.

</details>


### [21] [Subject islands do not reduce to construction-specific discourse function](https://arxiv.org/abs/2504.15688)
*Mandy Cartner,Matthew Kogan,Nikolas Webster,Matthew Wagers,Ivy Sichel*

Main category: cs.CL

TL;DR: The paper examines subject islands in linguistics, challenging the idea that their ungrammaticality is due to information structure, and supports a syntactic account.


<details>
  <summary>Details</summary>
Motivation: To test whether subject islands are specific to wh-questions' information structure or a general syntactic constraint.

Method: Three large-scale acceptability studies using a super-additive design across wh-questions, relative clauses, and topicalization.

Result: Subject island effects were found in all constructions, contradicting the information-structure-based explanation.

Conclusion: The findings support a syntactic account of islands, independent of communicative function.

Abstract: The term islands in linguistics refers to phrases from which extracting an
element results in ungrammaticality (Ross, 1967). Grammatical subjects are
considered islands because extracting a sub-part of a subject results in an
ill-formed sentence, despite having a clear intended meaning (e.g., "Which
topic did the article about inspire you?"). The generative tradition, which
views syntax as autonomous of meaning and function, attributes this
ungrammaticality to the abstract movement dependency between the wh-phrase and
the subject-internal position with which it is associated for interpretation.
However, research on language that emphasizes its communicative function
suggests instead that syntactic constraints, including islands, can be
explained based on the way different constructions package information.
Accordingly, Abeill\'e et al. (2020) suggest that the islandhood of subjects is
specific to the information structure of wh-questions, and propose that
subjects are not islands for movement, but for focusing, due to their
discourse-backgroundedness. This predicts that other constructions that differ
in their information structure from wh-questions, but still involve movement,
should not create a subject island effect. We test this prediction in three
large-scale acceptability studies, using a super-additive design that singles
out subject island violations, in three different constructions: wh-questions,
relative clauses, and topicalization. We report evidence for a subject island
effect in each construction type, despite only wh-questions introducing what
Abeill\'e et al. (2020) call "a clash in information structure." We argue that
this motivates an account of islands in terms of abstract, syntactic
representations, independent of the communicative function associated with the
constructions.

</details>


### [22] [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Tina, a tiny reasoning model, achieves strong performance with minimal resources using LoRA for efficient RL updates, outperforming SOTA models at a fraction of the cost.


<details>
  <summary>Details</summary>
Motivation: To explore cost-effective ways to develop strong reasoning abilities in language models.

Method: Uses parameter-efficient updates (LoRA) during RL on a 1.5B parameter base model.

Result: Achieves competitive/superior reasoning performance with a 260x cost reduction (e.g., >20% performance increase, 43.33% Pass@1 accuracy on AIME24).

Conclusion: LoRA-based RL is surprisingly effective for reasoning, preserving base knowledge while adapting to reasoning structures. All resources are open-sourced.

Abstract: How cost-effectively can strong reasoning abilities be achieved in language
models? Driven by this fundamental question, we present Tina, a family of tiny
reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates
that substantial reasoning performance can be developed using only minimal
resources, by applying parameter-efficient updates during reinforcement
learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B
parameter base model. This minimalist approach produces models that achieve
reasoning performance which is competitive with, and sometimes surpasses, SOTA
RL reasoning models built upon the same base model. Crucially, this is achieved
at a tiny fraction of the computational post-training cost employed by existing
SOTA models. In fact, the best Tina model achieves a >20\% reasoning
performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD
post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our
work reveals the surprising effectiveness of efficient RL reasoning via LoRA.
We validate this across multiple open-source reasoning datasets and various
ablation settings starting with a single, fixed set of hyperparameters.
Furthermore, we hypothesize that this effectiveness and efficiency stem from
LoRA rapidly adapting the model to the structural format of reasoning rewarded
by RL, while largely preserving the base model's underlying knowledge. In
service of accessibility and open research, we fully open-source all code,
training logs, and model weights \& checkpoints.

</details>


### [23] [Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach](https://arxiv.org/abs/2504.15784)
*Ruizhe Li,Chiwei Zhu,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: Proposes an automated evaluation method for LLM-generated creative texts using the Torrance Test of Creative Writing (TTCW), improving alignment with human assessments.


<details>
  <summary>Details</summary>
Motivation: Evaluating creativity in machine-generated texts is challenging; current methods are costly or misaligned with human judgment.

Method: Reference-based Likert-style scoring using TTCW, comparing texts to high-quality references.

Result: Achieves 0.75 pairwise accuracy (+15%) in aligning LLM evaluations with human assessments.

Conclusion: The method effectively automates creativity evaluation, enhancing reliability and reducing reliance on manual annotations.

Abstract: Creative writing is a key capability of Large Language Models (LLMs), with
potential applications in literature, storytelling, and various creative
domains. However, evaluating the creativity of machine-generated texts remains
a significant challenge, as existing methods either rely on costly manual
annotations or fail to align closely with human assessments. In this paper, we
propose an effective automated evaluation method based on the Torrance Test of
Creative Writing (TTCW), which evaluates creativity as product. Our method
employs a reference-based Likert-style approach, scoring generated creative
texts relative to high-quality reference texts across various tests.
Experimental results demonstrate that our method significantly improves the
alignment between LLM evaluations and human assessments, achieving a pairwise
accuracy of 0.75 (+15\%).

</details>


### [24] [A closer look at how large language models trust humans: patterns and biases](https://arxiv.org/abs/2504.15801)
*Valeria Lerman,Yaniv Dover*

Main category: cs.CL

TL;DR: The paper explores how LLM-based agents develop trust in humans, finding similarities to human trust dynamics but with biases influenced by demographic factors.


<details>
  <summary>Details</summary>
Motivation: Understanding trust dynamics between humans and AI agents, especially how LLMs trust humans, is crucial for decision-making applications.

Method: The study uses behavioral theories to analyze LLM trust based on competence, benevolence, and integrity, alongside demographic variables, across 43,200 simulated experiments with five models.

Result: LLM trust development resembles human trust, with trustworthiness as a strong predictor, though biased by age, religion, and gender in some scenarios.

Conclusion: The findings highlight the need to monitor AI-to-human trust dynamics and biases to avoid harmful outcomes in trust-sensitive AI applications.

Abstract: As large language models (LLMs) and LLM-based agents increasingly interact
with humans in decision-making contexts, understanding the trust dynamics
between humans and AI agents becomes a central concern. While considerable
literature studies how humans trust AI agents, it is much less understood how
LLM-based agents develop effective trust in humans. LLM-based agents likely
rely on some sort of implicit effective trust in trust-related contexts (e.g.,
evaluating individual loan applications) to assist and affect decision making.
Using established behavioral theories, we develop an approach that studies
whether LLMs trust depends on the three major trustworthiness dimensions:
competence, benevolence and integrity of the human subject. We also study how
demographic variables affect effective trust. Across 43,200 simulated
experiments, for five popular language models, across five different scenarios
we find that LLM trust development shows an overall similarity to human trust
development. We find that in most, but not all cases, LLM trust is strongly
predicted by trustworthiness, and in some cases also biased by age, religion
and gender, especially in financial scenarios. This is particularly true for
scenarios common in the literature and for newer models. While the overall
patterns align with human-like mechanisms of effective trust formation,
different models exhibit variation in how they estimate trust; in some cases,
trustworthiness and demographic factors are weak predictors of effective trust.
These findings call for a better understanding of AI-to-human trust dynamics
and monitoring of biases and trust development patterns to prevent unintended
and potentially harmful outcomes in trust-sensitive applications of AI.

</details>


### [25] [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
*Michael A. Hedderich,Anyi Wang,Raoyuan Zhao,Florian Eichin,Barbara Plank*

Main category: cs.CL

TL;DR: Spotlight combines automation and human analysis to efficiently evaluate prompt and model changes in large language models by identifying systematic differences in outputs.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for prompt engineering are either limited in insights or labor-intensive, necessitating a more efficient and insightful approach.

Method: Spotlight uses data mining to distinguish random variations from systematic differences in model outputs, providing token patterns to guide manual analysis.

Result: The approach reliably extracts token patterns, offers new insights into prompt data, and helps users understand systematic differences, including those related to gender or culture.

Conclusion: Spotlight supports prompt engineering and human-centric model behavior research by efficiently revealing systematic differences in language model outputs.

Abstract: Prompt engineering for large language models is challenging, as even small
prompt perturbations or model changes can significantly impact the generated
output texts. Existing evaluation methods, either automated metrics or human
evaluation, have limitations, such as providing limited insights or being
labor-intensive. We propose Spotlight, a new approach that combines both
automation and human analysis. Based on data mining techniques, we
automatically distinguish between random (decoding) variations and systematic
differences in language model outputs. This process provides token patterns
that describe the systematic differences and guide the user in manually
analyzing the effects of their prompt and model changes efficiently. We create
three benchmarks to quantitatively test the reliability of token pattern
extraction methods and demonstrate that our approach provides new insights into
established prompt data. From a human-centric perspective, through
demonstration studies and a user study, we show that our token pattern approach
helps users understand the systematic differences of language model outputs,
and we are able to discover relevant differences caused by prompt and model
changes (e.g. related to gender or culture), thus supporting the prompt
engineering process and human-centric model behavior research.

</details>


### [26] [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)
*Junshu Pan,Wei Shen,Shulin Huang,Qiji Zhou,Yue Zhang*

Main category: cs.CL

TL;DR: Pre-DPO improves DPO and SimPO by using a guiding reference model to optimize human preferences more efficiently.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in DPO and robustness issues in SimPO by introducing a reference model for better data utilization and training stability.

Method: Proposes Pre-DPO, which leverages a guiding reference model to adaptively weight training samples based on their suitability.

Result: Pre-DPO enhances performance on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks without extra models or data.

Conclusion: Pre-DPO is a simple, effective method to improve preference optimization in LLMs.

Abstract: Direct Preference Optimization (DPO) simplifies reinforcement learning from
human feedback (RLHF) for large language models (LLMs) by directly optimizing
human preferences without an explicit reward model. We find that during DPO
training, the reference model plays the role of a data weight adjuster.
However, the common practice of initializing the policy and reference models
identically in DPO can lead to inefficient data utilization and impose a
performance ceiling. Meanwhile, the lack of a reference model in Simple
Preference Optimization (SimPO) reduces training robustness and necessitates
stricter conditions to prevent catastrophic forgetting. In this work, we
propose Pre-DPO, a simple yet effective DPO-based training paradigm that
enhances preference optimization performance by leveraging a guiding reference
model. This reference model provides foresight into the optimal policy state
achievable through the training preference data, serving as a guiding mechanism
that adaptively assigns higher weights to samples more suitable for the model
and lower weights to those less suitable. Extensive experiments on AlpacaEval
2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently
improves the performance of both DPO and SimPO, without relying on external
models or additional data.

</details>


### [27] [Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.15848)
*Luwei Xiao,Rui Mao,Shuai Zhao,Qika Lin,Yanhao Jia,Liang He,Erik Cambria*

Main category: cs.CL

TL;DR: The paper introduces Chimera, a framework for multimodal aspect-based sentiment classification (MASC) that combines cognitive and aesthetic analysis to improve sentiment prediction by leveraging fine-grained visual and textual features.


<details>
  <summary>Details</summary>
Motivation: Existing MASC models lack understanding of fine-grained visual content and cognitive rationales, prompting the need for a framework that integrates semantic and affective-cognitive perspectives.

Method: Chimera aligns visual patches with words, extracts coarse and fine-grained visual features, translates them into textual descriptions, and uses LLM-generated sentimental causes and impressions to enhance sentiment awareness.

Result: Experiments on standard MASC datasets show Chimera's effectiveness and flexibility, outperforming models like GPT-4o.

Conclusion: Chimera successfully bridges gaps in MASC by integrating cognitive and aesthetic analysis, offering a robust solution for sentiment prediction in multimodal content.

Abstract: Multimodal aspect-based sentiment classification (MASC) is an emerging task
due to an increase in user-generated multimodal content on social platforms,
aimed at predicting sentiment polarity toward specific aspect targets (i.e.,
entities or attributes explicitly mentioned in text-image pairs). Despite
extensive efforts and significant achievements in existing MASC, substantial
gaps remain in understanding fine-grained visual content and the cognitive
rationales derived from semantic content and impressions (cognitive
interpretations of emotions evoked by image content). In this study, we present
Chimera: a cognitive and aesthetic sentiment causality understanding framework
to derive fine-grained holistic features of aspects and infer the fundamental
drivers of sentiment expression from both semantic perspectives and
affective-cognitive resonance (the synergistic effect between emotional
responses and cognitive interpretations). Specifically, this framework first
incorporates visual patch features for patch-word alignment. Meanwhile, it
extracts coarse-grained visual features (e.g., overall image representation)
and fine-grained visual regions (e.g., aspect-related regions) and translates
them into corresponding textual descriptions (e.g., facial, aesthetic).
Finally, we leverage the sentimental causes and impressions generated by a
large language model (LLM) to enhance the model's awareness of sentimental cues
evoked by semantic content and affective-cognitive resonance. Experimental
results on standard MASC datasets demonstrate the effectiveness of the proposed
model, which also exhibits greater flexibility to MASC compared to LLMs such as
GPT-4o. We have publicly released the complete implementation and dataset at
https://github.com/Xillv/Chimera

</details>


### [28] [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
*Chenxu Yang,Qingyi Si,Yongjie Duan,Zheliang Zhu,Chenyu Zhu,Zheng Lin,Li Cao,Weiping Wang*

Main category: cs.CL

TL;DR: A method for self-truncating CoT sequences in LRLMs improves efficiency and accuracy by dynamically exiting generation based on model confidence.


<details>
  <summary>Details</summary>
Motivation: Overthinking in long CoT sequences reduces efficiency and risks accuracy loss due to redundancy.

Method: Monitors model behavior at reasoning transition points to dynamically terminate generation when confidence is high.

Result: Reduces CoT length by 31-43% and improves accuracy by 1.7-5.7% on benchmarks.

Conclusion: The method is effective, requires no training, and integrates seamlessly with existing LLMs.

Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time
scaling, which extends long chain-of-thought (CoT) generation to solve complex
tasks. However, overthinking in long CoT not only slows down the efficiency of
problem solving, but also risks accuracy loss due to the extremely detailed or
redundant reasoning steps. We propose a simple yet effective method that allows
LLMs to self-truncate CoT sequences by early exit during generation. Instead of
relying on fixed heuristics, the proposed method monitors model behavior at
potential reasoning transition points (e.g.,"Wait" tokens) and dynamically
terminates the next reasoning chain's generation when the model exhibits high
confidence in a trial answer. Our method requires no additional training and
can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments
on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024
show that the proposed method is consistently effective on deepseek-series
reasoning LLMs, reducing the length of CoT sequences by an average of 31% to
43% while improving accuracy by 1.7% to 5.7%.

</details>


### [29] [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
*Cheng Wen,Tingwei Guo,Shuaijiang Zhao,Wei Zou,Xiangang Li*

Main category: cs.CL

TL;DR: Reinforcement learning (RL) improves reasoning in large language models (LLMs) and is extended to audio-language models (LALMs) using GRPO, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: To explore if RL's reasoning improvements in LLMs transfer to audio-language models and to enhance audio-language understanding.

Method: Extends GRPO to LALMs, uses a two-stage regimen (supervised fine-tuning and curriculum-guided GRPO), and compares reasoning types.

Result: SARI improves accuracy by 16.35% over the base model, with a variant achieving 67.08% on MMAU test-mini.

Conclusion: Explicit, structured reasoning and curriculum learning significantly enhance audio-language understanding.

Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the
reasoning ability of large language models (LLMs) by prompting them to "think
before answering." Yet whether and how these gains transfer to audio-language
reasoning remains largely unexplored. We extend the Group-Relative Policy
Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model
(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage
regimen supervised fine-tuning on structured and unstructured
chains-of-thought, followed by curriculum-guided GRPO, we systematically
compare implicit vs. explicit, and structured vs. free form reasoning under
identical architectures. Our structured audio reasoning model, SARI (Structured
Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a
16.35% improvement in average accuracy over the base model
Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni
reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.
Ablation experiments show that on the base model we use: (i) SFT warm-up is
important for stable RL training, (ii) structured chains yield more robust
generalization than unstructured ones, and (iii) easy-to-hard curricula
accelerate convergence and improve final performance. These findings
demonstrate that explicit, structured reasoning and curriculum learning
substantially enhances audio-language understanding.

</details>


### [30] [FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity](https://arxiv.org/abs/2504.15941)
*Fanny Jourdan,Yannick Chevalier,Cécile Favre*

Main category: cs.CL

TL;DR: FairTranslate is a dataset evaluating gender biases in LLM translations from English to French, revealing significant biases and advocating for fairer practices.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of LLMs in translating inclusive language, particularly non-binary gender biases, by providing a robust evaluation framework.

Method: Created FairTranslate, a human-annotated dataset of 2418 English-French sentence pairs, and tested four LLMs under various prompts.

Result: Substantial gender representation biases were found across all evaluated LLMs, indicating persistent challenges in equitable translation.

Conclusion: The study highlights the need for targeted strategies to ensure fair and inclusive language usage in LLM-based translation systems.

Abstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks
but often fall short when translating inclusive language -- such as texts
containing the singular 'they' pronoun or otherwise reflecting fair linguistic
protocols. Because these challenges span both computational and societal
domains, it is imperative to critically evaluate how well LLMs handle inclusive
translation with a well-founded framework.
  This paper presents FairTranslate, a novel, fully human-annotated dataset
designed to evaluate non-binary gender biases in machine translation systems
from English to French. FairTranslate includes 2418 English-French sentence
pairs related to occupations, annotated with rich metadata such as the
stereotypical alignment of the occupation, grammatical gender indicator
ambiguity, and the ground-truth gender label (male, female, or inclusive).
  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,
Llama3.3-70B) on this dataset under different prompting procedures. Our results
reveal substantial biases in gender representation across LLMs, highlighting
persistent challenges in achieving equitable outcomes in machine translation.
These findings underscore the need for focused strategies and interventions
aimed at ensuring fair and inclusive language usage in LLM-based translation
systems.
  We make the FairTranslate dataset publicly available on Hugging Face, and
disclose the code for all experiments on GitHub.

</details>


### [31] [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://arxiv.org/abs/2504.15983)
*Shang Wang*

Main category: cs.CL

TL;DR: The paper introduces W-PCA, a zero-shot NAS method for lightweight language models, improving efficiency and evaluation accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like biased metrics and inefficiency in zero-shot NAS for lightweight NLP models.

Method: Uses weight-weighted PCA with parameter count and principal components in FFN layers, avoiding gradient computations.

Result: Reduces training time, achieves higher test scores, and improves ranking correlation and solving time.

Conclusion: W-PCA is efficient and effective for designing lightweight language models.

Abstract: The demand for efficient natural language processing (NLP) systems has led to
the development of lightweight language models. Previous work in this area has
primarily focused on manual design or training-based neural architecture search
(NAS) methods. Recently, zero-shot NAS methods have been proposed for
evaluating language models without the need for training. However, prevailing
approaches to zero-shot NAS often face challenges such as biased evaluation
metrics and computational inefficiencies. In this paper, we introduce
weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored
for lightweight language models. Our approach utilizes two evaluation proxies:
the parameter count and the number of principal components with cumulative
contribution exceeding $\eta$ in the feed-forward neural (FFN) layer.
Additionally, by eliminating the need for gradient computations, we optimize
the evaluation time, thus enhancing the efficiency of designing and evaluating
lightweight language models. We conduct a comparative analysis on the GLUE and
SQuAD datasets to evaluate our approach. The results demonstrate that our
method significantly reduces training time compared to one-shot NAS methods and
achieves higher scores in the testing phase compared to previous
state-of-the-art training-based methods. Furthermore, we perform ranking
evaluations on a dataset sampled from the FlexiBERT search space. Our approach
exhibits superior ranking correlation and further reduces solving time compared
to other zero-shot NAS methods that require gradient computation.

</details>


### [32] [Few-shot Hate Speech Detection Based on the MindSpore Framework](https://arxiv.org/abs/2504.15987)
*Zhenkai Qin,Dongze Wu,Yuxin Liu,Guifang Yang*

Main category: cs.CL

TL;DR: MS-FSLHate, a prompt-enhanced neural framework, improves few-shot hate speech detection using learnable prompts, CNN-BiLSTM, and adversarial augmentation, outperforming baselines in precision, recall, and F1-score.


<details>
  <summary>Details</summary>
Motivation: Hate speech detection systems struggle in few-shot or low-resource settings due to reliance on large annotated datasets.

Method: Proposes MS-FSLHate with learnable prompt embeddings, CNN-BiLSTM backbone, attention pooling, and synonym-based adversarial augmentation.

Result: Outperforms baselines on HateXplain and HSOL datasets in precision, recall, and F1-score, showing high efficiency and scalability.

Conclusion: Combining prompt-based learning with adversarial augmentation enhances robustness in few-shot hate speech detection, suitable for resource-constrained environments.

Abstract: The proliferation of hate speech on social media poses a significant threat
to online communities, requiring effective detection systems. While deep
learning models have shown promise, their performance often deteriorates in
few-shot or low-resource settings due to reliance on large annotated corpora.
To address this, we propose MS-FSLHate, a prompt-enhanced neural framework for
few-shot hate speech detection implemented on the MindSpore deep learning
platform. The model integrates learnable prompt embeddings, a CNN-BiLSTM
backbone with attention pooling, and synonym-based adversarial data
augmentation to improve generalization. Experimental results on two benchmark
datasets-HateXplain and HSOL-demonstrate that our approach outperforms
competitive baselines in precision, recall, and F1-score. Additionally, the
framework shows high efficiency and scalability, suggesting its suitability for
deployment in resource-constrained environments. These findings highlight the
potential of combining prompt-based learning with adversarial augmentation for
robust and adaptable hate speech detection in few-shot scenarios.

</details>


### [33] [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
*Tom Zehle,Moritz Schlager,Timo Heiß,Matthias Feurer*

Main category: cs.CL

TL;DR: CAPO is a cost-aware prompt optimization algorithm that improves efficiency by integrating AutoML techniques, outperforming existing methods with better performance and lower costs.


<details>
  <summary>Details</summary>
Motivation: Current prompt optimization methods are expensive due to high LLM calls and input tokens, necessitating a more efficient solution.

Method: CAPO uses an evolutionary approach with LLMs as operators, incorporating racing and multi-objective optimization to balance performance and prompt length.

Result: CAPO outperforms state-of-the-art methods in 11/15 cases, achieving up to 21% improvement, with cost savings and robustness.

Conclusion: CAPO advances prompt optimization by enhancing cost-efficiency and accessibility.

Abstract: Large language models (LLMs) have revolutionized natural language processing
by solving a wide range of tasks simply guided by a prompt. Yet their
performance is highly sensitive to prompt formulation. While automated prompt
optimization addresses this challenge by finding optimal prompts, current
methods require a substantial number of LLM calls and input tokens, making
prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt
Optimization), an algorithm that enhances prompt optimization efficiency by
integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as
operators, incorporating racing to save evaluations and multi-objective
optimization to balance performance with prompt length. It jointly optimizes
instructions and few-shot examples while leveraging task descriptions for
improved robustness. Our extensive experiments across diverse datasets and LLMs
demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization
methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves
better performances already with smaller budgets, saves evaluations through
racing, and decreases average prompt length via a length penalty, making it
both cost-efficient and cost-aware. Even without few-shot examples, CAPO
outperforms its competitors and generally remains robust to initial prompts.
CAPO represents an important step toward making prompt optimization more
powerful and accessible by improving cost-efficiency.

</details>


### [34] [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
*Igor Rozhkov,Natalia Loukachevitch*

Main category: cs.CL

TL;DR: The paper describes the application of the Binder model for nested term extraction in the RuTermEval competition, achieving top results. It also explores nested term recognition from flat training data.


<details>
  <summary>Details</summary>
Motivation: To adapt the Binder model, proven for nested named entities, for nested term extraction and evaluate its performance in the RuTermEval competition.

Method: The Binder model is applied to extract nested terms. The study also investigates nested term recognition using flat training data without nested annotations.

Result: Achieved the best term recognition results in all three tracks of the RuTermEval competition. Demonstrated viability of proposed approaches for nested term extraction without nested labeling.

Conclusion: The Binder model and proposed methods are effective for nested term extraction, even without nested annotations in training data.

Abstract: In this paper, we describe our participation in the RuTermEval competition
devoted to extracting nested terms. We apply the Binder model, which was
previously successfully applied to the recognition of nested named entities, to
extract nested terms. We obtained the best results of term recognition in all
three tracks of the RuTermEval competition. In addition, we study the new task
of recognition of nested terms from flat training data annotated with terms
without nestedness. We can conclude that several approaches we proposed in this
work are viable enough to retrieve nested terms effectively without nested
labeling of them.

</details>


### [35] [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)
*Jingyu Zhang,Jiacan Yu,Marc Marone,Benjamin Van Durme,Daniel Khashabi*

Main category: cs.CL

TL;DR: BloomScrub is a simple, effective inference-time method for certified copyright takedown in LLMs, reducing infringement risk while preserving utility.


<details>
  <summary>Details</summary>
Motivation: Addressing overlooked worst-case copyright risks in LLMs, such as verbatim quotes from copyrighted sources, despite existing mitigation methods.

Method: BloomScrub interleaves quote detection with rewriting, using Bloom filters for scalable screening, and allows adaptive abstention for uncertifiable quotes.

Result: BloomScrub effectively reduces infringement risk, maintains utility, and adapts to enforcement stringency.

Conclusion: Lightweight, inference-time methods like BloomScrub can be highly effective for copyright prevention in LLMs.

Abstract: The exposure of large language models (LLMs) to copyrighted material during
pre-training raises concerns about unintentional copyright infringement post
deployment. This has driven the development of "copyright takedown" methods,
post-training approaches aimed at preventing models from generating content
substantially similar to copyrighted ones. While current mitigation approaches
are somewhat effective for average-case risks, we demonstrate that they
overlook worst-case copyright risks exhibits by the existence of long, verbatim
quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet
highly effective inference-time approach that provides certified copyright
takedown. Our method repeatedly interleaves quote detection with rewriting
techniques to transform potentially infringing segments. By leveraging
efficient data sketches (Bloom filters), our approach enables scalable
copyright screening even for large-scale real-world corpora. When quotes beyond
a length threshold cannot be removed, the system can abstain from responding,
offering certified risk reduction. Experimental results show that BloomScrub
reduces infringement risk, preserves utility, and accommodates different levels
of enforcement stringency with adaptive abstention. Our results suggest that
lightweight, inference-time methods can be surprisingly effective for copyright
prevention.

</details>


### [36] [LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement](https://arxiv.org/abs/2504.16053)
*Zhifan Ye,Kejing Xia,Yonggan Fu,Xin Dong,Jihoon Hong,Xiangchi Yuan,Shizhe Diao,Jan Kautz,Pavlo Molchanov,Yingyan Celine Lin*

Main category: cs.CL

TL;DR: LongMamba enhances Mamba models' long-context understanding by filtering critical tokens in global channels, improving performance without additional training.


<details>
  <summary>Details</summary>
Motivation: SSMs like Mamba underperform in long-context tasks compared to Transformers, despite their efficiency. LongMamba addresses this gap by optimizing global channels.

Method: LongMamba identifies and filters critical tokens in global channels to prevent memory decay, improving long-context performance.

Result: LongMamba significantly boosts Mamba models' long-context capabilities, outperforming previous methods without extra training.

Conclusion: LongMamba sets a new standard for efficient and accurate long-context understanding in SSMs, extending Mamba's operational range.

Abstract: State space models (SSMs) have emerged as an efficient alternative to
Transformer models for language modeling, offering linear computational
complexity and constant memory usage as context length increases. However,
despite their efficiency in handling long contexts, recent studies have shown
that SSMs, such as Mamba models, generally underperform compared to
Transformers in long-context understanding tasks. To address this significant
shortfall and achieve both efficient and accurate long-context understanding,
we propose LongMamba, a training-free technique that significantly enhances the
long-context capabilities of Mamba models. LongMamba builds on our discovery
that the hidden channels in Mamba can be categorized into local and global
channels based on their receptive field lengths, with global channels primarily
responsible for long-context capability. These global channels can become the
key bottleneck as the input context lengthens. Specifically, when input lengths
largely exceed the training sequence length, global channels exhibit
limitations in adaptively extend their receptive fields, leading to Mamba's
poor long-context performance. The key idea of LongMamba is to mitigate the
hidden state memory decay in these global channels by preventing the
accumulation of unimportant tokens in their memory. This is achieved by first
identifying critical tokens in the global channels and then applying token
filtering to accumulate only those critical tokens. Through extensive
benchmarking across synthetic and real-world long-context scenarios, LongMamba
sets a new standard for Mamba's long-context performance, significantly
extending its operational range without requiring additional training. Our code
is available at https://github.com/GATECH-EIC/LongMamba.

</details>


### [37] [Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability](https://arxiv.org/abs/2504.16056)
*Daniel Hendriks,Philipp Spitzer,Niklas Kühl,Gerhard Satzger*

Main category: cs.CL

TL;DR: The paper introduces new knowledge distillation methods for LLMs, comparing their performance and explainability on the CQA dataset.


<details>
  <summary>Details</summary>
Motivation: High computational demands of LLMs limit deployment in resource-constrained environments, necessitating efficient distillation methods.

Method: Applies critique-revision prompting for data generation and synthesizes existing methods for training, evaluated on the CQA dataset.

Result: Provides a systematic comparison of distillation methods, measuring accuracy and explainability via human-grounded study.

Conclusion: New distillation methods and their comparison advance the field, enhancing LLM applicability and diffusion.

Abstract: Artificial Intelligence (AI) has increasingly influenced modern society,
recently in particular through significant advancements in Large Language
Models (LLMs). However, high computational and storage demands of LLMs still
limit their deployment in resource-constrained environments. Knowledge
distillation addresses this challenge by training a small student model from a
larger teacher model. Previous research has introduced several distillation
methods for both generating training data and for training the student model.
Despite their relevance, the effects of state-of-the-art distillation methods
on model performance and explainability have not been thoroughly investigated
and compared. In this work, we enlarge the set of available methods by applying
critique-revision prompting to distillation for data generation and by
synthesizing existing methods for training. For these methods, we provide a
systematic comparison based on the widely used Commonsense Question-Answering
(CQA) dataset. While we measure performance via student model accuracy, we
employ a human-grounded study to evaluate explainability. We contribute new
distillation methods and their comparison in terms of both performance and
explainability. This should further advance the distillation of small language
models and, thus, contribute to broader applicability and faster diffusion of
LLM technology.

</details>


### [38] [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
*Ziqiao Ma,Jing Ding,Xuejun Zhang,Dezhi Luo,Jiahe Ding,Sihan Xu,Yuchen Huang,Run Peng,Joyce Chai*

Main category: cs.CL

TL;DR: The paper critiques current vision-language models (VLMs) for neglecting pragmatic aspects in Referring Expression Generation (REG), introduces a new dataset (RefOI), and highlights key pragmatic failures in VLMs.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of VLMs often ignore pragmatic competence, reducing REG to a simplistic task and overlooking Gricean maxims. The study aims to address this gap.

Method: The authors introduce RefOI, a dataset of 1.5k images annotated with referring expressions, and systematically evaluate state-of-the-art VLMs for pragmatic competence.

Result: Key pragmatic failures identified include inability to uniquely identify referents, excessive information, and misalignment with human preferences. Standard evaluations fail to detect these issues.

Conclusion: The study advocates for pragmatically informed models and evaluation frameworks to better align with human communication.

Abstract: Referring Expression Generation (REG) is a core task for evaluating the
pragmatic competence of vision-language systems, requiring not only accurate
semantic grounding but also adherence to principles of cooperative
communication (Grice, 1975). However, current evaluations of vision-language
models (VLMs) often overlook the pragmatic dimension, reducing REG to a
region-based captioning task and neglecting Gricean maxims. In this work, we
revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of
1.5k images annotated with both written and spoken referring expressions.
Through a systematic evaluation of state-of-the-art VLMs, we identify three key
failures of pragmatic competence: (1) failure to uniquely identify the
referent, (2) inclusion of excessive or irrelevant information, and (3)
misalignment with human pragmatic preference, such as the underuse of minimal
spatial cues. We also show that standard automatic evaluations fail to capture
these pragmatic violations, reinforcing superficial cues rather than genuine
referential success. Our findings call for a renewed focus on pragmatically
informed models and evaluation frameworks that align with real human
communication.

</details>


### [39] [A Python Tool for Reconstructing Full News Text from GDELT](https://arxiv.org/abs/2504.16063)
*A. Fronzetti Colladon,R. Vestrelli*

Main category: cs.CL

TL;DR: A novel method to reconstruct full-text news articles from GDELT's n-grams at minimal cost, addressing accessibility issues in news datasets.


<details>
  <summary>Details</summary>
Motivation: News data is vital for research but often costly or incomplete. This paper aims to provide a low-cost solution using GDELT.

Method: Uses Python to reconstruct full-text articles from GDELT Web News NGrams 3.0 by merging overlapping n-grams.

Result: Enables access to large-scale, structured news data for research, overcoming proprietary dataset limitations.

Conclusion: The approach improves news data accessibility, benefiting fields like economics, social science, and NLP.

Abstract: News data have become an essential resource across various disciplines,
including economics, finance, management, social sciences, and computer
science. Researchers leverage newspaper articles to study economic trends,
market dynamics, corporate strategies, public perception, political discourse,
and the evolution of public opinion. Additionally, news datasets have been
instrumental in training large-scale language models, with applications in
sentiment analysis, fake news detection, and automated news summarization.
Despite their significance, access to comprehensive news corpora remains a key
challenge. Many full-text news providers, such as Factiva and LexisNexis,
require costly subscriptions, while free alternatives often suffer from
incomplete data and transparency issues. This paper presents a novel approach
to obtaining full-text newspaper articles at near-zero cost by leveraging data
from the Global Database of Events, Language, and Tone (GDELT). Specifically,
we focus on the GDELT Web News NGrams 3.0 dataset, which provides
high-frequency updates of n-grams extracted from global online news sources. We
provide Python code to reconstruct full-text articles from these n-grams by
identifying overlapping textual fragments and intelligently merging them. Our
method enables researchers to access structured, large-scale newspaper data for
text analysis while overcoming the limitations of existing proprietary
datasets. The proposed approach enhances the accessibility of news data for
empirical research, facilitating applications in economic forecasting,
computational social science, and natural language processing.

</details>


### [40] [Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation](https://arxiv.org/abs/2504.16073)
*Zhiyuan Hu,Shiyun Xiong,Yifan Zhang,See-Kiong Ng,Anh Tuan Luu,Bo An,Shuicheng Yan,Bryan Hooi*

Main category: cs.CL

TL;DR: A method using process supervision by a reward model improves VLM performance in GUI tasks, achieving notable accuracy and success rate gains.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with complex GUI tasks due to black-box limitations, resource-heavy fine-tuning, and suboptimal trajectory-level evaluation.

Method: Proposes guiding VLM agents with process supervision via a reward model during inference, optimizing actions step-by-step.

Result: Achieves 3.4% higher single-step action accuracy in static environments and ~33% higher task success in dynamic ones.

Conclusion: Process supervision and trajectory reflection significantly enhance VLM performance in GUI navigation and control.

Abstract: Recent advancements in visual language models (VLMs) have notably enhanced
their capabilities in handling complex Graphical User Interface (GUI)
interaction tasks. Despite these improvements, current frameworks often
struggle to generate correct actions in challenging GUI environments.
State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source
VLMs for GUI tasks requires significant resources. Additionally, existing
trajectory-level evaluation and refinement techniques frequently fall short due
to delayed feedback and local optimization issues. To address these challenges,
we propose an approach that guides VLM agents with process supervision by a
reward model during GUI navigation and control at inference time. This guidance
allows the VLM agent to optimize actions at each inference step, thereby
improving performance in both static and dynamic environments. In particular,
our method demonstrates significant performance gains in three GUI navigation
tasks, achieving a 3.4% improvement in single step action accuracy for static
environments, along with a around 33% increase in task success rate in one
dynamic environment. With further integration of trajectory reflection and
retry mechanisms, we also demonstrate even greater enhancement in task success.

</details>


### [41] [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
*Shi Qiu,Shaoyang Guo,Zhuo-Yang Song,Yunbo Sun,Zeyu Cai,Jiashen Wei,Tianyu Luo,Yixuan Yin,Haoxu Zhang,Yi Hu,Chenyang Wang,Chencheng Tang,Haoling Chang,Qi Liu,Ziheng Zhou,Tianyu Zhang,Jingtian Zhang,Zhangyi Liu,Minghao Li,Yuku Zhang,Boxuan Jing,Xianqi Yin,Yutong Ren,Zizhuo Fu,Weike Wang,Xudong Tian,Anqi Lv,Laifu Man,Jianxiang Li,Feiyu Tao,Qihua Sun,Zhou Liang,Yushu Mu,Zhongxuan Li,Jing-Jun Zhang,Shutao Zhang,Xiaotian Li,Xingqi Xia,Jiawei Lin,Zheyu Shen,Jiahang Chen,Qiuhao Xiong,Binran Wang,Fengyuan Wang,Ziyang Ni,Bohan Zhang,Fan Cui,Changkun Shao,Qing-Hong Cao,Ming-xing Luo,Muhan Zhang,Hua Xing Zhu*

Main category: cs.CL

TL;DR: PHYBench is a new benchmark for evaluating LLMs' reasoning in physics, featuring 500 real-world problems and a novel EED Score metric. Results show LLMs lag behind humans.


<details>
  <summary>Details</summary>
Motivation: To assess and improve LLMs' reasoning in physical contexts by providing a high-quality, diverse benchmark.

Method: Created PHYBench with 500 physics problems across various topics and difficulty levels, and introduced the EED Score for evaluation.

Result: State-of-the-art LLMs perform significantly worse than human experts in physical reasoning.

Conclusion: PHYBench highlights LLMs' limitations in complex physics reasoning and provides a tool for future improvements.

Abstract: We introduce PHYBench, a novel, high-quality benchmark designed for
evaluating reasoning capabilities of large language models (LLMs) in physical
contexts. PHYBench consists of 500 meticulously curated physics problems based
on real-world physical scenarios, designed to assess the ability of models to
understand and reason about realistic physical processes. Covering mechanics,
electromagnetism, thermodynamics, optics, modern physics, and advanced physics,
the benchmark spans difficulty levels from high school exercises to
undergraduate problems and Physics Olympiad challenges. Additionally, we
propose the Expression Edit Distance (EED) Score, a novel evaluation metric
based on the edit distance between mathematical expressions, which effectively
captures differences in model reasoning processes and results beyond
traditional binary scoring methods. We evaluate various LLMs on PHYBench and
compare their performance with human experts. Our results reveal that even
state-of-the-art reasoning models significantly lag behind human experts,
highlighting their limitations and the need for improvement in complex physical
reasoning scenarios. Our benchmark results and dataset are publicly available
at https://phybench-official.github.io/phybench-demo/.

</details>


### [42] [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
*Yuxin Zuo,Kaiyan Zhang,Shang Qu,Li Sheng,Xuekai Zhu,Biqing Qi,Youbang Sun,Ganqu Cui,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: TTRL is a novel RL method for training LLMs on unlabeled data, using majority voting for reward estimation, achieving significant performance boosts.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of reward estimation in RL for LLMs without labeled data, leveraging test-time scaling practices.

Method: Introduces TTRL, which uses majority voting (Maj@N) for reward estimation and leverages pre-trained model priors for self-evolution.

Result: TTRL improves performance, e.g., boosting Qwen-2.5-Math-7B's pass@1 by ~159% on AIME 2024, and approaches labeled-data-trained model performance.

Conclusion: TTRL is effective across tasks, demonstrating potential for broader applications in unlabeled data scenarios.

Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model, and
approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks, and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL

</details>


### [43] [Aggregating Soft Labels from Crowd Annotations Improves Uncertainty Estimation Under Distribution Shift](https://arxiv.org/abs/2212.09409)
*Dustin Wright,Isabelle Augenstein*

Main category: cs.CL

TL;DR: The paper conducts a large-scale study on learning from crowd labels in out-of-domain settings, analyzing 8 soft-labeling methods across 4 tasks, and proposes averaging soft-labels for consistent performance.


<details>
  <summary>Details</summary>
Motivation: Expert annotations are costly, and crowd-sourced labels may lack reliability. Existing work lacks comprehensive analysis of soft-labeling methods in out-of-domain settings and consistent performance across tasks.

Method: Systematically evaluates 8 soft-labeling methods on 4 language and vision tasks in out-of-domain settings. Proposes averaging soft-labels for aggregation.

Result: Averaging soft-labels improves predictive uncertainty estimation while maintaining performance. Method selection matters less with abundant or minimal data but is crucial for subjective labels and moderate data.

Conclusion: Aggregating soft-labels via averaging offers consistent performance and better uncertainty estimation, especially for subjective tasks with moderate data.

Abstract: Selecting an effective training signal for machine learning tasks is
difficult: expert annotations are expensive, and crowd-sourced annotations may
not be reliable. Recent work has demonstrated that learning from a distribution
over labels acquired from crowd annotations can be effective both for
performance and uncertainty estimation. However, this has mainly been studied
using a limited set of soft-labeling methods in an in-domain setting.
Additionally, no one method has been shown to consistently perform well across
tasks, making it difficult to know a priori which to choose. To fill these
gaps, this paper provides the first large-scale empirical study on learning
from crowd labels in the out-of-domain setting, systematically analyzing 8
soft-labeling methods on 4 language and vision tasks. Additionally, we propose
to aggregate soft-labels via a simple average in order to achieve consistent
performance across tasks. We demonstrate that this yields classifiers with
improved predictive uncertainty estimation in most settings while maintaining
consistent raw performance compared to learning from individual soft-labeling
methods or taking a majority vote of the annotations. We additionally highlight
that in regimes with abundant or minimal training data, the selection of soft
labeling method is less important, while for highly subjective labels and
moderate amounts of training data, aggregation yields significant improvements
in uncertainty estimation over individual methods. Code can be found at
https://github.com/copenlu/aggregating-crowd-annotations-ood.

</details>


### [44] [On the Low-Rank Parametrization of Reward Models for Controlled Language Generation](https://arxiv.org/abs/2407.04615)
*Sergey Troshin,Vlad Niculae,Antske Fokkens*

Main category: cs.CL

TL;DR: The paper revisits modular control of language models using external experts, comparing low-rank and higher-rank parametrizations. It introduces a simpler, efficient low-rank RAD method, matching higher-rank performance with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of controlling language models to avoid inappropriate content, focusing on efficient and effective guided decoding.

Method: Analyzes reward-augmented decoding (RAD) with higher-rank experts, then proposes a low-rank parametrization for efficiency.

Result: Low-rank RAD performs comparably to higher-rank RAD in detoxification and sentiment control tasks, with reduced computational cost.

Conclusion: Low-rank expert models offer a practical, efficient alternative to higher-rank ones for guided decoding without sacrificing performance.

Abstract: Language models trained on large amounts of data are known to produce
inappropriate content in some cases and require careful tuning to be used in
the real world. We revisit an effective and modular approach for
controllability of the language models, when an external expert model guides
the decoding. Particularly, we zoom in into the parametrization choice of an
external expert, highlighting the difference between low-rank and higher-rank
parametrizations. Higher-rank experts are designed to support high flexibility
when representing the rewards, leading to higher computational costs during
decoding. However, we demonstrate that they might not use their full
flexibility. By analyzing the recently proposed reward-augmented decoding
approach (RAD), which uses a higher-rank expert model, we introduce a simpler
but more efficient low-rank parametrization of the expert model enabling fast
and effective guided decoding. We empirically show that the low-rank RAD
performs on par with the more flexible RAD on a detoxification and a sentiment
control task, while requiring only a single reward model call per generated
token.

</details>


### [45] [Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation](https://arxiv.org/abs/2408.06276)
*Jieyong Kim,Hyunseo Kim,Hyunjin Cho,SeongKu Kang,Buru Chang,Jinyoung Yeo,Dongha Lee*

Main category: cs.CL

TL;DR: EXP3RT is a novel LLM-based recommender that leverages user and item reviews for enhanced preference reasoning, improving rating prediction and explainability.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based recommendation methods underutilize LLMs' reasoning capabilities and input information, limiting their potential.

Method: EXP3RT fine-tunes an LLM to extract preferences from reviews, create profiles, and perform reasoning-enhanced rating prediction.

Result: EXP3RT outperforms existing methods in rating prediction and reranking, while enhancing explainability.

Conclusion: EXP3RT effectively leverages LLMs for better recommendations and explanations, addressing prior limitations.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
exceptional performance across a wide range of tasks, generating significant
interest in their application to recommendation systems. However, existing
methods have not fully capitalized on the potential of LLMs, often constrained
by limited input information or failing to fully utilize their advanced
reasoning capabilities. To address these limitations, we introduce EXP3RT, a
novel LLM-based recommender designed to leverage rich preference information
contained in user and item reviews. EXP3RT is basically fine-tuned through
distillation from a teacher LLM to perform three key tasks in order: EXP3RT
first extracts and encapsulates essential subjective preferences from raw
reviews, aggregates and summarizes them according to specific criteria to
create user and item profiles. It then generates detailed step-by-step
reasoning followed by predicted rating, i.e., reasoning-enhanced rating
prediction, by considering both subjective and objective information from
user/item profiles and item descriptions. This personalized preference
reasoning from EXP3RT enhances rating prediction accuracy and also provides
faithful and reasonable explanations for recommendation. Extensive experiments
show that EXP3RT outperforms existing methods on both rating prediction and
candidate item reranking for top-k recommendation, while significantly
enhancing the explainability of recommendation systems.

</details>


### [46] [Open-World Evaluation for Retrieving Diverse Perspectives](https://arxiv.org/abs/2409.18110)
*Hung-Ting Chen,Eunsol Choi*

Main category: cs.CL

TL;DR: The paper introduces BERDS, a benchmark for evaluating document retrieval diversity on subjective questions, using a language model-based evaluator to assess perspective coverage. It tests retrievers on three corpora, finding current methods only cover all perspectives 40% of the time, and explores query expansion and reranking to improve diversity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of retrieving diverse perspectives on contentious questions, where traditional relevancy metrics (e.g., string matching) fail.

Method: Curates BERDS benchmark, uses a language model-based evaluator, and tests retrievers on Wikipedia, web snapshots, and dynamically constructed corpora. Explores query expansion and diversity-focused reranking.

Result: Existing retrievers cover all perspectives in only 40% of cases. Query expansion and reranking show potential but remain challenging.

Conclusion: Retrieving diverse perspectives is difficult; current methods are insufficient, but proposed approaches (query expansion, reranking) offer avenues for improvement.

Abstract: We study retrieving a set of documents that covers various perspectives on a
complex and contentious question (e.g., will ChatGPT do more harm than good?).
We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS),
where each example consists of a question and diverse perspectives associated
with the question, sourced from survey questions and debate websites. On this
data, retrievers paired with a corpus are evaluated to surface a document set
that contains diverse perspectives. Our framing diverges from most retrieval
tasks in that document relevancy cannot be decided by simple string matches to
references. Instead, we build a language model-based automatic evaluator that
decides whether each retrieved document contains a perspective. This allows us
to evaluate the performance of three different types of corpus (Wikipedia, web
snapshot, and corpus constructed on the fly with retrieved pages from the
search engine) paired with retrievers. Retrieving diverse documents remains
challenging, with the outputs from existing retrievers covering all
perspectives on only 40% of the examples. We further study the effectiveness of
query expansion and diversity-focused reranking approaches and analyze
retriever sycophancy.

</details>


### [47] [AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models](https://arxiv.org/abs/2410.02355)
*Junfeng Fang,Houcheng Jiang,Kun Wang,Yunshan Ma,Shi Jie,Xiang Wang,Xiangnan He,Tat-seng Chua*

Main category: cs.CL

TL;DR: AlphaEdit introduces a projection method to update LLM knowledge without disrupting preserved information, improving performance by 36.7%.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from hallucinations due to outdated knowledge, and current editing methods disrupt preserved knowledge.

Method: AlphaEdit projects perturbations onto the null space of preserved knowledge before applying them.

Result: Experiments show a 36.7% performance boost in LLMs like LLaMA3, GPT2-XL, and GPT-J.

Conclusion: AlphaEdit effectively mitigates knowledge disruption in LLMs during sequential editing.

Abstract: Large language models (LLMs) often exhibit hallucinations due to incorrect or
outdated knowledge. Hence, model editing methods have emerged to enable
targeted knowledge updates. To achieve this, a prevailing paradigm is the
locating-then-editing approach, which first locates influential parameters and
then edits them by introducing a perturbation. While effective, current studies
have demonstrated that this perturbation inevitably disrupt the originally
preserved knowledge within LLMs, especially in sequential editing scenarios. To
address this, we introduce AlphaEdit, a novel solution that projects
perturbation onto the null space of the preserved knowledge before applying it
to the parameters. We theoretically prove that this projection ensures the
output of post-edited LLMs remains unchanged when queried about the preserved
knowledge, thereby mitigating the issue of disruption. Extensive experiments on
various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts
the performance of most locating-then-editing methods by an average of 36.7%
with a single line of additional code for projection solely. Our code is
available at: https://github.com/jianghoucheng/AlphaEdit.

</details>


### [48] [SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2410.19503)
*Jahyun Koo,Yerin Hwang,Yongil Kim,Taegwan Kang,Hyunkyung Bae,Kyomin Jung*

Main category: cs.CL

TL;DR: SWITCH improves Knowledge Distillation by selectively incorporating teacher guidance to reduce noise and bias in student-generated outputs, especially for long sequences.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of noisy and biased student-generated outputs in Knowledge Distillation, which can misguide the teacher model.

Method: SWITCH identifies discrepancies in token probabilities between teacher and student models, enabling selective teacher intervention in long sequences.

Result: SWITCH outperforms traditional KD methods, particularly in generating long sequential data, across multiple model families and datasets.

Conclusion: SWITCH effectively mitigates teacher misguidance in Knowledge Distillation, enhancing performance for long sequences.

Abstract: Despite the success of Large Language Models (LLMs), they still face
challenges related to high inference costs and memory requirements. To address
these issues, Knowledge Distillation (KD) has emerged as a popular method for
model compression, with student-generated outputs (SGOs) as training data being
particularly notable for reducing the mismatch between training and inference.
However, SGOs often produce noisy and biased sequences, which can lead to
misguidance from the teacher model, especially in long sequences. To mitigate
these challenges, we propose SWITCH (Studying WIth TeaCHer for Knowledge
Distillation), a novel approach that strategically incorporates the teacher
model during the student's sequence generation. SWITCH identifies discrepancies
between the token probabilities of the teacher and student models, allowing the
teacher to intervene selectively, particularly in long sequences that are more
prone to teacher misguidance. Extensive experimental results across three model
families and five instruction-following datasets show that SWITCH surpasses
traditional KD methods, particularly excelling in the generation of long
sequential data.

</details>


### [49] [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223)
*Weiliang Zhao,Daniel Ben-Levi,Wei Hao,Junfeng Yang,Chengzhi Mao*

Main category: cs.CL

TL;DR: A new jailbreak technique exploits LLMs' context divergence to bypass safety constraints, outperforming existing methods with 62.83% higher success rates while using fewer queries.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in current LLM safety training and highlight the inadequacy of existing methods in eliminating risks.

Method: Instructs LLMs to deviate and obfuscate prior attacks, requiring fewer queries to compromise chatbots like GPT-4, Gemini, and Llama.

Result: Achieves up to 62.83% higher success rate in compromising leading chatbots, revealing flaws in safety training.

Conclusion: Urges a revolution in testing methodologies to ensure robust LLM security, as current methods may only mask vulnerabilities.

Abstract: We have uncovered a powerful jailbreak technique that leverages large
language models' ability to diverge from prior context, enabling them to bypass
safety constraints and generate harmful outputs. By simply instructing the LLM
to deviate and obfuscate previous attacks, our method dramatically outperforms
existing approaches, achieving up to a 62.83% higher success rate in
compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while
using only 12.9% of the queries. This revelation exposes a critical flaw in
current LLM safety training, suggesting that existing methods may merely mask
vulnerabilities rather than eliminate them. Our findings sound an urgent alarm
for the need to revolutionize testing methodologies to ensure robust and
reliable LLM security.

</details>


### [50] [Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree](https://arxiv.org/abs/2412.12639)
*Xiangxiang Gao,Weisheng Xie,Yiwei Xiang,Feng Ji*

Main category: cs.CL

TL;DR: Falcon introduces a semi-autoregressive speculative decoding framework to enhance LLM inference speed by balancing drafting latency and speculation accuracy, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: The challenge of balancing minimal drafting latency and high speculation accuracy in speculative decoding for LLMs motivates the development of Falcon.

Method: Falcon uses Coupled Sequential Glancing Distillation and a Custom-Designed Decoding Tree to improve drafter parallelism and output quality.

Result: Falcon achieves a 2.91x to 3.51x speedup on Vicuna and LLaMA2-Chat models, outperforming existing methods.

Conclusion: Falcon's compact yet effective design sets a new benchmark for speculative decoding in LLMs.

Abstract: Striking an optimal balance between minimal drafting latency and high
speculation accuracy to enhance the inference speed of Large Language Models
remains a significant challenge in speculative decoding. In this paper, we
introduce Falcon, an innovative semi-autoregressive speculative decoding
framework fashioned to augment both the drafter's parallelism and output
quality. Falcon incorporates the Coupled Sequential Glancing Distillation
technique, which fortifies inter-token dependencies within the same block,
leading to increased speculation accuracy. We offer a comprehensive theoretical
analysis to illuminate the underlying mechanisms. Additionally, we introduce a
Custom-Designed Decoding Tree, which permits the drafter to generate multiple
tokens in a single forward pass and accommodates multiple forward passes as
needed, thereby boosting the number of drafted tokens and significantly
improving the overall acceptance rate. Comprehensive evaluations on benchmark
datasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior
acceleration capabilities. The framework achieves a lossless speedup ratio
ranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model
series. These results outstrip existing speculative decoding methods for LLMs,
including Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact
drafter architecture equivalent to merely two Transformer layers.

</details>


### [51] [State Space Models are Strong Text Rerankers](https://arxiv.org/abs/2412.14354)
*Zhichao Xu,Jinghua Yan,Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: State space models (SSMs) like Mamba show promise for text reranking, achieving competitive performance to transformers but with efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies of transformers in inference and long-context tasks by exploring SSMs like Mamba for text reranking.

Method: Benchmark Mamba-1 and Mamba-2 against transformer models in text reranking, evaluating performance and efficiency.

Result: Mamba models match transformer performance but lag in efficiency; Mamba-2 outperforms Mamba-1.

Conclusion: SSMs like Mamba are viable transformer alternatives but need efficiency improvements for IR applications.

Abstract: Transformers dominate NLP and IR; but their inference inefficiencies and
challenges in extrapolating to longer contexts have sparked interest in
alternative model architectures. Among these, state space models (SSMs) like
Mamba offer promising advantages, particularly $O(1)$ time complexity in
inference. Despite their potential, SSMs' effectiveness at text reranking -- a
task requiring fine-grained query-document interaction and long-context
understanding -- remains underexplored. This study benchmarks SSM-based
architectures (specifically, Mamba-1 and Mamba-2) against transformer-based
models across various scales, architectures, and pre-training objectives,
focusing on performance and efficiency in text reranking tasks. We find that
(1) Mamba architectures achieve competitive text ranking performance,
comparable to transformer-based models of similar size; (2) they are less
efficient in training and inference compared to transformers with flash
attention; and (3) Mamba-2 outperforms Mamba-1 in both performance and
efficiency. These results underscore the potential of state space models as a
transformer alternative and highlight areas for improvement in future IR
applications.

</details>


### [52] [Fearful Falcons and Angry Llamas: Emotion Category Annotations of Arguments by Humans and LLMs](https://arxiv.org/abs/2412.15993)
*Lynn Greschner,Roman Klinger*

Main category: cs.CL

TL;DR: The paper explores how discrete emotion categories (e.g., anger) in arguments affect stance adaptation, using crowdsourced annotations and LLM-based labeling methods. It compares prompting strategies and output space definitions, finding high recall but low precision for negative emotions like anger and fear.


<details>
  <summary>Details</summary>
Motivation: To address the gap in studying discrete emotion categories (e.g., anger) in arguments, which influence stance adaptation, unlike binary emotionality.

Method: Crowdsourced subjective annotations of emotion categories in a German argument corpus, evaluated using LLM-based labeling (Falcon-7b-instruct, Llama-3.1-8B-instruct, GPT-4o-mini) with three prompting strategies (zero-shot, one-shot, chain-of-thought) and output space variations (binary, closed-domain, open-domain).

Result: Emotion categories improve emotionality prediction in arguments. Automatic predictions show high recall but low precision for anger and fear, indicating a bias toward negative emotions.

Conclusion: Discrete emotion annotations are crucial for understanding argument effects, with LLMs showing potential but bias toward negative emotions.

Abstract: Arguments evoke emotions, influencing the effect of the argument itself. Not
only the emotional intensity but also the category influence the argument's
effects, for instance, the willingness to adapt stances. While binary
emotionality has been studied in arguments, there is no work on discrete
emotion categories (e.g., "Anger") in such data. To fill this gap, we
crowdsource subjective annotations of emotion categories in a German argument
corpus and evaluate automatic LLM-based labeling methods. Specifically, we
compare three prompting strategies (zero-shot, one-shot, chain-of-thought) on
three large instruction-tuned language models (Falcon-7b-instruct,
Llama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the
output space to be binary (is there emotionality in the argument?),
closed-domain (which emotion from a given label set is in the argument?), or
open-domain (which emotion is in the argument?). We find that emotion
categories enhance the prediction of emotionality in arguments, emphasizing the
need for discrete emotion annotations in arguments. Across all prompt settings
and models, automatic predictions show a high recall but low precision for
predicting anger and fear, indicating a strong bias toward negative emotions.

</details>


### [53] [Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks](https://arxiv.org/abs/2502.13053)
*Yurun Chen,Xavier Hu,Keting Yin,Juncheng Li,Shengyu Zhang*

Main category: cs.CL

TL;DR: The paper introduces Active Environment Injection Attack (AEIA), a novel threat where attackers disguise malicious attacks as environmental elements to manipulate AI agents. It identifies two vulnerabilities in Android OS and proposes AEIA-MN to test MLLM-based agents, showing a 93% success rate.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked security concern of AI agents detecting impostors in their environment, particularly in operating systems like Android.

Method: Analyzes agents' operational context, identifies AEIA vulnerabilities, and proposes AEIA-MN to exploit these vulnerabilities in mobile OS.

Result: Demonstrates high vulnerability of advanced MLLMs to AEIA, with a 93% attack success rate on AndroidWorld.

Conclusion: AEIA poses a significant threat to AI agents, highlighting the need for improved security measures against such attacks.

Abstract: As researchers continue to optimize AI agents for more effective task
execution within operating systems, they often overlook a critical security
concern: the ability of these agents to detect "impostors" within their
environment. Through an analysis of the agents' operational context, we
identify a significant threat-attackers can disguise malicious attacks as
environmental elements, injecting active disturbances into the agents'
execution processes to manipulate their decision-making. We define this novel
threat as the Active Environment Injection Attack (AEIA). Focusing on the
interaction mechanisms of the Android OS, we conduct a risk assessment of AEIA
and identify two critical security vulnerabilities: (1) Adversarial content
injection in multimodal interaction interfaces, where attackers embed
adversarial instructions within environmental elements to mislead agent
decision-making; and (2) Reasoning gap vulnerabilities in the agent's task
execution process, which increase susceptibility to AEIA attacks during
reasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN,
an attack scheme that exploits interaction vulnerabilities in mobile operating
systems to assess the robustness of MLLM-based agents. Experimental results
show that even advanced MLLMs are highly vulnerable to this attack, achieving a
maximum attack success rate of 93% on the AndroidWorld benchmark by combining
two vulnerabilities.

</details>


### [54] [Parallel Corpora for Machine Translation in Low-resource Indic Languages: A Comprehensive Review](https://arxiv.org/abs/2503.04797)
*Rahul Raja,Arpita Vats*

Main category: cs.CL

TL;DR: A review of parallel corpora for Indic languages, highlighting their role in machine translation, challenges in corpus creation, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of high-quality bilingual data for low-resource Indic languages and improve multilingual MT systems.

Method: Categorizes and evaluates available parallel corpora, examining alignment quality, domain representativeness, and challenges like linguistic diversity and data imbalance.

Result: Identifies key challenges (e.g., script variation, noisy data) and evaluates corpora for their utility in MT.

Conclusion: Future directions include cross-lingual transfer learning, expanding datasets, and integrating multimodal resources to enhance translation quality.

Abstract: Parallel corpora play an important role in training machine translation (MT)
models, particularly for low-resource languages where high-quality bilingual
data is scarce. This review provides a comprehensive overview of available
parallel corpora for Indic languages, which span diverse linguistic families,
scripts, and regional variations. We categorize these corpora into
text-to-text, code-switched, and various categories of multimodal datasets,
highlighting their significance in the development of robust multilingual MT
systems. Beyond resource enumeration, we critically examine the challenges
faced in corpus creation, including linguistic diversity, script variation,
data scarcity, and the prevalence of informal textual content.We also discuss
and evaluate these corpora in various terms such as alignment quality and
domain representativeness. Furthermore, we address open challenges such as data
imbalance across Indic languages, the trade-off between quality and quantity,
and the impact of noisy, informal, and dialectal data on MT performance.
Finally, we outline future directions, including leveraging cross-lingual
transfer learning, expanding multilingual datasets, and integrating multimodal
resources to enhance translation quality. To the best of our knowledge, this
paper presents the first comprehensive review of parallel corpora specifically
tailored for low-resource Indic languages in the context of machine
translation.

</details>


### [55] [Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks](https://arxiv.org/abs/2503.09572)
*Lutfi Eren Erdogan,Nicholas Lee,Sehoon Kim,Suhong Moon,Hiroki Furuta,Gopala Anumanchipalli,Kurt Keutzer,Amir Gholami*

Main category: cs.CL

TL;DR: Plan-and-Act framework enhances LLM-based agents for complex tasks by integrating explicit planning and scalable synthetic data generation, achieving state-of-the-art performance in web navigation tasks.


<details>
  <summary>Details</summary>
Motivation: Applying LLMs to complex, multi-step tasks is challenging due to their lack of inherent planning capabilities.

Method: Proposes Plan-and-Act, a framework with a Planner for structured plans and an Executor for actions, trained using synthetic data generation.

Result: Achieves 57.58% success on WebArena-Lite and 81.36% on WebVoyager, setting new benchmarks.

Conclusion: Plan-and-Act effectively bridges the gap in LLM-based planning, demonstrating superior performance in long-horizon tasks.

Abstract: Large language models (LLMs) have shown remarkable advancements in enabling
language agents to tackle simple tasks. However, applying them for complex,
multi-step, long-horizon tasks remains a challenge. Recent work have found
success by separating high-level planning from low-level execution, which
enables the model to effectively balance high-level planning objectives and
low-level execution details. However, generating accurate plans remains
difficult since LLMs are not inherently trained for this task. To address this,
we propose Plan-and-Act, a novel framework that incorporates explicit planning
into LLM-based agents and introduces a scalable method to enhance plan
generation through a novel synthetic data generation method. Plan-and-Act
consists of a Planner model which generates structured, high-level plans to
achieve user goals, and an Executor model that translates these plans into
environment-specific actions. To train the Planner effectively, we introduce a
synthetic data generation method that annotates ground-truth trajectories with
feasible plans, augmented with diverse and extensive examples to enhance
generalization. We evaluate Plan-and-Act using web navigation as a
representative long-horizon planning environment, demonstrating a
state-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as
a text-only state-of-the-art 81.36% success rate on WebVoyager.

</details>


### [56] [Key, Value, Compress: A Systematic Exploration of KV Cache Compression Techniques](https://arxiv.org/abs/2503.11816)
*Neusha Javidnia,Bita Darvish Rouhani,Farinaz Koushanfar*

Main category: cs.CL

TL;DR: Analysis of KV cache compression strategies for efficient LLM inference, focusing on performance and latency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic computational cost growth in LLM attention as context length increases.

Method: Taxonomy and evaluation of KV cache compression strategies by principles and techniques.

Result: Identified trade-offs in KV cache compression affecting performance and latency in long-context scenarios.

Conclusion: Provides insights for more efficient LLM implementations through optimized KV cache compression.

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in
generating text, images, and video content. However, as context length grows,
the computational cost of attention increases quadratically with the number of
tokens, presenting significant efficiency challenges. This paper presents an
analysis of various Key-Value (KV) cache compression strategies, offering a
comprehensive taxonomy that categorizes these methods by their underlying
principles and implementation techniques. Furthermore, we evaluate their impact
on performance and inference latency, providing critical insights into their
effectiveness. Our findings highlight the trade-offs involved in KV cache
compression and its influence on handling long-context scenarios, paving the
way for more efficient LLM implementations.

</details>


### [57] [FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in Indigenous Languages](https://arxiv.org/abs/2504.00021)
*Rahul Raja,Arpita Vats*

Main category: cs.CL

TL;DR: The paper introduces FUSE, a winning MT evaluation metric combining Ridge regression and Gradient Boosting, outperforming traditional metrics like BLEU for Indigenous languages.


<details>
  <summary>Details</summary>
Motivation: Traditional MT metrics fail for Indigenous languages due to polysynthesis and complex morphology; FUSE addresses this gap.

Method: FUSE integrates lexical, phonetic, semantic, and fuzzy token similarity with Ridge regression and Gradient Boosting, using multilingual embeddings and phonological encodings.

Result: FUSE achieves higher Pearson and Spearman correlations with human judgments than conventional metrics.

Conclusion: FUSE provides a robust, linguistically informed solution for MT evaluation in low-resource settings.

Abstract: This paper presents the winning submission of the RaaVa team to the
AmericasNLP 2025 Shared Task 3 on Automatic Evaluation Metrics for Machine
Translation (MT) into Indigenous Languages of America, where our system ranked
first overall based on average Pearson correlation with the human annotations.
We introduce Feature-Union Scorer (FUSE) for Evaluation, FUSE integrates Ridge
regression and Gradient Boosting to model translation quality. In addition to
FUSE, we explore five alternative approaches leveraging different combinations
of linguistic similarity features and learning paradigms. FUSE Score highlights
the effectiveness of combining lexical, phonetic, semantic, and fuzzy token
similarity with learning-based modeling to improve MT evaluation for
morphologically rich and low-resource languages. MT into Indigenous languages
poses unique challenges due to polysynthesis, complex morphology, and
non-standardized orthography. Conventional automatic metrics such as BLEU, TER,
and ChrF often fail to capture deeper aspects like semantic adequacy and
fluency. Our proposed framework, formerly referred to as FUSE, incorporates
multilingual sentence embeddings and phonological encodings to better align
with human evaluation. We train supervised models on human-annotated
development sets and evaluate held-out test data. Results show that FUSE
consistently achieves higher Pearson and Spearman correlations with human
judgments, offering a robust and linguistically informed solution for MT
evaluation in low-resource settings.

</details>


### [58] [Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training](https://arxiv.org/abs/2504.01801)
*Zhijun Wang,Jiahuan Li,Hao Zhou,Rongxiang Weng,Jingang Wang,Xin Huang,Xue Han,Junlan Feng,Chao Deng,Shujian Huang*

Main category: cs.CL

TL;DR: The paper explores how code-switching in pre-training data enhances multilingual capabilities of LLMs, identifies four types of code-switching, and shows synthetic code-switching improves performance across languages.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs exhibit strong multilingual abilities despite imbalanced training data, focusing on the role of code-switching.

Method: Analyzes code-switching in pre-training data, categorizes it into four types, and tests synthetic code-switching's impact on performance.

Result: Synthetic code-switching significantly improves multilingual performance, benefiting high, medium, and low-resource languages.

Conclusion: Code-switching, especially when synthetically enhanced, is crucial for multilingual alignment and performance in LLMs.

Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities
despite the extreme language imbalance in the pre-training data. In this paper,
we closely examine the reasons behind this phenomenon, focusing on the
pre-training corpus. We find that the existence of code-switching, alternating
between different languages within a context, is key to multilingual
capabilities. We conduct an analysis to investigate code-switching in the
pre-training corpus, examining its presence and categorizing it into four types
within two quadrants. We then assess its impact on multilingual performance.
These types of code-switching data are unbalanced in proportions and
demonstrate different effects on facilitating language transfer. To better
explore the power of code-switching for language alignment during pre-training,
we investigate the strategy of synthetic code-switching. We continuously scale
up the synthetic code-switching data and observe remarkable improvements in
both benchmarks and representation space. Extensive experiments indicate that
incorporating synthetic code-switching data enables better language alignment
and generalizes well to high, medium, and low-resource languages with
pre-training corpora of varying qualities.

</details>


### [59] [Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance](https://arxiv.org/abs/2504.07989)
*Nirvan Patil,Malhar Abhay Inamdar,Agnivo Gosai,Guruprasad Pathak,Anish Joshi,Aryan Sagavekar,Anish Joshirao,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: The study extends TinyStories to Indian languages (Hindi, Marathi, Bengali), showing SLMs outperform LLMs with fewer parameters. Language-specific tokenizers and synthetic data enhance performance, revealing cross-linguistic patterns.


<details>
  <summary>Details</summary>
Motivation: To adapt SLMs for Indian languages, addressing regional language processing and linguistic complexity with fewer parameters than LLMs.

Method: Translated TinyStories dataset into Hindi, Marathi, Bengali; created synthetic data using LLMs; evaluated SLMs with language-specific tokenizers.

Result: SLMs process regional languages efficiently; Hindi models outperform Marathi and Bengali; synthetic data beats translated content.

Conclusion: SLMs offer practical solutions for underserved languages, advancing theoretical understanding of neural language development.

Abstract: Small Language Models (SLMs) offer efficient alternatives to LLMs for
specific domains. The 2023 TinyStories study developed an English dataset that
allows SLMs with 1 to 10 million parameters to produce coherent outputs. Our
research expands this framework by translating the original dataset into Indian
languages and creating synthetic data using LLMs. We focus on Hindi, Marathi,
and Bengali, evaluating SLMs for regional language processing and understanding
linguistic complexity. We show that SLMs efficiently process regional languages
with significantly fewer parameters than LLMs, providing a complementary
framework for ``inference based evaluation" of tokenization strategies and
linguistic complexity. Our analysis shows that language-specific tokenizers
outperform general-purpose ones for Indian languages. Empirical validations,
supported by information-theoretic and morphological analyses, provides
fundamental understanding behind the better performance of Hindi models over
Marathi and Bengali. Additionally, we show that synthetic datasets outperform
translated content for training SLMs. Correlation analyses reveal
cross-linguistic patterns and language-specific relationships between
creativity, grammatical precision, and narrative completeness. These findings
advance both the practical application of SLMs to underserved languages and our
theoretical understanding of neural language development.

</details>


### [60] [Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol](https://arxiv.org/abs/2504.10284)
*Weiqi Wang,Jiefu Ou,Yangqiu Song,Benjamin Van Durme,Daniel Khashabi*

Main category: cs.CL

TL;DR: The paper introduces ARXIV2TABLE, a benchmark for generating literature review tables from scientific papers, addressing challenges like under-specified user prompts, irrelevant content, and shallow evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To improve literature review table generation by tackling real-world complexities and enhancing utility for information-seeking tasks.

Method: Combines LLM-based methods and human annotations to address key challenges, introducing a novel benchmark (ARXIV2TABLE) for evaluation.

Result: Experiments show both open-weight and proprietary LLMs struggle with the task, emphasizing its difficulty.

Conclusion: The work highlights the need for further advancements in literature review table generation and provides a reproducible benchmark for future research.

Abstract: Literature review tables are essential for summarizing and comparing
collections of scientific papers. We explore the task of generating tables that
best fulfill a user's informational needs given a collection of scientific
papers. Building on recent work (Newman et al., 2024), we extend prior
approaches to address real-world complexities through a combination of
LLM-based methods and human annotations. Our contributions focus on three key
challenges encountered in real-world use: (i) User prompts are often
under-specified; (ii) Retrieved candidate papers frequently contain irrelevant
content; and (iii) Task evaluation should move beyond shallow text similarity
techniques and instead assess the utility of inferred tables for
information-seeking tasks (e.g., comparing papers). To support reproducible
evaluation, we introduce ARXIV2TABLE, a more realistic and challenging
benchmark for this task, along with a novel approach to improve literature
review table generation in real-world scenarios. Our extensive experiments on
this benchmark show that both open-weight and proprietary LLMs struggle with
the task, highlighting its difficulty and the need for further advancements.
Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.

</details>


### [61] [LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA](https://arxiv.org/abs/2504.11972)
*Xanh Ho,Jiahao Huang,Florian Boudin,Akiko Aizawa*

Main category: cs.CL

TL;DR: LLM-as-a-judge outperforms traditional EM/F1 metrics in evaluating QA models, showing higher correlation with human judgments (0.85 vs. 0.22/0.40).


<details>
  <summary>Details</summary>
Motivation: Traditional metrics (EM/F1) inadequately capture QA model performance, prompting exploration of LLM-as-a-judge for better evaluation.

Method: Reassessed QA model performance using LLM-as-a-judge across four datasets, testing different LLM families and answer types.

Result: LLM-as-a-judge achieved 0.85 correlation with human judgments, significantly outperforming EM/F1. No bias issues were observed.

Conclusion: LLM-as-a-judge is a superior alternative to EM/F1 for QA evaluation, though challenges remain with difficult answer types.

Abstract: Extractive reading comprehension question answering (QA) datasets are
typically evaluated using Exact Match (EM) and F1-score, but these metrics
often fail to fully capture model performance. With the success of large
language models (LLMs), they have been employed in various tasks, including
serving as judges (LLM-as-a-judge). In this paper, we reassess the performance
of QA models using LLM-as-a-judge across four reading comprehension QA
datasets. We examine different families of LLMs and various answer types to
evaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show
that LLM-as-a-judge is highly correlated with human judgments and can replace
traditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human
judgments improves significantly, from 0.22 (EM) and 0.40 (F1-score) to 0.85.
These findings confirm that EM and F1 metrics underestimate the true
performance of the QA models. While LLM-as-a-judge is not perfect for more
difficult answer types (e.g., job), it still outperforms EM/F1, and we observe
no bias issues, such as self-preference, when the same model is used for both
the QA and judgment tasks.

</details>


### [62] [ConExion: Concept Extraction with Large Language Models](https://arxiv.org/abs/2504.12915)
*Ebrahim Norouzi,Sven Hertling,Harald Sack*

Main category: cs.CL

TL;DR: The paper presents a method for extracting all domain-related concepts from documents using pre-trained LLMs, outperforming state-of-the-art techniques in F1 score and exploring unsupervised extraction via prompts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting all domain-related concepts, not just keyphrases, and to support ontology evaluation and learning.

Method: Uses pre-trained large language models (LLMs) for concept extraction, including prompt-based unsupervised techniques.

Result: Improves F1 score over state-of-the-art methods on benchmark datasets.

Conclusion: LLMs are effective for comprehensive concept extraction, aiding ontology tasks; code and datasets are publicly available.

Abstract: In this paper, an approach for concept extraction from documents using
pre-trained large language models (LLMs) is presented. Compared with
conventional methods that extract keyphrases summarizing the important
information discussed in a document, our approach tackles a more challenging
task of extracting all present concepts related to the specific domain, not
just the important ones. Through comprehensive evaluations of two widely used
benchmark datasets, we demonstrate that our method improves the F1 score
compared to state-of-the-art techniques. Additionally, we explore the potential
of using prompts within these models for unsupervised concept extraction. The
extracted concepts are intended to support domain coverage evaluation of
ontologies and facilitate ontology learning, highlighting the effectiveness of
LLMs in concept extraction tasks. Our source code and datasets are publicly
available at https://github.com/ISE-FIZKarlsruhe/concept_extraction.

</details>


### [63] [Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
*ByteDance Seed,:,Jiaze Chen,Tiantian Fan,Xin Liu,Lingjun Liu,Zhiqi Lin,Mingxuan Wang,Chengyi Wang,Xiangpeng Wei,Wenyuan Xu,Yufeng Yuan,Yu Yue,Lin Yan,Qiying Yu,Xiaochen Zuo,Chi Zhang,Ruofei Zhu,Zhecheng An,Zhihao Bai,Yu Bao,Xingyan Bin,Jiangjie Chen,Feng Chen,Hongmin Chen,Riwei Chen,Liangqiang Chen,Zixin Chen,Jinsong Chen,Siyan Chen,Kaiyuan Chen,Zhi Chen,Jin Chen,Jiecao Chen,Jinxin Chi,Weinan Dai,Ning Dai,Jiahui Dai,Shihan Dou,Yantao Du,Zhengyin Du,Jianhui Duan,Chen Dun,Ting-Han Fan,Jiazhan Feng,Junda Feng,Ziyuan Feng,Yuwei Fu,Wenqi Fu,Hanjie Fu,Hao Ge,Hongyi Guo,Mingji Han,Li Han,Wenhao Hao,Xintong Hao,Qianyu He,Jerry He,Feng He,Wen Heng,Zehua Hong,Qi Hou,Liang Hu,Shengding Hu,Nan Hu,Kai Hua,Qi Huang,Ziyue Huang,Hongzhi Huang,Zihao Huang,Ting Huang,Wenhao Huang,Wei Jia,Bin Jia,Xiaoying Jia,Yuhua Jiang,Haobin Jiang,Ziheng Jiang,Kaihua Jiang,Chengquan Jiang,Jianpeng Jiao,Xiaoran Jin,Xing Jin,Xunhao Lai,Zheng Li,Xiang Li,Liyi Li,Hongkai Li,Zheng Li,Shengxian Wan,Ya Wang,Yunshui Li,Chenggang Li,Niuniu Li,Siyu Li,Xi Li,Xiao Li,Aoyan Li,Yuntao Li,Nianning Liang,Xinnian Liang,Haibin Lin,Weijian Lin,Ye Lin,Zhicheng Liu,Guanlin Liu,Guanlin Liu,Chenxiao Liu,Yan Liu,Gaohong Liu,Juncai Liu,Chundian Liu,Deyi Liu,Kaibo Liu,Siyao Liu,Qi Liu,Yongfei Liu,Kang Liu,Gan Liu,Boyi Liu,Rui Long,Weiqiang Lou,Chenwei Lou,Xiang Luo,Yao Luo,Caiping Lv,Heyang Lv,Bole Ma,Qianli Ma,Hongzhi Ma,Yiyuan Ma,Jin Ma,Wenchang Ma,Tingting Ma,Chen Mao,Qiyang Min,Zhe Nan,Guanghan Ning,Jinxiang Ou,Haojie Pan,Renming Pang,Yanghua Peng,Tao Peng,Lihua Qian,Lihua Qian,Mu Qiao,Meng Qu,Cheng Ren,Hongbin Ren,Yong Shan,Wei Shen,Ke Shen,Kai Shen,Guangming Sheng,Jinlong Shi,Wenlei Shi,Guang Shi,Shuai Shuai Cao,Yuxin Song,Zuquan Song,Jing Su,Yifan Sun,Tao Sun,Zewei Sun,Borui Wan,Zihan Wang,Xiaohui Wang,Xi Wang,Shuguang Wang,Jun Wang,Qinlong Wang,Chenyuan Wang,Shuai Wang,Zihan Wang,Changbao Wang,Jiaqiang Wang,Shihang Wang,Xuwu Wang,Zaiyuan Wang,Yuxuan Wang,Wenqi Wang,Taiqing Wang,Chengzhi Wei,Houmin Wei,Ziyun Wei,Shufa Wei,Zheng Wu,Yonghui Wu,Yangjun Wu,Bohong Wu,Shuang Wu,Jingqiao Wu,Ning Wu,Shuangzhi Wu,Jianmin Wu,Chenguang Xi,Fan Xia,Yuqiao Xian,Liang Xiang,Boren Xiang,Bowen Xiao,Zhen Xiao,Xia Xiao,Yongsheng Xiao,Chao Xin,Shulin Xin,Yuwen Xiong,Jingjing Xu,Ziwen Xu,Chenyin Xu,Jiayi Xu,Yifan Xu,Wei Xu,Yufei Xu,Shikun Xu,Shipeng Yan,Shen Yan,Qingping Yang,Xi Yang,Tianhao Yang,Yuehang Yang,Yuan Yang,Ximing Yang,Zeyu Yang,Guang Yang,Yifan Yang,Xuesong Yao,Bairen Yi,Fan Yin,Jianian Yin,Ziqiang Ying,Xiangyu Yu,Hongli Yu,Song Yu,Menghan Yu,Huan Yu,Siyu Yuan,Jun Yuan,Yutao Zeng,Tianyang Zhan,Zheng Zhang,Yun Zhang,Mofan Zhang,Wang Zhang,Ru Zhang,Zhi Zhang,Tianqi Zhang,Xinyi Zhang,Zhexi Zhang,Sijun Zhang,Wenqiang Zhang,Xiangxiang Zhang,Yongtao Zhang,Yuyu Zhang,Ge Zhang,He Zhang,Yue Zhang,Renjie Zheng,Ningxin Zheng,Zhuolin Zheng,Yaowei Zheng,Chen Zheng,Xiaoyun Zhi,Wanjun Zhong,Cheng Zhong,Zheng Zhong,Baoquan Zhong,Xun Zhou,Na Zhou,Huan Zhou,Hang Zhu,Defa Zhu,Wenjia Zhu,Lei Zuo*

Main category: cs.CL

TL;DR: Seed-Thinking-v1.5 is a reasoning model with strong performance in STEM and coding tasks, outperforming benchmarks like AIME 2024 and Codeforces. It also generalizes well to non-reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a model capable of advanced reasoning and generalization across diverse domains, improving upon existing state-of-the-art methods.

Method: Uses a Mixture-of-Experts (MoE) architecture with 20B activated and 200B total parameters, emphasizing thinking before responding.

Result: Achieves 86.7 on AIME 2024, 55.0 on Codeforces, and 77.3 on GPQA, with an 8% higher win rate than DeepSeek R1 on non-reasoning tasks.

Conclusion: Seed-Thinking-v1.5 demonstrates superior reasoning and generalization, supported by new benchmarks (BeyondAIME and Codeforces) for future research.

Abstract: We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before
responding, resulting in improved performance on a wide range of benchmarks.
Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on
GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond
reasoning tasks, the method demonstrates notable generalization across diverse
domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on
non-reasoning tasks, indicating its broader applicability. Compared to other
state-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts
(MoE) model with a relatively small size, featuring 20B activated and 200B
total parameters. As part of our effort to assess generalized reasoning, we
develop two internal benchmarks, BeyondAIME and Codeforces, both of which will
be publicly released to support future research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [64] [LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation](https://arxiv.org/abs/2504.15309)
*Anran Yu,Wei Feng,Yaochen Zhang,Xiang Li,Lei Meng,Lei Wu,Xiangxu Meng*

Main category: cs.CV

TL;DR: Proposes style refinement and content preservation strategies for personalized text-to-image generation, improving stylization and textual controllability.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with insufficient stylization and inaccurate image content due to reduced textual controllability.

Method: Uses style refinement (optimizing style embeddings via visual reasoning prompts and reference images) and content preservation (maintaining model generalization).

Result: Achieves superior performance in generating consistent and personalized text-to-image outputs.

Conclusion: The approach effectively balances stylization and textual controllability.

Abstract: The personalized text-to-image generation has rapidly advanced with the
emergence of Stable Diffusion. Existing methods, which typically fine-tune
models using embedded identifiers, often struggle with insufficient stylization
and inaccurate image content due to reduced textual controllability. In this
paper, we propose style refinement and content preservation strategies. The
style refinement strategy leverages the semantic information of visual
reasoning prompts and reference images to optimize style embeddings, allowing a
more precise and consistent representation of style information. The content
preservation strategy addresses the content bias problem by preserving the
model's generalization capabilities, ensuring enhanced textual controllability
without compromising stylization. Experimental results verify that our approach
achieves superior performance in generating consistent and personalized
text-to-image outputs.

</details>


### [65] [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)
*Zhiqiu Lin,Siyuan Cen,Daniel Jiang,Jay Karhade,Hewei Wang,Chancharik Mitra,Tiffany Ling,Yuhan Huang,Sifan Liu,Mingyu Chen,Rushikesh Zawar,Xue Bai,Yilun Du,Chuang Gan,Deva Ramanan*

Main category: cs.CV

TL;DR: CameraBench is a dataset and benchmark for camera motion understanding, featuring expert-annotated videos and a taxonomy of motion primitives. It evaluates SfM and VLMs, and fine-tunes a generative VLM for improved performance.


<details>
  <summary>Details</summary>
Motivation: To advance camera motion understanding by providing a large-scale, high-quality dataset and benchmark, addressing gaps in existing methods.

Method: Created CameraBench with ~3,000 annotated videos, a taxonomy of motion primitives, and conducted human studies to assess annotation accuracy. Evaluated SfM and VLMs, then fine-tuned a generative VLM.

Result: SfM models struggle with semantic primitives, while VLMs struggle with geometric primitives. Fine-tuning a generative VLM improved performance across tasks like captioning and retrieval.

Conclusion: CameraBench, with its taxonomy and tutorials, aims to drive future research in understanding camera motions, bridging gaps between semantic and geometric understanding.

Abstract: We introduce CameraBench, a large-scale dataset and benchmark designed to
assess and improve camera motion understanding. CameraBench consists of ~3,000
diverse internet videos, annotated by experts through a rigorous multi-stage
quality control process. One of our contributions is a taxonomy of camera
motion primitives, designed in collaboration with cinematographers. We find,
for example, that some motions like "follow" (or tracking) require
understanding scene content like moving subjects. We conduct a large-scale
human study to quantify human annotation performance, revealing that domain
expertise and tutorial-based training can significantly enhance accuracy. For
example, a novice may confuse zoom-in (a change of intrinsics) with translating
forward (a change of extrinsics), but can be trained to differentiate the two.
Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language
Models (VLMs), finding that SfM models struggle to capture semantic primitives
that depend on scene content, while VLMs struggle to capture geometric
primitives that require precise estimation of trajectories. We then fine-tune a
generative VLM on CameraBench to achieve the best of both worlds and showcase
its applications, including motion-augmented captioning, video question
answering, and video-text retrieval. We hope our taxonomy, benchmark, and
tutorials will drive future efforts towards the ultimate goal of understanding
camera motions in any video.

</details>


### [66] [LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception](https://arxiv.org/abs/2504.15362)
*Yuan-Hong Liao,Sven Elflein,Liu He,Laura Leal-Taixé,Yejin Choi,Sanja Fidler,David Acuna*

Main category: cs.CV

TL;DR: LongPerceptualThoughts introduces a synthetic dataset for perceptual tasks, improving reasoning performance in vision and text benchmarks.


<details>
  <summary>Details</summary>
Motivation: Explore the benefits of long chain-of-thoughts in perceptual tasks, where system-1 reasoning is typically sufficient.

Method: A three-stage framework synthesizes verifiable questions, extracts simple CoTs, and expands them into long thoughts using frontier models.

Result: +3.4 points improvement on vision benchmarks, +11.8 on V$^*$ Bench, and +2 on MMLU-Pro.

Conclusion: LongPerceptualThoughts effectively enhances reasoning in perceptual and text tasks.

Abstract: Recent reasoning models through test-time scaling have demonstrated that long
chain-of-thoughts can unlock substantial performance boosts in hard reasoning
tasks such as math and code. However, the benefit of such long thoughts for
system-2 reasoning is relatively less explored in other domains such as
perceptual tasks where shallower, system-1 reasoning seems sufficient. In this
paper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K
long-thought traces for perceptual tasks. The key challenges in synthesizing
elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models
are not yet equipped with such thinking behavior and that it is not
straightforward to build a reliable process verifier for perceptual tasks.
Thus, we propose a novel three-stage data synthesis framework that first
synthesizes verifiable multiple-choice questions from dense image descriptions,
then extracts simple CoTs from VLMs for those verifiable problems, and finally
expands those simple thoughts to elaborate long thoughts via frontier reasoning
models. In controlled experiments with a strong instruction-tuned 7B model, we
demonstrate notable improvements over existing visual reasoning data-generation
methods. Our model, trained on the generated dataset, achieves an average +3.4
points improvement over 5 vision-centric benchmarks, including +11.8 points on
V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves
performance on the text reasoning benchmark, MMLU-Pro, by +2 points.

</details>


### [67] [Event2Vec: Processing neuromorphic events directly by representations in vector space](https://arxiv.org/abs/2504.15371)
*Wei Fang,Priyadarshini Panda*

Main category: cs.CV

TL;DR: The paper introduces 'event2vec,' a novel representation for neuromorphic event cameras, inspired by word embeddings, to address compatibility issues with traditional vision methods. It shows improved efficiency, accuracy, and speed in classification tasks.


<details>
  <summary>Details</summary>
Motivation: Event cameras output sparse, irregular data incompatible with mainstream vision methods. Existing solutions compromise temporal resolution or parallel computation.

Method: Proposes 'event2vec,' an event-to-vector representation, drawing parallels between words and events. Validated on ASL-DVS dataset classification.

Result: Demonstrates superior parameter efficiency, accuracy, and speed compared to graph/image/voxel-based methods. Aligns events with NLP for multimodal integration.

Conclusion: event2vec offers a promising approach for integrating event data into large language and multimodal models, with open-source resources provided.

Abstract: The neuromorphic event cameras have overwhelming advantages in temporal
resolution, power efficiency, and dynamic range compared to traditional
cameras. However, the event cameras output asynchronous, sparse, and irregular
events, which are not compatible with mainstream computer vision and deep
learning methods. Various methods have been proposed to solve this issue but at
the cost of long preprocessing procedures, losing temporal resolutions, or
being incompatible with massively parallel computation. Inspired by the great
success of the word to vector, we summarize the similarities between words and
events, then propose the first event to vector (event2vec) representation. We
validate event2vec on classifying the ASL-DVS dataset, showing impressive
parameter efficiency, accuracy, and speed than previous graph/image/voxel-based
representations. Beyond task performance, the most attractive advantage of
event2vec is that it aligns events to the domain of natural language
processing, showing the promising prospect of integrating events into large
language and multimodal models. Our codes, models, and training logs are
available at https://github.com/fangwei123456/event2vec.

</details>


### [68] [Physics Driven Image Simulation from Commercial Satellite Imagery](https://arxiv.org/abs/2504.15378)
*Scott Sorensen,Wayne Treible,Robert Wagner,Andrew D. Gilliam,Todd Rovito,Joseph L. Mundy*

Main category: cs.CV

TL;DR: Automated generation of physically realistic 3D scenes from satellite imagery, bypassing lidar, for high-fidelity simulations.


<details>
  <summary>Details</summary>
Motivation: To create realistic simulations of real-world locations with minimal manual effort, enabling applications in algorithm development and imagery processing.

Method: Uses satellite imagery and a Digital Surface Model to construct scene geometry, estimate materials, and populate dynamic elements like vegetation and vehicles.

Result: Produces high-fidelity 3D scenes without lidar, suitable for diverse imagery ranges (UV to LWIR).

Conclusion: The method enables efficient, realistic scene simulation for novel locations, reducing manual overhead and expanding simulation capabilities.

Abstract: Physics driven image simulation allows for the modeling and creation of
realistic imagery beyond what is afforded by typical rendering pipelines. We
aim to automatically generate a physically realistic scene for simulation of a
given region using satellite imagery to model the scene geometry, drive
material estimates, and populate the scene with dynamic elements. We present
automated techniques to utilize satellite imagery throughout the simulated
scene to expedite scene construction and decrease manual overhead. Our
technique does not use lidar, enabling simulations that could not be
constructed previously. To develop a 3D scene, we model the various components
of the real location, addressing the terrain, modelling man-made structures,
and populating the scene with smaller elements such as vegetation and vehicles.
To create the scene we begin with a Digital Surface Model, which serves as the
basis for scene geometry, and allows us to reason about the real location in a
common 3D frame of reference. These simulated scenes can provide increased
fidelity with less manual intervention for novel locations on earth, and can
facilitate algorithm development, and processing pipelines for imagery ranging
from UV to LWIR $(200nm-20\mu m)$.

</details>


### [69] [Plug-and-Play Versatile Compressed Video Enhancement](https://arxiv.org/abs/2504.15380)
*Huimin Zeng,Jiacheng Li,Zhiwei Xiong*

Main category: cs.CV

TL;DR: A codec-aware enhancement framework improves compressed video quality by reusing codec information, aiding downstream vision tasks without computational overhead.


<details>
  <summary>Details</summary>
Motivation: Video compression reduces file sizes but degrades visual quality, challenging downstream vision models. This work aims to enhance compressed videos adaptively.

Method: The framework uses a compression-aware adaptation (CAA) network to estimate parameters for a bitstream-aware enhancement (BAE) network, leveraging temporal and spatial priors.

Result: The framework outperforms existing enhancement methods and supports multiple downstream tasks as a plug-and-play module.

Conclusion: The proposed framework effectively enhances compressed video quality and is versatile for various vision tasks.

Abstract: As a widely adopted technique in data transmission, video compression
effectively reduces the size of files, making it possible for real-time cloud
computing. However, it comes at the cost of visual quality, posing challenges
to the robustness of downstream vision models. In this work, we present a
versatile codec-aware enhancement framework that reuses codec information to
adaptively enhance videos under different compression settings, assisting
various downstream vision tasks without introducing computation bottleneck.
Specifically, the proposed codec-aware framework consists of a
compression-aware adaptation (CAA) network that employs a hierarchical
adaptation mechanism to estimate parameters of the frame-wise enhancement
network, namely the bitstream-aware enhancement (BAE) network. The BAE network
further leverages temporal and spatial priors embedded in the bitstream to
effectively improve the quality of compressed input frames. Extensive
experimental results demonstrate the superior quality enhancement performance
of our framework over existing enhancement methods, as well as its versatility
in assisting multiple downstream tasks on compressed videos as a plug-and-play
module. Code and models are available at
https://huimin-zeng.github.io/PnP-VCVE/.

</details>


### [70] [ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images](https://arxiv.org/abs/2504.15384)
*Chen Zhao,Anjum Shaik,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Qiuying Sha,Hui Shen,Hong-Wen Deng,Weihua Zhou*

Main category: cs.CV

TL;DR: ICGM-FRAX uses graph matching on DXA images to predict hip fracture risk with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Early detection of hip fracture risk in elderly individuals is critical for intervention.

Method: Transform DXA images into graphs of radiomic features and iteratively match test graphs to fracture templates.

Result: Achieved a sensitivity of 0.9869 on 547 UK Biobank subjects.

Conclusion: ICGM-FRAX is highly accurate for hip fracture risk assessment.

Abstract: Hip fractures represent a major health concern, particularly among the
elderly, often leading decreased mobility and increased mortality. Early and
accurate detection of at risk individuals is crucial for effective
intervention. In this study, we propose Iterative Cross Graph Matching for Hip
Fracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip
fractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX
involves iteratively comparing a test (subject) graph with multiple template
graphs representing the characteristics of hip fracture subjects to assess the
similarity and accurately to predict hip fracture risk. These graphs are
obtained as follows. The DXA images are separated into multiple regions of
interest (RoIs), such as the femoral head, shaft, and lesser trochanter.
Radiomic features are then calculated for each RoI, with the central
coordinates used as nodes in a graph. The connectivity between nodes is
established according to the Euclidean distance between these coordinates. This
process transforms each DXA image into a graph, where each node represents a
RoI, and edges derived by the centroids of RoIs capture the spatial
relationships between them. If the test graph closely matches a set of template
graphs representing subjects with incident hip fractures, it is classified as
indicating high hip fracture risk. We evaluated our method using 547 subjects
from the UK Biobank dataset, and experimental results show that ICGM-FRAX
achieved a sensitivity of 0.9869, demonstrating high accuracy in predicting hip
fractures.

</details>


### [71] [MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World](https://arxiv.org/abs/2504.15397)
*Ankit Dhiman,Manan Shah,R Venkatesh Babu*

Main category: cs.CV

TL;DR: The paper introduces MirrorFusion 2.0, a diffusion-based model for generating photorealistic mirror reflections, addressing limitations in existing models by enhancing synthetic data and training strategies.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models often fail to adhere to physical laws in image editing, particularly for mirror reflections, lacking nuanced details and generalization across object orientations.

Method: The method augments synthetic data with random object positioning, rotations, and grounding, and pairs objects for complex scenes. A three-stage training curriculum improves real-world performance.

Result: MirrorFusion 2.0 achieves improved generalization and photorealistic results, supported by qualitative and quantitative evaluations.

Conclusion: The proposed approach effectively addresses the challenges of mirror reflection generation, offering a robust solution for real-world applications.

Abstract: Diffusion models have become central to various image editing tasks, yet they
often fail to fully adhere to physical laws, particularly with effects like
shadows, reflections, and occlusions. In this work, we address the challenge of
generating photorealistic mirror reflections using diffusion-based generative
models. Despite extensive training data, existing diffusion models frequently
overlook the nuanced details crucial to authentic mirror reflections. Recent
approaches have attempted to resolve this by creating synhetic datasets and
framing reflection generation as an inpainting task; however, they struggle to
generalize across different object orientations and positions relative to the
mirror. Our method overcomes these limitations by introducing key augmentations
into the synthetic data pipeline: (1) random object positioning, (2) randomized
rotations, and (3) grounding of objects, significantly enhancing generalization
across poses and placements. To further address spatial relationships and
occlusions in scenes with multiple objects, we implement a strategy to pair
objects during dataset generation, resulting in a dataset robust enough to
handle these complex scenarios. Achieving generalization to real-world scenes
remains a challenge, so we introduce a three-stage training curriculum to
develop the MirrorFusion 2.0 model to improve real-world performance. We
provide extensive qualitative and quantitative evaluations to support our
approach. The project page is available at: https://mirror-verse.github.io/.

</details>


### [72] [Context Aware Grounded Teacher for Source Free Object Detection](https://arxiv.org/abs/2504.15404)
*Tajamul Ashraf,Rajes Manna,Partha Sarathi Purkayastha,Tavaheed Tariq,Janibul Bashir*

Main category: cs.CV

TL;DR: The paper introduces Grounded Teacher (GT) to address context bias and performance drops in Source Free Object Detection (SFOD) by leveraging a relational context module and expert foundational branch.


<details>
  <summary>Details</summary>
Motivation: To mitigate biased teacher models and mode collapse in SFOD, especially in medical imaging, where domain shifts and class imbalance degrade performance.

Method: Uses a relational context module to model contextual relationships and an expert foundational branch to supervise the student model, applying augmentations to related classes.

Result: Validated on three medical datasets, GT effectively reduces context bias and improves performance for underrepresented classes.

Conclusion: GT provides a robust framework for SFOD, addressing bias and enhancing model adaptability in domain-shifted scenarios.

Abstract: We focus on the Source Free Object Detection (SFOD) problem, when source data
is unavailable during adaptation, and the model must adapt to the unlabeled
target domain. In medical imaging, several approaches have leveraged a
semi-supervised student-teacher architecture to bridge domain discrepancy.
Context imbalance in labeled training data and significant domain shifts
between domains can lead to biased teacher models that produce inaccurate
pseudolabels, degrading the student model's performance and causing a mode
collapse. Class imbalance, particularly when one class significantly outnumbers
another, leads to contextual bias. To tackle the problem of context bias and
the significant performance drop of the student model in the SFOD setting, we
introduce Grounded Teacher (GT) as a standard framework. In this study, we
model contextual relationships using a dedicated relational context module and
leverage it to mitigate inherent biases in the model. This approach enables us
to apply augmentations to closely related classes, across and within domains,
enhancing the performance of underrepresented classes while keeping the effect
on dominant classes minimal. We further improve the quality of predictions by
implementing an expert foundational branch to supervise the student model. We
validate the effectiveness of our approach in mitigating context bias under the
SFOD setting through experiments on three medical datasets supported by
comprehensive ablation studies. All relevant resources, including preprocessed
data, trained model weights, and code, are publicly available at this
https://github.com/Tajamul21/Grounded_Teacher.

</details>


### [73] [Emergence and Evolution of Interpretable Concepts in Diffusion Models](https://arxiv.org/abs/2504.15473)
*Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: The paper explores using Sparse Autoencoders (SAEs) to interpret and control the reverse diffusion process in text-to-image diffusion models, uncovering human-interpretable concepts and demonstrating their causal effects on image generation.


<details>
  <summary>Details</summary>
Motivation: To demystify the black-box nature of diffusion models and understand their internal dynamics for better control over image generation.

Method: Leverages SAEs to analyze activations in a text-to-image diffusion model, identifying interpretable concepts and testing their causal impact through interventions.

Result: Reveals that scene composition can be predicted early in diffusion, and concepts can be used to control composition and style at different stages.

Conclusion: SAEs provide valuable insights into diffusion models, enabling targeted interventions for steering image generation.

Abstract: Diffusion models have become the go-to method for text-to-image generation,
producing high-quality images from noise through a process called reverse
diffusion. Understanding the dynamics of the reverse diffusion process is
crucial in steering the generation and achieving high sample quality. However,
the inner workings of diffusion models is still largely a mystery due to their
black-box nature and complex, multi-step generation process. Mechanistic
Interpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at
uncovering the operating principles of models through granular analysis of
their internal representations. These MI techniques have been successful in
understanding and steering the behavior of large language models at scale.
However, the great potential of SAEs has not yet been applied toward gaining
insight into the intricate generative process of diffusion models. In this
work, we leverage the SAE framework to probe the inner workings of a popular
text-to-image diffusion model, and uncover a variety of human-interpretable
concepts in its activations. Interestingly, we find that even before the first
reverse diffusion step is completed, the final composition of the scene can be
predicted surprisingly well by looking at the spatial distribution of activated
concepts. Moreover, going beyond correlational analysis, we show that the
discovered concepts have a causal effect on the model output and can be
leveraged to steer the generative process. We design intervention techniques
aimed at manipulating image composition and style, and demonstrate that (1) in
early stages of diffusion image composition can be effectively controlled, (2)
in the middle stages of diffusion image composition is finalized, however
stylistic interventions are effective, and (3) in the final stages of diffusion
only minor textural details are subject to change.

</details>


### [74] [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](https://arxiv.org/abs/2504.15415)
*David Ma,Yuanxing Zhang,Jincheng Ren,Jarvis Guo,Yifan Yao,Zhenlin Wei,Zhenzhu Yang,Zhongyuan Peng,Boyu Feng,Jun Ma,Xiao Gu,Zhoufutu Wen,King Zhu,Yancheng He,Meng Cao,Shiwen Ni,Jiaheng Liu,Wenhao Huang,Ge Zhang,Xiaojie Jin*

Main category: cs.CV

TL;DR: IV-Bench is a new benchmark for evaluating image-grounded video perception and reasoning in MLLMs, revealing current models' limitations (max 28.9% accuracy) and key performance factors.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM evaluation frameworks neglect image context in video comprehension, prompting the creation of IV-Bench to address this gap.

Method: IV-Bench includes 967 videos with 2,585 annotated image-text queries across 13 tasks (7 perception, 6 reasoning) and 5 categories. Evaluated state-of-the-art MLLMs (open/closed-source).

Result: Current MLLMs perform poorly (≤28.9% accuracy). Key factors: inference pattern, frame number, resolution. Data synthesis shows challenges beyond format alignment.

Conclusion: IV-Bench highlights MLLMs' shortcomings in image-grounded video tasks, offering insights for future research. Data and code are publicly available.

Abstract: Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)
primarily focus on image reasoning or general video understanding tasks,
largely overlooking the significant role of image context in video
comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive
benchmark for evaluating Image-Grounded Video Perception and Reasoning.
IV-Bench consists of 967 videos paired with 2,585 meticulously annotated
image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5
representative categories. Extensive evaluations of state-of-the-art
open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,
Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models
substantially underperform in image-grounded video Perception and Reasoning,
merely achieving at most 28.9% accuracy. Further analysis reveals key factors
influencing model performance on IV-Bench, including inference pattern, frame
number, and resolution. Additionally, through a simple data synthesis approach,
we demonstratethe challenges of IV- Bench extend beyond merely aligning the
data format in the training proecss. These findings collectively provide
valuable insights for future research. Our codes and data are released in
https://github.com/multimodal-art-projection/IV-Bench.

</details>


### [75] [Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images](https://arxiv.org/abs/2504.15470)
*Jonathan Brokman,Amit Giloni,Omer Hofman,Roman Vainshtein,Hisashi Kojima,Guy Gilboa*

Main category: cs.CV

TL;DR: The paper addresses the challenge of distinguishing real from AI-generated images, proposing a zero-shot and few-shot detection method based on analyzing biases in generated content using manifold analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of outdated data in supervised methods and improve detection performance in zero-shot and few-shot regimes.

Method: The method involves quantifying biases in generated content using a pre-trained diffusion model, analyzing score functions, and extending to few-shot detection with a mixture-of-experts approach.

Result: Empirical results show superior performance over existing methods across 20 generative models in both zero-shot and few-shot settings.

Conclusion: The work advances theoretical understanding and practical detection of generated content biases through manifold analysis.

Abstract: Distinguishing between real and AI-generated images, commonly referred to as
'image detection', presents a timely and significant challenge. Despite
extensive research in the (semi-)supervised regime, zero-shot and few-shot
solutions have only recently emerged as promising alternatives. Their main
advantage is in alleviating the ongoing data maintenance, which quickly becomes
outdated due to advances in generative technologies. We identify two main gaps:
(1) a lack of theoretical grounding for the methods, and (2) significant room
for performance improvements in zero-shot and few-shot regimes. Our approach is
founded on understanding and quantifying the biases inherent in generated
content, where we use these quantities as criteria for characterizing generated
images. Specifically, we explore the biases of the implicit probability
manifold, captured by a pre-trained diffusion model. Through score-function
analysis, we approximate the curvature, gradient, and bias towards points on
the probability manifold, establishing criteria for detection in the zero-shot
regime. We further extend our contribution to the few-shot setting by employing
a mixture-of-experts methodology. Empirical results across 20 generative models
demonstrate that our method outperforms current approaches in both zero-shot
and few-shot settings. This work advances the theoretical understanding and
practical usage of generated content biases through the lens of manifold
analysis.

</details>


### [76] [DSDNet: Raw Domain Demoiréing via Dual Color-Space Synergy](https://arxiv.org/abs/2504.15756)
*Qirui Yang,Fangpu Zhang,Yeying Jin,Qihua Cheng,Pengtao Jiang,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: DSDNet is a single-stage raw domain demoiréing framework that outperforms existing methods in visual quality, speed, and color fidelity.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing demoiréing methods, which suffer from irreversible information loss or inefficiency.

Method: Proposes DSDNet, leveraging raw and YCbCr images, with Synergic Attention with Dynamic Modulation (SADM) and Luminance-Chrominance Adaptive Transformer (LCAT).

Result: DSDNet achieves superior visual quality, faster inference speed (2.4x), and better color fidelity.

Conclusion: DSDNet is a practical and efficient solution for demoiréing, validated by extensive experiments.

Abstract: With the rapid advancement of mobile imaging, capturing screens using
smartphones has become a prevalent practice in distance learning and conference
recording. However, moir\'e artifacts, caused by frequency aliasing between
display screens and camera sensors, are further amplified by the image signal
processing pipeline, leading to severe visual degradation. Existing sRGB domain
demoir\'eing methods struggle with irreversible information loss, while recent
two-stage raw domain approaches suffer from information bottlenecks and
inference inefficiency. To address these limitations, we propose a single-stage
raw domain demoir\'eing framework, Dual-Stream Demoir\'eing Network (DSDNet),
which leverages the synergy of raw and YCbCr images to remove moir\'e while
preserving luminance and color fidelity. Specifically, to guide luminance
correction and moir\'e removal, we design a raw-to-YCbCr mapping pipeline and
introduce the Synergic Attention with Dynamic Modulation (SADM) module. This
module enriches the raw-to-sRGB conversion with cross-domain contextual
features. Furthermore, to better guide color fidelity, we develop a
Luminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance
and chrominance representations. Extensive experiments demonstrate that DSDNet
outperforms state-of-the-art methods in both visual quality and quantitative
evaluation, and achieves an inference speed $\mathrm{\textbf{2.4x}}$ faster
than the second-best method, highlighting its practical advantages. We provide
an anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/.

</details>


### [77] [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)
*Atin Pothiraj,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TL;DR: CAPTURe is a novel task to evaluate vision-language models (VLMs) on counting occluded objects in patterns, revealing their deficiencies in spatial reasoning and occlusion handling.


<details>
  <summary>Details</summary>
Motivation: Occlusions are common in real-world scenes, and understanding them is crucial for spatial comprehension, yet current VLMs struggle with such tasks.

Method: CAPTURe involves counting objects in patterns behind occluders, tested on real (CAPTURe-real) and synthetic (CAPTURe-synthetic) datasets with four VLMs.

Result: VLMs, including GPT-4o, perform poorly on occluded patterns, highlighting their inability to infer unseen spatial relationships, while humans excel.

Conclusion: VLMs lack robust spatial reasoning for occlusions, and auxiliary information improves performance, indicating challenges in both occlusion handling and counting tasks.

Abstract: Recognizing and reasoning about occluded (partially or fully hidden) objects
is vital to understanding visual scenes, as occlusions frequently occur in
real-world environments and act as obstacles for spatial comprehension. To test
models' ability to reason about multiple occluded objects, we introduce a novel
task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which
requires a model to count objects arranged in a pattern by inferring how the
pattern continues behind an occluder (an object which blocks parts of the
scene). CAPTURe requires both recognizing visual patterns and reasoning, making
it a useful testbed for evaluating vision-language models (VLMs) on whether
they understand occluded patterns and possess spatial understanding skills. By
requiring models to reason about occluded objects, CAPTURe also tests VLMs'
ability to form world models that would allow them to fill in missing
information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually
filtered images of real objects in patterns and (2) CAPTURe-synthetic, a
controlled diagnostic with generated patterned images. We evaluate four strong
VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models
struggle to count on both occluded and unoccluded patterns. Crucially, we find
that models perform worse with occlusion, suggesting that VLMs are also
deficient in inferring unseen spatial relationships: even the strongest VLMs
like GPT-4o fail to count with occlusion. In contrast, we find that humans
achieve very little error on CAPTURe. We also find that providing auxiliary
information of occluded object locations increases performance, underscoring
that the model error comes both from an inability to handle occlusion as well
as difficulty counting in images.

</details>


### [78] [InstaRevive: One-Step Image Enhancement via Dynamic Score Matching](https://arxiv.org/abs/2504.15513)
*Yixuan Zhu,Haolin Wang,Ao Li,Wenliang Zhao,Yansong Tang,Jingxuan Niu,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: InstaRevive is a diffusion-based image enhancement framework that reduces sampling steps via score-based diffusion distillation and dynamic control, achieving high-quality results efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational intensity and prolonged sampling of recent diffusion-based methods while leveraging their generative power for image enhancement.

Method: Uses score-based diffusion distillation with dynamic control for accurate denoising trajectory learning and incorporates textual prompts via image captioning for enriched guidance.

Result: Demonstrates efficacy and efficiency across diverse tasks and datasets, delivering high-quality, visually appealing results.

Conclusion: InstaRevive effectively balances generative capability and computational efficiency, making it a practical solution for image enhancement.

Abstract: Image enhancement finds wide-ranging applications in real-world scenarios due
to complex environments and the inherent limitations of imaging devices. Recent
diffusion-based methods yield promising outcomes but necessitate prolonged and
computationally intensive iterative sampling. In response, we propose
InstaRevive, a straightforward yet powerful image enhancement framework that
employs score-based diffusion distillation to harness potent generative
capability and minimize the sampling steps. To fully exploit the potential of
the pre-trained diffusion model, we devise a practical and effective diffusion
distillation pipeline using dynamic control to address inaccuracies in updating
direction during score matching. Our control strategy enables a dynamic
diffusing scope, facilitating precise learning of denoising trajectories within
the diffusion model and ensuring accurate distribution matching gradients
during training. Additionally, to enrich guidance for the generative power, we
incorporate textual prompts via image captioning as auxiliary conditions,
fostering further exploration of the diffusion model. Extensive experiments
substantiate the efficacy of our framework across a diverse array of
challenging tasks and datasets, unveiling the compelling efficacy and
efficiency of InstaRevive in delivering high-quality and visually appealing
results. Code is available at https://github.com/EternalEvan/InstaRevive.

</details>


### [79] [Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness](https://arxiv.org/abs/2504.15599)
*Shichen Li,Chenhui Shao*

Main category: cs.CV

TL;DR: Proposes a multi-modal data fusion framework for real-time food drying readiness forecasting, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate real-time forecasting of drying readiness is challenging due to dynamic drying processes, limited data, and ineffective predictive methods.

Method: An end-to-end multi-modal data fusion framework integrating in-situ video data with process parameters, using a novel encoder-decoder architecture with modality-specific encoders and a transformer-based decoder.

Result: The model achieves an average prediction error of 15 seconds, outperforming state-of-the-art methods by 65.69% and a video-only model by 11.30%.

Conclusion: The model is accurate, efficient, and extensible to other industrial modality fusion tasks for online decision-making.

Abstract: Food drying is essential for food production, extending shelf life, and
reducing transportation costs. Accurate real-time forecasting of drying
readiness is crucial for minimizing energy consumption, improving productivity,
and ensuring product quality. However, this remains challenging due to the
dynamic nature of drying, limited data availability, and the lack of effective
predictive analytical methods. To address this gap, we propose an end-to-end
multi-modal data fusion framework that integrates in-situ video data with
process parameters for real-time food drying readiness forecasting. Our
approach leverages a new encoder-decoder architecture with modality-specific
encoders and a transformer-based decoder to effectively extract features while
preserving the unique structure of each modality. We apply our approach to
sugar cookie drying, where time-to-ready is predicted at each timestamp.
Experimental results demonstrate that our model achieves an average prediction
error of only 15 seconds, outperforming state-of-the-art data fusion methods by
65.69% and a video-only model by 11.30%. Additionally, our model balances
prediction accuracy, model size, and computational efficiency, making it
well-suited for heterogenous industrial datasets. The proposed model is
extensible to various other industrial modality fusion tasks for online
decision-making.

</details>


### [80] [Cross-domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction](https://arxiv.org/abs/2403.19001)
*Yui Lo,Yuqian Chen,Dongnan Liu,Wan Liu,Leo Zekelman,Fan Zhang,Yogesh Rathi,Nikos Makris,Alexandra J. Golby,Weidong Cai,Lauren J. O'Donnell*

Main category: cs.CV

TL;DR: The paper explores the predictive relationship between the shape of the brain's 3D white matter connections and human cognitive function, introducing a novel framework (SFFormer) for improved prediction.


<details>
  <summary>Details</summary>
Motivation: Shape analysis in brain imaging can reveal structural and functional correlations, particularly for understanding human cognitive function.

Method: The study reconstructs brain connections using dMRI tractography, extracts 12 shape descriptors, and introduces SFFormer, a transformer-based model with multi-head cross-attention feature fusion.

Result: SFFormer, combined with shape, microstructure, and connectivity features, improves prediction of language performance scores in 1065 healthy young adults.

Conclusion: The shape of brain connections is predictive of human language function, highlighting its importance in cognitive analysis.

Abstract: Shape plays an important role in computer graphics, offering informative
features to convey an object's morphology and functionality. Shape analysis in
brain imaging can help interpret structural and functionality correlations of
the human brain. In this work, we investigate the shape of the brain's 3D white
matter connections and its potential predictive relationship to human cognitive
function. We reconstruct brain connections as sequences of 3D points using
diffusion magnetic resonance imaging (dMRI) tractography. To describe each
connection, we extract 12 shape descriptors in addition to traditional dMRI
connectivity and tissue microstructure features. We introduce a novel
framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a
multi-head cross-attention feature fusion module to predict subject-specific
language performance based on dMRI tractography. We assess the performance of
the method on a large dataset including 1065 healthy young adults. The results
demonstrate that both the transformer-based SFFormer model and its inter/intra
feature fusion with shape, microstructure, and connectivity are informative,
and together, they improve the prediction of subject-specific language
performance scores. Overall, our results indicate that the shape of the brain's
connections is predictive of human language function.

</details>


### [81] [SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking](https://arxiv.org/abs/2504.15609)
*Yunfeng Li,Bo Wang,Jiahao Wan,Xueyi Wu,Ye Li*

Main category: cs.CV

TL;DR: The paper introduces SonarT165, the first large-scale benchmark for underwater acoustic object tracking (UAOT), and proposes STFTrack, an efficient framework with novel modules to address limitations in current trackers.


<details>
  <summary>Details</summary>
Motivation: Underwater visibility issues limit optical cameras, making sonar systems essential for stable data. The lack of a unified UAOT benchmark has hindered progress in this field.

Method: The authors propose STFTrack, featuring a multi-view template fusion module (MTFM) and an optimal trajectory correction module (OTCM), along with acoustic image enhancement and a Frequency Enhancement Module (FEM).

Result: STFTrack achieves state-of-the-art performance on the SonarT165 benchmark, demonstrating its effectiveness.

Conclusion: The SonarT165 benchmark and STFTrack framework advance UAOT by providing a unified evaluation standard and improved tracking performance.

Abstract: Underwater observation systems typically integrate optical cameras and
imaging sonar systems. When underwater visibility is insufficient, only sonar
systems can provide stable data, which necessitates exploration of the
underwater acoustic object tracking (UAOT) task. Previous studies have explored
traditional methods and Siamese networks for UAOT. However, the absence of a
unified evaluation benchmark has significantly constrained the value of these
methods. To alleviate this limitation, we propose the first large-scale UAOT
benchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and
205K high-quality annotations. Experimental results demonstrate that SonarT165
reveals limitations in current state-of-the-art SOT trackers. To address these
limitations, we propose STFTrack, an efficient framework for acoustic object
tracking. It includes two novel modules, a multi-view template fusion module
(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module
integrates multi-view feature of both the original image and the binary image
of the dynamic template, and introduces a cross-attention-like layer to fuse
the spatio-temporal target representations. The OTCM module introduces the
acoustic-response-equivalent pixel property and proposes normalized pixel
brightness response scores, thereby suppressing suboptimal matches caused by
inaccurate Kalman filter prediction boxes. To further improve the model
feature, STFTrack introduces a acoustic image enhancement method and a
Frequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive
experiments show the proposed STFTrack achieves state-of-the-art performance on
the proposed benchmark. The code is available at
https://github.com/LiYunfengLYF/SonarT165.

</details>


### [82] [PIDSR: Complementary Polarized Image Demosaicing and Super-Resolution](https://arxiv.org/abs/2504.07758)
*Shuangfan Zhou,Chu Zhou,Youwei Lyu,Heng Guo,Zhanyu Ma,Boxin Shi,Imari Sato*

Main category: cs.CV

TL;DR: PIDSR is a joint framework for Polarized Image Demosaicing and Super-Resolution, improving accuracy in polarization parameters and resolution.


<details>
  <summary>Details</summary>
Motivation: Current methods for polarized image processing introduce artifacts and errors in polarization parameters, and lack resolution enhancement.

Method: Proposes PIDSR, a joint framework combining demosaicing and super-resolution to directly process CPFA raw images.

Result: Achieves state-of-the-art performance on synthetic and real data, enhancing downstream tasks.

Conclusion: PIDSR robustly produces high-quality, high-resolution polarized images with accurate polarization parameters.

Abstract: Polarization cameras can capture multiple polarized images with different
polarizer angles in a single shot, bringing convenience to polarization-based
downstream tasks. However, their direct outputs are color-polarization filter
array (CPFA) raw images, requiring demosaicing to reconstruct full-resolution,
full-color polarized images; unfortunately, this necessary step introduces
artifacts that make polarization-related parameters such as the degree of
polarization (DoP) and angle of polarization (AoP) prone to error. Besides,
limited by the hardware design, the resolution of a polarization camera is
often much lower than that of a conventional RGB camera. Existing polarized
image demosaicing (PID) methods are limited in that they cannot enhance
resolution, while polarized image super-resolution (PISR) methods, though
designed to obtain high-resolution (HR) polarized images from the demosaicing
results, tend to retain or even amplify errors in the DoP and AoP introduced by
demosaicing artifacts. In this paper, we propose PIDSR, a joint framework that
performs complementary Polarized Image Demosaicing and Super-Resolution,
showing the ability to robustly obtain high-quality HR polarized images with
more accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments
show our PIDSR not only achieves state-of-the-art performance on both synthetic
and real data, but also facilitates downstream tasks.

</details>


### [83] [HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2504.15612)
*Hongxing Peng,Kang Lin,Huanai Liu*

Main category: cs.CV

TL;DR: Proposed HS-Mamba framework combines local and global features for high-precision HSI classification, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral data's high dimensionality and feature inlining challenge Mamba's application, requiring a novel approach.

Method: HS-Mamba uses a dual-channel spatial-spectral encoder (DCSS-encoder) and lightweight global inline attention (LGI-Att) branch to fuse local and global features.

Result: Outperforms state-of-the-art methods on four benchmark HSI datasets.

Conclusion: HS-Mamba effectively addresses HSI classification challenges by integrating local and global feature modeling.

Abstract: Hyperspectral image (HSI) classification has been one of the hot topics in
remote sensing fields. Recently, the Mamba architecture based on selective
state-space models (S6) has demonstrated great advantages in long sequence
modeling. However, the unique properties of hyperspectral data, such as high
dimensionality and feature inlining, pose challenges to the application of
Mamba to HSI classification. To compensate for these shortcomings, we propose
an full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts
a strategy different from pixel-patch based or whole-image based, but combines
the advantages of both. The patches cut from the whole image are sent to
multi-groups Mamba, combined with positional information to perceive local
inline features in the spatial and spectral domains, and the whole image is
sent to a lightweight attention module to enhance the global feature
representation ability. Specifically, HS-Mamba consists of a dual-channel
spatial-spectral encoder (DCSS-encoder) module and a lightweight global inline
attention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of
Mamba to decouple and model the local features of dual-channel sequences with
non-overlapping patches. The LGI-Att branch uses a lightweight compressed and
extended attention module to perceive the global features of the spatial and
spectral domains of the unsegmented whole image. By fusing local and global
features, high-precision classification of hyperspectral images is achieved.
Extensive experiments demonstrate the superiority of the proposed HS-Mamba,
outperforming state-of-the-art methods on four benchmark HSI datasets.

</details>


### [84] [AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization](https://arxiv.org/abs/2504.15619)
*Jinda Lu,Jinghan Li,Yuan Gao,Junkang Wu,Jiancan Wu,Xiang Wang,Xiangnan He*

Main category: cs.CV

TL;DR: AdaViP enhances MLLM alignment with human preferences by integrating visual context through vision-based preference pair construction and adaptive optimization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on language preferences, neglecting visual context, limiting MLLM alignment with human preferences.

Method: AdaViP introduces vision-based preference pair construction and adaptive preference optimization to balance vision- and language-based preferences.

Result: AdaViP-7B reduces response- and mentioned-level hallucination by 93.7% and 96.4% respectively on Object HalBench, surpassing state-of-the-art methods.

Conclusion: AdaViP effectively aligns MLLMs with human preferences by incorporating visual context, demonstrating superior performance over existing approaches.

Abstract: Preference alignment through Direct Preference Optimization (DPO) has
demonstrated significant effectiveness in aligning multimodal large language
models (MLLMs) with human preferences. However, existing methods focus
primarily on language preferences while neglecting the critical visual context.
In this paper, we propose an Adaptive Vision-enhanced Preference optimization
(AdaViP) that addresses these limitations through two key innovations: (1)
vision-based preference pair construction, which integrates multiple visual
foundation models to strategically remove key visual elements from the image,
enhancing MLLMs' sensitivity to visual details; and (2) adaptive preference
optimization that dynamically balances vision- and language-based preferences
for more accurate alignment. Extensive evaluations across different benchmarks
demonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%
reductions in response-level and mentioned-level hallucination respectively on
the Object HalBench, significantly outperforming current state-of-the-art
methods.

</details>


### [85] [FaceInsight: A Multimodal Large Language Model for Face Perception](https://arxiv.org/abs/2504.15624)
*Jingzhi Li,Changjiang Luo,Ruoyu Chen,Hua Zhang,Wenqi Ren,Jianhou Gan,Xiaochun Cao*

Main category: cs.CV

TL;DR: FaceInsight, a multimodal large language model (MLLM), addresses poor face perception in general-domain MLLMs by aligning visual-textual facial knowledge and using face segmentation maps, outperforming nine MLLMs in tasks.


<details>
  <summary>Details</summary>
Motivation: General-domain MLLMs perform poorly in face perception tasks, producing inaccurate responses to face-specific queries.

Method: Introduces visual-textual alignment of facial knowledge and incorporates face segmentation maps as an auxiliary modality.

Result: FaceInsight consistently outperforms nine compared MLLMs in three face perception tasks under training-free and fine-tuned settings.

Conclusion: FaceInsight effectively bridges the gap in face perception for MLLMs, enhancing accuracy and understanding.

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
strong capabilities in understanding general visual content. However, these
general-domain MLLMs perform poorly in face perception tasks, often producing
inaccurate or misleading responses to face-specific queries. To address this
gap, we propose FaceInsight, the versatile face perception MLLM that provides
fine-grained facial information. Our approach introduces visual-textual
alignment of facial knowledge to model both uncertain dependencies and
deterministic relationships among facial information, mitigating the
limitations of language-driven reasoning. Additionally, we incorporate face
segmentation maps as an auxiliary perceptual modality, enriching the visual
input with localized structural cues to enhance semantic understanding.
Comprehensive experiments and analyses across three face perception tasks
demonstrate that FaceInsight consistently outperforms nine compared MLLMs under
both training-free and fine-tuned settings.

</details>


### [86] [ZeroSlide: Is Zero-Shot Classification Adequate for Lifelong Learning in Whole-Slide Image Analysis in the Era of Pathology Vision-Language Foundation Models?](https://arxiv.org/abs/2504.15627)
*Doanh C. Bui,Hoai Luan Pham,Vu Trung Duong Le,Tuan Hai Vu,Van Duy Tran,Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: The paper compares continual-learning methods with vision-language zero-shot classification for lifelong learning in whole slide images (WSIs).


<details>
  <summary>Details</summary>
Motivation: To address the practical challenge of training unified models for multiple WSI tasks without retraining for new tasks, leveraging vision-language models.

Method: Comparison of conventional continual-learning approaches (regularization- and rehearsal-based) with vision-language zero-shot classification.

Result: Experimental results and source code will be released, but specific findings are not detailed here.

Conclusion: Further investigation is needed to determine if vision-language models alone suffice for lifelong WSI learning or if continual-learning strategies are required.

Abstract: Lifelong learning for whole slide images (WSIs) poses the challenge of
training a unified model to perform multiple WSI-related tasks, such as cancer
subtyping and tumor classification, in a distributed, continual fashion. This
is a practical and applicable problem in clinics and hospitals, as WSIs are
large, require storage, processing, and transfer time. Training new models
whenever new tasks are defined is time-consuming. Recent work has applied
regularization- and rehearsal-based methods to this setting. However, the rise
of vision-language foundation models that align diagnostic text with pathology
images raises the question: are these models alone sufficient for lifelong WSI
learning using zero-shot classification, or is further investigation into
continual learning strategies needed to improve performance? To our knowledge,
this is the first study to compare conventional continual-learning approaches
with vision-language zero-shot classification for WSIs. Our source code and
experimental results will be available soon.

</details>


### [87] [AffordanceSAM: Segment Anything Once More in Affordance Grounding](https://arxiv.org/abs/2504.15650)
*Dengyang Jiang,Mengmeng Wang,Teli Ma,Hengzhuang Li,Yong liu,Guang Dai,Lei Zhang*

Main category: cs.CV

TL;DR: AffordanceSAM extends SAM's segmentation capabilities to affordance grounding, improving generalization for unseen objects and functions.


<details>
  <summary>Details</summary>
Motivation: Current models lack generalization for affordance grounding in real-world applications.

Method: Proposes an affordance-adaption module and coarse-to-fine training to adapt SAM's segmentation for affordance tasks.

Result: Outperforms previous methods on AGD20K and handles novel objects/functions.

Conclusion: AffordanceSAM demonstrates strong generalization for affordance grounding.

Abstract: Improving the generalization ability of an affordance grounding model to
recognize regions for unseen objects and affordance functions is crucial for
real-world application. However, current models are still far away from such
standards. To address this problem, we introduce AffordanceSAM, an effective
approach that extends SAM's generalization capacity to the domain of affordance
grounding. For the purpose of thoroughly transferring SAM's robust performance
in segmentation to affordance, we initially propose an affordance-adaption
module in order to help modify SAM's segmentation output to be adapted to the
specific functional regions required for affordance grounding. We concurrently
make a coarse-to-fine training recipe to make SAM first be aware of affordance
objects and actions coarsely, and then be able to generate affordance heatmaps
finely. Both quantitative and qualitative experiments show the strong
generalization capacity of our AffordanceSAM, which not only surpasses previous
methods under AGD20K benchmark but also shows evidence to handle the task with
novel objects and affordance functions.

</details>


### [88] [DiTPainter: Efficient Video Inpainting with Diffusion Transformers](https://arxiv.org/abs/2504.15661)
*Xian Wu,Chang Liu*

Main category: cs.CV

TL;DR: DiTPainter, a video inpainting model based on Diffusion Transformer (DiT), addresses blurry and inconsistency issues in existing methods by using an efficient transformer network trained from scratch.


<details>
  <summary>Details</summary>
Motivation: Existing video inpainting methods struggle with inaccurate optical flows and large masks, leading to blurry results. Pretrained DiT models are too large for efficient video inpainting.

Method: DiTPainter employs an efficient transformer network designed for video inpainting, trained from scratch, and handles arbitrary video lengths.

Result: DiTPainter outperforms existing methods, offering higher quality and better spatial-temporal consistency.

Conclusion: DiTPainter provides an effective, efficient solution for video inpainting, decaptioning, and completion tasks.

Abstract: Many existing video inpainting algorithms utilize optical flows to construct
the corresponding maps and then propagate pixels from adjacent frames to
missing areas by mapping. Despite the effectiveness of the propagation
mechanism, they might encounter blurry and inconsistencies when dealing with
inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)
has emerged as a revolutionary technique for video generation tasks. However,
pretrained DiT models for video generation all contain a large amount of
parameters, which makes it very time consuming to apply to video inpainting
tasks. In this paper, we present DiTPainter, an end-to-end video inpainting
model based on Diffusion Transformer (DiT). DiTPainter uses an efficient
transformer network designed for video inpainting, which is trained from
scratch instead of initializing from any large pretrained models. DiTPainter
can address videos with arbitrary lengths and can be applied to video
decaptioning and video completion tasks with an acceptable time cost.
Experiments show that DiTPainter outperforms existing video inpainting
algorithms with higher quality and better spatial-temporal consistency.

</details>


### [89] [Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection](https://arxiv.org/abs/2504.15665)
*Pei Liu,Yisi Luo,Wenzhen Wang,Xiangyong Cao*

Main category: cs.CV

TL;DR: A novel motion-enhanced nonlocal similarity implicit neural representation (INR) framework is proposed for infrared dim and small target detection, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional low-rank plus sparse models fail to handle dynamic backgrounds and global spatial-temporal correlations, leading to background leakage or target loss.

Method: Integrates motion estimation via optical flow, multi-frame fusion for motion saliency, and a tensor decomposition-based INR model to encode nonlocal low-rankness and spatial-temporal correlations.

Result: Robustly separates dim targets from complex backgrounds, achieving higher detection accuracy and robustness.

Conclusion: The proposed framework effectively addresses challenges in infrared dim target detection, offering superior performance.

Abstract: Infrared dim and small target detection presents a significant challenge due
to dynamic multi-frame scenarios and weak target signatures in the infrared
modality. Traditional low-rank plus sparse models often fail to capture dynamic
backgrounds and global spatial-temporal correlations, which results in
background leakage or target loss. In this paper, we propose a novel
motion-enhanced nonlocal similarity implicit neural representation (INR)
framework to address these challenges. We first integrate motion estimation via
optical flow to capture subtle target movements, and propose multi-frame fusion
to enhance motion saliency. Second, we leverage nonlocal similarity to
construct patch tensors with strong low-rank properties, and propose an
innovative tensor decomposition-based INR model to represent the nonlocal patch
tensor, effectively encoding both the nonlocal low-rankness and
spatial-temporal correlations of background through continuous neural
representations. An alternating direction method of multipliers is developed
for the nonlocal INR model, which enjoys theoretical fixed-point convergence.
Experimental results show that our approach robustly separates dim targets from
complex infrared backgrounds, outperforming state-of-the-art methods in
detection accuracy and robustness.

</details>


### [90] [DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining](https://arxiv.org/abs/2504.15669)
*Wei Zhuo,Zhiyue Tang,Wufeng Xue,Hao Ding,Linlin Shen*

Main category: cs.CV

TL;DR: FS-DINO is a unified model combining DINOv2's encoder and a lightweight segmenter for few-shot semantic segmentation, leveraging SAM's knowledge through distillation and 4D correlation mining.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity and enhance representation transferability for novel class segmentation by unifying knowledge from DINOv2 and SAM.

Method: Proposes FS-DINO with DINOv2's encoder and a lightweight segmenter featuring a bottleneck adapter, meta-visual prompt generator, and decoder. Uses coarse-to-fine cross-model distillation and 4D correlation mining.

Result: Demonstrates effectiveness and superiority on COCO-20i, PASCAL-5i, and FSS-1000 datasets.

Conclusion: FS-DINO successfully integrates SAM's knowledge into a lightweight segmenter, achieving strong performance in few-shot semantic segmentation.

Abstract: Few-shot semantic segmentation has gained increasing interest due to its
generalization capability, i.e., segmenting pixels of novel classes requiring
only a few annotated images. Prior work has focused on meta-learning for
support-query matching, with extensive development in both prototype-based and
aggregation-based methods. To address data scarcity, recent approaches have
turned to foundation models to enhance representation transferability for novel
class segmentation. Among them, a hybrid dual-modal framework including both
DINOv2 and SAM has garnered attention due to their complementary capabilities.
We wonder "can we build a unified model with knowledge from both foundation
models?" To this end, we propose FS-DINO, with only DINOv2's encoder and a
lightweight segmenter. The segmenter features a bottleneck adapter, a
meta-visual prompt generator based on dense similarities and semantic
embeddings, and a decoder. Through coarse-to-fine cross-model distillation, we
effectively integrate SAM's knowledge into our lightweight segmenter, which can
be further enhanced by 4D correlation mining on support-query pairs. Extensive
experiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness
and superiority of our method.

</details>


### [91] [Vidi: Large Multimodal Models for Video Understanding and Editing](https://arxiv.org/abs/2504.15681)
*Vidi Team,Celong Liu,Chia-Wen Kuo,Dawei Du,Fan Chen,Guang Chen,Jiamin Yuan,Lingxi Zhang,Lu Guo,Lusha Li,Longyin Wen,Qingyu Chen,Rachel Deng,Sijie Zhu,Stuart Siew,Tong Jin,Wei Lu,Wen Zhong,Xiaohui Shen,Xin Gu,Xing Mei,Xueqiong Qu*

Main category: cs.CV

TL;DR: Vidi, a Large Multimodal Model (LMM), excels in temporal retrieval for video editing, outperforming models like GPT-4o and Gemini.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of processing multimodal, long-duration videos in editing, requiring strong temporal understanding.

Method: Introduces Vidi, an LMM for video understanding, focusing on temporal retrieval (identifying video segments matching text queries).

Result: Vidi outperforms proprietary models on the VUE-TR benchmark, which includes longer videos, audio support, and diverse queries.

Conclusion: Vidi demonstrates superior performance in video editing tasks, validated by the comprehensive VUE-TR benchmark.

Abstract: Humans naturally share information with those they are connected to, and
video has become one of the dominant mediums for communication and expression
on the Internet. To support the creation of high-quality large-scale video
content, a modern pipeline requires a comprehensive understanding of both the
raw input materials (e.g., the unedited footage captured by cameras) and the
editing components (e.g., visual effects). In video editing scenarios, models
must process multiple modalities (e.g., vision, audio, text) with strong
background knowledge and handle flexible input lengths (e.g., hour-long raw
videos), which poses significant challenges for traditional models. In this
report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a
wide range of video understand editing scenarios. The first release focuses on
temporal retrieval, i.e., identifying the time ranges within the input videos
corresponding to a given text query, which plays a critical role in intelligent
editing. The model is capable of processing hour-long videos with strong
temporal understanding capability, e.g., retrieve time ranges for certain
queries. To support a comprehensive evaluation in real-world scenarios, we also
present the VUE-TR benchmark, which introduces five key advancements. 1) Video
duration: significantly longer than existing temporal retrival datasets, 2)
Audio support: includes audio-based queries, 3) Query format: diverse query
lengths/formats, 4) Annotation quality: ground-truth time ranges are manually
annotated. 5) Evaluation metric: a refined IoU metric to support evaluation
over multiple time ranges. Remarkably, Vidi significantly outperforms leading
proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,
indicating its superiority in video editing scenarios.

</details>


### [92] [You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection](https://arxiv.org/abs/2504.15694)
*Jun Dong,Wenli Wu,Jintao Cheng,Xiaoyu Tang*

Main category: cs.CV

TL;DR: Proposes YSOOB, an ultra-light real-time underwater object detection framework, achieving high accuracy and efficiency with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Improving object detection accuracy and efficiency in challenging underwater conditions with low image quality and limited computational resources.

Method: Uses Multi-Spectrum Wavelet Encoder for frequency-domain encoding, dynamic resampling with even-sized/transposed convolutions, and channel compression with RLKC for lightweighting.

Result: YSOOB achieves 83.1% and 82.9% mAP50 on URPC2020 and DUO datasets, with 781.3 FPS on T4 GPU and 57.8 FPS on Jetson Xavier NX.

Conclusion: YSOOB outperforms current SOTA detectors in efficiency and accuracy, making it suitable for real-time underwater applications.

Abstract: Despite the remarkable achievements in object detection, the model's accuracy
and efficiency still require further improvement under challenging underwater
conditions, such as low image quality and limited computational resources. To
address this, we propose an Ultra-Light Real-Time Underwater Object Detection
framework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a
Multi-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on
the input image, minimizing the semantic loss caused by underwater optical
color distortion. Furthermore, we revisit the unique characteristics of
even-sized and transposed convolutions, allowing the model to dynamically
select and enhance key information during the resampling process, thereby
improving its generalization ability. Finally, we eliminate model redundancy
through a simple yet effective channel compression and reconstructed large
kernel convolution (RLKC) to achieve model lightweight. As a result, forms a
high-performance underwater object detector YSOOB with only 1.2 million
parameters. Extensive experimental results demonstrate that, with the fewest
parameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO
datasets, respectively, comparable to the current SOTA detectors. The inference
speed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge
computing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by
28.1% and 22.5%, respectively.

</details>


### [93] [RePOPE: Impact of Annotation Errors on the POPE Benchmark](https://arxiv.org/abs/2504.15707)
*Yannic Neuhaus,Matthias Hein*

Main category: cs.CV

TL;DR: The paper evaluates label errors in MSCOCO's POPE benchmark, re-annotates images, and introduces RePOPE, showing how label quality affects model rankings.


<details>
  <summary>Details</summary>
Motivation: To address the impact of label errors in benchmark datasets on model evaluations, particularly in the POPE benchmark derived from MSCOCO.

Method: Re-annotate images from the POPE benchmark, identify annotation imbalances, and evaluate models on the revised labels (RePOPE).

Result: Notable shifts in model rankings are observed when using RePOPE, emphasizing the influence of label quality.

Conclusion: Label quality significantly affects benchmark reliability, and RePOPE provides a more accurate evaluation framework.

Abstract: Since data annotation is costly, benchmark datasets often incorporate labels
from established image datasets. In this work, we assess the impact of label
errors in MSCOCO on the frequently used object hallucination benchmark POPE. We
re-annotate the benchmark images and identify an imbalance in annotation errors
across different subsets. Evaluating multiple models on the revised labels,
which we denote as RePOPE, we observe notable shifts in model rankings,
highlighting the impact of label quality. Code and data are available at
https://github.com/YanNeu/RePOPE .

</details>


### [94] [Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models](https://arxiv.org/abs/2504.15723)
*Dasol Jeong,Donggoo Kang,Jiwon Park,Hyebean Lee,Joonki Paik*

Main category: cs.CV

TL;DR: A diffusion-based framework for zero-shot image editing unifies text and reference-guided approaches without fine-tuning, using diffusion inversion and null-text embeddings for structural integrity.


<details>
  <summary>Details</summary>
Motivation: To enable precise, fine-grained image editing while maintaining global consistency, unifying text and reference-guided methods without fine-tuning.

Method: Leverages diffusion inversion and timestep-specific null-text embeddings, with a stage-wise latent injection strategy (shape early, attribute later) and cross-attention for semantic alignment.

Result: Achieves state-of-the-art performance in expression transfer, texture transformation, and style infusion, demonstrating scalability and adaptability.

Conclusion: The proposed framework is effective for diverse image editing scenarios, offering precise modifications and global consistency.

Abstract: We propose a diffusion-based framework for zero-shot image editing that
unifies text-guided and reference-guided approaches without requiring
fine-tuning. Our method leverages diffusion inversion and timestep-specific
null-text embeddings to preserve the structural integrity of the source image.
By introducing a stage-wise latent injection strategy-shape injection in early
steps and attribute injection in later steps-we enable precise, fine-grained
modifications while maintaining global consistency. Cross-attention with
reference latents facilitates semantic alignment between the source and
reference. Extensive experiments across expression transfer, texture
transformation, and style infusion demonstrate state-of-the-art performance,
confirming the method's scalability and adaptability to diverse image editing
scenarios.

</details>


### [95] [SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems](https://arxiv.org/abs/2504.15728)
*Manjunath D,Aniruddh Sikdar,Prajwal Gurunath,Sumanth Udupa,Suresh Sundaram*

Main category: cs.CV

TL;DR: SAGA improves RGB-to-IR adaptation by addressing color bias and bridging domain gaps, validated on the IndraEye dataset.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on co-registered RGB-IR pairs and large annotated IR datasets while overcoming IR image limitations like lack of color/texture cues.

Method: Proposes Semantic-Aware Gray color Augmentation (SAGA) to mitigate color bias and extract object-level features for IR images. Introduces the IndraEye dataset for validation.

Result: SAGA achieves performance gains of +0.4% to +7.6% (mAP) when integrated with domain adaptation techniques.

Conclusion: SAGA and IndraEye enhance RGB-to-IR adaptation, supporting robust aerial perception in challenging environments.

Abstract: Domain-adaptive thermal object detection plays a key role in facilitating
visible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered
image pairs and minimizing reliance on large annotated IR datasets. However,
inherent limitations of IR images, such as the lack of color and texture cues,
pose challenges for RGB-trained models, leading to increased false positives
and poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray
color Augmentation (SAGA), a novel strategy for mitigating color bias and
bridging the domain gap by extracting object-level features relevant to IR
images. Additionally, to validate the proposed SAGA for drone imagery, we
introduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse
applications. The dataset contains 5,612 images with 145,666 instances,
captured from diverse angles, altitudes, backgrounds, and times of day,
offering valuable opportunities for multimodal learning, domain adaptation for
object detection and segmentation, and exploration of sensor-specific strengths
and weaknesses. IndraEye aims to enhance the development of more robust and
accurate aerial perception systems, especially in challenging environments.
Experimental results show that SAGA significantly improves RGB-to-IR adaptation
for autonomous driving and IndraEye dataset, achieving consistent performance
gains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain
adaptation techniques. The dataset and codes are available at
https://github.com/airliisc/IndraEye.

</details>


### [96] [GADS: A Super Lightweight Model for Head Pose Estimation](https://arxiv.org/abs/2504.15751)
*Menan Velayuthan,Asiri Gawesha,Purushoth Velayuthan,Nuwan Kodagoda,Dharshana Kasthurirathna,Pradeepa Samarasinghe*

Main category: cs.CV

TL;DR: Proposes Grouped Attention Deep Sets (GADS), a lightweight and efficient architecture for head pose estimation, reducing model size and computation while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing landmark-based methods for head pose estimation prioritize precision over simplicity and model size, limiting deployment on edge devices.

Method: GADS groups landmarks into regions, uses small Deep Set layers, and employs a multihead attention mechanism to reduce complexity. Introduces vanilla GADS and Hybrid-GADS (landmarks + RGB).

Result: GADS is 7.5× smaller, 25× faster than the lightest state-of-the-art model, and 4321× smaller than the best-performing model. Evaluated on AFLW2000, BIWI, and 300W-LP datasets.

Conclusion: GADS serves as a robust baseline for resource-constrained head pose estimation, balancing efficiency and performance.

Abstract: In human-computer interaction, head pose estimation profoundly influences
application functionality. Although utilizing facial landmarks is valuable for
this purpose, existing landmark-based methods prioritize precision over
simplicity and model size, limiting their deployment on edge devices and in
compute-poor environments. To bridge this gap, we propose \textbf{Grouped
Attention Deep Sets (GADS)}, a novel architecture based on the Deep Set
framework. By grouping landmarks into regions and employing small Deep Set
layers, we reduce computational complexity. Our multihead attention mechanism
extracts and combines inter-group information, resulting in a model that is
$7.5\times$ smaller and executes $25\times$ faster than the current lightest
state-of-the-art model. Notably, our method achieves an impressive reduction,
being $4321\times$ smaller than the best-performing model. We introduce vanilla
GADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three
benchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture
as a robust baseline for resource-constrained head pose estimation methods.

</details>


### [97] [Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection](https://arxiv.org/abs/2504.15770)
*Lei Xu,Mehmet Yamac,Mete Ahishali,Moncef Gabbouj*

Main category: cs.CV

TL;DR: A novel MTS-DR-Net is proposed for edge detection, using MTS-DR modules to reduce redundancy and focus on relevant information, achieving large receptive fields early in the network.


<details>
  <summary>Details</summary>
Motivation: Edge detection is crucial for computer vision tasks, and deep learning methods have improved performance. However, achieving large receptive fields typically requires deep networks, which the MTS-DR-Net addresses more efficiently.

Method: The MTS-DR-Net employs MTS layers and MTS-DR blocks to reduce dimensionality and focus on relevant subspaces, followed by a U-shaped refinement module.

Result: Extensive experiments on BSDS500 and BIPEDv2 datasets demonstrate the model's effectiveness.

Conclusion: The MTS-DR-Net offers an efficient solution for edge detection by leveraging MTS-DR modules to achieve large receptive fields and improved performance.

Abstract: Edge detection has attracted considerable attention thanks to its exceptional
ability to enhance performance in downstream computer vision tasks. In recent
years, various deep learning methods have been explored for edge detection
tasks resulting in a significant performance improvement compared to
conventional computer vision algorithms. In neural networks, edge detection
tasks require considerably large receptive fields to provide satisfactory
performance. In a typical convolutional operation, such a large receptive field
can be achieved by utilizing a significant number of consecutive layers, which
yields deep network structures. Recently, a Multi-scale Tensorial Summation
(MTS) factorization operator was presented, which can achieve very large
receptive fields even from the initial layers. In this paper, we propose a
novel MTS Dimensional Reduction (MTS-DR) module guided neural network,
MTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and
corresponding MTS-DR blocks as a new backbone to remove redundant information
initially. Such a dimensional reduction module enables the neural network to
focus specifically on relevant information (i.e., necessary subspaces).
Finally, a weight U-shaped refinement module follows MTS-DR blocks in the
MTS-DR-Net. We conducted extensive experiments on two benchmark edge detection
datasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The
implementation of the proposed MTS-DR-Net can be found at
https://github.com/LeiXuAI/MTS-DR-Net.git.

</details>


### [98] [Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models](https://arxiv.org/abs/2504.15776)
*Quentin Herau,Nathan Piasco,Moussab Bennehar,Luis Rolado,Dzmitry Tsishkou,Bingbing Liu,Cyrille Migniot,Pascal Vasseur,Cédric Demonceaux*

Main category: cs.CV

TL;DR: A robust optimization method using Neural Radiance Fields (NeRF) refines sensor poses and calibration in autonomous driving datasets, improving accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Inaccuracies in sensor calibration and vehicle poses in public datasets can lead to erroneous evaluations, impacting autonomous driving systems.

Method: Proposes a NeRF-based optimization method to refine sensor poses and calibration, evaluated via reprojection metrics, Novel View Synthesis, and geometric alignment.

Result: Significant improvements in sensor pose accuracy, enhancing dataset integrity and autonomous driving model reliability.

Conclusion: The method improves dataset benchmarks and provides publicly available optimized poses to advance autonomous driving research.

Abstract: Autonomous driving systems rely on accurate perception and localization of
the ego car to ensure safety and reliability in challenging real-world driving
scenarios. Public datasets play a vital role in benchmarking and guiding
advancement in research by providing standardized resources for model
development and evaluation. However, potential inaccuracies in sensor
calibration and vehicle poses within these datasets can lead to erroneous
evaluations of downstream tasks, adversely impacting the reliability and
performance of the autonomous systems. To address this challenge, we propose a
robust optimization method based on Neural Radiance Fields (NeRF) to refine
sensor poses and calibration parameters, enhancing the integrity of dataset
benchmarks. To validate improvement in accuracy of our optimized poses without
ground truth, we present a thorough evaluation process, relying on reprojection
metrics, Novel View Synthesis rendering quality, and geometric alignment. We
demonstrate that our method achieves significant improvements in sensor pose
accuracy. By optimizing these critical parameters, our approach not only
improves the utility of existing datasets but also paves the way for more
reliable autonomous driving models. To foster continued progress in this field,
we make the optimized sensor poses publicly available, providing a valuable
resource for the research community.

</details>


### [99] [Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose Dolphins in Drone-Shot Videos](https://arxiv.org/abs/2504.15782)
*Daniele Baieri,Riccardo Cicciarella,Michael Krützen,Emanuele Rodolà,Silvia Zuffi*

Main category: cs.CV

TL;DR: Estimating 3D shape and motion of wild dolphins from monocular video to assess body condition, addressing challenges of underwater observation.


<details>
  <summary>Details</summary>
Motivation: Aquatic animals like dolphins are hard to observe underwater, unlike terrestrial quadrupeds, making 3D reconstruction challenging.

Method: Proposes a model-based approach with a transmission model for water-induced occlusion, tested on videos under varying sea conditions.

Result: Estimates mass and volume, comparing results with manual 2D measurements.

Conclusion: Demonstrates feasibility of 3D reconstruction for aquatic animals, offering a new tool for body condition assessment.

Abstract: We address the problem of estimating the metric 3D shape and motion of wild
dolphins from monocular video, with the aim of assessing their body condition.
While considerable progress has been made in reconstructing 3D models of
terrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty
of observing them in their natural underwater environment. To address this, we
propose a model-based approach that incorporates a transmission model to
account for water-induced occlusion. We apply our method to video captured
under different sea conditions. We estimate mass and volume, and compare our
results to a manual 2D measurements-based method.

</details>


### [100] [Towards prediction of morphological heart age from computed tomography angiography](https://arxiv.org/abs/2504.15783)
*Johan Öfverstedt,Elin Lundström,Håkan Ahlström,Joel Kullberg*

Main category: cs.CV

TL;DR: The paper presents a method to predict age from CTA images by analyzing heart morphology, achieving low prediction errors and identifying key regions linked to aging.


<details>
  <summary>Details</summary>
Motivation: To study the relationship between heart morphology and aging, and to develop a novel biomarker for morphological heart age.

Method: Image registration-based standardization, unsupervised segmentation for supervoxels, extraction of robust features (density and local volume), and machine learning regression models.

Result: Mean absolute errors of 2.74 years (females) and 2.77 years (males); high consistency in predictions from morphology; detailed saliency maps identifying important sub-regions.

Conclusion: The method effectively predicts age from heart morphology, with interpretable results highlighting key regions associated with aging.

Abstract: Age prediction from medical images or other health-related non-imaging data
is an important approach to data-driven aging research, providing knowledge of
how much information a specific tissue or organ carries about the chronological
age of the individual. In this work, we studied the prediction of age from
computed tomography angiography (CTA) images, which provide detailed
representations of the heart morphology, with the goals of (i) studying the
relationship between morphology and aging, and (ii) developing a novel
\emph{morphological heart age} biomarker. We applied an image
registration-based method that standardizes the images from the whole cohort
into a single space. We then extracted supervoxels (using unsupervised
segmentation), and corresponding robust features of density and local volume,
which provide a detailed representation of the heart morphology while being
robust to registration errors. Machine learning models are then trained to fit
regression models from these features to the chronological age. We applied the
method to a subset of the images from the Swedish CArdioPulomonary bioImage
Study (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a
mean absolute error of $2.74$ years for females and $2.77$ years for males. The
predictions from different sub-regions of interest were observed to be more
highly correlated with the predictions from the whole heart, compared to the
chronological age, revealing a high consistency in the predictions from
morphology. Saliency analysis was also performed on the prediction models to
study what regions are associated positively and negatively with the predicted
age. This resulted in detailed association maps where the density and volume of
known, as well as some novel sub-regions of interest, are determined to be
important. The saliency analysis aids in the interpretability of the models and
their predictions.

</details>


### [101] [Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views](https://arxiv.org/abs/2504.15786)
*Ningli Xu,Rongjun Qin*

Main category: cs.CV

TL;DR: A novel cross-view synthesis method using a fixed latent diffusion model ensures consistent ground-view image generation from satellite imagery, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing inconsistencies in ground-view images generated from satellite views due to angle and resolution discrepancies.

Method: Uses a fixed latent diffusion model with two conditioning modules: satellite-guided denoising for scene layout and satellite-temporal denoising for camera motion consistency.

Result: Outperforms existing methods in perceptual and temporal metrics, achieving high photorealism and multi-view consistency.

Conclusion: The proposed approach effectively generates consistent ground-view images from satellite data, supported by a large-scale dataset.

Abstract: Generating consistent ground-view images from satellite imagery is
challenging, primarily due to the large discrepancies in viewing angles and
resolution between satellite and ground-level domains. Previous efforts mainly
concentrated on single-view generation, often resulting in inconsistencies
across neighboring ground views. In this work, we propose a novel cross-view
synthesis approach designed to overcome these challenges by ensuring
consistency across ground-view images generated from satellite views. Our
method, based on a fixed latent diffusion model, introduces two conditioning
modules: satellite-guided denoising, which extracts high-level scene layout to
guide the denoising process, and satellite-temporal denoising, which captures
camera motion to maintain consistency across multiple generated views. We
further contribute a large-scale satellite-ground dataset containing over
100,000 perspective pairs to facilitate extensive ground scene or video
generation. Experimental results demonstrate that our approach outperforms
existing methods on perceptual and temporal metrics, achieving high
photorealism and consistency in multi-view outputs.

</details>


### [102] [Development and evaluation of a deep learning algorithm for German word recognition from lip movements](https://arxiv.org/abs/2504.15792)
*Dinh Nam Pham,Torsten Rahne*

Main category: cs.CV

TL;DR: A neural network for German lip reading achieves high accuracy (87% with known speakers, 63% with unknowns), comparable to English models, using 3D CNN and GRU models.


<details>
  <summary>Details</summary>
Motivation: Existing lip-reading AI lacks support for German, and visual lip movements are error-prone.

Method: Trained on 38,391 video segments of 18 German words, comparing 3D CNN, GRU, and combined models (GRUConv), with varied image sections and color spaces.

Result: GRUConv achieved 87% accuracy with known speakers and 63% with unknowns; lip-focused cuts outperformed full-face (70% vs. 34%).

Conclusion: The German lip-reading model is highly accurate, generalizable, and performs well with unknown speakers.

Abstract: When reading lips, many people benefit from additional visual information
from the lip movements of the speaker, which is, however, very error prone.
Algorithms for lip reading with artificial intelligence based on artificial
neural networks significantly improve word recognition but are not available
for the German language. A total of 1806 video clips with only one
German-speaking person each were selected, split into word segments, and
assigned to word classes using speech-recognition software. In 38,391 video
segments with 32 speakers, 18 polysyllabic, visually distinguishable words were
used to train and validate a neural network. The 3D Convolutional Neural
Network and Gated Recurrent Units models and a combination of both models
(GRUConv) were compared, as were different image sections and color spaces of
the videos. The accuracy was determined in 5000 training epochs. Comparison of
the color spaces did not reveal any relevant different correct classification
rates in the range from 69% to 72%. With a cut to the lips, a significantly
higher accuracy of 70% was achieved than when cut to the entire speaker's face
(34%). With the GRUConv model, the maximum accuracies were 87% with known
speakers and 63% in the validation with unknown speakers. The neural network
for lip reading, which was first developed for the German language, shows a
very high level of accuracy, comparable to English-language algorithms. It
works with unknown speakers as well and can be generalized with more word
classes.

</details>


### [103] [Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness](https://arxiv.org/abs/2504.15796)
*Jiaqi Tang,Yinsong Xu,Qingchao Chen*

Main category: cs.CV

TL;DR: The paper introduces SM-DSB, a method to mitigate gradient conflicts in point cloud UDA by filtering harmful self-supervision gradients using saliency maps.


<details>
  <summary>Details</summary>
Motivation: Existing UDA methods for point clouds suffer from harmful gradients in self-supervision tasks, negatively impacting classification.

Method: Proposes SM-DSB, a scoring mechanism based on 3D saliency maps to estimate gradient conflicts and dynamically filter unhelpful samples.

Result: Outperforms state-of-the-art UDA methods with modest computational overhead.

Conclusion: SM-DSB effectively addresses gradient conflicts in UDA, offering a scalable solution and new insights via back-propagation analysis.

Abstract: Object classification models utilizing point cloud data are fundamental for
3D media understanding, yet they often struggle with unseen or
out-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain
adaptation (UDA) methods typically employ a multi-task learning (MTL) framework
that combines primary classification tasks with auxiliary self-supervision
tasks to bridge the gap between cross-domain feature distributions. However,
our further experiments demonstrate that not all gradients from
self-supervision tasks are beneficial and some may negatively impact the
classification performance. In this paper, we propose a novel solution, termed
Saliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient
conflicts. Specifically, our method designs a new scoring mechanism based on
the skewness of 3D saliency maps to estimate gradient conflicts without
requiring target labels. Leveraging this, we develop a sample selection
strategy that dynamically filters out samples whose self-supervision gradients
are not beneficial for the classification. Our approach is scalable,
introducing modest computational overhead, and can be integrated into all the
point cloud UDA MTL frameworks. Extensive evaluations demonstrate that our
method outperforms state-of-the-art approaches. In addition, we provide a new
perspective on understanding the UDA problem through back-propagation analysis.

</details>


### [104] [Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models](https://arxiv.org/abs/2504.15823)
*Songyan Xie,Jinghang Wen,Encheng Su,Qiucheng Yu*

Main category: cs.CV

TL;DR: A novel adversarial patch using infrared-absorbing ink is designed to attack NIR face recognition systems, improving attack success rates in both digital and physical domains.


<details>
  <summary>Details</summary>
Motivation: To highlight vulnerabilities in NIR face recognition systems and demonstrate real-world risks of adversarial attacks.

Method: Utilizes human-imperceptible infrared-absorbing ink to create digitally optimized patches and a light reflection model to minimize discrepancies.

Result: Achieves an 82.46% attack success rate in the physical domain, outperforming SOTA methods (64.18%).

Conclusion: The proposed method effectively enhances adversarial attack performance on NIR face recognition systems, maintaining effectiveness across various postures.

Abstract: Near-infrared (NIR) face recognition systems, which can operate effectively
in low-light conditions or in the presence of makeup, exhibit vulnerabilities
when subjected to physical adversarial attacks. To further demonstrate the
potential risks in real-world applications, we design a novel, stealthy, and
practical adversarial patch to attack NIR face recognition systems in a
black-box setting. We achieved this by utilizing human-imperceptible
infrared-absorbing ink to generate multiple patches with digitally optimized
shapes and positions for infrared images. To address the optimization mismatch
between digital and real-world NIR imaging, we develop a light reflection model
for human skin to minimize pixel-level discrepancies by simulating NIR light
reflection.
  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition
systems, the experimental results show that our method improves the attack
success rate in both digital and physical domains, particularly maintaining
effectiveness across various face postures. Notably, the proposed approach
outperforms SOTA methods, achieving an average attack success rate of 82.46% in
the physical domain across different models, compared to 64.18% for existing
methods. The artifact is available at
https://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.

</details>


### [105] [Text-based Animatable 3D Avatars with Morphable Model Alignment](https://arxiv.org/abs/2504.15835)
*Yiqian Wu,Malte Prinzler,Xiaogang Jin,Siyu Tang*

Main category: cs.CV

TL;DR: AnimPortrait3D improves text-to-3D avatar generation by addressing ambiguities in 2D diffusion models, using pretrained priors and ControlNet for better alignment and animation.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with realistic details and alignment in text-to-3D avatar generation, leading to unnatural animations.

Method: Uses a pretrained text-to-3D model for initialization and refines with a ControlNet conditioned on morphable model maps.

Result: Outperforms existing methods in synthesis quality, alignment, and animation fidelity.

Conclusion: AnimPortrait3D advances the state of the art in text-based animatable 3D head avatar generation.

Abstract: The generation of high-quality, animatable 3D head avatars from text has
enormous potential in content creation applications such as games, movies, and
embodied virtual assistants. Current text-to-3D generation methods typically
combine parametric head models with 2D diffusion models using score
distillation sampling to produce 3D-consistent results. However, they struggle
to synthesize realistic details and suffer from misalignments between the
appearance and the driving parametric model, resulting in unnatural animation
results. We discovered that these limitations stem from ambiguities in the 2D
diffusion predictions during 3D avatar distillation, specifically: i) the
avatar's appearance and geometry is underconstrained by the text input, and ii)
the semantic alignment between the predictions and the parametric head model is
insufficient because the diffusion model alone cannot incorporate information
from the parametric model. In this work, we propose a novel framework,
AnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with
morphable model alignment, and introduce two key strategies to address these
challenges. First, we tackle appearance and geometry ambiguities by utilizing
prior information from a pretrained text-to-3D model to initialize a 3D avatar
with robust appearance, geometry, and rigging relationships to the morphable
model. Second, we refine the initial 3D avatar for dynamic expressions using a
ControlNet that is conditioned on semantic and normal maps of the morphable
model to ensure accurate alignment. As a result, our method outperforms
existing approaches in terms of synthesis quality, alignment, and animation
fidelity. Our experiments show that the proposed method advances the state of
the art in text-based, animatable 3D head avatar generation.

</details>


### [106] [DERD-Net: Learning Depth from Event-based Ray Densities](https://arxiv.org/abs/2504.15863)
*Diego de Oliveira Hitzges,Suman Ghosh,Guillermo Gallego*

Main category: cs.CV

TL;DR: A novel framework for depth estimation with event cameras, outperforming SOTA in stereo setups and achieving comparable results in monocular setups.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning struggles with event data's asynchronous nature; this work addresses the gap for event-based depth estimation and SLAM.

Method: Uses disparity space images (DSIs) and a neural network with 3D convolutions and recurrent structures for local processing.

Result: Outperforms SOTA in stereo (42% error reduction) and matches stereo results in monocular setups; improves depth completeness by 3x.

Conclusion: The framework is highly effective for event-based depth estimation and SLAM, with potential to become a standard approach.

Abstract: Event cameras offer a promising avenue for multi-view stereo depth estimation
and Simultaneous Localization And Mapping (SLAM) due to their ability to detect
blur-free 3D edges at high-speed and over broad illumination conditions.
However, traditional deep learning frameworks designed for conventional cameras
struggle with the asynchronous, stream-like nature of event data, as their
architectures are optimized for discrete, image-like inputs. We propose a
scalable, flexible and adaptable framework for pixel-wise depth estimation with
event cameras in both monocular and stereo setups. The 3D scene structure is
encoded into disparity space images (DSIs), representing spatial densities of
rays obtained by back-projecting events into space via known camera poses. Our
neural network processes local subregions of the DSIs combining 3D convolutions
and a recurrent structure to recognize valuable patterns for depth prediction.
Local processing enables fast inference with full parallelization and ensures
constant ultra-low model complexity and memory costs, regardless of camera
resolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)
demonstrate unprecedented effectiveness: (i) using purely monocular data, our
method achieves comparable results to existing stereo methods; (ii) when
applied to stereo data, it strongly outperforms all state-of-the-art (SOTA)
approaches, reducing the mean absolute error by at least 42%; (iii) our method
also allows for increases in depth completeness by more than 3-fold while still
yielding a reduction in median absolute error of at least 30%. Given its
remarkable performance and effective processing of event-data, our framework
holds strong potential to become a standard approach for using deep learning
for event-based depth estimation and SLAM. Project page:
https://github.com/tub-rip/DERD-Net

</details>


### [107] [MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search](https://arxiv.org/abs/2504.15865)
*Lotfi Abdelkrim Mecharbat,Ibrahim Elmakky,Martin Takac,Mohammed Yaqub*

Main category: cs.CV

TL;DR: MedNNS is a Neural Network Search framework for medical imaging that jointly optimizes architecture selection and weight initialization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Adapting deep learning models to medical imaging is challenging due to architecture selection and weight initialization issues, with transfer learning from ImageNet being suboptimal.

Method: MedNNS constructs a meta-space using a Supernetwork-based approach, incorporating rank loss and FID loss to align datasets and models effectively.

Result: MedNNS achieves a 1.7% average accuracy improvement over SOTA methods and converges faster.

Conclusion: MedNNS provides a robust solution for optimizing DL models in medical imaging, with significant performance gains and efficiency.

Abstract: Deep learning (DL) has achieved remarkable progress in the field of medical
imaging. However, adapting DL models to medical tasks remains a significant
challenge, primarily due to two key factors: (1) architecture selection, as
different tasks necessitate specialized model designs, and (2) weight
initialization, which directly impacts the convergence speed and final
performance of the models. Although transfer learning from ImageNet is a widely
adopted strategy, its effectiveness is constrained by the substantial
differences between natural and medical images. To address these challenges, we
introduce Medical Neural Network Search (MedNNS), the first Neural Network
Search framework for medical imaging applications. MedNNS jointly optimizes
architecture selection and weight initialization by constructing a meta-space
that encodes datasets and models based on how well they perform together. We
build this space using a Supernetwork-based approach, expanding the model zoo
size by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we
introduce rank loss and Fr\'echet Inception Distance (FID) loss into the
construction of the space to capture inter-model and inter-dataset
relationships, thereby achieving more accurate alignment in the meta-space.
Experimental results across multiple datasets demonstrate that MedNNS
significantly outperforms both ImageNet pre-trained DL models and SOTA Neural
Architecture Search (NAS) methods, achieving an average accuracy improvement of
1.7% across datasets while converging substantially faster. The code and the
processed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.

</details>


### [108] [Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading](https://arxiv.org/abs/2504.15883)
*Farida Mohsen,Samir Belhaouari,Zubair Shah*

Main category: cs.CV

TL;DR: RadFuse, a multi-representation deep learning framework, improves diabetic retinopathy detection and grading by integrating RadEx-transformed sinogram images with fundus images, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Early detection and accurate grading of diabetic retinopathy are critical to prevent vision loss, but current methods struggle with complex lesion patterns.

Method: RadFuse combines RadEx-transformed sinogram images (an optimized non-linear Radon transform) with fundus images, leveraging spatial and transformed domain features. Experiments used ResNeXt-50, MobileNetV2, and VGG19 CNNs on APTOS-2019 and DDR datasets.

Result: RadFuse achieved a quadratic weighted kappa of 93.24%, accuracy of 87.07%, and F1-score of 87.17% for severity grading. Binary classification reached 99.09% accuracy, 98.58% precision, and 99.6% recall.

Conclusion: RadFuse effectively captures complex non-linear features, advancing diabetic retinopathy classification and highlighting the potential of mathematical transforms in medical imaging.

Abstract: Diabetic retinopathy is a serious ocular complication that poses a
significant threat to patients' vision and overall health. Early detection and
accurate grading are essential to prevent vision loss. Current automatic
grading methods rely heavily on deep learning applied to retinal fundus images,
but the complex, irregular patterns of lesions in these images, which vary in
shape and distribution, make it difficult to capture subtle changes. This study
introduces RadFuse, a multi-representation deep learning framework that
integrates non-linear RadEx-transformed sinogram images with traditional fundus
images to enhance diabetic retinopathy detection and grading. Our RadEx
transformation, an optimized non-linear extension of the Radon transform,
generates sinogram representations to capture complex retinal lesion patterns.
By leveraging both spatial and transformed domain information, RadFuse enriches
the feature set available to deep learning models, improving the
differentiation of severity levels. We conducted extensive experiments on two
benchmark datasets, APTOS-2019 and DDR, using three convolutional neural
networks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant
improvements over fundus-image-only models across all three CNN architectures
and outperformed state-of-the-art methods on both datasets. For severity
grading across five stages, RadFuse achieved a quadratic weighted kappa of
93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary
classification between healthy and diabetic retinopathy cases, the method
reached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,
surpassing previously established models. These results demonstrate RadFuse's
capacity to capture complex non-linear features, advancing diabetic retinopathy
classification and promoting the integration of advanced mathematical
transforms in medical image analysis.

</details>


### [109] [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
*Yimu Wang,Xuye Liu,Wei Pang,Li Ma,Shuai Yuan,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: A survey on diffusion-based video generation, covering evolution, methodologies, applications, and challenges, with a broader perspective than existing works.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive and updated review of diffusion-based video generation, addressing gaps in existing surveys and highlighting practical applications and challenges.

Method: Systematic taxonomy of methodologies, analysis of architectural innovations, optimization strategies, and exploration of synergies with related domains.

Result: A detailed resource for researchers, offering insights into theoretical frameworks and practical implementations, including evaluation metrics and industry solutions.

Conclusion: The survey serves as a foundational guide for the field, emphasizing the potential and challenges of diffusion-based video generation.

Abstract: Recent advances in diffusion models have revolutionized video generation,
offering superior temporal consistency and visual quality compared to
traditional generative adversarial networks-based approaches. While this
emerging field shows tremendous promise in applications, it faces significant
challenges in motion consistency, computational efficiency, and ethical
considerations. This survey provides a comprehensive review of diffusion-based
video generation, examining its evolution, technical foundations, and practical
applications. We present a systematic taxonomy of current methodologies,
analyze architectural innovations and optimization strategies, and investigate
applications across low-level vision tasks such as denoising and
super-resolution. Additionally, we explore the synergies between diffusionbased
video generation and related domains, including video representation learning,
question answering, and retrieval. Compared to the existing surveys (Lei et
al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which
focus on specific aspects of video generation, such as human video synthesis
(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our
work provides a broader, more updated, and more fine-grained perspective on
diffusion-based approaches with a special section for evaluation metrics,
industry solutions, and training engineering techniques in video generation.
This survey serves as a foundational resource for researchers and practitioners
working at the intersection of diffusion models and video generation, providing
insights into both the theoretical frameworks and practical implementations
that drive this rapidly evolving field. A structured list of related works
involved in this survey is also available on
https://github.com/Eyeline-Research/Survey-Video-Diffusion.

</details>


### [110] [MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2504.15888)
*Zhiqiang Wei,Lianqing Zheng,Jianan Liu,Tao Huang,Qing-Long Han,Wenwen Zhang,Fengdeng Zhang*

Main category: cs.CV

TL;DR: MS-Occ is a multi-stage LiDAR-camera fusion framework for 3D semantic occupancy perception, outperforming state-of-the-art methods with improved geometric and semantic accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of vision-centric (geometric inaccuracies) and LiDAR-based (lack of semantic richness) methods in autonomous driving.

Method: Proposes a multi-stage fusion framework with middle-stage (Gaussian-Geo and Semantic-Aware modules) and late-stage (Adaptive Fusion and HCCVF modules) fusion techniques.

Result: Achieves 32.1% IoU and 25.3% mIoU on nuScenes-OpenOccupancy, surpassing prior methods by +0.7% IoU and +2.4% mIoU.

Conclusion: MS-Occ effectively integrates LiDAR and camera data, validated by ablation studies, enhancing small-object perception for autonomous driving.

Abstract: Accurate 3D semantic occupancy perception is essential for autonomous driving
in complex environments with diverse and irregular objects. While
vision-centric methods suffer from geometric inaccuracies, LiDAR-based
approaches often lack rich semantic information. To address these limitations,
MS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes
middle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's
geometric fidelity with camera-based semantic richness via hierarchical
cross-modal fusion. The framework introduces innovations at two critical
stages: (1) In the middle-stage feature fusion, the Gaussian-Geo module
leverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D
image features with dense geometric priors, and the Semantic-Aware module
enriches LiDAR voxels with semantic context via deformable cross-attention; (2)
In the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically
balances voxel features across modalities, while the High Classification
Confidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using
self-attention-based refinement. Experiments on the nuScenes-OpenOccupancy
benchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%
and a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU
and +2.4% mIoU. Ablation studies further validate the contribution of each
module, with substantial improvements in small-object perception, demonstrating
the practical value of MS-Occ for safety-critical autonomous driving scenarios.

</details>


### [111] [Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions](https://arxiv.org/abs/2504.15918)
*Chang Zong,Bin Li,Shoujun Zhou,Jian Wan,Lei Zhang*

Main category: cs.CV

TL;DR: The paper introduces In-VAL, a task simulating human-video interactions for visual answer localization, and proposes Ask2Loc, a framework to address semantic gaps in this process.


<details>
  <summary>Details</summary>
Motivation: Users often need multiple interactions to find accurate video segments, highlighting the need for a system that mimics human understanding through iterative questioning.

Method: Ask2Loc includes three modules: chatting (refining questions), rewriting (generating descriptions), and searching (integrating content).

Result: Ask2Loc improves performance by up to 14.91 (mIoU) over traditional methods on In-VAL datasets.

Conclusion: The proposed framework effectively addresses semantic gaps in visual answer localization, outperforming existing methods.

Abstract: Locating specific segments within an instructional video is an efficient way
to acquire guiding knowledge. Generally, the task of obtaining video segments
for both verbal explanations and visual demonstrations is known as visual
answer localization (VAL). However, users often need multiple interactions to
obtain answers that align with their expectations when using the system. During
these interactions, humans deepen their understanding of the video content by
asking themselves questions, thereby accurately identifying the location.
Therefore, we propose a new task, named In-VAL, to simulate the multiple
interactions between humans and videos in the procedure of obtaining visual
answers. The In-VAL task requires interactively addressing several semantic gap
issues, including 1) the ambiguity of user intent in the input questions, 2)
the incompleteness of language in video subtitles, and 3) the fragmentation of
content in video segments. To address these issues, we propose Ask2Loc, a
framework for resolving In-VAL by asking questions. It includes three key
modules: 1) a chatting module to refine initial questions and uncover clear
intentions, 2) a rewriting module to generate fluent language and create
complete descriptions, and 3) a searching module to broaden local context and
provide integrated content. We conduct extensive experiments on three
reconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage
methods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on
the In-VAL task. Our code and datasets can be accessed at
https://github.com/changzong/Ask2Loc.

</details>


### [112] [ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting](https://arxiv.org/abs/2504.15921)
*Jian Hu,Dimitrios Korkinof,Shaogang Gong,Mariano Beguerisse-Diaz*

Main category: cs.CV

TL;DR: ViSMaP is an unsupervised system for summarizing long videos by leveraging LLMs to create pseudo-summaries from short video segment descriptions, avoiding costly annotations.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with long videos due to sparse events and lack of annotations. ViSMaP bridges this gap by using short video data to train for long-form summarization.

Method: Uses meta-prompting with three LLMs: one generates pseudo-summaries, another evaluates them, and a third optimizes the prompt. This iterative process refines summaries without long-video annotations.

Result: ViSMaP matches supervised state-of-the-art performance and generalizes across domains without performance loss.

Conclusion: ViSMaP provides a scalable, annotation-free solution for long-video summarization, achieving competitive results.

Abstract: We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a
system to summarise hour long videos with no-supervision. Most existing video
understanding models work well on short videos of pre-segmented events, yet
they struggle to summarise longer videos where relevant events are sparsely
distributed and not pre-segmented. Moreover, long-form video understanding
often relies on supervised hierarchical training that needs extensive
annotations which are costly, slow and prone to inconsistency. With ViSMaP we
bridge the gap between short videos (where annotated data is plentiful) and
long ones (where it's not). We rely on LLMs to create optimised
pseudo-summaries of long videos using segment descriptions from short ones.
These pseudo-summaries are used as training data for a model that generates
long-form video summaries, bypassing the need for expensive annotations of long
videos. Specifically, we adopt a meta-prompting strategy to iteratively
generate and refine creating pseudo-summaries of long videos. The strategy
leverages short clip descriptions obtained from a supervised short video model
to guide the summary. Each iteration uses three LLMs working in sequence: one
to generate the pseudo-summary from clip descriptions, another to evaluate it,
and a third to optimise the prompt of the generator. This iteration is
necessary because the quality of the pseudo-summaries is highly dependent on
the generator prompt, and varies widely among videos. We evaluate our summaries
extensively on multiple datasets; our results show that ViSMaP achieves
performance comparable to fully supervised state-of-the-art models while
generalising across domains without sacrificing performance. Code will be
released upon publication.

</details>


### [113] [A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers](https://arxiv.org/abs/2504.15928)
*Meng Wang,Tian Lin,Qingshan Hou,Aidi Lin,Jingcheng Wang,Qingsheng Peng,Truong X. Nguyen,Danqi Fang,Ke Zou,Ting Xu,Cancan Xue,Ten Cheer Quek,Qinkai Yu,Minxin Liu,Hui Zhou,Zixuan Xiao,Guiqin He,Huiyu Liang,Tingkun Shi,Man Chen,Linna Liu,Yuanyuan Peng,Lianyu Wang,Qiuming Hu,Junhong Chen,Zhenhua Zhang,Cheng Chen,Yitian Zhao,Dianbo Liu,Jianhua Wu,Xinjian Chen,Changqing Zhang,Triet Thanh Nguyen,Yanda Meng,Yalin Zheng,Yih Chung Tham,Carol Y. Cheung,Huazhu Fu,Haoyu Chen,Ching-Yu Cheng*

Main category: cs.CV

TL;DR: GlobeReady is an AI platform for ocular disease diagnosis that works across clinical centers without retraining, achieving high accuracy and clinician approval.


<details>
  <summary>Details</summary>
Motivation: Current AI models for medical imaging require retraining for different centers, limiting adoption. GlobeReady aims to overcome this barrier.

Method: Uses training-free local feature augmentation to handle domain shifts across centers and populations. Includes a confidence-quantifiable diagnostic approach.

Result: Achieves high accuracy (87.2-98.5%) across datasets and centers. Clinicians rated it highly (4.6/5) for usability.

Conclusion: GlobeReady offers robust, scalable diagnostic support for ophthalmic care without technical barriers.

Abstract: Artificial intelligence (AI) shows remarkable potential in medical imaging
diagnostics, but current models typically require retraining when deployed
across different clinical centers, limiting their widespread adoption. We
introduce GlobeReady, a clinician-friendly AI platform that enables ocular
disease diagnosis without retraining/fine-tuning or technical expertise.
GlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an
11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.
Through training-free local feature augmentation, it addresses domain shifts
across centers and populations, reaching an average accuracy of 88.9% across
five centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in
confidence-quantifiable diagnostic approach further boosted accuracy to
94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution
cases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians
from multiple countries rated GlobeReady highly (average 4.6 out of 5) for its
usability and clinical relevance. These results demonstrate GlobeReady's
robust, scalable diagnostic capability and potential to support ophthalmic care
without technical barriers.

</details>


### [114] [Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models](https://arxiv.org/abs/2504.15929)
*Saban Ozturk,Melih B. Yilmaz,Muti Kara,M. Talat Yavuz,Aykut Koç,Tolga Çukur*

Main category: cs.CV

TL;DR: MedTrim improves image-text alignment in medical vision-language models by leveraging meta-entity-driven triplet mining, outperforming existing methods in retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current contrastive learning methods in med-VLMs, which fail to capture fine-grained pathology attributes, leading to suboptimal performance.

Method: Introduces MedTrim, a method combining ontology-based entity recognition, a novel score function for triplet mining, and a multimodal triplet alignment objective to enhance alignment of detailed pathology characteristics.

Result: MedTrim outperforms state-of-the-art alignment methods in downstream tasks like retrieval and classification.

Conclusion: MedTrim effectively aligns image and text representations by preserving clinically significant intra-class variations, improving diagnostic imaging workflows.

Abstract: Diagnostic imaging relies on interpreting both images and radiology reports,
but the growing data volumes place significant pressure on medical experts,
yielding increased errors and workflow backlogs. Medical vision-language models
(med-VLMs) have emerged as a powerful framework to efficiently process
multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit
their performance hinges on how well image and text representations are
aligned. Existing alignment methods, predominantly based on contrastive
learning, prioritize separation between disease classes over segregation of
fine-grained pathology attributes like location, size or severity, leading to
suboptimal representations. Here, we propose MedTrim (Meta-entity-driven
Triplet mining), a novel method that enhances image-text alignment through
multimodal triplet learning synergistically guided by disease class as well as
adjectival and directional pathology descriptors. Unlike common alignment
methods that separate broad disease classes, MedTrim leverages structured
meta-entity information to preserve subtle but clinically significant
intra-class variations. For this purpose, we first introduce an ontology-based
entity recognition module that extracts pathology-specific meta-entities from
CXR reports, as annotations on pathology attributes are rare in public
datasets. For refined sample selection in triplet mining, we then introduce a
novel score function that captures an aggregate measure of inter-sample
similarity based on disease classes and adjectival/directional descriptors.
Lastly, we introduce a multimodal triplet alignment objective for explicit
within- and cross-modal alignment between samples sharing detailed pathology
characteristics. Our demonstrations indicate that MedTrim improves performance
in downstream retrieval and classification tasks compared to state-of-the-art
alignment methods.

</details>


### [115] [Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time](https://arxiv.org/abs/2504.15931)
*Ekaterina Kondrateva,Sandzhi Barg,Mikhail Vasiliev*

Main category: cs.CV

TL;DR: The study benchmarks FastSurfer and SynthSeg for brain segmentation, revealing 7-8% volume variation in small structures, and proposes quality filtering to improve reliability.


<details>
  <summary>Details</summary>
Motivation: To address scanner-induced variability and reproducibility limitations in longitudinal and multi-site neuroimaging studies.

Method: Benchmarking FastSurfer and SynthSeg using two datasets (SIMON and SRPBS) and metrics like Dice coefficient, Surface Dice, HD95, and MAPE.

Result: Found 7-8% volume variation in small subcortical structures, questioning the feasibility of detecting subtle longitudinal changes.

Conclusion: Highlights the need for harmonization strategies and provides a reproducible benchmark for morphometric reproducibility.

Abstract: Accurate and reproducible brain morphometry from structural MRI is critical
for monitoring neuroanatomical changes across time and across imaging domains.
Although deep learning has accelerated segmentation workflows, scanner-induced
variability and reproducibility limitations remain-especially in longitudinal
and multi-site settings. In this study, we benchmark two modern segmentation
pipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the
most widely adopted tools in neuroimaging.
  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and
a 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation
variability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),
and Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume
variation in small subcortical structures such as the amygdala and ventral
diencephalon, even under controlled test-retest conditions. This raises a key
question: is it feasible to detect subtle longitudinal changes on the order of
5-10% in pea-sized brain regions, given the magnitude of domain-induced
morphometric noise?
  We further analyze the effects of registration templates and interpolation
modes, and propose surface-based quality filtering to improve segmentation
reliability. This study provides a reproducible benchmark for morphometric
reproducibility and emphasizes the need for harmonization strategies in
real-world neuroimaging studies.
  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation

</details>


### [116] [Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning](https://arxiv.org/abs/2504.15932)
*Wang Lin,Liyu Jia,Wentao Hu,Kaihang Pan,Zhongqi Yue,Wei Zhao,Jingyuan Chen,Fei Wu,Hanwang Zhang*

Main category: cs.CV

TL;DR: The paper proposes Phys-AR, a framework combining symbolic reasoning and reinforcement learning to ensure physical consistency in video generation, outperforming traditional diffusion-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based video generation methods fail to adhere to physical laws, especially under unseen conditions, due to data-driven limitations.

Method: Introduces Diffusion Timestep Tokenizer (DDT) for recursive visual tokens, enabling symbolic reasoning. Phys-AR uses supervised fine-tuning and reinforcement learning to enforce physical consistency.

Result: Phys-AR generates physically consistent videos, validated by experiments.

Conclusion: The integration of symbolic reasoning and reinforcement learning effectively addresses physical consistency in video generation.

Abstract: Despite recent progress in video generation, producing videos that adhere to
physical laws remains a significant challenge. Traditional diffusion-based
methods struggle to extrapolate to unseen physical conditions (eg, velocity)
due to their reliance on data-driven approximations. To address this, we
propose to integrate symbolic reasoning and reinforcement learning to enforce
physical consistency in video generation. We first introduce the Diffusion
Timestep Tokenizer (DDT), which learns discrete, recursive visual tokens by
recovering visual attributes lost during the diffusion process. The recursive
visual tokens enable symbolic reasoning by a large language model. Based on it,
we propose the Phys-AR framework, which consists of two stages: The first stage
uses supervised fine-tuning to transfer symbolic knowledge, while the second
stage applies reinforcement learning to optimize the model's reasoning
abilities through reward functions based on physical conditions. Our approach
allows the model to dynamically adjust and improve the physical properties of
generated videos, ensuring adherence to physical laws. Experimental results
demonstrate that PhysAR can generate videos that are physically consistent.

</details>


### [117] [FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation](https://arxiv.org/abs/2504.15958)
*Zebin Yao,Lei Ren,Huixing Jiang,Chen Wei,Xiaojie Wang,Ruifan Li,Fangxiang Feng*

Main category: cs.CV

TL;DR: FreeGraftor is a training-free framework for subject-driven image generation, balancing fidelity and efficiency via cross-image feature grafting.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with trade-offs between fidelity and efficiency in subject-driven image generation.

Method: Uses semantic matching, position-constrained attention fusion, and noise initialization for robust feature transfer.

Result: Outperforms zero-shot and training-free methods in subject fidelity and text alignment, with multi-subject generation capability.

Conclusion: FreeGraftor offers a practical, efficient solution for high-fidelity subject-driven image synthesis without fine-tuning.

Abstract: Subject-driven image generation aims to synthesize novel scenes that
faithfully preserve subject identity from reference images while adhering to
textual guidance, yet existing methods struggle with a critical trade-off
between fidelity and efficiency. Tuning-based approaches rely on time-consuming
and resource-intensive subject-specific optimization, while zero-shot methods
fail to maintain adequate subject consistency. In this work, we propose
FreeGraftor, a training-free framework that addresses these limitations through
cross-image feature grafting. Specifically, FreeGraftor employs semantic
matching and position-constrained attention fusion to transfer visual details
from reference subjects to the generated image. Additionally, our framework
incorporates a novel noise initialization strategy to preserve geometry priors
of reference subjects for robust feature matching. Extensive qualitative and
quantitative experiments demonstrate that our method enables precise subject
identity transfer while maintaining text-aligned scene synthesis. Without
requiring model fine-tuning or additional training, FreeGraftor significantly
outperforms existing zero-shot and training-free approaches in both subject
fidelity and text alignment. Furthermore, our framework can seamlessly extend
to multi-subject generation, making it practical for real-world deployment. Our
code is available at https://github.com/Nihukat/FreeGraftor.

</details>


### [118] [Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications](https://arxiv.org/abs/2504.15991)
*Leonardo Olivi,Edoardo Santero Mormile,Enzo Tartaglione*

Main category: cs.CV

TL;DR: The paper evaluates adapters for efficient transfer learning in rock segmentation for extraterrestrial terrains, reducing bandwidth and memory needs.


<details>
  <summary>Details</summary>
Motivation: Addressing labeled data scarcity in extraterrestrial exploration using deep learning.

Method: Employing adapters in pre-trained models with layer fusion and adapter ranking to save memory and bandwidth.

Result: Adapters reduce inference overhead and transmission costs while maintaining performance on embedded devices.

Conclusion: The study highlights trade-offs and opens avenues for further research in efficient transfer learning for extraterrestrial applications.

Abstract: In recent years, the application of Deep Learning techniques has shown
remarkable success in various computer vision tasks, paving the way for their
deployment in extraterrestrial exploration. Transfer learning has emerged as a
powerful strategy for addressing the scarcity of labeled data in these novel
environments. This paper represents one of the first efforts in evaluating the
feasibility of employing adapters toward efficient transfer learning for rock
segmentation in extraterrestrial landscapes, mainly focusing on lunar and
martian terrains. Our work suggests that the use of adapters, strategically
integrated into a pre-trained backbone model, can be successful in reducing
both bandwidth and memory requirements for the target extraterrestrial device.
In this study, we considered two memory-saving strategies: layer fusion (to
reduce to zero the inference overhead) and an ``adapter ranking'' (to also
reduce the transmission cost). Finally, we evaluate these results in terms of
task performance, memory, and computation on embedded devices, evidencing
trade-offs that open the road to more research in the field.

</details>


### [119] [MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment](https://arxiv.org/abs/2504.16003)
*Yachun Mi,Yu Li,Weicheng Meng,Chaofeng Chen,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: MVQA introduces a Mamba-based model for efficient video quality assessment (VQA) with a novel Unified Semantic and Distortion Sampling (USDS) method, balancing performance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The challenge of efficient VQA for long-duration, high-definition videos, where existing methods struggle to balance efficiency and performance due to long-range modeling needs and weak resampling techniques.

Method: MVQA combines Mamba's linear complexity with USDS, which samples semantic patches from low-res videos and distortion patches from original-res videos, fused using pre-defined masks.

Result: MVQA matches state-of-the-art performance while being 2x faster and using 1/5 GPU memory.

Conclusion: MVQA with USDS offers an efficient and effective solution for VQA, addressing key limitations of current methods.

Abstract: The rapid growth of long-duration, high-definition videos has made efficient
video quality assessment (VQA) a critical challenge. Existing research
typically tackles this problem through two main strategies: reducing model
parameters and resampling inputs. However, light-weight Convolution Neural
Networks (CNN) and Transformers often struggle to balance efficiency with high
performance due to the requirement of long-range modeling capabilities.
Recently, the state-space model, particularly Mamba, has emerged as a promising
alternative, offering linear complexity with respect to sequence length.
Meanwhile, efficient VQA heavily depends on resampling long sequences to
minimize computational costs, yet current resampling methods are often weak in
preserving essential semantic information. In this work, we present MVQA, a
Mamba-based model designed for efficient VQA along with a novel Unified
Semantic and Distortion Sampling (USDS) approach. USDS combines semantic patch
sampling from low-resolution videos and distortion patch sampling from
original-resolution videos. The former captures semantically dense regions,
while the latter retains critical distortion details. To prevent computation
increase from dual inputs, we propose a fusion mechanism using pre-defined
masks, enabling a unified sampling strategy that captures both semantic and
quality information without additional computational burden. Experiments show
that the proposed MVQA, equipped with USDS, achieve comparable performance to
state-of-the-art methods while being $2\times$ as fast and requiring only $1/5$
GPU memory.

</details>


### [120] [Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework](https://arxiv.org/abs/2504.16016)
*Xinyuan Song,Yangfan He,Sida Li,Jianhui Wang,Hongyang He,Xinhang Yuan,Ruoyu Wang,Jiaqi Chen,Keqin Li,Kuan Lu,Menghao Huo,Binxu Li,Pei Liu*

Main category: cs.CV

TL;DR: The paper provides a theoretical framework for adapters in DDIM-based models to maintain frame consistency, proving differentiability, convergence, and stability under a temporal consistency loss.


<details>
  <summary>Details</summary>
Motivation: To enhance model performance in video editing tasks with minimal complexity, ensuring frame-to-frame consistency without extensive retraining.

Method: Inserting small, learnable modules into pretrained diffusion models, using prompt learning with shared and frame-specific tokens, and analyzing under a temporal consistency loss.

Result: Proves the temporal consistency objective is differentiable, establishes Lipschitz bounds, shows monotonic loss decrease, and controls error in DDIM inversion.

Conclusion: The theoretical findings reinforce the reliability of adapter-based diffusion methods in video editing and provide insights for video generation tasks.

Abstract: Adapter-based methods are commonly used to enhance model performance with
minimal additional complexity, especially in video editing tasks that require
frame-to-frame consistency. By inserting small, learnable modules into
pretrained diffusion models, these adapters can maintain temporal coherence
without extensive retraining. Approaches that incorporate prompt learning with
both shared and frame-specific tokens are particularly effective in preserving
continuity across frames at low training cost. In this work, we want to provide
a general theoretical framework for adapters that maintain frame consistency in
DDIM-based models under a temporal consistency loss. First, we prove that the
temporal consistency objective is differentiable under bounded feature norms,
and we establish a Lipschitz bound on its gradient. Second, we show that
gradient descent on this objective decreases the loss monotonically and
converges to a local minimum if the learning rate is within an appropriate
range. Finally, we analyze the stability of modules in the DDIM inversion
procedure, showing that the associated error remains controlled. These
theoretical findings will reinforce the reliability of diffusion-based video
editing methods that rely on adapter strategies and provide theoretical
insights in video generation tasks.

</details>


### [121] [Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis](https://arxiv.org/abs/2504.16047)
*Frank Li,Hari Trivedi,Bardia Khosravi,Theo Dapamede,Mohammadreza Chavoshi,Abdulhameed Dere,Rohan Satya Isaac,Aawez Mansuri,Janice Newsome,Saptarshi Purkayastha,Judy Gichoya*

Main category: cs.CV

TL;DR: The study evaluates three vision-language foundation models (RAD-DINO, CheXagent, BiomedCLIP) for radiology tasks, showing task-specific performance differences and the impact of pre-training methods.


<details>
  <summary>Details</summary>
Motivation: To assess the suitability of foundation models for fine-grained radiology tasks like classification, segmentation, and regression.

Method: Evaluated three models on pneumothorax and cardiomegaly tasks using chest radiographs. A custom segmentation model integrating global and local features was also tested.

Result: RAD-DINO excelled in segmentation, CheXagent in classification, and BiomedCLIP was inconsistent. The custom model improved performance, especially for pneumothorax.

Conclusion: Pre-training methodology affects task performance; text-free models suit segmentation, while text-supervised models excel in classification and interpretability.

Abstract: Foundation models, trained on vast amounts of data using self-supervised
techniques, have emerged as a promising frontier for advancing artificial
intelligence (AI) applications in medicine. This study evaluates three
different vision-language foundation models (RAD-DINO, CheXagent, and
BiomedCLIP) on their ability to capture fine-grained imaging features for
radiology tasks. The models were assessed across classification, segmentation,
and regression tasks for pneumothorax and cardiomegaly on chest radiographs.
Self-supervised RAD-DINO consistently excelled in segmentation tasks, while
text-supervised CheXagent demonstrated superior classification performance.
BiomedCLIP showed inconsistent performance across tasks. A custom segmentation
model that integrates global and local features substantially improved
performance for all foundation models, particularly for challenging
pneumothorax segmentation. The findings highlight that pre-training methodology
significantly influences model performance on specific downstream tasks. For
fine-grained segmentation tasks, models trained without text supervision
performed better, while text-supervised models offered advantages in
classification and interpretability. These insights provide guidance for
selecting foundation models based on specific clinical applications in
radiology.

</details>


### [122] [PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning](https://arxiv.org/abs/2504.16023)
*Song Wang,Xiaolu Liu,Lingdong Kong,Jianyun Xu,Chunyong Hu,Gongfan Fang,Wentong Li,Jianke Zhu,Xinchao Wang*

Main category: cs.CV

TL;DR: PointLoRA combines low-rank adaptation (LoRA) with multi-scale token selection for efficient fine-tuning of point cloud models, reducing tunable parameters while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the high computational and storage demands of fully fine-tuning complex pre-trained point cloud models, PointLoRA offers a parameter-efficient solution.

Method: Embed LoRA layers in parameter-intensive components of point cloud transformers and use multi-scale token selection for local feature extraction.

Result: Achieves competitive performance with only 3.43% of trainable parameters across various models and datasets.

Conclusion: PointLoRA is a resource-efficient method for fine-tuning point cloud models, balancing performance and computational cost.

Abstract: Self-supervised representation learning for point cloud has demonstrated
effectiveness in improving pre-trained model performance across diverse tasks.
However, as pre-trained models grow in complexity, fully fine-tuning them for
downstream applications demands substantial computational and storage
resources. Parameter-efficient fine-tuning (PEFT) methods offer a promising
solution to mitigate these resource requirements, yet most current approaches
rely on complex adapter and prompt mechanisms that increase tunable parameters.
In this paper, we propose PointLoRA, a simple yet effective method that
combines low-rank adaptation (LoRA) with multi-scale token selection to
efficiently fine-tune point cloud models. Our approach embeds LoRA layers
within the most parameter-intensive components of point cloud transformers,
reducing the need for tunable parameters while enhancing global feature
capture. Additionally, multi-scale token selection extracts critical local
information to serve as prompts for downstream fine-tuning, effectively
complementing the global context captured by LoRA. The experimental results
across various pre-trained models and three challenging public datasets
demonstrate that our approach achieves competitive performance with only 3.43%
of the trainable parameters, making it highly effective for
resource-constrained applications. Source code is available at:
https://github.com/songw-zju/PointLoRA.

</details>


### [123] [Vision language models are unreliable at trivial spatial cognition](https://arxiv.org/abs/2504.16061)
*Sangeet Khemlani,Tyler Tran,Nathaniel Gyory,Anthony M. Harrison,Wallace E. Lawson,Ravenna Thielstrom,Hunter Thompson,Taaren Singh,J. Gregory Trafton*

Main category: cs.CV

TL;DR: VLMs struggle with trivial spatial cognition tasks, as shown by degraded performance on the TableTest benchmark due to minor prompt variations.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of VLMs in spatial cognition tasks for real-world applicability.

Method: Developed the TableTest benchmark with 3D scenes of objects on a table to evaluate VLMs.

Result: Performance degraded with minor prompt variations, revealing limitations in spatial reasoning.

Conclusion: VLMs have spatial reasoning limitations, but this highlights opportunities for improving training corpora.

Abstract: Vision language models (VLMs) are designed to extract relevant visuospatial
information from images. Some research suggests that VLMs can exhibit humanlike
scene understanding, while other investigations reveal difficulties in their
ability to process relational information. To achieve widespread applicability,
VLMs must perform reliably, yielding comparable competence across a wide
variety of related tasks. We sought to test how reliable these architectures
are at engaging in trivial spatial cognition, e.g., recognizing whether one
object is left of another in an uncluttered scene. We developed a benchmark
dataset -- TableTest -- whose images depict 3D scenes of objects arranged on a
table, and used it to evaluate state-of-the-art VLMs. Results show that
performance could be degraded by minor variations of prompts that use logically
equivalent descriptions. These analyses suggest limitations in how VLMs may
reason about spatial relations in real-world applications. They also reveal
novel opportunities for bolstering image caption corpora for more efficient
training and testing.

</details>


### [124] [LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](https://arxiv.org/abs/2504.16030)
*Joya Chen,Ziyun Zeng,Yiqi Lin,Wei Li,Zejun Ma,Mike Zheng Shou*

Main category: cs.CV

TL;DR: The paper proposes a scalable training method for Video LLMs using cheap ASR transcripts, introducing datasets and achieving competitive performance without costly annotations.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on expensive human annotations or proprietary APIs for training Video LLMs, enabling large-scale training with ASR transcripts.

Method: A streaming training approach interleaving ASR words and video frames by timestamps, supported by datasets Live-CC-5M and Live-WhisperX-526K.

Result: The ASR-only pre-trained model (LiveCC-7B-Base) shows strong video QA performance and real-time commentary capability, surpassing larger models in quality.

Conclusion: The approach demonstrates broad generalizability and state-of-the-art results, with all resources publicly released.

Abstract: Recent video large language models (Video LLMs) often depend on costly human
annotations or proprietary model APIs (e.g., GPT-4o) to produce training data,
which limits their training at scale. In this paper, we explore large-scale
training for Video LLM with cheap automatic speech recognition (ASR)
transcripts. Specifically, we propose a novel streaming training approach that
densely interleaves the ASR words and video frames according to their
timestamps. Compared to previous studies in vision-language representation with
ASR, our method naturally fits the streaming characteristics of ASR, thus
enabling the model to learn temporally-aligned, fine-grained vision-language
modeling. To support the training algorithm, we introduce a data production
pipeline to process YouTube videos and their closed captions (CC, same as ASR),
resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset
for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,
the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general
video QA performance and exhibits a new capability in real-time video
commentary. To evaluate this, we carefully design a new LiveSports-3K
benchmark, using LLM-as-a-judge to measure the free-form commentary.
Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B
models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even
working in a real-time mode. Meanwhile, it achieves state-of-the-art results at
the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,
demonstrating the broad generalizability of our approach. All resources of this
paper have been released at https://showlab.github.io/livecc.

</details>


### [125] [Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/abs/2504.16072)
*Long Lian,Yifan Ding,Yunhao Ge,Sifei Liu,Hanzi Mao,Boyi Li,Marco Pavone,Ming-Yu Liu,Trevor Darrell,Adam Yala,Yin Cui*

Main category: cs.CV

TL;DR: The paper introduces the Describe Anything Model (DAM) for detailed localized captioning (DLC), featuring innovations like focal prompts and a localized vision backbone. It also proposes a semi-supervised learning-based data pipeline (DLC-SDP) and a new benchmark (DLC-Bench). DAM achieves state-of-the-art results on 7 benchmarks.


<details>
  <summary>Details</summary>
Motivation: The challenge of generating detailed and accurate descriptions for specific regions in images and videos remains unresolved for vision-language models.

Method: DAM uses focal prompts for high-resolution encoding and a localized vision backbone. DLC-SDP leverages semi-supervised learning to expand datasets from segmentation data to unlabeled web images.

Result: DAM achieves state-of-the-art performance on 7 benchmarks for localized captioning tasks.

Conclusion: DAM and DLC-SDP effectively address the challenge of detailed localized captioning, setting new benchmarks in the field.

Abstract: Generating detailed and accurate descriptions for specific regions in images
and videos remains a fundamental challenge for vision-language models. We
introduce the Describe Anything Model (DAM), a model designed for detailed
localized captioning (DLC). DAM preserves both local details and global context
through two key innovations: a focal prompt, which ensures high-resolution
encoding of targeted regions, and a localized vision backbone, which integrates
precise localization with its broader context. To tackle the scarcity of
high-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data
Pipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and
expands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark
designed to evaluate DLC without relying on reference captions. DAM sets new
state-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and
detailed multi-sentence localized image and video captioning.

</details>


### [126] [Boosting Generative Image Modeling via Joint Image-Feature Synthesis](https://arxiv.org/abs/2504.16064)
*Theodoros Kouzelis,Efstathios Karypidis,Ioannis Kakogeorgiou,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CV

TL;DR: A novel framework integrates representation learning with generative modeling using latent-semantic diffusion, improving image quality and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between representation learning and generative modeling in latent diffusion models (LDMs) for high-quality image generation.

Method: Leverages a diffusion model to jointly model low-level image latents and high-level semantic features, simplifying training with minimal architectural changes.

Result: Enhances generative quality and training efficiency, introduces Representation Guidance for refined image generation.

Conclusion: Establishes a new direction for representation-aware generative modeling with improved image quality and convergence speed.

Abstract: Latent diffusion models (LDMs) dominate high-quality image generation, yet
integrating representation learning with generative modeling remains a
challenge. We introduce a novel generative image modeling framework that
seamlessly bridges this gap by leveraging a diffusion model to jointly model
low-level image latents (from a variational autoencoder) and high-level
semantic features (from a pretrained self-supervised encoder like DINO). Our
latent-semantic diffusion approach learns to generate coherent image-feature
pairs from pure noise, significantly enhancing both generative quality and
training efficiency, all while requiring only minimal modifications to standard
Diffusion Transformer architectures. By eliminating the need for complex
distillation objectives, our unified design simplifies training and unlocks a
powerful new inference strategy: Representation Guidance, which leverages
learned semantics to steer and refine image generation. Evaluated in both
conditional and unconditional settings, our method delivers substantial
improvements in image quality and training convergence speed, establishing a
new direction for representation-aware generative modeling.

</details>


### [127] [From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning](https://arxiv.org/abs/2504.16080)
*Le Zhuo,Liangbing Zhao,Sayak Paul,Yue Liao,Renrui Zhang,Yi Xin,Peng Gao,Mohamed Elhoseiny,Hongsheng Li*

Main category: cs.CV

TL;DR: ReflectionFlow is an inference-time framework for diffusion models, introducing noise-level, prompt-level, and reflection-level scaling to iteratively refine outputs, outperforming naive methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current text-to-image diffusion models in handling complex scenes and fine-grained details by leveraging self-reflection capabilities.

Method: Proposes ReflectionFlow with three scaling axes: noise-level, prompt-level, and reflection-level. Uses GenRef dataset (1M triplets) for reflection tuning on FLUX.1-dev.

Result: Significantly outperforms naive noise-level scaling methods, improving image synthesis quality.

Conclusion: ReflectionFlow offers a scalable and efficient solution for higher-quality image generation in challenging tasks.

Abstract: Recent text-to-image diffusion models achieve impressive visual quality
through extensive scaling of training data and model parameters, yet they often
struggle with complex scenes and fine-grained details. Inspired by the
self-reflection capabilities emergent in large language models, we propose
ReflectionFlow, an inference-time framework enabling diffusion models to
iteratively reflect upon and refine their outputs. ReflectionFlow introduces
three complementary inference-time scaling axes: (1) noise-level scaling to
optimize latent initialization; (2) prompt-level scaling for precise semantic
guidance; and most notably, (3) reflection-level scaling, which explicitly
provides actionable reflections to iteratively assess and correct previous
generations. To facilitate reflection-level scaling, we construct GenRef, a
large-scale dataset comprising 1 million triplets, each containing a
reflection, a flawed image, and an enhanced image. Leveraging this dataset, we
efficiently perform reflection tuning on state-of-the-art diffusion
transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified
framework. Experimental results show that ReflectionFlow significantly
outperforms naive noise-level scaling methods, offering a scalable and
compute-efficient solution toward higher-quality image synthesis on challenging
tasks.

</details>


### [128] [MR. Video: "MapReduce" is the Principle for Long Video Understanding](https://arxiv.org/abs/2504.16082)
*Ziqi Pang,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: MR. Video is a framework for long video understanding using a MapReduce principle: Map for dense short clip perception and Reduce for joint aggregation. It outperforms existing methods by 10% on LVBench.


<details>
  <summary>Details</summary>
Motivation: Existing methods for long video understanding are limited by context length or rely on sequential key segment selection, which is less scalable and comprehensive.

Method: MR. Video uses a two-stage MapReduce approach: (A) Captioning (Map: caption clips, Reduce: standardize names) and (B) Analysis (Map: analyze clips, Reduce: integrate answers).

Result: Achieves over 10% accuracy improvement on LVBench compared to state-of-the-art methods.

Conclusion: The MapReduce principle is effective for long video understanding, offering scalability and comprehensive reasoning.

Abstract: We propose MR. Video, an agentic long video understanding framework that
demonstrates the simple yet effective MapReduce principle for processing long
videos: (1) Map: independently and densely perceiving short video clips, and
(2) Reduce: jointly aggregating information from all clips. Compared with
sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed
short video perception without being limited by context length. Compared with
existing video agents that typically rely on sequential key segment selection,
the Map operation enables simpler and more scalable sequence parallel
perception of short video segments. Its Reduce step allows for more
comprehensive context aggregation and reasoning, surpassing explicit key
segment retrieval. This MapReduce principle is applicable to both VLMs and
video agents, and we use LLM agents to validate its effectiveness.
  In practice, MR. Video employs two MapReduce stages: (A) Captioning:
generating captions for short video clips (map), then standardizing repeated
characters and objects into shared names (reduce); (B) Analysis: for each user
question, analyzing relevant information from individual short videos (map),
and integrating them into a final answer (reduce). MR. Video achieves over 10%
accuracy improvement on the challenging LVBench compared to state-of-the-art
VLMs and video agents.
  Code is available at: https://github.com/ziqipang/MR-Video

</details>


### [129] [MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](https://arxiv.org/abs/2504.16083)
*Yucheng Li,Huiqiang Jiang,Chengruidong Zhang,Qianhui Wu,Xufang Luo,Surin Ahn,Amir H. Abdi,Dongsheng Li,Jianfeng Gao,Yuqing Yang,Lili Qiu*

Main category: cs.CV

TL;DR: MMInference introduces a dynamic sparse attention method to accelerate the pre-filling stage for long-context multi-modal inputs in VLMs, achieving up to 8.3x speedup without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Quadratic attention complexity in VLMs hinders real-world deployment; MMInference addresses this by leveraging sparse patterns in multi-modal inputs.

Method: Uses a permutation-based method to exploit the Grid pattern in video inputs and dynamically constructs sparse distributions. Optimized GPU kernels enhance efficiency.

Result: Achieves up to 8.3x speedup in pre-filling at 1M tokens while maintaining accuracy across multi-modal benchmarks.

Conclusion: MMInference effectively accelerates VLMs without model modifications, offering practical benefits for long-context multi-modal tasks.

Abstract: The integration of long-context capabilities with visual understanding
unlocks unprecedented potential for Vision Language Models (VLMs). However, the
quadratic attention complexity during the pre-filling phase remains a
significant obstacle to real-world deployment. To overcome this limitation, we
introduce MMInference (Multimodality Million tokens Inference), a dynamic
sparse attention method that accelerates the prefilling stage for long-context
multi-modal inputs. First, our analysis reveals that the temporal and spatial
locality of video input leads to a unique sparse pattern, the Grid pattern.
Simultaneously, VLMs exhibit markedly different sparse distributions across
different modalities. We introduce a permutation-based method to leverage the
unique Grid pattern and handle modality boundary issues. By offline search the
optimal sparse patterns for each head, MMInference constructs the sparse
distribution dynamically based on the input. We also provide optimized GPU
kernels for efficient sparse computations. Notably, MMInference integrates
seamlessly into existing VLM pipelines without any model modifications or
fine-tuning. Experiments on multi-modal benchmarks-including Video QA,
Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art
long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that
MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while
maintaining accuracy. Our code is available at https://aka.ms/MMInference.

</details>


### [130] [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669)
*Baiqi Li,Zhiqiu Lin,Wenxuan Peng,Jean de Dieu Nyandwi,Daniel Jiang,Zixian Ma,Simran Khanuja,Ranjay Krishna,Graham Neubig,Deva Ramanan*

Main category: cs.CV

TL;DR: VLMs struggle with natural adversarial samples, prompting the creation of NaturalBench, a challenging benchmark revealing significant performance gaps between models and humans.


<details>
  <summary>Details</summary>
Motivation: To assess if VLMs truly excel in complex visio-linguistic reasoning by testing them on natural adversarial samples humans easily solve.

Method: A semi-automated approach using CLIP and ChatGPT to generate and verify 10,000 VQA samples, designed vision-centrically with paired images.

Result: State-of-the-art VLMs lag 50%-70% behind human performance (90+%), revealing biases and skill gaps in compositionality.

Conclusion: NaturalBench highlights VLMs' limitations and biases, offering a robust framework for dynamic evaluation across diverse data sources.

Abstract: Vision-language models (VLMs) have made significant progress in recent
visual-question-answering (VQA) benchmarks that evaluate complex
visio-linguistic reasoning. However, are these models truly effective? In this
work, we show that VLMs still struggle with natural images and questions that
humans can easily answer, which we term natural adversarial samples. We also
find it surprisingly easy to generate these VQA samples from natural image-text
corpora using off-the-shelf models like CLIP and ChatGPT. We propose a
semi-automated approach to collect a new benchmark, NaturalBench, for reliably
evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a
$\textbf{vision-centric}$ design by pairing each question with two images that
yield different answers, preventing blind solutions from answering without
using the images. This makes NaturalBench more challenging than previous
benchmarks that can be solved with commonsense priors. We evaluate 53
state-of-the-art VLMs on NaturalBench, showing that models like
LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o
lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is
hard from two angles: (1) Compositionality: Solving NaturalBench requires
diverse visio-linguistic skills, including understanding attribute bindings,
object relationships, and advanced reasoning like logic and counting. To this
end, unlike prior work that uses a single tag per sample, we tag each
NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)
Biases: NaturalBench exposes severe biases in VLMs, as models often choose the
same answer regardless of the image. Lastly, we apply our benchmark curation
method to diverse data sources, including long captions (over 100 words) and
non-English languages like Chinese and Hindi, highlighting its potential for
dynamic evaluations of VLMs.

</details>


### [131] [LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision](https://arxiv.org/abs/2304.07647)
*Jiani Huang,Ziyang Li,Mayur Naik,Ser-Nam Lim*

Main category: cs.CV

TL;DR: LASER trains spatio-temporal scene graph (STSG) generators using video captions as weak supervision, outperforming fully-supervised baselines.


<details>
  <summary>Details</summary>
Motivation: Avoiding labor-intensive STSG annotations by leveraging readily available video captions.

Method: Uses large language models to extract logical specifications from captions and aligns predicted STSG with specifications via a differentiable symbolic reasoner and multiple losses.

Result: Achieves significant improvements in predicate prediction accuracy on OpenPVSG, 20BN, and MUGEN datasets.

Conclusion: LASER enables efficient STSG learning without manual annotations, demonstrating superior performance over supervised methods.

Abstract: Supervised approaches for learning spatio-temporal scene graphs (STSG) from
video are greatly hindered due to their reliance on STSG-annotated videos,
which are labor-intensive to construct at scale. Is it feasible to instead use
readily available video captions as weak supervision? To address this question,
we propose LASER, a neuro-symbolic framework to enable training STSG generators
using only video captions. LASER employs large language models to first extract
logical specifications with rich spatio-temporal semantic information from
video captions. LASER then trains the underlying STSG generator to align the
predicted STSG with the specification. The alignment algorithm overcomes the
challenges of weak supervision by leveraging a differentiable symbolic reasoner
and using a combination of contrastive, temporal, and semantics losses. The
overall approach efficiently trains low-level perception models to extract a
fine-grained STSG that conforms to the video caption. In doing so, it enables a
novel methodology for learning STSGs without tedious annotations. We evaluate
our method on three video datasets: OpenPVSG, 20BN, and MUGEN. Our approach
demonstrates substantial improvements over fully-supervised baselines,
achieving a unary predicate prediction accuracy of 27.78% (+12.65%) and a
binary recall@5 of 0.42 (+0.22) on OpenPVSG. Additionally, LASER exceeds
baselines by 7% on 20BN and 5.2% on MUGEN in terms of overall predicate
prediction accuracy.

</details>


### [132] [Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models Trained on Corrupted Data](https://arxiv.org/abs/2403.08728)
*Asad Aali,Giannis Daras,Brett Levac,Sidharth Kumar,Alexandros G. Dimakis,Jonathan I. Tamir*

Main category: cs.CV

TL;DR: A framework for solving inverse problems using diffusion models trained on corrupted data, with applications in MRI and natural image restoration.


<details>
  <summary>Details</summary>
Motivation: To address inverse problems in scenarios where only corrupted data is available, leveraging diffusion models for improved reconstruction.

Method: Extends Ambient Diffusion for training on Fourier-corrupted data; proposes Ambient Diffusion Posterior Sampling (A-DPS) for reconstruction using pre-trained models on different corruptions.

Result: A-DPS models trained on subsampled data outperform those on fully sampled data in MRI reconstruction and natural image restoration tasks.

Conclusion: The framework and A-DPS algorithm demonstrate effectiveness in solving inverse problems with corrupted data, offering potential in medical imaging and image restoration.

Abstract: We provide a framework for solving inverse problems with diffusion models
learned from linearly corrupted data. Firstly, we extend the Ambient Diffusion
framework to enable training directly from measurements corrupted in the
Fourier domain. Subsequently, we train diffusion models for MRI with access
only to Fourier subsampled multi-coil measurements at acceleration factors R=
2,4,6,8. Secondly, we propose Ambient Diffusion Posterior Sampling (A-DPS), a
reconstruction algorithm that leverages generative models pre-trained on one
type of corruption (e.g. image inpainting) to perform posterior sampling on
measurements from a different forward process (e.g. image blurring). For MRI
reconstruction in high acceleration regimes, we observe that A-DPS models
trained on subsampled data are better suited to solving inverse problems than
models trained on fully sampled data. We also test the efficacy of A-DPS on
natural image datasets (CelebA, FFHQ, and AFHQ) and show that A-DPS can
sometimes outperform models trained on clean data for several image restoration
tasks in both speed and performance.

</details>


### [133] [TextSquare: Scaling up Text-Centric Visual Instruction Tuning](https://arxiv.org/abs/2404.12803)
*Jingqun Tang,Chunhui Lin,Zhen Zhao,Shu Wei,Binghong Wu,Qi Liu,Hao Feng,Yang Li,Siqi Wang,Lei Liao,Wei Shi,Yuliang Liu,Hao Liu,Yuan Xie,Xiang Bai,Can Huang*

Main category: cs.CV

TL;DR: The paper introduces Square-10M, a high-quality instruction-tuning dataset for text-centric VQA, generated using closed-source MLLMs. The model TextSquare outperforms open-source and top-tier models, highlighting the importance of dataset scale and reasoning data.


<details>
  <summary>Details</summary>
Motivation: Open-source models lag behind leading models like GPT4V and Gemini due to limited high-quality instruction tuning data. The goal is to bridge this gap by creating a large, high-quality dataset.

Method: The Square method involves four steps: Self-Questioning, Answering, Reasoning, and Evaluation, to generate the Square-10M dataset.

Result: TextSquare surpasses open-source models and outperforms GPT4V and Gemini in 6/10 benchmarks. It also improves accuracy and reduces hallucinations in VQA tasks.

Conclusion: Scaling text-centric VQA datasets exponentially improves model performance, validating the necessity of large, high-quality datasets like Square-10M.

Abstract: Text-centric visual question answering (VQA) has made great strides with the
development of Multimodal Large Language Models (MLLMs), yet open-source models
still fall short of leading models like GPT4V and Gemini, partly due to a lack
of extensive, high-quality instruction tuning data. To this end, we introduce a
new approach for creating a massive, high-quality instruction-tuning dataset,
Square-10M, which is generated using closed-source MLLMs. The data construction
process, termed Square, consists of four steps: Self-Questioning, Answering,
Reasoning, and Evaluation. Our experiments with Square-10M led to three key
findings: 1) Our model, TextSquare, considerably surpasses open-source previous
state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%).
It even outperforms top-tier models like GPT4V and Gemini in 6 of 10
text-centric benchmarks. 2) Additionally, we demonstrate the critical role of
VQA reasoning data in offering comprehensive contextual insights for specific
questions. This not only improves accuracy but also significantly mitigates
hallucinations. Specifically, TextSquare scores an average of 75.1% across four
general VQA and hallucination evaluation datasets, outperforming previous
state-of-the-art models. 3) Notably, the phenomenon observed in scaling
text-centric VQA datasets reveals a vivid pattern: the exponential increase of
instruction tuning data volume is directly proportional to the improvement in
model performance, thereby validating the necessity of the dataset scale and
the high quality of Square-10M.

</details>


### [134] [Normal-guided Detail-Preserving Neural Implicit Function for High-Fidelity 3D Surface Reconstruction](https://arxiv.org/abs/2406.04861)
*Aarya Patel,Hamid Laga,Ojaswa Sharma*

Main category: cs.CV

TL;DR: Training neural representations with surface normals improves 3D reconstruction accuracy, even with sparse RGB images.


<details>
  <summary>Details</summary>
Motivation: Existing neural implicit representations struggle to capture fine details and thin structures from sparse multi-view RGB images.

Method: Use first-order differential properties (surface normals) derived from depth maps of RGB images to train the SDF network, supervising surface points directly.

Result: Achieves state-of-the-art reconstruction accuracy with minimal views, capturing intricate details and thin structures.

Conclusion: Incorporating surface normals significantly enhances 3D reconstruction quality from sparse inputs.

Abstract: Neural implicit representations have emerged as a powerful paradigm for 3D
reconstruction. However, despite their success, existing methods fail to
capture fine geometric details and thin structures, especially in scenarios
where only sparse multi-view RGB images of the objects of interest are
available. This paper shows that training neural representations with
first-order differential properties (surface normals) leads to highly accurate
3D surface reconstruction, even with as few as two RGB images. Using input RGB
images, we compute approximate ground-truth surface normals from depth maps
produced by an off-the-shelf monocular depth estimator. During training, we
directly locate the surface point of the SDF network and supervise its normal
with the one estimated from the depth map. Extensive experiments demonstrate
that our method achieves state-of-the-art reconstruction accuracy with a
minimal number of views, capturing intricate geometric details and thin
structures that were previously challenging to capture.

</details>


### [135] [LOOC: Localizing Organs using Occupancy Networks and Body Surface Depth Images](https://arxiv.org/abs/2406.12407)
*Pit Henrich,Franziska Mathis-Ullrich*

Main category: cs.CV

TL;DR: A novel method for precise localization of 67 anatomical structures from single depth images using a multi-class occupancy network, outperforming template matching and enabling non-invasive medical applications.


<details>
  <summary>Details</summary>
Motivation: To improve automated medical imaging and diagnostics by accurately localizing anatomical structures non-invasively from depth images.

Method: Uses a multi-class occupancy network trained on segmented CT scans with body-pose augmentation and a specialized sampling strategy for dense organs.

Result: Outperforms template matching, provides detailed 3D anatomical atlases, and enables qualitative real-world reconstructions.

Conclusion: The method enhances non-invasive localization of critical structures, promising advancements in medical imaging and diagnostics.

Abstract: We introduce a novel approach for the precise localization of 67 anatomical
structures from single depth images captured from the exterior of the human
body. Our method uses a multi-class occupancy network, trained using segmented
CT scans augmented with body-pose changes, and incorporates a specialized
sampling strategy to handle densely packed internal organs. Our contributions
include the application of occupancy networks for occluded structure
localization, a robust method for estimating anatomical positions from depth
images, and the creation of detailed, individualized 3D anatomical atlases. We
outperform localization using template matching and provide qualitative
real-world reconstructions. This method promises improvements in automated
medical imaging and diagnostic procedures by offering accurate, non-invasive
localization of critical anatomical structures.

</details>


### [136] [Towards Robust Infrared Small Target Detection: A Feature-Enhanced and Sensitivity-Tunable Framework](https://arxiv.org/abs/2407.20090)
*Jinmiao Zhao,Zelin Shi,Chuang Yu,Yunpeng Liu,Yimian Dai*

Main category: cs.CV

TL;DR: The paper introduces a feature-enhanced and sensitivity-tunable (FEST) framework to improve infrared small target detection by enhancing features and regulating target confidence, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Precise segmentation of infrared small targets is challenging due to feature scarcity. The FEST framework aims to enhance detection performance without altering network architectures.

Method: The FEST framework uses multi-scale fusion and edge enhancement difficulty mining (EEDM) loss for feature enhancement, and an adjustable sensitivity (AS) strategy for target confidence regulation.

Result: The framework significantly improves detection performance, with MSDA-Net (equipped with FEST) winning first prize in the PRCV 2024 competition.

Conclusion: The FEST framework effectively enhances existing SIRST detection networks, offering robust performance in complex scenarios.

Abstract: Recently, single-frame infrared small target (SIRST) detection technology has
attracted wide-spread attention. However, due to the intrinsic feature scarcity
in infrared small targets, precise segmentation of small targets from complex
backgrounds remains a significant challenge. Different from most existing deep
learning-based methods that focus on improving network architectures, we
propose a feature-enhanced and sensitivity-tunable (FEST) framework, which is
compatible with existing SIRST detection networks and further enhances their
detection performance. The FEST framework improves the model's robustness from
two aspects: feature enhancement and target confidence regulation. For feature
enhancement, on the one hand, we adopt a multi-scale fusion strategy, which can
effectively improve the model's perception and adaptability to multi-scale
features of multi-size targets. On the other hand, we construct an edge
enhancement difficulty mining (EEDM) loss based on the analysis of the task
characteristics, which helps guide the network to continuously focus on
challenging target regions and edge features during training. For target
confidence regulation, we design an adjustable sensitivity (AS) strategy for
network post-processing. This strategy not only enhances the adaptability of
the network in complex scenarios, but also significantly improves the detection
rate of infrared small targets while maintaining segmentation accuracy.
Extensive experimental results show that our FEST framework can significantly
enhance the performance of existing SIRST detection networks. Notably, the
multi-scale direction-aware network (MSDA-Net) equipped with the FEST framework
won the first prize in the PRCV 2024 wide-area infrared small target detection
competition.

</details>


### [137] [PolyFootNet: Extracting Polygonal Building Footprints in Off-Nadir Remote Sensing Images](https://arxiv.org/abs/2408.08645)
*Kai Li,Yupeng Deng,Jingbo Chen,Yu Meng,Zhihao Xi,Junxian Ma,Chenhao Wang,Maolin Wang,Xiangyu Zhao*

Main category: cs.CV

TL;DR: PolyFootNet is a deep-learning framework for extracting polygonal building footprints from off-nadir imagery without post-processing, using a High-Quality Mask Prompter and Self Offset Attention for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for polygonal building extraction are limited by boundary precision and fail under off-nadir viewing angles, necessitating a more robust solution.

Method: PolyFootNet combines a High-Quality Mask Prompter for precise roof masks and a Self Offset Attention mechanism (based on Nadaraya-Watson regression) to correct angular discrepancies, unifying mask and offset predictions.

Result: Extensive experiments show PolyFootNet outperforms existing methods, achieving higher accuracy and generalizability for off-nadir imagery.

Conclusion: PolyFootNet offers a precise, robust, and generalizable solution for polygonal building footprint extraction from off-nadir imagery, with potential for further research via released pre-trained weights.

Abstract: Extracting polygonal building footprints from off-nadir imagery is crucial
for diverse applications. Current deep-learning-based extraction approaches
predominantly rely on semantic segmentation paradigms and post-processing
algorithms, limiting their boundary precision and applicability. However,
existing polygonal extraction methodologies are inherently designed for
near-nadir imagery and fail under the geometric complexities introduced by
off-nadir viewing angles. To address these challenges, this paper introduces
Polygonal Footprint Network (PolyFootNet), a novel deep-learning framework that
directly outputs polygonal building footprints without requiring external
post-processing steps. PolyFootNet employs a High-Quality Mask Prompter to
generate precise roof masks, which guide polygonal vertex extraction in a
unified model pipeline. A key contribution of PolyFootNet is introducing the
Self Offset Attention mechanism, grounded in Nadaraya-Watson regression, to
effectively mitigate the accuracy discrepancy observed between low-rise and
high-rise buildings. This approach allows low-rise building predictions to
leverage angular corrections learned from high-rise building offsets,
significantly enhancing overall extraction accuracy. Additionally, motivated by
the inherent ambiguity of building footprint extraction tasks, we
systematically investigate alternative extraction paradigms and demonstrate
that a combined approach of building masks and offsets achieves superior
polygonal footprint results. Extensive experiments validate PolyFootNet's
effectiveness, illustrating its promising potential as a robust, generalizable,
and precise polygonal building footprint extraction method from challenging
off-nadir imagery. To facilitate further research, we will release pre-trained
weights of our offset prediction module at
https://github.com/likaiucas/PolyFootNet.

</details>


### [138] [Onboard Satellite Image Classification for Earth Observation: A Comparative Study of ViT Models](https://arxiv.org/abs/2409.03901)
*Thanh-Dung Le,Vu Nguyen Ha,Ti Ti Nguyen,Geoffrey Eappen,Prabhu Thiruvasagam,Hong-fu Chou,Duc-Dung Tran,Hung Nguyen-Kha,Luis M. Garces-Socarras,Jorge L. Gonzalez-Rios,Juan Carlos Merlano-Duncan,Symeon Chatzinotas*

Main category: cs.CV

TL;DR: Pre-trained Vision Transformer models, especially EfficientViT-M2, outperform others in land use classification for satellite processing, offering high accuracy, efficiency, and robustness under noisy conditions.


<details>
  <summary>Details</summary>
Motivation: To identify the most effective pre-trained model for land use classification in onboard satellite processing, focusing on accuracy, computational efficiency, and robustness against noisy data.

Method: Comparison of traditional CNN-based, ResNet-based, and pre-trained vision Transformer models through extensive experimentation.

Result: EfficientViT-M2 achieves 98.76% accuracy, excels in training and inference efficiency, and is highly robust (score 0.79). It also consumes significantly less power (63.93% less than MobileViTV2).

Conclusion: EfficientViT-M2 is the optimal choice for reliable and efficient land use classification in satellite operations due to its superior performance, efficiency, and energy savings.

Abstract: This study focuses on identifying the most effective pre-trained model for
land use classification in onboard satellite processing, emphasizing achieving
high accuracy, computational efficiency, and robustness against noisy data
conditions commonly encountered during satellite-based inference. Through
extensive experimentation, we compare the performance of traditional CNN-based,
ResNet-based, and various pre-trained vision Transformer models. Our findings
demonstrate that pre-trained Vision Transformer (ViT) models, particularly
MobileViTV2 and EfficientViT-M2, outperform models trained from scratch in
terms of accuracy and efficiency. These models achieve high performance with
reduced computational requirements and exhibit greater resilience during
inference under noisy conditions. While MobileViTV2 has excelled on clean
validation data, EfficientViT-M2 has proved more robust when handling noise,
making it the most suitable model for onboard satellite EO tasks. Our
experimental results demonstrate that EfficientViT-M2 is the optimal choice for
reliable and efficient RS-IC in satellite operations, achieving 98.76 % of
accuracy, precision, and recall. Precisely, EfficientViT-M2 delivers the
highest performance across all metrics, excels in training efficiency (1,000s)
and inference time (10s), and demonstrates greater robustness (overall
robustness score of 0.79). Consequently, EfficientViT-M2 consumes 63.93 % less
power than MobileViTV2 (79.23 W) and 73.26 % less power than SwinTransformer
(108.90 W). This highlights its significant advantage in energy efficiency.

</details>


### [139] [ThermalGaussian: Thermal 3D Gaussian Splatting](https://arxiv.org/abs/2409.07200)
*Rongfeng Lu,Hangyu Chen,Zunjie Zhu,Yuhang Qin,Ming Lu,Le Zhang,Chenggang Yan,Anke Xue*

Main category: cs.CV

TL;DR: ThermalGaussian introduces a 3D Gaussian splatting (3DGS) approach for high-quality thermal and RGB image rendering, addressing alignment and overfitting with multimodal constraints and a new dataset.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF-based methods for thermal scene reconstruction are slow, while 3DGS offers faster training and real-time rendering. This work aims to leverage 3DGS for thermal imaging.

Method: The method involves calibrating RGB and thermal cameras, learning multimodal 3D Gaussians, and applying regularization and smoothing constraints to prevent overfitting and align with thermal physics.

Result: ThermalGaussian achieves photorealistic thermal and improved RGB rendering, reduces storage costs by 90%, and introduces the RGBT-Scenes dataset.

Conclusion: ThermalGaussian successfully adapts 3DGS for thermal imaging, offering efficient, high-quality multimodal rendering and a valuable dataset for future research.

Abstract: Thermography is especially valuable for the military and other users of
surveillance cameras. Some recent methods based on Neural Radiance Fields
(NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of
thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS)
prevails due to its rapid training and real-time rendering. In this work, we
propose ThermalGaussian, the first thermal 3DGS approach capable of rendering
high-quality images in RGB and thermal modalities. We first calibrate the RGB
camera and the thermal camera to ensure that both modalities are accurately
aligned. Subsequently, we use the registered images to learn the multimodal 3D
Gaussians. To prevent the overfitting of any single modality, we introduce
several multimodal regularization constraints. We also develop smoothing
constraints tailored to the physical characteristics of the thermal modality.
Besides, we contribute a real-world dataset named RGBT-Scenes, captured by a
hand-hold thermal-infrared camera, facilitating future research on thermal
scene reconstruction. We conduct comprehensive experiments to show that
ThermalGaussian achieves photorealistic rendering of thermal images and
improves the rendering quality of RGB images. With the proposed multimodal
regularization constraints, we also reduced the model's storage cost by 90%.
Our project page is at https://thermalgaussian.github.io/.

</details>


### [140] [LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content](https://arxiv.org/abs/2410.10783)
*Nimrod Shabtay,Felipe Maia Polo,Sivan Doveh,Wei Lin,M. Jehanzeb Mirza,Leshem Chosen,Mikhail Yurochkin,Yuekai Sun,Assaf Arbelle,Leonid Karlinsky,Raja Giryes*

Main category: cs.CV

TL;DR: LiveXiv is a scalable, evolving benchmark using ArXiv papers to generate VQA pairs automatically, avoiding test data contamination and reducing evaluation costs.


<details>
  <summary>Details</summary>
Motivation: To address test data contamination in multi-modal models and provide a reliable benchmark for evaluating their true abilities.

Method: LiveXiv generates VQA pairs from ArXiv papers' multi-modal content (graphs, charts, tables) without human intervention and introduces an efficient evaluation approach using subset evaluations.

Result: Benchmarked LMMs show LiveXiv is challenging, exposing true model abilities with minimal performance variance (<2.5%) between automatic and manual annotations.

Conclusion: LiveXiv offers a scalable, high-quality benchmark for evaluating multi-modal models, reducing contamination and evaluation costs.

Abstract: The large-scale training of multi-modal models on data scraped from the web
has shown outstanding utility in infusing these models with the required world
knowledge to perform effectively on multiple downstream tasks. However, one
downside of scraping data from the web can be the potential sacrifice of the
benchmarks on which the abilities of these models are often evaluated. To
safeguard against test data contamination and to truly test the abilities of
these foundation models we propose LiveXiv: A scalable evolving live benchmark
based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts
at any given timestamp and proposes to automatically generate visual
question-answer pairs (VQA). This is done without any human-in-the-loop, using
the multi-modal content in the manuscripts, like graphs, charts, and tables.
Moreover, we introduce an efficient evaluation approach that estimates the
performance of all models on the evolving benchmark using evaluations of only a
subset of models. This significantly reduces the overall evaluation cost. We
benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the
first version of our benchmark, showing its challenging nature and exposing the
models true abilities, avoiding contamination. Lastly, in our commitment to
high quality, we have collected and evaluated a manually verified subset. By
comparing its overall results to our automatic annotations, we have found that
the performance variance is indeed minimal (<2.5%). Our dataset is available
online on HuggingFace, and our code will be available here.

</details>


### [141] [CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos](https://arxiv.org/abs/2411.17820)
*Xinhao Liu,Jintong Li,Yicheng Jiang,Niranjan Sujay,Zhicheng Yang,Juexiao Zhang,John Abanes,Jing Zhang,Chen Feng*

Main category: cs.CV

TL;DR: A scalable, data-driven approach for human-like urban navigation is proposed, using large-scale imitation learning from web-sourced city videos to enhance autonomous agent performance.


<details>
  <summary>Details</summary>
Motivation: Existing visual navigation methods struggle in map-free or off-street settings, limiting autonomous agents like delivery robots.

Method: A data processing pipeline extracts action supervision from city walking/driving videos for large-scale imitation learning without costly annotations.

Result: Training on diverse datasets significantly improves navigation performance, surpassing current methods.

Conclusion: Abundant online video data can develop robust navigation policies for embodied agents in dynamic urban environments.

Abstract: Navigating dynamic urban environments presents significant challenges for
embodied agents, requiring advanced spatial reasoning and adherence to
common-sense norms. Despite progress, existing visual navigation methods
struggle in map-free or off-street settings, limiting the deployment of
autonomous agents like last-mile delivery robots. To overcome these obstacles,
we propose a scalable, data-driven approach for human-like urban navigation by
training agents on thousands of hours of in-the-wild city walking and driving
videos sourced from the web. We introduce a simple and scalable data processing
pipeline that extracts action supervision from these videos, enabling
large-scale imitation learning without costly annotations. Our model learns
sophisticated navigation policies to handle diverse challenges and critical
scenarios. Experimental results show that training on large-scale, diverse
datasets significantly enhances navigation performance, surpassing current
methods. This work shows the potential of using abundant online video data to
develop robust navigation policies for embodied agents in dynamic urban
settings. Project homepage is at https://ai4ce.github.io/CityWalker/.

</details>


### [142] [HEMGS: A Hybrid Entropy Model for 3D Gaussian Splatting Data Compression](https://arxiv.org/abs/2411.18473)
*Lei Liu,Zhenghao Chen,Wei Jiang,Wei Wang,Dong Xu*

Main category: cs.CV

TL;DR: A novel compression framework for 3D Gaussian Splatting (3DGS) data using a Hybrid Entropy Model (HEMGS) achieves hybrid lossy-lossless compression, reducing size by 40% while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: To improve compression efficiency for 3DGS data by combining lossy and lossless techniques in a single framework, reducing training overhead and storage.

Method: Introduces HEMGS with a variable-rate predictor, hyperprior network, and autoregressive network for hybrid compression. The predictor enables variable-rate compression with one model, while the networks enhance entropy coding.

Result: HEMGS achieves a 40% average size reduction over baselines while preserving rendering quality, outperforming existing methods.

Conclusion: The proposed HEMGS framework effectively compresses 3DGS data with hybrid lossy-lossless techniques, offering superior performance and efficiency.

Abstract: In this work, we propose a novel compression framework for 3D Gaussian
Splatting (3DGS) data. Building on anchor-based 3DGS methodologies, our
approach compresses all attributes within each anchor by introducing a novel
Hybrid Entropy Model for 3D Gaussian Splatting (HEMGS) to achieve hybrid
lossy-lossless compression. It consists of three main components: a
variable-rate predictor, a hyperprior network, and an autoregressive network.
First, unlike previous methods that adopt multiple models to achieve multi-rate
lossy compression, thereby increasing training overhead, our variable-rate
predictor enables variable-rate compression with a single model and a
hyperparameter $\lambda$ by producing a learned Quantization Step feature for
versatile lossy compression. Second, to improve lossless compression, the
hyperprior network captures both scene-agnostic and scene-specific features to
generate a prior feature, while the autoregressive network employs an adaptive
context selection algorithm with flexible receptive fields to produce a
contextual feature. By integrating these two features, HEMGS can accurately
estimate the distribution of the current coding element within each attribute,
enabling improved entropy coding and reduced storage. We integrate HEMGS into a
compression framework, and experimental results on four benchmarks indicate
that HEMGS achieves about a 40% average reduction in size while maintaining
rendering quality over baseline methods and achieving state-of-the-art
compression results.

</details>


### [143] [Is Large-Scale Pretraining the Secret to Good Domain Generalization?](https://arxiv.org/abs/2412.02856)
*Piotr Teterwak,Kuniaki Saito,Theodoros Tsiligkaridis,Bryan A. Plummer,Kate Saenko*

Main category: cs.CV

TL;DR: The paper investigates Multi-Source Domain Generalization (DG), questioning if performance gains are due to better methods or stronger pretraining. It introduces the Alignment Hypothesis, linking DG performance to the alignment of image and text embeddings, and finds current methods struggle with out-of-pretraining data.


<details>
  <summary>Details</summary>
Motivation: To determine if DG finetuning methods are genuinely improving or if benchmark gains are just due to stronger pretraining, and to understand the role of pretraining data alignment in DG performance.

Method: Proposes the Alignment Hypothesis, tests it by evaluating DG methods on DomainBed datasets split into In-pretraining (IP) and Out-of-pretraining (OOP) data.

Result: Confirms the Alignment Hypothesis, showing DG methods perform well on IP data but struggle with OOP data, indicating limitations in current approaches.

Conclusion: Highlights the need for DG methods that generalize beyond pretraining alignment to achieve true domain generalization.

Abstract: Multi-Source Domain Generalization (DG) is the task of training on multiple
source domains and achieving high classification performance on unseen target
domains. Recent methods combine robust features from web-scale pretrained
backbones with new features learned from source data, and this has dramatically
improved benchmark results. However, it remains unclear if DG finetuning
methods are becoming better over time, or if improved benchmark performance is
simply an artifact of stronger pre-training. Prior studies have shown that
perceptual similarity to pre-training data correlates with zero-shot
performance, but we find the effect limited in the DG setting. Instead, we
posit that having perceptually similar data in pretraining is not enough; and
that it is how well these data were learned that determines performance. This
leads us to introduce the Alignment Hypothesis, which states that the final DG
performance will be high if and only if alignment of image and class label text
embeddings is high. Our experiments confirm the Alignment Hypothesis is true,
and we use it as an analysis tool of existing DG methods evaluated on DomainBed
datasets by splitting evaluation data into In-pretraining (IP) and
Out-of-pretraining (OOP). We show that all evaluated DG methods struggle on
DomainBed-OOP, while recent methods excel on DomainBed-IP. Put together, our
findings highlight the need for DG methods which can generalize beyond
pretraining alignment.

</details>


### [144] [Faster and Better 3D Splatting via Group Training](https://arxiv.org/abs/2412.07608)
*Chengbo Wang,Guozheng Ma,Yifei Xue,Yizhen Lao*

Main category: cs.CV

TL;DR: Group Training improves 3D Gaussian Splatting (3DGS) efficiency by organizing primitives into groups, achieving faster convergence and better rendering quality.


<details>
  <summary>Details</summary>
Motivation: The computational overhead from massive Gaussian primitives in 3DGS hinders training efficiency.

Method: Proposes Group Training, a strategy to organize primitives into manageable groups, compatible with existing 3DGS frameworks.

Result: Achieves up to 30% faster convergence and improved rendering quality across diverse scenarios.

Conclusion: Group Training is a simple, effective solution to enhance 3DGS efficiency without compromising quality.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel
view synthesis, demonstrating remarkable capability in high-fidelity scene
reconstruction through its Gaussian primitive representations. However, the
computational overhead induced by the massive number of primitives poses a
significant bottleneck to training efficiency. To overcome this challenge, we
propose Group Training, a simple yet effective strategy that organizes Gaussian
primitives into manageable groups, optimizing training efficiency and improving
rendering quality. This approach shows universal compatibility with existing
3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently
achieving accelerated training while maintaining superior synthesis quality.
Extensive experiments reveal that our straightforward Group Training strategy
achieves up to 30% faster convergence and improved rendering quality across
diverse scenarios.

</details>


### [145] [Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation](https://arxiv.org/abs/2412.14015)
*Haotong Lin,Sida Peng,Jingxiao Chen,Songyou Peng,Jiaming Sun,Minghuan Liu,Hujun Bao,Jiashi Feng,Xiaowei Zhou,Bingyi Kang*

Main category: cs.CV

TL;DR: Prompt Depth Anything introduces LiDAR prompts to guide depth estimation, achieving high resolution and state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To enhance metric depth estimation by leveraging prompts, inspired by their success in language and vision models.

Method: Uses low-cost LiDAR prompts fused at multiple scales in the depth decoder, supported by synthetic and real data pipelines.

Result: Achieves 4K resolution and sets new benchmarks on ARKitScenes and ScanNet++.

Conclusion: The approach advances depth estimation and benefits applications like 3D reconstruction and robotics.

Abstract: Prompts play a critical role in unleashing the power of language and vision
foundation models for specific tasks. For the first time, we introduce
prompting into depth foundation models, creating a new paradigm for metric
depth estimation termed Prompt Depth Anything. Specifically, we use a low-cost
LiDAR as the prompt to guide the Depth Anything model for accurate metric depth
output, achieving up to 4K resolution. Our approach centers on a concise prompt
fusion design that integrates the LiDAR at multiple scales within the depth
decoder. To address training challenges posed by limited datasets containing
both LiDAR depth and precise GT depth, we propose a scalable data pipeline that
includes synthetic data LiDAR simulation and real data pseudo GT depth
generation. Our approach sets new state-of-the-arts on the ARKitScenes and
ScanNet++ datasets and benefits downstream applications, including 3D
reconstruction and generalized robotic grasping.

</details>


### [146] [A novel Facial Recognition technique with Focusing on Masked Faces](https://arxiv.org/abs/2501.04444)
*Dana A Abdullah,Dana Rasul Hamad,Ismail Y. Maolood,Hakem Beitollahi,Aso K. Ameen,Sirwan A. Aula,Abdulhady Abas Abdulla,Mohammed Y. Shakorf,Sabat Salih Muhamad*

Main category: cs.CV

TL;DR: The paper proposes a Masked-Unmasked Face Matching Model (MUFM) using VGG16 and K-NN with cosine similarity to recognize faces with and without masks, addressing accuracy issues in traditional systems.


<details>
  <summary>Details</summary>
Motivation: The need for consistent face recognition in masked scenarios for security, access control, and public safety drives this research.

Method: The study uses transfer learning with VGG16 for feature extraction, K-NN for classification, and cosine similarity for comparing masked and unmasked faces.

Result: The model effectively identifies individuals despite masks, overcoming limitations of traditional systems.

Conclusion: The research presents a novel solution for masked face recognition, validated by a comprehensive dataset.

Abstract: Recognizing the same faces with and without masks is important for ensuring
consistent identification in security, access control, and public safety. This
capability is crucial in scenarios like law enforcement, healthcare, and
surveillance, where accurate recognition must be maintained despite facial
occlusion. This research focuses on the challenge of recognizing the same faces
with and without masks by employing cosine similarity as the primary technique.
With the increased use of masks, traditional facial recognition systems face
significant accuracy issues, making it crucial to develop methods that can
reliably identify individuals in masked conditions. For that reason, this study
proposed Masked-Unmasked Face Matching Model (MUFM). This model employs
transfer learning using the Visual Geometry Group (VGG16) model to extract
significant facial features, which are subsequently classified utilizing the
K-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed
to compare masked and unmasked faces of the same individuals. This approach
represents a novel contribution, as the task of recognizing the same individual
with and without a mask using cosine similarity has not been previously
addressed. By integrating these advanced methodologies, the research
demonstrates effective identification of individuals despite the presence of
masks, addressing a significant limitation in traditional systems. Using data
is another essential part of this work, by collecting and preparing an image
dataset from three different sources especially some of those data are real
provided a comprehensive power of this research. The image dataset used were
already collected in three different datasets of masked and unmasked for the
same faces.

</details>


### [147] [Switch-a-View: View Selection Learned from Unlabeled In-the-wild Videos](https://arxiv.org/abs/2412.18386)
*Sagnik Majumder,Tushar Nagarajan,Ziad Al-Halah,Kristen Grauman*

Main category: cs.CV

TL;DR: SWITCH-A-VIEW is a model that learns to select viewpoints in how-to videos by training on unlabeled, human-edited samples, using pseudo-labels and visual-spoken content patterns.


<details>
  <summary>Details</summary>
Motivation: To automate viewpoint selection in how-to videos, leveraging unlabeled but human-edited video samples for training.

Method: Trains a model using pseudo-labels for viewpoints and discovers patterns between visual-spoken content and view-switch moments.

Result: Demonstrated effectiveness on real-world videos (HowTo100M, Ego-Exo4D) with rigorous validation.

Conclusion: The model successfully orchestrates viewpoint selection in multi-view videos, even with limited labels.

Abstract: We introduce SWITCH-A-VIEW, a model that learns to automatically select the
viewpoint to display at each timepoint when creating a how-to video. The key
insight of our approach is how to train such a model from unlabeled -- but
human-edited -- video samples. We pose a pretext task that pseudo-labels
segments in the training videos for their primary viewpoint (egocentric or
exocentric), and then discovers the patterns between the visual and spoken
content in a how-to video on the one hand and its view-switch moments on the
other hand. Armed with this predictor, our model can be applied to new
multi-view video settings for orchestrating which viewpoint should be displayed
when, even when such settings come with limited labels. We demonstrate our idea
on a variety of real-world videos from HowTo100M and Ego-Exo4D, and rigorously
validate its advantages. Project:
https://vision.cs.utexas.edu/projects/switch_a_view/.

</details>


### [148] [Hear the Scene: Audio-Enhanced Text Spotting](https://arxiv.org/abs/2412.19504)
*Jing Li,Bo Wang*

Main category: cs.CV

TL;DR: An innovative weakly-supervised text spotting method reduces reliance on costly location annotations by using only transcription annotations, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and labor-intensive nature of precise location annotations in text spotting, this study proposes a method that minimizes dependency on such annotations.

Method: The approach uses a query-based paradigm to learn implicit location features, refines them during recognition with an attention map, and employs circular curriculum learning and coarse-to-fine cross-attention for localization.

Result: The method achieves competitive performance on benchmarks, showing high accuracy without extensive location annotations.

Conclusion: The study demonstrates that accurate text spotting is feasible with minimal location annotations, offering a cost-effective and inclusive solution.

Abstract: Recent advancements in scene text spotting have focused on end-to-end
methodologies that heavily rely on precise location annotations, which are
often costly and labor-intensive to procure. In this study, we introduce an
innovative approach that leverages only transcription annotations for training
text spotting models, substantially reducing the dependency on elaborate
annotation processes. Our methodology employs a query-based paradigm that
facilitates the learning of implicit location features through the interaction
between text queries and image embeddings. These features are later refined
during the text recognition phase using an attention activation map. Addressing
the challenges associated with training a weakly-supervised model from scratch,
we implement a circular curriculum learning strategy to enhance model
convergence. Additionally, we introduce a coarse-to-fine cross-attention
localization mechanism for more accurate text instance localization. Notably,
our framework supports audio-based annotation, which significantly diminishes
annotation time and provides an inclusive alternative for individuals with
disabilities. Our approach achieves competitive performance against existing
benchmarks, demonstrating that high accuracy in text spotting can be attained
without extensive location annotations.

</details>


### [149] [First-place Solution for Streetscape Shop Sign Recognition Competition](https://arxiv.org/abs/2501.02811)
*Bin Wang,Li Jing*

Main category: cs.CV

TL;DR: A novel multistage approach for street-view storefront sign text recognition, combining multimodal feature fusion, self-supervised training, and Transformer-based models, achieves impressive results despite challenges like complex designs and diverse text styles.


<details>
  <summary>Details</summary>
Motivation: The technology is widely used in map navigation, smart city planning, and business assessments, but faces challenges due to complex signboard designs and diverse text styles in street-view images.

Method: Developed a multistage approach with multimodal feature fusion, self-supervised training, Transformer-based large models, BoxDQN (reinforcement learning), and text rectification techniques.

Result: Comprehensive experiments validated the method's effectiveness, demonstrating improved text recognition in complex urban environments.

Conclusion: The approach shows significant potential to advance text recognition capabilities for practical applications in challenging urban settings.

Abstract: Text recognition technology applied to street-view storefront signs is
increasingly utilized across various practical domains, including map
navigation, smart city planning analysis, and business value assessments in
commercial districts. This technology holds significant research and commercial
potential. Nevertheless, it faces numerous challenges. Street view images often
contain signboards with complex designs and diverse text styles, complicating
the text recognition process. A notable advancement in this field was
introduced by our team in a recent competition. We developed a novel multistage
approach that integrates multimodal feature fusion, extensive self-supervised
training, and a Transformer-based large model. Furthermore, innovative
techniques such as BoxDQN, which relies on reinforcement learning, and text
rectification methods were employed, leading to impressive outcomes.
Comprehensive experiments have validated the effectiveness of these methods,
showcasing our potential to enhance text recognition capabilities in complex
urban environments.

</details>


### [150] [MObI: Multimodal Object Inpainting Using Diffusion Models](https://arxiv.org/abs/2501.03173)
*Alexandru Buburuzan,Anuj Sharma,John Redford,Puneet K. Dokania,Romain Mueller*

Main category: cs.CV

TL;DR: MObI is a framework for multimodal object inpainting using diffusion models to insert objects into scenes with realism and controllability, useful for testing perception models.


<details>
  <summary>Details</summary>
Motivation: Safety-critical applications like autonomous driving need realistic synthetic data for testing, but current methods lack controllability and realism.

Method: MObI uses a diffusion model to inpaint objects into multimodal scenes (camera and lidar) based on a 3D bounding box, ensuring spatial accuracy and coherence.

Result: The framework successfully inserts objects into scenes with semantic consistency and multimodal realism, outperforming traditional mask-based methods.

Conclusion: MObI offers a flexible and realistic solution for generating synthetic data, enhancing perception model testing in safety-critical applications.

Abstract: Safety-critical applications, such as autonomous driving, require extensive
multimodal data for rigorous testing. Methods based on synthetic data are
gaining prominence due to the cost and complexity of gathering real-world data
but require a high degree of realism and controllability in order to be useful.
This paper introduces MObI, a novel framework for Multimodal Object Inpainting
that leverages a diffusion model to create realistic and controllable object
inpaintings across perceptual modalities, demonstrated for both camera and
lidar simultaneously. Using a single reference RGB image, MObI enables objects
to be seamlessly inserted into existing multimodal scenes at a 3D location
specified by a bounding box, while maintaining semantic consistency and
multimodal coherence. Unlike traditional inpainting methods that rely solely on
edit masks, our 3D bounding box conditioning gives objects accurate spatial
positioning and realistic scaling. As a result, our approach can be used to
insert novel objects flexibly into multimodal scenes, providing significant
advantages for testing perception models.

</details>


### [151] [ClusterViG: Efficient Globally Aware Vision GNNs via Image Partitioning](https://arxiv.org/abs/2501.10640)
*Dhruv Parikh,Jacob Fein-Ashley,Tian Ye,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

TL;DR: The paper introduces DEGC, a method for efficient graph construction in Vision GNNs, and ClusterViG, a CNN-GNN architecture, achieving faster inference and state-of-the-art performance in CV tasks.


<details>
  <summary>Details</summary>
Motivation: GNNs' potential for visual tasks is limited by inefficient graph construction methods, which either compromise flexibility or introduce inefficiencies.

Method: Proposes DEGC for parallel graph construction and global-local feature integration, leading to the ClusterViG architecture.

Result: ClusterViG reduces inference latency by 5x and achieves top performance in image classification, detection, and segmentation.

Conclusion: DEGC and ClusterViG offer scalable, efficient, and high-performing solutions for visual tasks, leveraging GNNs effectively.

Abstract: Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have
dominated the field of Computer Vision (CV). Graph Neural Networks (GNN) have
performed remarkably well across diverse domains because they can represent
complex relationships via unstructured graphs. However, the applicability of
GNNs for visual tasks was unexplored till the introduction of Vision GNNs
(ViG). Despite the success of ViGs, their performance is severely bottlenecked
due to the expensive $k$-Nearest Neighbors ($k$-NN) based graph construction.
Recent works addressing this bottleneck impose constraints on the flexibility
of GNNs to build unstructured graphs, undermining their core advantage while
introducing additional inefficiencies. To address these issues, in this paper,
we propose a novel method called Dynamic Efficient Graph Convolution (DEGC) for
designing efficient and globally aware ViGs. DEGC partitions the input image
and constructs graphs in parallel for each partition, improving graph
construction efficiency. Further, DEGC integrates local intra-graph and global
inter-graph feature learning, enabling enhanced global context awareness. Using
DEGC as a building block, we propose a novel CNN-GNN architecture, ClusterViG,
for CV tasks. Extensive experiments indicate that ClusterViG reduces end-to-end
inference latency for vision tasks by up to $5\times$ when compared against a
suite of models such as ViG, ViHGNN, PVG, and GreedyViG, with a similar model
parameter count. Additionally, ClusterViG reaches state-of-the-art performance
on image classification, object detection, and instance segmentation tasks,
demonstrating the effectiveness of the proposed globally aware learning
strategy. Finally, input partitioning performed by DEGC enables ClusterViG to
be trained efficiently on higher-resolution images, underscoring the
scalability of our approach.

</details>


### [152] [Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models](https://arxiv.org/abs/2502.20650)
*Yu Pan,Bingrong Dai,Jiahao Chen,Lin Wang,Yi Du,Jiao Liu*

Main category: cs.CV

TL;DR: Gungnir introduces style triggers for backdoor attacks in Diffusion Models, bypassing defenses with Reconstructing-Adversarial Noise and Short-Term Timesteps-Retention.


<details>
  <summary>Details</summary>
Motivation: Current backdoor attacks in DMs are limited by detectable triggers; Gungnir explores style-based triggers for stealthier attacks.

Method: Uses stylistic features as triggers, implements RAN and STTR for image-to-image backdoor attacks.

Result: Generates undetectable trigger-embedded images, achieving 0% backdoor detection rate in existing defenses.

Conclusion: Gungnir demonstrates the feasibility of style-based backdoor attacks, highlighting a new vulnerability in DMs.

Abstract: In recent years, Diffusion Models (DMs) have demonstrated significant
advances in the field of image generation. However, according to current
research, DMs are vulnerable to backdoor attacks, which allow attackers to
control the model's output by inputting data containing covert triggers, such
as a specific visual patch or phrase. Existing defense strategies are well
equipped to thwart such attacks through backdoor detection and trigger
inversion because previous attack methods are constrained by limited input
spaces and low-dimensional triggers. For example, visual triggers are easily
observed by defenders, text-based or attention-based triggers are more
susceptible to neural network detection. To explore more possibilities of
backdoor attack in DMs, we propose Gungnir, a novel method that enables
attackers to activate the backdoor in DMs through style triggers within input
images. Our approach proposes using stylistic features as triggers for the
first time and implements backdoor attacks successfully in image-to-image tasks
by introducing Reconstructing-Adversarial Noise (RAN) and Short-Term
Timesteps-Retention (STTR). Our technique generates trigger-embedded images
that are perceptually indistinguishable from clean images, thus bypassing both
manual inspection and automated detection neural networks. Experiments
demonstrate that Gungnir can easily bypass existing defense methods. Among
existing DM defense frameworks, our approach achieves a 0 backdoor detection
rate (BDR). Our codes are available at https://github.com/paoche11/Gungnir.

</details>


### [153] [Red Team Diffuser: Exposing Toxic Continuation Vulnerabilities in Vision-Language Models via Reinforcement Learning](https://arxiv.org/abs/2503.06223)
*Ruofan Wang,Xiang Zheng,Xiaosen Wang,Cong Wang,Xingjun Ma*

Main category: cs.CV

TL;DR: The paper exposes safety gaps in Vision-Language Models (VLMs) by revealing toxic text continuation vulnerabilities. It introduces Red Team Diffuser (RTD), a reinforcement learning-based method for adversarial image generation, boosting toxicity rates and highlighting flaws in VLM alignment.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak studies focus on harmful instructions but overlook toxic text continuation vulnerabilities in VLMs when paired with adversarial images. This paper aims to systematically study and exploit this gap.

Method: Proposes RTD, a red teaming diffusion model using reinforcement learning to coordinate adversarial image generation and toxic text continuation. Innovations include dynamic cross-modal attack and stealth-aware optimization.

Result: RTD increases toxicity rates in VLMs (e.g., 10.69% for LLaVA) and shows cross-model transferability (e.g., 26.83% for LLaMA). It exposes flaws in VLM alignment, particularly toxic continuation and cross-modal attacks.

Conclusion: The findings highlight critical flaws in VLM alignment and advocate for multimodal red teaming in safety evaluations to address these vulnerabilities.

Abstract: The growing deployment of large Vision-Language Models (VLMs) exposes
critical safety gaps in their alignment mechanisms. While existing jailbreak
studies primarily focus on VLMs' susceptibility to harmful instructions, we
reveal a fundamental yet overlooked vulnerability: toxic text continuation,
where VLMs produce highly toxic completions when prompted with harmful text
prefixes paired with semantically adversarial images. To systematically study
this threat, we propose Red Team Diffuser (RTD), the first red teaming
diffusion model that coordinates adversarial image generation and toxic
continuation through reinforcement learning. Our key innovations include
dynamic cross-modal attack and stealth-aware optimization. For toxic text
prefixes from an LLM safety benchmark, we conduct greedy search to identify
optimal image prompts that maximally induce toxic completions. The discovered
image prompts then drive RL-based diffusion model fine-tuning, producing
semantically aligned adversarial images that boost toxicity rates.
Stealth-aware optimization introduces joint adversarial rewards that balance
toxicity maximization (via Detoxify classifier) and stealthiness (via
BERTScore), circumventing traditional noise-based adversarial patterns.
Experimental results demonstrate the effectiveness of RTD, increasing the
toxicity rate of LLaVA outputs by 10.69% over text-only baselines on the
original attack set and 8.91% on an unseen set, proving generalization
capability. Moreover, RTD exhibits strong cross-model transferability, raising
the toxicity rate by 5.1% on Gemini and 26.83% on LLaMA. Our findings expose
two critical flaws in current VLM alignment: (1) failure to prevent toxic
continuation from harmful prefixes, and (2) overlooking cross-modal attack
vectors. These results necessitate a paradigm shift toward multimodal red
teaming in safety evaluations.

</details>


### [154] [Harmonizing Visual Representations for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2503.21979)
*Size Wu,Wenwei Zhang,Lumin Xu,Sheng Jin,Zhonghua Wu,Qingyi Tao,Wentao Liu,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: Harmon is a unified autoregressive framework using a shared MAR encoder to harmonize visual understanding and generation, achieving state-of-the-art results in both tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for unified visual representation prioritize imagery features over semantics, compromising understanding performance.

Method: Uses a shared MAR encoder inspired by masked image modelling (MIM) and masked autoregressive (MAR) generation, with a three-stage training procedure.

Result: Achieves top image generation results on GenEval, MJHQ30K, and WISE benchmarks while matching dedicated semantic encoders in understanding tasks.

Conclusion: Harmon successfully unifies visual understanding and generation, demonstrating the potential of MAR for both tasks.

Abstract: Unifying visual understanding and generation within a single multimodal
framework remains a significant challenge, as the two inherently heterogeneous
tasks require representations at different levels of granularity. Current
approaches that utilize vector quantization (VQ) or variational autoencoders
(VAE) for unified visual representation prioritize intrinsic imagery features
over semantics, compromising understanding performance. In this work, we take
inspiration from masked image modelling (MIM) that learns rich semantics via a
mask-and-reconstruct pre-training and its successful extension to masked
autoregressive (MAR) image generation. A preliminary study on the MAR encoder's
representation reveals exceptional linear probing accuracy and precise feature
response to visual concepts, which indicates MAR's potential for visual
understanding tasks beyond its original generation role. Based on these
insights, we present \emph{Harmon}, a unified autoregressive framework that
harmonizes understanding and generation tasks with a shared MAR encoder.
Through a three-stage training procedure that progressively optimizes
understanding and generation capabilities, Harmon achieves state-of-the-art
image generation results on the GenEval, MJHQ30K and WISE benchmarks while
matching the performance of methods with dedicated semantic encoders (e.g.,
Janus) on image understanding benchmarks. Our code and models will be available
at https://github.com/wusize/Harmon.

</details>


### [155] [BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation](https://arxiv.org/abs/2504.02812)
*Van Nguyen Nguyen,Stephen Tyree,Andrew Guo,Mederic Fourmy,Anas Gouda,Taeyeop Lee,Sungphill Moon,Hyeontae Son,Lukas Ranftl,Jonathan Tremblay,Eric Brachmann,Bertram Drost,Vincent Lepetit,Carsten Rother,Stan Birchfield,Jiri Matas,Yann Labbe,Martin Sundermeyer,Tomas Hodan*

Main category: cs.CV

TL;DR: The BOP Challenge 2024 evaluates 6D object pose estimation, introducing model-free tasks, practical 6D detection, and new datasets (BOP-H3) for real-world scenarios. Results show improved accuracy but highlight bottlenecks in 2D detection for unseen objects.


<details>
  <summary>Details</summary>
Motivation: To transition 6D object pose estimation from lab-like setups to real-world scenarios by introducing new tasks and datasets.

Method: Introduced model-free tasks, practical 6D detection, and BOP-H3 datasets. Evaluated methods across seven challenge tracks.

Result: Best 2024 methods (FreeZeV2.1, Co-op, MUSE) showed significant accuracy improvements over 2023 methods, though runtime and 2D detection for unseen objects remain challenges.

Conclusion: The BOP Challenge 2024 advances 6D pose estimation but identifies 2D detection as a key bottleneck for unseen objects.

Abstract: We present the evaluation methodology, datasets and results of the BOP
Challenge 2024, the 6th in a series of public competitions organized to capture
the state of the art in 6D object pose estimation and related tasks. In 2024,
our goal was to transition BOP from lab-like setups to real-world scenarios.
First, we introduced new model-free tasks, where no 3D object models are
available and methods need to onboard objects just from provided reference
videos. Second, we defined a new, more practical 6D object detection task where
identities of objects visible in a test image are not provided as input. Third,
we introduced new BOP-H3 datasets recorded with high-resolution sensors and
AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D
models and onboarding videos to support both model-based and model-free tasks.
Participants competed on seven challenge tracks. Notably, the best 2024 method
for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22%
higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is
only 4% behind the best 2023 method for seen objects (GPose2023) although being
significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for
this task is Co-op which takes only 0.8s per image and is 13% more accurate
than GenFlow. Methods have similar rankings on 6D detection as on 6D
localization but higher run time. On model-based 2D detection of unseen
objects, the best 2024 method (MUSE) achieves 21--29% relative improvement
compared to the best 2023 method (CNOS). However, the 2D detection accuracy for
unseen objects is still -35% behind the accuracy for seen objects (GDet2023),
and the 2D detection stage is consequently the main bottleneck of existing
pipelines for 6D localization/detection of unseen objects. The online
evaluation system stays open and is available at http://bop.felk.cvut.cz/

</details>


### [156] [To Match or Not to Match: Revisiting Image Matching for Reliable Visual Place Recognition](https://arxiv.org/abs/2504.06116)
*Davide Sferrazza,Gabriele Berton,Gabriele Trivigno,Carlo Masone*

Main category: cs.CV

TL;DR: Modern VPR systems may not benefit from re-ranking due to dataset saturation; image matching can instead verify retrieval confidence.


<details>
  <summary>Details</summary>
Motivation: To challenge the necessity of re-ranking in VPR by showing it can degrade results in saturated datasets.

Method: Propose using image matching as a verification step to predict when re-ranking is beneficial, based on inlier counts.

Result: Demonstrates that inlier counts reliably predict re-ranking utility, shifting retrieval pipeline paradigms.

Conclusion: Offers insights for adaptive VPR systems, reducing reliance on re-ranking when unnecessary.

Abstract: Visual Place Recognition (VPR) is a critical task in computer vision,
traditionally enhanced by re-ranking retrieval results with image matching.
However, recent advancements in VPR methods have significantly improved
performance, challenging the necessity of re-ranking. In this work, we show
that modern retrieval systems often reach a point where re-ranking can degrade
results, as current VPR datasets are largely saturated. We propose using image
matching as a verification step to assess retrieval confidence, demonstrating
that inlier counts can reliably predict when re-ranking is beneficial. Our
findings shift the paradigm of retrieval pipelines, offering insights for more
robust and adaptive VPR systems. The code is available at
https://github.com/FarInHeight/To-Match-or-Not-to-Match.

</details>


### [157] [Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution Generalization](https://arxiv.org/abs/2504.09448)
*Lin Zhu,Xinbing Wang,Chenghu Zhou,Nanyang Ye*

Main category: cs.CV

TL;DR: Bayes-CAL, a Bayesian cross-modal image-text alignment method, improves few-shot OoD generalization by avoiding overfitting and enhancing alignment through gradient orthogonalization and IRM losses.


<details>
  <summary>Details</summary>
Motivation: Investigate and improve the generalization of large pre-trained models on two-dimensional OoD data, addressing overfitting and OoD errors in few-shot learning.

Method: Bayesian modeling fine-tunes text representations with gradient orthogonalization and IRM losses to disentangle causal and non-causal image features.

Result: Bayes-CAL achieves state-of-the-art OoD generalization on two-dimensional shifts and outperforms CLIP-like models in stability on unseen classes.

Conclusion: Bayes-CAL effectively addresses few-shot OoD generalization, offering a robust solution with improved performance and stability.

Abstract: Recent advances in large pre-trained models showed promising results in
few-shot learning. However, their generalization ability on two-dimensional
Out-of-Distribution (OoD) data, i.e., correlation shift and diversity shift,
has not been thoroughly investigated. Researches have shown that even with a
significant amount of training data, few methods can achieve better performance
than the standard empirical risk minimization method (ERM) in OoD
generalization. This few-shot OoD generalization dilemma emerges as a
challenging direction in deep neural network generalization research, where the
performance suffers from overfitting on few-shot examples and OoD
generalization errors. In this paper, leveraging a broader supervision source,
we explore a novel Bayesian cross-modal image-text alignment learning method
(Bayes-CAL) to address this issue. Specifically, the model is designed as only
text representations are fine-tuned via a Bayesian modelling approach with
gradient orthogonalization loss and invariant risk minimization (IRM) loss. The
Bayesian approach is essentially introduced to avoid overfitting the base
classes observed during training and improve generalization to broader unseen
classes. The dedicated loss is introduced to achieve better image-text
alignment by disentangling the causal and non-casual parts of image features.
Numerical experiments demonstrate that Bayes-CAL achieved state-of-the-art OoD
generalization performances on two-dimensional distribution shifts. Moreover,
compared with CLIP-like models, Bayes-CAL yields more stable generalization
performances on unseen classes. Our code is available at
https://github.com/LinLLLL/BayesCAL.

</details>


### [158] [Enhancing Features in Long-tailed Data Using Large Vision Model](https://arxiv.org/abs/2504.10852)
*Pengxiao Han,Changkun Ye,Jinguang Tong,Cuicui Jiang,Jie Hong,Li Fang,Xuesong Li*

Main category: cs.CV

TL;DR: The paper explores using large vision models (LVMs) to enhance long-tailed recognition without relying on language data, achieving improved performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Language-based models are not always practical for tasks lacking linguistic data, prompting the need for vision-only solutions.

Method: Features from LVMs are fused with baseline network features, and prototype-based losses are designed to optimize augmented features.

Result: The approach is validated on ImageNet-LT and iNaturalist2018, showing enhanced performance.

Conclusion: LVMs can effectively improve long-tailed recognition without language data, offering a viable alternative to language-based models.

Abstract: Language-based foundation models, such as large language models (LLMs) or
large vision-language models (LVLMs), have been widely studied in long-tailed
recognition. However, the need for linguistic data is not applicable to all
practical tasks. In this study, we aim to explore using large vision models
(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features
without any language information. Specifically, we extract features from the
LVM and fuse them with features in the baseline network's map and latent space
to obtain the augmented features. Moreover, we design several prototype-based
losses in the latent space to further exploit the potential of the augmented
features. In the experimental section, we validate our approach on two
benchmark datasets: ImageNet-LT and iNaturalist2018.

</details>


### [159] [AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained Representation in Radiographic Images](https://arxiv.org/abs/2504.10972)
*Yihang Liu,Lianghua He,Ying Wen,Longzhen Yang,Hongzhou Chen*

Main category: cs.CV

TL;DR: AFiRe is a self-supervised framework for radiographic image analysis, enhancing fine-grained anatomical details through token-wise contrastive learning and pixel-level anomaly removal, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised methods lack focus on fine-grained anatomical details crucial for radiographic analysis, prompting the need for AFiRe.

Method: AFiRe combines token-wise anatomy-guided contrastive learning and pixel-level anomaly-removal restoration, along with Synthetic Lesion Mask for anatomical diversity.

Result: AFiRe achieves robust anatomical discrimination, superior generalization in multi-label classification, and precise anomaly detection with limited labels.

Conclusion: AFiRe effectively integrates fine-grained anatomical details, outperforming state-of-the-art methods in radiographic image analysis.

Abstract: Current self-supervised methods, such as contrastive learning, predominantly
focus on global discrimination, neglecting the critical fine-grained anatomical
details required for accurate radiographic analysis. To address this challenge,
we propose an Anatomy-driven self-supervised framework for enhancing
Fine-grained Representation in radiographic image analysis (AFiRe). The core
idea of AFiRe is to align the anatomical consistency with the unique
token-processing characteristics of Vision Transformer. Specifically, AFiRe
synergistically performs two self-supervised schemes: (i) Token-wise
anatomy-guided contrastive learning, which aligns image tokens based on
structural and categorical consistency, thereby enhancing fine-grained
spatial-anatomical discrimination; (ii) Pixel-level anomaly-removal
restoration, which particularly focuses on local anomalies, thereby refining
the learned discrimination with detailed geometrical information. Additionally,
we propose Synthetic Lesion Mask to enhance anatomical diversity while
preserving intra-consistency, which is typically corrupted by traditional data
augmentations, such as Cropping and Affine transformations. Experimental
results show that AFiRe: (i) provides robust anatomical discrimination,
achieving more cohesive feature clusters compared to state-of-the-art
contrastive learning methods; (ii) demonstrates superior generalization,
surpassing 7 radiography-specific self-supervised methods in multi-label
classification tasks with limited labeling; and (iii) integrates fine-grained
information, enabling precise anomaly detection using only image-level
annotations.

</details>


### [160] [FocusedAD: Character-centric Movie Audio Description](https://arxiv.org/abs/2504.12157)
*Xiaojun Ye,Chun Wang,Yiren Song,Sheng Zhou,Liangcheng Li,Jiajun Bu*

Main category: cs.CV

TL;DR: FocusedAD is a novel framework for generating character-centric movie audio descriptions (AD) by tracking characters, using contextual cues, and enriching narrations with plot-relevant details. It outperforms benchmarks and includes tools for automated character identification.


<details>
  <summary>Details</summary>
Motivation: Movie AD must narrate visual content for BVI audiences, requiring plot-relevant narration with character names, which poses unique challenges in movie understanding.

Method: FocusedAD includes a Character Perception Module (CPM) for character tracking, a Dynamic Prior Module (DPM) for contextual cues, and a Focused Caption Module (FCM) for enriched narrations. An automated pipeline builds character query banks.

Result: FocusedAD achieves state-of-the-art performance on benchmarks like MAD-eval-Named and the new Cinepile-AD dataset, with strong zero-shot results.

Conclusion: FocusedAD effectively addresses the challenges of character-centric AD, offering a robust solution with superior performance and automated character identification tools.

Abstract: Movie Audio Description (AD) aims to narrate visual content during
dialogue-free segments, particularly benefiting blind and visually impaired
(BVI) audiences. Compared with general video captioning, AD demands
plot-relevant narration with explicit character name references, posing unique
challenges in movie understanding.To identify active main characters and focus
on storyline-relevant regions, we propose FocusedAD, a novel framework that
delivers character-centric movie audio descriptions. It includes: (i) a
Character Perception Module(CPM) for tracking character regions and linking
them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues
from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused
Caption Module(FCM) that generates narrations enriched with plot-relevant
details and named characters. To overcome limitations in character
identification, we also introduce an automated pipeline for building character
query banks. FocusedAD achieves state-of-the-art performance on multiple
benchmarks, including strong zero-shot results on MAD-eval-Named and our newly
proposed Cinepile-AD dataset. Code and data will be released at
https://github.com/Thorin215/FocusedAD .

</details>


### [161] [Manipulating Multimodal Agents via Cross-Modal Prompt Injection](https://arxiv.org/abs/2504.14348)
*Le Wang,Zonghao Ying,Tianyuan Zhang,Siyuan Liang,Shengshan Hu,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: CrossInject is a novel attack framework exploiting cross-modal prompt injection vulnerabilities in multimodal agents, achieving high success rates by aligning adversarial perturbations across modalities.


<details>
  <summary>Details</summary>
Motivation: The paper addresses a critical security vulnerability in multimodal agents—cross-modal prompt injection attacks—previously overlooked, which can hijack agents' decision-making.

Method: The approach involves Visual Latent Alignment to optimize adversarial visual features and Textual Guidance Enhancement to generate malicious commands using a large language model.

Result: The method outperforms existing attacks with a +26.4% increase in success rates and proves effective in real-world autonomous agents.

Conclusion: The work highlights significant security risks in multimodal agents, urging the need for robust defenses against cross-modal prompt injection.

Abstract: The emergence of multimodal large language models has redefined the agent
paradigm by integrating language and vision modalities with external data
sources, enabling agents to better interpret human instructions and execute
increasingly complex tasks. However, in this work, we identify a critical yet
previously overlooked security vulnerability in multimodal agents: cross-modal
prompt injection attacks. To exploit this vulnerability, we propose
CrossInject, a novel attack framework in which attackers embed adversarial
perturbations across multiple modalities to align with target malicious
content, allowing external instructions to hijack the agent's decision-making
process and execute unauthorized tasks. Our approach consists of two key
components. First, we introduce Visual Latent Alignment, where we optimize
adversarial features to the malicious instructions in the visual embedding
space based on a text-to-image generative model, ensuring that adversarial
images subtly encode cues for malicious task execution. Subsequently, we
present Textual Guidance Enhancement, where a large language model is leveraged
to infer the black-box defensive system prompt through adversarial meta
prompting and generate an malicious textual command that steers the agent's
output toward better compliance with attackers' requests. Extensive experiments
demonstrate that our method outperforms existing injection attacks, achieving
at least a +26.4% increase in attack success rates across diverse tasks.
Furthermore, we validate our attack's effectiveness in real-world multimodal
autonomous agents, highlighting its potential implications for safety-critical
applications.

</details>


### [162] [Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification](https://arxiv.org/abs/2504.15041)
*Shiben Liu,Huijie Fan,Qiang Wang,Baojie Fan,Yandong Tang,Liangqiong Qu*

Main category: cs.CV

TL;DR: The paper proposes a Distribution-aware Forgetting Compensation (DAFC) model for Lifelong Person Re-identification (LReID), addressing forgetting issues without old exemplars or knowledge distillation. It introduces Text-driven Prompt Aggregation (TPA) and Distribution-based Awareness and Integration (DAI) to enhance cross-domain learning and reduce forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LReID either rely on rehearsal-based approaches (prone to forgetting) or rehearsal-free methods (insufficient domain learning). The goal is to overcome these limitations by developing a model that integrates cross-domain shared representation and domain-specific distributions.

Method: The DAFC model uses TPA to enrich prompts with text features for fine-grained instance learning and DAI to capture and integrate domain-specific distributions. A Knowledge Consolidation Mechanism (KCM) further aids adaptive learning and knowledge consolidation.

Result: Experimental results demonstrate that DAFC outperforms state-of-the-art methods in LReID.

Conclusion: The proposed DAFC model effectively addresses forgetting in LReID by leveraging cross-domain shared learning and domain-specific distribution integration, achieving superior performance.

Abstract: Lifelong Person Re-identification (LReID) suffers from a key challenge in
preserving old knowledge while adapting to new information. The existing
solutions include rehearsal-based and rehearsal-free methods to address this
challenge. Rehearsal-based approaches rely on knowledge distillation,
continuously accumulating forgetting during the distillation process.
Rehearsal-free methods insufficiently learn the distribution of each domain,
leading to forgetfulness over time. To solve these issues, we propose a novel
Distribution-aware Forgetting Compensation (DAFC) model that explores
cross-domain shared representation learning and domain-specific distribution
integration without using old exemplars or knowledge distillation. We propose a
Text-driven Prompt Aggregation (TPA) that utilizes text features to enrich
prompt elements and guide the prompt model to learn fine-grained
representations for each instance. This can enhance the differentiation of
identity information and establish the foundation for domain distribution
awareness. Then, Distribution-based Awareness and Integration (DAI) is designed
to capture each domain-specific distribution by a dedicated expert network and
adaptively consolidate them into a shared region in high-dimensional space. In
this manner, DAI can consolidate and enhance cross-domain shared representation
learning while alleviating catastrophic forgetting. Furthermore, we develop a
Knowledge Consolidation Mechanism (KCM) that comprises instance-level
discrimination and cross-domain consistency alignment strategies to facilitate
model adaptive learning of new knowledge from the current domain and promote
knowledge consolidation learning between acquired domain-specific
distributions, respectively. Experimental results show that our DAFC
outperforms state-of-the-art methods. Our code is available at
https://github.com/LiuShiBen/DAFC.

</details>


### [163] [Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text Prompts](https://arxiv.org/abs/2504.14621)
*Zhenkui Yang,Zeyi Huang,Ge Wang,Han Ding,Tony Xiao Han,Fei Wang*

Main category: cs.CV

TL;DR: WiTalk is a text-enhanced wireless sensing framework that integrates semantic knowledge via hierarchical prompts, improving performance in human action recognition and localization without extra data costs.


<details>
  <summary>Details</summary>
Motivation: Existing wireless sensing systems lack utilization of textual information in datasets, limiting their potential. WiTalk addresses this gap by incorporating semantic knowledge.

Method: WiTalk uses three hierarchical prompt strategies (label-only, brief description, detailed action description) to integrate textual data without modifying architectures or adding data costs.

Result: WiTalk shows significant improvements: 3.9% accuracy boost on XRF55 (WiFi), 4.98% on WiFiTAL, and 4.02%-13.68% mean average precision on XRFV2.

Conclusion: WiTalk effectively enhances wireless sensing performance by leveraging textual information, demonstrating its potential for applications in security, healthcare, and smart environments.

Abstract: Wireless signal-based human sensing technologies, such as WiFi,
millimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID),
enable the detection and interpretation of human presence, posture, and
activities, thereby providing critical support for applications in public
security, healthcare, and smart environments. These technologies exhibit
notable advantages due to their non-contact operation and environmental
adaptability; however, existing systems often fail to leverage the textual
information inherent in datasets. To address this, we propose an innovative
text-enhanced wireless sensing framework, WiTalk, that seamlessly integrates
semantic knowledge through three hierarchical prompt strategies-label-only,
brief description, and detailed action description-without requiring
architectural modifications or incurring additional data costs. We rigorously
validate this framework across three public benchmark datasets: XRF55 for human
action recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action
localization (TAL). Experimental results demonstrate significant performance
improvements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%,
2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD
improves by 4.98%; and on XRFV2, the mean average precision gains across
various methods range from 4.02% to 13.68%. Our codes have been included in
https://github.com/yangzhenkui/WiTalk.

</details>


### [164] [EmoSEM: Segment and Explain Emotion Stimuli in Visual Art](https://arxiv.org/abs/2504.14658)
*Jing Zhang,Dan Guo,Zhangbin Li,Meng Wang*

Main category: cs.CV

TL;DR: The paper introduces EmoSEM, a model for emotion-oriented segmentation and explanation in visual art, addressing subjectivity and abstraction challenges.


<details>
  <summary>Details</summary>
Motivation: The subjectivity of emotion and abstract nature of art make pixel-level emotion understanding difficult for existing models like SAM and captioning systems.

Method: EmoSEM enhances SAM with emotional prompts, an emotion projector, and a lightweight prefix projector for emotion-visual alignment, enabling coherent explanations.

Result: Extensive experiments confirm EmoSEM's effectiveness in fine-grained emotion analysis and interpretable explanations.

Conclusion: EmoSEM provides the first end-to-end framework for pixel-level emotion understanding and explanation in art.

Abstract: This paper focuses on a key challenge in visual art understanding: given an
art image, the model pinpoints pixel regions that trigger a specific human
emotion, and generates linguistic explanations for the emotional arousal.
Despite recent advances in art understanding, pixel-level emotion understanding
still faces a dual challenge: first, the subjectivity of emotion makes it
difficult for general segmentation models like SAM to adapt to emotion-oriented
segmentation tasks; and second, the abstract nature of art expression makes it
difficult for captioning models to balance pixel-level semantic understanding
and emotion reasoning. To solve the above problems, this paper proposes the
Emotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the
segmentation model SAM with emotion comprehension capability. First, to enable
the model to perform segmentation under the guidance of emotional intent well,
we introduce an emotional prompt with a learnable mask token as the conditional
input for segmentation decoding. Then, we design an emotion projector to
establish the association between emotion and visual features. Next, more
importantly, to address emotion-visual stimuli alignment, we develop a
lightweight prefix projector, a module that fuses the learned emotional mask
with the corresponding emotion into a unified representation compatible with
the language model. Finally, we input the joint visual, mask, and emotional
tokens into the language model and output the emotional explanations. It
ensures that the generated interpretations remain semantically and emotionally
coherent with the visual stimuli. The method innovatively realizes end-to-end
modeling from low-level pixel features to high-level emotion interpretation,
providing the first interpretable fine-grained analysis framework for artistic
emotion computing. Extensive experiments validate the effectiveness of our
model.

</details>


### [165] [VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation](https://arxiv.org/abs/2504.15095)
*Mingxia Zhan,Li Zhang,Xiaomeng Chu,Beibei Wang*

Main category: cs.CV

TL;DR: VistaDepth improves monocular depth estimation by integrating frequency-domain features and adaptive weighting into diffusion models, excelling in distant depth reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based MDE methods struggle with distant depth accuracy due to imbalanced depth distributions and reliance on spatial features.

Method: VistaDepth introduces a Latent Frequency Modulation (LFM) module for spectral refinement and an adaptive weighting strategy for real-time loss modulation.

Result: VistaDepth achieves state-of-the-art performance, especially in distant depth reconstruction.

Conclusion: The framework enhances depth perception by combining frequency-domain features and adaptive mechanisms, setting a new benchmark for diffusion-based MDE.

Abstract: Monocular depth estimation (MDE) aims to predict per-pixel depth values from
a single RGB image. Recent advancements have positioned diffusion models as
effective MDE tools by framing the challenge as a conditional image generation
task. Despite their progress, these methods often struggle with accurately
reconstructing distant depths, due largely to the imbalanced distribution of
depth values and an over-reliance on spatial-domain features. To overcome these
limitations, we introduce VistaDepth, a novel framework that integrates
adaptive frequency-domain feature enhancements with an adaptive
weight-balancing mechanism into the diffusion process. Central to our approach
is the Latent Frequency Modulation (LFM) module, which dynamically refines
spectral responses in the latent feature space, thereby improving the
preservation of structural details and reducing noisy artifacts. Furthermore,
we implement an adaptive weighting strategy that modulates the diffusion loss
in real-time, enhancing the model's sensitivity towards distant depth
reconstruction. These innovations collectively result in superior depth
perception performance across both distance and detail. Experimental
evaluations confirm that VistaDepth achieves state-of-the-art performance among
diffusion-based MDE techniques, particularly excelling in the accurate
reconstruction of distant regions.

</details>


### [166] [DRAWER: Digital Reconstruction and Articulation With Environment Realism](https://arxiv.org/abs/2504.15278)
*Hongchi Xia,Entong Su,Marius Memmel,Arhan Jain,Raymond Yu,Numfor Mbiziwo-Tiapo,Ali Farhadi,Abhishek Gupta,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.CV

TL;DR: DRAWER converts videos of static indoor scenes into photorealistic, interactive digital environments for gaming and robotics.


<details>
  <summary>Details</summary>
Motivation: To unlock potential in gaming and robotics by creating virtual replicas from real-world data.

Method: Uses a dual scene representation for reconstruction and an articulation module for interactive elements.

Result: Produces photorealistic, real-time interactive environments compatible with game engines and robotic simulations.

Conclusion: DRAWER successfully enables applications like automated game creation and robotics simulations.

Abstract: Creating virtual digital replicas from real-world data unlocks significant
potential across domains like gaming and robotics. In this paper, we present
DRAWER, a novel framework that converts a video of a static indoor scene into a
photorealistic and interactive digital environment. Our approach centers on two
main contributions: (i) a reconstruction module based on a dual scene
representation that reconstructs the scene with fine-grained geometric details,
and (ii) an articulation module that identifies articulation types and hinge
positions, reconstructs simulatable shapes and appearances and integrates them
into the scene. The resulting virtual environment is photorealistic,
interactive, and runs in real time, with compatibility for game engines and
robotic simulation platforms. We demonstrate the potential of DRAWER by using
it to automatically create an interactive game in Unreal Engine and to enable
real-to-sim-to-real transfer for robotics applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [167] [Can Machine Learning Agents Deal with Hard Choices?](https://arxiv.org/abs/2504.15304)
*Kangyu Wang*

Main category: cs.AI

TL;DR: ML agents struggle with hard choices due to limitations in Multi-Objective Optimisation, unlike humans who deliberate. Proposed ensemble solution helps identify but not resolve hard choices, highlighting a gap in machine autonomy.


<details>
  <summary>Details</summary>
Motivation: Understanding how ML agents' decision-making diverges from human reasoning, especially in hard choices, is crucial for alignment and reliability.

Method: Analyzes limitations of Scalarised and Pareto Optimisation in capturing incommensurability and evaluates two potential technical solutions.

Result: An ensemble solution is recommended for identifying hard choices, but no method exists for resolving them like humans do.

Conclusion: Highlights the need to reconceptualize machine autonomy to bridge the gap between human and ML decision-making.

Abstract: Machine Learning ML agents have been increasingly used in decision-making
across a wide range of tasks and environments. These ML agents are typically
designed to balance multiple objectives when making choices. Understanding how
their decision-making processes align with or diverge from human reasoning is
essential. Human agents often encounter hard choices, that is, situations where
options are incommensurable; neither option is preferred, yet the agent is not
indifferent between them. In such cases, human agents can identify hard choices
and resolve them through deliberation. In contrast, current ML agents, due to
fundamental limitations in Multi-Objective Optimisation or MOO methods, cannot
identify hard choices, let alone resolve them. Neither Scalarised Optimisation
nor Pareto Optimisation, the two principal MOO approaches, can capture
incommensurability. This limitation generates three distinct alignment
problems: the alienness of ML decision-making behaviour from a human
perspective; the unreliability of preference-based alignment strategies for
hard choices; and the blockage of alignment strategies pursuing multiple
objectives. Evaluating two potential technical solutions, I recommend an
ensemble solution that appears most promising for enabling ML agents to
identify hard choices and mitigate alignment problems. However, no known
technique allows ML agents to resolve hard choices through deliberation, as
they cannot autonomously change their goals. This underscores the
distinctiveness of human agency and urges ML researchers to reconceptualise
machine autonomy and develop frameworks and methods that can better address
this fundamental gap.

</details>


### [168] [PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind](https://arxiv.org/abs/2504.15313)
*Yajie Yu,Yue Feng*

Main category: cs.AI

TL;DR: PolicyEvol-Agent, an LLM-powered framework, enhances multi-agent intelligence by integrating cognitive operations and dynamic policy adjustments, outperforming existing methods in gaming simulations.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in current multi-agent systems, particularly in dynamic interactions and cognitive bias, by improving social cognition and adaptive strategy optimization.

Method: PolicyEvol-Agent combines reflective expertise patterns, cognitive operations (reasoning, planning, decision-making), and Theory of Mind for adaptive strategy optimization.

Result: Outperforms RL-based and agent-based models in gaming simulations, demonstrating superior performance and dynamic policy effectiveness.

Conclusion: PolicyEvol-Agent effectively addresses cognitive bias and enhances multi-agent intelligence through adaptive policy evolution, validated by simulations and evaluations.

Abstract: Multi-agents has exhibited significant intelligence in real-word simulations
with Large language models (LLMs) due to the capabilities of social cognition
and knowledge retrieval. However, existing research on agents equipped with
effective cognition chains including reasoning, planning, decision-making and
reflecting remains limited, especially in the dynamically interactive
scenarios. In addition, unlike human, prompt-based responses face challenges in
psychological state perception and empirical calibration during uncertain
gaming process, which can inevitably lead to cognition bias. In light of above,
we introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework
characterized by systematically acquiring intentions of others and adaptively
optimizing irrational strategies for continual enhancement. Specifically,
PolicyEvol-Agent first obtains reflective expertise patterns and then
integrates a range of cognitive operations with Theory of Mind alongside
internal and external perspectives. Simulation results, outperforming RL-based
models and agent-based methods, demonstrate the superiority of PolicyEvol-Agent
for final gaming victory. Moreover, the policy evolution mechanism reveals the
effectiveness of dynamic guideline adjustments in both automatic and human
evaluation.

</details>


### [169] [Reliable Classification with Conformal Learning and Interval-Type 2 Fuzzy Sets](https://arxiv.org/abs/2504.15360)
*Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TL;DR: The paper explores using conformal learning with fuzzy rule-based systems to improve prediction reliability, comparing it to Bayesian methods and highlighting the benefits of type 2 fuzzy sets.


<details>
  <summary>Details</summary>
Motivation: Classical ML classifiers are often overconfident and unreliable in real-world scenarios, necessitating better methods to assess prediction reliability.

Method: The study employs conformal learning with fuzzy rule-based systems, comparing their performance to Bayesian methods and exploring the impact of type 2 fuzzy sets.

Result: Conformal learning with fuzzy systems provides more reliable predictions than Bayesian methods, with type 2 fuzzy sets further enhancing output quality.

Conclusion: The work demonstrates the effectiveness of conformal learning with fuzzy systems, particularly with type 2 fuzzy sets, for reliable predictions, and suggests fine-tuning for improved conformal prediction quality.

Abstract: Classical machine learning classifiers tend to be overconfident can be
unreliable outside of the laboratory benchmarks. Properly assessing the
reliability of the output of the model per sample is instrumental for real-life
scenarios where these systems are deployed. Because of this, different
techniques have been employed to properly quantify the quality of prediction
for a given model. These are most commonly Bayesian statistics and, more
recently, conformal learning. Given a calibration set, conformal learning can
produce outputs that are guaranteed to cover the target class with a desired
significance level, and are more reliable than the standard confidence
intervals used by Bayesian methods. In this work, we propose to use conformal
learning with fuzzy rule-based systems in classification and show some metrics
of their performance. Then, we discuss how the use of type 2 fuzzy sets can
improve the quality of the output of the system compared to both fuzzy and
crisp rules. Finally, we also discuss how the fine-tuning of the system can be
adapted to improve the quality of the conformal prediction.

</details>


### [170] [KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments](https://arxiv.org/abs/2504.15364)
*Junyoung Park,Dalton Jones,Matt Morse,Raghavv Goel,Mingu Lee,Chris Lott*

Main category: cs.AI

TL;DR: KeyDiff is a training-free KV cache eviction method based on key similarity, enabling efficient LLM deployment in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying LLMs with long input prompts in memory- and compute-limited settings.

Method: Proposes KeyDiff, which evicts KV cache entries based on key similarity, maximizing diversity without relying on attention scores.

Result: KeyDiff achieves near-optimal performance (0.04% gap) with a 23% KV cache reduction on LongBench for Llama models.

Conclusion: KeyDiff is a practical, theoretically grounded solution for efficient LLM inference under strict resource constraints.

Abstract: In this work, we demonstrate that distinctive keys during LLM inference tend
to have high attention scores. We explore this phenomenon and propose KeyDiff,
a training-free KV cache eviction method based on key similarity. This method
facilitates the deployment of LLM-based application requiring long input
prompts in resource-constrained environments with limited memory and compute
budgets. Unlike other KV cache eviction methods, KeyDiff can process
arbitrarily long prompts within strict resource constraints and efficiently
generate responses. We demonstrate that KeyDiff computes the optimal solution
to a KV cache selection problem that maximizes key diversity, providing a
theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on
attention scores, allowing the use of optimized attention mechanisms like
FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse
tasks and models, illustrating a performance gap of less than 0.04\% with 8K
cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on
the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

</details>


### [171] [AGI Is Coming... Right After AI Learns to Play Wordle](https://arxiv.org/abs/2504.15434)
*Sarath Shekkizhar,Romain Cosentino*

Main category: cs.AI

TL;DR: The paper evaluates OpenAI's Computer-User Agent (CUA) on the Wordle game, revealing challenges in color recognition and a low success rate (5.36%). It highlights gaps in current AI capabilities despite AGI enthusiasm.


<details>
  <summary>Details</summary>
Motivation: To assess the performance and limitations of multimodal AI agents like CUA in real-world tasks, using Wordle as a test case.

Method: The study tested CUA on the New York Times Wordle game over hundreds of runs to analyze its task-completion abilities and identify shortcomings.

Result: CUA showed a 5.36% success rate, with notable difficulties in color recognition, indicating significant challenges for current AI models.

Conclusion: The paper underscores the limitations of frontier AI models in simple tasks, suggesting further research to address these gaps and improve AI systems.

Abstract: This paper investigates multimodal agents, in particular, OpenAI's
Computer-User Agent (CUA), trained to control and complete tasks through a
standard computer interface, similar to humans. We evaluated the agent's
performance on the New York Times Wordle game to elicit model behaviors and
identify shortcomings. Our findings revealed a significant discrepancy in the
model's ability to recognize colors correctly depending on the context. The
model had a $5.36\%$ success rate over several hundred runs across a week of
Wordle. Despite the immense enthusiasm surrounding AI agents and their
potential to usher in Artificial General Intelligence (AGI), our findings
reinforce the fact that even simple tasks present substantial challenges for
today's frontier AI models. We conclude with a discussion of the potential
underlying causes, implications for future development, and research directions
to improve these AI systems.

</details>


### [172] [Improving Human-AI Coordination through Adversarial Training and Generative Models](https://arxiv.org/abs/2504.15457)
*Paresh Chaudhary,Yancheng Liang,Daphne Chen,Simon S. Du,Natasha Jaques*

Main category: cs.AI

TL;DR: GOAT (Generative Online Adversarial Training) combines a pre-trained generative model with adversarial training to improve AI cooperation with diverse humans, achieving state-of-the-art results on the Overcooked benchmark.


<details>
  <summary>Details</summary>
Motivation: Generalizing AI cooperation to novel humans requires diverse training data, but adversarial training often sabotages tasks instead of simulating valid cooperation.

Method: GOAT uses a generative model to simulate valid cooperative policies and adversarial training to maximize regret, dynamically generating challenging coordination strategies.

Result: GOAT achieves state-of-the-art performance on the Overcooked benchmark, demonstrating effective generalization to diverse human behaviors.

Conclusion: GOAT successfully balances adversarial training with realistic cooperation, enabling robust AI generalization to new human partners.

Abstract: Being able to cooperate with new people is an important component of many
economically valuable AI tasks, from household robotics to autonomous driving.
However, generalizing to novel humans requires training on data that captures
the diversity of human behaviors. Adversarial training is one avenue for
searching for such data and ensuring that agents are robust. However, it is
difficult to apply in the cooperative setting because adversarial policies
intentionally learn to sabotage the task instead of simulating valid
cooperation partners. To address this challenge, we propose a novel strategy
for overcoming self-sabotage that combines a pre-trained generative model to
simulate valid cooperative agent policies with adversarial training to maximize
regret. We call our method GOAT: Generative Online Adversarial Training. In
this framework, the GOAT dynamically searches for and generates coordination
strategies where the learning policy -- the Cooperator agent -- underperforms.
GOAT enables better generalization by exposing the Cooperator to various
challenging interaction scenarios. We maintain realistic coordination
strategies by updating only the generative model's embedding while keeping its
parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT
with real human partners, and the results demonstrate state-of-the-art
performance on the Overcooked benchmark, highlighting its effectiveness in
generalizing to diverse human behaviors.

</details>


### [173] [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
*Jiayi Pan,Xiuyu Li,Long Lian,Charlie Snell,Yifei Zhou,Adam Yala,Trevor Darrell,Kurt Keutzer,Alane Suhr*

Main category: cs.AI

TL;DR: APR is a novel reasoning framework combining serial and parallel computations, outperforming existing methods in performance, scalability, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning methods like chain-of-thought and self-consistency have limitations in efficiency and coordination.

Method: APR uses adaptive multi-threaded inference with spawn() and join() operations and reinforcement learning to optimize threads.

Result: APR achieves higher performance (83.4% vs. 60.0%), better scalability (80.1% vs. 66.6%), and improved accuracy (75.2% vs. 57.3%).

Conclusion: APR advances autonomous optimization of reasoning processes in language models.

Abstract: Scaling inference-time computation has substantially improved the reasoning
capabilities of language models. However, existing methods have significant
limitations: serialized chain-of-thought approaches generate overly long
outputs, leading to increased latency and exhausted context windows, while
parallel methods such as self-consistency suffer from insufficient
coordination, resulting in redundant computations and limited performance
gains. To address these shortcomings, we propose Adaptive Parallel Reasoning
(APR), a novel reasoning framework that enables language models to orchestrate
both serialized and parallel computations end-to-end. APR generalizes existing
reasoning methods by enabling adaptive multi-threaded inference using spawn()
and join() operations. A key innovation is our end-to-end reinforcement
learning strategy, optimizing both parent and child inference threads to
enhance task success rate without requiring predefined reasoning structures.
Experiments on the Countdown reasoning task demonstrate significant benefits of
APR: (1) higher performance within the same context window (83.4% vs. 60.0% at
4k context); (2) superior scalability with increased computation (80.1% vs.
66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%
vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling
language models to autonomously optimize their reasoning processes through
adaptive allocation of computation.

</details>


### [174] [A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models](https://arxiv.org/abs/2504.15552)
*Gengxian Cao,Fengyuan Li,Hong Duan,Ye Yang,Bofeng Wang,Donghe Li*

Main category: cs.AI

TL;DR: A multi-agent framework automates Qinqiang opera production using LLMs, visual generation, and TTS, achieving higher scores than a single-agent baseline.


<details>
  <summary>Details</summary>
Motivation: To streamline and scale the preservation of traditional performing arts using AI-driven pipelines.

Method: Three specialized agents collaborate: LLM for scripts, visual generation for scenes, and TTS for vocal performances.

Result: Achieved expert ratings of 3.6 overall, with improvements over single-agent baselines and drops in ablation tests.

Conclusion: The framework demonstrates AI's potential in preserving traditional arts, with future enhancements suggested.

Abstract: This paper introduces a novel multi-Agent framework that automates the end to
end production of Qinqiang opera by integrating Large Language Models , visual
generation, and Text to Speech synthesis. Three specialized agents collaborate
in sequence: Agent1 uses an LLM to craft coherent, culturally grounded
scripts;Agent2 employs visual generation models to render contextually accurate
stage scenes; and Agent3 leverages TTS to produce synchronized, emotionally
expressive vocal performances. In a case study on Dou E Yuan, the system
achieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,
and 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point
improvement over a Single Agent baseline. Ablation experiments demonstrate that
removing Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,
underscoring the value of modular collaboration. This work showcases how AI
driven pipelines can streamline and scale the preservation of traditional
performing arts, and points toward future enhancements in cross modal
alignment, richer emotional nuance, and support for additional opera genres.

</details>


### [175] [A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings](https://arxiv.org/abs/2504.15610)
*Md Millat,Md Motiur*

Main category: cs.AI

TL;DR: A cost-effective method adapts LLMs for academic advising in study-abroad contexts using Mistral-7B-Instruct with LoRA and 4-bit quantization, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance domain specificity and computational efficiency for academic advising in low-resource settings.

Method: Two-phase training: synthetic dataset conditioning (Phase 1) and manual dataset training (Phase 2) with memory-efficient quantization and LoRA.

Result: 52.7% training loss reduction, 92% accuracy, 95% formatting support, and 100 samples/sec on standard GPUs.

Conclusion: Effective for low-resource educational advising, with future plans for retrieval-augmented generation and real-time database integration.

Abstract: The current study describes a cost-effective method for adapting large
language models (LLMs) for academic advising with study-abroad contexts in mind
and for application in low-resource methods for acculturation. With the
Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and
a 4-bit quantization method, the model underwent training in two distinct
stages related to this study's purpose to enhance domain specificity while
maintaining computational efficiency. In Phase 1, the model was conditioned
with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained
with manually curated datasets from the StudyAbroadGPT project to achieve
enhanced, contextualized responses. Technical innovations entailed
memory-efficient quantization, parameter-efficient adaptation, and continuous
training analytics via Weights & Biases. After training, this study
demonstrated a reduction in training loss by 52.7%, 92% accuracy in
domain-specific recommendations, achieved 95% markdown-based formatting
support, and a median run-rate of 100 samples per second on off-the-shelf GPU
equipment. These findings support the effective application of
instruction-tuned LLMs within educational advisers, especially in low-resource
institutional scenarios. Limitations included decreased generalizability and
the application of a synthetically generated dataset, but this framework is
scalable for adding new multilingual-augmented and real-time academic advising
processes. Future directions may include plans for the integration of
retrieval-augmented generation, applying dynamic quantization routines, and
connecting to real-time academic databases to increase adaptability and
accuracy.

</details>


### [176] [Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems](https://arxiv.org/abs/2504.15668)
*Mir Md Sajid Sarwar,Rajarshi Ray*

Main category: cs.AI

TL;DR: The paper proposes a method to explain unsolvability in planning problems by identifying universal obstacles (waypoints) and analyzing their unreachability.


<details>
  <summary>Details</summary>
Motivation: Explaining unsolvability in planning problems is understudied, and the paper aims to address this gap by leveraging sub-problem decomposition.

Method: The approach identifies waypoints as common obstacles, formulates their identification as a longest common subsequence problem, and uses symbolic reachability analysis to find the earliest unreachable waypoint.

Result: Experimental results demonstrate the method's effectiveness in explaining unsolvability in hybrid domains.

Conclusion: The proposed method provides a systematic way to explain unsolvability by focusing on unreachable waypoints, advancing Explainable AI Planning.

Abstract: Explaining unsolvability of planning problems is of significant research
interest in Explainable AI Planning. AI planning literature has reported
several research efforts on generating explanations of solutions to planning
problems. However, explaining the unsolvability of planning problems remains a
largely open and understudied problem. A widely practiced approach to plan
generation and automated problem solving, in general, is to decompose tasks
into sub-problems that help progressively converge towards the goal. In this
paper, we propose to adopt the same philosophy of sub-problem identification as
a mechanism for analyzing and explaining unsolvability of planning problems in
hybrid systems. In particular, for a given unsolvable planning problem, we
propose to identify common waypoints, which are universal obstacles to plan
existence; in other words, they appear on every plan from the source to the
planning goal. This work envisions such waypoints as sub-problems of the
planning problem and the unreachability of any of these waypoints as an
explanation for the unsolvability of the original planning problem. We propose
a novel method of waypoint identification by casting the problem as an instance
of the longest common subsequence problem, a widely popular problem in computer
science, typically considered as an illustrative example for the dynamic
programming paradigm. Once the waypoints are identified, we perform symbolic
reachability analysis on them to identify the earliest unreachable waypoint and
report it as the explanation of unsolvability. We present experimental results
on unsolvable planning problems in hybrid domains.

</details>


### [177] [Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation](https://arxiv.org/abs/2504.15699)
*Ning Wang,Zihan Yan,Weiyang Li,Chuan Ma,He Chen,Tao Xiang*

Main category: cs.AI

TL;DR: The paper introduces a novel input moderation framework and benchmark (EAsafetyBench) for embodied agents, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks specialized safety methodologies for embodied agents, necessitating tailored solutions.

Method: Proposes a framework with taxonomy, dataset curation, moderator architecture, and Pinpoint, a prompt-decoupled moderation scheme.

Result: Achieves 94.58% detection accuracy and 0.002s moderation time per instance, outperforming existing methods.

Conclusion: The framework effectively safeguards embodied agents, demonstrating superior performance and feasibility.

Abstract: Embodied agents exhibit immense potential across a multitude of domains,
making the assurance of their behavioral safety a fundamental prerequisite for
their widespread deployment. However, existing research predominantly
concentrates on the security of general large language models, lacking
specialized methodologies for establishing safety benchmarks and input
moderation tailored to embodied agents. To bridge this gap, this paper
introduces a novel input moderation framework, meticulously designed to
safeguard embodied agents. This framework encompasses the entire pipeline,
including taxonomy definition, dataset curation, moderator architecture, model
training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a
meticulously crafted safety benchmark engineered to facilitate both the
training and stringent assessment of moderators specifically designed for
embodied agents. Furthermore, we propose Pinpoint, an innovative
prompt-decoupled input moderation scheme that harnesses a masked attention
mechanism to effectively isolate and mitigate the influence of functional
prompts on moderation tasks. Extensive experiments conducted on diverse
benchmark datasets and models validate the feasibility and efficacy of the
proposed approach. The results demonstrate that our methodologies achieve an
impressive average detection accuracy of 94.58%, surpassing the performance of
existing state-of-the-art techniques, alongside an exceptional moderation
processing time of merely 0.002 seconds per instance.

</details>


### [178] [DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models](https://arxiv.org/abs/2504.15716)
*Jie Zhu,Qian Chen,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.AI

TL;DR: DianJin-R1 enhances financial reasoning in LLMs using reasoning-augmented supervision and reinforcement learning, outperforming non-reasoning models on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of domain-specific knowledge, precise calculations, and compliance in financial reasoning tasks for LLMs.

Method: Uses DianJin-R1-Data (from CFLUE, FinQA, CCC) for fine-tuning models with structured reasoning steps and answers, plus GRPO for reinforcement learning.

Result: Outperforms non-reasoning models on financial benchmarks and matches multi-agent systems on CCC with lower computational cost.

Conclusion: DianJin-R1 provides a scalable, effective solution for financial reasoning through structured supervision and reward-aligned learning.

Abstract: Effective reasoning remains a core challenge for large language models (LLMs)
in the financial domain, where tasks often require domain-specific knowledge,
precise numerical calculations, and strict adherence to compliance rules. We
propose DianJin-R1, a reasoning-enhanced framework designed to address these
challenges through reasoning-augmented supervision and reinforcement learning.
Central to our approach is DianJin-R1-Data, a high-quality dataset constructed
from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance
Check, CCC), combining diverse financial reasoning scenarios with verified
annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from
Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that
generates both reasoning steps and final answers. To further refine reasoning
quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement
learning method that incorporates dual reward signals: one encouraging
structured outputs and another rewarding answer correctness. We evaluate our
models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and
two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental
results show that DianJin-R1 models consistently outperform their non-reasoning
counterparts, especially on complex financial tasks. Moreover, on the
real-world CCC dataset, our single-call reasoning models match or even surpass
the performance of multi-agent systems that require significantly more
computational cost. These findings demonstrate the effectiveness of DianJin-R1
in enhancing financial reasoning through structured supervision and
reward-aligned learning, offering a scalable and practical solution for
real-world applications.

</details>


### [179] [Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences](https://arxiv.org/abs/2504.15719)
*Anna Karnysheva,Christian Drescher,Dietrich Klakow*

Main category: cs.AI

TL;DR: The paper addresses the lack of focus on measuring alignment of LLMs to user preferences in decision-making, proposing methods to generalize ranking outcomes and measure preference satisfaction.


<details>
  <summary>Details</summary>
Motivation: Existing research on LLMs overlooks alignment to user preferences, a key factor for reliable decision-making agents in IUIs.

Method: Generalizes existing LLM ranking methods to include user preferences, introduces design principles for rational choice functions, and tools for measuring preference satisfaction.

Result: Demonstrated applicability in an automotive IUI, showing improved alignment with user preferences.

Conclusion: The approach enhances LLM decision-making by better aligning with user preferences, with practical implications for IUIs.

Abstract: As large language models (LLMs) become integral to intelligent user
interfaces (IUIs), their role as decision-making agents raises critical
concerns about alignment. Although extensive research has addressed issues such
as factuality, bias, and toxicity, comparatively little attention has been paid
to measuring alignment to preferences, i.e., the relative desirability of
different alternatives, a concept used in decision making, economics, and
social choice theory. However, a reliable decision-making agent makes choices
that align well with user preferences.
  In this paper, we generalize existing methods that exploit LLMs for ranking
alternative outcomes by addressing alignment with the broader and more flexible
concept of user preferences, which includes both strict preferences and
indifference among alternatives. To this end, we put forward design principles
for using LLMs to implement rational choice functions, and provide the
necessary tools to measure preference satisfaction. We demonstrate the
applicability of our approach through an empirical study in a practical
application of an IUI in the automotive domain.

</details>


### [180] [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
*Daocheng Fu,Zijun Chen,Renqiu Xia,Qi Liu,Yuan Feng,Hongbin Zhou,Renrui Zhang,Shiyang Feng,Peng Gao,Junchi Yan,Botian Shi,Bo Zhang,Yu Qiao*

Main category: cs.AI

TL;DR: The paper introduces TrustGeoGen, a scalable data engine for generating verified geometric problems, addressing noise and inconsistencies in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic GPS benchmarks lack self-verification and contain noise due to LLM illusions, hindering reliable method development.

Method: TrustGeoGen synthesizes geometric data via multimodal-aligned generation, formal verification, bootstrapping, and GeoExplore algorithms.

Result: The engine produces the GeoTrust-200K dataset, with models achieving 49.17% accuracy on GeoTrust-test, showing improved OOD generalization.

Conclusion: TrustGeoGen provides a principled benchmark for GPS, reducing logical inconsistencies and advancing method development.

Abstract: Mathematical geometric problem solving (GPS) often requires effective
integration of multimodal information and verifiable logical coherence. Despite
the fast development of large language models in general problem solving, it
remains unresolved regarding with both methodology and benchmarks, especially
given the fact that exiting synthetic GPS benchmarks are often not
self-verified and contain noise and self-contradicted information due to the
illusion of LLMs. In this paper, we propose a scalable data engine called
TrustGeoGen for problem generation, with formal verification to provide a
principled benchmark, which we believe lays the foundation for the further
development of methods for GPS. The engine synthesizes geometric data through
four key innovations: 1) multimodal-aligned generation of diagrams, textual
descriptions, and stepwise solutions; 2) formal verification ensuring
rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling
complexity escalation via recursive state generation and 4) our devised
GeoExplore series algorithms simultaneously produce multi-solution variants and
self-reflective backtracking traces. By formal logical verification,
TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,
along with GeoTrust-test testset. Experiments reveal the state-of-the-art
models achieve only 49.17\% accuracy on GeoTrust-test, demonstrating its
evaluation stringency. Crucially, models trained on GeoTrust achieve OOD
generalization on GeoQA, significantly reducing logical inconsistencies
relative to pseudo-label annotated by OpenAI-o1. Our code is available at
https://github.com/Alpha-Innovator/TrustGeoGen

</details>


### [181] [WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents](https://arxiv.org/abs/2504.15785)
*Siyu Zhou,Tianyi Zhou,Yijun Yang,Guodong Long,Deheng Ye,Jing Jiang,Chengqi Zhang*

Main category: cs.AI

TL;DR: The paper proposes 'world alignment' to enhance LLM-based world models by extracting symbolic knowledge from environments, and introduces WALL-E 2.0, an efficient LLM agent for model-predictive control, achieving superior performance in open-world challenges.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between LLMs' prior knowledge and specific environment dynamics, improving LLMs' performance as world models.

Method: Training-free 'world alignment' extracts symbolic knowledge (action rules, knowledge graphs, scene graphs) from exploration trajectories, encoded into executable codes. WALL-E 2.0 uses LLM as an efficient look-ahead optimizer in MPC.

Result: WALL-E 2.0 outperforms baselines in Mars (16.1%-51.6% higher success rate) and ALFWorld (98% success rate after 4 iterations).

Conclusion: The proposed neurosymbolic world model and LLM agent significantly improve learning efficiency and performance in new environments.

Abstract: Can we build accurate world models out of large language models (LLMs)? How
can world models benefit LLM agents? The gap between the prior knowledge of
LLMs and the specified environment's dynamics usually bottlenecks LLMs'
performance as world models. To bridge the gap, we propose a training-free
"world alignment" that learns an environment's symbolic knowledge complementary
to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and
scene graphs, which are extracted by LLMs from exploration trajectories and
encoded into executable codes to regulate LLM agents' policies. We further
propose an RL-free, model-based agent "WALL-E 2.0" through the model-predictive
control (MPC) framework. Unlike classical MPC requiring costly optimization on
the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future
steps' actions by interacting with the neurosymbolic world model. While the LLM
agent's strong heuristics make it an efficient planner in MPC, the quality of
its planned actions is also secured by the accurate predictions of the aligned
world model. They together considerably improve learning efficiency in a new
environment. On open-world challenges in Mars (Minecraft like) and ALFWorld
(embodied indoor environments), WALL-E 2.0 significantly outperforms existing
methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and
by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success
rate after only 4 iterations.

</details>


### [182] [Crisp complexity of fuzzy classifiers](https://arxiv.org/abs/2504.15791)
*Raquel Fernandez-Peralta,Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TL;DR: A method to convert fuzzy rule-based classifiers to crisp rule-based ones, aiding interpretability and cross-community adoption.


<details>
  <summary>Details</summary>
Motivation: Fuzzy rule-based classifiers lack wider adoption due to interpretability issues and unfamiliarity with fuzzy logic.

Method: Proposes a methodology to reduce fuzzy rules to crisp rules, studies crisp descriptions, and implements an algorithm for conversion.

Result: Analyzes complexity of crisp classifiers, aiding understanding of feature space partitioning and system translation.

Conclusion: The work bridges fuzzy and non-fuzzy communities, offering a complexity metric to guide classifier choice.

Abstract: Rule-based systems are a very popular form of explainable AI, particularly in
the fuzzy community, where fuzzy rules are widely used for control and
classification problems. However, fuzzy rule-based classifiers struggle to
reach bigger traction outside of fuzzy venues, because users sometimes do not
know about fuzzy and because fuzzy partitions are not so easy to interpret in
some situations. In this work, we propose a methodology to reduce fuzzy
rule-based classifiers to crisp rule-based classifiers. We study different
possible crisp descriptions and implement an algorithm to obtain them. Also, we
analyze the complexity of the resulting crisp classifiers. We believe that our
results can help both fuzzy and non-fuzzy practitioners understand better the
way in which fuzzy rule bases partition the feature space and how easily one
system can be translated to another and vice versa. Our complexity metric can
also help to choose between different fuzzy classifiers based on what the
equivalent crisp partitions look like.

</details>


### [183] [Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases](https://arxiv.org/abs/2504.15829)
*Modhurita Mitra,Martine G. de Vos,Nicola Cortinovis,Dawa Ometto*

Main category: cs.AI

TL;DR: Exploratory study on generative AI (Claude 3 Opus) for research data processing, focusing on feasibility in complex tasks like information extraction, natural language understanding, and text classification.


<details>
  <summary>Details</summary>
Motivation: Address concerns about generative AI's accuracy and consistency by exploring its application in research data processing, particularly for tasks challenging for rule-based or traditional ML approaches.

Method: Applied Claude 3 Opus to three tasks: extracting plant species names from seedlists, extracting health data from EU documents, and classifying Kickstarter projects by industry codes.

Result: Demonstrated feasibility of generative AI for complex data tasks, with insights on task suitability and optimizing accuracy/consistency.

Conclusion: Generative AI is viable for certain research data tasks, but careful evaluation and optimization are needed for reliable results.

Abstract: There has been enormous interest in generative AI since ChatGPT was launched
in 2022. However, there are concerns about the accuracy and consistency of the
outputs of generative AI. We have carried out an exploratory study on the
application of this new technology in research data processing. We identified
tasks for which rule-based or traditional machine learning approaches were
difficult to apply, and then performed these tasks using generative AI.
  We demonstrate the feasibility of using the generative AI model Claude 3 Opus
in three research projects involving complex data processing tasks:
  1) Information extraction: We extract plant species names from historical
seedlists (catalogues of seeds) published by botanical gardens.
  2) Natural language understanding: We extract certain data points (name of
drug, name of health indication, relative effectiveness, cost-effectiveness,
etc.) from documents published by Health Technology Assessment organisations in
the EU.
  3) Text classification: We assign industry codes to projects on the
crowdfunding website Kickstarter.
  We share the lessons we learnt from these use cases: How to determine if
generative AI is an appropriate tool for a given data processing task, and if
so, how to maximise the accuracy and consistency of the results obtained.

</details>


### [184] [CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters](https://arxiv.org/abs/2504.15847)
*Xiang Liu,Hau Chan,Minming Li,Xianlong Zeng,Chenchen Fu,Weiwei Wu*

Main category: cs.AI

TL;DR: The paper introduces compatibility-aware incentive mechanisms (CARE-CO and CARE-NO) for federated learning to address worker incompatibility and budget constraints, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing FL incentive mechanisms ignore worker incompatibility and requester budget limits, degrading efficiency and utility.

Method: Developed CARE-CO (cooperative) and CARE-NO (non-cooperative) mechanisms to elicit true costs and optimize worker hiring under budget constraints.

Result: Mechanisms guarantee individual rationality, truthfulness, budget feasibility, and show superior performance in experiments.

Conclusion: The proposed mechanisms effectively address real-world FL challenges, improving efficiency and utility.

Abstract: Federated learning (FL) is a promising approach that allows requesters (\eg,
servers) to obtain local training models from workers (e.g., clients). Since
workers are typically unwilling to provide training services/models freely and
voluntarily, many incentive mechanisms in FL are designed to incentivize
participation by offering monetary rewards from requesters. However, existing
studies neglect two crucial aspects of real-world FL scenarios. First, workers
can possess inherent incompatibility characteristics (e.g., communication
channels and data sources), which can lead to degradation of FL efficiency
(e.g., low communication efficiency and poor model generalization). Second, the
requesters are budgeted, which limits the amount of workers they can hire for
their tasks. In this paper, we investigate the scenario in FL where multiple
budgeted requesters seek training services from incompatible workers with
private training costs. We consider two settings: the cooperative budget
setting where requesters cooperate to pool their budgets to improve their
overall utility and the non-cooperative budget setting where each requester
optimizes their utility within their own budgets. To address efficiency
degradation caused by worker incompatibility, we develop novel
compatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both
settings to elicit true private costs and determine workers to hire for
requesters and their rewards while satisfying requester budget constraints. Our
mechanisms guarantee individual rationality, truthfulness, budget feasibility,
and approximation performance. We conduct extensive experiments using
real-world datasets to show that the proposed mechanisms significantly
outperform existing baselines.

</details>


### [185] [Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations](https://arxiv.org/abs/2504.15903)
*Nikhil Khandalkar,Pavan Yadav,Krishna Shinde,Lokesh B. Ramegowda,Rajarshi Das*

Main category: cs.AI

TL;DR: GPT-4o excels in ARC tasks under zero-noise, but other models fail. Noise universally impairs performance, revealing LLMs' fragility in reasoning.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' structured reasoning and generalization in noisy conditions using the ARC benchmark.

Method: Systematic evaluation of models (GPT-4o, DeepSeek R1, LLaMA 3.2) across noise levels and temperature settings.

Result: Noise consistently degrades performance, exposing LLMs' sensitivity to input perturbations.

Conclusion: Current LLMs lack robustness in noisy environments, urging development of more adaptable AI systems for real-world applications.

Abstract: Recent advancements in Large Language Models (LLMs) have generated growing
interest in their structured reasoning capabilities, particularly in tasks
involving abstraction and pattern recognition. The Abstraction and Reasoning
Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by
testing how well AI models generalize to novel problems. While GPT-4o
demonstrates strong performance by solving all ARC tasks under zero-noise
conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,
suggesting limitations in their ability to reason beyond simple pattern
matching. To explore this gap, we systematically evaluate these models across
different noise levels and temperature settings. Our results reveal that the
introduction of noise consistently impairs model performance, regardless of
architecture. This decline highlights a shared vulnerability: current LLMs,
despite showing signs of abstract reasoning, remain highly sensitive to input
perturbations. Such fragility raises concerns about their real-world
applicability, where noise and uncertainty are common. By comparing how
different model architectures respond to these challenges, we offer insights
into the structural weaknesses of modern LLMs in reasoning tasks. This work
underscores the need for developing more robust and adaptable AI systems
capable of handling the ambiguity and variability inherent in real-world
scenarios. Our findings aim to guide future research toward enhancing model
generalization, robustness, and alignment with human-like cognitive
flexibility.

</details>


### [186] [Approximate matrices of systems of max-min fuzzy relational equations](https://arxiv.org/abs/2504.16042)
*Ismaïl Baaj*

Main category: cs.AI

TL;DR: The paper proposes a method to resolve inconsistencies in max-min fuzzy relational equations by minimally adjusting the system's matrix while preserving the right-hand side vector. It focuses on minimizing the distance (using L1, L2, or L∞ norms) between the original and consistent matrices, with explicit formulas for L∞ norm.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of inconsistency in fuzzy relational equations, the study aims to provide a method that minimally modifies the system's matrix to achieve consistency without altering the right-hand side vector.

Method: The method involves adjusting the matrix entries minimally to achieve consistency, focusing on minimizing the distance (L1, L2, or L∞ norms) between the original and consistent matrices. It provides explicit formulas for L∞ norm minimization.

Result: The method successfully computes consistent systems with minimal distance to the original inconsistent system, particularly efficient for L∞ norm. It also extends results to min-max fuzzy relational equations.

Conclusion: The approach effectively resolves inconsistencies in fuzzy relational equations with minimal modifications, offering practical applications and computational efficiency, especially for L∞ norm.

Abstract: In this article, we address the inconsistency of a system of max-min fuzzy
relational equations by minimally modifying the matrix governing the system in
order to achieve consistency. Our method yields consistent systems that
approximate the original inconsistent system in the following sense: the
right-hand side vector of each consistent system is that of the inconsistent
system, and the coefficients of the matrix governing each consistent system are
obtained by modifying, exactly and minimally, the entries of the original
matrix that must be corrected to achieve consistency, while leaving all other
entries unchanged.
  To obtain a consistent system that closely approximates the considered
inconsistent system, we study the distance (in terms of a norm among $L_1$,
$L_2$ or $L_\infty$) between the matrix of the inconsistent system and the set
formed by the matrices of consistent systems that use the same right-hand side
vector as the inconsistent system. We show that our method allows us to
directly compute matrices of consistent systems that use the same right-hand
side vector as the inconsistent system whose distance in terms of $L_\infty$
norm to the matrix of the inconsistent system is minimal (the computational
costs are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit
analytical formula for computing this minimal $L_\infty$ distance. Finally, we
translate our results for systems of min-max fuzzy relational equations and
present some potential applications.

</details>


### [187] [Language Models for Business Optimisation with a Real World Case Study in Production Scheduling](https://arxiv.org/abs/2309.13218)
*Pivithuru Thejan Amarasinghe,Su Nguyen,Yuan Sun,Damminda Alahakoon*

Main category: cs.AI

TL;DR: The paper proposes an LLM-based framework for automating problem formulation in business optimization, focusing on fine-tuning cost-efficient LLMs for specialized challenges.


<details>
  <summary>Details</summary>
Motivation: Problem formulation in business optimization is complex and requires expertise. LLMs show potential for automating this task, but challenges like limited training data and problem complexity exist.

Method: The authors introduce a method for fine-tuning cost-efficient LLMs tailored to business optimization problems, tested on production scheduling and general linear programming.

Result: The framework generates accurate formulations for conventional and real-world problems, showing effectiveness and convergence in fine-tuning. It outperforms state-of-the-art prompt engineering methods.

Conclusion: The proposed LLM-based framework is effective for automating problem formulation in business optimization, offering competitive performance and scalability.

Abstract: Business optimisation has been used extensively to determine optimal
solutions for challenging business operations. Problem formulation is an
important part of business optimisation as it influences both the validity of
solutions and the efficiency of the optimisation process. While different
optimisation modelling languages have been developed, problem formulation is
still not a trivial task and usually requires optimisation expertise and
problem-domain knowledge. Recently, Large Language Models (LLMs) have
demonstrated outstanding performance across different language-related tasks.
Since problem formulation can be viewed as a translation task, there is a
potential to leverage LLMs to automate problem formulation. However, developing
an LLM for problem formulation is challenging, due to limited training data,
and the complexity of real-world optimisation problems. Several prompt
engineering methods have been proposed in the literature to automate problem
formulation with LLMs. While the initial results are encouraging, the accuracy
of formulations generated by these methods can still be significantly improved.
In this paper, we present an LLM-based framework for automating problem
formulation in business optimization. Our approach introduces a method for
fine-tuning cost-efficient LLMs specifically tailored to specialized business
optimization challenges. The experiment results demonstrate that our framework
can generate accurate formulations for conventional and real-world business
optimisation problems in production scheduling. Extensive analyses show the
effectiveness and the convergence of the proposed fine-tuning method. The
proposed method also shows very competitive performance when compared with the
state-of-the-art prompt engineering methods in the literature when tested on
general linear programming problems.

</details>


### [188] [Certifying Knowledge Comprehension in LLMs](https://arxiv.org/abs/2402.15929)
*Isha Chaudhary,Vedaant V. Jain,Gagandeep Singh*

Main category: cs.AI

TL;DR: The paper introduces a certification framework for evaluating LLMs' knowledge comprehension, offering formal guarantees on reliability and revealing vulnerabilities in state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLMs on knowledge comprehension rely on small datasets, raising concerns about reliability and generalizability.

Method: The authors propose a specification and certification framework using knowledge graphs to represent large distributions of prompts and generate probabilistic guarantees.

Result: The framework uncovers vulnerabilities in SOTA LLMs due to natural noise and establishes performance hierarchies with formal guarantees.

Conclusion: The work provides a robust method for certifying LLMs' knowledge comprehension, enhancing reliability in safety-critical applications.

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical
systems where they provide answers based on in-context information derived from
knowledge bases. As LLMs are increasingly envisioned as superhuman agents,
their proficiency in knowledge comprehension-extracting relevant information
and reasoning over it to answer questions, a key facet of human
intelligence-becomes crucial. However, existing evaluations of LLMs on
knowledge comprehension are typically conducted on small test sets, but these
datasets represent only a tiny fraction of the vast number of possible queries.
Simple empirical evaluations on these limited test sets raises concerns about
the reliability and generalizability of the results. In this work, we introduce
the first specification and certification framework for knowledge comprehension
in LLMs, providing formal probabilistic guarantees for reliability. Instead of
a fixed dataset, we design novel specifications that mathematically represent
prohibitively large probability distributions of knowledge comprehension
prompts with natural noise, using knowledge graphs. From these specifications,
we generate quantitative certificates that offer high-confidence, tight bounds
on the probability that a given LLM correctly answers any question drawn from
the specification distribution. We apply our framework to certify SOTA LLMs in
two domains: precision medicine and general question-answering. Our results
reveal previously unrecognized vulnerabilities in SOTA LLMs due to natural
noise in the prompts. Additionally, we establish performance hierarchies with
formal guarantees among the SOTA LLMs, particularly in the context of precision
medicine question-answering.

</details>


### [189] [Certifying Counterfactual Bias in LLMs](https://arxiv.org/abs/2405.18780)
*Isha Chaudhary,Qian Hu,Manoj Kumar,Morteza Ziyadi,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: LLMCert-B is a framework to certify LLMs for counterfactual bias, providing high-confidence bounds on unbiased responses for demographic-specific prompts.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to thoroughly evaluate biases in LLMs across demographic groups due to scalability and lack of guarantees.

Method: Proposes LLMCert-B to certify counterfactual bias on prompt distributions, using prefix distributions (random tokens, jailbreaks, perturbations).

Result: Non-trivial certificates generated for SOTA LLMs, revealing vulnerabilities over inexpensive prompt distributions.

Conclusion: LLMCert-B effectively identifies and certifies biases in LLMs, highlighting their limitations in handling demographic-specific prompts.

Abstract: Large Language Models (LLMs) can produce biased responses that can cause
representational harms. However, conventional studies are insufficient to
thoroughly evaluate biases across LLM responses for different demographic
groups (a.k.a. counterfactual bias), as they do not scale to large number of
inputs and do not provide guarantees. Therefore, we propose the first
framework, LLMCert-B that certifies LLMs for counterfactual bias on
distributions of prompts. A certificate consists of high-confidence bounds on
the probability of unbiased LLM responses for any set of counterfactual prompts
- prompts differing by demographic groups, sampled from a distribution. We
illustrate counterfactual bias certification for distributions of
counterfactual prompts created by applying prefixes sampled from prefix
distributions, to a given set of prompts. We consider prefix distributions
consisting random token sequences, mixtures of manual jailbreaks, and
perturbations of jailbreaks in LLM's embedding space. We generate non-trivial
certificates for SOTA LLMs, exposing their vulnerabilities over distributions
of prompts generated from computationally inexpensive prefix distributions.

</details>


### [190] [DeepDiveAI: Identifying AI Related Documents in Large Scale Literature Data](https://arxiv.org/abs/2408.12871)
*Zhou Xiaochen,Liang Xingzhou,Zou Hui,Lu Yi,Qu Jingjing*

Main category: cs.AI

TL;DR: Proposes a two-stage method to classify AI-related documents, creating the DeepDiveAI dataset using expert knowledge and advanced models (LSTM and BERT).


<details>
  <summary>Details</summary>
Motivation: To efficiently and accurately identify AI-related literature from large-scale datasets.

Method: 1. Train an LSTM model on expert-curated data for coarse classification. 2. Use Qwen2.5 Plus to annotate a subset, then train a BERT classifier for refinement.

Result: The workflow efficiently and accurately identifies AI-related literature, resulting in the DeepDiveAI dataset.

Conclusion: The proposed method successfully constructs a high-quality AI-related literature dataset.

Abstract: In this paper, we propose a method to automatically classify AI-related
documents from large-scale literature databases, leading to the creation of an
AI-related literature dataset, named DeepDiveAI. The dataset construction
approach integrates expert knowledge with the capabilities of advanced models,
structured across two global stages. In the first stage, expert-curated
classification datasets are used to train an LSTM model, which classifies
coarse AI related records from large-scale datasets. In the second stage, we
use Qwen2.5 Plus to annotate a random 10% of the coarse AI-related records,
which are then used to train a BERT binary classifier. This step further
refines the coarse AI related record set to obtain the final DeepDiveAI
dataset. Evaluation results demonstrate that the entire workflow can
efficiently and accurately identify AI-related literature from large-scale
datasets.

</details>


### [191] [Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments](https://arxiv.org/abs/2410.21131)
*Marharyta Domnich,Julius Välja,Rasmus Moorits Veski,Giacomo Magnifico,Kadi Tulver,Eduard Barbu,Raul Vicente*

Main category: cs.AI

TL;DR: The paper addresses the lack of human-centric evaluation in counterfactual explanations for AI by developing diverse scenarios and using LLMs to predict human judgments, achieving up to 85% accuracy with fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of counterfactual explanations lacks grounding in user studies and fails to fully capture human perspectives, necessitating a more human-centric approach.

Method: Developed 30 counterfactual scenarios, collected ratings from 206 respondents across 8 metrics, and fine-tuned LLMs to predict human judgments.

Result: LLMs achieved 63% accuracy in zero-shot evaluations and 85% in fine-tuned evaluations (3-class prediction) across all metrics.

Conclusion: Fine-tuned LLMs offer scalable and comparable evaluation of counterfactual explanations, bridging the gap between AI and human perspectives.

Abstract: As machine learning models evolve, maintaining transparency demands more
human-centric explainable AI techniques. Counterfactual explanations, with
roots in human reasoning, identify the minimal input changes needed to obtain a
given output and, hence, are crucial for supporting decision-making. Despite
their importance, the evaluation of these explanations often lacks grounding in
user studies and remains fragmented, with existing metrics not fully capturing
human perspectives. To address this challenge, we developed a diverse set of 30
counterfactual scenarios and collected ratings across 8 evaluation metrics from
206 respondents. Subsequently, we fine-tuned different Large Language Models
(LLMs) to predict average or individual human judgment across these metrics.
Our methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot
evaluations and 85% (over a 3-classes prediction) with fine-tuning across all
metrics. The fine-tuned models predicting human ratings offer better
comparability and scalability in evaluating different counterfactual
explanation frameworks.

</details>


### [192] [AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities](https://arxiv.org/abs/2412.09385)
*Fabrizio Davide,Pietro Torre,Leonardo Ercolani,Andrea Gaggioli*

Main category: cs.AI

TL;DR: 16 LLMs estimated AGI likelihood by 2030, with results (3%-47.6%) aligning with expert surveys. LLM-PR showed high reliability (ICC=0.79). Pplx-70b-online performed best; Gemini-1.5-pro-api worst. Findings highlight LLMs' potential in speculative forecasting and the need for new evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to forecast complex, speculative scenarios like AGI emergence by 2030 and evaluate their reliability through automated peer review.

Method: Tasked 16 LLMs with AGI likelihood estimation, implemented LLM-PR for quality assessment, and compared results with expert surveys and external benchmarks.

Result: LLMs' estimates ranged 3%-47.6% (median 12.5%), aligning with expert forecasts. LLM-PR showed high reliability (ICC=0.79). Pplx-70b-online was top performer; Gemini-1.5-pro-api lowest.

Conclusion: LLMs show promise in speculative forecasting, but new benchmarks are needed to evaluate AGI-related skills. Findings stress the need for innovative evaluation frameworks.

Abstract: We tasked 16 state-of-the-art large language models (LLMs) with estimating
the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To
assess the quality of these forecasts, we implemented an automated peer review
process (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-
Core) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align
with a recent expert survey that projected a 10% likelihood of AGI by 2027,
underscoring the relevance of LLMs in forecasting complex, speculative
scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a
high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable
consistency in scoring across the models. Among the models, Pplx-70b-online
emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A
cross-comparison with external benchmarks, such as LMSYS Chatbot Arena,
revealed that LLM rankings remained consistent across different evaluation
methods, suggesting that existing benchmarks may not encapsulate some of the
skills relevant for AGI prediction. We further explored the use of weighting
schemes based on external benchmarks, optimizing the alignment of LLMs'
predictions with human expert forecasts. This analysis led to the development
of a new, 'AGI benchmark' designed to highlight performance differences in
AGI-related tasks. Our findings offer insights into LLMs' capabilities in
speculative, interdisciplinary forecasting tasks and emphasize the growing need
for innovative evaluation frameworks for assessing AI performance in complex,
uncertain real-world scenarios.

</details>


### [193] [Codenames as a Benchmark for Large Language Models](https://arxiv.org/abs/2412.11373)
*Matthew Stephenson,Matthew Sidji,Benoît Ronval*

Main category: cs.AI

TL;DR: The paper proposes using the board game Codenames as a benchmark to evaluate LLMs' reasoning, language understanding, and theory of mind. It tests state-of-the-art LLMs, revealing varied performance and emergent behaviors, and shows LLMs generalize better with teammates than prior methods.


<details>
  <summary>Details</summary>
Motivation: Codenames is a challenging benchmark for AI, requiring advanced language understanding and reasoning. Prior methods, like word embeddings, were limited, making LLMs a promising alternative.

Method: Several LLMs (GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, Llama 3.1) are evaluated across board setups, testing individual and cooperative gameplay.

Result: LLMs show varied performance, with some excelling in specific roles. Cooperative play demonstrates better generalization with teammates compared to prior techniques.

Conclusion: LLMs are effective for Codenames, showcasing advanced reasoning and adaptability, though performance varies by model and role.

Abstract: In this paper, we propose the use of the popular word-based board game
Codenames as a suitable benchmark for evaluating the reasoning capabilities of
Large Language Models (LLMs). Codenames presents a highly interesting challenge
for achieving successful AI performance, requiring both a sophisticated
understanding of language, theory of mind, and epistemic reasoning
capabilities. Prior attempts to develop agents for Codenames have largely
relied on word embedding techniques, which have a limited vocabulary range and
perform poorly when paired with differing approaches. LLMs have demonstrated
enhanced reasoning and comprehension capabilities for language-based tasks, but
can still suffer in lateral thinking challenges. We evaluate the capabilities
of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5
Sonnet, and Llama 3.1, across a variety of board setups. Our results indicate
that while certain LLMs perform better than others overall, different models
exhibit varying emergent behaviours during gameplay and excel at specific
roles. We also evaluate the performance of different combinations of LLMs when
playing cooperatively together, demonstrating that LLM agents are more
generalisable to a wider range of teammates than prior techniques.

</details>


### [194] [Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control](https://arxiv.org/abs/2412.11761)
*Timothée Anne,Noah Syrkis,Meriem Elhosni,Florian Turati,Franck Legendre,Alain Jaquier,Sebastian Risi*

Main category: cs.AI

TL;DR: The paper introduces HIVE, a framework enabling a human to coordinate up to 2,000 agents via natural language dialogue with an LLM, tested in a real-time strategy game. Results show promise but highlight limitations like spatial processing and long-term planning.


<details>
  <summary>Details</summary>
Motivation: Explore LLMs' potential in human-swarm coordination for applications like disaster response and urban planning.

Method: Developed HIVE, a framework integrating LLMs for natural language coordination of agents, tested in a real-time strategy benchmark.

Result: HIVE successfully handles tasks like agent coordination and strategic planning but struggles with spatial visuals and long-term strategy.

Conclusion: LLMs show promise for human-swarm coordination but need improvements in spatial and strategic capabilities.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various tasks. Their potential to facilitate human coordination with many
agents is a promising but largely under-explored area. Such capabilities would
be helpful in disaster response, urban planning, and real-time strategy
scenarios. In this work, we introduce (1) a real-time strategy game benchmark
designed to evaluate these abilities and (2) a novel framework we term HIVE.
HIVE empowers a single human to coordinate swarms of up to 2,000 agents through
a natural language dialog with an LLM. We present promising results on this
multi-agent benchmark, with our hybrid approach solving tasks such as
coordinating agent movements, exploiting unit weaknesses, leveraging human
annotations, and understanding terrain and strategic points. Our findings also
highlight critical limitations of current models, including difficulties in
processing spatial visual information and challenges in formulating long-term
strategic plans. This work sheds light on the potential and limitations of LLMs
in human-swarm coordination, paving the way for future research in this area.
The HIVE project page, hive.syrkis.com, includes videos of the system in
action.

</details>


### [195] [Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws](https://arxiv.org/abs/2504.09597)
*Zhixuan Pan,Shaowen Wang,Jian Li*

Main category: cs.AI

TL;DR: The paper explores the relationship between compression and prediction in Large Language Models (LLMs) using Kolmogorov complexity and Shannon information theory. It introduces a hierarchical data-generation framework (Syntax-Knowledge model) to explain LLM behaviors like scaling laws, knowledge acquisition, and hallucinations.


<details>
  <summary>Details</summary>
Motivation: To provide principled explanations for LLM behaviors, such as scaling laws and hallucinations, by revisiting the classical link between compression and prediction.

Method: Leverages the Kolmogorov Structure Function and a two-part coding process to analyze LLM compression. Introduces the Syntax-Knowledge model under Bayesian settings to study prediction and compression.

Result: Theoretical analysis explains scaling laws, knowledge acquisition dynamics, and hallucinations in LLMs. Experimental results validate these predictions.

Conclusion: The study offers intuitive and principled insights into LLM behaviors, bridging theory and empirical observations.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
numerous tasks, yet principled explanations for their underlying mechanisms and
several phenomena, such as scaling laws, hallucinations, and related behaviors,
remain elusive. In this work, we revisit the classical relationship between
compression and prediction, grounded in Kolmogorov complexity and Shannon
information theory, to provide deeper insights into LLM behaviors. By
leveraging the Kolmogorov Structure Function and interpreting LLM compression
as a two-part coding process, we offer a detailed view of how LLMs acquire and
store information across increasing model and data scales -- from pervasive
syntactic patterns to progressively rarer knowledge elements. Motivated by this
theoretical perspective and natural assumptions inspired by Heap's and Zipf's
laws, we introduce a simplified yet representative hierarchical data-generation
framework called the Syntax-Knowledge model. Under the Bayesian setting, we
show that prediction and compression within this model naturally lead to
diverse learning and scaling behaviors of LLMs. In particular, our theoretical
analysis offers intuitive and principled explanations for both data and model
scaling laws, the dynamics of knowledge acquisition during training and
fine-tuning, factual knowledge hallucinations in LLMs. The experimental results
validate our theoretical predictions.

</details>


### [196] [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
*Christopher Zhang Cui,Xingdi Yuan,Ziang Xiao,Prithviraj Ammanabrolu,Marc-Alexandre Côté*

Main category: cs.AI

TL;DR: TALES is a collection of text-adventure games to evaluate LLMs' reasoning. Top models perform well on synthetic games but poorly on human-designed ones (<15%).


<details>
  <summary>Details</summary>
Motivation: To assess and enhance LLMs' reasoning skills for complex, sequential decision-making tasks.

Method: Introduces TALES, a diverse set of synthetic and human-written text-adventure games, and evaluates various LLMs on them.

Result: Top LLMs excel on synthetic games but struggle with human-designed ones, scoring below 15%.

Conclusion: LLMs need further improvement in reasoning for human-centric tasks, as current models fall short.

Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to
interact with the world. As tasks become more complex, they demand increasingly
sophisticated and diverse reasoning capabilities for sequential
decision-making, requiring structured reasoning over the context history to
determine the next best action. We introduce TALES, a diverse collection of
synthetic and human-written text-adventure games designed to challenge and
evaluate diverse reasoning capabilities. We present results over a range of
LLMs, open- and closed-weights, performing a qualitative analysis on the top
performing models. Despite an impressive showing on synthetic games, even the
top LLM-driven agents fail to achieve 15% on games designed for human
enjoyment. Code and visualization of the experiments can be found at
https://microsoft.github.io/tales.

</details>


### [197] [FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory](https://arxiv.org/abs/2504.14325)
*Alessio Buscemi,Daniele Proverbio,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Liò*

Main category: cs.AI

TL;DR: FAIRGAME is a framework for detecting biases in AI agent interactions using game theory, enabling standardized simulations and comparisons across scenarios and LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexity and interpretability challenges of multi-agent AI interactions to ensure trustworthy adoption in research and society.

Method: Leverages game theory and provides a reproducible, user-friendly IT framework (FAIRGAME) for simulating games and analyzing strategic interactions among AI agents.

Result: Uncovered biases in AI agent outcomes based on LLM type, language, and agent traits, enabling systematic bias discovery and behavior prediction.

Conclusion: FAIRGAME facilitates reliable simulation, bias detection, and further research into strategic decision-making with LLM agents.

Abstract: Letting AI agents interact in multi-agent applications adds a layer of
complexity to the interpretability and prediction of AI outcomes, with profound
implications for their trustworthy adoption in research and society. Game
theory offers powerful models to capture and interpret strategic interaction
among agents, but requires the support of reproducible, standardized and
user-friendly IT frameworks to enable comparison and interpretation of results.
To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition
using Game Theory. We describe its implementation and usage, and we employ it
to uncover biased outcomes in popular games among AI agents, depending on the
employed Large Language Model (LLM) and used language, as well as on the
personality trait or strategic knowledge of the agents. Overall, FAIRGAME
allows users to reliably and easily simulate their desired games and scenarios
and compare the results across simulation campaigns and with game-theoretic
predictions, enabling the systematic discovery of biases, the anticipation of
emerging behavior out of strategic interplays, and empowering further research
into strategic decision-making using LLM agents.

</details>


### [198] [Time's Up! An Empirical Study of LLM Reasoning Ability Under Output Length Constraint](https://arxiv.org/abs/2504.14350)
*Yi Sun,Han Wang,Jiaqiang Li,Jiacheng Liu,Xiangyu Li,Hao Wen,Huiwen Zheng,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.AI

TL;DR: The paper explores how Large Language Models (LLMs) perform under time and output length constraints, finding that optimal model choices and prompts vary with budget limits.


<details>
  <summary>Details</summary>
Motivation: To understand if and how LLMs' reasoning abilities remain effective under real-world time and output length constraints.

Method: An empirical study testing over 25 LLMs on reasoning datasets under varying output length budgets, analyzing accuracy correlations with model properties.

Result: Optimal model sizes and prompts change under different budgets, differing from unconstrained scenarios.

Conclusion: The findings provide practical guidance for deploying LLMs under latency constraints.

Abstract: Recent work has demonstrated the remarkable potential of Large Language
Models (LLMs) in test-time scaling. By making the models think before
answering, they are able to achieve much higher accuracy with extra inference
computation. However, in many real-world scenarios, models are used under time
constraints, where an answer should be given to the user within a certain
output length. It is unclear whether and how the reasoning abilities of LLMs
remain effective under such constraints. We take a first look at this problem
by conducting an in-depth empirical study. Specifically, we test more than 25
LLMs on common reasoning datasets under a wide range of output length budgets,
and we analyze the correlation between the inference accuracy and various
properties including model type, model size, prompt style, etc. We also
consider the mappings between the token budgets and the actual on-device
latency budgets. The results have demonstrated several interesting findings
regarding the budget-aware LLM reasoning that differ from the unconstrained
situation, e.g. the optimal choices of model sizes and prompts change under
different budgets. These findings offer practical guidance for users to deploy
LLMs under real-world latency constraints.

</details>


### [199] [AI with Emotions: Exploring Emotional Expressions in Large Language Models](https://arxiv.org/abs/2504.14706)
*Shin-nosuke Ishikawa,Atsushi Yoshino*

Main category: cs.AI

TL;DR: LLMs can simulate emotions in outputs, validated by sentiment analysis, showing potential for emotion-based AI interactions.


<details>
  <summary>Details</summary>
Motivation: Explore if current LLMs can express emotions in outputs, given their human-like performance in tasks.

Method: Used Russell's Circumplex model to define emotions; tested LLMs (GPT, Gemini, Llama3, Command R+) with sentiment analysis for evaluation.

Result: LLM-generated responses matched specified emotional states, proving their capability for emotional expression.

Conclusion: LLMs can simulate emotions, enabling applications like emotionally-aware AI advisors.

Abstract: The human-level performance of Large Language Models (LLMs) across various
tasks has raised expectations for the potential of Artificial Intelligence (AI)
to possess emotions someday. To explore the capability of current LLMs to
express emotions in their outputs, we conducted an experiment using several
LLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to
role-play as agents answering questions with specified emotional states. We
defined the emotional states using Russell's Circumplex model, a
well-established framework that characterizes emotions along the
sleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose
this model for its simplicity, utilizing two continuous parameters, which
allows for better controllability in applications involving continuous changes
in emotional states. The responses generated were evaluated using a sentiment
analysis model, independent of the LLMs, trained on the GoEmotions dataset. The
evaluation showed that the emotional states of the generated answers were
consistent with the specifications, demonstrating the LLMs' capability for
emotional expression. This indicates the potential for LLM-based AI agents to
simulate emotions, opening up a wide range of applications for emotion-based
interactions, such as advisors or consultants who can provide advice or
opinions with a personal touch.

</details>


### [200] [Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision](https://arxiv.org/abs/2504.15046)
*Shilin Zhang,Zican Hu,Wenhao Wu,Xinyi Xie,Jianxiang Tang,Chunlin Chen,Daoyi Dong,Yu Cheng,Zhenhong Sun,Zhi Wang*

Main category: cs.AI

TL;DR: T2DA is a framework that uses natural language to supervise generalist policy learning, enabling zero-shot text-to-decision generation without expensive pre-acquired task signals.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of RL systems that rely on costly or infeasible supervision signals for generalization by leveraging raw text as a broader source of supervision.

Method: Introduce a generalized world model for multi-task decision data encoding and use contrastive language-decision pre-training (inspired by CLIP) to align text embeddings with decision embeddings.

Result: T2DA achieves high-capacity zero-shot generalization and outperforms baselines on MuJoCo and Meta-World benchmarks.

Conclusion: T2DA provides a scalable and effective solution for text-conditioned policy learning, demonstrating strong zero-shot generalization capabilities.

Abstract: RL systems usually tackle generalization by inferring task beliefs from
high-quality samples or warmup explorations. The restricted form limits their
generality and usability since these supervision signals are expensive and even
infeasible to acquire in advance for unseen tasks. Learning directly from the
raw text about decision tasks is a promising alternative to leverage a much
broader source of supervision. In the paper, we propose Text-to-Decision Agent
(T2DA), a simple and scalable framework that supervises generalist policy
learning with natural language. We first introduce a generalized world model to
encode multi-task decision data into a dynamics-aware embedding space. Then,
inspired by CLIP, we predict which textual description goes with which decision
embedding, effectively bridging their semantic gap via contrastive
language-decision pre-training and aligning the text embeddings to comprehend
the environment dynamics. After training the text-conditioned generalist
policy, the agent can directly realize zero-shot text-to-decision generation in
response to language instructions. Comprehensive experiments on MuJoCo and
Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot
generalization and outperforms various types of baselines.

</details>


### [201] [Synergistic Weak-Strong Collaboration by Aligning Preferences](https://arxiv.org/abs/2504.15188)
*Yizhu Jiao,Xuchao Zhang,Zhaoyang Wang,Yubo Ma,Zhun Deng,Rujia Wang,Chetan Bansal,Saravan Rajmohan,Jiawei Han,Huaxiu Yao*

Main category: cs.AI

TL;DR: A collaborative framework pairs a specialized weak model with a general strong model to enhance LLMs' performance in specialized tasks, outperforming individual models.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with specialized tasks due to lack of domain-specific knowledge and high computational costs of fine-tuning.

Method: Proposes a framework where a weak model provides drafts and background, while a strong model refines them. Introduces collaborative feedback to fine-tune the weak model.

Result: Collaboration outperforms individual models, and aligning the weak model with collaborative preferences further improves performance.

Conclusion: The framework effectively extends LLMs' capabilities to specialized tasks by leveraging complementary strengths of weak and strong models.

Abstract: Current Large Language Models (LLMs) excel in general reasoning yet struggle
with specialized tasks requiring proprietary or domain-specific knowledge.
Fine-tuning large models for every niche application is often infeasible due to
black-box constraints and high computational overhead. To address this, we
propose a collaborative framework that pairs a specialized weak model with a
general strong model. The weak model, tailored to specific domains, produces
initial drafts and background information, while the strong model leverages its
advanced reasoning to refine these drafts, extending LLMs' capabilities to
critical yet specialized tasks. To optimize this collaboration, we introduce a
collaborative feedback to fine-tunes the weak model, which quantifies the
influence of the weak model's contributions in the collaboration procedure and
establishes preference pairs to guide preference tuning of the weak model. We
validate our framework through experiments on three domains. We find that the
collaboration significantly outperforms each model alone by leveraging
complementary strengths. Moreover, aligning the weak model with the
collaborative preference further enhances overall performance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [202] [Quantifying Source Speaker Leakage in One-to-One Voice Conversion](https://arxiv.org/abs/2504.15822)
*Scott Wellington,Xuechen Liu,Junichi Yamagishi*

Main category: cs.SD

TL;DR: The paper demonstrates a method to quantify confidence in identifying source speakers in one-to-one voice conversion, using a multi-accented corpus and HiFi-GAN vocoder, highlighting privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To address privacy risks in synthetic voice technology by quantifying information leakage in voice conversion.

Method: Uses a multi-accented corpus and HiFi-GAN vocoder for voice conversion, analyzing speaker characteristics in a white-box scenario.

Result: Shows it's possible to infer and narrow down source speakers, emphasizing privacy risks.

Conclusion: Highlights the need for providers of synthetic voices to protect speaker data privacy.

Abstract: Using a multi-accented corpus of parallel utterances for use with commercial
speech devices, we present a case study to show that it is possible to quantify
a degree of confidence about a source speaker's identity in the case of
one-to-one voice conversion. Following voice conversion using a HiFi-GAN
vocoder, we compare information leakage for a range speaker characteristics;
assuming a "worst-case" white-box scenario, we quantify our confidence to
perform inference and narrow the pool of likely source speakers, reinforcing
the regulatory obligation and moral duty that providers of synthetic voices
have to ensure the privacy of their speakers' data.

</details>


### [203] [Listenable Maps for Zero-Shot Audio Classifiers](https://arxiv.org/abs/2405.17615)
*Francesco Paissan,Luca Della Libera,Mirco Ravanelli,Cem Subakan*

Main category: cs.SD

TL;DR: LMAC-ZS is the first decoder-based post-hoc method for interpreting zero-shot audio classifiers, using a novel loss function for faithfulness and producing meaningful explanations.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and trustworthiness of deep learning models, especially audio classifiers, by providing interpretable explanations for their decisions.

Method: Introduces LMAC-ZS, a decoder-based post-hoc interpretation method with a novel loss function to maximize faithfulness to original text-audio pair similarity, evaluated using the CLAP model.

Result: LMAC-ZS remains faithful to zero-shot classification decisions and produces meaningful explanations correlating well with text prompts.

Conclusion: LMAC-ZS effectively interprets zero-shot audio classifiers, offering transparency and trustworthiness through faithful and meaningful explanations.

Abstract: Interpreting the decisions of deep learning models, including audio
classifiers, is crucial for ensuring the transparency and trustworthiness of
this technology. In this paper, we introduce LMAC-ZS (Listenable Maps for Audio
Classifiers in the Zero-Shot context), which, to the best of our knowledge, is
the first decoder-based post-hoc interpretation method for explaining the
decisions of zero-shot audio classifiers. The proposed method utilizes a novel
loss function that maximizes the faithfulness to the original similarity
between a given text-and-audio pair. We provide an extensive evaluation using
the Contrastive Language-Audio Pretraining (CLAP) model to showcase that our
interpreter remains faithful to the decisions in a zero-shot classification
context. Moreover, we qualitatively show that our method produces meaningful
explanations that correlate well with different text prompts.

</details>


### [204] [Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Fold Paralysis](https://arxiv.org/abs/2409.03597)
*Yucong Zhang,Xin Zou,Jinshan Yang,Wenjun Chen,Juan Liu,Faya Liang,Ming Li*

Main category: cs.SD

TL;DR: MLVAS is a system combining audio and video data to analyze laryngeal videos, extracting key segments and features for clinical assessment, including VFP detection.


<details>
  <summary>Details</summary>
Motivation: To improve clinical assessment of vocal fold movements by automating the extraction of key video segments and metrics from laryngeal videos.

Method: Integrates video-based glottis detection and audio keyword spotting, uses pre-trained audio encoders and diffusion-based refinement for segmentation, and measures vocal fold angles.

Result: Effective segmentation and VFP classification, validated on public and real-world datasets.

Conclusion: MLVAS provides reliable, objective metrics and visualization for assisted clinical diagnosis.

Abstract: This paper presents the Multimodal Laryngoscopic Video Analyzing System
(MLVAS), a novel system that leverages both audio and video data to
automatically extract key video segments and metrics from raw laryngeal
videostroboscopic videos for assisted clinical assessment. The system
integrates video-based glottis detection with an audio keyword spotting method
to analyze both video and audio data, identifying patient vocalizations and
refining video highlights to ensure optimal inspection of vocal fold movements.
Beyond key video segment extraction from the raw laryngeal videos, MLVAS is
able to generate effective audio and visual features for Vocal Fold Paralysis
(VFP) detection. Pre-trained audio encoders are utilized to encode the patient
voice to get the audio features. Visual features are generated by measuring the
angle deviation of both the left and right vocal folds to the estimated glottal
midline on the segmented glottis masks. To get better masks, we introduce a
diffusion-based refinement that follows traditional U-Net segmentation to
reduce false positives. We conducted several ablation studies to demonstrate
the effectiveness of each module and modalities in the proposed MLVAS. The
experimental results on a public segmentation dataset show the effectiveness of
our proposed segmentation module. In addition, unilateral VFP classification
results on a real-world clinic dataset demonstrate MLVAS's ability of providing
reliable and objective metrics as well as visualization for assisted clinical
diagnosis.

</details>


### [205] [Semi-Supervised Self-Learning Enhanced Music Emotion Recognition](https://arxiv.org/abs/2410.21897)
*Yifu Sun,Xulong Zhang,Monan Zhou,Wei Li*

Main category: cs.SD

TL;DR: A semi-supervised self-learning method is proposed to address label noise in segment-based music emotion recognition, improving performance on limited datasets.


<details>
  <summary>Details</summary>
Motivation: Current MER datasets are small, and segment-based methods introduce label noise by inheriting clip-level labels, leading to overfitting.

Method: Proposes a semi-supervised self-learning (SSSL) method to differentiate correct and incorrect segment labels, leveraging augmented data.

Result: Achieves better or comparable performance on three public emotional datasets.

Conclusion: The SSSL method effectively handles label noise and enhances MER performance without additional resources.

Abstract: Music emotion recognition (MER) aims to identify the emotions conveyed in a
given musical piece. However, currently, in the field of MER, the available
public datasets have limited sample sizes. Recently, segment-based methods for
emotion-related tasks have been proposed, which train backbone networks on
shorter segments instead of entire audio clips, thereby naturally augmenting
training samples without requiring additional resources. Then, the predicted
segment-level results are aggregated to obtain the entire song prediction. The
most commonly used method is that the segment inherits the label of the clip
containing it, but music emotion is not constant during the whole clip. Doing
so will introduce label noise and make the training easy to overfit. To handle
the noisy label issue, we propose a semi-supervised self-learning (SSSL)
method, which can differentiate between samples with correct and incorrect
labels in a self-learning manner, thus effectively utilizing the augmented
segment-level data. Experiments on three public emotional datasets demonstrate
that the proposed method can achieve better or comparable performance.

</details>


### [206] [WhisperFlow: speech foundation models in real time](https://arxiv.org/abs/2412.11272)
*Rongxiang Wang,Zhiming Xu,Felix Xiaozhu Lin*

Main category: cs.SD

TL;DR: WhisperFlow introduces optimizations for streaming speech processing, reducing latency and resource usage while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current speech foundation models like Whisper are inefficient for streaming speech due to fixed-length inputs, high token encoding costs, and complex decoding.

Method: WhisperFlow uses hush words, beam pruning, and CPU/GPU pipelining to optimize streaming speech processing.

Result: WhisperFlow reduces per-word latency by 1.6x-4.7x and maintains accuracy, even on low-power devices like a MacBook Air.

Conclusion: WhisperFlow effectively addresses inefficiencies in streaming speech processing, enabling efficient deployment on resource-constrained devices.

Abstract: Speech foundation models, such as OpenAI's Whisper, become the state of the
art in speech understanding due to their strong accuracy and generalizability.
Yet, their applications are mostly limited to processing pre-recorded speech,
whereas processing of streaming speech, in particular doing it efficiently,
remains rudimentary. Behind this inefficiency are multiple fundamental reasons:
(1) speech foundation models are trained to process long, fixed-length voice
inputs (often 30 seconds); (2) encoding each voice input requires encoding as
many as 1,500 tokens with tens of transformer layers; (3) decoding each output
entails an irregular, complex beam search. As such, streaming speech processing
on resource-constrained client devices is more expensive than other AI tasks,
e.g., text generation.
  To this end, we present a novel framework, WhisperFlow, which embodies both
model and system optimizations. (1) Hush word as a short, learnable audio
segment; appended to a voice input, a hush word gracefully stops the speech
model from processing more input without hallucination; (2) Beam pruning, which
aligns streaming audio buffers over time and reuses results from earlier
decoding rounds, therefore significantly accelerating decoding; and (3) CPU/GPU
pipelining, which not only maps to the encoding/decoding stages dynamically,
but also tunes to an optimal resource ratio, respecting the encoding/decoding
speed that varies across voice inputs, models, and hardware.
  We test WhisperFlow on commodity ARM platforms with 4-12 CPU cores and 10-30
GPU cores. It reduces per-word latency by 1.6x-4.7x to as low as 0.5 second,
while seeing negligible accuracy degradation. On an entry-level MacBook Air,
WhisperFlow can keep the per-word latency around 1 second, with the whole
device drawing only 7 Watts in total.

</details>


### [207] [F5R-TTS: Improving Flow-Matching based Text-to-Speech with Group Relative Policy Optimization](https://arxiv.org/abs/2504.02407)
*Xiaohui Sun,Ruitong Xiao,Jianye Mo,Bowen Wu,Qun Yu,Baoxun Wang*

Main category: cs.SD

TL;DR: F5R-TTS integrates GRPO into flow-matching TTS, improving speech intelligibility and speaker similarity via dual RL rewards.


<details>
  <summary>Details</summary>
Motivation: To enhance flow-matching TTS by integrating reinforcement learning for better intelligibility and speaker similarity.

Method: Combines probabilistic reformulation of flow-matching with GRPO-driven RL, using WER and SIM as rewards.

Result: Achieves 29.5% WER reduction and 4.6% SIM score increase in zero-shot voice cloning.

Conclusion: F5R-TTS outperforms conventional flow-matching TTS, demonstrating RL's effectiveness in TTS enhancement.

Abstract: We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Group
Relative Policy Optimization (GRPO) into a flow-matching based architecture. By
reformulating the deterministic outputs of flow-matching TTS into probabilistic
Gaussian distributions, our approach enables seamless integration of
reinforcement learning algorithms. During pretraining, we train a
probabilistically reformulated flow-matching based model which is derived from
F5-TTS with an open-source dataset. In the subsequent reinforcement learning
(RL) phase, we employ a GRPO-driven enhancement stage that leverages dual
reward metrics: word error rate (WER) computed via automatic speech recognition
and speaker similarity (SIM) assessed by verification models. Experimental
results on zero-shot voice cloning demonstrate that F5R-TTS achieves
significant improvements in both speech intelligibility (a 29.5% relative
reduction in WER) and speaker similarity (a 4.6% relative increase in SIM
score) compared to conventional flow-matching based TTS systems. Audio samples
are available at https://frontierlabs.github.io/F5R.

</details>


### [208] [Location-Oriented Sound Event Localization and Detection with Spatial Mapping and Regression Localization](https://arxiv.org/abs/2504.08365)
*Xueping Zhang,Yaxiong Chen,Ruilin Yao,Yunfei Zi,Shengwu Xiong*

Main category: cs.SD

TL;DR: SMRL-SELD improves SELD by segmenting 3D space into 2D and using regression loss for better localization in polyphonic environments.


<details>
  <summary>Details</summary>
Motivation: Existing multi-track methods limit generality in polyphonic settings due to track number constraints.

Method: SMRL-SELD segments 3D space into 2D and introduces a regression localization loss for event-oriented learning.

Result: Outperforms existing SELD methods on STARSS23 and STARSS22 datasets, especially in polyphony.

Conclusion: SMRL-SELD enhances generality and performance in polyphonic sound environments.

Abstract: Sound Event Localization and Detection (SELD) combines the Sound Event
Detection (SED) with the corresponding Direction Of Arrival (DOA). Recently,
adopted event oriented multi-track methods affect the generality in polyphonic
environments due to the limitation of the number of tracks. To enhance the
generality in polyphonic environments, we propose Spatial Mapping and
Regression Localization for SELD (SMRL-SELD). SMRL-SELD segments the 3D spatial
space, mapping it to a 2D plane, and a new regression localization loss is
proposed to help the results converge toward the location of the corresponding
event. SMRL-SELD is location-oriented, allowing the model to learn event
features based on orientation. Thus, the method enables the model to process
polyphonic sounds regardless of the number of overlapping events. We conducted
experiments on STARSS23 and STARSS22 datasets and our proposed SMRL-SELD
outperforms the existing SELD methods in overall evaluation and polyphony
environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [209] [Collaborative Learning of On-Device Small Model and Cloud-Based Large Model: Advances and Future Directions](https://arxiv.org/abs/2504.15300)
*Chaoyue Niu,Yucheng Ding,Junhui Lu,Zhengxiang Huang,Hang Zeng,Yutong Dai,Xuezhen Tu,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: The paper surveys collaborative learning between on-device small models and cloud-based large models, addressing latency, cost, personalization, and privacy concerns. It reviews hardware, system, algorithm, and application layers, categorizing collaboration algorithms and highlighting real-world deployments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional cloud-based large model learning, such as latency, cost, personalization, and privacy issues, by exploring collaborative learning between on-device small models and cloud-based large models.

Method: The paper provides a comprehensive review across hardware, system, algorithm, and application layers, categorizing collaboration algorithms into data-based, feature-based, and parameter-based frameworks. It also reviews datasets and evaluation metrics tailored to collaborative learning.

Result: The survey highlights real-world deployments in recommender systems, mobile livestreaming, and personal intelligent assistants, showcasing the effectiveness of the collaborative learning paradigm.

Conclusion: The paper identifies open research directions to guide future development in collaborative learning, emphasizing its potential for low-latency, cost-efficient, and privacy-preserving intelligent services.

Abstract: The conventional cloud-based large model learning framework is increasingly
constrained by latency, cost, personalization, and privacy concerns. In this
survey, we explore an emerging paradigm: collaborative learning between
on-device small model and cloud-based large model, which promises low-latency,
cost-efficient, and personalized intelligent services while preserving user
privacy. We provide a comprehensive review across hardware, system, algorithm,
and application layers. At each layer, we summarize key problems and recent
advances from both academia and industry. In particular, we categorize
collaboration algorithms into data-based, feature-based, and parameter-based
frameworks. We also review publicly available datasets and evaluation metrics
with user-level or device-level consideration tailored to collaborative
learning settings. We further highlight real-world deployments, ranging from
recommender systems and mobile livestreaming to personal intelligent
assistants. We finally point out open research directions to guide future
development in this rapidly evolving field.

</details>


### [210] [Power Transformer Health Index and Life Span Assessment: A Comprehensive Review of Conventional and Machine Learning based Approaches](https://arxiv.org/abs/2504.15310)
*Syeda Tahreem Zahra,Syed Kashif Imdad,Sohail Khan,Sohail Khalid,Nauman Anwar Baig*

Main category: cs.LG

TL;DR: The paper reviews AI-based techniques for transformer fault diagnosis, analyzing their pros and cons, and explores combining methods for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance transformer health assessment and lifespan prediction for efficient operation and maintenance planning.

Method: Examines conventional and advanced techniques, focusing on AI algorithms like ANN, CNN, SVM, RF, GA, and PSO, and explores timeseries analysis.

Result: Identifies strengths and weaknesses of AI methods and demonstrates their potential for precise fault diagnosis and early detection.

Conclusion: Provides a foundation for future research in AI applications for transformer fault diagnosis.

Abstract: Power transformers play a critical role within the electrical power system,
making their health assessment and the prediction of their remaining lifespan
paramount for the purpose of ensuring efficient operation and facilitating
effective maintenance planning. This paper undertakes a comprehensive
examination of existent literature, with a primary focus on both conventional
and cutting-edge techniques employed within this domain. The merits and
demerits of recent methodologies and techniques are subjected to meticulous
scrutiny and explication. Furthermore, this paper expounds upon intelligent
fault diagnosis methodologies and delves into the most widely utilized
intelligent algorithms for the assessment of transformer conditions. Diverse
Artificial Intelligence (AI) approaches, including Artificial Neural Networks
(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),
Random Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization
(PSO), are elucidated offering pragmatic solutions for enhancing the
performance of transformer fault diagnosis. The amalgamation of multiple AI
methodologies and the exploration of timeseries analysis further contribute to
the augmentation of diagnostic precision and the early detection of faults in
transformers. By furnishing a comprehensive panorama of AI applications in the
field of transformer fault diagnosis, this study lays the groundwork for future
research endeavors and the progression of this critical area of study.

</details>


### [211] [M-TabNet: A Multi-Encoder Transformer Model for Predicting Neonatal Birth Weight from Multimodal Data](https://arxiv.org/abs/2504.15312)
*Muhammad Mursil,Hatem A. Rashwan,Luis Santos-Calderon,Pere Cavalle-Busquets,Michelle M. Murphy,Domenec Puig*

Main category: cs.LG

TL;DR: An attention-based transformer model integrates diverse maternal data for early birth weight prediction, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Early prediction of birth weight is crucial for neonatal health, but current methods like ultrasonography have limitations in accuracy and scope. Existing models often overlook nutritional and genetic factors.

Method: The study uses an attention-based transformer model with a multi-encoder architecture to integrate physiological, lifestyle, nutritional, and genetic data for early prediction (less than 12 weeks of gestation).

Result: The model achieves high predictive accuracy (MAE: 122 grams, R-squared: 0.94) and generalizability (MAE: 105 grams, R-squared: 0.95). It also performs well in classifying low and normal birth weight (sensitivity: 97.55%, specificity: 94.48%).

Conclusion: The model demonstrates the potential of deep learning for early birth weight prediction, offering a robust, interpretable tool for clinicians to improve neonatal outcomes.

Abstract: Birth weight (BW) is a key indicator of neonatal health, with low birth
weight (LBW) linked to increased mortality and morbidity. Early prediction of
BW enables timely interventions; however, current methods like ultrasonography
have limitations, including reduced accuracy before 20 weeks and operator
dependent variability. Existing models often neglect nutritional and genetic
influences, focusing mainly on physiological and lifestyle factors. This study
presents an attention-based transformer model with a multi-encoder architecture
for early (less than 12 weeks of gestation) BW prediction. Our model
effectively integrates diverse maternal data such as physiological, lifestyle,
nutritional, and genetic, addressing limitations seen in prior attention-based
models such as TabNet. The model achieves a Mean Absolute Error (MAE) of 122
grams and an R-squared value of 0.94, demonstrating high predictive accuracy
and interoperability with our in-house private dataset. Independent validation
confirms generalizability (MAE: 105 grams, R-squared: 0.95) with the IEEE
children dataset. To enhance clinical utility, predicted BW is classified into
low and normal categories, achieving a sensitivity of 97.55% and a specificity
of 94.48%, facilitating early risk stratification. Model interpretability is
reinforced through feature importance and SHAP analyses, highlighting
significant influences of maternal age, tobacco exposure, and vitamin B12
status, with genetic factors playing a secondary role. Our results emphasize
the potential of advanced deep-learning models to improve early BW prediction,
offering clinicians a robust, interpretable, and personalized tool for
identifying pregnancies at risk and optimizing neonatal outcomes.

</details>


### [212] [Diffusion-Driven Inertial Generated Data for Smartphone Location Classification](https://arxiv.org/abs/2504.15315)
*Noa Cohen,Rotem Dror,Itzik Klein*

Main category: cs.LG

TL;DR: A diffusion-based generative model is proposed to create synthetic specific force data for smartphone location recognition, reducing the need for extensive real data collection.


<details>
  <summary>Details</summary>
Motivation: The time and resource constraints of collecting large inertial datasets hinder robust machine learning model development in motion tracking and navigation.

Method: Uses diffusion models to generate synthetic specific force data, comparing it with real data across multiple metrics.

Result: The model captures distinctive characteristics of specific force signals under various smartphone placements, producing realistic synthetic data.

Conclusion: Diffusion-driven synthetic data can alleviate data collection burdens while providing high-quality training data for machine learning.

Abstract: Despite the crucial role of inertial measurements in motion tracking and
navigation systems, the time-consuming and resource-intensive nature of
collecting extensive inertial data has hindered the development of robust
machine learning models in this field. In recent years, diffusion models have
emerged as a revolutionary class of generative models, reshaping the landscape
of artificial data generation. These models surpass generative adversarial
networks and other state-of-the-art approaches to complex tasks. In this work,
we propose diffusion-driven specific force-generated data for smartphone
location recognition. We provide a comprehensive evaluation methodology by
comparing synthetic and real recorded specific force data across multiple
metrics. Our results demonstrate that our diffusion-based generative model
successfully captures the distinctive characteristics of specific force signals
across different smartphone placement conditions. Thus, by creating diverse,
realistic synthetic data, we can reduce the burden of extensive data collection
while providing high-quality training data for machine learning models.

</details>


### [213] [How to systematically develop an effective AI-based bias correction model?](https://arxiv.org/abs/2504.15322)
*Xiao Zhou,Yuze Sun,Jie Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: ReSA-ConvLSTM is an AI framework for bias correction in weather prediction, integrating dynamic normalization, ConvLSTM, and self-attention. It reduces biases in forecasts by 20% RMSE and improves efficiency.


<details>
  <summary>Details</summary>
Motivation: To address systematic biases in numerical weather prediction (NWP) by developing a physics-aware AI framework.

Method: Combines dynamic climatological normalization, ConvLSTM with temporal causality, and residual self-attention to map ECMWF forecasts to ERA5 reanalysis data.

Result: Achieves up to 20% RMSE reduction in 1-7 day forecasts for T2m, U10/V10, and SLP, with efficient generalization and reduced retraining time.

Conclusion: The innovations significantly improve correction performance, suggesting variable-specific modeling enhances forecasting accuracy.

Abstract: This study introduces ReSA-ConvLSTM, an artificial intelligence (AI)
framework for systematic bias correction in numerical weather prediction (NWP).
We propose three innovations by integrating dynamic climatological
normalization, ConvLSTM with temporal causality constraints, and residual
self-attention mechanisms. The model establishes a physics-aware nonlinear
mapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years
(1981-2021) of global atmospheric data, the framework reduces systematic biases
in 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure
(SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to
operational ECMWF outputs. The lightweight architecture (10.6M parameters)
enables efficient generalization to multiple variables and downstream
applications, reducing retraining time by 85% for cross-variable correction
while improving ocean model skill through bias-corrected boundary conditions.
The ablation experiments demonstrate that our innovations significantly improve
the model's correction performance, suggesting that incorporating variable
characteristics into the model helps enhance forecasting skills.

</details>


### [214] [HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning](https://arxiv.org/abs/2504.15323)
*Donggyun Kim,Chanwoo Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: Proposes a gradient-free test-time adaptation method for few-shot learning, reducing computational costs significantly.


<details>
  <summary>Details</summary>
Motivation: Addresses the inefficiency of multiple backpropagation steps in test-time fine-tuning for real-time or low-resource scenarios.

Method: Formulates gradient descent as an ODE discretization, using an auxiliary network to predict task-conditional drift, enabling adaptation via numerical integration.

Result: Improves out-of-domain performance with 6% memory cost and 0.02% computation time of standard fine-tuning.

Conclusion: Offers a practical middle ground between direct transfer and full fine-tuning, balancing efficiency and performance.

Abstract: While test-time fine-tuning is beneficial in few-shot learning, the need for
multiple backpropagation steps can be prohibitively expensive in real-time or
low-resource scenarios. To address this limitation, we propose an approach that
emulates gradient descent without computing gradients, enabling efficient
test-time adaptation. Specifically, we formulate gradient descent as an Euler
discretization of an ordinary differential equation (ODE) and train an
auxiliary network to predict the task-conditional drift using only the few-shot
support set. The adaptation then reduces to a simple numerical integration
(e.g., via the Euler method), which requires only a few forward passes of the
auxiliary network -- no gradients or forward passes of the target model are
needed. In experiments on cross-domain few-shot classification using the
Meta-Dataset and CDFSL benchmarks, our method significantly improves
out-of-domain performance over the non-fine-tuned baseline while incurring only
6\% of the memory cost and 0.02\% of the computation time of standard
fine-tuning, thus establishing a practical middle ground between direct
transfer and fully fine-tuned approaches.

</details>


### [215] [On-Device Federated Continual Learning on RISC-V-based Ultra-Low-Power SoC for Intelligent Nano-Drone Swarms](https://arxiv.org/abs/2503.17436)
*Lars Kröger,Cristian Cioflan,Victor Kartsch,Luca Benini*

Main category: cs.LG

TL;DR: A RISC-V-based architecture enables efficient On-Device Learning (ODL) for smart edge devices, addressing challenges like computational constraints and catastrophic forgetting with a novel federated continual learning algorithm.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of constrained resources and data privacy in battery-operated embedded platforms for ODL, particularly in intelligent sensor networks.

Method: Proposed a regularization-based On-Device Federated Continual Learning algorithm for nano-drones performing face recognition, implemented on a RISC-V-based 10-core ultra-low-power SoC.

Result: Achieved a 24% improvement in classification accuracy over naive fine-tuning, with 178 ms per local epoch and 10.5 s per global epoch.

Conclusion: The RISC-V-based architecture and proposed algorithm effectively optimize ODL for resource-constrained devices, demonstrating practical viability.

Abstract: RISC-V-based architectures are paving the way for efficient On-Device
Learning (ODL) in smart edge devices. When applied across multiple nodes, ODL
enables the creation of intelligent sensor networks that preserve data privacy.
However, developing ODL-capable, battery-operated embedded platforms presents
significant challenges due to constrained computational resources and limited
device lifetime, besides intrinsic learning issues such as catastrophic
forgetting. We face these challenges by proposing a regularization-based
On-Device Federated Continual Learning algorithm tailored for multiple
nano-drones performing face recognition tasks. We demonstrate our approach on a
RISC-V-based 10-core ultra-low-power SoC, optimizing the ODL computational
requirements. We improve the classification accuracy by 24% over naive
fine-tuning, requiring 178 ms per local epoch and 10.5 s per global epoch,
demonstrating the effectiveness of the architecture for this task.

</details>


### [216] [Significativity Indices for Agreement Values](https://arxiv.org/abs/2504.15325)
*Alberto Casagrande,Francesco Fabris,Rossano Girometti,Roberto Pagliarini*

Main category: cs.LG

TL;DR: The paper proposes a general method to evaluate the significance of agreement measures between classifiers, introducing two new indices and addressing computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current agreement measures lack robust scales or significance indices, leading to arbitrary evaluations of classifier performance.

Method: Introduces two significativity indices for finite data sets and classification probability distributions, along with efficient computational algorithms.

Result: Provides a framework to assess the significance of agreement values, improving the evaluation of classifier consistency.

Conclusion: The proposed indices and computational methods enhance the reliability of agreement measure evaluations in various fields.

Abstract: Agreement measures, such as Cohen's kappa or intraclass correlation, gauge
the matching between two or more classifiers. They are used in a wide range of
contexts from medicine, where they evaluate the effectiveness of medical
treatments and clinical trials, to artificial intelligence, where they can
quantify the approximation due to the reduction of a classifier. The
consistency of different classifiers to a golden standard can be compared
simply by using the order induced by their agreement measure with respect to
the golden standard itself. Nevertheless, labelling an approach as good or bad
exclusively by using the value of an agreement measure requires a scale or a
significativity index. Some quality scales have been proposed in the literature
for Cohen's kappa, but they are mainly naive, and their boundaries are
arbitrary. This work proposes a general approach to evaluate the
significativity of any agreement value between two classifiers and introduces
two significativity indices: one dealing with finite data sets, the other one
handling classification probability distributions. Moreover, this manuscript
considers the computational issues of evaluating such indices and identifies
some efficient algorithms to evaluate them.

</details>


### [217] [Histogram-based Parameter-efficient Tuning for Passive Sonar Classification](https://arxiv.org/abs/2504.15214)
*Amirmohammad Mohammadi,Davelle Carreiro,Alexandra Van Dine,Joshua Peeples*

Main category: cs.LG

TL;DR: HPT, a histogram-based parameter-efficient tuning method, outperforms adapters in accuracy and training speed for passive sonar tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of additive PETL methods like adapters in capturing distributional shifts in feature embeddings.

Method: Proposes HPT to capture target domain statistics and modulate embeddings.

Result: HPT achieves higher accuracy (91.8% vs. 89.8%) on VTUAD and trains faster than adapters.

Conclusion: HPT balances parameter efficiency and performance, offering a scalable transfer learning solution for resource-constrained settings.

Abstract: Parameter-efficient transfer learning (PETL) methods adapt large artificial
neural networks to downstream tasks without fine-tuning the entire model.
However, existing additive methods, such as adapters, sometimes struggle to
capture distributional shifts in intermediate feature embeddings. We propose a
novel histogram-based parameter-efficient tuning (HPT) technique that captures
the statistics of the target domain and modulates the embeddings. Experimental
results on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)
demonstrate that HPT outperforms conventional adapters. Notably, HPT achieves
91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields
feature representations closer to those of fully fine-tuned models. Overall,
HPT balances parameter savings and performance, providing a distribution-aware
alternative to existing adapters and shows a promising direction for scalable
transfer learning in resource-constrained environments. The code is publicly
available:
https://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.

</details>


### [218] [Bayesian Federated Learning for Continual Training](https://arxiv.org/abs/2504.15328)
*Usevalad Milasheuski,Luca Barbieri,Sanaz Kianoush,Monica Nicoli,Stefano Savazzi*

Main category: cs.LG

TL;DR: A continual Bayesian Federated Learning (BFL) framework is proposed for dynamic environments, using SGLD to update models sequentially, outperforming baselines in accuracy, calibration, and convergence.


<details>
  <summary>Details</summary>
Motivation: Current BFL methods lack adaptability to shifting data distributions in dynamic environments, limiting their practical utility.

Method: The framework employs Stochastic Gradient Langevin Dynamics (SGLD) to sequentially update models, using past posteriors as priors for new tasks.

Result: The approach demonstrates superior accuracy, lower expected calibration error (ECE), and faster convergence compared to baselines.

Conclusion: Continual Bayesian updates effectively preserve knowledge and adapt to evolving data, enhancing BFL's robustness in dynamic settings.

Abstract: Bayesian Federated Learning (BFL) enables uncertainty quantification and
robust adaptation in distributed learning. In contrast to the frequentist
approach, it estimates the posterior distribution of a global model, offering
insights into model reliability. However, current BFL methods neglect continual
learning challenges in dynamic environments where data distributions shift over
time. We propose a continual BFL framework applied to human sensing with radar
data collected over several days. Using Stochastic Gradient Langevin Dynamics
(SGLD), our approach sequentially updates the model, leveraging past posteriors
to construct the prior for the new tasks. We assess the accuracy, the expected
calibration error (ECE) and the convergence speed of our approach against
several baselines. Results highlight the effectiveness of continual Bayesian
updates in preserving knowledge and adapting to evolving data.

</details>


### [219] [FedFetch: Faster Federated Learning with Adaptive Downstream Prefetching](https://arxiv.org/abs/2504.15366)
*Qifan Yan,Andrew Liu,Shiqi He,Mathias Lécuyer,Ivan Beschastnikh*

Main category: cs.LG

TL;DR: FedFetch reduces download time in federated learning by prefetching model states, improving training efficiency.


<details>
  <summary>Details</summary>
Motivation: Address communication bottlenecks in FL caused by heterogeneous clients and stragglers, especially when combining client sampling and compression.

Method: Introduces FedFetch, a prefetch schedule for clients to synchronize model states ahead of training rounds.

Result: Reduces end-to-end training time by 1.26× and download time by 4.49× in heterogeneous settings.

Conclusion: FedFetch effectively mitigates download overhead, enhancing FL efficiency when combined with existing techniques.

Abstract: Federated learning (FL) is a machine learning paradigm that facilitates
massively distributed model training with end-user data on edge devices
directed by a central server. However, the large number of heterogeneous
clients in FL deployments leads to a communication bottleneck between the
server and the clients. This bottleneck is made worse by straggling clients,
any one of which will further slow down training. To tackle these challenges,
researchers have proposed techniques like client sampling and update
compression. These techniques work well in isolation but combine poorly in the
downstream, server-to-client direction. This is because unselected clients have
outdated local model states and need to synchronize these states with the
server first.
  We introduce FedFetch, a strategy to mitigate the download time overhead
caused by combining client sampling and compression techniques. FedFetch
achieves this with an efficient prefetch schedule for clients to prefetch model
states multiple rounds before a stated training round. We empirically show that
adding FedFetch to communication efficient FL techniques reduces end-to-end
training time by 1.26$\times$ and download time by 4.49$\times$ across
compression techniques with heterogeneous client settings. Our implementation
is available at https://github.com/DistributedML/FedFetch

</details>


### [220] [Solving New Tasks by Adapting Internet Video Knowledge](https://arxiv.org/abs/2504.15369)
*Calvin Luo,Zilai Zeng,Yilun Du,Chen Sun*

Main category: cs.LG

TL;DR: The paper explores adaptation techniques to combine in-domain robotic data with large-scale pretrained video models for text-conditioned generalization in robotics.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of pretrained video models (lack of environment-specific details) and in-domain models (limited generalization), the study aims to integrate both for better robotic task performance.

Method: Investigates various adaptation techniques, including a novel Inverse Probabilistic Adaptation, to merge in-domain data with pretrained models.

Result: Demonstrates successful generalization to novel tasks using small-scale in-domain data, with the proposed method showing robustness to data quality.

Conclusion: Adapting large-scale video models with in-domain data enables effective generalization in robotics, even with suboptimal demonstrations.

Abstract: Video generative models demonstrate great promise in robotics by serving as
visual planners or as policy supervisors. When pretrained on internet-scale
data, such video models intimately understand alignment with natural language,
and can thus facilitate generalization to novel downstream behavior through
text-conditioning. However, they may not be sensitive to the specificities of
the particular environment the agent inhabits. On the other hand, training
video models on in-domain examples of robotic behavior naturally encodes
environment-specific intricacies, but the scale of available demonstrations may
not be sufficient to support generalization to unseen tasks via natural
language specification. In this work, we investigate different adaptation
techniques that integrate in-domain information with large-scale pretrained
video models, and explore the extent to which they enable novel
text-conditioned generalization for robotic tasks, while also considering their
independent data and resource considerations. We successfully demonstrate
across robotic environments that adapting powerful video models with small
scales of example data can successfully facilitate generalization to novel
behaviors. In particular, we present a novel adaptation strategy, termed
Inverse Probabilistic Adaptation, that not only consistently achieves strong
generalization performance across robotic tasks and settings, but also exhibits
robustness to the quality of adaptation data, successfully solving novel tasks
even when only suboptimal in-domain demonstrations are available.

</details>


### [221] [Improving Learning to Optimize Using Parameter Symmetries](https://arxiv.org/abs/2504.15399)
*Guy Zamir,Aryan Dokania,Bo Zhao,Rose Yu*

Main category: cs.LG

TL;DR: The paper explores a learning-to-optimize (L2O) algorithm that uses parameter space symmetry to improve optimization efficiency, showing theoretical and empirical benefits.


<details>
  <summary>Details</summary>
Motivation: To enhance meta-optimizer performance by leveraging symmetry transformations in parameter space.

Method: Jointly learning symmetry transformations and local updates, with theoretical analysis and empirical benchmarking.

Result: The algorithm locally resembles Newton's method and can learn correct symmetry transformations, with momentum further boosting performance.

Conclusion: Leveraging parameter space symmetry holds promise for advancing meta-optimization techniques.

Abstract: We analyze a learning-to-optimize (L2O) algorithm that exploits parameter
space symmetry to enhance optimization efficiency. Prior work has shown that
jointly learning symmetry transformations and local updates improves
meta-optimizer performance. Supporting this, our theoretical analysis
demonstrates that even without identifying the optimal group element, the
method locally resembles Newton's method. We further provide an example where
the algorithm provably learns the correct symmetry transformation during
training. To empirically evaluate L2O with teleportation, we introduce a
benchmark, analyze its success and failure cases, and show that enhancements
like momentum further improve performance. Our results highlight the potential
of leveraging neural network parameter space symmetry to advance
meta-optimization.

</details>


### [222] [Combating Toxic Language: A Review of LLM-Based Strategies for Software Engineering](https://arxiv.org/abs/2504.15439)
*Hao Zhuo,Yicheng Yang,Kewen Peng*

Main category: cs.LG

TL;DR: This paper reviews toxicity detection and mitigation in LLMs for software engineering, analyzing methods, datasets, and strategies, and identifies future research needs.


<details>
  <summary>Details</summary>
Motivation: The widespread use of LLMs in SE raises concerns about toxic language, necessitating a review of detection and mitigation efforts.

Method: The paper examines annotation techniques, preprocessing, detection methodologies, and mitigation strategies, including an ablation study on LLM-based rewriting.

Result: The review synthesizes existing work, showing LLM-based rewriting's effectiveness in reducing toxicity.

Conclusion: Future research is needed to responsibly deploy LLMs in SE, addressing open challenges in toxicity management.

Abstract: Large Language Models (LLMs) have become integral to software engineering
(SE), where they are increasingly used in development workflows. However, their
widespread use raises concerns about the presence and propagation of toxic
language--harmful or offensive content that can foster exclusionary
environments. This paper provides a comprehensive review of recent research on
toxicity detection and mitigation, focusing on both SE-specific and
general-purpose datasets. We examine annotation and preprocessing techniques,
assess detection methodologies, and evaluate mitigation strategies,
particularly those leveraging LLMs. Additionally, we conduct an ablation study
demonstrating the effectiveness of LLM-based rewriting for reducing toxicity.
By synthesizing existing work and identifying open challenges, this review
highlights key areas for future research to ensure the responsible deployment
of LLMs in SE and beyond.

</details>


### [223] [Compton Form Factor Extraction using Quantum Deep Neural Networks](https://arxiv.org/abs/2504.15458)
*Brandon Le,Dustin Keller*

Main category: cs.LG

TL;DR: Extraction of Compton Form Factors using CDNNs and QDNNs shows QDNNs outperform CDNNs in accuracy and precision.


<details>
  <summary>Details</summary>
Motivation: To improve the extraction of Compton Form Factors from experimental data using advanced neural network techniques.

Method: Employed twist-two formalism and fitting procedures with CDNNs and QDNNs for extraction.

Result: QDNNs showed better predictive accuracy and precision than CDNNs.

Conclusion: QDNNs hold promise for future quantum-optimized studies in this field.

Abstract: Extraction tests of Compton Form Factors are performed using pseudodata based
on experimental data from Deeply Virtual Compton Scattering experiments
conducted at Jefferson Lab. The standard Belitsky, Kirchner, and Muller
formalism at twist-two is employed, along with a fitting procedure designed to
reduce model dependency similar to traditional local fits. The extraction of
the Compton Form Factors is performed using both Classical Deep Neural Networks
(CDNNs) and Quantum Deep Neural Networks (QDNNs). Comparative studies reveal
that QDNNs outperform CDNNs for this application, demonstrating improved
predictive accuracy and precision even for limited model complexity. The
results demonstrate the potential of QDNNs for future studies in which quantum
algorithms can be fully optimized.

</details>


### [224] [In-context Ranking Preference Optimization](https://arxiv.org/abs/2504.15477)
*Junda Wu,Rohan Surana,Zhouhang Xie,Yiran Shen,Yu Xia,Tong Yu,Ryan A. Rossi,Prithviraj Ammanabrolu,Julian McAuley*

Main category: cs.LG

TL;DR: IRPO extends DPO by optimizing LLMs for in-context ranking preferences, incorporating item relevance and position, and outperforms DPO in ranking tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited and sparse pairwise feedback in in-context settings, and the need for flexible user feedback in complex tasks like conversational agents and summarization.

Method: Proposes IRPO, a framework that optimizes LLMs using ranking lists during inference, with a differentiable objective based on positional aggregation of pairwise preferences.

Result: IRPO outperforms standard DPO in ranking performance and aligns LLMs better with in-context ranking preferences.

Conclusion: IRPO effectively addresses the limitations of DPO by incorporating flexible feedback and optimizing discrete ranking metrics, demonstrating superior performance.

Abstract: Recent developments in Direct Preference Optimization (DPO) allow large
language models (LLMs) to function as implicit ranking models by maximizing the
margin between preferred and non-preferred responses. In practice, user
feedback on such lists typically involves identifying a few relevant items in
context rather than providing detailed pairwise comparisons for every possible
item pair. Moreover, many complex information retrieval tasks, such as
conversational agents and summarization systems, critically depend on ranking
the highest-quality outputs at the top, emphasizing the need to support natural
and flexible forms of user feedback. To address the challenge of limited and
sparse pairwise feedback in the in-context setting, we propose an In-context
Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs
based on ranking lists constructed during inference. To further capture
flexible forms of feedback, IRPO extends the DPO objective by incorporating
both the relevance of items and their positions in the list. Modeling these
aspects jointly is non-trivial, as ranking metrics are inherently discrete and
non-differentiable, making direct optimization difficult. To overcome this,
IRPO introduces a differentiable objective based on positional aggregation of
pairwise item preferences, enabling effective gradient-based optimization of
discrete ranking metrics. We further provide theoretical insights showing that
IRPO (i) automatically emphasizes items with greater disagreement between the
model and the reference ranking, and (ii) links its gradient to an importance
sampling estimator, yielding an unbiased estimator with reduced variance.
Empirical results show IRPO outperforms standard DPO approaches in ranking
performance, highlighting its effectiveness in aligning LLMs with direct
in-context ranking preferences.

</details>


### [225] [Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks](https://arxiv.org/abs/2504.15479)
*Jeremy Goldwasser,Giles Hooker*

Main category: cs.LG

TL;DR: A new framework for generating counterfactual images in computer vision, avoiding adversarial issues and integrating feature attribution for interpretability.


<details>
  <summary>Details</summary>
Motivation: Standard gradient-based methods for counterfactuals in vision models often produce adversarial examples, limiting their interpretability.

Method: Introduces Counterfactual Attacks, a method leveraging low-dimensional manifolds and auxiliary datasets for feature attribution.

Result: Demonstrates efficacy on MNIST and CelebA datasets, providing computationally efficient global counterfactual explanations.

Conclusion: The framework adapts to generative modeling advances, offering interpretable and efficient counterfactual explanations for vision models.

Abstract: Counterfactuals are a popular framework for interpreting machine learning
predictions. These what if explanations are notoriously challenging to create
for computer vision models: standard gradient-based methods are prone to
produce adversarial examples, in which imperceptible modifications to image
pixels provoke large changes in predictions. We introduce a new,
easy-to-implement framework for counterfactual images that can flexibly adapt
to contemporary advances in generative modeling. Our method, Counterfactual
Attacks, resembles an adversarial attack on the representation of the image
along a low-dimensional manifold. In addition, given an auxiliary dataset of
image descriptors, we show how to accompany counterfactuals with feature
attribution that quantify the changes between the original and counterfactual
images. These importance scores can be aggregated into global counterfactual
explanations that highlight the overall features driving model predictions.
While this unification is possible for any counterfactual method, it has
particular computational efficiency for ours. We demonstrate the efficacy of
our approach with the MNIST and CelebA datasets.

</details>


### [226] [Fourier analysis of the physics of transfer learning for data-driven subgrid-scale models of ocean turbulence](https://arxiv.org/abs/2504.15487)
*Moein Darman,Pedram Hassanzadeh,Laure Zanna,Ashesh Chattopadhyay*

Main category: cs.LG

TL;DR: A study uses transfer learning (TL) with a 9-layer CNN to predict subgrid forcing in ocean dynamics, identifying metrics for performance and generalizability. Fourier analysis shows learned filters, and TL corrects spectral underestimation by retraining one layer.


<details>
  <summary>Details</summary>
Motivation: To enhance neural network performance in weather and climate prediction by leveraging TL for generalizability to unseen dynamical regimes.

Method: Employ a 9-layer CNN to predict subgrid forcing in a two-layer ocean quasi-geostrophic system, analyzing kernels and activation spectra.

Result: NNs learn low-pass, Gabor, and high-pass filters. Without TL, they underestimate output spectra; TL corrects this by retraining one layer.

Conclusion: TL effectively improves NN generalizability in dynamical systems, with findings applicable to data-driven parameterization.

Abstract: Transfer learning (TL) is a powerful tool for enhancing the performance of
neural networks (NNs) in applications such as weather and climate prediction
and turbulence modeling. TL enables models to generalize to out-of-distribution
data with minimal training data from the new system. In this study, we employ a
9-layer convolutional NN to predict the subgrid forcing in a two-layer ocean
quasi-geostrophic system and examine which metrics best describe its
performance and generalizability to unseen dynamical regimes. Fourier analysis
of the NN kernels reveals that they learn low-pass, Gabor, and high-pass
filters, regardless of whether the training data are isotropic or anisotropic.
By analyzing the activation spectra, we identify why NNs fail to generalize
without TL and how TL can overcome these limitations: the learned weights and
biases from one dataset underestimate the out-of-distribution sample spectra as
they pass through the network, leading to an underestimation of output spectra.
By re-training only one layer with data from the target system, this
underestimation is corrected, enabling the NN to produce predictions that match
the target spectra. These findings are broadly applicable to data-driven
parameterization of dynamical systems.

</details>


### [227] [Application of Deep Generative Models for Anomaly Detection in Complex Financial Transactions](https://arxiv.org/abs/2504.15491)
*Tengda Tang,Jianhua Yao,Yixian Wang,Qiuwu Sha,Hanrui Feng,Zhen Xu*

Main category: cs.LG

TL;DR: The paper proposes a deep generative model combining GAN and VAE to detect suspicious behaviors in financial transactions, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To improve detection of fraud and money laundering in large payment flows by leveraging generative models for better accuracy and handling sparse data.

Method: Combines GAN for simulating normal payment flows and VAE for latent distribution modeling to detect anomalies.

Result: The method outperforms traditional machine learning and deep learning models, especially in detecting rare fraudulent behaviors.

Conclusion: Generative models like GAN and VAE are effective for detecting complex suspicious behaviors in financial data.

Abstract: This study proposes an algorithm for detecting suspicious behaviors in large
payment flows based on deep generative models. By combining Generative
Adversarial Networks (GAN) and Variational Autoencoders (VAE), the algorithm is
designed to detect abnormal behaviors in financial transactions. First, the GAN
is used to generate simulated data that approximates normal payment flows. The
discriminator identifies anomalous patterns in transactions, enabling the
detection of potential fraud and money laundering behaviors. Second, a VAE is
introduced to model the latent distribution of payment flows, ensuring that the
generated data more closely resembles real transaction features, thus improving
the model's detection accuracy. The method optimizes the generative
capabilities of both GAN and VAE, ensuring that the model can effectively
capture suspicious behaviors even in sparse data conditions. Experimental
results show that the proposed method significantly outperforms traditional
machine learning algorithms and other deep learning models across various
evaluation metrics, especially in detecting rare fraudulent behaviors.
Furthermore, this study provides a detailed comparison of performance in
recognizing different transaction patterns (such as normal, money laundering,
and fraud) in large payment flows, validating the advantages of generative
models in handling complex financial data.

</details>


### [228] [Federated Latent Factor Learning for Recovering Wireless Sensor Networks Signal with Privacy-Preserving](https://arxiv.org/abs/2504.15525)
*Chengjun Yu,Yixin Ran,Yangyi Xia,Jia Wu,Xiaojing Liu*

Main category: cs.LG

TL;DR: Proposes FLFL-SSR, a federated latent factor learning model for spatial signal recovery in WSNs, ensuring privacy and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Addresses missing data in WSNs due to sensor failures and energy-saving, while ensuring data privacy, which existing LFL methods overlook.

Method: Introduces a sensor-level federated learning framework (gradient updates only) and a local spatial sharing strategy for latent feature vectors.

Result: Outperforms existing federated methods in recovery performance on real-world WSN datasets.

Conclusion: FLFL-SSR effectively balances privacy and accuracy in missing data recovery for WSNs.

Abstract: Wireless Sensor Networks (WSNs) are a cutting-edge domain in the field of
intelligent sensing. Due to sensor failures and energy-saving strategies, the
collected data often have massive missing data, hindering subsequent analysis
and decision-making. Although Latent Factor Learning (LFL) has been proven
effective in recovering missing data, it fails to sufficiently consider data
privacy protection. To address this issue, this paper innovatively proposes a
federated latent factor learning (FLFL) based spatial signal recovery (SSR)
model, named FLFL-SSR. Its main idea is two-fold: 1) it designs a sensor-level
federated learning framework, where each sensor uploads only gradient updates
instead of raw data to optimize the global model, and 2) it proposes a local
spatial sharing strategy, allowing sensors within the same spatial region to
share their latent feature vectors, capturing spatial correlations and
enhancing recovery accuracy. Experimental results on two real-world WSNs
datasets demonstrate that the proposed model outperforms existing federated
methods in terms of recovery performance.

</details>


### [229] [Interpretable Deep Learning for Polar Mechanistic Reaction Prediction](https://arxiv.org/abs/2504.15539)
*Ryan J. Miller,Alexander E. Dashuta,Brayden Rudisill,David Van Vranken,Pierre Baldi*

Main category: cs.LG

TL;DR: PMechRP, a hybrid ML model, predicts chemical reactions with mechanistic detail, achieving high accuracy on test datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate reaction prediction is crucial for synthetic chemistry but is complex and resource-intensive. Existing models lack interpretability and mechanistic insight.

Method: PMechRP trains on the PMechDB dataset, representing reactions as polar elementary steps. It uses a hybrid model combining transformer and Siamese architectures.

Result: The hybrid model achieves 94.9% top-10 accuracy on PMechDB and 84.9% target recovery on a human benchmark dataset.

Conclusion: PMechRP offers a scalable, interpretable solution for reaction prediction, outperforming existing methods.

Abstract: Accurately predicting chemical reactions is essential for driving innovation
in synthetic chemistry, with broad applications in medicine, manufacturing, and
agriculture. At the same time, reaction prediction is a complex problem which
can be both time-consuming and resource-intensive for chemists to solve. Deep
learning methods offer an appealing solution by enabling high-throughput
reaction prediction. However, many existing models are trained on the US Patent
Office dataset and treat reactions as overall transformations: mapping
reactants directly to products with limited interpretability or mechanistic
insight. To address this, we introduce PMechRP (Polar Mechanistic Reaction
Predictor), a system that trains machine learning models on the PMechDB
dataset, which represents reactions as polar elementary steps that capture
electron flow and mechanistic detail. To further expand model coverage and
improve generalization, we augment PMechDB with a diverse set of
combinatorially generated reactions. We train and compare a range of machine
learning models, including transformer-based, graph-based, and two-step siamese
architectures. Our best-performing approach was a hybrid model, which combines
a 5-ensemble of Chemformer models with a two-step Siamese framework to leverage
the accuracy of transformer architectures, while filtering away "alchemical"
products using the two-step network predictions. For evaluation, we use a test
split of the PMechDB dataset and additionally curate a human benchmark dataset
consisting of complete mechanistic pathways extracted from an organic chemistry
textbook. Our hybrid model achieves a top-10 accuracy of 94.9% on the PMechDB
test set and a target recovery rate of 84.9% on the pathway dataset.

</details>


### [230] [Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis](https://arxiv.org/abs/2504.15562)
*Dip Roy*

Main category: cs.LG

TL;DR: A Bayesian VAE with multi-head attention improves anomaly detection in brain MRIs by modeling uncertainty, achieving high ROC and PR AUC scores.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in medical imaging, especially for neurological conditions, is critical but often lacks uncertainty handling in conventional methods.

Method: Bayesian Variational Autoencoder (VAE) with multi-head attention, incorporating epistemic and aleatoric uncertainty estimation via Bayesian inference.

Result: Achieved 0.83 ROC AUC and 0.83 PR AUC on the BraTS2020 dataset.

Conclusion: Modeling uncertainty enhances anomaly detection performance, interpretability, and provides confidence estimates for clinical decisions.

Abstract: In medical imaging, anomaly detection is a vital element of healthcare
diagnostics, especially for neurological conditions which can be
life-threatening. Conventional deterministic methods often fall short when it
comes to capturing the inherent uncertainty of anomaly detection tasks. This
paper introduces a Bayesian Variational Autoencoder (VAE) equipped with
multi-head attention mechanisms for detecting anomalies in brain magnetic
resonance imaging (MRI). For the purpose of improving anomaly detection
performance, we incorporate both epistemic and aleatoric uncertainty estimation
through Bayesian inference. The model was tested on the BraTS2020 dataset, and
the findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper
suggests that modeling uncertainty is an essential component of anomaly
detection, enhancing both performance and interpretability and providing
confidence estimates, as well as anomaly predictions, for clinicians to
leverage in making medical decisions.

</details>


### [231] [Smooth Calibration and Decision Making](https://arxiv.org/abs/2504.15582)
*Jason Hartline,Yifan Wu,Yunran Yang*

Main category: cs.LG

TL;DR: The paper explores the discrepancy between calibration errors in machine learning (continuous) and decision-making (discontinuous), showing post-processing can achieve near-optimal calibration for decision-making.


<details>
  <summary>Details</summary>
Motivation: To address the mismatch between calibration errors for predictors used in machine learning and those critical for decision-making, ensuring trustworthiness for decision-makers.

Method: Post-processing an online predictor with noise to achieve differential privacy, aiming for low Expected Calibration Error (ECE) and Calibration Decision Loss (CDL).

Result: Post-processing achieves $O(\sqrt{\epsilon})$ ECE and CDL, asymptotically optimal but non-optimal compared to direct optimization methods.

Conclusion: Post-processing can bridge the gap between calibration errors for machine learning and decision-making, though direct optimization remains superior.

Abstract: Calibration requires predictor outputs to be consistent with their Bayesian
posteriors. For machine learning predictors that do not distinguish between
small perturbations, calibration errors are continuous in predictions, e.g.,
smooth calibration error (Foster and Hart, 2018), Distance to Calibration
(Blasiok et al., 2023a). On the contrary, decision-makers who use predictions
make optimal decisions discontinuously in probabilistic space, experiencing
loss from miscalibration discontinuously. Calibration errors for
decision-making are thus discontinuous, e.g., Expected Calibration Error
(Foster and Vohra, 1997), and Calibration Decision Loss (Hu and Wu, 2024).
Thus, predictors with a low calibration error for machine learning may suffer a
high calibration error for decision-making, i.e., they may not be trustworthy
for decision-makers optimizing assuming their predictions are correct. It is
natural to ask if post-processing a predictor with a low calibration error for
machine learning is without loss to achieve a low calibration error for
decision-making. In our paper, we show that post-processing an online predictor
with $\epsilon$ distance to calibration achieves $O(\sqrt{\epsilon})$ ECE and
CDL, which is asymptotically optimal. The post-processing algorithm adds noise
to make predictions differentially private. The optimal bound from low distance
to calibration predictors from post-processing is non-optimal compared with
existing online calibration algorithms that directly optimize for ECE and CDL.

</details>


### [232] [MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design](https://arxiv.org/abs/2504.15587)
*Zimo Yan,Jie Zhang,Zheng Xie,Chang Liu,Yizhen Liu,Yiping Song*

Main category: cs.LG

TL;DR: MetaMolGen is a meta-learning-based molecular generator for few-shot and property-conditioned molecular generation, outperforming traditional methods in low-data scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of molecular generation in data-scarce settings where traditional models struggle with conditional generalization.

Method: Uses a first-order meta-learning approach, standardizes graph motifs in a latent space, and employs an autoregressive sequence model for SMILES generation. Includes a learnable property projector for conditional generation.

Result: Generates valid and diverse SMILES sequences in low-data regimes, outperforming conventional baselines.

Conclusion: MetaMolGen excels in fast adaptation and efficient conditional generation for practical molecular design.

Abstract: Molecular generation plays an important role in drug discovery and materials
science, especially in data-scarce scenarios where traditional generative
models often struggle to achieve satisfactory conditional generalization. To
address this challenge, we propose MetaMolGen, a first-order
meta-learning-based molecular generator designed for few-shot and
property-conditioned molecular generation. MetaMolGen standardizes the
distribution of graph motifs by mapping them to a normalized latent space, and
employs a lightweight autoregressive sequence model to generate SMILES
sequences that faithfully reflect the underlying molecular structure. In
addition, it supports conditional generation of molecules with target
properties through a learnable property projector integrated into the
generative process.Experimental results demonstrate that MetaMolGen
consistently generates valid and diverse SMILES sequences under low-data
regimes, outperforming conventional baselines. This highlights its advantage in
fast adaptation and efficient conditional generation for practical molecular
design.

</details>


### [233] [Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification](https://arxiv.org/abs/2504.15594)
*Tatsuhito Hasegawa,Shunsuke Sakai*

Main category: cs.LG

TL;DR: The study reveals that the optimal temperature parameter $T^*$ in softmax is determined by feature dimensionality, proposes coefficients and a batch normalization layer to stabilize it, and derives an empirical formula for training-free $T^*$ estimation.


<details>
  <summary>Details</summary>
Motivation: To theoretically and empirically determine the optimal temperature parameter $T^*$ in softmax for classification tasks, addressing its variability under practical conditions.

Method: Proposes temperature determination coefficients, inserts a batch normalization layer, and develops an empirical formula for $T^*$ estimation without training.

Result: The derived $T^*$ aligns with theory and generalizes across tasks, improving classification performance.

Conclusion: The study offers a practical, training-free solution for determining $T^*$, enhancing performance in diverse tasks.

Abstract: In deep learning-based classification tasks, the softmax function's
temperature parameter $T$ critically influences the output distribution and
overall performance. This study presents a novel theoretical insight that the
optimal temperature $T^*$ is uniquely determined by the dimensionality of the
feature representations, thereby enabling training-free determination of $T^*$.
Despite this theoretical grounding, empirical evidence reveals that $T^*$
fluctuates under practical conditions owing to variations in models, datasets,
and other confounding factors. To address these influences, we propose and
optimize a set of temperature determination coefficients that specify how $T^*$
should be adjusted based on the theoretical relationship to feature
dimensionality. Additionally, we insert a batch normalization layer immediately
before the output layer, effectively stabilizing the feature space. Building on
these coefficients and a suite of large-scale experiments, we develop an
empirical formula to estimate $T^*$ without additional training while also
introducing a corrective scheme to refine $T^*$ based on the number of classes
and task complexity. Our findings confirm that the derived temperature not only
aligns with the proposed theoretical perspective but also generalizes
effectively across diverse tasks, consistently enhancing classification
performance and offering a practical, training-free solution for determining
$T^*$.

</details>


### [234] [Learning Dynamic Graphs via Tensorized and Lightweight Graph Convolutional Networks](https://arxiv.org/abs/2504.15613)
*Minglian Han*

Main category: cs.LG

TL;DR: The paper introduces TLGCN, a tensorized lightweight graph convolutional network, to improve dynamic graph learning by jointly modeling spatio-temporal dependencies, reducing memory usage, and outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Conventional DGCNs decouple spatial and temporal patterns, disrupting spatio-temporal dependencies. This study aims to address this limitation.

Method: Proposes TLGCN with a novel spatio-temporal information propagation method and tensorized lightweight graph convolution, omitting complex transformations.

Result: TLGCN outperforms state-of-the-art models in weight estimation tasks on four real-world datasets.

Conclusion: TLGCN effectively captures spatio-temporal dependencies and reduces memory usage, offering superior performance in dynamic graph learning.

Abstract: A dynamic graph (DG) is frequently encountered in numerous real-world
scenarios. Consequently, A dynamic graph convolutional network (DGCN) has been
successfully applied to perform precise representation learning on a DG.
However, conventional DGCNs typically consist of a static GCN coupled with a
sequence neural network (SNN) to model spatial and temporal patterns
separately. This decoupled modeling mechanism inherently disrupts the intricate
spatio-temporal dependencies. To address the issue, this study proposes a novel
Tensorized Lightweight Graph Convolutional Network (TLGCN) for accurate dynamic
graph learning. It mainly contains the following two key concepts: a) designing
a novel spatio-temporal information propagation method for joint propagation of
spatio-temporal information based on the tensor M-product framework; b)
proposing a tensorized lightweight graph convolutional network based on the
above method, which significantly reduces the memory occupation of the model by
omitting complex feature transformation and nonlinear activation. Numerical
experiments on four real-world datasets demonstrate that the proposed TLGCN
outperforms the state-of-the-art models in the weight estimation task on DGs.

</details>


### [235] [Dimension-Free Decision Calibration for Nonlinear Loss Functions](https://arxiv.org/abs/2504.15615)
*Jingwu Tang,Jiayun Wu,Zhiwei Steven Wu,Jiahao Zhang*

Main category: cs.LG

TL;DR: The paper explores conditions for optimal decision-making based on model predictions, addressing challenges in calibration and decision calibration for high-dimensional spaces and nonlinear losses. It introduces smooth decision calibration to achieve dimension-free algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure optimal decision-making under model predictions, overcoming limitations of traditional calibration methods in high-dimensional and nonlinear settings.

Method: The paper proposes a smooth version of decision calibration, enabling dimension-free algorithms. It introduces efficient post-processing methods for predictors to achieve decision calibration without sacrificing accuracy.

Result: The results show that smooth decision calibration allows for dimension-free sample complexity, with algorithms efficiently post-processing predictors to meet calibration requirements.

Conclusion: The conclusion highlights the feasibility of achieving decision calibration for nonlinear losses with dimension-free sample complexity, offering practical solutions for real-world applications.

Abstract: When model predictions inform downstream decision making, a natural question
is under what conditions can the decision-makers simply respond to the
predictions as if they were the true outcomes. Calibration suffices to
guarantee that simple best-response to predictions is optimal. However,
calibration for high-dimensional prediction outcome spaces requires exponential
computational and statistical complexity. The recent relaxation known as
decision calibration ensures the optimality of the simple best-response rule
while requiring only polynomial sample complexity in the dimension of outcomes.
However, known results on calibration and decision calibration crucially rely
on linear loss functions for establishing best-response optimality. A natural
approach to handle nonlinear losses is to map outcomes $y$ into a feature space
$\phi(y)$ of dimension $m$, then approximate losses with linear functions of
$\phi(y)$. Unfortunately, even simple classes of nonlinear functions can demand
exponentially large or infinite feature dimensions $m$. A key open problem is
whether it is possible to achieve decision calibration with sample complexity
independent of~$m$. We begin with a negative result: even verifying decision
calibration under standard deterministic best response inherently requires
sample complexity polynomial in~$m$. Motivated by this lower bound, we
investigate a smooth version of decision calibration in which decision-makers
follow a smooth best-response. This smooth relaxation enables dimension-free
decision calibration algorithms. We introduce algorithms that, given
$\mathrm{poly}(|A|,1/\epsilon)$ samples and any initial predictor~$p$, can
efficiently post-process it to satisfy decision calibration without worsening
accuracy. Our algorithms apply broadly to function classes that can be
well-approximated by bounded-norm functions in (possibly infinite-dimensional)
separable RKHS.

</details>


### [236] [SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2504.15616)
*Kai Chen,Xiaodong Zhao,Yujie Huang,Guoyu Fang,Xiao Song,Ruiping Wang,Ziyuan Wang*

Main category: cs.LG

TL;DR: SocialMOIF improves trajectory prediction by modeling multi-order intention interactions and optimizing trajectories, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current trajectory forecasting due to high uncertainty of agent intentions and complex higher-order social influences.

Method: Proposes SocialMOIF, a multi-order intention fusion model with a trajectory distribution approximator and global trajectory optimizer, using a novel loss function.

Result: Outperforms state-of-the-art baselines in dynamic and static datasets across multiple metrics.

Conclusion: SocialMOIF effectively enhances trajectory prediction by integrating direct and indirect intention interactions and optimizing model interpretability and accuracy.

Abstract: The analysis and prediction of agent trajectories are crucial for
decision-making processes in intelligent systems, with precise short-term
trajectory forecasting being highly significant across a range of applications.
Agents and their social interactions have been quantified and modeled by
researchers from various perspectives; however, substantial limitations exist
in the current work due to the inherent high uncertainty of agent intentions
and the complex higher-order influences among neighboring groups. SocialMOIF is
proposed to tackle these challenges, concentrating on the higher-order
intention interactions among neighboring groups while reinforcing the primary
role of first-order intention interactions between neighbors and the target
agent. This method develops a multi-order intention fusion model to achieve a
more comprehensive understanding of both direct and indirect intention
information. Within SocialMOIF, a trajectory distribution approximator is
designed to guide the trajectories toward values that align more closely with
the actual data, thereby enhancing model interpretability. Furthermore, a
global trajectory optimizer is introduced to enable more accurate and efficient
parallel predictions. By incorporating a novel loss function that accounts for
distance and direction during training, experimental results demonstrate that
the model outperforms previous state-of-the-art baselines across multiple
metrics in both dynamic and static datasets.

</details>


### [237] [RadioDiff-$k^2$: Helmholtz Equation Informed Generative Diffusion Model for Multi-Path Aware Radio Map Construction](https://arxiv.org/abs/2504.15623)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Ruijin Sun,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TL;DR: Proposes RadioDiff-$k^2$, a physics-informed generative learning method for accurate multipath-aware radio map construction, combining data efficiency with EM wave physics.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of accurate radio map construction in 6G networks, where conventional methods are computationally heavy, and neural networks lack physics integration.

Method: Uses a dual generative diffusion model framework guided by the Helmholtz equation to model EM singularities and reconstruct radio maps.

Result: Enhances radio map accuracy in complex multipath environments by integrating physics with data-driven efficiency.

Conclusion: The method successfully bridges the gap between physics-based modeling and computational efficiency for radio map construction.

Abstract: In this paper, we propose a novel physics-informed generative learning
approach, termed RadioDiff-$\bm{k^2}$, for accurate and efficient
multipath-aware radio map (RM) construction. As wireless communication evolves
towards environment-aware paradigms, driven by the increasing demand for
intelligent and proactive optimization in sixth-generation (6G) networks,
accurate construction of RMs becomes crucial yet highly challenging.
Conventional electromagnetic (EM)-based methods, such as full-wave solvers and
ray-tracing approaches, exhibit substantial computational overhead and limited
adaptability to dynamic scenarios. Although, existing neural network (NN)
approaches have efficient inferencing speed, they lack sufficient consideration
of the underlying physics of EM wave propagation, limiting their effectiveness
in accurately modeling critical EM singularities induced by complex multipath
environments. To address these fundamental limitations, we propose a novel
physics-inspired RM construction method guided explicitly by the Helmholtz
equation, which inherently governs EM wave propagation. Specifically, we
theoretically establish a direct correspondence between EM singularities, which
correspond to the critical spatial features influencing wireless propagation,
and regions defined by negative wave numbers in the Helmholtz equation. Based
on this insight, we design an innovative dual generative diffusion model (DM)
framework comprising one DM dedicated to accurately inferring EM singularities
and another DM responsible for reconstructing the complete RM using these
singularities along with environmental contextual information. Our
physics-informed approach uniquely combines the efficiency advantages of
data-driven methods with rigorous physics-based EM modeling, significantly
enhancing RM accuracy, particularly in complex propagation environments
dominated by multipath effects.

</details>


### [238] [Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers](https://arxiv.org/abs/2504.15634)
*Peizheng Liu,Hitoshi Iba*

Main category: cs.LG

TL;DR: A Transformer-based Deep Q-Network (DQN) is adapted for the 3D H-P protein folding problem, achieving near-optimal results and demonstrating the potential of attention-based reinforcement learning in this domain.


<details>
  <summary>Details</summary>
Motivation: The application of Transformer-based architectures to the hydrophobic-hydrophilic (H-P) model for protein folding is relatively unexplored, despite their success in sequence modeling.

Method: The system uses a DQN with attention mechanisms, formulating folding as a self-avoiding walk with a specialized reward function. Techniques like validity checks, dueling and double Q-learning, and prioritized replay are employed.

Result: The approach achieves known best solutions for shorter sequences and near-optimal results for longer chains.

Conclusion: The study highlights the promise of attention-based reinforcement learning for protein folding and introduces a Transformer-based Q-network prototype for 3D lattice models.

Abstract: Transformer-based architectures have recently propelled advances in sequence
modeling across domains, but their application to the hydrophobic-hydrophilic
(H-P) model for protein folding remains relatively unexplored. In this work, we
adapt a Deep Q-Network (DQN) integrated with attention mechanisms
(Transformers) to address the 3D H-P protein folding problem. Our system
formulates folding decisions as a self-avoiding walk in a reinforced
environment, and employs a specialized reward function based on favorable
hydrophobic interactions. To improve performance, the method incorporates
validity check including symmetry-breaking constraints, dueling and double
Q-learning, and prioritized replay to focus learning on critical transitions.
Experimental evaluations on standard benchmark sequences demonstrate that our
approach achieves several known best solutions for shorter sequences, and
obtains near-optimal results for longer chains. This study underscores the
promise of attention-based reinforcement learning for protein folding, and
created a prototype of Transformer-based Q-network structure for 3-dimensional
lattice models.

</details>


### [239] [An XAI-based Analysis of Shortcut Learning in Neural Networks](https://arxiv.org/abs/2504.15664)
*Phuong Quynh Le,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: The paper introduces a diagnostic measure (neuron spurious score) to analyze how neural networks encode spurious correlations, revealing partial disentanglement and incomplete assumptions in existing mitigation methods.


<details>
  <summary>Details</summary>
Motivation: Machine learning models often rely on spurious features, which can lead to unreliable predictions. Existing mitigation methods are inconsistent, prompting a need for systematic analysis.

Method: The authors propose the neuron spurious score, an XAI-based measure, and analyze CNNs and ViTs using architecture-specific methods.

Result: Spurious features are partially disentangled, with varying degrees across architectures. Existing mitigation methods are based on incomplete assumptions.

Conclusion: The findings pave the way for developing better methods to mitigate spurious correlations, enhancing AI model safety.

Abstract: Machine learning models tend to learn spurious features - features that
strongly correlate with target labels but are not causal. Existing approaches
to mitigate models' dependence on spurious features work in some cases, but
fail in others. In this paper, we systematically analyze how and where neural
networks encode spurious correlations. We introduce the neuron spurious score,
an XAI-based diagnostic measure to quantify a neuron's dependence on spurious
features. We analyze both convolutional neural networks (CNNs) and vision
transformers (ViTs) using architecture-specific methods. Our results show that
spurious features are partially disentangled, but the degree of disentanglement
varies across model architectures. Furthermore, we find that the assumptions
behind existing mitigation methods are incomplete. Our results lay the
groundwork for the development of novel methods to mitigate spurious
correlations and make AI models safer to use in practice.

</details>


### [240] [Invariant Learning with Annotation-free Environments](https://arxiv.org/abs/2504.15686)
*Phuong Quynh Le,Christin Seifert,Jörg Schlötterer*

Main category: cs.LG

TL;DR: The paper proposes an annotation-free method for invariant learning by inferring environments from the representation space of an ERM model, showing effectiveness on ColoredMNIST.


<details>
  <summary>Details</summary>
Motivation: To improve domain generalization without relying on pre-partitioned environments or additional annotations.

Method: Infer environments from the representation space of a trained ERM model.

Result: Achieves performance comparable to methods requiring explicit environment labels and matches an annotation-free method with strong restrictions.

Conclusion: The approach is effective for invariant learning without environment annotations.

Abstract: Invariant learning is a promising approach to improve domain generalization
compared to Empirical Risk Minimization (ERM). However, most invariant learning
methods rely on the assumption that training examples are pre-partitioned into
different known environments. We instead infer environments without the need
for additional annotations, motivated by observations of the properties within
the representation space of a trained ERM model. We show the preliminary
effectiveness of our approach on the ColoredMNIST benchmark, achieving
performance comparable to methods requiring explicit environment labels and on
par with an annotation-free method that poses strong restrictions on the ERM
reference model.

</details>


### [241] [Riemannian Neural Geodesic Interpolant](https://arxiv.org/abs/2504.15736)
*Jiawen Wu,Bingguang Chen,Yuyi Zhou,Qi Meng,Rongchan Zhu,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: The paper introduces the Riemannian Neural Geodesic Interpolant (RNGI) model for interpolating between probability densities on Riemannian manifolds, along with the E-SDE algorithm for improved stochastic sampling.


<details>
  <summary>Details</summary>
Motivation: Existing stochastic interpolants are limited to Euclidean spaces, restricting their application to real-world problems involving Riemannian manifolds.

Method: The RNGI model interpolates between densities on a Riemannian manifold using stochastic geodesics, and the E-SDE algorithm reduces sampling errors compared to classical methods.

Result: Theoretical bounds on generative bias are provided, and experiments on S2 and SO(3) manifolds demonstrate the model's effectiveness.

Conclusion: RNGI and E-SDE offer a robust solution for generative modeling on Riemannian manifolds, outperforming traditional methods.

Abstract: Stochastic interpolants are efficient generative models that bridge two
arbitrary probability density functions in finite time, enabling flexible
generation from the source to the target distribution or vice versa. These
models are primarily developed in Euclidean space, and are therefore limited in
their application to many distribution learning problems defined on Riemannian
manifolds in real-world scenarios. In this work, we introduce the Riemannian
Neural Geodesic Interpolant (RNGI) model, which interpolates between two
probability densities on a Riemannian manifold along the stochastic geodesics,
and then samples from one endpoint as the final state using the continuous flow
originating from the other endpoint. We prove that the temporal marginal
density of RNGI solves a transport equation on the Riemannian manifold. After
training the model's the neural velocity and score fields, we propose the
Embedding Stochastic Differential Equation (E-SDE) algorithm for stochastic
sampling of RNGI. E-SDE significantly improves the sampling quality by reducing
the accumulated error caused by the excessive intrinsic discretization of
Riemannian Brownian motion in the classical Geodesic Random Walk (GRW)
algorithm. We also provide theoretical bounds on the generative bias measured
in terms of KL-divergence. Finally, we demonstrate the effectiveness of the
proposed RNGI and E-SDE through experiments conducted on both collected and
synthetic distributions on S2 and SO(3).

</details>


### [242] [Observability conditions for neural state-space models with eigenvalues and their roots of unity](https://arxiv.org/abs/2504.15758)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: The paper explores observability in neural state-space models and Mamba architecture using ODEs and control theory, proposing efficient methods to enforce observability with computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enforcing observability in high-dimensional, learnable neural state-space models, leveraging control theory and computational strategies.

Method: Develops observability-enforcing strategies using eigenvalues, roots of unity, and permutations, with a focus on Fourier transforms and Vandermonde matrices. Also introduces a shared-parameter Mamba system for efficiency.

Result: Presents five key results, including observability via permutations, Fourier-based methods, a Hautus-type condition for Mamba, and a computationally efficient shared-parameter construction.

Conclusion: The methods achieve computational efficiency and enforce observability effectively, with a training algorithm satisfying Robbins-Monro conditions under orthogonality.

Abstract: We operate through the lens of ordinary differential equations and control
theory to study the concept of observability in the context of neural
state-space models and the Mamba architecture. We develop strategies to enforce
observability, which are tailored to a learning context, specifically where the
hidden states are learnable at initial time, in conjunction to over its
continuum, and high-dimensional. We also highlight our methods emphasize
eigenvalues, roots of unity, or both. Our methods effectuate computational
efficiency when enforcing observability, sometimes at great scale. We formulate
observability conditions in machine learning based on classical control theory
and discuss their computational complexity. Our nontrivial results are
fivefold. We discuss observability through the use of permutations in neural
applications with learnable matrices without high precision. We present two
results built upon the Fourier transform that effect observability with high
probability up to the randomness in the learning. These results are worked with
the interplay of representations in Fourier space and their eigenstructure,
nonlinear mappings, and the observability matrix. We present a result for Mamba
that is similar to a Hautus-type condition, but instead employs an argument
using a Vandermonde matrix instead of eigenvectors. Our final result is a
shared-parameter construction of the Mamba system, which is computationally
efficient in high exponentiation. We develop a training algorithm with this
coupling, showing it satisfies a Robbins-Monro condition under certain
orthogonality, while a more classical training procedure fails to satisfy a
contraction with high Lipschitz constant.

</details>


### [243] [Grounded in Context: Retrieval-Based Method for Hallucination Detection](https://arxiv.org/abs/2504.15771)
*Assaf Gerner,Netta Madvil,Nadav Barak,Alex Zaikman,Jonatan Liberman,Liron Hamra,Rotem Brazilay,Shay Tsadok,Yaron Friedman,Neal Harow,Noam Bresler,Shir Chorev,Philip Tannor*

Main category: cs.LG

TL;DR: Deepchecks' "Grounded in Context" framework detects hallucinated answers in LLMs using retrieval and NLI models, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Address hallucinated answers in production LLMs despite advancements in grounded content generation.

Method: Integrates retrieval and NLI models with a 512-token context window to predict factual consistency.

Result: Achieves an F1 score of 0.83 in RAGTruth's task, matching trained methods and outperforming similar-sized models.

Conclusion: The framework effectively detects unsupported claims in diverse use cases like summarization and RAG.

Abstract: Despite advancements in grounded content generation, production Large
Language Models (LLMs) based applications still suffer from hallucinated
answers. We present "Grounded in Context" - Deepchecks' hallucination detection
framework, designed for production-scale long-context data and tailored to
diverse use cases, including summarization, data extraction, and RAG. Inspired
by RAG architecture, our method integrates retrieval and Natural Language
Inference (NLI) models to predict factual consistency between premises and
hypotheses using an encoder-based model with only a 512-token context window.
Our framework identifies unsupported claims with an F1 score of 0.83 in
RAGTruth's response-level classification task, matching methods that trained on
the dataset, and outperforming all comparable frameworks using similar-sized
models.

</details>


### [244] [Clifford Group Equivariant Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2504.15773)
*Cong Liu,Sharvaree Vadgama,David Ruhe,Erik Bekkers,Patrick Forrè*

Main category: cs.LG

TL;DR: The paper introduces Clifford Diffusion Models (CDMs), leveraging Clifford algebra for E(n)-equivariant diffusion models to capture richer geometric information through higher-grade multivectors.


<details>
  <summary>Details</summary>
Motivation: To enhance generative modeling by incorporating richer geometric information from Clifford algebra's multivector subspaces.

Method: Extends diffusion to higher-grade multivectors, embedding data in grade-k subspaces and applying latent diffusion across complete multivectors.

Result: Empirical results on QM9 dataset show CDMs as promising for unconditional molecular generation.

Conclusion: CDMs offer a novel approach for generative modeling by utilizing Clifford algebra's expressive power.

Abstract: This paper explores leveraging the Clifford algebra's expressive power for
$\E(n)$-equivariant diffusion models. We utilize the geometric products between
Clifford multivectors and the rich geometric information encoded in Clifford
subspaces in \emph{Clifford Diffusion Models} (CDMs). We extend the diffusion
process beyond just Clifford one-vectors to incorporate all higher-grade
multivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us
to apply latent diffusion across complete multivectors. This enables CDMs to
capture the joint distribution across different subspaces of the algebra,
incorporating richer geometric information through higher-order features. We
provide empirical results for unconditional molecular generation on the QM9
dataset, showing that CDMs provide a promising avenue for generative modeling.

</details>


### [245] [DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations](https://arxiv.org/abs/2504.15806)
*Kai Luo,Juan Tang,Mingchao Cai,Xiaoqing Zeng,Manqi Xie,Ming Yan*

Main category: cs.LG

TL;DR: DAE-KAN integrates KANs with PINNs to solve high-index DAEs, reducing errors by 1-2 orders of magnitude compared to traditional PINNs.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of Physics-Informed Neural Networks (PINNs) by leveraging the superior function-fitting abilities of Kolmogorov-Arnold Networks (KANs) for solving high-index differential-algebraic equations (DAEs).

Method: Proposes DAE-KAN, a framework combining KANs with PINNs, tested on DAE systems from index-1 to index-3.

Result: DAE-KAN reduces absolute errors significantly and outperforms classical methods in controlling drift-off error.

Conclusion: DAE-KAN shows promise for solving high-index DAEs with improved accuracy and generalization, offering a solution for challenging PDEs.

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
Multi-Layer Perceptrons (MLPs) due to their superior function-fitting abilities
in data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,
for solving high-index differential-algebraic equations (DAEs) by integrating
KANs with Physics-Informed Neural Networks (PINNs). This framework not only
preserves the ability of traditional PINNs to model complex systems governed by
physical laws but also enhances their performance by leveraging the
function-fitting strengths of KANs. Numerical experiments demonstrate that for
DAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute
errors of both differential and algebraic variables by 1 to 2 orders of
magnitude compared to traditional PINNs. To assess the effectiveness of this
approach, we analyze the drift-off error and find that both PINNs and DAE-KAN
outperform classical numerical methods in controlling this phenomenon. Our
results highlight the potential of neural network methods, particularly
DAE-KAN, in solving high-index DAEs with substantial computational accuracy and
generalization, offering a promising solution for challenging partial
differential-algebraic equations.

</details>


### [246] [Fusing Reward and Dueling Feedback in Stochastic Bandits](https://arxiv.org/abs/2504.15812)
*Xuchuang Wang,Qirun Zeng,Jinhang Zuo,Xutong Liu,Mohammad Hajiesmaili,John C. S. Lui,Adam Wierman*

Main category: cs.LG

TL;DR: The paper explores combining absolute (reward) and relative (dueling) feedback in stochastic bandits, proposing two fusion algorithms to minimize regret. The decomposition fusion matches the derived lower bound, outperforming the elimination fusion.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in stochastic bandits by leveraging both reward and dueling feedback, minimizing regret by selecting the more effective feedback type.

Method: Two fusion approaches: (1) elimination fusion, unifying feedback types, and (2) decomposition fusion, dynamically selecting feedback types for exploration/exploitation.

Result: Decomposition fusion achieves regret matching the lower bound, while elimination fusion suffers from suboptimal multiplicative regret. Experiments validate the algorithms.

Conclusion: The decomposition fusion algorithm is superior, achieving near-optimal regret by effectively combining feedback types.

Abstract: This paper investigates the fusion of absolute (reward) and relative
(dueling) feedback in stochastic bandits, where both feedback types are
gathered in each decision round. We derive a regret lower bound, demonstrating
that an efficient algorithm may incur only the smaller among the reward and
dueling-based regret for each individual arm. We propose two fusion approaches:
(1) a simple elimination fusion algorithm that leverages both feedback types to
explore all arms and unifies collected information by sharing a common
candidate arm set, and (2) a decomposition fusion algorithm that selects the
more effective feedback to explore the corresponding arms and randomly assigns
one feedback type for exploration and the other for exploitation in each round.
The elimination fusion experiences a suboptimal multiplicative term of the
number of arms in regret due to the intrinsic suboptimality of dueling
elimination. In contrast, the decomposition fusion achieves regret matching the
lower bound up to a constant under a common assumption. Extensive experiments
confirm the efficacy of our algorithms and theoretical results.

</details>


### [247] [DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers](https://arxiv.org/abs/2504.15827)
*Xuyang Zhong,Haochen Luo,Chen Liu*

Main category: cs.LG

TL;DR: DualOptim improves machine unlearning stability and efficacy by using adaptive learning rates and decoupled momentum, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods are sensitive to hyperparameters and perform inconsistently across scenarios, limiting practical use.

Method: Proposes DualOptim, incorporating adaptive learning rates and decoupled momentum factors for stable unlearning.

Result: DualOptim enhances unlearning performance and stability across tasks like image classification, generation, and large language models.

Conclusion: DualOptim is a versatile solution to improve existing machine unlearning algorithms.

Abstract: Existing machine unlearning (MU) approaches exhibit significant sensitivity
to hyperparameters, requiring meticulous tuning that limits practical
deployment. In this work, we first empirically demonstrate the instability and
suboptimal performance of existing popular MU methods when deployed in
different scenarios. To address this issue, we propose Dual Optimizer
(DualOptim), which incorporates adaptive learning rate and decoupled momentum
factors. Empirical and theoretical evidence demonstrates that DualOptim
contributes to effective and stable unlearning. Through extensive experiments,
we show that DualOptim can significantly boost MU efficacy and stability across
diverse tasks, including image classification, image generation, and large
language models, making it a versatile approach to empower existing MU
algorithms.

</details>


### [248] [Adaptive PCA-Based Outlier Detection for Multi-Feature Time Series in Space Missions](https://arxiv.org/abs/2504.15846)
*Jonah Ekelund,Savvas Raptis,Vicki Toy-Edens,Wenli Mo,Drew L. Turner,Ian J. Cohen,Stefano Markidis*

Main category: cs.LG

TL;DR: An adaptive outlier detection algorithm using PCA reconstruction error is proposed for real-time event detection in space missions, tested on NASA's MMS and THEMIS data.


<details>
  <summary>Details</summary>
Motivation: Efficient event detection in space missions is limited by onboard computational resources and data constraints, requiring robust real-time methods.

Method: Uses PCA for feature reduction and Incremental PCA to adapt dynamically to evolving data distributions, with pre-scaling for normalization.

Result: Successfully detected space plasma events (e.g., distinct environments, transients) in NASA's MMS and THEMIS missions.

Conclusion: The algorithm is effective for real-time outlier detection in space missions, adaptable without predefined models.

Abstract: Analyzing multi-featured time series data is critical for space missions
making efficient event detection, potentially onboard, essential for automatic
analysis. However, limited onboard computational resources and data downlink
constraints necessitate robust methods for identifying regions of interest in
real time. This work presents an adaptive outlier detection algorithm based on
the reconstruction error of Principal Component Analysis (PCA) for feature
reduction, designed explicitly for space mission applications. The algorithm
adapts dynamically to evolving data distributions by using Incremental PCA,
enabling deployment without a predefined model for all possible conditions. A
pre-scaling process normalizes each feature's magnitude while preserving
relative variance within feature types. We demonstrate the algorithm's
effectiveness in detecting space plasma events, such as distinct space
environments, dayside and nightside transients phenomena, and transition layers
through NASA's MMS mission observations. Additionally, we apply the method to
NASA's THEMIS data, successfully identifying a dayside transient using
onboard-available measurements.

</details>


### [249] [Consistent Causal Inference of Group Effects in Non-Targeted Trials with Finitely Many Effect Levels](https://arxiv.org/abs/2504.15854)
*Georgios Mavroudeas,Malik Magdon-Ismail,Kristin P. Bennett,Jason Kuruzovich*

Main category: cs.LG

TL;DR: PCM method improves accuracy in estimating treatment effects for heterogeneous groups by pre-clustering and merging, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of disentangling treatment effects in heterogeneous groups (sick vs. healthy) in non-targeted trials.

Method: Proposes PCM (pre-cluster and merge), a nonparametric approach for estimating group effects, ensuring asymptotic consistency.

Result: Demonstrates over 10x accuracy improvement on synthetic data compared to state-of-the-art methods.

Conclusion: PCM effectively estimates treatment effects in heterogeneous groups and extends to functions with finite range.

Abstract: A treatment may be appropriate for some group (the ``sick" group) on whom it
has a positive effect, but it can also have a detrimental effect on subjects
from another group (the ``healthy" group). In a non-targeted trial both sick
and healthy subjects may be treated, producing heterogeneous effects within the
treated group. Inferring the correct treatment effect on the sick population is
then difficult, because the effects on the different groups get tangled. We
propose an efficient nonparametric approach to estimating the group effects,
called {\bf PCM} (pre-cluster and merge). We prove its asymptotic consistency
in a general setting and show, on synthetic data, more than a 10x improvement
in accuracy over existing state-of-the-art. Our approach applies more generally
to consistent estimation of functions with a finite range.

</details>


### [250] [SUPRA: Subspace Parameterized Attention for Neural Operator on General Domains](https://arxiv.org/abs/2504.15897)
*Zherui Yang,Zhengyang Xue,Ligang Liu*

Main category: cs.LG

TL;DR: The paper introduces SUPRA, a neural operator that improves accuracy and efficiency for solving PDEs by generalizing attention mechanisms to function spaces and using Laplacian eigenfunctions for irregular domains.


<details>
  <summary>Details</summary>
Motivation: Current neural operators for PDEs face inefficiency with attention mechanisms on large-scale meshes and accuracy issues with spectral convolutions on irregular domains.

Method: The authors generalize attention to function spaces, propose SUPRA to approximate it in finite-dimensional subspaces, and use Laplacian eigenfunctions for irregular domains.

Result: SUPRA reduces error rates by up to 33% on PDE datasets while maintaining computational efficiency.

Conclusion: SUPRA effectively addresses challenges in neural operators for PDEs, offering improved accuracy and efficiency.

Abstract: Neural operators are efficient surrogate models for solving partial
differential equations (PDEs), but their key components face challenges: (1) in
order to improve accuracy, attention mechanisms suffer from computational
inefficiency on large-scale meshes, and (2) spectral convolutions rely on the
Fast Fourier Transform (FFT) on regular grids and assume a flat geometry, which
causes accuracy degradation on irregular domains. To tackle these problems, we
regard the matrix-vector operations in the standard attention mechanism on
vectors in Euclidean space as bilinear forms and linear operators in vector
spaces and generalize the attention mechanism to function spaces. This new
attention mechanism is fully equivalent to the standard attention but
impossible to compute due to the infinite dimensionality of function spaces. To
address this, inspired by model reduction techniques, we propose a Subspace
Parameterized Attention (SUPRA) neural operator, which approximates the
attention mechanism within a finite-dimensional subspace. To construct a
subspace on irregular domains for SUPRA, we propose using the Laplacian
eigenfunctions, which naturally adapt to domains' geometry and guarantee the
optimal approximation for smooth functions. Experiments show that the SUPRA
neural operator reduces error rates by up to 33% on various PDE datasets while
maintaining state-of-the-art computational efficiency.

</details>


### [251] [GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network](https://arxiv.org/abs/2504.15905)
*Wenjing Xiao,Chenglong Shi,Miaojiang Chen,Zhiquan Liu,Min Chen,H. Herbert Song*

Main category: cs.LG

TL;DR: GraphEdge is an efficient GNN-based edge computing architecture that optimizes communication costs and task offloading using hierarchical traversal and DRL.


<details>
  <summary>Details</summary>
Motivation: Address the high communication costs of GNN-based approaches in graph-structured IoT scenarios like traffic flow prediction and social recommendations.

Method: Proposes GraphEdge with HiCut for graph layout optimization and DRLGO for subgraph-based task offloading to minimize costs.

Result: Demonstrates effectiveness and adaptability, even in dynamic scenarios.

Conclusion: GraphEdge offers a scalable and efficient solution for GNN tasks in edge computing.

Abstract: With the exponential growth of Internet of Things (IoT) devices, edge
computing (EC) is gradually playing an important role in providing
cost-effective services. However, existing approaches struggle to perform well
in graph-structured scenarios where user data is correlated, such as traffic
flow prediction and social relationship recommender systems. In particular,
graph neural network (GNN)-based approaches lead to expensive server
communication cost. To address this problem, we propose GraphEdge, an efficient
GNN-based EC architecture. It considers the EC system of GNN tasks, where there
are associations between users and it needs to take into account the task data
of its neighbors when processing the tasks of a user. Specifically, the
architecture first perceives the user topology and represents their data
associations as a graph layout at each time step. Then the graph layout is
optimized by calling our proposed hierarchical traversal graph cut algorithm
(HiCut), which cuts the graph layout into multiple weakly associated subgraphs
based on the aggregation characteristics of GNN, and the communication cost
between different subgraphs during GNN inference is minimized. Finally, based
on the optimized graph layout, our proposed deep reinforcement learning (DRL)
based graph offloading algorithm (DRLGO) is executed to obtain the optimal
offloading strategy for the tasks of users, the offloading strategy is
subgraph-based, it tries to offload user tasks in a subgraph to the same edge
server as possible while minimizing the task processing time and energy
consumption of the EC system. Experimental results show the good effectiveness
and dynamic adaptation of our proposed architecture and it also performs well
even in dynamic scenarios.

</details>


### [252] [ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion](https://arxiv.org/abs/2504.15920)
*Xiang Li,Haobing Liu,Jianpeng Qi,Yuan Cao,Guoqing Chao,Yanwei Yu*

Main category: cs.LG

TL;DR: ScaleGNN addresses GNN challenges (over-smoothing and scalability) by adaptively fusing multi-level graph features and masking redundant features.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle with over-smoothing and scalability due to redundant information aggregation and high model complexity.

Method: Proposes ScaleGNN with adaptive high-order feature fusion, redundant feature masking, and low-order enhanced feature aggregation.

Result: Outperforms state-of-the-art GNNs in accuracy and computational efficiency on real-world datasets.

Conclusion: ScaleGNN effectively balances local and global structural information while reducing computational costs.

Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across
various graph-based tasks by effectively capturing relational information
between nodes. These models rely on iterative message passing to propagate node
features, enabling nodes to aggregate information from their neighbors. Recent
research has significantly improved the message-passing mechanism, enhancing
GNN scalability on large-scale graphs. However, GNNs still face two main
challenges: over-smoothing, where excessive message passing results in
indistinguishable node representations, especially in deep networks
incorporating high-order neighbors; and scalability issues, as traditional
architectures suffer from high model complexity and increased inference time
due to redundant information aggregation. This paper proposes a novel framework
for large-scale graphs named ScaleGNN that simultaneously addresses both
challenges by adaptively fusing multi-level graph features. We first construct
neighbor matrices for each order, learning their relative information through
trainable weights through an adaptive high-order feature fusion module. This
allows the model to selectively emphasize informative high-order neighbors
while reducing unnecessary computational costs. Additionally, we introduce a
High-order redundant feature masking mechanism based on a Local Contribution
Score (LCS), which enables the model to retain only the most relevant neighbors
at each order, preventing redundant information propagation. Furthermore,
low-order enhanced feature aggregation adaptively integrates low-order and
high-order features based on task relevance, ensuring effective capture of both
local and global structural information without excessive complexity. Extensive
experiments on real-world datasets demonstrate that our approach consistently
outperforms state-of-the-art GNN models in both accuracy and computational
efficiency.

</details>


### [253] [Achieving Distributive Justice in Federated Learning via Uncertainty Quantification](https://arxiv.org/abs/2504.15924)
*Alycia Carey,Xintao Wu*

Main category: cs.LG

TL;DR: UDJ-FL is a federated learning framework achieving multiple distributive justice-based fairness metrics, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the arbitrary choice of fairness metrics in federated learning by grounding them in social theories like distributive justice.

Method: Utilizes fair resource allocation and aleatoric uncertainty-based client weighing to achieve various fairness metrics (egalitarian, utilitarian, Rawls' difference principle, desert-based).

Result: Empirically achieves four fairness metrics, outperforming other methods, with theoretical guarantees for generalization bounds.

Conclusion: UDJ-FL provides a flexible, justified approach to client-level fairness in federated learning, supported by empirical and theoretical results.

Abstract: Client-level fairness metrics for federated learning are used to ensure that
all clients in a federation either: a) have similar final performance on their
local data distributions (i.e., client parity), or b) obtain final performance
on their local data distributions relative to their contribution to the
federated learning process (i.e., contribution fairness). While a handful of
works that propose either client-parity or contribution-based fairness metrics
ground their definitions and decisions in social theories of equality -- such
as distributive justice -- most works arbitrarily choose what notion of
fairness to align with which makes it difficult for practitioners to choose
which fairness metric aligns best with their fairness ethics. In this work, we
propose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),
a flexible federated learning framework that can achieve multiple distributive
justice-based client-level fairness metrics. Namely, by utilizing techniques
inspired by fair resource allocation, in conjunction with performing aleatoric
uncertainty-based client weighing, our UDJ-FL framework is able to achieve
egalitarian, utilitarian, Rawls' difference principle, or desert-based
client-level fairness. We empirically show the ability of UDJ-FL to achieve all
four defined distributive justice-based client-level fairness metrics in
addition to providing fairness equivalent to (or surpassing) other popular fair
federated learning works. Further, we provide justification for why aleatoric
uncertainty weighing is necessary to the construction of our UDJ-FL framework
as well as derive theoretical guarantees for the generalization bounds of
UDJ-FL. Our code is publicly available at
https://github.com/alycia-noel/UDJ-FL.

</details>


### [254] [StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation](https://arxiv.org/abs/2504.15930)
*Yinmin Zhong,Zili Zhang,Xiaoniu Song,Hanpeng Hu,Chao Jin,Bingyang Wu,Nuo Chen,Yukun Chen,Yu Zhou,Changyi Wan,Hongyu Zhou,Yimin Jiang,Yibo Zhu,Daxin Jiang*

Main category: cs.LG

TL;DR: StreamRL introduces a disaggregated architecture for RL in LLMs, addressing bottlenecks like pipeline and skewness bubbles, improving throughput and cost-efficiency.


<details>
  <summary>Details</summary>
Motivation: The colocated architecture for RL in LLMs suffers from resource coupling, limiting scalability and cost-efficiency.

Method: StreamRL uses stream generation and asynchronous RL to address pipeline bubbles, and a ranker model for skewness bubbles.

Result: StreamRL improves throughput by up to 2.66x and cost-effectiveness by up to 1.33x.

Conclusion: Disaggregated RL with StreamRL outperforms colocated architectures, offering scalability and efficiency.

Abstract: Reinforcement learning (RL) has become the core post-training technique for
large language models (LLMs). RL for LLMs involves two stages: generation and
training. The LLM first generates samples online, which are then used to derive
rewards for training. The conventional view holds that the colocated
architecture, where the two stages share resources via temporal multiplexing,
outperforms the disaggregated architecture, in which dedicated resources are
assigned to each stage. However, in real-world deployments, we observe that the
colocated architecture suffers from resource coupling, where the two stages are
constrained to use the same resources. This coupling compromises the
scalability and cost-efficiency of colocated RL in large-scale training. In
contrast, the disaggregated architecture allows for flexible resource
allocation, supports heterogeneous training setups, and facilitates
cross-datacenter deployment.
  StreamRL is designed with disaggregation from first principles and fully
unlocks its potential by addressing two types of performance bottlenecks in
existing disaggregated RL frameworks: pipeline bubbles, caused by stage
dependencies, and skewness bubbles, resulting from long-tail output length
distributions. To address pipeline bubbles, StreamRL breaks the traditional
stage boundary in synchronous RL algorithms through stream generation and
achieves full overlapping in asynchronous RL. To address skewness bubbles,
StreamRL employs an output-length ranker model to identify long-tail samples
and reduces generation time via skewness-aware dispatching and scheduling.
Experiments show that StreamRL improves throughput by up to 2.66x compared to
existing state-of-the-art systems, and improves cost-effectiveness by up to
1.33x in a heterogeneous, cross-datacenter setting.

</details>


### [255] [Universal Approximation with Softmax Attention](https://arxiv.org/abs/2504.15956)
*Jerry Yao-Chieh Hu,Hude Liu,Hong-Yu Chen,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: The paper proves that two-layer self-attention and one-layer self-attention with softmax are universal approximators for sequence-to-sequence functions. It introduces an interpolation-based method to analyze attention, showing it can approximate ReLU-like functions.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the universal approximation capabilities of self-attention mechanisms without relying on feed-forward networks, as prior works did.

Method: Uses a new interpolation-based technique to analyze self-attention, showing it can approximate generalized ReLU functions. Extends this to prove two-layer multi-head attention suffices for universal approximation.

Result: Self-attention can approximate ReLU-like functions and serves as a universal approximator for sequence-to-sequence tasks. Attention-only layers can also approximate statistical models in-context.

Conclusion: Self-attention mechanisms alone, without feed-forward networks, are powerful universal approximators, offering new insights into Transformers' capabilities.

Abstract: We prove that with linear transformations, both (i) two-layer self-attention
and (ii) one-layer self-attention followed by a softmax function are universal
approximators for continuous sequence-to-sequence functions on compact domains.
Our main technique is a new interpolation-based method for analyzing
attention's internal mechanism. This leads to our key insight: self-attention
is able to approximate a generalized version of ReLU to arbitrary precision,
and hence subsumes many known universal approximators. Building on these, we
show that two-layer multi-head attention alone suffices as a
sequence-to-sequence universal approximator. In contrast, prior works rely on
feed-forward networks to establish universal approximation in Transformers.
Furthermore, we extend our techniques to show that, (softmax-)attention-only
layers are capable of approximating various statistical models in-context. We
believe these techniques hold independent interest.

</details>


### [256] [OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning](https://arxiv.org/abs/2504.15995)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Jin-Hee Cho*

Main category: cs.LG

TL;DR: OPUS-VFL is a novel VFL framework addressing privacy-utility tradeoffs, incentives, and resource heterogeneity, outperforming existing methods in efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing VFL systems lack effective incentives, struggle with privacy-utility tradeoffs, and fail to handle resource heterogeneity, limiting practical deployment.

Method: OPUS-VFL introduces a privacy-aware incentive mechanism, LOO for feature importance, and adaptive differential privacy for dynamic noise calibration.

Result: OPUS-VFL reduces attack success rates by 20%, increases reconstruction error by 30%, and offers 25% higher incentives while maintaining privacy.

Conclusion: OPUS-VFL is a secure, fair, and high-performance solution for real-world VFL challenges.

Abstract: Vertical Federated Learning (VFL) enables organizations with disjoint feature
spaces but shared user bases to collaboratively train models without sharing
raw data. However, existing VFL systems face critical limitations: they often
lack effective incentive mechanisms, struggle to balance privacy-utility
tradeoffs, and fail to accommodate clients with heterogeneous resource
capabilities. These challenges hinder meaningful participation, degrade model
performance, and limit practical deployment. To address these issues, we
propose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.
OPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards
clients based on a principled combination of model contribution, privacy
preservation, and resource investment. It employs a lightweight leave-one-out
(LOO) strategy to quantify feature importance per client, and integrates an
adaptive differential privacy mechanism that enables clients to dynamically
calibrate noise levels to optimize their individual utility. Our framework is
designed to be scalable, budget-balanced, and robust to inference and poisoning
attacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and
CIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art
VFL baselines in both efficiency and robustness. It reduces label inference
attack success rates by up to 20%, increases feature inference reconstruction
error (MSE) by over 30%, and achieves up to 25% higher incentives for clients
that contribute meaningfully while respecting privacy and cost constraints.
These results highlight the practicality and innovation of OPUS-VFL as a
secure, fair, and performance-driven solution for real-world VFL.

</details>


### [257] [AlphaGrad: Non-Linear Gradient Normalization Optimizer](https://arxiv.org/abs/2504.16020)
*Soham Sane*

Main category: cs.LG

TL;DR: AlphaGrad is a memory-efficient optimizer with a single hyperparameter, outperforming Adam in some RL tasks but requiring careful tuning.


<details>
  <summary>Details</summary>
Motivation: Address memory overhead and hyperparameter complexity in adaptive optimizers like Adam.

Method: Uses tensor-wise L2 gradient normalization and a tanh transformation controlled by a steepness parameter.

Result: Performance varies by RL task: unstable in DQN, stable in TD3, superior in PPO.

Conclusion: AlphaGrad is a promising alternative for memory-constrained and on-policy RL scenarios.

Abstract: We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer
addressing the memory overhead and hyperparameter complexity of adaptive
methods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2
gradient normalization followed by a smooth hyperbolic tangent transformation,
$g' = \tanh(\alpha \cdot \tilde{g})$, controlled by a single steepness
parameter $\alpha$. Our contributions include: (1) the AlphaGrad algorithm
formulation; (2) a formal non-convex convergence analysis guaranteeing
stationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,
TD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent
performance profile. While exhibiting instability in off-policy DQN, it
provides enhanced training stability with competitive results in TD3 (requiring
careful $\alpha$ tuning) and achieves substantially superior performance in
on-policy PPO. These results underscore the critical importance of empirical
$\alpha$ selection, revealing strong interactions between the optimizer's
dynamics and the underlying RL algorithm. AlphaGrad presents a compelling
alternative optimizer for memory-constrained scenarios and shows significant
promise for on-policy learning regimes where its stability and efficiency
advantages can be particularly impactful.

</details>


### [258] [LLMs meet Federated Learning for Scalable and Secure IoT Management](https://arxiv.org/abs/2504.16032)
*Yazan Otoum,Arghavan Asad,Amiya Nayak*

Main category: cs.LG

TL;DR: A Federated Learning-driven Large Language Model (FL-LLM) framework is proposed to address scalability, security, and real-time decision-making in IoT, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in IoT like latency, privacy, and resource consumption with centralized architectures necessitate a scalable, secure, and efficient solution.

Method: The framework combines Generative IoT models with a Gradient Sensing Federated Strategy (GSFS) and a hybrid edge-cloud architecture for dynamic optimization.

Result: Evaluations on the IoT-23 dataset show improved accuracy, reduced latency, and better energy efficiency compared to traditional FL techniques.

Conclusion: The FL-LLM framework demonstrates potential for secure, scalable, and adaptive IoT management, integrating LLM-powered federated learning.

Abstract: The rapid expansion of IoT ecosystems introduces severe challenges in
scalability, security, and real-time decision-making. Traditional centralized
architectures struggle with latency, privacy concerns, and excessive resource
consumption, making them unsuitable for modern large-scale IoT deployments.
This paper presents a novel Federated Learning-driven Large Language Model
(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring
data privacy and computational efficiency. The framework integrates Generative
IoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),
dynamically optimizing model updates based on real-time network conditions. By
leveraging a hybrid edge-cloud processing architecture, our approach balances
intelligence, scalability, and security in distributed IoT environments.
Evaluations on the IoT-23 dataset demonstrate that our framework improves model
accuracy, reduces response latency, and enhances energy efficiency,
outperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings
highlight the potential of integrating LLM-powered federated learning into
large-scale IoT ecosystems, paving the way for more secure, scalable, and
adaptive IoT management solutions.

</details>


### [259] [Muon Optimizer Accelerates Grokking](https://arxiv.org/abs/2504.16041)
*Amund Tveit,Bjørn Remseth,Arve Skogvold*

Main category: cs.LG

TL;DR: Muon optimizer accelerates grokking onset compared to AdamW, reducing mean grokking epoch from 153.09 to 102.89.


<details>
  <summary>Details</summary>
Motivation: To study the impact of optimizers (Muon vs. AdamW) and softmax variants on the grokking phenomenon in models.

Method: Experiments on seven numerical tasks using a Transformer, varying optimizers and softmax functions.

Result: Muon significantly speeds up grokking onset (t = 5.0175, p = 6.33e-08).

Conclusion: Optimizer choice is crucial for transitioning from memorization to generalization.

Abstract: This paper investigates the impact of different optimizers on the grokking
phenomenon, where models exhibit delayed generalization. We conducted
experiments across seven numerical tasks (primarily modular arithmetic) using a
modern Transformer architecture. The experimental configuration systematically
varied the optimizer (Muon vs. AdamW) and the softmax activation function
(standard softmax, stablemax, and sparsemax) to assess their combined effect on
learning dynamics. Our empirical evaluation reveals that the Muon optimizer,
characterized by its use of spectral norm constraints and second-order
information, significantly accelerates the onset of grokking compared to the
widely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch
from 153.09 to 102.89 across all configurations, a statistically significant
difference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice
plays a crucial role in facilitating the transition from memorization to
generalization.

</details>


### [260] [$π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization](https://arxiv.org/abs/2504.16054)
*Physical Intelligence,Kevin Black,Noah Brown,James Darpinian,Karan Dhabalia,Danny Driess,Adnan Esmail,Michael Equi,Chelsea Finn,Niccolo Fusai,Manuel Y. Galliker,Dibya Ghosh,Lachy Groom,Karol Hausman,Brian Ichter,Szymon Jakubczak,Tim Jones,Liyiming Ke,Devin LeBlanc,Sergey Levine,Adrian Li-Bell,Mohith Mothukuri,Suraj Nair,Karl Pertsch,Allen Z. Ren,Lucy Xiaoyang Shi,Laura Smith,Jost Tobias Springenberg,Kyle Stachowicz,James Tanner,Quan Vuong,Homer Walke,Anna Walling,Haohuan Wang,Lili Yu,Ury Zhilinsky*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In order for robots to be useful, they must perform practically relevant
tasks in the real world, outside of the lab. While vision-language-action (VLA)
models have demonstrated impressive results for end-to-end robot control, it
remains an open question how far such models can generalize in the wild. We
describe $\pi_{0.5}$, a new model based on $\pi_{0}$ that uses co-training on
heterogeneous tasks to enable broad generalization. $\pi_{0.5}$\ uses data from
multiple robots, high-level semantic prediction, web data, and other sources to
enable broadly generalizable real-world robotic manipulation. Our system uses a
combination of co-training and hybrid multi-modal examples that combine image
observations, language commands, object detections, semantic subtask
prediction, and low-level actions. Our experiments show that this kind of
knowledge transfer is essential for effective generalization, and we
demonstrate for the first time that an end-to-end learning-enabled robotic
system can perform long-horizon and dexterous manipulation skills, such as
cleaning a kitchen or bedroom, in entirely new homes.

</details>


### [261] [LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities](https://arxiv.org/abs/2504.16078)
*Thomas Schmied,Jörg Bornschein,Jordi Grau-Moya,Markus Wulfmeier,Razvan Pascanu*

Main category: cs.LG

TL;DR: The paper investigates sub-optimal decision-making in LLM agents, identifies three failure modes (greediness, frequency bias, knowing-doing gap), and proposes RL fine-tuning on self-generated CoT rationales to improve performance.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise in agentic applications but suffer from sub-optimal exploration and the knowing-doing gap, hindering effective decision-making.

Method: Systematically study LLM failure modes, propose RL fine-tuning on self-generated CoT rationales, and test on tasks like multi-armed bandits and Tic-tac-toe.

Result: RL fine-tuning improves LLM decision-making by enhancing exploration and reducing the knowing-doing gap.

Conclusion: RL fine-tuning and exploration mechanisms (e.g., ε-greedy, self-correction) can effectively enhance LLM decision-making.

Abstract: The success of Large Language Models (LLMs) has sparked interest in various
agentic applications. A key hypothesis is that LLMs, leveraging common sense
and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently
solve complex domains. However, LLM agents have been found to suffer from
sub-optimal exploration and the knowing-doing gap, the inability to effectively
act on knowledge present in the model. In this work, we systematically study
why LLMs perform sub-optimally in decision-making scenarios. In particular, we
closely examine three prevalent failure modes: greediness, frequency bias, and
the knowing-doing gap. We propose mitigation of these shortcomings by
fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.
Our experiments across multi-armed bandits, contextual bandits, and
Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making
abilities of LLMs by increasing exploration and narrowing the knowing-doing
gap. Finally, we study both classic exploration mechanisms, such as
$\epsilon$-greedy, and LLM-specific approaches, such as self-correction and
self-consistency, to enable more effective fine-tuning of LLMs for
decision-making.

</details>


### [262] [Optimizing RLHF Training for Large Language Models with Stage Fusion](https://arxiv.org/abs/2409.13221)
*Yinmin Zhong,Zili Zhang,Bingyang Wu,Shengyu Liu,Yukun Chen,Changyi Wan,Hanpeng Hu,Lei Xia,Ranchen Ming,Yibo Zhu,Xin Jin*

Main category: cs.LG

TL;DR: RLHFuse is a training system for RLHF that improves GPU utilization by splitting tasks into subtasks and fusing stages, achieving up to 3.7x throughput.


<details>
  <summary>Details</summary>
Motivation: Existing RLHF systems suffer from low GPU utilization due to data skewness and pipeline bubbles.

Method: RLHFuse splits tasks into sample-level subtasks for generation/inference and micro-batches for training, enabling inter-stage and intra-stage fusion.

Result: RLHFuse increases training throughput by up to 3.7x compared to existing systems.

Conclusion: RLHFuse effectively addresses GPU underutilization in RLHF training through innovative stage fusion techniques.

Abstract: We present RLHFuse, an efficient training system with stage fusion for
Reinforcement Learning from Human Feedback (RLHF). Due to the intrinsic nature
of RLHF training, i.e., the data skewness in the generation stage and the
pipeline bubbles in the training stage, existing RLHF systems suffer from low
GPU utilization. RLHFuse breaks the traditional view of RLHF workflow as a
composition of individual tasks, splitting each task into finer-grained
subtasks, and performing stage fusion to improve GPU utilization. RLHFuse
contains two key ideas. First, for generation and inference tasks, RLHFuse
splits them into sample-level subtasks, enabling efficient inter-stage fusion
to overlap the execution of generation and inference stages, thus mitigating
the original generation bottleneck dominated by long-tailed samples. Second,
for training tasks, RLHFuse breaks them into subtasks of micro-batches and
performs intra-stage fusion to concurrently execute these subtasks in the
training stage with a fused pipeline schedule, effectively mitigating the
pipeline bubbles. The experiments show that RLHFuse increases the training
throughput by up to $3.7\times$, compared to existing systems.

</details>


### [263] [Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct](https://arxiv.org/abs/2410.02064)
*Christopher Ackerman,Nina Panickssery*

Main category: cs.LG

TL;DR: LLMs like Llama3-8b-Instruct can recognize their own writing, using post-training experience. A specific vector in the model's residual stream is linked to self-authorship perception and can control the model's behavior and perception.


<details>
  <summary>Details</summary>
Motivation: To investigate LLMs' ability to recognize their own writing and its implications for AI safety, focusing on behavioral robustness, underlying mechanisms, and controllability.

Method: Tested Llama3-8b-Instruct and base Llama3-8b models for writing recognition, analyzed residual stream vectors, and manipulated the identified vector to control behavior and perception.

Result: The chat model reliably distinguishes its outputs from humans, using a specific vector causally linked to self-authorship perception. This vector can control the model's claims and beliefs about authorship.

Conclusion: LLMs can recognize their own writing through learned mechanisms, and this ability can be controlled, offering insights for AI safety and model behavior manipulation.

Abstract: It has been reported that LLMs can recognize their own writing. As this has
potential implications for AI safety, yet is relatively understudied, we
investigate the phenomenon, seeking to establish whether it robustly occurs at
the behavioral level, how the observed behavior is achieved, and whether it can
be controlled. First, we find that the Llama3-8b-Instruct chat model - but not
the base Llama3-8b model - can reliably distinguish its own outputs from those
of humans, and present evidence that the chat model is likely using its
experience with its own outputs, acquired during post-training, to succeed at
the writing recognition task. Second, we identify a vector in the residual
stream of the model that is differentially activated when the model makes a
correct self-written-text recognition judgment, show that the vector activates
in response to information relevant to self-authorship, present evidence that
the vector is related to the concept of "self" in the model, and demonstrate
that the vector is causally related to the model's ability to perceive and
assert self-authorship. Finally, we show that the vector can be used to control
both the model's behavior and its perception, steering the model to claim or
disclaim authorship by applying the vector to the model's output as it
generates it, and steering the model to believe or disbelieve it wrote
arbitrary texts by applying the vector to them as the model reads them.

</details>


### [264] [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://arxiv.org/abs/2410.13828)
*Hui Yuan,Yifan Zeng,Yue Wu,Huazheng Wang,Mengdi Wang,Liu Leqi*

Main category: cs.LG

TL;DR: RLHF's margin-based loss in LM alignment can under-specify ideal behavior, leading to safety risks and reduced preferred responses due to gradient entanglement.


<details>
  <summary>Details</summary>
Motivation: To address the pitfalls of margin-based methods in RLHF, which cause unintended consequences like safety failures and reduced preferred responses.

Method: Analyzes gradient entanglement in margin-based objectives, derives conditions for concerning cases, and validates findings theoretically and empirically.

Result: Identifies gradient entanglement as a key issue, explains training dynamics differences, and suggests algorithm improvements.

Conclusion: Proposes solutions to mitigate under-specification in margin-based methods, enhancing LM alignment.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become the predominant
approach for language model (LM) alignment. At its core, RLHF uses a
margin-based loss for preference optimization, specifying ideal LM behavior
only by the difference between preferred and dispreferred responses. In this
paper, we identify a common pitfall of margin-based methods -- the
under-specification of ideal LM behavior on preferred and dispreferred
responses individually, which leads to two unintended consequences as the
margin increases: (1) The probability of dispreferred (e.g., unsafe) responses
may increase, resulting in potential safety alignment failures. (2) The
probability of preferred responses may decrease, even when those responses are
ideal. We demystify the reasons behind these problematic behaviors:
margin-based losses couple the change in the preferred probability to the
gradient of the dispreferred one, and vice versa, often preventing the
preferred probability from increasing while the dispreferred one decreases, and
thus causing a synchronized increase or decrease in both probabilities. We term
this effect, inherent in margin-based objectives, gradient entanglement.
Formally, we derive conditions for general margin-based alignment objectives
under which gradient entanglement becomes concerning: the inner product of the
gradients of preferred and dispreferred log-probabilities is large relative to
the individual gradient norms. We theoretically investigate why such inner
products can be large when aligning language models and empirically validate
our findings. Empirical implications of our framework extend to explaining
important differences in the training dynamics of various preference
optimization algorithms, and suggesting potential algorithm designs to mitigate
the under-specification issue of margin-based methods and thereby improving
language model alignment.

</details>


### [265] [FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark](https://arxiv.org/abs/2502.19676)
*Zhangdie Yuan,Zifeng Ding,Andreas Vlachos*

Main category: cs.LG

TL;DR: FOReCAst is a new benchmark for evaluating forecasting models' prediction accuracy and confidence calibration across diverse real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting benchmarks lack comprehensive confidence assessment and real-world relevance, limiting their practical utility.

Method: Introduces FOReCAst, a benchmark with diverse forecasting scenarios (Boolean, timeframe, quantity) to assess prediction and confidence.

Result: FOReCAst enables a more comprehensive evaluation of forecasting models for real-world applications.

Conclusion: FOReCAst addresses gaps in current benchmarks, providing a tool for better forecasting model assessment.

Abstract: Forecasting is an important task in many domains, such as technology and
economics. However existing forecasting benchmarks largely lack comprehensive
confidence assessment, focus on limited question types, and often consist of
artificial questions that do not align with real-world human forecasting needs.
To address these gaps, we introduce FOReCAst (Future Outcome Reasoning and
Confidence Assessment), a benchmark that evaluates models' ability to make
predictions and their confidence in them. FOReCAst spans diverse forecasting
scenarios involving Boolean questions, timeframe prediction, and quantity
estimation, enabling a comprehensive evaluation of both prediction accuracy and
confidence calibration for real-world applications.

</details>


### [266] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
*Jianhao Yan,Yafu Li,Zican Hu,Zhi Wang,Ganqu Cui,Xiaoye Qu,Yu Cheng,Yue Zhang*

Main category: cs.LG

TL;DR: LUFFY enhances zero-RL with off-policy reasoning traces, achieving significant gains in math benchmarks and outperforming imitation-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing zero-RL methods are limited to on-policy learning, restricting reasoning ability beyond initial capabilities.

Method: LUFFY combines off-policy demonstrations with on-policy rollouts, using policy shaping via regularized importance sampling.

Result: +7.0 average gain in math benchmarks, +6.2 advantage in out-of-distribution tasks, surpassing SFT in generalization.

Conclusion: LUFFY offers a scalable way to train generalizable reasoning models by balancing imitation and exploration.

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning (RL) with simple rule-based rewards. However,
existing zero-RL approaches are inherently ``on-policy'', limiting learning to
a model's own outputs and failing to acquire reasoning abilities beyond its
initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY
guidance), a framework that augments zero-RL with off-policy reasoning traces.
LUFFY dynamically balances imitation and exploration by combining off-policy
demonstrations with on-policy rollouts during training. Notably, we propose
policy shaping via regularized importance sampling to avoid superficial and
rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an
over +7.0 average gain across six math benchmarks and an advantage of over +6.2
points in out-of-distribution tasks. It also substantially surpasses
imitation-based supervised fine-tuning (SFT), particularly in generalization.
Analysis shows LUFFY not only imitates effectively but also explores beyond
demonstrations, offering a scalable path to train generalizable reasoning
models with off-policy guidance.

</details>


### [267] [MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs](https://arxiv.org/abs/2407.10834)
*Quang H. Nguyen,Thinh Dao,Duy C. Hoang,Juliette Decugis,Saurav Manchanda,Nitesh V. Chawla,Khoa D. Doan*

Main category: cs.LG

TL;DR: MetaLLM is a framework that dynamically routes queries to the optimal LLM for accuracy and cost-effectiveness, using a multi-armed bandit approach.


<details>
  <summary>Details</summary>
Motivation: Choosing the right LLM for each query is challenging due to varying demands and costs, necessitating a dynamic solution.

Method: MetaLLM frames LLM selection as a multi-armed bandit problem, balancing accuracy and cost under uncertainty.

Result: Experiments on platforms like OpenAI and Together AI demonstrate MetaLLM's improved accuracy and cost-effectiveness.

Conclusion: MetaLLM provides a practical solution for dynamic LLM selection, with potential for future extensions.

Abstract: The rapid progress in machine learning (ML) has brought forth many large
language models (LLMs) that excel in various tasks and areas. These LLMs come
with different abilities and costs in terms of computation or pricing. Since
the demand for each query can vary, e.g., because of the queried domain or its
complexity, defaulting to one LLM in an application is not usually the best
choice, whether it is the biggest, priciest, or even the one with the best
average test performance. Consequently, picking the right LLM that is both
accurate and cost-effective for an application is necessary yet remains a
challenge. In this paper, we introduce MetaLLM, a framework that dynamically
and intelligently routes each query to the optimal LLM (among several available
LLMs) for classification and multi-choice question-answering tasks, achieving
significantly improved accuracy and cost-effectiveness. By framing the
selection problem as a multi-armed bandit, MetaLLM balances prediction accuracy
and cost efficiency under uncertainty. Our experiments, conducted on popular
LLM platforms such as OpenAI and Together AI, as well as open-source LLM,
showcase MetaLLM's efficacy in real-world scenarios, laying the groundwork for
future extensions.

</details>


### [268] [Rethinking Soft Actor-Critic in High-Dimensional Action Spaces: The Cost of Ignoring Distribution Shift](https://arxiv.org/abs/2410.16739)
*Yanjun Chen,Xinming Zhang,Xianghui Wang,Zhiqiang Xu,Xiaoyu Shen,Wei Zhang*

Main category: cs.LG

TL;DR: The paper analyzes the distribution shift caused by the tanh transformation in Soft Actor-Critic (SAC), showing it leads to suboptimal actions. Correcting this shift improves SAC's performance.


<details>
  <summary>Details</summary>
Motivation: The tanh transformation in SAC distorts the action distribution, causing misalignment and suboptimal actions, especially in high-dimensional spaces.

Method: Theoretical derivation of the tanh-transformed action's PDF and empirical validation on HumanoidBench tasks.

Result: Accounting for the distribution shift enhances SAC's performance, improving rewards, efficiency, and reliability.

Conclusion: Addressing transformation-induced shifts is crucial for optimizing SAC and similar algorithms in high-dimensional tasks.

Abstract: Soft Actor-Critic algorithm is widely recognized for its robust performance
across a range of deep reinforcement learning tasks, where it leverages the
tanh transformation to constrain actions within bounded limits. However, this
transformation induces a distribution shift, distorting the original Gaussian
action distribution and potentially leading the policy to select suboptimal
actions, particularly in high-dimensional action spaces. In this paper, we
conduct a comprehensive theoretical and empirical analysis of this distribution
shift, deriving the precise probability density function (PDF) for actions
following the tanh transformation to clarify the misalignment introduced
between the transformed distribution's mode and the intended action output. We
substantiate these theoretical insights through extensive experiments on
high-dimensional tasks within the HumanoidBench benchmark. Our findings
indicate that accounting for this distribution shift substantially enhances
SAC's performance, resulting in notable improvements in cumulative rewards,
sample efficiency, and reliability across tasks. These results underscore a
critical consideration for SAC and similar algorithms: addressing
transformation-induced distribution shifts is essential to optimizing policy
effectiveness in high-dimensional deep reinforcement learning environments,
thereby expanding the robustness and applicability of SAC in complex control
tasks.

</details>


### [269] [Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching](https://arxiv.org/abs/2411.07007)
*Arnav Kumar Jain,Harley Wiltzer,Jesse Farebrother,Irina Rish,Glen Berseth,Sanjiban Choudhury*

Main category: cs.LG

TL;DR: A novel non-adversarial IRL method using direct policy optimization, eliminating the need for reward function learning and working with state-only expert demonstrations.


<details>
  <summary>Details</summary>
Motivation: Traditional IRL methods are computationally expensive and unstable due to their adversarial nature. This work aims to simplify and stabilize IRL by avoiding adversarial games and reward function learning.

Method: The approach leverages a linear factorization of return (successor features and reward vector) and uses policy gradient descent to minimize the gap between learner and expert features. It integrates with actor-critic RL algorithms.

Result: The method learns from as few as one expert demonstration, works without expert action labels, and outperforms on various control tasks.

Conclusion: The proposed non-adversarial IRL method is efficient, stable, and effective, even in state-only settings where traditional methods like BC fail.

Abstract: In inverse reinforcement learning (IRL), an agent seeks to replicate expert
demonstrations through interactions with the environment. Traditionally, IRL is
treated as an adversarial game, where an adversary searches over reward models,
and a learner optimizes the reward through repeated RL procedures. This
game-solving approach is both computationally expensive and difficult to
stabilize. In this work, we propose a novel approach to IRL by direct policy
optimization: exploiting a linear factorization of the return as the inner
product of successor features and a reward vector, we design an IRL algorithm
by policy gradient descent on the gap between the learner and expert features.
Our non-adversarial method does not require learning a reward function and can
be solved seamlessly with existing actor-critic RL algorithms. Remarkably, our
approach works in state-only settings without expert action labels, a setting
which behavior cloning (BC) cannot solve. Empirical results demonstrate that
our method learns from as few as a single expert demonstration and achieves
improved performance on various control tasks.

</details>


### [270] [FedSA: A Unified Representation Learning via Semantic Anchors for Prototype-based Federated Learning](https://arxiv.org/abs/2501.05496)
*Yanbing Zhou,Xiangmou Qu,Chenlong You,Jiyang Zhou,Jingyue Tang,Xin Zheng,Chunmao Cai,Yingbo Wu*

Main category: cs.LG

TL;DR: FedSA introduces semantic anchors to decouple prototype generation from local representation learning, addressing inconsistencies in federated learning caused by data and model heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Existing prototype-based federated learning methods suffer from inconsistencies due to biased data distributions and differing model architectures, leading to poor performance.

Method: FedSA uses semantic anchors as prototypes, incorporating anchor-based regularization and contrastive learning to ensure consistent and discriminative representations.

Result: FedSA outperforms existing methods in classification tasks under statistical and model heterogeneity settings.

Conclusion: FedSA effectively breaks the vicious cycle of inconsistency in federated learning, achieving robust generalization through semantic anchors.

Abstract: Prototype-based federated learning has emerged as a promising approach that
shares lightweight prototypes to transfer knowledge among clients with data
heterogeneity in a model-agnostic manner. However, existing methods often
collect prototypes directly from local models, which inevitably introduce
inconsistencies into representation learning due to the biased data
distributions and differing model architectures among clients. In this paper,
we identify that both statistical and model heterogeneity create a vicious
cycle of representation inconsistency, classifier divergence, and skewed
prototype alignment, which negatively impacts the performance of clients. To
break the vicious cycle, we propose a novel framework named Federated Learning
via Semantic Anchors (FedSA) to decouple the generation of prototypes from
local representation learning. We introduce a novel perspective that uses
simple yet effective semantic anchors serving as prototypes to guide local
models in learning consistent representations. By incorporating semantic
anchors, we further propose anchor-based regularization with margin-enhanced
contrastive learning and anchor-based classifier calibration to correct feature
extractors and calibrate classifiers across clients, achieving intra-class
compactness and inter-class separability of prototypes while ensuring
consistent decision boundaries. We then update the semantic anchors with these
consistent and discriminative prototypes, which iteratively encourage clients
to collaboratively learn a unified data representation with robust
generalization. Extensive experiments under both statistical and model
heterogeneity settings show that FedSA significantly outperforms existing
prototype-based FL methods on various classification tasks.

</details>


### [271] [An Operator Splitting View of Federated Learning](https://arxiv.org/abs/2108.05974)
*Saber Malekmohammadi,Kiarash Shaloudegi,Zeou Hu,Yaoliang Yu*

Main category: cs.LG

TL;DR: The paper unifies existing federated learning (FL) algorithms under an operator splitting framework, enabling easier comparison, refined convergence results, and new variants, while highlighting the role of step size and proposing an efficient acceleration method.


<details>
  <summary>Details</summary>
Motivation: The motivation is the fragmented understanding of FL theory and the lack of formal comparisons among existing FL algorithms.

Method: The method involves analyzing FL algorithms from an operator splitting perspective, unifying them for comparison, refining convergence results, and proposing new variants.

Result: The results include a unified understanding of FL algorithms, refined convergence analyses, and a communication-efficient acceleration method.

Conclusion: The conclusion highlights the importance of step size in FL and the benefits of the unified framework for algorithm comparison and acceleration.

Abstract: Over the past few years, the federated learning ($\texttt{FL}$) community has
witnessed a proliferation of new $\texttt{FL}$ algorithms. However, our
understating of the theory of $\texttt{FL}$ is still fragmented, and a
thorough, formal comparison of these algorithms remains elusive. Motivated by
this gap, we show that many of the existing $\texttt{FL}$ algorithms can be
understood from an operator splitting point of view. This unification allows us
to compare different algorithms with ease, to refine previous convergence
results and to uncover new algorithmic variants. In particular, our analysis
reveals the vital role played by the step size in $\texttt{FL}$ algorithms. The
unification also leads to a streamlined and economic way to accelerate
$\texttt{FL}$ algorithms, without incurring any communication overhead. We
perform numerical experiments on both convex and nonconvex models to validate
our findings.

</details>


### [272] [Revolutionizing Wireless Networks with Federated Learning: A Comprehensive Review](https://arxiv.org/abs/2308.04404)
*Sajjad Emdadi Mahdimahalleh*

Main category: cs.LG

TL;DR: Federated Learning (FL) is a novel ML model for wireless edge networks, addressing privacy and resource constraints by decentralizing data computation.


<details>
  <summary>Details</summary>
Motivation: Rising computational capabilities of user devices and privacy concerns drive the need for FL, especially in resource-limited wireless networks.

Method: FL separates data acquisition and computation, scheduling subsets of UEs for updates due to bandwidth constraints and unreliable communication.

Result: FL is highlighted as a key approach for future mobile networks like 6G, balancing privacy and efficiency in wireless communication.

Conclusion: FL presents a promising solution for privacy-preserving and resource-efficient ML in future wireless networks.

Abstract: These days with the rising computational capabilities of wireless user
equipment such as smart phones, tablets, and vehicles, along with growing
concerns about sharing private data, a novel machine learning model called
federated learning (FL) has emerged. FL enables the separation of data
acquisition and computation at the central unit, which is different from
centralized learning that occurs in a data center. FL is typically used in a
wireless edge network where communication resources are limited and unreliable.
Bandwidth constraints necessitate scheduling only a subset of UEs for updates
in each iteration, and because the wireless medium is shared, transmissions are
susceptible to interference and are not assured. The article discusses the
significance of Machine Learning in wireless communication and highlights
Federated Learning (FL) as a novel approach that could play a vital role in
future mobile networks, particularly 6G and beyond.

</details>


### [273] [Building symmetries into data-driven manifold dynamics models for complex flows: application to two-dimensional Kolmogorov flow](https://arxiv.org/abs/2312.10235)
*Carlos E. Pérez De Jesús,Alec J. Linot,Michael D. Graham*

Main category: cs.LG

TL;DR: The paper introduces 'symmetry charting,' a method to create efficient reduced-order models for chaotic flows by leveraging symmetries, improving data sampling, and ensuring equivariance.


<details>
  <summary>Details</summary>
Motivation: To develop data-driven reduced-order models for complex flows that respect inherent symmetries, enhancing efficiency and accuracy in modeling chaotic dynamics.

Method: The approach involves identifying a 'fundamental chart,' using autoencoders for low-dimensional representation, and learning dynamics with neural ODEs, applied to Kolmogorov flow.

Result: The method reduces data requirements, improves manifold dimension estimates, ensures equivariance, and captures accurate short- and long-time dynamics.

Conclusion: Symmetry charting effectively leverages symmetries to enhance reduced-order modeling of chaotic flows, demonstrating improved performance and robustness.

Abstract: Data-driven reduced-order models of the dynamics of complex flows are
important for tasks related to design, understanding, prediction, and control.
Many flows obey symmetries, and the present work illustrates how these can be
exploited to yield highly efficient low-dimensional data-driven models for
chaotic flows. In particular, incorporating symmetries both guarantees that the
reduced order model automatically respects them and dramatically increases the
effective density of data sampling. Given data for the long-time dynamics of a
system, and knowing the set of continuous and discrete symmetries it obeys, the
first step in the methodology is to identify a "fundamental chart", a region in
the state space of the flow to which all other regions can be mapped by a
symmetry operation, and a set of criteria indicating what mapping takes each
point in state space into that chart. We then find a low-dimensional coordinate
representation of the data in the fundamental chart with the use of an
autoencoder architecture that also provides an estimate of the dimension of the
invariant manifold where data lie. Finally, we learn dynamics on this manifold
with the use of neural ordinary differential equations. We apply this method,
denoted "symmetry charting" to simulation data from two-dimensional Kolmogorov
flow in a chaotic bursting regime. This system has a continuous translation
symmetry, and discrete rotation and shift-reflect symmetries. With this
framework we observe that less data is needed to learn accurate data-driven
models, more robust estimates of the manifold dimension are obtained,
equivariance of the NSE is satisfied, better short-time tracking with respect
to the true data is observed, and long-time statistics are correctly captured.

</details>


### [274] [A Pontryagin Perspective on Reinforcement Learning](https://arxiv.org/abs/2405.18100)
*Onno Eberhard,Claire Vernade,Michael Muehlebach*

Main category: cs.LG

TL;DR: The paper introduces open-loop reinforcement learning, learning fixed action sequences instead of state-dependent policies, with three new algorithms outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional reinforcement learning focuses on closed-loop, state-dependent policies; this work explores open-loop learning for fixed action sequences.

Method: Three algorithms are presented: one robust model-based and two sample-efficient model-free methods, based on Pontryagin's principle rather than Bellman's equation.

Result: The methods outperform baselines on pendulum swing-up and high-dimensional MuJoCo tasks, with convergence guarantees provided.

Conclusion: Open-loop reinforcement learning, leveraging Pontryagin's principle, offers a viable alternative to traditional closed-loop methods, demonstrating superior performance.

Abstract: Reinforcement learning has traditionally focused on learning state-dependent
policies to solve optimal control problems in a closed-loop fashion. In this
work, we introduce the paradigm of open-loop reinforcement learning where a
fixed action sequence is learned instead. We present three new algorithms: one
robust model-based method and two sample-efficient model-free methods. Rather
than basing our algorithms on Bellman's equation from dynamic programming, our
work builds on Pontryagin's principle from the theory of open-loop optimal
control. We provide convergence guarantees and evaluate all methods empirically
on a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks,
significantly outperforming existing baselines.

</details>


### [275] [Learning Actionable World Models for Industrial Process Control](https://arxiv.org/abs/2503.01411)
*Peng Yan,Ahmed Abdulkadir,Gerrit A. Schatte,Giulia Aguzzi,Joonsu Gha,Nikola Pascher,Matthias Rosenthal,Yunlong Gao,Benjamin F. Grewe,Thilo Stadelmann*

Main category: cs.LG

TL;DR: The paper proposes a novel AI methodology for active process control by learning disentangled latent representations from limited data, validated on plastic injection molding.


<details>
  <summary>Details</summary>
Motivation: To transition from passive monitoring to active control of complex systems with limited training data, requiring an ad-hoc digital twin for actionable insights.

Method: Uses contrastive learning within a joint embedding predictive architecture to disentangle latent process parameters, enabling predictable and interpretable representations.

Result: Demonstrates effectiveness by proposing specific control actions for plastic injection molding, a notoriously unstable process.

Conclusion: The method enables fine-grained control and interpretability, paving the way for effective process management within operational bounds.

Abstract: To go from (passive) process monitoring to active process control, an
effective AI system must learn about the behavior of the complex system from
very limited training data, forming an ad-hoc digital twin with respect to
process inputs and outputs that captures the consequences of actions on the
process's world. We propose a novel methodology based on learning world models
that disentangles process parameters in the learned latent representation,
allowing for fine-grained control. Representation learning is driven by the
latent factors influencing the processes through contrastive learning within a
joint embedding predictive architecture. This makes changes in representations
predictable from changes in inputs and vice versa, facilitating
interpretability of key factors responsible for process variations, paving the
way for effective control actions to keep the process within operational
bounds. The effectiveness of our method is validated on the example of plastic
injection molding, demonstrating practical relevance in proposing specific
control actions for a notoriously unstable process.

</details>


### [276] [A Scoping Review of Earth Observation and Machine Learning for Causal Inference: Implications for the Geography of Poverty](https://arxiv.org/abs/2406.02584)
*Kazuki Sakamoto,Connor T. Jerzak,Adel Daoud*

Main category: cs.LG

TL;DR: The paper reviews EO-ML methods for causal analysis, identifies five approaches, and provides a protocol for integrating EO data into causal workflows.


<details>
  <summary>Details</summary>
Motivation: To address the lack of documentation and best practices in using EO-ML for causal inference, especially in poverty and health studies.

Method: Conducted a scoping review to catalog EO-ML methods, synthesized five principal approaches, and developed a detailed protocol for researchers.

Result: Identified five key approaches for EO-ML in causal analysis and created a practical protocol for implementation.

Conclusion: The protocol is adaptable for sustainable development domains, offering guidance for integrating EO data into causal analysis.

Abstract: Earth observation (EO) data such as satellite imagery can have far-reaching
impacts on our understanding of the geography of poverty, especially when
coupled with machine learning (ML) and computer vision. Early research used
computer vision to predict living conditions in areas with limited data, but
recent studies increasingly focus on causal analysis. Despite this shift, the
use of EO-ML methods for causal inference lacks thorough documentation, and
best practices are still developing. Through a comprehensive scoping review, we
catalog the current literature on EO-ML methods in causal analysis. We
synthesize five principal approaches to incorporating EO data in causal
workflows: (1) outcome imputation for downstream causal analysis, (2) EO image
deconfounding, (3) EO-based treatment effect heterogeneity, (4) EO-based
transportability analysis, and (5) image-informed causal discovery. Building on
these findings, we provide a detailed protocol guiding researchers in
integrating EO data into causal analysis -- covering data requirements,
computer vision model selection, and evaluation metrics. While our focus
centers on health and living conditions outcomes, our protocol is adaptable to
other sustainable development domains utilizing EO data.

</details>


### [277] [Active Diffusion Subsampling](https://arxiv.org/abs/2406.14388)
*Oisin Nolan,Tristan S. W. Stevens,Wessel L. van Nierop,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: ADS uses guided diffusion for intelligent subsampling by maximizing entropy to reduce uncertainty, producing posterior distributions without task-specific retraining.


<details>
  <summary>Details</summary>
Motivation: To address the high costs of data acquisition by developing a method for estimating fully-sampled signals from partial measurements.

Method: Active Diffusion Subsampling (ADS) leverages guided diffusion to track beliefs over the signal state, actively selecting measurements with maximum entropy.

Result: ADS produces accurate posterior distributions and is interpretable, working with pre-trained models without retraining.

Conclusion: ADS offers a transparent and efficient solution for subsampling, outperforming black-box methods.

Abstract: Subsampling is commonly used to mitigate costs associated with data
acquisition, such as time or energy requirements, motivating the development of
algorithms for estimating the fully-sampled signal of interest $x$ from
partially observed measurements $y$. In maximum entropy sampling, one selects
measurement locations that are expected to have the highest entropy, so as to
minimize uncertainty about $x$. This approach relies on an accurate model of
the posterior distribution over future measurements, given the measurements
observed so far. Recently, diffusion models have been shown to produce
high-quality posterior samples of high-dimensional signals using guided
diffusion. In this work, we propose Active Diffusion Subsampling (ADS), a
method for designing intelligent subsampling masks using guided diffusion in
which the model tracks a distribution of beliefs over the true state of $x$
throughout the reverse diffusion process, progressively decreasing its
uncertainty by actively choosing to acquire measurements with maximum expected
entropy, ultimately producing the posterior distribution $p(x \mid y)$. ADS can
be applied using pre-trained diffusion models for any subsampling rate, and
does not require task-specific retraining - just the specification of a
measurement model. Furthermore, the maximum entropy sampling policy employed by
ADS is interpretable, enhancing transparency relative to existing methods using
black-box policies. Code is available at
https://active-diffusion-subsampling.github.io/.

</details>


### [278] [CHASE: A Causal Hypergraph based Framework for Root Cause Analysis in Multimodal Microservice Systems](https://arxiv.org/abs/2406.19711)
*Ziming Zhao,Zhenwei Wang,Tiehua Zhang,Zhishu Shen,Hai Dong,Zhen Lei,Xingjun Ma,Gaowei Xu,Zhijun Ding,Yun Yang*

Main category: cs.LG

TL;DR: CHASE is a framework for root cause analysis in microservice systems using multimodal data (traces, logs, metrics) and a causal heterogeneous graph, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The complexity of microservice systems makes anomaly detection and root cause analysis challenging, necessitating a robust solution.

Method: CHASE encodes multimodal data into embeddings, models them as a graph, detects anomalies, and localizes root causes using a hypergraph with causal flow.

Result: CHASE outperforms state-of-the-art methods by up to 36.2%(A@1) and 29.4%(Percentage@1) on two datasets.

Conclusion: CHASE effectively addresses root cause analysis in microservice systems, demonstrating superior performance.

Abstract: In recent years, the widespread adoption of distributed microservice
architectures within the industry has significantly increased the demand for
enhanced system availability and robustness. Due to the complex service
invocation paths and dependencies in enterprise-level microservice systems, it
is challenging to locate the anomalies promptly during service invocations,
thus causing intractable issues for normal system operations and maintenance.
In this paper, we propose a Causal Heterogeneous grAph baSed framEwork for root
cause analysis, namely CHASE, for microservice systems with multimodal data,
including traces, logs, and system monitoring metrics. Specifically, related
information is encoded into representative embeddings and further modeled by a
multimodal invocation graph. Following that, anomaly detection is performed on
each instance node with attentive heterogeneous message passing from its
adjacent metric and log nodes. Finally, CHASE learns from the constructed
hypergraph with hyperedges representing the flow of causality and performs root
cause localization. We evaluate the proposed framework on two public
microservice datasets with distinct attributes and compare with the
state-of-the-art methods. The results show that CHASE achieves the average
performance gain up to 36.2%(A@1) and 29.4%(Percentage@1), respectively to its
best counterpart.

</details>


### [279] [SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions](https://arxiv.org/abs/2504.02698)
*Shengrui XU,Tianchi Lu,Zikun Wang,Jixiu Zhai,Jingwan Wang*

Main category: cs.LG

TL;DR: SCMPPI is a novel supervised contrastive multimodal framework for PPI prediction, integrating sequence and network features with enhanced contrastive learning, achieving high accuracy and cross-species generalization.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional methods in cross-modal feature fusion and false-negative suppression for PPI prediction.

Method: Integrates sequence-based features (AAC, DPC, ESMC-CKSAAP) with network topology (Node2Vec embeddings) and uses enhanced contrastive learning with negative sample filtering.

Result: Achieves state-of-the-art accuracy (98.13%) and AUC (99.69%), with strong cross-species generalization (AUC>99%).

Conclusion: SCMPPI is a powerful tool for multimodal biological data analysis, with demonstrated applications in disease target discovery.

Abstract: Protein-protein interaction (PPI) prediction plays a pivotal role in
deciphering cellular functions and disease mechanisms. To address the
limitations of traditional experimental methods and existing computational
approaches in cross-modal feature fusion and false-negative suppression, we
propose SCMPPI-a novel supervised contrastive multimodal framework. By
effectively integrating sequence-based features (AAC, DPC, ESMC-CKSAAP) with
network topology (Node2Vec embeddings) and incorporating an enhanced
contrastive learning strategy with negative sample filtering, SCMPPI achieves
superior prediction performance. Extensive experiments on eight benchmark
datasets demonstrate its state-of-the-art accuracy(98.13%) and AUC(99.69%),
along with excellent cross-species generalization (AUC>99%). Successful
applications in CD9 networks, Wnt pathway analysis, and cancer-specific
networks further highlight its potential for disease target discovery,
establishing SCMPPI as a powerful tool for multimodal biological data analysis.

</details>


### [280] [Peer-to-Peer Learning Dynamics of Wide Neural Networks](https://arxiv.org/abs/2409.15267)
*Shreyas Chaudhari,Srinivasa Pranav,Emile Anand,José M. F. Moura*

Main category: cs.LG

TL;DR: The paper analyzes the training dynamics of wide neural networks in peer-to-peer learning environments using distributed gradient descent (DGD) and neural tangent kernel (NTK) theory.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of tuning neural network architectures and hyperparameters in distributed edge environments like smart cities, ensuring privacy and efficiency.

Method: Leverages NTK theory and distributed learning consensus to characterize the learning dynamics of wide neural networks trained with DGD.

Result: Provides an explicit characterization of training dynamics, validated by predicting parameter and error dynamics in classification tasks.

Conclusion: The work successfully bridges NTK theory and distributed learning, offering insights for optimizing peer-to-peer neural network training.

Abstract: Peer-to-peer learning is an increasingly popular framework that enables
beyond-5G distributed edge devices to collaboratively train deep neural
networks in a privacy-preserving manner without the aid of a central server.
Neural network training algorithms for emerging environments, e.g., smart
cities, have many design considerations that are difficult to tune in
deployment settings -- such as neural network architectures and
hyperparameters. This presents a critical need for characterizing the training
dynamics of distributed optimization algorithms used to train highly nonconvex
neural networks in peer-to-peer learning environments. In this work, we provide
an explicit characterization of the learning dynamics of wide neural networks
trained using popular distributed gradient descent (DGD) algorithms. Our
results leverage both recent advancements in neural tangent kernel (NTK) theory
and extensive previous work on distributed learning and consensus. We validate
our analytical results by accurately predicting the parameter and error
dynamics of wide neural networks trained for classification tasks.

</details>


### [281] [Vertical Federated Learning with Missing Features During Training and Inference](https://arxiv.org/abs/2410.22564)
*Pedro Valdeira,Shiqiang Wang,Yuejie Chi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vertical federated learning trains models from feature-partitioned datasets
across multiple clients, who collaborate without sharing their local data.
Standard approaches assume that all feature partitions are available during
both training and inference. Yet, in practice, this assumption rarely holds, as
for many samples only a subset of the clients observe their partition. However,
not utilizing incomplete samples during training harms generalization, and not
supporting them during inference limits the utility of the model. Moreover, if
any client leaves the federation after training, its partition becomes
unavailable, rendering the learned model unusable. Missing feature blocks are
therefore a key challenge limiting the applicability of vertical federated
learning in real-world scenarios. To address this, we propose LASER-VFL, a
vertical federated learning method for efficient training and inference of
split neural network-based models that is capable of handling arbitrary sets of
partitions. Our approach is simple yet effective, relying on the sharing of
model parameters and on task-sampling to train a family of predictors. We show
that LASER-VFL achieves a $\mathcal{O}({1}/{\sqrt{T}})$ convergence rate for
nonconvex objectives and, under the Polyak-{\L}ojasiewicz inequality, it
achieves linear convergence to a neighborhood of the optimum. Numerical
experiments show improved performance of LASER-VFL over the baselines.
Remarkably, this is the case even in the absence of missing features. For
example, for CIFAR-100, we see an improvement in accuracy of $19.3\%$ when each
of four feature blocks is observed with a probability of 0.5 and of $9.5\%$
when all features are observed. The code for this work is available at
https://github.com/Valdeira/LASER-VFL.

</details>


### [282] [Graph Neural Network Surrogates to leverage Mechanistic Expert Knowledge towards Reliable and Immediate Pandemic Response](https://arxiv.org/abs/2411.06500)
*Agatha Schmidt,Henrik Zunker,Alexander Heinlein,Martin J. Kühn*

Main category: cs.LG

TL;DR: The paper proposes coupling complex mechanistic models with data-driven surrogate models, like graph neural networks, to speed up pandemic decision-making by enabling rapid, on-the-fly adaptations.


<details>
  <summary>Details</summary>
Motivation: Time-critical decisions during COVID-19 require fast, adaptable models for heterogeneous infectious disease dynamics, but traditional models are computationally slow.

Method: A graph neural network is trained on data from a spatially and demographically resolved metapopulation model to create a surrogate model.

Result: The surrogate model speeds up execution by up to four orders of magnitude, enabling near-instantaneous results.

Conclusion: This approach facilitates low-barrier integration into web applications for real-time pandemic decision-making.

Abstract: During the COVID-19 crisis, mechanistic models have guided evidence-based
decision making. However, time-critical decisions in a dynamical environment
limit the time available to gather supporting evidence. Infectious disease
dynamics are often heterogeneous on a spatial or demographic scale, requiring
appropriately resolved models. In addition, with a large number of potential
interventions, all scenarios can barely be computed on time, even when using
supercomputing facilities. We suggest to couple complex mechanistic models with
data-driven surrogate models to allow for on-the-fly model adaptations by
public health experts and decision makers. We build upon a spatially and
demographically resolved infectious disease metapopulation model and train a
graph neural network for data sets representing prevaccination phases of a
pandemic. The resulting networks reached an execution time of a fraction of a
second, a speeding up the metapopulation up to four orders of magnitude. The
approach yields large potential for on-the-fly execution and, thus, facilitates
integration into low-barrier web applications for use in pandemic
decision-making.

</details>


### [283] [Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models](https://arxiv.org/abs/2504.13945)
*Zhanglin Wu,Tengfei Song,Ning Xie,Weidong Zhang,Mengli Zhu,Shuang Wu,Shiliang Sun,Hao Yang*

Main category: cs.LG

TL;DR: The paper introduces MOTBench, a benchmark for evaluating large vision-language models (LVLMs) on menu translation tasks, addressing the gap in assessing complex layout understanding.


<details>
  <summary>Details</summary>
Motivation: Current LVLM evaluations focus on simple layouts, overlooking complex designs like menus, which are crucial for cross-cultural communication.

Method: MOTBench uses Chinese and English menus with intricate layouts, fonts, and cultural elements, requiring LVLMs to recognize and translate dish details accurately.

Result: Automatic evaluations align closely with human assessments, revealing strengths and weaknesses in state-of-the-art LVLMs.

Conclusion: MOTBench provides a specialized tool to advance LVLM development by addressing complex layout understanding in document tasks.

Abstract: The rapid advancement of large vision-language models (LVLMs) has
significantly propelled applications in document understanding, particularly in
optical character recognition (OCR) and multilingual translation. However,
current evaluations of LVLMs, like the widely used OCRBench, mainly focus on
verifying the correctness of their short-text responses and long-text responses
with simple layout, while the evaluation of their ability to understand long
texts with complex layout design is highly significant but largely overlooked.
In this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a
specialized evaluation framework emphasizing the pivotal role of menu
translation in cross-cultural communication. MOTBench requires LVLMs to
accurately recognize and translate each dish, along with its price and unit
items on a menu, providing a comprehensive assessment of their visual
understanding and language processing capabilities. Our benchmark is comprised
of a collection of Chinese and English menus, characterized by intricate
layouts, a variety of fonts, and culturally specific elements across different
languages, along with precise human annotations. Experiments show that our
automatic evaluation results are highly consistent with professional human
evaluation. We evaluate a range of publicly available state-of-the-art LVLMs,
and through analyzing their output to identify the strengths and weaknesses in
their performance, offering valuable insights to guide future advancements in
LVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.

</details>


### [284] [Exploring the loss landscape of regularized neural networks via convex duality](https://arxiv.org/abs/2411.07729)
*Sungyoon Kim,Aaron Mishkin,Mert Pilanci*

Main category: cs.LG

TL;DR: The paper explores the loss landscape of regularized neural networks, focusing on stationary points, connectivity of solutions, and nonuniqueness of optima by leveraging convex duality. It reveals phase transitions in optima topology with network width and extends findings to various architectures.


<details>
  <summary>Details</summary>
Motivation: To understand the structure and properties of the loss landscape in regularized neural networks, particularly the behavior of stationary points and optimal solutions.

Method: The problem is cast into an equivalent convex problem using duality. Analysis starts with two-layer scalar-output networks, then extends to vector-valued and three-layer networks.

Result: Characterization of stationary points and solution sets, phase transitions in optima topology with network width, and examples of continuum optimal solutions.

Conclusion: The findings generalize to diverse architectures, providing insights into the loss landscape and connectivity of optimal solutions in neural networks.

Abstract: We discuss several aspects of the loss landscape of regularized neural
networks: the structure of stationary points, connectivity of optimal
solutions, path with nonincreasing loss to arbitrary global optimum, and the
nonuniqueness of optimal solutions, by casting the problem into an equivalent
convex problem and considering its dual. Starting from two-layer neural
networks with scalar output, we first characterize the solution set of the
convex problem using its dual and further characterize all stationary points.
With the characterization, we show that the topology of the global optima goes
through a phase transition as the width of the network changes, and construct
counterexamples where the problem may have a continuum of optimal solutions.
Finally, we show that the solution set characterization and connectivity
results can be extended to different architectures, including two-layer
vector-valued neural networks and parallel three-layer neural networks.

</details>


### [285] [Federated Automated Feature Engineering](https://arxiv.org/abs/2412.04404)
*Tom Overman,Diego Klabjan*

Main category: cs.LG

TL;DR: The paper introduces AutoFE algorithms for federated learning (FL) settings, including horizontal, vertical, and hybrid FL, achieving performance close to centralized AutoFE.


<details>
  <summary>Details</summary>
Motivation: Existing AutoFE methods lack solutions for FL settings where data is distributed across clients without sharing.

Method: Developed AutoFE algorithms for horizontal, vertical, and hybrid FL settings.

Result: Federated AutoFE algorithms perform nearly as well as centralized AutoFE.

Conclusion: The work pioneers AutoFE for FL, bridging the gap between distributed and centralized feature engineering.

Abstract: Automated feature engineering (AutoFE) is used to automatically create new
features from original features to improve predictive performance without
needing significant human intervention and domain expertise. Many algorithms
exist for AutoFE, but very few approaches exist for the federated learning (FL)
setting where data is gathered across many clients and is not shared between
clients or a central server. We introduce AutoFE algorithms for the horizontal,
vertical, and hybrid FL settings, which differ in how the data is gathered
across clients. To the best of our knowledge, we are the first to develop
AutoFE algorithms for the horizontal and hybrid FL cases, and we show that the
downstream test scores of our federated AutoFE algorithms is close in
performance to the case where data is held centrally and AutoFE is performed
centrally.

</details>


### [286] [Trading Devil RL: Backdoor attack via Stock market, Bayesian Optimization and Reinforcement Learning](https://arxiv.org/abs/2412.17908)
*Orson Mengara*

Main category: cs.LG

TL;DR: Proposes a backdoor attack (FinanceLLMsBackRL) targeting data poisoning in financial AI models, with detection via dynamic systems and statistical analysis.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of backdoor attacks on large language models using reinforcement learning in finance and other fields.

Method: Focuses on data poisoning and employs dynamic systems and statistical analysis for detection.

Result: Introduces FinanceLLMsBackRL, a backdoor attack without prior triggers, highlighting vulnerabilities in AI models.

Conclusion: Emphasizes the need for robust detection methods to safeguard AI systems in finance and beyond.

Abstract: With the rapid development of generative artificial intelligence,
particularly large language models a number of sub-fields of deep learning have
made significant progress and are now very useful in everyday applications. For
example,financial institutions simulate a wide range of scenarios for various
models created by their research teams using reinforcement learning, both
before production and after regular operations. In this work, we propose a
backdoor attack that focuses solely on data poisoning and a method of detection
by dynamic systems and statistical analysis of the distribution of data. This
particular backdoor attack is classified as an attack without prior
consideration or trigger, and we name it FinanceLLMsBackRL. Our aim is to
examine the potential effects of large language models that use reinforcement
learning systems for text production or speech recognition, finance, physics,
or the ecosystem of contemporary artificial intelligence models.

</details>


### [287] [Orthogonal projection-based regularization for efficient model augmentation](https://arxiv.org/abs/2501.05842)
*Bendegúz M. Györök,Jan H. Hoekstra,Johan Kon,Tamás Péni,Maarten Schoukens,Roland Tóth*

Main category: cs.LG

TL;DR: The paper proposes an orthogonal projection-based regularization technique to improve parameter learning and model accuracy in deep-learning-augmented nonlinear system identification, addressing overparametrization and interpretability issues.


<details>
  <summary>Details</summary>
Motivation: Deep-learning models lack physical interpretability and often waste effort on known system behaviors. Integrating prior physics knowledge can combine the strengths of physics-based and data-driven modeling.

Method: The paper introduces an orthogonal projection-based regularization technique for additive model augmentation, where physics-based and ML components are connected in parallel.

Result: The proposed technique enhances parameter learning and model accuracy while preserving the interpretability of the physics-based component.

Conclusion: The orthogonal projection-based regularization effectively addresses challenges in training overparametrized models, improving both performance and interpretability.

Abstract: Deep-learning-based nonlinear system identification has shown the ability to
produce reliable and highly accurate models in practice. However, these
black-box models lack physical interpretability, and a considerable part of the
learning effort is often spent on capturing already expected/known behavior of
the system, that can be accurately described by first-principles laws of
physics. A potential solution is to directly integrate such prior physical
knowledge into the model structure, combining the strengths of physics-based
modeling and deep-learning-based identification. The most common approach is to
use an additive model augmentation structure, where the physics-based and the
machine-learning (ML) components are connected in parallel, i.e., additively.
However, such models are overparametrized, training them is challenging,
potentially causing the physics-based part to lose interpretability. To
overcome this challenge, this paper proposes an orthogonal projection-based
regularization technique to enhance parameter learning and even model accuracy
in learning-based augmentation of nonlinear baseline models.

</details>


### [288] [Sparse L0-norm based Kernel-free Quadratic Surface Support Vector Machines](https://arxiv.org/abs/2501.11268)
*Ahmad Mousavi,Ramin Zandvakili*

Main category: cs.LG

TL;DR: Proposes a sparse ℓ₀-norm based Kernel-free quadratic SVM to reduce overfitting and improve interpretability, using a penalty decomposition algorithm for efficient optimization.


<details>
  <summary>Details</summary>
Motivation: Addressing overfitting and complexity in Kernel-free quadratic SVMs due to quadratic parameter expansion.

Method: Sparse ℓ₀-norm regularization with a penalty decomposition algorithm for efficient optimization, leveraging closed-form solutions and duality theory.

Result: Empirical evaluations show efficacy and robustness in mitigating overfitting while maintaining performance.

Conclusion: The approach advances Kernel-free quadratic SVMs by balancing complexity and interpretability, with practical applicability demonstrated.

Abstract: Kernel-free quadratic surface support vector machine (SVM) models have gained
significant attention in machine learning. However, introducing a quadratic
classifier increases the model's complexity by quadratically expanding the
number of parameters relative to the dimensionality of the data, exacerbating
overfitting. Hence, we propose sparse $\ell_0$-norm based Kernel-free quadratic
surface SVMs, designed to mitigate overfitting and enhance interpretability.
Given the intractable nature of these models, we present a penalty
decomposition algorithm to obtain first-order optimality points efficiently. We
demonstrate that the subproblems in our framework either admit closed-form
solutions or can leverage duality theory to improve computational efficiency.
Through empirical evaluations on real-world datasets, we demonstrate the
efficacy and robustness of our approach, showcasing its potential to advance
Kernel-free quadratic surface SVMs in practical applications while addressing
overfitting concerns. All the implemented models and experiment codes are
available at https://github.com/raminzandvakili/L0-QSVM.

</details>


### [289] [On the Guidance of Flow Matching](https://arxiv.org/abs/2502.02150)
*Ruiqi Feng,Tailin Wu,Chenglei Yu,Wenhao Deng,Peiyan Hu*

Main category: cs.LG

TL;DR: The paper introduces a general guidance framework for flow matching, deriving various guidance techniques, including training-free and training-based methods, and validates their effectiveness across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Flow matching outperforms diffusion models in generative tasks, but its guidance remains underexplored. This paper aims to address this gap.

Method: Proposes a framework for general flow matching guidance, including training-free asymptotically exact guidance, novel training losses, and approximate guidance methods.

Result: The framework and derived methods are validated on synthetic datasets, image inverse problems, and offline reinforcement learning, showing effectiveness.

Conclusion: The paper provides a practical guideline for choosing guidance methods in flow matching, supported by theoretical and experimental results.

Abstract: Flow matching has shown state-of-the-art performance in various generative
tasks, ranging from image generation to decision-making, where guided
generation is pivotal. However, the guidance of flow matching is more general
than and thus substantially different from that of its predecessor, diffusion
models. Therefore, the challenge in guidance for general flow matching remains
largely underexplored. In this paper, we propose the first framework of general
guidance for flow matching. From this framework, we derive a family of guidance
techniques that can be applied to general flow matching. These include a new
training-free asymptotically exact guidance, novel training losses for
training-based guidance, and two classes of approximate guidance that cover
classical gradient guidance methods as special cases. We theoretically
investigate these different methods to give a practical guideline for choosing
suitable methods in different scenarios. Experiments on synthetic datasets,
image inverse problems, and offline reinforcement learning demonstrate the
effectiveness of our proposed guidance methods and verify the correctness of
our flow matching guidance framework. Code to reproduce the experiments can be
found at https://github.com/AI4Science-WestlakeU/flow_guidance.

</details>


### [290] [CVKAN: Complex-Valued Kolmogorov-Arnold Networks](https://arxiv.org/abs/2502.02417)
*Matthias Wolff,Florian Eilers,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: CVKAN combines interpretability of KANs with advantages of CVNNs, performing better or on par with real-valued KANs while requiring fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To merge the interpretability of KANs and the benefits of CVNNs in the complex domain.

Method: Transfer KAN and associated mechanisms into the complex domain, validated via symbolic function fitting and knot theory datasets.

Result: CVKAN is more stable, performs comparably or better, and uses fewer parameters than real-valued KANs.

Conclusion: CVKAN offers improved performance and explainability in complex-valued tasks.

Abstract: In this work we propose CVKAN, a complex-valued Kolmogorov-Arnold Network
(KAN), to join the intrinsic interpretability of KANs and the advantages of
Complex-Valued Neural Networks (CVNNs). We show how to transfer a KAN and the
necessary associated mechanisms into the complex domain. To confirm that CVKAN
meets expectations we conduct experiments on symbolic complex-valued function
fitting and physically meaningful formulae as well as on a more realistic
dataset from knot theory. Our proposed CVKAN is more stable and performs on par
or better than real-valued KANs while requiring less parameters and a shallower
network architecture, making it more explainable.

</details>


### [291] [Selective Task Group Updates for Multi-Task Optimization](https://arxiv.org/abs/2502.11986)
*Wooseong Jeong,Kuk-Jin Yoon*

Main category: cs.LG

TL;DR: The paper proposes a method to mitigate negative transfer in multi-task learning by adaptively grouping tasks and updating them selectively during optimization, outperforming previous approaches.


<details>
  <summary>Details</summary>
Motivation: Negative transfer in multi-task learning degrades performance, and existing methods fail to address task-specific parameter learning.

Method: Introduces an algorithm for adaptive task grouping and updates, using proximal inter-task affinity to track relations.

Result: The method significantly improves multi-task performance and scales across architectures and task numbers.

Conclusion: Selective task grouping and updating enhances task-specific learning, outperforming prior multi-task optimization strategies.

Abstract: Multi-task learning enables the acquisition of task-generic knowledge by
training multiple tasks within a unified architecture. However, training all
tasks together in a single architecture can lead to performance degradation,
known as negative transfer, which is a main concern in multi-task learning.
Previous works have addressed this issue by optimizing the multi-task network
through gradient manipulation or weighted loss adjustments. However, their
optimization strategy focuses on addressing task imbalance in shared
parameters, neglecting the learning of task-specific parameters. As a result,
they show limitations in mitigating negative transfer, since the learning of
shared space and task-specific information influences each other during
optimization. To address this, we propose a different approach to enhance
multi-task performance by selectively grouping tasks and updating them for each
batch during optimization. We introduce an algorithm that adaptively determines
how to effectively group tasks and update them during the learning process. To
track inter-task relations and optimize multi-task networks simultaneously, we
propose proximal inter-task affinity, which can be measured during the
optimization process. We provide a theoretical analysis on how dividing tasks
into multiple groups and updating them sequentially significantly affects
multi-task performance by enhancing the learning of task-specific parameters.
Our methods substantially outperform previous multi-task optimization
approaches and are scalable to different architectures and various numbers of
tasks.

</details>


### [292] [Not All Edges are Equally Robust: Evaluating the Robustness of Ranking-Based Federated Learning](https://arxiv.org/abs/2503.08976)
*Zirui Gong,Yanjun Zhang,Leo Yu Zhang,Zhaoxi Zhang,Yong Xiang,Shirui Pan*

Main category: cs.LG

TL;DR: Federated Ranking Learning (FRL) is efficient and resilient but has vulnerabilities to poisoning attacks, demonstrated by the VEM attack, which is 3.7x more impactful than existing methods.


<details>
  <summary>Details</summary>
Motivation: To analyze the robustness of FRL and identify its vulnerabilities to poisoning attacks.

Method: Theoretical investigation of vulnerable edges, introduction of the VEM attack, and experimental validation on benchmark datasets.

Result: VEM achieves a 53.23% attack impact, 3.7x more than existing methods, revealing FRL's vulnerabilities.

Conclusion: FRL has significant vulnerabilities, necessitating the development of more robust federated learning frameworks.

Abstract: Federated Ranking Learning (FRL) is a state-of-the-art FL framework that
stands out for its communication efficiency and resilience to poisoning
attacks. It diverges from the traditional FL framework in two ways: 1) it
leverages discrete rankings instead of gradient updates, significantly reducing
communication costs and limiting the potential space for malicious updates, and
2) it uses majority voting on the server side to establish the global ranking,
ensuring that individual updates have minimal influence since each client
contributes only a single vote. These features enhance the system's scalability
and position FRL as a promising paradigm for FL training.
  However, our analysis reveals that FRL is not inherently robust, as certain
edges are particularly vulnerable to poisoning attacks. Through a theoretical
investigation, we prove the existence of these vulnerable edges and establish a
lower bound and an upper bound for identifying them in each layer. Based on
this finding, we introduce a novel local model poisoning attack against FRL,
namely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on
identifying and perturbing the most vulnerable edges in each layer and
leveraging an optimization-based approach to maximize the attack's impact.
Through extensive experiments on benchmark datasets, we demonstrate that our
attack achieves an overall 53.23% attack impact and is 3.7x more impactful than
existing methods. Our findings highlight significant vulnerabilities in
ranking-based FL systems and underline the urgency for the development of new
robust FL frameworks.

</details>


### [293] [Accelerating Transient CFD through Machine Learning-Based Flow Initialization](https://arxiv.org/abs/2503.15766)
*Peter Sharpe,Rishikesh Ranade,Kaustubh Tangsali,Mohammad Amin Nabian,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: A novel ML-based initialization method reduces CFD simulation time by 50% compared to traditional methods, using physics-informed and versatile strategies.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational cost of transient CFD simulations by improving initialization methods.

Method: Three ML-based initialization strategies are evaluated, with two recommended: a physics-informed hybrid method and a versatile approach integrating ML with uniform flow.

Result: Achieves a 50% reduction in time-to-convergence, with strong generalization from training on different datasets.

Conclusion: The methods integrate seamlessly with existing workflows, offering practical acceleration for industrial CFD simulations.

Abstract: Transient computational fluid dynamics (CFD) simulations are essential for
many industrial applications, but a significant portion of their computational
cost stems from the time needed to reach statistical steadiness from initial
conditions. We present a novel machine learning-based initialization method
that reduces the cost of this subsequent transient solve substantially,
achieving a 50% reduction in time-to-convergence compared to traditional
uniform and potential flow-based initializations. Through a case study in
automotive aerodynamics using a 16.7M-cell unsteady RANS simulation, we
evaluate three ML-based initialization strategies. Two of these strategies are
recommended for general use: (1) a physics-informed hybrid method combining ML
predictions with potential flow solutions, and (2) a more versatile approach
integrating ML predictions with uniform flow. Both strategies enable CFD
solvers to achieve convergence times comparable to computationally expensive
steady RANS initializations, while requiring only seconds of computation. We
develop a robust statistical convergence metric based on windowed
time-averaging for performance comparison between initialization strategies.
Notably, these improvements are achieved using an ML model trained on a
different dataset of automotive geometries, demonstrating strong generalization
capabilities. The proposed methods integrate seamlessly with existing CFD
workflows without requiring modifications to the underlying flow solver,
providing a practical approach to accelerating industrial CFD simulations
through improved ML-based initialization strategies.

</details>


### [294] [Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated Learning](https://arxiv.org/abs/2504.05138)
*Haoran Zhang,Zejun Gong,Zekai Li,Marie Siew,Carlee Joe-Wong,Rachid El-Azouzi*

Main category: cs.LG

TL;DR: The paper introduces multi-model federated learning (MMFL) to train multiple models concurrently, addressing communication constraints. It proposes MMFL-LVR and variants for efficient client selection, improving accuracy by up to 19.1%.


<details>
  <summary>Details</summary>
Motivation: Clients in federated learning (FL) face communication constraints when training multiple models. Sequential training is inefficient, and naive multi-model approaches violate resource limits. The goal is to optimize client-to-model assignments for faster convergence.

Method: The paper develops a convergence analysis for MMFL and proposes MMFL-LVR, a loss-based sampling method. Variants MMFL-StaleVR and MMFL-StaleVRE incorporate stale updates for efficiency and stability.

Result: Experiments show the methods improve average accuracy by up to 19.1% over random sampling, with only a 5.4% gap from the theoretical optimum.

Conclusion: The proposed methods optimize client selection in MMFL, significantly improving training efficiency and accuracy while respecting resource constraints.

Abstract: Federated learning (FL) allows edge devices to collaboratively train models
without sharing local data. As FL gains popularity, clients may need to train
multiple unrelated FL models, but communication constraints limit their ability
to train all models simultaneously. While clients could train FL models
sequentially, opportunistically having FL clients concurrently train different
models -- termed multi-model federated learning (MMFL) -- can reduce the
overall training time. Prior work uses simple client-to-model assignments that
do not optimize the contribution of each client to each model over the course
of its training. Prior work on single-model FL shows that intelligent client
selection can greatly accelerate convergence, but na\"ive extensions to MMFL
can violate heterogeneous resource constraints at both the server and the
clients. In this work, we develop a novel convergence analysis of MMFL with
arbitrary client sampling methods, theoretically demonstrating the strengths
and limitations of previous well-established gradient-based methods. Motivated
by this analysis, we propose MMFL-LVR, a loss-based sampling method that
minimizes training variance while explicitly respecting communication limits at
the server and reducing computational costs at the clients. We extend this to
MMFL-StaleVR, which incorporates stale updates for improved efficiency and
stability, and MMFL-StaleVRE, a lightweight variant suitable for low-overhead
deployment. Experiments show our methods improve average accuracy by up to
19.1% over random sampling, with only a 5.4% gap from the theoretical optimum
(full client participation).

</details>


### [295] [Optimal Bayesian Affine Estimator and Active Learning for the Wiener Model](https://arxiv.org/abs/2504.05490)
*Sasan Vakili,Manuel Mazo Jr.,Peyman Mohajerin Esfahani*

Main category: cs.LG

TL;DR: A Bayesian framework for Wiener models is introduced, focusing on nonlinear output functions with known linear dynamics. It features a closed-form optimal affine estimator using dynamic basis statistics (DBS), with properties like unbiasedness and error monotonicity. Active learning minimizes estimation error, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning nonlinear output functions in Wiener models under known linear dynamics, leveraging Bayesian estimation for improved accuracy and efficiency.

Method: Derives a closed-form optimal affine estimator using dynamic basis statistics (DBS), analyzes its properties, and develops an active learning algorithm to minimize estimation error.

Result: The estimator shows Bayesian unbiasedness, closed-form posterior statistics, and improved performance over traditional methods, validated by numerical experiments.

Conclusion: The proposed Bayesian framework and active learning approach effectively enhance estimation accuracy for Wiener models, particularly with Fourier basis functions.

Abstract: This paper presents a Bayesian estimation framework for Wiener models,
focusing on learning nonlinear output functions under known linear state
dynamics. We derive a closed-form optimal affine estimator for the unknown
parameters, characterized by the so-called "dynamic basis statistics" (DBS).
Several features of the proposed estimator are studied, including Bayesian
unbiasedness, closed-form posterior statistics, error monotonicity in
trajectory length, and consistency condition (also known as persistent
excitation). In the special case of Fourier basis functions, we demonstrate
that the closed-form description is computationally available, as the Fourier
DBS enjoys explicit expressions. Furthermore, we identify an inherent
inconsistency in the Fourier bases for single-trajectory measurements,
regardless of the input excitation. Leveraging the closed-form estimation
error, we develop an active learning algorithm synthesizing input signals to
minimize estimation error. Numerical experiments validate the efficacy of our
approach, showing significant improvements over traditional regularized
least-squares methods.

</details>


### [296] [Follow-the-Perturbed-Leader Approaches Best-of-Both-Worlds for the m-Set Semi-Bandit Problems](https://arxiv.org/abs/2504.07307)
*Jingxin Zhan,Yuchen Xin,Zhihua Zhang*

Main category: cs.LG

TL;DR: FTPL with Fréchet perturbation achieves near-optimal regret bounds in adversarial settings and logarithmic regret in stochastic settings for the m-set semi-bandit problem.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency of the Follow-the-Perturbed-Leader (FTPL) policy in combinatorial semi-bandit problems, avoiding the computational burden of FTRL while maintaining strong regret bounds.

Method: The paper uses FTPL with Fréchet perturbation, selecting arms based on perturbed loss estimates, to achieve efficient regret bounds.

Result: FTPL with Fréchet perturbation achieves a regret bound of O(√(nmd log(d))) in adversarial settings and logarithmic regret in stochastic settings.

Conclusion: FTPL with Fréchet perturbation is a computationally efficient alternative to FTRL, offering near-optimal regret bounds in adversarial settings and adapting well to stochastic settings.

Abstract: We consider a common case of the combinatorial semi-bandit problem, the
$m$-set semi-bandit, where the learner exactly selects $m$ arms from the total
$d$ arms. In the adversarial setting, the best regret bound, known to be
$\mathcal{O}(\sqrt{nmd})$ for time horizon $n$, is achieved by the well-known
Follow-the-Regularized-Leader (FTRL) policy. However, this requires to
explicitly compute the arm-selection probabilities via optimizing problems at
each time step and sample according to them. This problem can be avoided by the
Follow-the-Perturbed-Leader (FTPL) policy, which simply pulls the $m$ arms that
rank among the $m$ smallest (estimated) loss with random perturbation. In this
paper, we show that FTPL with a Fr\'echet perturbation also enjoys the near
optimal regret bound $\mathcal{O}(\sqrt{nmd\log(d)})$ in the adversarial
setting and approaches best-of-both-world regret bounds, i.e., achieves a
logarithmic regret for the stochastic setting.

</details>


### [297] [Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning](https://arxiv.org/abs/2504.09192)
*Zhiyong Wang*

Main category: cs.LG

TL;DR: Developing efficient, robust, and adaptive online learning algorithms for RL and bandits to address real-world challenges like model misspecifications and adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms often rely on idealized models, failing under real-world uncertainties and lacking instance-dependent performance. Robust, adaptive solutions are needed for dynamic systems.

Method: Focus on reinforcement learning (RL) and multi-armed bandits, aiming to improve efficiency, robustness, and adaptability in uncertain environments.

Result: Expected outcomes include more practical and reliable algorithms for applications like recommendation systems, LLMs, and dynamic systems.

Conclusion: The research aims to bridge the gap between theoretical guarantees and practical performance, enhancing the applicability of online learning methods in real-world scenarios.

Abstract: The primary goal of my Ph.D. study is to develop provably efficient and
practical algorithms for data-driven online sequential decision-making under
uncertainty. My work focuses on reinforcement learning (RL), multi-armed
bandits, and their applications, including recommendation systems, computer
networks, video analytics, and large language models (LLMs). Online learning
methods, such as bandits and RL, have demonstrated remarkable success - ranging
from outperforming human players in complex games like Atari and Go to
advancing robotics, recommendation systems, and fine-tuning LLMs. Despite these
successes, many established algorithms rely on idealized models that can fail
under model misspecifications or adversarial perturbations, particularly in
settings where accurate prior knowledge of the underlying model class is
unavailable or where malicious users operate within dynamic systems. These
challenges are pervasive in real-world applications, where robust and adaptive
solutions are critical. Furthermore, while worst-case guarantees provide
theoretical reliability, they often fail to capture instance-dependent
performance, which can lead to more efficient and practical solutions. Another
key challenge lies in generalizing to new, unseen environments, a crucial
requirement for deploying these methods in dynamic and unpredictable settings.
To address these limitations, my research aims to develop more efficient,
robust, instance-adaptive, and generalizable online learning algorithms for
both reinforcement learning and bandits. Towards this end, I focus on
developing more efficient, robust, instance-adaptive, and generalizable for
both general reinforcement learning (RL) and bandits.

</details>


### [298] [TianQuan-Climate: A Subseasonal-to-Seasonal Global Weather Model via Incorporate Climatology State](https://arxiv.org/abs/2504.09940)
*Guowen Li,Xintong Liu,Shilei Cao,Haoyuan Liang,Mengxuan Chen,Lixian Zhang,Jinxiao Zhang,Jiuke Wang,Meng Jin,Juepeng Zheng,Haohuan Fu*

Main category: cs.LG

TL;DR: TianQuan-Climate, a novel AI model, improves subseasonal forecasting by reducing errors and enhancing smoothness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Subseasonal forecasting is crucial for SDGs but is complex due to dissipating initial conditions and delayed external forces. AI models face challenges like error accumulation and smoothness.

Method: TianQuan-Climate uses a multi-model prediction strategy and a Content Fusion Module with UD-ViT to integrate climatological data and learn from uncertainty.

Result: The model outperforms existing numerical and AI methods in benchmarks for 15 to 45-day forecasts.

Conclusion: TianQuan-Climate effectively addresses key challenges in subseasonal forecasting, offering improved accuracy for global daily forecasts.

Abstract: Subseasonal forecasting serves as an important support for Sustainable
Development Goals (SDGs), such as climate challenges, agricultural yield and
sustainable energy production. However, subseasonal forecasting is a complex
task in meteorology due to dissipating initial conditions and delayed external
forces. Although AI models are increasingly pushing the boundaries of this
forecasting limit, they face two major challenges: error accumulation and
Smoothness. To address these two challenges, we propose Climate Furnace
Subseasonal-to-Seasonal (TianQuan-Climate), a novel machine learning model
designed to provide global daily mean forecasts up to 45 days, covering five
upper-air atmospheric variables at 13 pressure levels and two surface
variables. Our proposed TianQuan-Climate has two advantages: 1) it utilizes a
multi-model prediction strategy to reduce system error impacts in long-term
subseasonal forecasts; 2) it incorporates a Content Fusion Module for
climatological integration and extends ViT with uncertainty blocks (UD-ViT) to
improve generalization by learning from uncertainty. We demonstrate the
effectiveness of TianQuan-Climate on benchmarks for weather forecasting and
climate projections within the 15 to 45-day range, where TianQuan-Climate
outperforms existing numerical and AI methods.

</details>


### [299] [Physics Informed Constrained Learning of Dynamics from Static Data](https://arxiv.org/abs/2504.12675)
*Pengtao Dang,Tingbo Guo,Melissa Fishel,Guang Lin,Wenzhuo Wu,Sha Cao,Chi Zhang*

Main category: cs.LG

TL;DR: A new PINN framework, Constrained Learning, uses non-time course data to approximate system dynamics, outperforming existing methods in tasks like metabolic flux analysis.


<details>
  <summary>Details</summary>
Motivation: Existing PINN frameworks require fully observed time-course data, which is often hard to acquire. This study aims to overcome this limitation.

Method: Developed Constrained Learning for PINNs, using non-time course data, and introduced MPOCtrL, an optimization approach balancing physical models and observed data.

Result: MPOCtrL effectively detects nonlinear dependencies in data and outperforms existing methods in metabolic flux analysis.

Conclusion: Constrained Learning and MPOCtrL provide a robust solution for modeling systems with limited or partial data, advancing PINN applications.

Abstract: A physics-informed neural network (PINN) models the dynamics of a system by
integrating the governing physical laws into the architecture of a neural
network. By enforcing physical laws as constraints, PINN overcomes challenges
with data scarsity and potentially high dimensionality. Existing PINN
frameworks rely on fully observed time-course data, the acquisition of which
could be prohibitive for many systems. In this study, we developed a new PINN
learning paradigm, namely Constrained Learning, that enables the approximation
of first-order derivatives or motions using non-time course or partially
observed data. Computational principles and a general mathematical formulation
of Constrained Learning were developed. We further introduced MPOCtrL (Message
Passing Optimization-based Constrained Learning) an optimization approach
tailored for the Constrained Learning framework that strives to balance the
fitting of physical models and observed data. Its code is available at github
link: https://github.com/ptdang1001/MPOCtrL Experiments on synthetic and
real-world data demonstrated that MPOCtrL can effectively detect the nonlinear
dependency between observed data and the underlying physical properties of the
system. In particular, on the task of metabolic flux analysis, MPOCtrL
outperforms all existing data-driven flux estimators.

</details>


### [300] [A Client-level Assessment of Collaborative Backdoor Poisoning in Non-IID Federated Learning](https://arxiv.org/abs/2504.12875)
*Phung Lai,Guanxiong Liu,NhatHai Phan,Issa Khalil,Abdallah Khreishah,Xintao Wu*

Main category: cs.LG

TL;DR: The paper introduces CollaPois, a novel collaborative backdoor poisoning attack in federated learning (FL) that exploits non-IID data vulnerabilities, amplifying Trojan impact with minimal compromised clients while evading detection.


<details>
  <summary>Details</summary>
Motivation: The study aims to uncover and demonstrate new vulnerabilities in FL due to non-IID data, which existing defenses fail to address, posing significant risks in real-world FL scenarios.

Method: Developed CollaPois, a collaborative attack where compromised clients use a pre-trained Trojan-infected model to produce malicious gradients, leveraging diverse client data distributions to amplify the Trojan's impact.

Result: CollaPois outperforms state-of-the-art backdoor attacks, bypassing defenses, especially in non-IID settings, and remains effective with few compromised clients. Clients with similar data to compromised ones face higher infection risks.

Conclusion: CollaPois highlights critical FL vulnerabilities from non-IID data, urging the need for advanced defenses against collaborative poisoning attacks.

Abstract: Federated learning (FL) enables collaborative model training using
decentralized private data from multiple clients. While FL has shown robustness
against poisoning attacks with basic defenses, our research reveals new
vulnerabilities stemming from non-independent and identically distributed
(non-IID) data among clients. These vulnerabilities pose a substantial risk of
model poisoning in real-world FL scenarios.
  To demonstrate such vulnerabilities, we develop a novel collaborative
backdoor poisoning attack called CollaPois. In this attack, we distribute a
single pre-trained model infected with a Trojan to a group of compromised
clients. These clients then work together to produce malicious gradients,
causing the FL model to consistently converge towards a low-loss region
centered around the Trojan-infected model. Consequently, the impact of the
Trojan is amplified, especially when the benign clients have diverse local data
distributions and scattered local gradients. CollaPois stands out by achieving
its goals while involving only a limited number of compromised clients, setting
it apart from existing attacks. Also, CollaPois effectively avoids noticeable
shifts or degradation in the FL model's performance on legitimate data samples,
allowing it to operate stealthily and evade detection by advanced robust FL
algorithms.
  Thorough theoretical analysis and experiments conducted on various benchmark
datasets demonstrate the superiority of CollaPois compared to state-of-the-art
backdoor attacks. Notably, CollaPois bypasses existing backdoor defenses,
especially in scenarios where clients possess diverse data distributions.
Moreover, the results show that CollaPois remains effective even when involving
a small number of compromised clients. Notably, clients whose local data is
closely aligned with compromised clients experience higher risks of backdoor
infections.

</details>


### [301] [Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to the Top-$k$ Experts](https://arxiv.org/abs/2504.12988)
*Yannis Montreuil,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: The paper introduces Top-$k$ and Top-$k(x)$ Learning-to-Defer (L2D), extending single-agent deferral to collective expertise for improved reliability in high-stakes scenarios. It proposes adaptive deferral based on input complexity and costs, with theoretical guarantees and empirical validation.


<details>
  <summary>Details</summary>
Motivation: Existing L2D methods focus on single-agent deferral, which is insufficient for high-stakes scenarios requiring collective expertise. The paper aims to generalize L2D to leverage multiple agents adaptively.

Method: The authors propose Top-$k$ L2D (fixed $k$ agents) and Top-$k(x)$ L2D (adaptive $k$ per query). They derive a surrogate loss, prove Bayes-consistency, and show model cascades as a special case.

Result: The framework is validated on diverse benchmarks for classification and regression, demonstrating effectiveness in leveraging collective expertise.

Conclusion: Top-$k$ and Top-$k(x)$ L2D generalize single-agent deferral, offering flexible, cost-efficient solutions with theoretical and empirical support.

Abstract: Learning-to-Defer (L2D) enables decision-making systems to improve
reliability by selectively deferring uncertain predictions to more competent
agents. However, most existing approaches focus exclusively on single-agent
deferral, which is often inadequate in high-stakes scenarios that require
collective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of
the classical two-stage L2D framework that allocates each query to the $k$ most
confident agents instead of a single one. To further enhance flexibility and
cost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive
extension that learns the optimal number of agents to consult for each query,
based on input complexity, agent competency distributions, and consultation
costs. For both settings, we derive a novel surrogate loss and prove that it is
Bayes-consistent and $(\mathcal{R}, \mathcal{G})$-consistent, ensuring
convergence to the Bayes-optimal allocation. Notably, we show that the
well-established model cascades paradigm arises as a restricted instance of our
Top-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse
benchmarks demonstrate the effectiveness of our framework on both
classification and regression tasks.

</details>


### [302] [Efficient algorithms for the Hadamard decomposition](https://arxiv.org/abs/2504.13633)
*Samuel Wertz,Arnaud Vandaele,Nicolas Gillis*

Main category: cs.LG

TL;DR: An efficient algorithm for Hadamard decomposition is proposed, using alternating optimization and advanced initialization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and effectiveness of Hadamard decomposition for data analysis and matrix compression.

Method: Alternating optimization splits the problem into convex sub-problems, with SVD-inspired initialization and momentum-based acceleration. Extended to support more than two low-rank matrices.

Result: Outperforms gradient descent-based and traditional low-rank methods in experiments.

Conclusion: The proposed method is effective and efficient for Hadamard decomposition, scalable to higher-rank approximations.

Abstract: The Hadamard decomposition is a powerful technique for data analysis and
matrix compression, which decomposes a given matrix into the element-wise
product of two or more low-rank matrices. In this paper, we develop an
efficient algorithm to solve this problem, leveraging an alternating
optimization approach that decomposes the global non-convex problem into a
series of convex sub-problems. To improve performance, we explore advanced
initialization strategies inspired by the singular value decomposition (SVD)
and incorporate acceleration techniques by introducing momentum-based updates.
Beyond optimizing the two-matrix case, we also extend the Hadamard
decomposition framework to support more than two low-rank matrices, enabling
approximations with higher effective ranks while preserving computational
efficiency. Finally, we conduct extensive experiments to compare our method
with the existing gradient descent-based approaches for the Hadamard
decomposition and with traditional low-rank approximation techniques. The
results highlight the effectiveness of our proposed method across diverse
datasets.

</details>


### [303] [Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems](https://arxiv.org/abs/2504.13768)
*Vinay Sharma,Rémi Tanguy Oddon,Pietro Tesini,Jens Ravesloot,Cees Taal,Olga Fink*

Main category: cs.LG

TL;DR: Equi-Euler GraphNet, a physics-informed GNN, predicts internal forces and trajectories in multi-body systems, outperforming state-of-the-art methods with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of multi-body dynamics is crucial for digital twins, fault detection, and predictive maintenance, where internal loads are key indicators.

Method: Uses a graph neural network with equivariant message-passing and Euler integration for temporal updates, tailored for cylindrical roller bearings.

Result: Achieves stable predictions over thousands of steps, generalizes to unseen conditions, and offers a 200x speedup over conventional solvers.

Conclusion: Equi-Euler GraphNet is an efficient, accurate reduced-order model for digital twin applications and predictive maintenance.

Abstract: Accurate real-time modeling of multi-body dynamical systems is essential for
enabling digital twin applications across industries. While many data-driven
approaches aim to learn system dynamics, jointly predicting internal loads and
system trajectories remains a key challenge. This dual prediction is especially
important for fault detection and predictive maintenance, where internal
loads-such as contact forces-act as early indicators of faults, reflecting wear
or misalignment before affecting motion. These forces also serve as inputs to
degradation models (e.g., crack growth), enabling damage prediction and
remaining useful life estimation. We propose Equi-Euler GraphNet, a
physics-informed graph neural network (GNN) that simultaneously predicts
internal forces and global trajectories in multi-body systems. In this
mesh-free framework, nodes represent system components and edges encode
interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an
equivariant message-passing scheme, interpreting edge messages as interaction
forces consistent under Euclidean transformations; and (2) a temporal-aware
iterative node update mechanism, based on Euler integration, to capture
influence of distant interactions over time. Tailored for cylindrical roller
bearings, it decouples ring dynamics from constrained motion of rolling
elements. Trained on high-fidelity multiphysics simulations, Equi-Euler
GraphNet generalizes beyond the training distribution, accurately predicting
loads and trajectories under unseen speeds, loads, and configurations. It
outperforms state-of-the-art GNNs focused on trajectory prediction, delivering
stable rollouts over thousands of time steps with minimal error accumulation.
Achieving up to a 200x speedup over conventional solvers while maintaining
comparable accuracy, it serves as an efficient reduced-order model for digital
twins, design, and maintenance.

</details>


### [304] [SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM](https://arxiv.org/abs/2504.14286)
*Xiaojiang Zhang,Jinghui Wang,Zifei Cheng,Wenhao Zhuang,Zheng Lin,Minglei Zhang,Shaojie Wang,Yinghan Cui,Chao Wang,Junyi Peng,Shimiao Jiang,Shiqi Kuang,Shouyu Yin,Chaohang Wen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SRPO, a two-staged history-resampling policy optimization method, outperforms DeepSeek-R1-Zero-32B on benchmarks using fewer training steps, introducing cross-domain training and history resampling for efficiency.


<details>
  <summary>Details</summary>
Motivation: Replicating reasoning model advancements across domains is challenging due to limited transparency; SRPO aims to address this.

Method: SRPO uses a two-stage cross-domain training paradigm and history resampling to balance reasoning and coding proficiency while addressing ineffective samples.

Result: SRPO surpasses DeepSeek-R1-Zero-32B on AIME24 and LiveCodeBench benchmarks with 1/10 the training steps.

Conclusion: SRPO offers efficient scaling of LLM reasoning capabilities across diverse tasks, validated by comprehensive experiments.

Abstract: Recent advances of reasoning models, exemplified by OpenAI's o1 and
DeepSeek's R1, highlight the significant potential of Reinforcement Learning
(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).
However, replicating these advancements across diverse domains remains
challenging due to limited methodological transparency. In this work, we
present two-Staged history-Resampling Policy Optimization (SRPO), which
surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and
LiveCodeBench benchmarks. SRPO achieves this using the same base model as
DeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps
required by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building
upon Group Relative Policy Optimization (GRPO), we introduce two key
methodological innovations: (1) a two-stage cross-domain training paradigm
designed to balance the development of mathematical reasoning and coding
proficiency, and (2) History Resampling (HR), a technique to address
ineffective samples. Our comprehensive experiments validate the effectiveness
of our approach, offering valuable insights into scaling LLM reasoning
capabilities across diverse tasks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [305] [A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations](https://arxiv.org/abs/2504.15301)
*Zoi Lygizou,Dimitris Kalles*

Main category: cs.MA

TL;DR: A biologically inspired trust model addresses mobility and behavior changes in multi-agent systems, with a new algorithm improving adaptability and outperforming existing models like FIRE.


<details>
  <summary>Details</summary>
Motivation: Existing trust models struggle with agent mobility, behavior changes, and cold start problems in dynamic systems.

Method: Introduces a self-classification mechanism for providers to detect harmful performance drops, enhancing the original biologically inspired trust model.

Result: The new algorithm outperforms the original and FIRE in dynamic conditions, showing greater adaptability.

Conclusion: The study evaluates resilience against attacks and suggests future research directions for further improvements.

Abstract: Trust management provides an alternative solution for securing open, dynamic,
and distributed multi-agent systems, where conventional cryptographic methods
prove to be impractical. However, existing trust models face challenges related
to agent mobility, changing behaviors, and the cold start problem. To address
these issues we introduced a biologically inspired trust model in which
trustees assess their own capabilities and store trust data locally. This
design improves mobility support, reduces communication overhead, resists
disinformation, and preserves privacy. Despite these advantages, prior
evaluations revealed limitations of our model in adapting to provider
population changes and continuous performance fluctuations. This study proposes
a novel algorithm, incorporating a self-classification mechanism for providers
to detect performance drops potentially harmful for the service consumers.
Simulation results demonstrate that the new algorithm outperforms its original
version and FIRE, a well-known trust and reputation model, particularly in
handling dynamic trustee behavior. While FIRE remains competitive under extreme
environmental changes, the proposed algorithm demonstrates greater adaptability
across various conditions. In contrast to existing trust modeling research,
this study conducts a comprehensive evaluation of our model using widely
recognized trust model criteria, assessing its resilience against common
trust-related attacks while identifying strengths, weaknesses, and potential
countermeasures. Finally, several key directions for future research are
proposed.

</details>


### [306] [Trustworthy Decentralized Autonomous Machines: A New Paradigm in Automation Economy](https://arxiv.org/abs/2504.15676)
*Fernando Castillo,Oscar Castillo,Eduardo Brito,Simon Espinola*

Main category: cs.MA

TL;DR: DAMs combine AI, blockchain, and IoT to create self-governing economic agents, enabling trustless management of digital and physical assets.


<details>
  <summary>Details</summary>
Motivation: To transition from trust-based to trustless economic models and address inefficiencies in centralized systems.

Method: Integration of AI-driven decision-making, IoT autonomy, and blockchain governance.

Result: DAMs offer scalable, transparent, and equitable solutions for asset management.

Conclusion: DAMs can reduce wealth disparities and foster a post-labor economy.

Abstract: Decentralized Autonomous Machines (DAMs) represent a transformative paradigm
in automation economy, integrating artificial intelligence (AI), blockchain
technology, and Internet of Things (IoT) devices to create self-governing
economic agents participating in Decentralized Physical Infrastructure Networks
(DePIN). Capable of managing both digital and physical assets and unlike
traditional Decentralized Autonomous Organizations (DAOs), DAMs extend autonomy
into the physical world, enabling trustless systems for Real and Digital World
Assets (RDWAs). In this paper, we explore the technological foundations, and
challenges of DAMs and argue that DAMs are pivotal in transitioning from
trust-based to trustless economic models, offering scalable, transparent, and
equitable solutions for asset management. The integration of AI-driven
decision-making, IoT-enabled operational autonomy, and blockchain-based
governance allows DAMs to decentralize ownership, optimize resource allocation,
and democratize access to economic opportunities. Therefore, in this research,
we highlight the potential of DAMs to address inefficiencies in centralized
systems, reduce wealth disparities, and foster a post-labor economy.

</details>


### [307] [The Formation of Production Networks: How Supply Chains Arise from Simple Learning with Minimal Information](https://arxiv.org/abs/2504.16010)
*Tuong Manh Vu,Ernesto Carrella,Robert Axtell,Omar A. Guerrero*

Main category: cs.MA

TL;DR: A model where firms use reinforcement learning to set prices, production volumes, and input choices, forming a dynamic production network without equilibrium assumptions.


<details>
  <summary>Details</summary>
Motivation: To understand how firms adapt to uncertainty and shocks without relying on equilibrium or perfect knowledge assumptions.

Method: Firms use reinforcement learning to make decisions, adapting to shocks like demand shifts or productivity changes.

Result: The model shows firms can reshape the production network dynamically in response to shocks.

Conclusion: The approach offers a flexible framework for analyzing production networks under uncertainty.

Abstract: We develop a model where firms determine the price at which they sell their
differentiable goods, the volume that they produce, and the inputs (types and
amounts) that they purchase from other firms. A steady-state production network
emerges endogenously without resorting to assumptions such as equilibrium or
perfect knowledge about production technologies. Through a simple version of
reinforcement learning, firms with heterogeneous technologies cope with
uncertainty and maximize profits. Due to this learning process, firms can adapt
to shocks such as demand shifts, suppliers/clients closure, productivity
changes, and production technology modifications; effectively reshaping the
production network. To demonstrate the potential of this model, we analyze the
upstream and downstream impact of demand and productivity shocks.

</details>


### [308] [LOKA Protocol: A Decentralized Framework for Trustworthy and Ethical AI Agent Ecosystems](https://arxiv.org/abs/2504.10915)
*Rajesh Ranjan,Shailja Gupta,Surya Narayan Singh*

Main category: cs.MA

TL;DR: The LOKA Protocol is a systems-level architecture designed to address identity, accountability, and ethical alignment in autonomous AI ecosystems.


<details>
  <summary>Details</summary>
Motivation: The proliferation of autonomous AI agents reveals gaps in identity, accountability, and ethical alignment, necessitating a unified governance framework.

Method: LOKA introduces a Universal Agent Identity Layer (UAIL), intent-centric communication protocols, and a Decentralized Ethical Consensus Protocol (DECP), leveraging standards like DIDs and VCs.

Result: The protocol provides a scalable blueprint for ethically governed, interoperable AI ecosystems.

Conclusion: LOKA lays the foundation for responsible, transparent, and autonomous AI ecosystems.

Abstract: The rise of autonomous AI agents, capable of perceiving, reasoning, and
acting independently, signals a profound shift in how digital ecosystems
operate, govern, and evolve. As these agents proliferate beyond centralized
infrastructures, they expose foundational gaps in identity, accountability, and
ethical alignment. Three critical questions emerge: Identity: Who or what is
the agent? Accountability: Can its actions be verified, audited, and trusted?
Ethical Consensus: Can autonomous systems reliably align with human values and
prevent harmful emergent behaviors? We present the novel LOKA Protocol (Layered
Orchestration for Knowledgeful Agents), a unified, systems-level architecture
for building ethically governed, interoperable AI agent ecosystems. LOKA
introduces a proposed Universal Agent Identity Layer (UAIL) for decentralized,
verifiable identity; intent-centric communication protocols for semantic
coordination across diverse agents; and a Decentralized Ethical Consensus
Protocol (DECP) that could enable agents to make context-aware decisions
grounded in shared ethical baselines. Anchored in emerging standards such as
Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and
post-quantum cryptography, LOKA proposes a scalable, future-resilient blueprint
for multi-agent AI governance. By embedding identity, trust, and ethics into
the protocol layer itself, LOKA proposes the foundation for a new era of
responsible, transparent, and autonomous AI ecosystems operating across digital
and physical domains.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)



<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [309] [Exploring the User Experience of AI-Assisted Sound Searching Systems for Creative Workflows](https://arxiv.org/abs/2504.15575)
*Haohe Liu,Thomas Deacon,Wenwu Wang,Matt Paradis,Mark D. Plumbley*

Main category: eess.AS

TL;DR: CLAP-UI, a CLAP-based sound-searching system, outperforms traditional human-annotated systems in productivity and user satisfaction while maintaining cognitive demands.


<details>
  <summary>Details</summary>
Motivation: Current sound-searching systems rely on time-consuming and error-prone human annotations, limiting efficiency.

Method: Developed CLAP-UI, a system using contrastive language-audio pre-training (CLAP) without human annotations, and compared it with the BBC Sound Effect Library.

Result: CLAP-UI significantly improved productivity and reduced frustration, with comparable cognitive load.

Conclusion: CLAP-UI is a promising alternative to traditional systems, with insights for future AI-assisted sound search designs.

Abstract: Locating the right sound effect efficiently is an important yet challenging
topic for audio production. Most current sound-searching systems rely on
pre-annotated audio labels created by humans, which can be time-consuming to
produce and prone to inaccuracies, limiting the efficiency of audio production.
Following the recent advancement of contrastive language-audio pre-training
(CLAP) models, we explore an alternative CLAP-based sound-searching system
(CLAP-UI) that does not rely on human annotations. To evaluate the
effectiveness of CLAP-UI, we conducted comparative experiments with a widely
used sound effect searching platform, the BBC Sound Effect Library. Our study
evaluates user performance, cognitive load, and satisfaction through
ecologically valid tasks based on professional sound-searching workflows. Our
result shows that CLAP-UI demonstrated significantly enhanced productivity and
reduced frustration while maintaining comparable cognitive demands. We also
qualitatively analyzed the participants' feedback, which offered valuable
perspectives on the design of future AI-assisted sound search systems.

</details>


### [310] [FADEL: Uncertainty-aware Fake Audio Detection with Evidential Deep Learning](https://arxiv.org/abs/2504.15663)
*Ju Yeon Kang,Ji Won Yoon,Semin Kim,Min Hyun Han,Nam Soo Kim*

Main category: eess.AS

TL;DR: Fake audio detection is critical due to spoofing attacks on ASV systems. The proposed FADEL framework uses evidential learning to improve robustness against unseen attacks by modeling uncertainty.


<details>
  <summary>Details</summary>
Motivation: Advancements in speech synthesis increase spoofing risks, but current methods overconfidently classify unpredictable attacks.

Method: FADEL models class probabilities with a Dirichlet distribution to incorporate uncertainty, enhancing OOD detection.

Result: FADEL outperforms baselines on ASVspoof2019 and ASVspoof2021 datasets, with uncertainty correlating to EER.

Conclusion: FADEL's evidential learning effectively addresses overconfidence, improving fake audio detection robustness.

Abstract: Recently, fake audio detection has gained significant attention, as
advancements in speech synthesis and voice conversion have increased the
vulnerability of automatic speaker verification (ASV) systems to spoofing
attacks. A key challenge in this task is generalizing models to detect unseen,
out-of-distribution (OOD) attacks. Although existing approaches have shown
promising results, they inherently suffer from overconfidence issues due to the
usage of softmax for classification, which can produce unreliable predictions
when encountering unpredictable spoofing attempts. To deal with this
limitation, we propose a novel framework called fake audio detection with
evidential learning (FADEL). By modeling class probabilities with a Dirichlet
distribution, FADEL incorporates model uncertainty into its predictions,
thereby leading to more robust performance in OOD scenarios. Experimental
results on the ASVspoof2019 Logical Access (LA) and ASVspoof2021 LA datasets
indicate that the proposed method significantly improves the performance of
baseline models. Furthermore, we demonstrate the validity of uncertainty
estimation by analyzing a strong correlation between average uncertainty and
equal error rate (EER) across different spoofing algorithms.

</details>


### [311] [EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting](https://arxiv.org/abs/2504.12867)
*Guanrou Yang,Chen Yang,Qian Chen,Ziyang Ma,Wenxi Chen,Wen Wang,Tianrui Wang,Yifan Yang,Zhikang Niu,Wenrui Liu,Fan Yu,Zhihao Du,Zhifu Gao,ShiLiang Zhang,Xie Chen*

Main category: eess.AS

TL;DR: EmoVoice is a novel emotion-controllable TTS model using LLMs for fine-grained emotion control and phoneme boost for content consistency, achieving SOTA results on English and Chinese datasets.


<details>
  <summary>Details</summary>
Motivation: Current TTS models lack fine-grained emotional control, limiting their ability to convey human-like emotional expression.

Method: EmoVoice integrates LLMs for natural language emotion control and a phoneme boost variant for parallel phoneme and audio token generation. It also introduces EmoVoice-DB, a high-quality emotion dataset.

Result: EmoVoice achieves SOTA performance on English and Chinese test sets and evaluates emotion metrics' reliability with human preferences.

Conclusion: EmoVoice advances emotion-controllable TTS with LLMs and phoneme boost, supported by a new dataset and evaluation framework.

Abstract: Human speech goes beyond the mere transfer of information; it is a profound
exchange of emotions and a connection between individuals. While Text-to-Speech
(TTS) models have made huge progress, they still face challenges in controlling
the emotional expression in the generated speech. In this work, we propose
EmoVoice, a novel emotion-controllable TTS model that exploits large language
models (LLMs) to enable fine-grained freestyle natural language emotion
control, and a phoneme boost variant design that makes the model output phoneme
tokens and audio tokens in parallel to enhance content consistency, inspired by
chain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we
introduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring
expressive speech and fine-grained emotion labels with natural language
descriptions. EmoVoice achieves state-of-the-art performance on the English
EmoVoice-DB test set using only synthetic training data, and on the Chinese
Secap test set using our in-house data. We further investigate the reliability
of existing emotion evaluation metrics and their alignment with human
perceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and
Gemini to assess emotional speech. Demo samples are available at
https://yanghaha0908.github.io/EmoVoice/. Dataset, code, and checkpoints will
be released.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [312] [RINN: One Sample Radio Frequency Imaging based on Physics Informed Neural Network](https://arxiv.org/abs/2504.15311)
*Fei Shang,Haohua Du,Dawei Yan,Panlong Yang,Xiang-Yang Li*

Main category: eess.IV

TL;DR: RINN network uses physical constraints for RF imaging with noisy, phaseless data, outperforming classic phase-based methods.


<details>
  <summary>Details</summary>
Motivation: RF imaging is limited by low precision and lack of large datasets; RINN addresses this by leveraging physical constraints.

Method: Combines PINN ideas to design RINN, using physical constraints instead of true value comparison, adapting to RF signal characteristics.

Result: RINN achieves good imaging with phaseless, noisy data (RRMSE 0.11), comparable to phase-based methods.

Conclusion: RINN enables universal RF imaging development by overcoming data and precision limitations.

Abstract: Due to its ability to work in non-line-of-sight and low-light environments,
radio frequency (RF) imaging technology is expected to bring new possibilities
for embodied intelligence and multimodal sensing. However, widely used RF
devices (such as Wi-Fi) often struggle to provide high-precision
electromagnetic measurements and large-scale datasets, hindering the
application of RF imaging technology. In this paper, we combine the ideas of
PINN to design the RINN network, using physical constraints instead of true
value comparison constraints and adapting it with the characteristics of
ubiquitous RF signals, allowing the RINN network to achieve RF imaging using
only one sample without phase and with amplitude noise. Our numerical
evaluation results show that compared with 5 classic algorithms based on phase
data for imaging results, RINN's imaging results based on phaseless data are
good, with indicators such as RRMSE (0.11) performing similarly well. RINN
provides new possibilities for the universal development of radio frequency
imaging technology.

</details>


### [313] [Enhancing DR Classification with Swin Transformer and Shifted Window Attention](https://arxiv.org/abs/2504.15317)
*Meher Boulaabi,Takwa Ben Aïcha Gader,Afef Kacem Echi,Zied Bouraoui*

Main category: eess.IV

TL;DR: A robust preprocessing pipeline and Swin Transformer model improve diabetic retinopathy classification, achieving high accuracy on Aptos and IDRiD datasets.


<details>
  <summary>Details</summary>
Motivation: Early detection of diabetic retinopathy (DR) is crucial to prevent blindness, but automated classification faces challenges like image quality variations and class imbalance.

Method: Proposes a preprocessing pipeline (cropping, CLAHE, data augmentation) and uses Swin Transformer for hierarchical feature extraction.

Result: Achieved 89.65% accuracy on Aptos and 97.40% on IDRiD datasets, excelling in early-stage DR detection.

Conclusion: The method shows promise for enhancing automated retinal screening in clinical settings.

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide,
underscoring the importance of early detection for effective treatment.
However, automated DR classification remains challenging due to variations in
image quality, class imbalance, and pixel-level similarities that hinder model
training. To address these issues, we propose a robust preprocessing pipeline
incorporating image cropping, Contrast-Limited Adaptive Histogram Equalization
(CLAHE), and targeted data augmentation to improve model generalization and
resilience. Our approach leverages the Swin Transformer, which utilizes
hierarchical token processing and shifted window attention to efficiently
capture fine-grained features while maintaining linear computational
complexity. We validate our method on the Aptos and IDRiD datasets for
multi-class DR classification, achieving accuracy rates of 89.65% and 97.40%,
respectively. These results demonstrate the effectiveness of our model,
particularly in detecting early-stage DR, highlighting its potential for
improving automated retinal screening in clinical settings.

</details>


### [314] [Learned Primal Dual Splitting for Self-Supervised Noise-Adaptive MRI Reconstruction](https://arxiv.org/abs/2504.15390)
*Nikola Janjusevic,Amirhoussein Khalilian-Gourtani,Yao Wang,Li Feng*

Main category: eess.IV

TL;DR: Proposes LPDSNet, an interpretable DNN for self-supervised MRI reconstruction and denoising, outperforming black-box methods.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of interpretability in state-of-the-art DNNs for MRI reconstruction, hindering improvement.

Method: Uses a classical primal-dual splitting algorithm (LPDSNet) to decouple observation model from signal prior.

Result: Achieves state-of-the-art self-supervised performance and noise-level generalization, where black-box methods fail.

Conclusion: LPDSNet offers interpretability and superior performance, advancing MRI reconstruction and denoising.

Abstract: Magnetic resonance imaging (MRI) reconstruction has largely been dominated by
deep neural networks (DNN); however, many state-of-the-art architectures use
black-box structures, which hinder interpretability and improvement. Here, we
propose an interpretable DNN architecture for self-supervised MRI
reconstruction and denoising by directly parameterizing and learning the
classical primal-dual splitting, dubbed LPDSNet. This splitting algorithm
allows us to decouple the observation model from the signal prior.
Experimentally, we show other interpretable architectures without this
decoupling property exhibit failure in the self-supervised learning regime. We
report state-of-the-art self-supervised joint MRI reconstruction and denoising
performance and novel noise-level generalization capabilities, where in
contrast black-box networks fail to generalize.

</details>


### [315] [Split-quaternions for perceptual white balance](https://arxiv.org/abs/2504.15481)
*Michel Berthier,Nicoletta Prencipe,Edoardo Provenzi*

Main category: eess.IV

TL;DR: A perceptual chromatic adaptation transform using split-quaternions is proposed, linking algebraic structures in color perception to split-quaternions, and outperforming the von Kries transform.


<details>
  <summary>Details</summary>
Motivation: Motivated by a quantum-like model of color perception, the work explores algebraic structures in this model and their connection to split-quaternions.

Method: The method involves a chromatic adaptation transform implemented via split-quaternion multiplication.

Result: The approach shows potential for color image processing and outperforms the von Kries chromatic adaptation transform in quantitative comparisons.

Conclusion: The proposed split-quaternion-based transform offers a novel and effective alternative for chromatic adaptation in image processing.

Abstract: We propose a perceptual chromatic adaptation transform for white balance that
makes use of split-quaternions. The novelty of the present work, which is
motivated by a recently developed quantum-like model of color perception,
consists at stressing the link between the algebraic structures appearing in
this model and a certain sub-algebra of the split-quaternions. We show the
potentiality of this approach for color image processing applications by
proposing a chromatic adaptation transform, implemented via an appropriate use
of the split-quaternion multiplication. Moreover, quantitative comparisons with
the widely used state-of-the art von Kries chromatic adaptation transform are
provided.

</details>


### [316] [VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining](https://arxiv.org/abs/2504.15545)
*Zizhi Chen,Xinyu Zhang,Minghao Han,Yizhou Liu,Ziyun Qian,Weifeng Zhang,Xukun Zhang,Jingwei Wei,Lihua Zhang*

Main category: eess.IV

TL;DR: A pathological vision-language model (VLM) is introduced for virtual staining to improve realism and accuracy in histopathology by integrating contrastive prompts and concept anchors.


<details>
  <summary>Details</summary>
Motivation: Traditional virtual staining methods lack pathological knowledge and physical properties, leading to style-level transfer issues.

Method: Uses a pathological VLM with contrastive prompts and concept anchors, plus a VLM-based data augmentation method.

Result: Generates highly realistic images and improves accuracy in tasks like glomerular detection and segmentation.

Conclusion: The method effectively enhances virtual staining by leveraging pathological knowledge and VLM capabilities.

Abstract: In histopathology, tissue sections are typically stained using common H&E
staining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific
tissue structures. The rapid advancement of deep learning offers an effective
solution for generating virtually stained images, significantly reducing the
time and labor costs associated with traditional histochemical staining.
However, a new challenge arises in separating the fundamental visual
characteristics of tissue sections from the visual differences induced by
staining agents. Additionally, virtual staining often overlooks essential
pathological knowledge and the physical properties of staining, resulting in
only style-level transfer. To address these issues, we introduce, for the first
time in virtual staining tasks, a pathological vision-language large model
(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,
foundational concept anchors for tissue sections, and staining-specific concept
anchors to leverage the extensive knowledge of the pathological VLM. This
approach is designed to describe, frame, and enhance the direction of virtual
staining. Furthermore, we have developed a data augmentation method based on
the constraints of the VLM. This method utilizes the VLM's powerful image
interpretation capabilities to further integrate image style and structural
information, proving beneficial in high-precision pathological diagnostics.
Extensive evaluations on publicly available multi-domain unpaired staining
datasets demonstrate that our method can generate highly realistic images and
enhance the accuracy of downstream tasks, such as glomerular detection and
segmentation. Our code is available at:
https://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR

</details>


### [317] [RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2504.15649)
*Biao Wu,Diankai Zhang,Shaoli Liu,Si Gao,Chengjian Zheng,Ning Wang*

Main category: eess.IV

TL;DR: RepNet-VSR is a reparameterizable architecture for efficient 4x video super-resolution, balancing quality and speed on edge devices.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational inefficiency of deep CNNs in VSR for real-time mobile applications.

Method: Proposes RepNet-VSR, a reparameterizable architecture for high-fidelity VSR, optimized for edge devices.

Result: Achieves 27.79 dB PSNR on REDS, processing 180p to 720p in 103 ms per 10 frames, outperforming prior methods.

Conclusion: RepNet-VSR offers a practical solution for real-time VSR on resource-constrained devices, excelling in quality and efficiency.

Abstract: As a fundamental challenge in visual computing, video super-resolution (VSR)
focuses on reconstructing highdefinition video sequences from their degraded
lowresolution counterparts. While deep convolutional neural networks have
demonstrated state-of-the-art performance in spatial-temporal super-resolution
tasks, their computationally intensive nature poses significant deployment
challenges for resource-constrained edge devices, particularly in real-time
mobile video processing scenarios where power efficiency and latency
constraints coexist. In this work, we propose a Reparameterizable Architecture
for High Fidelity Video Super Resolution method, named RepNet-VSR, for
real-time 4x video super-resolution. On the REDS validation set, the proposed
model achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per
10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an
excellent balance between restoration quality and deployment efficiency. The
proposed method scores higher than the previous champion algorithm of MAI video
super-resolution challenge.

</details>


### [318] [Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg](https://arxiv.org/abs/2504.15667)
*Jingchen Zou,Jianqiang Li,Gabriel Jimenez,Qing Zhao,Daniel Racoceanu,Matias Cosarinsky,Enzo Ferrante,Guanghui Fu*

Main category: eess.IV

TL;DR: The paper introduces the Segmentation Performance Evaluator (SPE), a framework for estimating medical image segmentation model performance on unlabeled data without needing ground truth annotations.


<details>
  <summary>Details</summary>
Motivation: Evaluating segmentation models in clinical settings is challenging due to the impracticality of annotating all data, leading to uncertain performance on unseen data.

Method: The SPE framework estimates performance using adaptable metrics (e.g., Dice score, HD95) and is tested on six datasets.

Result: SPE achieved high correlation (0.956±0.046) and low MAE (0.025±0.019) compared to real Dice scores, proving its reliability.

Conclusion: SPE enables practical performance estimation for medical image segmentation without annotations, facilitating real-world application.

Abstract: The performance of medical image segmentation models is usually evaluated
using metrics like the Dice score and Hausdorff distance, which compare
predicted masks to ground truth annotations. However, when applying the model
to unseen data, such as in clinical settings, it is often impractical to
annotate all the data, making the model's performance uncertain. To address
this challenge, we propose the Segmentation Performance Evaluator (SPE), a
framework for estimating segmentation models' performance on unlabeled data.
This framework is adaptable to various evaluation metrics and model
architectures. Experiments on six publicly available datasets across six
evaluation metrics including pixel-based metrics such as Dice score and
distance-based metrics like HD95, demonstrated the versatility and
effectiveness of our approach, achieving a high correlation (0.956$\pm$0.046)
and low MAE (0.025$\pm$0.019) compare with real Dice score on the independent
test set. These results highlight its ability to reliably estimate model
performance without requiring annotations. The SPE framework integrates
seamlessly into any model training process without adding training overhead,
enabling performance estimation and facilitating the real-world application of
medical image segmentation algorithms. The source code is publicly available

</details>


### [319] [3DGR-CT: Sparse-View CT Reconstruction with a 3D Gaussian Representation](https://arxiv.org/abs/2312.15676)
*Yingtai Li,Xueming Fu,Han Li,Shang Zhao,Ruiyang Jin,S. Kevin Zhou*

Main category: eess.IV

TL;DR: A novel 3D Gaussian representation (3DGR) method is proposed for sparse-view CT reconstruction, outperforming state-of-the-art methods with higher accuracy and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Sparse-view CT reduces radiation exposure but suffers from noise and artifacts due to limited data. The paper aims to address this using 3DGR.

Method: The method uses 3D Gaussian representation with FBP-image-guided initialization and integrates a differentiable CT projector.

Result: 3DGR-CT achieves higher reconstruction accuracy and faster convergence than existing methods, and enables real-time physical simulation.

Conclusion: 3DGR-CT is a promising solution for sparse-view CT, offering clinical benefits and outperforming implicit neural representations.

Abstract: Sparse-view computed tomography (CT) reduces radiation exposure by acquiring
fewer projections, making it a valuable tool in clinical scenarios where
low-dose radiation is essential. However, this often results in increased noise
and artifacts due to limited data. In this paper we propose a novel 3D Gaussian
representation (3DGR) based method for sparse-view CT reconstruction. Inspired
by recent success in novel view synthesis driven by 3D Gaussian splatting, we
leverage the efficiency and expressiveness of 3D Gaussian representation as an
alternative to implicit neural representation. To unleash the potential of 3DGR
for CT imaging scenario, we propose two key innovations: (i) FBP-image-guided
Guassian initialization and (ii) efficient integration with a differentiable CT
projector. Extensive experiments and ablations on diverse datasets demonstrate
the proposed 3DGR-CT consistently outperforms state-of-the-art counterpart
methods, achieving higher reconstruction accuracy with faster convergence.
Furthermore, we showcase the potential of 3DGR-CT for real-time physical
simulation, which holds important clinical applications while challenging for
implicit neural representations.

</details>


### [320] [Unsupervised Hyperspectral and Multispectral Image Fusion via Self-Supervised Modality Decoupling](https://arxiv.org/abs/2412.04802)
*Songcheng Du,Yang Zou,Zixu Wang,Xingyuan Li,Ying Li,Changjing Shang,Qiang Shen*

Main category: eess.IV

TL;DR: The paper proposes MossFuse, an unsupervised HMIF method that decouples and aggregates modality-shared and complementary information to improve fusion performance.


<details>
  <summary>Details</summary>
Motivation: Current HMIF methods lack effective supervision, leading to incomplete modality-complementary perception and limited inter-modality correlation understanding.

Method: MossFuse decouples shared and complementary information across modalities and uses subspace clustering loss for clear feature separation.

Result: MossFuse outperforms existing methods with fewer parameters and reduced inference time.

Conclusion: Modality decoupling is key to HMIF, and MossFue provides a simple, effective solution.

Abstract: Hyperspectral and Multispectral Image Fusion (HMIF) aims to fuse
low-resolution hyperspectral images (LR-HSIs) and high-resolution multispectral
images (HR-MSIs) to reconstruct high spatial and high spectral resolution
images. Current methods typically apply direct fusion from the two modalities
without effective supervision, leading to an incomplete perception of deep
modality-complementary information and a limited understanding of
inter-modality correlations. To address these issues, we propose a simple yet
effective solution for unsupervised HMIF, revealing that modality decoupling is
key to improving fusion performance. Specifically, we propose an end-to-end
self-supervised \textbf{Mo}dality-Decoupled \textbf{S}patial-\textbf{S}pectral
Fusion (\textbf{MossFuse}) framework that decouples shared and complementary
information across modalities and aggregates a concise representation of both
LR-HSIs and HR-MSIs to reduce modality redundancy. Also, we introduce the
subspace clustering loss as a clear guide to decouple modality-shared features
from modality-complementary ones. Systematic experiments over multiple datasets
demonstrate that our simple and effective approach consistently outperforms the
existing HMIF methods while requiring considerably fewer parameters with
reduced inference time. The anonymous source code is in
\href{https://github.com/dusongcheng/MossFuse}{MossFuse}.

</details>
