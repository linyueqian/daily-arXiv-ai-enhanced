<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 126]
- [cs.CV](#cs.CV) [Total: 176]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.SD](#cs.SD) [Total: 11]
- [cs.LG](#cs.LG) [Total: 212]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 5]
- [eess.AS](#eess.AS) [Total: 10]
- [eess.IV](#eess.IV) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EvidenceOutcomes: a Dataset of Clinical Trial Publications with Clinically Meaningful Outcomes](https://arxiv.org/pdf/2506.05380)
*Yiliang Zhou, Abigail M. Newbury, Gongbo Zhang, Betina Ross Idnay, Hao Liu, Chunhua Weng, Yifan Peng*

Main category: cs.CL

TL;DR: The paper introduces EvidenceOutcomes, a high-quality annotated corpus for extracting clinically meaningful outcomes from biomedical literature, addressing gaps in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks often neglect or oversimplify Outcomes in PICO elements, prompting the need for a robust corpus for better extraction and synthesis in evidence-based medicine.

Method: Developed annotation guidelines with clinicians and NLP experts, annotated 500 PubMed abstracts and 140 from EBM-NLP, and fine-tuned PubMedBERT for outcome extraction.

Result: Achieved high inter-rater agreement (0.76) and PubMedBERT performance (F1-scores: 0.69 entity, 0.76 token).

Conclusion: EvidenceOutcomes serves as a benchmark for future ML algorithms to extract clinically meaningful outcomes from biomedical literature.

Abstract: The fundamental process of evidence extraction and synthesis in
evidence-based medicine involves extracting PICO (Population, Intervention,
Comparison, and Outcome) elements from biomedical literature. However,
Outcomes, being the most complex elements, are often neglected or
oversimplified in existing benchmarks. To address this issue, we present
EvidenceOutcomes, a novel, large, annotated corpus of clinically meaningful
outcomes extracted from biomedical literature. We first developed a robust
annotation guideline for extracting clinically meaningful outcomes from text
through iteration and discussion with clinicians and Natural Language
Processing experts. Then, three independent annotators annotated the Results
and Conclusions sections of a randomly selected sample of 500 PubMed abstracts
and 140 PubMed abstracts from the existing EBM-NLP corpus. This resulted in
EvidenceOutcomes with high-quality annotations of an inter-rater agreement of
0.76. Additionally, our fine-tuned PubMedBERT model, applied to these 500
PubMed abstracts, achieved an F1-score of 0.69 at the entity level and 0.76 at
the token level on the subset of 140 PubMed abstracts from the EBM-NLP corpus.
EvidenceOutcomes can serve as a shared benchmark to develop and test future
machine learning algorithms to extract clinically meaningful outcomes from
biomedical abstracts.

</details>


### [2] [LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models](https://arxiv.org/pdf/2506.05385)
*Xinxin Li, Huiyao Chen, Chengjun Liu, Jing Li, Meishan Zhang, Jun Yu, Min Zhang*

Main category: cs.CL

TL;DR: LLMs equipped with retrieval-augmented generation and self-correction outperform encoder-decoder models in SRL tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between generative LLMs and encoder-decoder models in semantic role labeling (SRL).

Method: Introduces retrieval-augmented generation (leveraging external linguistic knowledge) and self-correction (identifying and fixing inconsistent outputs) for LLMs.

Result: Achieves state-of-the-art performance on CPB1.0, CoNLL-2009, and CoNLL-2012 benchmarks in Chinese and English.

Conclusion: First successful application of LLMs surpassing encoder-decoder models in SRL.

Abstract: Semantic role labeling (SRL) is a crucial task of natural language processing
(NLP). Although generative decoder-based large language models (LLMs) have
achieved remarkable success across various NLP tasks, they still lag behind
state-of-the-art encoder-decoder (BERT-like) models in SRL. In this work, we
seek to bridge this gap by equipping LLMs for SRL with two mechanisms: (a)
retrieval-augmented generation and (b) self-correction. The first mechanism
enables LLMs to leverage external linguistic knowledge such as predicate and
argument structure descriptions, while the second allows LLMs to identify and
correct inconsistent SRL outputs. We conduct extensive experiments on three
widely-used benchmarks of SRL (CPB1.0, CoNLL-2009, and CoNLL-2012). Results
demonstrate that our method achieves state-of-the-art performance in both
Chinese and English, marking the first successful application of LLMs to
surpass encoder-decoder approaches in SRL.

</details>


### [3] [Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes](https://arxiv.org/pdf/2506.05386)
*Lo Pang-Yun Ting, Chengshuai Zhao, Yu-Hua Zeng, Yuan Jee Lim, Kun-Ta Chuang*

Main category: cs.CL

TL;DR: R2AG, a reinforced retriever for long-form discharge instruction generation, uses reinforcement learning and a medical knowledge graph to improve LLM performance in clinical note generation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods struggle with generating long-form clinical notes from limited patient data, necessitating a more effective approach.

Method: Proposes R2AG, which retrieves reasoning paths from a medical knowledge graph using reinforcement learning, and GRO to enhance retrieval quality with group-relative rewards.

Result: R2AG outperforms baselines on the MIMIC-IV-Note dataset in clinical efficacy and natural language generation metrics.

Conclusion: R2AG effectively fills semantic gaps in sparse input scenarios and improves LLM reasoning by focusing on key evidence, reducing clinical misinterpretation.

Abstract: Clinical note generation aims to automatically produce free-text summaries of
a patient's condition and diagnostic process, with discharge instructions being
a representative long-form example. While recent large language model
(LLM)-based methods pre-trained on general clinical corpora show promise in
clinical text generation, they fall short in producing long-form notes from
limited patient information. In this paper, we propose R2AG, the first
reinforced retriever for long-form discharge instruction generation based on
pre-admission data. R2AG is trained with reinforcement learning to retrieve
reasoning paths from a medical knowledge graph, providing explicit semantic
guidance to the LLM. To bridge the information gap, we propose Group-Based
Retriever Optimization (GRO) which improves retrieval quality with
group-relative rewards, encouraging reasoning leaps for deeper inference by the
LLM. Comprehensive experiments on the MIMIC-IV-Note dataset show that R2AG
outperforms baselines in both clinical efficacy and natural language generation
metrics. Further analysis reveals that R2AG fills semantic gaps in sparse input
scenarios, and retrieved reasoning paths help LLMs avoid clinical
misinterpretation by focusing on key evidence and following coherent reasoning.

</details>


### [4] [Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs](https://arxiv.org/pdf/2506.05387)
*Jaydip Sen, Saptarshi Sengupta. Subhasis Dasgupta*

Main category: cs.CL

TL;DR: The paper introduces Adaptive Semantic-Aware Typicality Sampling (ASTS), an improved decoding strategy for LLMs, outperforming traditional methods in fluency, diversity, and coherence.


<details>
  <summary>Details</summary>
Motivation: Traditional decoding methods like top-k and nucleus sampling fail to balance fluency, diversity, and coherence in text generation.

Method: ASTS enhances Locally Typical Sampling with dynamic entropy thresholding, multi-objective scoring, and reward-penalty adjustments.

Result: ASTS outperforms existing techniques, reducing repetition, improving semantic alignment, and enhancing fluency across benchmarks.

Conclusion: ASTS is a superior decoding strategy for LLMs, ensuring coherent and diverse text generation efficiently.

Abstract: This chapter explores advancements in decoding strategies for large language
models (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)
algorithm. Traditional decoding methods, such as top-k and nucleus sampling,
often struggle to balance fluency, diversity, and coherence in text generation.
To address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)
is proposed as an improved version of LTS, incorporating dynamic entropy
thresholding, multi-objective scoring, and reward-penalty adjustments. ASTS
ensures contextually coherent and diverse text generation while maintaining
computational efficiency. Its performance is evaluated across multiple
benchmarks, including story generation and abstractive summarization, using
metrics such as perplexity, MAUVE, and diversity scores. Experimental results
demonstrate that ASTS outperforms existing sampling techniques by reducing
repetition, enhancing semantic alignment, and improving fluency.

</details>


### [5] [taz2024full: Analysing German Newspapers for Gender Bias and Discrimination across Decades](https://arxiv.org/pdf/2506.05388)
*Stefanie Urchs, Veronika Thurner, Matthias Aßenmacher, Christian Heumann, Stephanie Thiemichen*

Main category: cs.CL

TL;DR: The paper introduces taz2024full, the largest open-access German newspaper corpus (1.8M articles, 1980-2024), and demonstrates its use for gender bias research, revealing male overrepresentation but recent progress toward balance.


<details>
  <summary>Details</summary>
Motivation: Limited large-scale German corpora hinder NLP and CSS research, especially on societal issues like gender bias.

Method: A scalable, structured analysis pipeline is used to study gender representation, actor mentions, sentiment, and framing in the corpus.

Result: The corpus reveals consistent male overrepresentation but a gradual shift toward balanced gender coverage in recent years.

Conclusion: taz2024full enables diverse applications in German NLP and CSS, promoting inclusive and reproducible research.

Abstract: Open-access corpora are essential for advancing natural language processing
(NLP) and computational social science (CSS). However, large-scale resources
for German remain limited, restricting research on linguistic trends and
societal issues such as gender bias. We present taz2024full, the largest
publicly available corpus of German newspaper articles to date, comprising over
1.8 million texts from taz, spanning 1980 to 2024.
  As a demonstration of the corpus's utility for bias and discrimination
research, we analyse gender representation across four decades of reporting. We
find a consistent overrepresentation of men, but also a gradual shift toward
more balanced coverage in recent years. Using a scalable, structured analysis
pipeline, we provide a foundation for studying actor mentions, sentiment, and
linguistic framing in German journalistic texts.
  The corpus supports a wide range of applications, from diachronic language
analysis to critical media studies, and is freely available to foster inclusive
and reproducible research in German-language NLP.

</details>


### [6] [Multi-Agent Collaboration via Cross-Team Orchestration](https://arxiv.org/pdf/2406.08979)
*Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, YiFei Wang, Rennai Qiu, Yufan Dang, Weize Chen, Cheng Yang, Ye Tian, Xuantang Xiong, Lei Han*

Main category: cs.CL

TL;DR: Croto is a multi-team framework for LLM-driven agents to explore multiple decision paths, improving software quality and generalizing to other tasks like story generation.


<details>
  <summary>Details</summary>
Motivation: Current LLM-driven agent teams complete only one development chain, missing opportunities for better solutions by not exploring multiple paths.

Method: Introduces Cross-Team Orchestration (Croto), enabling teams to propose diverse solutions and collaborate for superior outcomes.

Result: Experiments show improved software quality and promising generalization in tasks like story generation.

Conclusion: Croto enhances solution exploration and collaboration among LLM-driven teams, outperforming baselines and generalizing across domains.

Abstract: Large Language Models (LLMs) have significantly impacted various domains,
especially through organized LLM-driven autonomous agents. A representative
scenario is in software development, where agents can collaborate in a team
like humans, following predefined phases to complete sub-tasks sequentially.
However, for an agent team, each phase yields only one possible outcome. This
results in the completion of only one development chain, thereby losing the
opportunity to explore multiple potential decision paths within the solution
space. Consequently leading to suboptimal results or extensive trial and error.
To address this, we introduce Cross-Team Orchestration (Croto), a scalable
multi-team framework that enables orchestrated teams to jointly propose various
task-oriented solutions and interact with their insights in a self-independence
while cross-team collaboration environment for superior solutions generation.
Experiments reveal a notable increase in software quality compared to
state-of-the-art baselines. We further tested our framework on story generation
tasks, which demonstrated a promising generalization ability of our framework
in other domains. The code and data is available at
https://github.com/OpenBMB/ChatDev/tree/macnet

</details>


### [7] [Understanding Gender Bias in AI-Generated Product Descriptions](https://arxiv.org/pdf/2506.05390)
*Markelle Kelly, Mohammad Tahaei, Padhraic Smyth, Lauren Wilcox*

Main category: cs.CL

TL;DR: The paper explores gender bias in LLMs for e-commerce, focusing on product description generation, identifying unique biases like clothing size assumptions and stereotypical advertising, and analyzing GPT-3.5 and an e-commerce LLM.


<details>
  <summary>Details</summary>
Motivation: To uncover novel forms of gender bias in LLMs applied to e-commerce, an understudied area, and align findings with existing AI harm frameworks.

Method: Developed a taxonomy of gender bias in product descriptions, analyzed biases in GPT-3.5 and an e-commerce LLM, and compared them to general AI harm categories.

Result: Found unique biases (e.g., clothing size assumptions, stereotypical advertising) in both models, highlighting exclusionary norms, stereotyping, and performance disparities.

Conclusion: The study reveals under-explored gender biases in e-commerce LLMs, emphasizing the need for specialized detection and mitigation strategies.

Abstract: While gender bias in large language models (LLMs) has been extensively
studied in many domains, uses of LLMs in e-commerce remain largely unexamined
and may reveal novel forms of algorithmic bias and harm. Our work investigates
this space, developing data-driven taxonomic categories of gender bias in the
context of product description generation, which we situate with respect to
existing general purpose harms taxonomies. We illustrate how AI-generated
product descriptions can uniquely surface gender biases in ways that require
specialized detection and mitigation approaches. Further, we quantitatively
analyze issues corresponding to our taxonomic categories in two models used for
this task -- GPT-3.5 and an e-commerce-specific LLM -- demonstrating that these
forms of bias commonly occur in practice. Our results illuminate unique,
under-explored dimensions of gender bias, such as assumptions about clothing
size, stereotypical bias in which features of a product are advertised, and
differences in the use of persuasive language. These insights contribute to our
understanding of three types of AI harms identified by current frameworks:
exclusionary norms, stereotyping, and performance disparities, particularly for
the context of e-commerce.

</details>


### [8] [Are Large Language Models Good Temporal Graph Learners?](https://arxiv.org/pdf/2506.05393)
*Shenyang Huang, Ali Parviz, Emma Kondrup, Zachary Yang, Zifeng Ding, Michael Bronstein, Reihaneh Rabbany, Guillaume Rabusseau*

Main category: cs.CL

TL;DR: TGTalker is a new framework applying LLMs to dynamic graphs, achieving competitive link prediction and providing textual explanations.


<details>
  <summary>Details</summary>
Motivation: The gap in applying LLMs to real-world dynamic graphs, unlike synthetic ones, drives the need for TGTalker.

Method: TGTalker leverages recency bias and temporal neighbors, converting graph data to natural language for LLMs.

Result: TGTalker matches or outperforms state-of-the-art TGNN models in link prediction across five real-world networks.

Conclusion: TGTalker advances temporal graph learning with LLMs, offering explainability and competitive performance.

Abstract: Large Language Models (LLMs) have recently driven significant advancements in
Natural Language Processing and various other applications. While a broad range
of literature has explored the graph-reasoning capabilities of LLMs, including
their use of predictors on graphs, the application of LLMs to dynamic graphs --
real world evolving networks -- remains relatively unexplored. Recent work
studies synthetic temporal graphs generated by random graph models, but
applying LLMs to real-world temporal graphs remains an open question. To
address this gap, we introduce Temporal Graph Talker (TGTalker), a novel
temporal graph learning framework designed for LLMs. TGTalker utilizes the
recency bias in temporal graphs to extract relevant structural information,
converted to natural language for LLMs, while leveraging temporal neighbors as
additional information for prediction. TGTalker demonstrates competitive link
prediction capabilities compared to existing Temporal Graph Neural Network
(TGNN) models. Across five real-world networks, TGTalker performs competitively
with state-of-the-art temporal graph methods while consistently outperforming
popular models such as TGN and HTGN. Furthermore, TGTalker generates textual
explanations for each prediction, thus opening up exciting new directions in
explainability and interpretability for temporal link prediction. The code is
publicly available at https://github.com/shenyangHuang/TGTalker.

</details>


### [9] [Auto Review: Second Stage Error Detection for Highly Accurate Information Extraction from Phone Conversations](https://arxiv.org/pdf/2506.05400)
*Ayesha Qamar, Arushi Raghuvanshi, Conal Sathi, Youngseo Son*

Main category: cs.CL

TL;DR: Auto Review automates post-call verification in healthcare, reducing manual effort while maintaining accuracy. A second-stage pipeline improves transcript quality by leveraging multiple ASR alternatives and pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: Manual verification of noisy call transcripts is labor-intensive. Automating this process can save time and improve patient care.

Method: Introduces Auto Review and a second-stage postprocessing pipeline using multiple ASR alternatives and pseudo-labeling without manual corrections.

Result: Substantial improvements in transcript accuracy and efficiency of Auto Review.

Conclusion: The proposed pipeline enhances automated verification, addressing ASR and domain-specific jargon challenges.

Abstract: Automating benefit verification phone calls saves time in healthcare and
helps patients receive treatment faster. It is critical to obtain highly
accurate information in these phone calls, as it can affect a patient's
healthcare journey. Given the noise in phone call transcripts, we have a
two-stage system that involves a post-call review phase for potentially noisy
fields, where human reviewers manually verify the extracted
data$\unicode{x2013}$a labor-intensive task. To automate this stage, we
introduce Auto Review, which significantly reduces manual effort while
maintaining a high bar for accuracy. This system, being highly reliant on call
transcripts, suffers a performance bottleneck due to automatic speech
recognition (ASR) issues. This problem is further exacerbated by the use of
domain-specific jargon in the calls. In this work, we propose a second-stage
postprocessing pipeline for accurate information extraction. We improve
accuracy by using multiple ASR alternatives and a pseudo-labeling approach that
does not require manually corrected transcripts. Experiments with
general-purpose large language models and feature-based model pipelines
demonstrate substantial improvements in the quality of corrected call
transcripts, thereby enhancing the efficiency of Auto Review.

</details>


### [10] [Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs](https://arxiv.org/pdf/2506.05410)
*Wanyun Cui, Mingwei Xu*

Main category: cs.CL

TL;DR: The paper introduces AsymKV, a training-free compression framework for KV caches in LLMs, addressing the asymmetry between keys and values to improve long-context modeling efficiency.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of attention mechanisms in LLMs makes long-context modeling inefficient. Existing KV cache compression methods overlook the asymmetry between keys (locally homogeneous) and values (heterogeneous), limiting their effectiveness.

Method: Proposes AsymKV, combining homogeneity-based key merging with lossless value compression, leveraging the observed asymmetry in KV caches.

Result: AsymKV outperforms existing methods, achieving an average score of 43.95 on LongBench with LLaMA3.1-8B, surpassing H$_2$O (38.89).

Conclusion: The asymmetry in KV caches is critical for efficient compression. AsymKV provides a superior, training-free solution for long-context modeling in LLMs.

Abstract: Recent advances in Large Language Models (LLMs) have highlighted the critical
importance of extending context length, yet the quadratic complexity of
attention mechanisms poses significant challenges for efficient long-context
modeling. KV cache compression has emerged as a key approach to address this
challenge. Through extensive empirical analysis, we reveal a fundamental yet
previously overlooked asymmetry in KV caches: while adjacent keys receive
similar attention weights (local homogeneity), adjacent values demonstrate
distinct heterogeneous distributions. This key-value asymmetry reveals a
critical limitation in existing compression methods that treat keys and values
uniformly. To address the limitation, we propose a training-free compression
framework (AsymKV) that combines homogeneity-based key merging with a
mathematically proven lossless value compression. Extensive experiments
demonstrate that AsymKV consistently outperforms existing long-context methods
across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV
achieves an average score of 43.95 on LongBench, surpassing SOTA methods like
H$_2$O (38.89) by a large margin.

</details>


### [11] [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/pdf/2506.05413)
*Patrik Czakó, Gábor Kertész, Sándor Szénási*

Main category: cs.CL

TL;DR: SmoothRot is a post-training quantization method for 4-bit LLMs, using channel-wise scaling and Hadamard transformations to handle activation outliers, improving accuracy without extra latency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of activation outliers in 4-bit quantization of LLMs, which degrade performance.

Method: Integrates channel-wise scaling with Hadamard transformations to transform outliers into quantization-friendly activations.

Result: Reduces performance gap between quantized and FP16 models by 10-30% on tasks like language generation and zero-shot reasoning.

Conclusion: SmoothRot effectively enhances 4-bit quantization efficiency in LLMs without adding inference latency.

Abstract: We present SmoothRot, a novel post-training quantization technique to enhance
the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot
addresses the critical challenge of massive activation outliers, by integrating
channel-wise scaling with Hadamard transformations. Our technique effectively
transforms extreme outliers into quantization-friendly activations,
significantly improving quantization accuracy. Experiments conducted on popular
LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot
consistently reduces the performance gap between quantized and FP16 models by
approximately 10-30\% across language generation and zero-shot reasoning tasks,
without introducing additional inference latency. Code is available at
https://github.com/czakop/smoothrot.

</details>


### [12] [Automatically Detecting Amusing Games in Wordle](https://arxiv.org/pdf/2506.05415)
*Ronaldo Luo, Gary Liang, Cindy Liu, Adam Kabbara, Minahil Bakhtawar, Kina Kim, Michael Guerzhoy*

Main category: cs.CL

TL;DR: The paper investigates predicting Reddit users' amusement at Wordle games using GPT-3.5 for reaction classification and feature extraction, showing weak but measurable predictability.


<details>
  <summary>Details</summary>
Motivation: To computationally predict which Wordle games amuse Reddit users, exploring the intersection of humor and creativity in games.

Method: Scraped 80k Reddit reactions, classified amusement using GPT-3.5, extracted game features, and analyzed their predictive power.

Result: Features weakly predict user amusement, confirming measurable creativity in Wordle games.

Conclusion: User amusement in Wordle is computationally predictable, revealing aspects of humor-infused creativity.

Abstract: We explore automatically predicting which Wordle games Reddit users find
amusing.
  We scrape approximately 80k reactions by Reddit users to Wordle games from
Reddit, classify the reactions as expressing amusement or not using OpenAI's
GPT-3.5 using few-shot prompting, and verify that GPT-3.5's labels roughly
correspond to human labels.
  We then extract features from Wordle games that can predict user amusement.
We demonstrate that the features indeed provide a (weak) signal that predicts
user amusement as predicted by GPT-3.5.
  Our results indicate that user amusement at Wordle games can be predicted
computationally to some extent. We explore which features of the game
contribute to user amusement.
  We find that user amusement is predictable, indicating a measurable aspect of
creativity infused into Wordle games through humor.

</details>


### [13] [TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages](https://arxiv.org/pdf/2402.16021)
*Minsu Kim, Jee-weon Jung, Hyeongseop Rha, Soumi Maiti, Siddhant Arora, Xuankai Chang, Shinji Watanabe, Yong Man Ro*

Main category: cs.CL

TL;DR: Proposes Tri-Modal Translation (TMT) model for translating between speech, image, and text by treating modalities as languages, reducing computational costs and outperforming single-model approaches.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of limited paired multi-modal data and high computational demands in multi-modal learning.

Method: Tokenizes speech and image data into discrete tokens, using a multi-modal encoder-decoder for translation, with modality-specific steps only in tokenization/detokenization.

Result: TMT outperforms single-model counterparts across all six modality translation tasks.

Conclusion: Unifying multi-modal tasks improves both practicality and performance.

Abstract: The capability to jointly process multi-modal information is becoming an
essential task. However, the limited number of paired multi-modal data and the
large computational requirements in multi-modal learning hinder the
development. We propose a novel Tri-Modal Translation (TMT) model that
translates between arbitrary modalities spanning speech, image, and text. We
introduce a novel viewpoint, where we interpret different modalities as
different languages, and treat multi-modal translation as a well-established
machine translation problem. To this end, we tokenize speech and image data
into discrete tokens, which provide a unified interface across modalities and
significantly decrease the computational cost. In the proposed TMT, a
multi-modal encoder-decoder conducts the core translation, whereas
modality-specific processing is conducted only within the tokenization and
detokenization stages. We evaluate the proposed TMT on all six modality
translation tasks. TMT outperforms single model counterparts consistently,
demonstrating that unifying tasks is beneficial not only for practicality but
also for performance.

</details>


### [14] [MLLM-CL: Continual Learning for Multimodal Large Language Models](https://arxiv.org/pdf/2506.05453)
*Hongbo Zhao, Fei Zhu, Rundong Wang, Gaofeng Meng, Zhaoxiang Zhang*

Main category: cs.CL

TL;DR: MLLM-CL introduces a benchmark for continual learning in multimodal large language models, addressing domain and ability adaptation with a parameter isolation method, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with dynamic real-world scenarios requiring continuous learning, and existing benchmarks/methods are limited.

Method: Proposes parameter isolation to prevent catastrophic interference and an MLLM-based routing mechanism.

Result: The approach integrates new knowledge and skills with minimal forgetting, outperforming existing methods.

Conclusion: MLLM-CL effectively addresses continual learning challenges in MLLMs, demonstrating superior performance.

Abstract: Recent Multimodal Large Language Models (MLLMs) excel in vision-language
understanding but face challenges in adapting to dynamic real-world scenarios
that require continuous integration of new knowledge and skills. While
continual learning (CL) offers a potential solution, existing benchmarks and
methods suffer from critical limitations. In this paper, we introduce MLLM-CL,
a novel benchmark encompassing domain and ability continual learning, where the
former focuses on independently and identically distributed (IID) evaluation
across evolving mainstream domains, whereas the latter evaluates on non-IID
scenarios with emerging model ability. Methodologically, we propose preventing
catastrophic interference through parameter isolation, along with an MLLM-based
routing mechanism. Extensive experiments demonstrate that our approach can
integrate domain-specific knowledge and functional abilities with minimal
forgetting, significantly outperforming existing methods.

</details>


### [15] [WER We Stand: Benchmarking Urdu ASR Models](https://arxiv.org/pdf/2409.11252)
*Samee Arif, Sualeha Farid, Aamina Jamal Khan, Mustafa Abbas, Agha Ali Raza, Awais Athar*

Main category: cs.CL

TL;DR: The paper evaluates Urdu ASR models (Whisper, MMS, Seamless-M4T) using WER and error analysis on read and conversational speech datasets. Seamless-large excels in read speech, while Whisper-large leads in conversational speech. It highlights challenges in assessing low-resource languages and the need for Urdu text normalization.


<details>
  <summary>Details</summary>
Motivation: To benchmark and analyze the performance of Urdu ASR models, addressing gaps in evaluation for low-resource languages and introducing a new conversational speech dataset.

Method: Evaluated three ASR models (Whisper, MMS, Seamless-M4T) using WER and error analysis (insertions, deletions, substitutions) on read and conversational speech datasets.

Result: Seamless-large performed best on read speech, while Whisper-large excelled on conversational speech. The study revealed challenges in quantitative assessment for Urdu ASR.

Conclusion: The findings underscore the need for robust Urdu text normalization and provide insights for improving ASR systems in low-resource languages.

Abstract: This paper presents a comprehensive evaluation of Urdu Automatic Speech
Recognition (ASR) models. We analyze the performance of three ASR model
families: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along
with a detailed examination of the most frequent wrong words and error types
including insertions, deletions, and substitutions. Our analysis is conducted
using two types of datasets, read speech and conversational speech. Notably, we
present the first conversational speech dataset designed for benchmarking Urdu
ASR models. We find that seamless-large outperforms other ASR models on the
read speech dataset, while whisper-large performs best on the conversational
speech dataset. Furthermore, this evaluation highlights the complexities of
assessing ASR models for low-resource languages like Urdu using quantitative
metrics alone and emphasizes the need for a robust Urdu text normalization
system. Our findings contribute valuable insights for developing robust ASR
systems for low-resource languages like Urdu.

</details>


### [16] [Multidimensional Analysis of Specific Language Impairment Using Unsupervised Learning Through PCA and Clustering](https://arxiv.org/pdf/2506.05498)
*Niruthiha Selvanayagam*

Main category: cs.CL

TL;DR: The study uses unsupervised machine learning to analyze language development in children with and without SLI, identifying two primary clusters and supporting a continuum model of language abilities.


<details>
  <summary>Details</summary>
Motivation: To improve early identification and targeted interventions for SLI by uncovering natural language development trajectories.

Method: Analyzed narrative samples from 1,163 children using PCA and clustering on 64 linguistic features.

Result: Identified two clusters: high production/low SLI and limited production/high syntactic complexity/high SLI, with boundary cases supporting a continuum model.

Conclusion: SLI is linked to reduced production capacity, not syntactic deficits, challenging categorical diagnostics and advocating for unsupervised learning in refining interventions.

Abstract: Specific Language Impairment (SLI) affects approximately 7 percent of
children, presenting as isolated language deficits despite normal cognitive
abilities, sensory systems, and supportive environments. Traditional diagnostic
approaches often rely on standardized assessments, which may overlook subtle
developmental patterns. This study aims to identify natural language
development trajectories in children with and without SLI using unsupervised
machine learning techniques, providing insights for early identification and
targeted interventions. Narrative samples from 1,163 children aged 4-16 years
across three corpora (Conti-Ramsden 4, ENNI, and Gillam) were analyzed using
Principal Component Analysis (PCA) and clustering. A total of 64 linguistic
features were evaluated to uncover developmental trajectories and distinguish
linguistic profiles. Two primary clusters emerged: (1) high language production
with low SLI prevalence, and (2) limited production but higher syntactic
complexity with higher SLI prevalence. Additionally, boundary cases exhibited
intermediate traits, supporting a continuum model of language abilities.
Findings suggest SLI manifests primarily through reduced production capacity
rather than syntactic complexity deficits. The results challenge categorical
diagnostic frameworks and highlight the potential of unsupervised learning
techniques for refining diagnostic criteria and intervention strategies.

</details>


### [17] [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/pdf/2505.15670)
*Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr Żelasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg*

Main category: cs.CL

TL;DR: A novel duplex speech-to-speech (S2S) architecture enables real-time adaptability, outperforming previous models in reasoning, turn-taking, and barge-in abilities while halving bitrate and skipping speech pretraining.


<details>
  <summary>Details</summary>
Motivation: Current speech models lack real-time adaptability like user barge-in, limiting intuitive human-computer interaction.

Method: Proposes a duplex S2S architecture with continuous user inputs and codec agent outputs, using a pretrained streaming encoder and separate architectures for user and agent modeling.

Result: Outperforms previous models in reasoning, turn-taking, and barge-in, reduces bitrate to 0.6 kbps, and requires less speech data by skipping pretraining.

Conclusion: The model simplifies building duplex S2S systems from LLMs and is the first openly available with training and inference code for reproducibility.

Abstract: Spoken dialogue is an intuitive form of human-computer interaction, yet
current speech language models often remain constrained to turn-based
exchanges, lacking real-time adaptability such as user barge-in. We propose a
novel duplex speech to speech (S2S) architecture featuring continuous user
inputs and codec agent outputs with channel fusion that directly models
simultaneous user and agent streams. Using a pretrained streaming encoder for
user input enables the first duplex S2S model without requiring speech
pretrain. Separate architectures for agent and user modeling facilitate codec
fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared
to previous works. Experimental results show that the proposed model
outperforms previous duplex models in reasoning, turn-taking, and barge-in
abilities. The model requires significantly less speech data, as speech
pretrain is skipped, which markedly simplifies the process of building a duplex
S2S model from any LLMs. Finally, it is the first openly available duplex S2S
model with training and inference code to foster reproducibility.

</details>


### [18] [Improving LLMs with a knowledge from databases](https://arxiv.org/pdf/2506.05560)
*Petr Máša*

Main category: cs.CL

TL;DR: The paper proposes using enhanced association rules to improve LLM answers via RAG, showing better performance than ChatGPT.


<details>
  <summary>Details</summary>
Motivation: To address the lack of control and safety in LLM-generated commands while improving answer accuracy using structured data.

Method: Generates rulesets from knowledge patterns, converts them to text, and integrates them into LLM via RAG.

Result: Significant improvement in dataset-based question answering compared to ChatGPT.

Conclusion: The method is promising and can be further enhanced with additional patterns or rule mining as an agent.

Abstract: Large language models (LLMs) are achieving significant progress almost every
moment now. Many advanced techniques have been introduced and widely accepted,
like retrieval-augmentation generation (RAG), agents, and tools. Tools can
query the database to answer questions from structured data files or perform
groupings or other statistics. This unlocks huge opportunities, such as it can
answer any question, but also poses threats, such as safety, because there is
no control over the commands that are created. We would like to discuss whether
we can create a new method that improves answers based on dataset/database via
some interpretable ML methods, namely enhanced association rules. The advantage
would be if the method can be also used in some safe technique like RAG.
Association rules have a sound history. Since the introduction of CN2 and
aproiri, many enhancements have been made. In parallel, enhanced association
rules have been introduced and evolved over the last 40 years. The general
problem is typically that there are too many rules. There are some techniques
for handling it, but when LLM emerged, it turned out to be the best use case
for the RAG technique for LLMs. We proposed a method that generates a ruleset
based on defined knowledge patterns, then converts rules into text form via a
rule-to-text converter, and includes the result as an RAG into LLM. We compared
this method with ChatGPT (even with using agents) and we have discovered a
significant improvement in answering questions based on the dataset. We have
also tried several strategies how much rules to generate. We found this
improvement interesting. Moreover, it can also be improved in many ways as
future work, like incorporating other patterns, the use of rule mining as an
agent, and many others.

</details>


### [19] [Combating Misinformation in the Arab World: Challenges & Opportunities](https://arxiv.org/pdf/2506.05582)
*Azza Abouzied, Firoj Alam, Raian Ali, Paolo Papotti*

Main category: cs.CL

TL;DR: The paper addresses misinformation challenges in the Arab region, focusing on detection, tracking, mitigation, and community engagement to build resilience.


<details>
  <summary>Details</summary>
Motivation: The Arab region's unique vulnerabilities to misinformation due to geopolitical, linguistic, and cultural factors drive the need for tailored solutions.

Method: The study explores strategies like grassroots fact-checking, cultural understanding, social correction, and collaborative networks.

Result: The approach highlights opportunities for a more resilient information ecosystem in the Arab world.

Conclusion: Community engagement and cultural awareness are key to combating misinformation in the region.

Abstract: Misinformation and disinformation pose significant risks globally, with the
Arab region facing unique vulnerabilities due to geopolitical instabilities,
linguistic diversity, and cultural nuances. We explore these challenges through
the key facets of combating misinformation: detection, tracking, mitigation and
community-engagement. We shed light on how connecting with grass-roots
fact-checking organizations, understanding cultural norms, promoting social
correction, and creating strong collaborative information networks can create
opportunities for a more resilient information ecosystem in the Arab world.

</details>


### [20] [UTSA-NLP at ArchEHR-QA 2025: Improving EHR Question Answering via Self-Consistency Prompting](https://arxiv.org/pdf/2506.05589)
*Sara Shields-Menard, Zach Reimers, Joshua Gardner, David Perry, Anthony Rios*

Main category: cs.CL

TL;DR: A system using large language models for clinical QA tasks, focusing on EHRs, with two-step processing: sentence relevance classification and response generation.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and reliability of answering clinical questions using EHRs by leveraging large language models.

Method: Uses few-shot prompting, self-consistency, and thresholding for sentence classification, comparing models (8B vs. 70B).

Result: Smaller 8B model outperforms larger 70B model for relevance identification; self-consistency with thresholding improves reliability.

Conclusion: Accurate sentence selection is key for high-quality responses, and self-consistency with thresholding enhances decision reliability.

Abstract: We describe our system for the ArchEHR-QA Shared Task on answering clinical
questions using electronic health records (EHRs). Our approach uses large
language models in two steps: first, to find sentences in the EHR relevant to a
clinician's question, and second, to generate a short, citation-supported
response based on those sentences. We use few-shot prompting, self-consistency,
and thresholding to improve the sentence classification step to decide which
sentences are essential. We compare several models and find that a smaller 8B
model performs better than a larger 70B model for identifying relevant
information. Our results show that accurate sentence selection is critical for
generating high-quality responses and that self-consistency with thresholding
helps make these decisions more reliable.

</details>


### [21] [SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs](https://arxiv.org/pdf/2506.05598)
*Michael J Ryan, Omar Shaikh, Aditri Bhagirath, Daniel Frees, William Held, Diyi Yang*

Main category: cs.CL

TL;DR: SynthesizeMe improves personalized LLM alignment by generating synthetic user personas from interactions, boosting accuracy by 4.4% on Chatbot Arena.


<details>
  <summary>Details</summary>
Motivation: Addressing the reliance on additional identity info for personalized reward models by leveraging user interactions.

Method: Generates and verifies reasoning for user preferences, induces synthetic personas, and filters interactions to build personalized prompts.

Result: 4.4% accuracy improvement on Chatbot Arena; top performance on PersonalRewardBench.

Conclusion: SynthesizeMe effectively enhances personalized LLM alignment without needing extra identity data.

Abstract: Recent calls for pluralistic alignment of Large Language Models (LLMs)
encourage adapting models to diverse user preferences. However, most prior work
on personalized reward models heavily rely on additional identity information,
such as demographic details or a predefined set of preference categories. To
this end, we introduce SynthesizeMe, an approach to inducing synthetic user
personas from user interactions for personalized reward modeling. SynthesizeMe
first generates and verifies reasoning to explain user preferences, then
induces synthetic user personas from that reasoning, and finally filters to
informative prior user interactions in order to build personalized prompts for
a particular user. We show that using SynthesizeMe induced prompts improves
personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining
SynthesizeMe derived prompts with a reward model achieves top performance on
PersonalRewardBench: a new curation of user-stratified interactions with
chatbots collected from 854 users of Chatbot Arena and PRISM.

</details>


### [22] [OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation](https://arxiv.org/pdf/2506.05606)
*Ziyi Wang, Yuxuan Lu, Wenbo Li, Amirali Amini, Bo Sun, Yakov Bart, Weimin Lyu, Jiri Gesi, Tian Wang, Jing Huang, Yu Su, Upol Ehsan, Malihe Alikhani, Toby Jia-Jun Li, Lydia Chilton, Dakuo Wang*

Main category: cs.CL

TL;DR: The paper introduces OPERA, a dataset for evaluating LLMs' ability to simulate user web actions, including personas, observations, actions, and rationales.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets for evaluating LLMs' simulation of real user behaviors, especially in web interactions.

Method: Developed OPERA dataset using an online questionnaire and a custom browser plugin to capture user personas, observations, actions, and rationales.

Result: Established a benchmark for evaluating LLMs' prediction of user actions and rationales.

Conclusion: OPERA enables future research on LLM agents as personalized digital twins for humans.

Abstract: Can large language models (LLMs) accurately simulate the next web action of a
specific user? While LLMs have shown promising capabilities in generating
``believable'' human behaviors, evaluating their ability to mimic real user
behaviors remains an open challenge, largely due to the lack of high-quality,
publicly available datasets that capture both the observable actions and the
internal reasoning of an actual human user. To address this gap, we introduce
OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected
from real human participants during online shopping sessions. OPERA is the
first public dataset that comprehensively captures: user personas, browser
observations, fine-grained web actions, and self-reported just-in-time
rationales. We developed both an online questionnaire and a custom browser
plugin to gather this dataset with high fidelity. Using OPERA, we establish the
first benchmark to evaluate how well current LLMs can predict a specific user's
next action and rationale with a given persona and <observation, action,
rationale> history. This dataset lays the groundwork for future research into
LLM agents that aim to act as personalized digital twins for human.

</details>


### [23] [Mitigating Confounding in Speech-Based Dementia Detection through Weight Masking](https://arxiv.org/pdf/2506.05610)
*Zhecheng Sheng, Xiruo Ding, Brian Hur, Changye Li, Trevor Cohen, Serguei Pakhomov*

Main category: cs.CL

TL;DR: The paper explores gender confounding in dementia detection using transformer models and proposes two methods to mitigate it, showing trade-offs in performance.


<details>
  <summary>Details</summary>
Motivation: Address the lack of research on gender effects in dementia detection using pre-trained language models, aiming to reduce bias.

Method: Proposes the Extended Confounding Filter and Dual Filter to isolate and ablate gender-related weights in transformer models.

Result: Transformer models overfit to training data; disrupting gender weights deconfounds the classifier but slightly reduces dementia detection accuracy.

Conclusion: Mitigating gender confounding is possible but comes at a cost to detection performance, highlighting a trade-off in bias reduction.

Abstract: Deep transformer models have been used to detect linguistic anomalies in
patient transcripts for early Alzheimer's disease (AD) screening. While
pre-trained neural language models (LMs) fine-tuned on AD transcripts perform
well, little research has explored the effects of the gender of the speakers
represented by these transcripts. This work addresses gender confounding in
dementia detection and proposes two methods: the $\textit{Extended Confounding
Filter}$ and the $\textit{Dual Filter}$, which isolate and ablate weights
associated with gender. We evaluate these methods on dementia datasets with
first-person narratives from patients with cognitive impairment and healthy
controls. Our results show transformer models tend to overfit to training data
distributions. Disrupting gender-related weights results in a deconfounded
dementia classifier, with the trade-off of slightly reduced dementia detection
performance.

</details>


### [24] [Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs](https://arxiv.org/pdf/2506.05629)
*Ananth Muppidi, Abhilash Nandy, Sambaran Bandyopadhyay*

Main category: cs.CL

TL;DR: Proposes ID-SPAM, a parameter-efficient fine-tuning method using input-dependent soft prompts with self-attention, improving performance and zero-shot transfer.


<details>
  <summary>Details</summary>
Motivation: Addresses the computational and technical challenges of fine-tuning large language models for domain-specific tasks.

Method: Introduces ID-SPAM, which generates soft prompts based on input tokens and uses self-attention to vary token importance.

Result: Outperforms state-of-the-art techniques on various tasks and enhances zero-shot domain transfer.

Conclusion: ID-SPAM is a simple, efficient solution for fine-tuning with minimal trainable parameters.

Abstract: The performance of large language models in domain-specific tasks
necessitates fine-tuning, which is computationally expensive and technically
challenging. This paper focuses on parameter-efficient fine-tuning using soft
prompting, a promising approach that adapts pre-trained models to downstream
tasks by learning a small set of parameters. We propose a novel Input Dependent
Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that
generates soft prompts based on the input tokens and attends different tokens
with varying importance. Our method is simple and efficient, keeping the number
of trainable parameters small. We show the merits of the proposed approach
compared to state-of-the-art techniques on various tasks and show the improved
zero shot domain transfer capability.

</details>


### [25] [IYKYK: Using language models to decode extremist cryptolects](https://arxiv.org/pdf/2506.05635)
*Christine de Kock, Arij Riabi, Zeerak Talat, Michael Sejr Schlichtkrull, Pranava Madhyastha, Ed Hovy*

Main category: cs.CL

TL;DR: Current language models struggle with detecting extremist cryptolects, but domain adaptation and specialized prompting improve performance. Datasets of 19.4M posts are released.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of language technologies in detecting and interpreting extremist in-group language (cryptolects) for automated moderation.

Method: Evaluated eight models across six tasks, focusing on detection and decoding of extremist language, with domain adaptation and specialized prompting.

Result: General-purpose LLMs fail consistently, but performance improves with domain-specific adjustments.

Conclusion: Specialized techniques and datasets are crucial for effective automated moderation of extremist content.

Abstract: Extremist groups develop complex in-group language, also referred to as
cryptolects, to exclude or mislead outsiders. We investigate the ability of
current language technologies to detect and interpret the cryptolects of two
online extremist platforms. Evaluating eight models across six tasks, our
results indicate that general purpose LLMs cannot consistently detect or decode
extremist language. However, performance can be significantly improved by
domain adaptation and specialised prompting techniques. These results provide
important insights to inform the development and deployment of automated
moderation technologies. We further develop and release novel labelled and
unlabelled datasets, including 19.4M posts from extremist platforms and
lexicons validated by human experts.

</details>


### [26] [A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition](https://arxiv.org/pdf/2506.05639)
*John Kirchenbauer, Janny Mongkolsupawan, Yuxin Wen, Tom Goldstein, Daphne Ippolito*

Main category: cs.CL

TL;DR: A new dataset is proposed to study how language models memorize facts and verbatim sequences, using synthetic fictional data.


<details>
  <summary>Details</summary>
Motivation: To better understand how language models memorize facts and verbatim sequences, which is less understood compared to sequence memorization.

Method: Creation of a synthetic dataset with fictional events and question-answer pairs, followed by training experiments to analyze memorization.

Result: Synthetic data effectively distinguishes between fact and verbatim memorization, though creating realistic fictional data poses challenges.

Conclusion: The proposed dataset aids in studying memorization processes, highlighting the potential and difficulties of synthetic data for such research.

Abstract: When language models are trained on textual data, they acquire both knowledge
about the structure of language as well as knowledge of facts about the world.
At inference time, their knowledge of facts can be leveraged to solve
interesting problems and perform useful knowledge work for users. It is well
known that language models can verbatim memorize long sequences from their
training data. However, it is much less well understood how language models
memorize facts seen during training. In this work, we propose a new dataset to
specifically empower researchers to study the dual processes of fact
memorization and verbatim sequence memorization. The dataset consists of
synthetically-generated, webtext-like documents about fictional events, as well
as question-answer pairs about the events. We conduct training experiments
showing how synthetic data about fictional events can be effective in teasing
apart different forms of memorization. We also document the challenges in
effectively building realistic, fictional synthetic data.

</details>


### [27] [Can LLMs Express Personality Across Cultures? Introducing CulturalPersonas for Evaluating Trait Alignment](https://arxiv.org/pdf/2506.05670)
*Priyanka Dey, Yugal Khanter, Aayush Bothra, Jieyu Zhao, Emilio Ferrara*

Main category: cs.CL

TL;DR: CulturalPersonas is a benchmark for evaluating LLMs' personality expression in culturally rich contexts, improving alignment with human norms.


<details>
  <summary>Details</summary>
Motivation: Address the overlooked interplay between culture and personality in LLMs, crucial for applications like tutoring and mental health.

Method: Introduces CulturalPersonas, a dataset with 3,000 scenario-based questions across six countries, evaluated using multiple-choice and open-ended formats.

Result: Shows improved alignment with human personality distributions (20% reduction in Wasserstein distance) and more culturally coherent outputs.

Conclusion: CulturalPersonas advances LLM alignment with global cultural norms, enabling more socially intelligent models.

Abstract: As LLMs become central to interactive applications, ranging from tutoring to
mental health, the ability to express personality in culturally appropriate
ways is increasingly important. While recent works have explored personality
evaluation of LLMs, they largely overlook the interplay between culture and
personality. To address this, we introduce CulturalPersonas, the first
large-scale benchmark with human validation for evaluating LLMs' personality
expression in culturally grounded, behaviorally rich contexts. Our dataset
spans 3,000 scenario-based questions across six diverse countries, designed to
elicit personality through everyday scenarios rooted in local values. We
evaluate three LLMs, using both multiple-choice and open-ended response
formats. Our results show that CulturalPersonas improves alignment with
country-specific human personality distributions (over a 20% reduction in
Wasserstein distance across models and countries) and elicits more expressive,
culturally coherent outputs compared to existing benchmarks. CulturalPersonas
surfaces meaningful modulated trait outputs in response to culturally grounded
prompts, offering new directions for aligning LLMs to global norms of behavior.
By bridging personality expression and cultural nuance, we envision that
CulturalPersonas will pave the way for more socially intelligent and globally
adaptive LLMs.

</details>


### [28] [Zero-Shot Event Causality Identification via Multi-source Evidence Fuzzy Aggregation with Large Language Models](https://arxiv.org/pdf/2506.05675)
*Zefan Zeng, Xingchen Hu, Qing Cheng, Weiping Ding, Wentao Li, Zhong Liu*

Main category: cs.CL

TL;DR: MEFA is a zero-shot framework using Multi-source Evidence Fuzzy Aggregation to improve Event Causality Identification (ECI), reducing reliance on annotated data and causal hallucination in LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing ECI models depend on large annotated datasets, and LLMs suffer from causal hallucination, leading to spurious causal links.

Method: MEFA decomposes causality into three main and three auxiliary tasks, uses prompts to guide LLMs, and aggregates evidence via fuzzy logic for scoring.

Result: MEFA outperforms baselines by 6.2% in F1-score and 9.3% in precision, reducing hallucination errors.

Conclusion: Task decomposition and fuzzy aggregation in MEFA effectively enhance zero-shot ECI performance.

Abstract: Event Causality Identification (ECI) aims to detect causal relationships
between events in textual contexts. Existing ECI models predominantly rely on
supervised methodologies, suffering from dependence on large-scale annotated
data. Although Large Language Models (LLMs) enable zero-shot ECI, they are
prone to causal hallucination-erroneously establishing spurious causal links.
To address these challenges, we propose MEFA, a novel zero-shot framework based
on Multi-source Evidence Fuzzy Aggregation. First, we decompose causality
reasoning into three main tasks (temporality determination, necessity analysis,
and sufficiency verification) complemented by three auxiliary tasks. Second,
leveraging meticulously designed prompts, we guide LLMs to generate uncertain
responses and deterministic outputs. Finally, we quantify LLM's responses of
sub-tasks and employ fuzzy aggregation to integrate these evidence for
causality scoring and causality determination. Extensive experiments on three
benchmarks demonstrate that MEFA outperforms second-best unsupervised baselines
by 6.2% in F1-score and 9.3% in precision, while significantly reducing
hallucination-induced errors. In-depth analysis verify the effectiveness of
task decomposition and the superiority of fuzzy aggregation.

</details>


### [29] [A Unified Representation for Continuity and Discontinuity: Syntactic and Computational Motivations](https://arxiv.org/pdf/2506.05686)
*Ratna Kandala, Prakash Mondal*

Main category: cs.CL

TL;DR: The paper proposes a unified representation for PSG, DG, and CG, simplifying syntactic and computational complexity, demonstrated through Turkish subordinate clauses.


<details>
  <summary>Details</summary>
Motivation: To integrate and unify the representational principles of PSG, DG, and CG for syntactic and computational simplicity.

Method: Introduces the correspondence principle and illustrates its application with Turkish discontinuous subordinate clauses.

Result: Achieves a unified representation that simplifies computational complexity and enhances syntactic analysis.

Conclusion: The unified approach integrates PSG, DG, and CG, offering theoretical and computational benefits for linguistic analysis.

Abstract: This paper advances a unified representation of linguistic structure for
three grammar formalisms, namely, Phrase Structure Grammar (PSG), Dependency
Grammar (DG) and Categorial Grammar (CG) from the perspective of syntactic and
computational complexity considerations. The correspondence principle is
proposed to enable a unified representation of the representational principles
from PSG, DG, and CG. To that end, the paper first illustrates a series of
steps in achieving a unified representation for a discontinuous subordinate
clause from Turkish as an illustrative case. This affords a new way of
approaching discontinuity in natural language from a theoretical point of view
that unites and integrates the basic tenets of PSG, DG, and CG, with
significant consequences for syntactic analysis. Then this paper demonstrates
that a unified representation can simplify computational complexity with
regards to the neurocognitive representation and processing of both continuous
and discontinuous sentences vis-\`a-vis the basic principles of PSG, DG, and
CG.

</details>


### [30] [When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation](https://arxiv.org/pdf/2506.05690)
*Zhishang Xiang, Chuanjie Wu, Qinggang Zhang, Shengyuan Chen, Zijin Hong, Xiao Huang, Jinsong Su*

Main category: cs.CL

TL;DR: GraphRAG-Bench is introduced to evaluate GraphRAG's effectiveness, comparing it to vanilla RAG across tasks like fact retrieval and reasoning, aiming to identify scenarios where graph structures benefit RAG systems.


<details>
  <summary>Details</summary>
Motivation: Recent studies show GraphRAG underperforming vanilla RAG, prompting the need to assess its effectiveness and identify scenarios where graph structures enhance RAG systems.

Method: Proposes GraphRAG-Bench, a benchmark with diverse tasks (fact retrieval, reasoning, summarization, generation) and systematic evaluation of GraphRAG's pipeline.

Result: GraphRAG-Bench provides insights into when GraphRAG outperforms vanilla RAG and why, offering practical guidelines.

Conclusion: GraphRAG-Bench serves as a tool to understand GraphRAG's strengths, aiding its practical application in RAG systems.

Abstract: Graph retrieval-augmented generation (GraphRAG) has emerged as a powerful
paradigm for enhancing large language models (LLMs) with external knowledge. It
leverages graphs to model the hierarchical structure between specific concepts,
enabling more coherent and effective knowledge retrieval for accurate
reasoning.Despite its conceptual promise, recent studies report that GraphRAG
frequently underperforms vanilla RAG on many real-world tasks. This raises a
critical question: Is GraphRAG really effective, and in which scenarios do
graph structures provide measurable benefits for RAG systems? To address this,
we propose GraphRAG-Bench, a comprehensive benchmark designed to evaluate
GraphRAG models onboth hierarchical knowledge retrieval and deep contextual
reasoning. GraphRAG-Bench features a comprehensive dataset with tasks of
increasing difficulty, coveringfact retrieval, complex reasoning, contextual
summarization, and creative generation, and a systematic evaluation across the
entire pipeline, from graph constructionand knowledge retrieval to final
generation. Leveraging this novel benchmark, we systematically investigate the
conditions when GraphRAG surpasses traditional RAG and the underlying reasons
for its success, offering guidelines for its practical application. All related
resources and analyses are collected for the community at
https://github.com/GraphRAG-Bench/GraphRAG-Benchmark.

</details>


### [31] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/pdf/2506.05695)
*Lingyuan Liu, Mengxiang Zhang*

Main category: cs.CL

TL;DR: The paper proposes POCL, a curriculum learning framework for Knowledge Distillation (KD) in LLMs, addressing distribution shifts and improving stability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods for LLMs suffer from issues like catastrophic forgetting and training-inference mismatch due to distribution shifts.

Method: POCL introduces a difficulty measurer and training scheduler to progressively train the student model from easy to hard samples with rising loss temperatures.

Result: Experiments show POCL consistently improves distilled student models across various KD methods and model families.

Conclusion: POCL effectively structures training data in KD to enhance stability and performance of distilled LLMs.

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [32] [RKEFino1: A Regulation Knowledge-Enhanced Large Language Model](https://arxiv.org/pdf/2506.05700)
*Yan Wang, Yueru He, Ruoyu Xiang, Jeff Zhao*

Main category: cs.CL

TL;DR: RKEFino1, a regulation-enhanced LLM, improves financial compliance tasks by integrating domain knowledge and introducing new QA and NER tasks.


<details>
  <summary>Details</summary>
Motivation: Address accuracy and compliance challenges in Digital Regulatory Reporting (DRR) using LLMs.

Method: Propose RKEFino1, fine-tuned with XBRL, CDM, and MOF knowledge, and introduce QA and Numerical NER tasks.

Result: RKEFino1 shows effectiveness and generalization in financial compliance tasks.

Conclusion: RKEFino1 is a promising solution for compliance-critical financial applications, now available on Hugging Face.

Abstract: Recent advances in large language models (LLMs) hold great promise for
financial applications but introduce critical accuracy and compliance
challenges in Digital Regulatory Reporting (DRR). To address these issues, we
propose RKEFino1, a regulation knowledge-enhanced financial reasoning model
built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We
formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce
a novel Numerical NER task covering financial entities in both sentences and
tables. Experimental results demonstrate the effectiveness and generalization
capacity of RKEFino1 in compliance-critical financial tasks. We have released
our model on Hugging Face.

</details>


### [33] [Large Language Models are Good Relational Learners](https://arxiv.org/pdf/2506.05725)
*Fang Wu, Vijay Prakash Dwivedi, Jure Leskovec*

Main category: cs.CL

TL;DR: Rel-LLM introduces a GNN-based encoder to generate structured relational prompts for LLMs, preserving database structures and outperforming text-based serialization methods in RDL tasks.


<details>
  <summary>Details</summary>
Motivation: Existing text-based serialization of relational data for LLMs ignores critical structures, introduces redundancy, and exceeds context limits.

Method: Uses a GNN encoder to extract local subgraphs, creating structured relational prompts within a RAG framework.

Result: Rel-LLM outperforms existing methods on RDL tasks, offering scalable integration of LLMs with structured data.

Conclusion: Rel-LLM effectively bridges LLMs and relational data, preserving structures and improving performance.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various domains, yet their application to relational deep learning (RDL)
remains underexplored. Existing approaches adapt LLMs by traversing relational
links between entities in a database and converting the structured data into
flat text documents. Still, this text-based serialization disregards critical
relational structures, introduces redundancy, and often exceeds standard LLM
context lengths. We introduce Rel-LLM, a novel architecture that utilizes a
graph neural network (GNN)- based encoder to generate structured relational
prompts for LLMs within a retrieval-augmented generation (RAG) framework.
Unlike traditional text-based serialization approaches, our method preserves
the inherent relational structure of databases while enabling LLMs to
effectively process and reason over complex entity relationships. Specifically,
the GNN encoder extracts a local subgraph around an entity to build feature
representations that contain relevant entity relationships and temporal
dependencies. These representations are transformed into structured prompts
using a denormalization process, effectively allowing the LLM to reason over
relational structures. Through extensive experiments, we demonstrate that
Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and
efficient approach to integrating LLMs with structured data sources. Code is
available at https://github.com/smiles724/Rel-LLM.

</details>


### [34] [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/pdf/2506.05735)
*Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, Olgica Milenkovic, Pan Li*

Main category: cs.CL

TL;DR: A framework for evaluating machine unlearning in LLMs using knowledge graphs and LLM judges to assess implicit knowledge retention.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods focus on explicit fact removal, ignoring latent dependencies and non-deterministic knowledge in LLMs, leading to incomplete unlearning.

Method: Proposes a knowledge unlearning evaluation framework using knowledge graphs and LLM judges with calibrated prompts to assess unlearning success.

Result: Experiments show the framework provides a more realistic assessment, revealing current strategies overestimate unlearning effectiveness.

Conclusion: The proposed framework offers a rigorous and realistic evaluation of unlearning, highlighting gaps in current methods.

Abstract: Machine unlearning techniques aim to mitigate unintended memorization in
large language models (LLMs). However, existing approaches predominantly focus
on the explicit removal of isolated facts, often overlooking latent inferential
dependencies and the non-deterministic nature of knowledge within LLMs.
Consequently, facts presumed forgotten may persist implicitly through
correlated information. To address these challenges, we propose a knowledge
unlearning evaluation framework that more accurately captures the implicit
structure of real-world knowledge by representing relevant factual contexts as
knowledge graphs with associated confidence scores. We further develop an
inference-based evaluation protocol leveraging powerful LLMs as judges; these
judges reason over the extracted knowledge subgraph to determine unlearning
success. Our LLM judges utilize carefully designed prompts and are calibrated
against human evaluations to ensure their trustworthiness and stability.
Extensive experiments on our newly constructed benchmark demonstrate that our
framework provides a more realistic and rigorous assessment of unlearning
performance. Moreover, our findings reveal that current evaluation strategies
tend to overestimate unlearning effectiveness. Our code is publicly available
at https://github.com/Graph-COM/Knowledge_Unlearning.git.

</details>


### [35] [LLM-Symbolic Integration for Robust Temporal Tabular Reasoning](https://arxiv.org/pdf/2506.05746)
*Atharv Kulkarni, Kushagra Dixit, Vivek Srikumar, Dan Roth, Vivek Gupta*

Main category: cs.CL

TL;DR: TempTabQA-C introduces a synthetic dataset and symbolic intermediate representation to improve LLMs' temporal tabular QA, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional prompting methods for LLMs struggle with temporal tabular QA due to memorization, table size sensitivity, and poor complex query handling.

Method: Uses a synthetic dataset (TempTabQA-C) and symbolic intermediate representation (tables to database schemas) with adaptive few-shot prompting for SQL query generation.

Result: Achieves superior robustness, scalability, and performance, setting a new benchmark for temporal reasoning with LLMs.

Conclusion: The structured approach enhances generalization and mitigates biases, improving LLM performance on temporal tabular QA tasks.

Abstract: Temporal tabular question answering presents a significant challenge for
Large Language Models (LLMs), requiring robust reasoning over structured data,
which is a task where traditional prompting methods often fall short. These
methods face challenges such as memorization, sensitivity to table size, and
reduced performance on complex queries. To overcome these limitations, we
introduce TempTabQA-C, a synthetic dataset designed for systematic and
controlled evaluations, alongside a symbolic intermediate representation that
transforms tables into database schemas. This structured approach allows LLMs
to generate and execute SQL queries, enhancing generalization and mitigating
biases. By incorporating adaptive few-shot prompting with contextually tailored
examples, our method achieves superior robustness, scalability, and
performance. Experimental results consistently highlight improvements across
key challenges, setting a new benchmark for robust temporal reasoning with
LLMs.

</details>


### [36] [Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning](https://arxiv.org/pdf/2506.05760)
*Xuanyu Lei, Chenliang Li, Yuning Wu, Kaiming Liu, Weizhou Shen, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu*

Main category: cs.CL

TL;DR: Writing-RL, an Adaptive Curriculum Reinforcement Learning framework, improves long-form writing by addressing SFT limitations with data selection, reward mechanisms, and dynamic reference scheduling.


<details>
  <summary>Details</summary>
Motivation: Existing SFT approaches for LLMs suffer from data saturation and limited learning capacity, prompting the need for a more effective framework.

Method: The framework includes Margin-aware Data Selection, Pairwise Comparison Reward, and Dynamic Reference Scheduling to adaptively enhance learning.

Result: Experiments show significant improvement in long-form writing performance and surprising generalization to long-input reasoning tasks.

Conclusion: Writing-RL offers a promising approach for advancing long-context training and rethinking LLM capabilities.

Abstract: Recent advances in Large Language Models (LLMs) have enabled strong
performance in long-form writing, yet existing supervised fine-tuning (SFT)
approaches suffer from limitations such as data saturation and restricted
learning capacity bounded by teacher signals. In this work, we present
Writing-RL: an Adaptive Curriculum Reinforcement Learning framework to advance
long-form writing capabilities beyond SFT. The framework consists of three key
components: Margin-aware Data Selection strategy that prioritizes samples with
high learning potential, Pairwise Comparison Reward mechanism that provides
discriminative learning signals in the absence of verifiable rewards, and
Dynamic Reference Scheduling approach, which plays a particularly critical role
by adaptively adjusting task difficulty based on evolving model performance.
Experiments on 7B-scale writer models show that our RL framework largely
improves long-form writing performance over strong SFT baselines. Furthermore,
we observe that models trained with long-output RL generalize surprisingly well
to long-input reasoning tasks, potentially offering a promising perspective for
rethinking long-context training.

</details>


### [37] [BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over Bio-Molecular Interactions](https://arxiv.org/pdf/2506.05766)
*Saptarshi Sengupta, Shuhua Yang, Paul Kwong Yu, Fali Wang, Suhang Wang*

Main category: cs.CL

TL;DR: BioMol-MQA introduces a multimodal QA dataset for polypharmacy, combining text and molecular structures in a KG, highlighting LLMs' struggles without strong RAG frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing RAG-based LLMs focus on single-modality (text) retrieval, but real-world problems like healthcare require multimodal (KG, text, molecular) retrieval and reasoning.

Method: Developed BioMol-MQA, a QA dataset with a multimodal KG (text and molecular structures) and challenging questions to test LLM capabilities.

Result: Benchmarks show LLMs struggle without proper background data, emphasizing the need for robust RAG frameworks.

Conclusion: BioMol-MQA addresses the gap in multimodal retrieval and reasoning, underscoring the importance of advanced RAG for LLMs in complex domains like healthcare.

Abstract: Retrieval augmented generation (RAG) has shown great power in improving Large
Language Models (LLMs). However, most existing RAG-based LLMs are dedicated to
retrieving single modality information, mainly text; while for many real-world
problems, such as healthcare, information relevant to queries can manifest in
various modalities such as knowledge graph, text (clinical notes), and complex
molecular structure. Thus, being able to retrieve relevant multi-modality
domain-specific information, and reason and synthesize diverse knowledge to
generate an accurate response is important. To address the gap, we present
BioMol-MQA, a new question-answering (QA) dataset on polypharmacy, which is
composed of two parts (i) a multimodal knowledge graph (KG) with text and
molecular structure for information retrieval; and (ii) challenging questions
that designed to test LLM capabilities in retrieving and reasoning over
multimodal KG to answer questions. Our benchmarks indicate that existing LLMs
struggle to answer these questions and do well only when given the necessary
background data, signaling the necessity for strong RAG frameworks.

</details>


### [38] [dots.llm1 Technical Report](https://arxiv.org/pdf/2506.05767)
*Bi Huo, Bin Tu, Cheng Qin, Da Zheng, Debing Zhang, Dongjie Zhang, En Li, Fu Guo, Jian Yao, Jie Lou, Junfeng Tian, Li Hu, Ran Zhu, Shengdong Chen, Shuo Liu, Su Guang, Te Wo, Weijun Zhang, Xiaoming Shi, Xinxin Peng, Xing Wu, Yawen Liu, Yuqiu Ji, Ze Wen, Zhenhai Liu, Zichao Li, Zilong Liao*

Main category: cs.CL

TL;DR: dots.llm1 is a large-scale Mixture of Experts (MoE) model with 142B parameters, activating only 14B per token. It matches state-of-the-art performance while reducing costs, trained on 11.2T tokens without synthetic data.


<details>
  <summary>Details</summary>
Motivation: To scale language models efficiently by reducing computational costs while maintaining performance.

Method: Uses MoE architecture, activating a subset of parameters per token, and a high-quality data processing pipeline.

Result: Achieves performance comparable to Qwen2.5-72B with lower training and inference costs.

Conclusion: dots.llm1 demonstrates efficient scaling of language models and opens research avenues with open-sourced checkpoints.

Abstract: Mixture of Experts (MoE) models have emerged as a promising paradigm for
scaling language models efficiently by activating only a subset of parameters
for each input token. In this report, we present dots.llm1, a large-scale MoE
model that activates 14B parameters out of a total of 142B parameters,
delivering performance on par with state-of-the-art models while reducing
training and inference costs. Leveraging our meticulously crafted and efficient
data processing pipeline, dots.llm1 achieves performance comparable to
Qwen2.5-72B after pretraining on 11.2T high-quality tokens and post-training to
fully unlock its capabilities. Notably, no synthetic data is used during
pretraining. To foster further research, we open-source intermediate training
checkpoints at every one trillion tokens, providing valuable insights into the
learning dynamics of large language models.

</details>


### [39] [Discrete Minds in a Continuous World: Do Language Models Know Time Passes?](https://arxiv.org/pdf/2506.05790)
*Minghan Wang, Ye Bai, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari*

Main category: cs.CL

TL;DR: LLMs show some awareness of time passage, adapting behavior based on token-time mapping, urgency, and time pressure, though capabilities vary by model size.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can perceive and adapt to the passage of time, beyond just temporal reasoning tasks.

Method: Three experiments: validating the Token-Time Hypothesis, testing response adaptation under urgency, and evaluating behavior in time-pressured navigation (BombRush).

Result: LLMs demonstrate time awareness, linking tokens to physical time, and adapt behavior, but performance depends on model size and reasoning.

Conclusion: This work lays groundwork for improving LLMs' temporal awareness in time-sensitive applications.

Abstract: While Large Language Models (LLMs) excel at temporal reasoning tasks like
event ordering and duration estimation, their ability to perceive the actual
passage of time remains unexplored. We investigate whether LLMs perceive the
passage of time and adapt their decision-making accordingly through three
complementary experiments. First, we introduce the Token-Time Hypothesis,
positing that LLMs can map discrete token counts to continuous wall-clock time,
and validate this through a dialogue duration judgment task. Second, we
demonstrate that LLMs could use this awareness to adapt their response length
while maintaining accuracy when users express urgency in question answering
tasks. Finally, we develop BombRush, an interactive navigation challenge that
examines how LLMs modify behavior under progressive time pressure in dynamic
environments. Our findings indicate that LLMs possess certain awareness of time
passage, enabling them to bridge discrete linguistic tokens and continuous
physical time, though this capability varies with model size and reasoning
abilities. This work establishes a theoretical foundation for enhancing
temporal awareness in LLMs for time-sensitive applications.

</details>


### [40] [MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning](https://arxiv.org/pdf/2506.05813)
*Ye Bai, Minghan Wang, Thuy-Trang Vu*

Main category: cs.CL

TL;DR: MAPLE is a multi-agent framework for table-based QA, mimicking human problem-solving with adaptive planning and long-term memory, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with complex reasoning in table-based QA, lacking error detection and experience reuse, unlike human problem-solving.

Method: MAPLE uses 4 components: Solver (ReAct reasoning), Checker (verification), Reflector (error correction), and Archiver (long-term memory).

Result: Achieves state-of-the-art performance on WiKiTQ and TabFact datasets across multiple LLM backbones.

Conclusion: MAPLE's feedback-driven, multi-agent approach effectively addresses limitations of existing methods, enhancing reasoning and experience reuse.

Abstract: Table-based question answering requires complex reasoning capabilities that
current LLMs struggle to achieve with single-pass inference. Existing
approaches, such as Chain-of-Thought reasoning and question decomposition, lack
error detection mechanisms and discard problem-solving experiences, contrasting
sharply with how humans tackle such problems. In this paper, we propose MAPLE
(Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that
mimics human problem-solving through specialized cognitive agents working in a
feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the
ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a
Reflector for error diagnosis and strategy correction, and (4) an Archiver
managing long-term memory for experience reuse and evolution. Experiments on
WiKiTQ and TabFact demonstrate significant improvements over existing methods,
achieving state-of-the-art performance across multiple LLM backbones.

</details>


### [41] [FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging](https://arxiv.org/pdf/2506.05828)
*Zichen Tang, Haihong E, Ziyan Ma, Haoyang He, Jiacheng Liu, Zhongjun Yang, Zihua Rong, Rongjin Li, Kun Ji, Qing Huang, Xinyang Hu, Yang Liu, Qianhe Zheng*

Main category: cs.CL

TL;DR: FinanceReasoning is a new benchmark for evaluating large reasoning models (LRMs) in financial numerical reasoning, offering credibility, comprehensiveness, and challenge.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack credibility, comprehensiveness, and challenge for evaluating LRMs in financial reasoning.

Method: Updated 15.6% of questions from public datasets, annotated 908 new questions with Python solutions, and constructed 3,133 Python-formatted functions. Also introduced 238 Hard problems requiring multiple financial formulas.

Result: Improved LRM performance (e.g., GPT-4o accuracy rose from 83.2% to 91.6%). Best model (OpenAI o1 with PoT) achieved 89.1% accuracy. Combining Reasoner and Programmer models further enhanced performance (e.g., DeepSeek-R1 improved from 83.2% to 87.8%).

Conclusion: FinanceReasoning advances LRM evaluation in financial reasoning, highlighting challenges in numerical precision and suggesting model combinations for improvement.

Abstract: We introduce FinanceReasoning, a novel benchmark designed to evaluate the
reasoning capabilities of large reasoning models (LRMs) in financial numerical
reasoning problems. Compared to existing benchmarks, our work provides three
key advancements. (1) Credibility: We update 15.6% of the questions from four
public datasets, annotating 908 new questions with detailed Python solutions
and rigorously refining evaluation standards. This enables an accurate
assessment of the reasoning improvements of LRMs. (2) Comprehensiveness:
FinanceReasoning covers 67.8% of financial concepts and formulas, significantly
surpassing existing datasets. Additionally, we construct 3,133 Python-formatted
functions, which enhances LRMs' financial reasoning capabilities through
refined knowledge (e.g., 83.2% $\rightarrow$ 91.6% for GPT-4o). (3) Challenge:
Models are required to apply multiple financial formulas for precise numerical
reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with
PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical
precision. We demonstrate that combining Reasoner and Programmer models can
effectively enhance LRMs' performance (e.g., 83.2% $\rightarrow$ 87.8% for
DeepSeek-R1). Our work paves the way for future research on evaluating and
improving LRMs in domain-specific complex reasoning tasks.

</details>


### [42] [Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models](https://arxiv.org/pdf/2506.05850)
*Cheonbok Park, Jeonghoon Kim, Joosung Lee, Sanghwan Bae, Jaegul Choo, Kangmin Yoo*

Main category: cs.CL

TL;DR: The paper identifies 'Cross-lingual Collapse,' where multilingual language models revert to their dominant pre-training language in reasoning tasks, even when prompted in another language. It explores this issue using GRPO on translated datasets, revealing imbalances and irreversible language drift.


<details>
  <summary>Details</summary>
Motivation: To investigate the mechanism behind multilingual reasoning in large reasoning models (LRMs) and understand the impact of pre-training language imbalances on reasoning tasks.

Method: Fine-tune multilingual LRMs with Group-Relative Policy Optimization (GRPO) on translated GSM8K and SimpleRL-Zoo datasets in Chinese, Korean, and Ukrainian, monitoring task accuracy and language consistency.

Result: GRPO amplifies language imbalances, eroding low-resource languages quickly. Language consistency rewards mitigate drift but reduce accuracy by 5-10 pp. The collapse is irreversible.

Conclusion: Not all languages are trained equally for reasoning, highlighting the roles of reward shaping, data difficulty, and pre-training priors in multilingual reasoning.

Abstract: We identify \textbf{Cross-lingual Collapse}, a systematic drift in which the
chain-of-thought (CoT) of a multilingual language model reverts to its dominant
pre-training language even when the prompt is expressed in a different
language. Recent large language models (LLMs) with reinforcement learning with
verifiable reward (RLVR) have achieved strong logical reasoning performances by
exposing their intermediate reasoning traces, giving rise to large reasoning
models (LRMs). However, the mechanism behind multilingual reasoning in LRMs is
not yet fully explored. To investigate the issue, we fine-tune multilingual
LRMs with Group-Relative Policy Optimization (GRPO) on translated versions of
the GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese,
Korean, and Ukrainian. During training, we monitor both task accuracy and
language consistency of the reasoning chains. Our experiments reveal three key
findings: (i) GRPO rapidly amplifies pre-training language imbalances, leading
to the erosion of low-resource languages within just a few hundred updates;
(ii) language consistency reward mitigates this drift but does so at the
expense of an almost 5 - 10 pp drop in accuracy. and (iii) the resulting
language collapse is severely damaging and largely irreversible, as subsequent
fine-tuning struggles to steer the model back toward its original
target-language reasoning capabilities. Together, these findings point to a
remarkable conclusion: \textit{not all languages are trained equally for
reasoning}. Furthermore, our paper sheds light on the roles of reward shaping,
data difficulty, and pre-training priors in eliciting multilingual reasoning.

</details>


### [43] [Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router](https://arxiv.org/pdf/2506.05901)
*Chenyang Shao, Xinyang Liu, Yutang Lin, Fengli Xu, Yong Li*

Main category: cs.CL

TL;DR: R2-Reasoner is a framework for collaborative reasoning across LLMs and SLMs, reducing costs by 86.85% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Hybrid approaches for task decomposition and allocation can improve efficiency and reduce costs in multi-step reasoning.

Method: Uses a Reinforced Model Router with a task decomposer and subtask allocator, trained via supervised fine-tuning and reinforcement learning.

Result: Reduces API costs by 86.85% while matching or exceeding baseline accuracy.

Conclusion: R2-Reasoner enables cost-effective and adaptive reasoning for LLMs.

Abstract: Multi-step reasoning has proven essential for enhancing the problem-solving
capabilities of Large Language Models (LLMs) by decomposing complex tasks into
intermediate steps, either explicitly or implicitly. Extending the reasoning
chain at test time through deeper thought processes or broader exploration, can
furthur improve performance, but often incurs substantial costs due to the
explosion in token usage. Yet, many reasoning steps are relatively simple and
can be handled by more efficient smaller-scale language models (SLMs). This
motivates hybrid approaches that allocate subtasks across models of varying
capacities. However, realizing such collaboration requires accurate task
decomposition and difficulty-aware subtask allocation, which is challenging. To
address this, we propose R2-Reasoner, a novel framework that enables
collaborative reasoning across heterogeneous LLMs by dynamically routing
sub-tasks based on estimated complexity. At the core of our framework is a
Reinforced Model Router, composed of a task decomposer and a subtask allocator.
The task decomposer segments complex input queries into logically ordered
subtasks, while the subtask allocator assigns each subtask to the most
appropriate model, ranging from lightweight SLMs to powerful LLMs, balancing
accuracy and efficiency. To train this router, we introduce a staged pipeline
that combines supervised fine-tuning on task-specific datasets with Group
Relative Policy Optimization algorithm, enabling self-supervised refinement
through iterative reinforcement learning. Extensive experiments across four
challenging benchmarks demonstrate that R2-Reasoner reduces API costs by 86.85%
while maintaining or surpassing baseline accuracy. Our framework paves the way
for more cost-effective and adaptive LLM reasoning. The code is open-source at
https://anonymous.4open.science/r/R2_Reasoner .

</details>


### [44] [Generating Grounded Responses to Counter Misinformation via Learning Efficient Fine-Grained Critiques](https://arxiv.org/pdf/2506.05924)
*Xiaofei Xu, Xiuzhen Zhang, Ke Deng*

Main category: cs.CL

TL;DR: MisMitiFact is an efficient framework for generating fact-grounded counter-responses to misinformation, using lightweight critique models to refine LLM outputs and achieve high throughput.


<details>
  <summary>Details</summary>
Motivation: The rise of fake news and misinformation demands scalable solutions, but manual fact-checking is costly and LLMs often hallucinate non-factual information.

Method: MisMitiFact uses fine-grained critique models trained on fact-checking data to identify and correct errors in LLM outputs, ensuring factual grounding.

Result: The framework matches LLM self-feedback quality while being 5x more efficient in feedback generation, enabling scalable misinformation mitigation.

Conclusion: MisMitiFact offers a cost-effective, high-throughput solution for automating fact-grounded counter-responses to misinformation.

Abstract: Fake news and misinformation poses a significant threat to society, making
efficient mitigation essential. However, manual fact-checking is costly and
lacks scalability. Large Language Models (LLMs) offer promise in automating
counter-response generation to mitigate misinformation, but a critical
challenge lies in their tendency to hallucinate non-factual information.
Existing models mainly rely on LLM self-feedback to reduce hallucination, but
this approach is computationally expensive. In this paper, we propose
MisMitiFact, Misinformation Mitigation grounded in Facts, an efficient
framework for generating fact-grounded counter-responses at scale. MisMitiFact
generates simple critique feedback to refine LLM outputs, ensuring responses
are grounded in evidence. We develop lightweight, fine-grained critique models
trained on data sourced from readily available fact-checking sites to identify
and correct errors in key elements such as numerals, entities, and topics in
LLM generations. Experiments show that MisMitiFact generates counter-responses
of comparable quality to LLMs' self-feedback while using significantly smaller
critique models. Importantly, it achieves ~5x increase in feedback generation
throughput, making it highly suitable for cost-effective, large-scale
misinformation mitigation. Code and LLM prompt templates are at
https://github.com/xxfwin/MisMitiFact.

</details>


### [45] [LengClaro2023: A Dataset of Administrative Texts in Spanish with Plain Language adaptations](https://arxiv.org/pdf/2506.05927)
*Belén Agüera-Marco, Itziar Gonzalez-Dios*

Main category: cs.CL

TL;DR: LengClaro2023 is a dataset of Spanish legal-administrative texts with two simplified versions for evaluating ATS systems.


<details>
  <summary>Details</summary>
Motivation: To provide a resource for evaluating automatic text simplification systems in Spanish, addressing the need for simplified legal-administrative texts.

Method: Created two simplified versions of texts from the Spanish Social Security website: one following arText claro recommendations and another incorporating plain language guidelines.

Result: A linguistic resource (LengClaro2023) with simplified legal-administrative texts for ATS evaluation.

Conclusion: LengClaro2023 serves as a valuable tool for assessing and improving ATS systems in Spanish.

Abstract: In this work, we present LengClaro2023, a dataset of legal-administrative
texts in Spanish. Based on the most frequently used procedures from the Spanish
Social Security website, we have created for each text two simplified
equivalents. The first version follows the recommendations provided by arText
claro. The second version incorporates additional recommendations from plain
language guidelines to explore further potential improvements in the system.
The linguistic resource created in this work can be used for evaluating
automatic text simplification (ATS) systems in Spanish.

</details>


### [46] [MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2506.05928)
*Jie Cao, Tianwei Lin, Hongyang He, Rolan Yan, Wenqiao Zhang, Juncheng Li, Dongping Zhang, Siliang Tang, Yueting Zhuang*

Main category: cs.CL

TL;DR: The paper proposes a heterogeneous Mixture-of-Adapters (MoA) method to improve parameter-efficient fine-tuning (PEFT) in LLMs, addressing issues like representation collapse and expert imbalance in homogeneous MoE-LoRA approaches.


<details>
  <summary>Details</summary>
Motivation: Existing homogeneous MoE-LoRA methods suffer from representation collapse and expert load imbalance, limiting LLM potential.

Method: Introduces MoA with two variants: Soft MoA (weighted fusion of expert outputs) and Sparse MoA (sparse expert activation).

Result: MoA outperforms homogeneous MoE-LoRA in performance and parameter efficiency.

Conclusion: Heterogeneous MoA enhances PEFT by leveraging diverse adapter structures, improving knowledge transfer to downstream tasks.

Abstract: Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts
(MoE) to further enhance the performance of parameter-efficient fine-tuning
(PEFT) methods in Large Language Model (LLM) applications. Existing methods
employ \emph{homogeneous} MoE-LoRA architectures composed of LoRA experts with
either similar or identical structures and capacities. However, these
approaches often suffer from representation collapse and expert load imbalance,
which negatively impact the potential of LLMs. To address these challenges, we
propose a \emph{heterogeneous} \textbf{Mixture-of-Adapters (MoA)} approach.
This method dynamically integrates PEFT adapter experts with diverse
structures, leveraging their complementary representational capabilities to
foster expert specialization, thereby enhancing the effective transfer of
pre-trained knowledge to downstream tasks. MoA supports two variants:
\textbf{(i)} \textit{Soft MoA} achieves fine-grained integration by performing
a weighted fusion of all expert outputs; \textbf{(ii)} \textit{Sparse MoA}
activates adapter experts sparsely based on their contribution, achieving this
with negligible performance degradation. Experimental results demonstrate that
heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance
and parameter efficiency. Our project is available at
https://github.com/DCDmllm/MoA.

</details>


### [47] [DynamicMind: A Tri-Mode Thinking System for Large Language Models](https://arxiv.org/pdf/2506.05936)
*Wei Li, Yanbin Wei, Qiushi Huang, Jiangyue Yan, Yang Chen, James T. Kwok, Yu Zhang*

Main category: cs.CL

TL;DR: DynamicMind introduces a tri-mode thinking system (Fast, Normal, Slow) for LLMs to adapt reasoning depth dynamically, improving zero-shot QA performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' inability to dynamically adjust reasoning depth for varying task complexities, leading to inefficiency or suboptimal performance.

Method: Proposes a tri-mode thinking system, Thinking Density metric, and Mind Router for optimal mode selection in zero-shot QA tasks.

Result: Superior performance in diverse QA benchmarks with balanced computational efficiency.

Conclusion: DynamicMind effectively enhances LLM adaptability and efficiency in zero-shot QA tasks.

Abstract: Modern large language models (LLMs) often struggle to dynamically adapt their
reasoning depth to varying task complexities, leading to suboptimal performance
or inefficient resource utilization. To address this, we introduce DynamicMind,
a novel tri-mode thinking system. DynamicMind empowers LLMs to autonomously
select between Fast, Normal, and Slow thinking modes for zero-shot question
answering (ZSQA) tasks through cognitive-inspired prompt engineering. Our
framework's core innovations include: (1) expanding the established
dual-process framework of fast and slow thinking into a tri-mode thinking
system involving a normal thinking mode to preserve the intrinsic capabilities
of LLM; (2) proposing the Thinking Density metric, which aligns computational
resource allocation with problem complexity; and (3) developing the Thinking
Mode Capacity (TMC) dataset and a lightweight Mind Router to predict the
optimal thinking mode. Extensive experiments across diverse mathematical,
commonsense, and scientific QA benchmarks demonstrate that DynamicMind achieves
superior ZSQA capabilities while establishing an effective trade-off between
performance and computational efficiency.

</details>


### [48] [IntentionESC: An Intention-Centered Framework for Enhancing Emotional Support in Dialogue Systems](https://arxiv.org/pdf/2506.05947)
*Xinjie Zhang, Wenxuan Wang, Qin Jin*

Main category: cs.CL

TL;DR: The paper introduces IntentionESC and ICECoT to improve emotional support by clarifying supporter intentions and enhancing LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: Unclear intentions in emotional support conversations lead to inappropriate strategies, necessitating a framework to guide supporters.

Method: Proposes IntentionESC for defining intentions and ICECoT for LLM reasoning, with automated annotation and evaluation.

Result: Validates the framework through experiments, showing improved emotional support responses.

Conclusion: The IntentionESC and ICECoT framework enhances emotional support by aligning intentions with strategies.

Abstract: In emotional support conversations, unclear intentions can lead supporters to
employ inappropriate strategies, inadvertently imposing their expectations or
solutions on the seeker. Clearly defined intentions are essential for guiding
both the supporter's motivations and the overall emotional support process. In
this paper, we propose the Intention-centered Emotional Support Conversation
(IntentionESC) framework, which defines the possible intentions of supporters
in emotional support conversations, identifies key emotional state aspects for
inferring these intentions, and maps them to appropriate support strategies.
While Large Language Models (LLMs) excel in text generating, they fundamentally
operate as probabilistic models trained on extensive datasets, lacking a true
understanding of human thought processes and intentions. To address this
limitation, we introduce the Intention Centric Chain-of-Thought (ICECoT)
mechanism. ICECoT enables LLMs to mimic human reasoning by analyzing emotional
states, inferring intentions, and selecting suitable support strategies,
thereby generating more effective emotional support responses. To train the
model with ICECoT and integrate expert knowledge, we design an automated
annotation pipeline that produces high-quality training data. Furthermore, we
develop a comprehensive evaluation scheme to assess emotional support efficacy
and conduct extensive experiments to validate our framework. Our data and code
are available at https://github.com/43zxj/IntentionESC_ICECoT.

</details>


### [49] [NameTag 3: A Tool and a Service for Multilingual/Multitagset NER](https://arxiv.org/pdf/2506.05949)
*Jana Straková, Milan Straka*

Main category: cs.CL

TL;DR: NameTag 3 is an open-source, multilingual NER tool with state-of-the-art performance, supporting flat and nested entities across 15 languages.


<details>
  <summary>Details</summary>
Motivation: To provide a versatile, high-performance NER tool that supports multiple languages, datasets, and tagsets, while being accessible via command-line and cloud services.

Method: Uses fine-tuned models (355M and 126M parameters) for flat and nested NER, trained on 21 corpora and three NE tagsets.

Result: Achieves state-of-the-art results on 21 test datasets in 15 languages and remains competitive elsewhere.

Conclusion: NameTag 3 is a powerful, accessible NER tool with broad language support and open-source availability, though models are non-commercial.

Abstract: We introduce NameTag 3, an open-source tool and cloud-based web service for
multilingual, multidataset, and multitagset named entity recognition (NER),
supporting both flat and nested entities. NameTag 3 achieves state-of-the-art
results on 21 test datasets in 15 languages and remains competitive on the
rest, even against larger models. It is available as a command-line tool and as
a cloud-based service, enabling use without local installation. NameTag 3 web
service currently provides flat NER for 17 languages, trained on 21 corpora and
three NE tagsets, all powered by a single 355M-parameter fine-tuned model; and
nested NER for Czech, powered by a 126M fine-tuned model. The source code is
licensed under open-source MPL 2.0, while the models are distributed under
non-commercial CC BY-NC-SA 4.0. Documentation is available at
https://ufal.mff.cuni.cz/nametag, source code at
https://github.com/ufal/nametag3, and trained models via https://lindat.cz. The
REST service and the web application can be found at
https://lindat.mff.cuni.cz/services/nametag/. A demonstration video is
available at https://www.youtube.com/watch?v=-gaGnP0IV8A.

</details>


### [50] [Elementary Math Word Problem Generation using Large Language Models](https://arxiv.org/pdf/2506.05950)
*Nimesh Ariyarathne, Harshani Bandara, Yasith Heshan, Omega Gamage, Surangika Ranathunga, Dilan Nayanajith, Yutharsan Sivapalan, Gayathri Lihinikaduarachchi, Tharoosha Vihidun, Meenambika Chandirakumar, Sanujen Premakumar, Sanjula Gathsara*

Main category: cs.CL

TL;DR: A system using Large Language Models (LLMs) generates Math Word Problems (MWPs) without requiring additional inputs like equations or partial problems, improving efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: To address the time-consuming nature of manually creating MWPs and the limitations of existing Deep Learning techniques that require extra inputs.

Method: The system leverages LLMs, experimenting with various models, prompting strategies, diversity techniques, and human feedback to enhance performance.

Result: Generated MWPs are high-quality with minimal errors, though LLMs struggle with grade and question type adherence.

Conclusion: The LLM-based system effectively generates MWPs but needs improvement in aligning with specific grade and question type requirements.

Abstract: Mathematics is often perceived as a complex subject by students, leading to
high failure rates in exams. To improve Mathematics skills, it is important to
provide sample questions for students to practice problem-solving. Manually
creating Math Word Problems (MWPs) is time consuming for tutors, because they
have to type in natural language while adhering to grammar and spelling rules
of the language. Existing Deep Learning techniques for MWP generation either
require a tutor to provide the initial portion of the MWP, and/or additional
information such as an equation. In this paper, we present an MWP generation
system based on Large Language Models (LLMs) that overcome the need for
additional input - the only input to our system is the number of MWPs needed,
the grade and the type of question (e.g. addition, subtraction). Unlike the
existing LLM-based solutions for MWP generation, we carried out an extensive
set of experiments involving different LLMs, prompting strategies, techniques
to improve the diversity of questions, as well as techniques that employ human
feedback to improve LLM performance. Human and automated evaluations confirmed
that the generated MWPs are high in quality, with minimal spelling and grammar
issues. However, LLMs still struggle to generate questions that adhere to the
specified grade and question type requirements.

</details>


### [51] [Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefixing Improves Theory of Mind in Large Language Models](https://arxiv.org/pdf/2506.05970)
*Kazutoshi Shinoda, Nobukatsu Hojo, Kyosuke Nishida, Yoshihiro Yamazaki, Keita Suzuki, Hiroaki Sugiyama, Kuniko Saito*

Main category: cs.CL

TL;DR: A new inference-time method, Shoes-of-Others (SoO) prefixing, improves Theory of Mind (ToM) in LLMs by specifying outputs with "Let's put ourselves in A's shoes," enhancing performance across diverse contexts.


<details>
  <summary>Details</summary>
Motivation: Existing ToM methods for LLMs are limited to specific contexts with world-state changes, lacking broader applicability.

Method: Proposes SoO prefixing, a simple prompt modification to enhance ToM without fine-tuning, applicable to conversational and narrative contexts.

Result: SoO prefixing consistently improves ToM performance across five mental state categories in benchmarks without world-state changes.

Conclusion: SoO prefixing effectively enhances ToM in LLMs by eliciting faithful thoughts, offering a versatile and assumption-light approach.

Abstract: Recent studies have shown that Theory of Mind (ToM) in large language models
(LLMs) has not reached human-level performance yet. Since fine-tuning LLMs on
ToM datasets often degrades their generalization, several inference-time
methods have been proposed to enhance ToM in LLMs. However, existing
inference-time methods for ToM are specialized for inferring beliefs from
contexts involving changes in the world state. In this study, we present a new
inference-time method for ToM, Shoes-of-Others (SoO) prefixing, which makes
fewer assumptions about contexts and is applicable to broader scenarios. SoO
prefixing simply specifies the beginning of LLM outputs with ``Let's put
ourselves in A's shoes.'', where A denotes the target character's name. We
evaluate SoO prefixing on two benchmarks that assess ToM in conversational and
narrative contexts without changes in the world state and find that it
consistently improves ToM across five categories of mental states. Our analysis
suggests that SoO prefixing elicits faithful thoughts, thereby improving the
ToM performance.

</details>


### [52] [LTG at SemEval-2025 Task 10: Optimizing Context for Classification of Narrative Roles](https://arxiv.org/pdf/2506.05976)
*Egil Rønningstad, Gaurav Negi*

Main category: cs.CL

TL;DR: A simple entity-oriented heuristic for context selection enables effective text classification with limited context window models, outperforming larger generative models.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of providing relevant document segments as context for classification with masked language models.

Method: Use entity-oriented heuristics for context selection and XLM-RoBERTa for classification.

Result: The approach matches or outperforms supervised fine-tuning with larger generative models.

Conclusion: Entity-oriented context selection is effective for classification with limited context models.

Abstract: Our contribution to the SemEval 2025 shared task 10, subtask 1 on entity
framing, tackles the challenge of providing the necessary segments from longer
documents as context for classification with a masked language model. We show
that a simple entity-oriented heuristics for context selection can enable text
classification using models with limited context window. Our context selection
approach and the XLM-RoBERTa language model is on par with, or outperforms,
Supervised Fine-Tuning with larger generative language models.

</details>


### [53] [Tau-Eval: A Unified Evaluation Framework for Useful and Private Text Anonymization](https://arxiv.org/pdf/2506.05979)
*Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi*

Main category: cs.CL

TL;DR: Tau-Eval is an open-source framework for benchmarking text anonymization methods, balancing privacy and utility.


<details>
  <summary>Details</summary>
Motivation: The challenge of evaluating text anonymization methods due to the trade-off between privacy protection and information preservation.

Method: Introduces Tau-Eval, a Python-based framework for benchmarking anonymization techniques using privacy and utility task sensitivity.

Result: A publicly available Python library, code, documentation, and tutorials for Tau-Eval.

Conclusion: Tau-Eval provides a standardized way to assess anonymization methods, addressing the lack of universal benchmarks.

Abstract: Text anonymization is the process of removing or obfuscating information from
textual data to protect the privacy of individuals. This process inherently
involves a complex trade-off between privacy protection and information
preservation, where stringent anonymization methods can significantly impact
the text's utility for downstream applications. Evaluating the effectiveness of
text anonymization proves challenging from both privacy and utility
perspectives, as there is no universal benchmark that can comprehensively
assess anonymization techniques across diverse, and sometimes contradictory
contexts. We present Tau-Eval, an open-source framework for benchmarking text
anonymization methods through the lens of privacy and utility task sensitivity.
A Python library, code, documentation and tutorials are publicly available.

</details>


### [54] [A Culturally-Rich Romanian NLP Dataset from "Who Wants to Be a Millionaire?" Videos](https://arxiv.org/pdf/2506.05991)
*Alexandru-Gabriel Ganea, Antonia-Adelina Popovici, Adrian-Marius Dumitran*

Main category: cs.CL

TL;DR: LLMs perform better on international questions than culturally specific ones, as shown by a Romanian game show dataset.


<details>
  <summary>Details</summary>
Motivation: To explore how cultural context affects LLM performance across languages.

Method: Used OCR, text extraction, and manual verification to create a multilingual dataset from a Romanian game show, then benchmarked LLMs.

Result: LLMs scored 80-95% on international questions vs. 50-75% on Romanian-specific ones.

Conclusion: Cultural context significantly impacts LLM performance, highlighting the need for culturally-aware NLP systems.

Abstract: Large Language Models (LLMs) demonstrate varying performance across languages
and cultural contexts. This study introduces a novel, culturally-rich,
multilingual dataset derived from video recordings of the Romanian game show
"Who Wants to Be a Millionaire?" (Vrei s\u{a} fii Milionar?). We employed an
innovative process combining optical character recognition (OCR), automated
text extraction, and manual verification to collect question-answer pairs,
enriching them with metadata including question domain (e.g., biology,
history), cultural relevance (Romanian-specific vs. international), and
difficulty. Benchmarking state-of-the-art LLMs, including Romanian-adapted
models, on this dataset revealed significant performance disparities: models
consistently achieve higher accuracy (80-95%) on international questions
compared to Romanian-specific cultural questions (50-75%). We further
investigate these differences through experiments involving machine translation
of Romanian questions into English and cross-lingual tests using a comparable
dataset in French. Our findings underscore the impact of cultural context and
data source on LLM performance and offer practical insights for building
robust, culturally-aware multilingual NLP systems, especially in educational
domains. The dataset is publicly available at Hugging Face.

</details>


### [55] [Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models](https://arxiv.org/pdf/2506.06008)
*Peijie Liu, Fengli Xu, Yong Li*

Main category: cs.CL

TL;DR: The paper explores the correlation between token probability monotonicity and Chain-of-Thought (CoT) reasoning effectiveness, proposing indicators and Dynamic CoT for efficient deployment.


<details>
  <summary>Details</summary>
Motivation: To understand inconsistent CoT performance gains and improve efficiency by leveraging token probability distributions.

Method: Proposes indicators based on token probability distributions and Dynamic CoT for task-specific reasoning selection.

Result: Indicators achieve 89.2% accuracy; Dynamic CoT reduces token use by 35% while maintaining accuracy.

Conclusion: Provides insights into CoT mechanisms and a framework for efficient deployment.

Abstract: Chain-of-Thought (CoT) technique has proven effective in improving the
performance of large language models (LLMs) on complex reasoning tasks.
However, the performance gains are inconsistent across different tasks, and the
underlying mechanism remains a long-standing research question. In this work,
we make a preliminary observation that the monotonicity of token probability
distributions may be correlated with the gains achieved through CoT reasoning.
Leveraging this insight, we propose two indicators based on the token
probability distribution to assess CoT effectiveness across different tasks. By
combining instance-level indicators with logistic regression model, we
introduce Dynamic CoT, a method that dynamically select between CoT and direct
answer. Furthermore, we extend Dynamic CoT to closed-source models by
transferring decision strategies learned from open-source models. Our
indicators for assessing CoT effectiveness achieve an accuracy of 89.2\%, and
Dynamic CoT reduces token consumption by more than 35\% while maintaining high
accuracy. Overall, our work offers a novel perspective on the underlying
mechanisms of CoT reasoning and provides a framework for its more efficient
deployment.

</details>


### [56] [Unlocking Recursive Thinking of LLMs: Alignment via Refinement](https://arxiv.org/pdf/2506.06009)
*Haoke Zhang, Xiaobo Liang, Cunxiang Wang, Juntao Li, Min Zhang*

Main category: cs.CL

TL;DR: AvR (Alignment via Refinement) enhances LLMs' recursive reasoning via long-form CoT, outperforming traditional methods with a 20% win rate boost on AlpacaEval 2.0.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack recursive reasoning without expert-curated data, limiting their potential.

Method: AvR integrates criticism and improvement actions with differentiable learning to optimize refinement-aware rewards.

Result: AvR improves LLaMA-3-8B-Instruct's win rate by over 20% using just 3k synthetic samples.

Conclusion: AvR unlocks LLMs' recursive reasoning potential, demonstrating significant performance gains.

Abstract: The OpenAI o1-series models have demonstrated that leveraging long-form Chain
of Thought (CoT) can substantially enhance performance. However, the recursive
thinking capabilities of Large Language Models (LLMs) remain limited,
particularly in the absence of expert-curated data for distillation. In this
paper, we propose \textbf{AvR}: \textbf{Alignment via Refinement}, a novel
method aimed at unlocking the potential of LLMs for recursive reasoning through
long-form CoT. AvR introduces a refinement process that integrates criticism
and improvement actions, guided by differentiable learning techniques to
optimize \textbf{refinement-aware rewards}. As a result, the synthesized
multi-round data can be organized as a long refinement thought, further
enabling test-time scaling. Experimental results show that AvR significantly
outperforms conventional preference optimization methods. Notably, with only 3k
synthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct
model by over 20\% in win rate on AlpacaEval 2.0. Our code is available at
Github (https://github.com/Banner-Z/AvR.git).

</details>


### [57] [AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search](https://arxiv.org/pdf/2506.06017)
*Yu Li, Lehui Li, Zhihao Wu, Qingmin Liao, Jianye Hao, Kun Shao, Fengli Xu, Yong Li*

Main category: cs.CL

TL;DR: A framework for improving LLM agent design by combining hierarchical search, predictive modeling, and MCTS, achieving 8.34% better performance.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing agent search methods: underuse of human-designed components, high evaluation costs, and inefficient search.

Method: Hierarchical search space, predictive value model, and hierarchical MCTS for efficient agent design.

Result: 8.34% average performance gain over baselines, faster search progress.

Conclusion: The framework enhances agent design efficiency and performance across diverse tasks.

Abstract: Large language model (LLM) agents have demonstrated strong capabilities
across diverse domains. However, designing high-performing agentic systems
remains challenging. Existing agent search methods suffer from three major
limitations: (1) an emphasis on optimizing agentic workflows while
under-utilizing proven human-designed components such as memory, planning, and
tool use; (2) high evaluation costs, as each newly generated agent must be
fully evaluated on benchmarks; and (3) inefficient search in large search
space. In this work, we introduce a comprehensive framework to address these
challenges. First, We propose a hierarchical search space that jointly models
agentic workflow and composable functional components, enabling richer agentic
system designs. Building on this structured design space, we introduce a
predictive value model that estimates agent performance given agentic system
and task description, allowing for efficient, low-cost evaluation during the
search process. Finally, we present a hierarchical Monte Carlo Tree Search
(MCTS) strategy informed by uncertainty to guide the search. Experiments on
seven benchmarks, covering embodied, math, web, tool, and game, show that our
method achieves an average performance gain of 8.34\% over state-of-the-art
baselines and exhibits faster search progress with steeper improvement
trajectories. Code repo is available at
https://github.com/Ericccc02/AgentSwift.

</details>


### [58] [When to Trust Context: Self-Reflective Debates for Context Reliability](https://arxiv.org/pdf/2506.06020)
*Zeqi Zhou, Fang Wu, Shayan Talaei, Haokai Zhao, Cheng Meixin, Tinson Xu, Amin Saberi, Yejin Choi*

Main category: cs.CL

TL;DR: SR-DCR improves reliability of large language models by combining self-confidence and multi-agent debate to resolve conflicts between parametric knowledge and contextual input.


<details>
  <summary>Details</summary>
Motivation: Address factual inconsistencies or hallucinations in large language models when contextual input conflicts with parametric knowledge.

Method: Proposes SR-DCR, a framework using token-level self-confidence and asymmetric multi-agent debate (critic, defender, judge) to evaluate context reliability.

Result: Outperforms classical debate and confidence-only baselines on ClashEval benchmark, enhancing robustness to misleading context with minimal overhead.

Conclusion: SR-DCR effectively balances accuracy and reliability in language models, offering a lightweight solution for contextual conflicts.

Abstract: Large language models frequently encounter conflicts between their parametric
knowledge and contextual input, often resulting in factual inconsistencies or
hallucinations. We propose Self-Reflective Debate for Contextual Reliability
(SR-DCR), a lightweight framework that integrates token-level self-confidence
with an asymmetric multi-agent debate to adjudicate such conflicts. A critic,
deprived of context, challenges a defender who argues from the given passage; a
judge model evaluates the debate and determines the context's reliability. The
final answer is selected by combining the verdict with model confidence.
Experiments on the ClashEval benchmark demonstrate that SR-DCR consistently
enhances robustness to misleading context while maintaining accuracy on
trustworthy inputs, outperforming both classical debate and confidence-only
baselines with minimal computational overhead. The code is available at
https://github.com/smiles724/Self-Reflective-Debates.

</details>


### [59] [Large Language Models are Demonstration Pre-Selectors for Themselves](https://arxiv.org/pdf/2506.06033)
*Jiarui Jin, Yuwei Wu, Haoxuan Li, Xiaoting He, Weinan Zhang, Yiming Yang, Yong Yu, Jun Wang, Mengyue Yang*

Main category: cs.CL

TL;DR: FEEDER is a pre-selection framework for in-context learning (ICL) that reduces computational costs by identifying a representative subset of demonstrations, maintaining performance while improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing ICL methods are computationally expensive due to repeated retrieval from large datasets. FEEDER aims to address this inefficiency.

Method: FEEDER uses "sufficiency" and "necessity" metrics and a tree-based algorithm to pre-select a representative subset of demonstrations tailored to specific LLMs.

Result: FEEDER reduces training data size by over 20% while maintaining performance and works with various downstream selection strategies.

Conclusion: FEEDER efficiently improves ICL and fine-tuning of LLMs by pre-selecting essential demonstrations, balancing performance and computational cost.

Abstract: In-context learning (ICL) with large language models (LLMs) delivers strong
few-shot performance by choosing few-shot demonstrations from the entire
training data. However, existing ICL methods, which rely on similarity or
diversity scores to choose demonstrations, incur high computational costs due
to repeatedly retrieval from large-scale datasets for each query. To this end,
we propose FEEDER (FEw yet Essential Demonstration prE-selectoR), a novel
pre-selection framework that identifies a representative subset of
demonstrations containing the most representative examples in the training
data, tailored to specific LLMs. To construct this subset, we introduce the
"sufficiency" and "necessity" metrics in the pre-selection stage and design a
tree-based algorithm to identify representative examples efficiently. Once
pre-selected, this representative subset can effectively replace the full
training data, improving efficiency while maintaining comparable performance in
ICL. Additionally, our pre-selected subset also benefits fine-tuning LLMs,
where we introduce a bi-level optimization method that enhances training
efficiency without sacrificing performance. Experiments with LLMs ranging from
300M to 8B parameters show that FEEDER can reduce training data size by over
20% while maintaining performance and seamlessly integrating with various
downstream demonstration selection strategies in ICL.

</details>


### [60] [MATP-BENCH: Can MLLM Be a Good Automated Theorem Prover for Multimodal Problems?](https://arxiv.org/pdf/2506.06034)
*Zhitao He, Zongwei Lyu, Dazhong Chen, Dadi Guo, Yi R. Fung*

Main category: cs.CL

TL;DR: The paper introduces MATP-BENCH, a benchmark for evaluating Multimodal Large Language Models (MLLMs) as automated theorem provers, highlighting their limitations in solving multimodal mathematical problems.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of MLLMs as Automated Theorem Provers (ATPs) in multimodal settings, given their success in other mathematical tasks.

Method: Developed MATP-BENCH, a benchmark with 1056 multimodal theorems from various educational levels, formalized in Lean 4, Coq, and Isabelle.

Result: Existing MLLMs solve only a limited number of MATP-BENCH problems, indicating a significant challenge.

Conclusion: MATP-BENCH presents an open challenge for advancing MLLMs in multimodal automated theorem proving.

Abstract: Numerous theorems, such as those in geometry, are often presented in
multimodal forms (e.g., diagrams). Humans benefit from visual reasoning in such
settings, using diagrams to gain intuition and guide the proof process. Modern
Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in solving a wide range of mathematical problems. However, the
potential of MLLMs as Automated Theorem Provers (ATPs), specifically in the
multimodal domain, remains underexplored. In this paper, we introduce the
Multimodal Automated Theorem Proving benchmark (MATP-BENCH), a new Multimodal,
Multi-level, and Multi-language benchmark designed to evaluate MLLMs in this
role as multimodal automated theorem provers. MATP-BENCH consists of 1056
multimodal theorems drawn from high school, university, and competition-level
mathematics. All these multimodal problems are accompanied by formalizations in
Lean 4, Coq and Isabelle, thus making the benchmark compatible with a wide
range of theorem-proving frameworks. MATP-BENCH requires models to integrate
sophisticated visual understanding with mastery of a broad spectrum of
mathematical knowledge and rigorous symbolic reasoning to generate formal
proofs. We use MATP-BENCH to evaluate a variety of advanced multimodal language
models. Existing methods can only solve a limited number of the MATP-BENCH
problems, indicating that this benchmark poses an open challenge for research
on automated theorem proving.

</details>


### [61] [Hey, That's My Data! Label-Only Dataset Inference in Large Language Models](https://arxiv.org/pdf/2506.06057)
*Chen Xiong, Zihao Wang, Rui Zhu, Tsung-Yi Ho, Pin-Yu Chen, Jingwei Xiong, Haixu Tang, Lucila Ohno-Machado*

Main category: cs.CL

TL;DR: CatShift is a label-only dataset-inference framework that detects unauthorized training data in LLMs by leveraging catastrophic forgetting, without needing internal model logits.


<details>
  <summary>Details</summary>
Motivation: The reliance of LLMs on large, often proprietary datasets raises copyright concerns, and existing methods fail when log probabilities are withheld or obfuscated.

Method: CatShift uses catastrophic forgetting—fine-tuning on suspicious data triggers output shifts, which are compared to shifts from a non-member validation set to infer dataset membership.

Result: Experiments show CatShift effectively identifies unauthorized training data in logit-inaccessible settings.

Conclusion: CatShift provides a practical solution for protecting proprietary data in LLMs without relying on internal model signals.

Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing
by excelling at interpreting, reasoning about, and generating human language.
However, their reliance on large-scale, often proprietary datasets poses a
critical challenge: unauthorized usage of such data can lead to copyright
infringement and significant financial harm. Existing dataset-inference methods
typically depend on log probabilities to detect suspicious training material,
yet many leading LLMs have begun withholding or obfuscating these signals. This
reality underscores the pressing need for label-only approaches capable of
identifying dataset membership without relying on internal model logits.
  We address this gap by introducing CatShift, a label-only dataset-inference
framework that capitalizes on catastrophic forgetting: the tendency of an LLM
to overwrite previously learned knowledge when exposed to new data. If a
suspicious dataset was previously seen by the model, fine-tuning on a portion
of it triggers a pronounced post-tuning shift in the model's outputs;
conversely, truly novel data elicits more modest changes. By comparing the
model's output shifts for a suspicious dataset against those for a known
non-member validation set, we statistically determine whether the suspicious
set is likely to have been part of the model's original training corpus.
Extensive experiments on both open-source and API-based LLMs validate
CatShift's effectiveness in logit-inaccessible settings, offering a robust and
practical solution for safeguarding proprietary data.

</details>


### [62] [Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2506.06060)
*Yingqi Hu, Zhuo Zhang, Jingyuan Zhang, Lizhen Qu, Zenglin Xu*

Main category: cs.CL

TL;DR: The paper introduces extraction attack algorithms for FedLLMs, operating under a realistic threat model, and evaluates their effectiveness in extracting PII, highlighting vulnerabilities and the need for defenses.


<details>
  <summary>Details</summary>
Motivation: To investigate the risk of training data extraction attacks in FedLLMs, given their memorization ability and data privacy concerns.

Method: Proposes simple yet effective extraction attack algorithms, leveraging contextual prefixes, and evaluates them using coverage rate and efficiency metrics on a real-world legal dataset.

Result: The method extracts up to 56.57% of victim-exclusive PII, with 'Address,' 'Birthday,' and 'Name' being most vulnerable.

Conclusion: The findings emphasize the need for robust defense strategies and provide a benchmark for future privacy-preserving federated learning research.

Abstract: Federated fine-tuning of large language models (FedLLMs) presents a promising
approach for achieving strong model performance while preserving data privacy
in sensitive domains. However, the inherent memorization ability of LLMs makes
them vulnerable to training data extraction attacks. To investigate this risk,
we introduce simple yet effective extraction attack algorithms specifically
designed for FedLLMs. In contrast to prior "verbatim" extraction attacks, which
assume access to fragments from all training data, our approach operates under
a more realistic threat model, where the attacker only has access to a single
client's data and aims to extract previously unseen personally identifiable
information (PII) from other clients. This requires leveraging contextual
prefixes held by the attacker to generalize across clients. To evaluate the
effectiveness of our approaches, we propose two rigorous metrics-coverage rate
and efficiency-and extend a real-world legal dataset with PII annotations
aligned with CPIS, GDPR, and CCPA standards, achieving 89.9% human-verified
precision. Experimental results show that our method can extract up to 56.57%
of victim-exclusive PII, with "Address," "Birthday," and "Name" being the most
vulnerable categories. Our findings underscore the pressing need for robust
defense strategies and contribute a new benchmark and evaluation framework for
future research in privacy-preserving federated learning.

</details>


### [63] [Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning](https://arxiv.org/pdf/2506.06069)
*Maor Ashkenazi, Ofir Brenner, Tal Furman Shohet, Eran Treister*

Main category: cs.CL

TL;DR: The paper proposes a zero-shot method (ATC) for detecting LLM-generated code by analyzing token-level entropy under approximated task conditioning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting LLM-generated code impacts security, intellectual property, and academic integrity, necessitating effective detection methods.

Method: The approach approximates the original task used to generate code and evaluates token-level entropy under this approximated task conditioning (ATC).

Result: ATC outperforms existing methods across benchmarks and generalizes across programming languages (Python, CPP, Java).

Conclusion: Task-level conditioning is crucial for detecting LLM-generated code, and ATC offers a practical, zero-shot solution without requiring access to the generator LLM or original prompts.

Abstract: Detecting Large Language Model (LLM)-generated code is a growing challenge
with implications for security, intellectual property, and academic integrity.
We investigate the role of conditional probability distributions in improving
zero-shot LLM-generated code detection, when considering both the code and the
corresponding task prompt that generated it. Our key insight is that when
evaluating the probability distribution of code tokens using an LLM, there is
little difference between LLM-generated and human-written code. However,
conditioning on the task reveals notable differences. This contrasts with
natural language text, where differences exist even in the unconditional
distributions. Leveraging this, we propose a novel zero-shot detection approach
that approximates the original task used to generate a given code snippet and
then evaluates token-level entropy under the approximated task conditioning
(ATC). We further provide a mathematical intuition, contextualizing our method
relative to previous approaches. ATC requires neither access to the generator
LLM nor the original task prompts, making it practical for real-world
applications. To the best of our knowledge, it achieves state-of-the-art
results across benchmarks and generalizes across programming languages,
including Python, CPP, and Java. Our findings highlight the importance of
task-level conditioning for LLM-generated code detection. The supplementary
materials and code are available at https://github.com/maorash/ATC, including
the dataset gathering implementation, to foster further research in this area.

</details>


### [64] [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/pdf/2506.06091)
*Qinyue Zheng, Salman Abdullah, Sam Rawal, Cyril Zakka, Sophie Ostmeier, Maximilian Purk, Eduardo Reis, Eric J. Topol, Jure Leskovec, Michael Moor*

Main category: cs.CL

TL;DR: MIRIAD introduces a curated medical QA corpus to improve LLM accuracy and reduce hallucinations in healthcare applications.


<details>
  <summary>Details</summary>
Motivation: LLMs generate inaccurate medical content, and existing RAG pipelines use noisy, unstructured text. MIRIAD aims to provide organized, high-quality medical knowledge.

Method: MIRIAD creates 5.8M QA pairs from peer-reviewed literature using a semi-automated pipeline with LLM generation, filtering, grounding, and human annotation.

Result: MIRIAD improves LLM accuracy by 6.7% and hallucination detection by 22.5-37% (F1 score) compared to unstructured RAG baselines.

Conclusion: MIRIAD enables reliable LLM applications in healthcare, including enhanced RAG, medical retrievers, and knowledge-grounded chat interfaces.

Abstract: LLMs are bound to transform healthcare with advanced decision support and
flexible chat assistants. However, LLMs are prone to generate inaccurate
medical content. To ground LLMs in high-quality medical knowledge, LLMs have
been equipped with external knowledge via RAG, where unstructured medical
knowledge is split into small text chunks that can be selectively retrieved and
integrated into the LLMs context. Yet, existing RAG pipelines rely on raw,
unstructured medical text, which can be noisy, uncurated and difficult for LLMs
to effectively leverage. Systematic approaches to organize medical knowledge to
best surface it to LLMs are generally lacking. To address these challenges, we
introduce MIRIAD, a large-scale, curated corpus of 5,821,948 medical QA pairs,
each rephrased from and grounded in a passage from peer-reviewed medical
literature using a semi-automated pipeline combining LLM generation, filtering,
grounding, and human annotation. Unlike prior medical corpora, which rely on
unstructured text, MIRIAD encapsulates web-scale medical knowledge in an
operationalized query-response format, which enables more targeted retrieval.
Experiments on challenging medical QA benchmarks show that augmenting LLMs with
MIRIAD improves accuracy up to 6.7% compared to unstructured RAG baselines with
the same source corpus and with the same amount of retrieved text. Moreover,
MIRIAD improved the ability of LLMs to detect medical hallucinations by 22.5 to
37% (increase in F1 score). We further introduce MIRIAD-Atlas, an interactive
map of MIRIAD spanning 56 medical disciplines, enabling clinical users to
visually explore, search, and refine medical knowledge. MIRIAD promises to
unlock a wealth of down-stream applications, including medical information
retrievers, enhanced RAG applications, and knowledge-grounded chat interfaces,
which ultimately enables more reliable LLM applications in healthcare.

</details>


### [65] [Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning](https://arxiv.org/pdf/2506.06093)
*Atharv Kulkarni, Vivek Srikumar*

Main category: cs.CL

TL;DR: The paper explores using reinforcement learning (RL) with execution-based feedback to improve SQL query generation from natural language, achieving significant accuracy gains without supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To investigate if a large language model (LLM) can be tuned for SQL generation by interacting with a database engine, avoiding reliance on text-code pairs.

Method: Frames the problem as RL, using execution-based feedback (scalar rewards) and the GRPO framework. Evaluated on a tabular reasoning benchmark.

Result: RL-tuning improved SQL generation accuracy from 31.49 to 49.83 and reduced errors from 25.43% to 14.71%, nearly matching a larger model's performance.

Conclusion: Execution-based feedback can enhance LLMs' symbolic reasoning, offering a viable alternative to supervised fine-tuning.

Abstract: In this work, we study the problem of code generation with a large language
model (LLM), with a focus on generating SQL queries from natural language
questions. We ask: Instead of using supervised fine tuning with text-code
pairs, can we tune a model by having it interact with a database engine? We
frame this problem as a reinforcement learning problem where the model receives
execution-based feedback from the environment in the form of scalar rewards.
These rewards penalize execution failures and assign positive values when a
query returns a correct answer. We use the rewards within the Group Relative
Policy Optimization (GRPO) framework. We use a tabular reasoning benchmark to
test and evaluate our findings. We find that with only weak supervision in the
form of question-answer pairs, RL-tuning improves the accuracy of model
generated SQL code from 31.49 to 49.83 while reducing error percentage from
25.43% to 14.71%. This improvement allowed the model nearly match the
performance performance to the larger SQLCoder-70B model. Our work demonstrates
the potential of using execution-based feedback to improve symbolic reasoning
capabilities of LLMs.

</details>


### [66] [Bridging the Gap: In-Context Learning for Modeling Human Disagreement](https://arxiv.org/pdf/2506.06113)
*Benedetta Muscato, Yue Li, Gizem Gezici, Zhixue Zhao, Fosca Giannotti*

Main category: cs.CL

TL;DR: LLMs can generate multi-perspective outputs in zero-shot settings but struggle in few-shot setups to fully capture human disagreement in subjective tasks like hate speech detection. Prompt design and demonstration selection significantly impact performance.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can reflect annotator disagreement and multiple perspectives in subjective NLP tasks, addressing the limitations of aggregated labels.

Method: Evaluated four open-source LLMs using in-context learning (ICL) in zero-shot and few-shot settings, testing three label modeling strategies and various demonstration selection methods (textual similarity, annotation disagreement, combined ranking, and example ordering).

Result: Multi-perspective generation works in zero-shot, but few-shot setups often fail to capture full human judgment diversity. Prompt design and demonstration selection matter, while example ordering has minimal impact.

Conclusion: Modeling subjectivity with LLMs is challenging, emphasizing the need for more perspective-aware and socially intelligent models.

Abstract: Large Language Models (LLMs) have shown strong performance on NLP
classification tasks. However, they typically rely on aggregated labels-often
via majority voting-which can obscure the human disagreement inherent in
subjective annotations. This study examines whether LLMs can capture multiple
perspectives and reflect annotator disagreement in subjective tasks such as
hate speech and offensive language detection. We use in-context learning (ICL)
in zero-shot and few-shot settings, evaluating four open-source LLMs across
three label modeling strategies: aggregated hard labels, and disaggregated hard
and soft labels. In few-shot prompting, we assess demonstration selection
methods based on textual similarity (BM25, PLM-based), annotation disagreement
(entropy), a combined ranking, and example ordering strategies (random vs.
curriculum-based). Results show that multi-perspective generation is viable in
zero-shot settings, while few-shot setups often fail to capture the full
spectrum of human judgments. Prompt design and demonstration selection notably
affect performance, though example ordering has limited impact. These findings
highlight the challenges of modeling subjectivity with LLMs and the importance
of building more perspective-aware, socially intelligent models.

</details>


### [67] [Phonetically-Augmented Discriminative Rescoring for Voice Search Error Correction](https://arxiv.org/pdf/2506.06117)
*Christophe Van Gysel, Maggie Wu, Lyan Verwimp, Caglar Tirkaz, Marco Bertola, Zhihong Lei, Youssef Oualil*

Main category: cs.CL

TL;DR: A phonetic correction system improves ASR accuracy for underrepresented movie titles by generating phonetic alternatives and rescoring them.


<details>
  <summary>Details</summary>
Motivation: High-quality ASR training data is costly, and recent or infrequent movie titles may suffer poor recognition due to insufficient representation.

Method: The system uses phonetic search to generate alternatives and a rescorer to combine and select the best output.

Result: Relative word error rate improvement of 4.4-7.6% on movie title benchmarks.

Conclusion: The proposed phonetic correction system effectively enhances ASR performance for underrepresented titles.

Abstract: End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using
paired audio-text samples that are expensive to obtain, since high-quality
ground-truth data requires human annotators. Voice search applications, such as
digital media players, leverage ASR to allow users to search by voice as
opposed to an on-screen keyboard. However, recent or infrequent movie titles
may not be sufficiently represented in the E2E ASR system's training data, and
hence, may suffer poor recognition.
  In this paper, we propose a phonetic correction system that consists of (a) a
phonetic search based on the ASR model's output that generates phonetic
alternatives that may not be considered by the E2E system, and (b) a rescorer
component that combines the ASR model recognition and the phonetic
alternatives, and select a final system output.
  We find that our approach improves word error rate between 4.4 and 7.6%
relative on benchmarks of popular movie titles over a series of competitive
baselines.

</details>


### [68] [Let's CONFER: A Dataset for Evaluating Natural Language Inference Models on CONditional InFERence and Presupposition](https://arxiv.org/pdf/2506.06133)
*Tara Azin, Daniel Dumitrescu, Diana Inkpen, Raj Singh*

Main category: cs.CL

TL;DR: The paper evaluates NLI models' ability to handle pragmatic inferences, especially presupposition in conditionals, using the CONFER dataset. It tests four NLI models and LLMs like GPT-4o, finding they struggle with presuppositional reasoning.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored area of fine-grained pragmatic inferences, particularly presupposition in conditionals, in NLI models.

Method: Introduces CONFER dataset, evaluates four NLI models (including pre-trained ones) and LLMs (GPT-4o, LLaMA, Gemma, DeepSeek-R1) in zero-shot and few-shot settings.

Result: NLI models struggle with presuppositional reasoning in conditionals; fine-tuning on existing datasets doesn't improve performance.

Conclusion: Current NLI models lack robustness in handling presuppositional reasoning, highlighting a gap for future research.

Abstract: Natural Language Inference (NLI) is the task of determining whether a
sentence pair represents entailment, contradiction, or a neutral relationship.
While NLI models perform well on many inference tasks, their ability to handle
fine-grained pragmatic inferences, particularly presupposition in conditionals,
remains underexplored. In this study, we introduce CONFER, a novel dataset
designed to evaluate how NLI models process inference in conditional sentences.
We assess the performance of four NLI models, including two pre-trained models,
to examine their generalization to conditional reasoning. Additionally, we
evaluate Large Language Models (LLMs), including GPT-4o, LLaMA, Gemma, and
DeepSeek-R1, in zero-shot and few-shot prompting settings to analyze their
ability to infer presuppositions with and without prior context. Our findings
indicate that NLI models struggle with presuppositional reasoning in
conditionals, and fine-tuning on existing NLI datasets does not necessarily
improve their performance.

</details>


### [69] [semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings in Interpretable Semantic Spaces](https://arxiv.org/pdf/2506.06169)
*Jwalanthi Ranganathan, Rohan Jha, Kanishka Misra, Kyle Mahowald*

Main category: cs.CL

TL;DR: The paper introduces 'semantic-features,' a library for analyzing contextualized word embeddings, and applies it to study how dative constructions influence semantic interpretations in sentences.


<details>
  <summary>Details</summary>
Motivation: To explore how contextualized word embeddings reflect semantic differences in dative constructions (e.g., animate vs. inanimate interpretations).

Method: Created a dataset of 450 sentence pairs with ambiguous recipients, analyzed embeddings using the 'semantic-features' tool across three masked language models.

Result: The embeddings showed expected sensitivities, confirming the tool's effectiveness in capturing semantic nuances.

Conclusion: The 'semantic-features' tool is promising for studying contextualized embeddings and their semantic interpretations.

Abstract: We introduce semantic-features, an extensible, easy-to-use library based on
Chronis et al. (2023) for studying contextualized word embeddings of LMs by
projecting them into interpretable spaces. We apply this tool in an experiment
where we measure the contextual effect of the choice of dative construction
(prepositional or double object) on the semantic interpretation of utterances
(Bresnan, 2007). Specifically, we test whether "London" in "I sent London the
letter." is more likely to be interpreted as an animate referent (e.g., as the
name of a person) than in "I sent the letter to London." To this end, we devise
a dataset of 450 sentence pairs, one in each dative construction, with
recipients being ambiguous with respect to person-hood vs. place-hood. By
applying semantic-features, we show that the contextualized word embeddings of
three masked language models show the expected sensitivities. This leaves us
optimistic about the usefulness of our tool.

</details>


### [70] [Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach](https://arxiv.org/pdf/2506.06175)
*James Ford, Anthony Rios*

Main category: cs.CL

TL;DR: A lightweight multi-agent pipeline reduces execution errors in translating chart descriptions to code, but issues like hallucinations and accessibility remain.


<details>
  <summary>Details</summary>
Motivation: To address the persistent 15% execution error rate in language models for chart generation, exploring whether it's due to model limitations or single-prompt design.

Method: Proposes a multi-agent pipeline (drafting, execution, repair, judgment) using GPT-4o-mini, tested on Text2Chart31 and ChartX benchmarks.

Result: Reduces execution errors to 4.5%-4.6%, outperforming baselines. However, hallucinations and poor accessibility (33.3%-7.2% compliance) persist.

Conclusion: Future work should prioritize chart aesthetics, semantic fidelity, and accessibility over execution reliability.

Abstract: Large language models can translate natural-language chart descriptions into
runnable code, yet approximately 15\% of the generated scripts still fail to
execute, even after supervised fine-tuning and reinforcement learning. We
investigate whether this persistent error rate stems from model limitations or
from reliance on a single-prompt design. To explore this, we propose a
lightweight multi-agent pipeline that separates drafting, execution, repair,
and judgment, using only an off-the-shelf GPT-4o-mini model. On the
\textsc{Text2Chart31} benchmark, our system reduces execution errors to 4.5\%
within three repair iterations, outperforming the strongest fine-tuned baseline
by nearly 5 percentage points while requiring significantly less compute.
Similar performance is observed on the \textsc{ChartX} benchmark, with an error
rate of 4.6\%, demonstrating strong generalization. Under current benchmarks,
execution success appears largely solved. However, manual review reveals that 6
out of 100 sampled charts contain hallucinations, and an LLM-based
accessibility audit shows that only 33.3\% (\textsc{Text2Chart31}) and 7.2\%
(\textsc{ChartX}) of generated charts satisfy basic colorblindness guidelines.
These findings suggest that future work should shift focus from execution
reliability toward improving chart aesthetics, semantic fidelity, and
accessibility.

</details>


### [71] [Detecting Voice Phishing with Precision: Fine-Tuning Small Language Models](https://arxiv.org/pdf/2506.06180)
*Ju Yong Sim, Seong Hwan Kim*

Main category: cs.CL

TL;DR: Fine-tuning Llama3 with VP evaluation criteria in prompts outperforms CoT technique, achieving GPT-4-level performance in VP detection.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of detecting voice phishing (VP) by leveraging small LMs and expert knowledge in prompts.

Method: Fine-tune Llama3 with VP evaluation criteria and CoT, test on adversarial datasets, and compare performance.

Result: Llama3-8B with VP criteria in prompts performs best among small LMs, matching GPT-4.

Conclusion: Expert knowledge in prompts is more effective than CoT for small LMs in VP detection.

Abstract: We develop a voice phishing (VP) detector by fine-tuning Llama3, a
representative open-source, small language model (LM). In the prompt, we
provide carefully-designed VP evaluation criteria and apply the
Chain-of-Thought (CoT) technique. To evaluate the robustness of LMs and
highlight differences in their performance, we construct an adversarial test
dataset that places the models under challenging conditions. Moreover, to
address the lack of VP transcripts, we create transcripts by referencing
existing or new types of VP techniques. We compare cases where evaluation
criteria are included, the CoT technique is applied, or both are used together.
In the experiment, our results show that the Llama3-8B model, fine-tuned with a
dataset that includes a prompt with VP evaluation criteria, yields the best
performance among small LMs and is comparable to that of a GPT-4-based VP
detector. These findings indicate that incorporating human expert knowledge
into the prompt is more effective than using the CoT technique for small LMs in
VP detection.

</details>


### [72] [Building Models of Neurological Language](https://arxiv.org/pdf/2506.06208)
*Henry Watkins*

Main category: cs.CL

TL;DR: Developed neurology-specific language models using RAG and representational models, with contributions in datasets, tools, and local deployment scripts.


<details>
  <summary>Details</summary>
Motivation: To address the need for domain-specific language models in neurology, adapting to advancements in medical LLMs.

Method: Leveraged retrieval-augmented generation (RAG) and representational models, created neurology datasets, and developed tools for multi-word expression extraction and graph-based analyses.

Result: Produced neurology-specific datasets, tools, and Docker containers for local hosting, with reported performance metrics and graph community results.

Conclusion: Future work includes exploring multimodal models using open-source architectures like phi-4.

Abstract: This report documents the development and evaluation of domain-specific
language models for neurology. Initially focused on building a bespoke model,
the project adapted to rapid advances in open-source and commercial medical
LLMs, shifting toward leveraging retrieval-augmented generation (RAG) and
representational models for secure, local deployment. Key contributions include
the creation of neurology-specific datasets (case reports, QA sets,
textbook-derived data), tools for multi-word expression extraction, and
graph-based analyses of medical terminology. The project also produced scripts
and Docker containers for local hosting. Performance metrics and graph
community results are reported, with future possible work open for multimodal
models using open-source architectures like phi-4.

</details>


### [73] [PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts](https://arxiv.org/pdf/2506.06211)
*Hengzhi Li, Brendon Jiang, Alexander Naehu, Regan Song, Justin Zhang, Megan Tjandrasuwita, Chanakya Ekbote, Steven-Shine Chen, Adithya Balachandran, Wei Dai, Rebecca Chang, Paul Pu Liang*

Main category: cs.CL

TL;DR: PuzzleWorld is a benchmark of 667 puzzlehunt-style problems to test open-ended, multimodal reasoning in models, revealing current limitations and potential improvements through fine-tuning on reasoning traces.


<details>
  <summary>Details</summary>
Motivation: To assess and improve foundation models' performance in open-ended, creative reasoning tasks like puzzlehunts, which mimic real-world problem-solving.

Method: Introduces PuzzleWorld, a benchmark with annotated solutions, reasoning traces, and cognitive skill labels, and evaluates state-of-the-art models.

Result: Current models perform poorly (1-14% accuracy), but fine-tuning on reasoning traces improves stepwise reasoning from 4% to 11%.

Conclusion: PuzzleWorld highlights the need for better models in open-ended reasoning and provides a tool for future research.

Abstract: Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined
problem definitions. In contrast to conventional reasoning benchmarks
consisting of tasks with clear instructions, puzzlehunts require models to
discover the underlying problem structure from multimodal evidence and
iterative reasoning, mirroring real-world domains such as scientific discovery,
exploratory data analysis, or investigative problem-solving. Despite recent
progress in foundation models, their performance on such open-ended settings
remains largely untested. In this paper, we introduce PuzzleWorld, a
large-scale benchmark of 667 puzzlehunt-style problems designed to assess
step-by-step, open-ended, and creative multimodal reasoning. Each puzzle is
annotated with the final solution, detailed reasoning traces, and cognitive
skill labels, enabling holistic benchmarking and fine-grained diagnostic
analysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,
with the best model solving only 14% of puzzles and reaching 40% stepwise
accuracy. To demonstrate the value of our reasoning annotations, we show that
fine-tuning a small model on reasoning traces improves stepwise reasoning from
4% to 11%, while training on final answers alone degrades performance to near
zero. Our error analysis reveals that current models exhibit myopic reasoning,
are bottlenecked by the limitations of language-based inference, and lack
sketching capabilities crucial for visual and spatial reasoning. We release
PuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on
building more general, open-ended, and creative reasoning systems.

</details>


### [74] [Can Theoretical Physics Research Benefit from Language Agents?](https://arxiv.org/pdf/2506.06214)
*Sirui Lu, Zhijing Jin, Terry Jingchen Zhang, Pavel Kos, J. Ignacio Cirac, Bernhard Schölkopf*

Main category: cs.CL

TL;DR: LLMs show promise for accelerating physics research but need integration with domain knowledge and tools to overcome current gaps in intuition and reasoning.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of LLMs in theoretical physics, addressing their current limitations and envisioning future specialized models.

Method: Analyzing LLM capabilities in physics tasks like math reasoning and code generation, identifying gaps, and proposing solutions.

Result: Identified gaps in physical intuition and reasoning, proposing future LLMs for multimodal data and hypothesis generation.

Conclusion: Collaboration between physics and AI is needed to develop physics-specialized LLMs and advance scientific discovery.

Abstract: Large Language Models (LLMs) are rapidly advancing across diverse domains,
yet their application in theoretical physics research is not yet mature. This
position paper argues that LLM agents can potentially help accelerate
theoretical, computational, and applied physics when properly integrated with
domain knowledge and toolbox. We analyze current LLM capabilities for physics
-- from mathematical reasoning to code generation -- identifying critical gaps
in physical intuition, constraint satisfaction, and reliable reasoning. We
envision future physics-specialized LLMs that could handle multimodal data,
propose testable hypotheses, and design experiments. Realizing this vision
requires addressing fundamental challenges: ensuring physical consistency, and
developing robust verification methods. We call for collaborative efforts
between physics and AI communities to help advance scientific discovery in
physics.

</details>


### [75] [Explaining Matters: Leveraging Definitions and Semantic Expansion for Sexism Detection](https://arxiv.org/pdf/2506.06238)
*Sahrish Khan, Arshad Jhumka, Gabriele Pergola*

Main category: cs.CL

TL;DR: The paper addresses challenges in detecting sexism online, proposing two prompt-based data augmentation techniques and an ensemble strategy to improve model performance on the EDOS dataset.


<details>
  <summary>Details</summary>
Motivation: Sexism detection is hindered by data sparsity, class imbalance, and nuanced language, requiring better methods for generalization and reliability.

Method: Proposes Definition-based Data Augmentation (DDA) and Contextual Semantic Expansion (CSE) for data enrichment, and an ensemble strategy for fine-grained classification.

Result: Achieves state-of-the-art performance on the EDOS dataset, with macro F1 improvements of 1.5 points for binary classification and 4.1 points for fine-grained classification.

Conclusion: The proposed methods effectively address key challenges in sexism detection, enhancing model performance and reliability.

Abstract: The detection of sexism in online content remains an open problem, as harmful
language disproportionately affects women and marginalized groups. While
automated systems for sexism detection have been developed, they still face two
key challenges: data sparsity and the nuanced nature of sexist language. Even
in large, well-curated datasets like the Explainable Detection of Online Sexism
(EDOS), severe class imbalance hinders model generalization. Additionally, the
overlapping and ambiguous boundaries of fine-grained categories introduce
substantial annotator disagreement, reflecting the difficulty of interpreting
nuanced expressions of sexism. To address these challenges, we propose two
prompt-based data augmentation techniques: Definition-based Data Augmentation
(DDA), which leverages category-specific definitions to generate
semantically-aligned synthetic examples, and Contextual Semantic Expansion
(CSE), which targets systematic model errors by enriching examples with
task-specific semantic features. To further improve reliability in fine-grained
classification, we introduce an ensemble strategy that resolves prediction ties
by aggregating complementary perspectives from multiple language models. Our
experimental evaluation on the EDOS dataset demonstrates state-of-the-art
performance across all tasks, with notable improvements of macro F1 by 1.5
points for binary classification (Task A) and 4.1 points for fine-grained
classification (Task C).

</details>


### [76] [Bridging External and Parametric Knowledge: Mitigating Hallucination of LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge](https://arxiv.org/pdf/2506.06240)
*Yi Sui, Chaozhuo Li, Chen Zhang, Dawei song, Qiuchi Li*

Main category: cs.CL

TL;DR: DSSP-RAG improves RAG by resolving knowledge conflicts between LLMs and external sources using mixed-attention and unsupervised hallucination detection.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods struggle with conflicts between LLM parametric knowledge and external knowledge, degrading performance.

Method: Proposes DSSP-RAG with mixed-attention for shared-private semantic synergy, hallucination detection, and EQ-based noise reduction.

Result: Outperforms baselines by resolving conflicts and enhancing knowledge complementarity.

Conclusion: DSSP-RAG effectively integrates internal and external knowledge, improving RAG performance and stability.

Abstract: Retrieval-augmented generation (RAG) is a cost-effective approach to mitigate
the hallucination of Large Language Models (LLMs) by incorporating the
retrieved external knowledge into the generation process. However, external
knowledge may conflict with the parametric knowledge of LLMs. Furthermore,
current LLMs lack inherent mechanisms for resolving such knowledge conflicts,
making traditional RAG methods suffer from degraded performance and stability.
Thus, we propose a Dual-Stream Knowledge-Augmented Framework for Shared-Private
Semantic Synergy (DSSP-RAG). Central to the framework is a novel approach that
refines self-attention into a mixed-attention, distinguishing shared and
private semantics for a controlled internal-external knowledge integration. To
effectively facilitate DSSP in RAG, we further introduce an unsupervised
hallucination detection method based on cognitive uncertainty, ensuring the
necessity of introducing knowledge, and an Energy Quotient (EQ) based on
attention difference matrices to reduce noise in the retrieved external
knowledge. Extensive experiments on benchmark datasets show that DSSP-RAG can
effectively resolve conflicts and enhance the complementarity of dual-stream
knowledge, leading to superior performance over strong baselines.

</details>


### [77] [Cartridges: Lightweight and general-purpose long context representations via self-study](https://arxiv.org/pdf/2506.06266)
*Sabri Eyuboglu, Ryan Ehrlich, Simran Arora, Neel Guha, Dylan Zinsley, Emily Liu, Will Tennien, Atri Rudra, James Zou, Azalia Mirhoseini, Christopher Re*

Main category: cs.CL

TL;DR: The paper proposes a method called 'Cartridge' to reduce the cost of serving large language models by training a smaller KV cache offline on a corpus, using a self-study training recipe to match ICL performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for answering queries grounded in large text corpora using large language models are costly due to high memory consumption of the KV cache.

Method: Train a smaller KV cache (Cartridge) offline on each corpus using self-study, a recipe involving synthetic conversations and context-distillation.

Result: Cartridges match ICL performance with 38.6x less memory and 26.4x higher throughput, extending effective context length and enabling composition without retraining.

Conclusion: Self-study-trained Cartridges offer a cost-effective alternative to ICL, improving efficiency and scalability for long-context tasks.

Abstract: Large language models are often used to answer queries grounded in large text
corpora (e.g. codebases, legal documents, or chat histories) by placing the
entire corpus in the context window and leveraging in-context learning (ICL).
Although current models support contexts of 100K-1M tokens, this setup is
costly to serve because the memory consumption of the KV cache scales with
input length. We explore an alternative: training a smaller KV cache offline on
each corpus. At inference time, we load this trained KV cache, which we call a
Cartridge, and decode a response. Critically, the cost of training a Cartridge
can be amortized across all the queries referencing the same corpus. However,
we find that the naive approach of training the Cartridge with next-token
prediction on the corpus is not competitive with ICL. Instead, we propose
self-study, a training recipe in which we generate synthetic conversations
about the corpus and train the Cartridge with a context-distillation objective.
We find that Cartridges trained with self-study replicate the functionality of
ICL, while being significantly cheaper to serve. On challenging long-context
benchmarks, Cartridges trained with self-study match ICL performance while
using 38.6x less memory and enabling 26.4x higher throughput. Self-study also
extends the model's effective context length (e.g. from 128k to 484k tokens on
MTOB) and surprisingly, leads to Cartridges that can be composed at inference
time without retraining.

</details>


### [78] [AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization](https://arxiv.org/pdf/2506.06273)
*Mukur Gupta, Nikhil Reddy Varimalla, Nicholas Deas, Melanie Subbiah, Kathleen McKeown*

Main category: cs.CL

TL;DR: AdvSumm is a training framework to reduce bias in LLM-based text summarization using adversarial perturbations, improving robustness without losing quality.


<details>
  <summary>Details</summary>
Motivation: LLMs inherit biases from pre-training data, leading to unfair outputs in summarization tasks.

Method: AdvSumm introduces a Perturber component for gradient-guided embedding-level perturbations to enhance robustness.

Result: AdvSumm reduces name-nationality and political framing biases better than standard transformers or data augmentation.

Conclusion: AdvSumm effectively mitigates bias in summarization while maintaining performance.

Abstract: Large Language Models (LLMs) have achieved impressive performance in text
summarization and are increasingly deployed in real-world applications.
However, these systems often inherit associative and framing biases from
pre-training data, leading to inappropriate or unfair outputs in downstream
tasks. In this work, we present AdvSumm (Adversarial Summarization), a
domain-agnostic training framework designed to mitigate bias in text
summarization through improved generalization. Inspired by adversarial
robustness, AdvSumm introduces a novel Perturber component that applies
gradient-guided perturbations at the embedding level of Sequence-to-Sequence
models, enhancing the model's robustness to input variations. We empirically
demonstrate that AdvSumm effectively reduces different types of bias in
summarization-specifically, name-nationality bias and political framing
bias-without compromising summarization quality. Compared to standard
transformers and data augmentation techniques like back-translation, AdvSumm
achieves stronger bias mitigation performance across benchmark datasets.

</details>


### [79] [CAT-LLM: Style-enhanced Large Language Models with Text Style Definition for Chinese Article-style Transfer](https://arxiv.org/pdf/2401.05707)
*Zhen Tao, Dinghao Xi, Zhiyu Li, Liumin Tang, Wei Xu*

Main category: cs.CL

TL;DR: The paper introduces CAT-LLM, a framework for Chinese long-text style transfer, featuring a pluggable Text Style Definition module and achieving high accuracy in style transfer and content preservation.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with the complexity of Chinese long texts, limiting their application in online entertainment and social media.

Method: CAT-LLM uses a Text Style Definition module to analyze and model styles at word and sentence levels, supporting dynamic expansion of style trees. It also employs GPT-3.5-Turbo for style transfer.

Result: CAT-LLM achieves 79.36% transfer accuracy and 96.47% content preservation F1 scores, demonstrating state-of-the-art performance.

Conclusion: CAT-LLM advances Chinese style transfer research, offering potential for digital media and automated content creation.

Abstract: Text style transfer plays a vital role in online entertainment and social
media. However, existing models struggle to handle the complexity of Chinese
long texts, such as rhetoric, structure, and culture, which restricts their
broader application. To bridge this gap, we propose a Chinese Article-style
Transfer (CAT-LLM) framework, which addresses the challenges of style transfer
in complex Chinese long texts. At its core, CAT-LLM features a bespoke
pluggable Text Style Definition (TSD) module that integrates machine learning
algorithms to analyze and model article styles at both word and sentence
levels. This module acts as a bridge, enabling LLMs to better understand and
adapt to the complexities of Chinese article styles. Furthermore, it supports
the dynamic expansion of internal style trees, enabling the framework to
seamlessly incorporate new and diverse style definitions, enhancing
adaptability and scalability for future research and applications.
Additionally, to facilitate robust evaluation, we created ten parallel datasets
using a combination of ChatGPT and various Chinese texts, each corresponding to
distinct writing styles, significantly improving the accuracy of the model
evaluation and establishing a novel paradigm for text style transfer research.
Extensive experimental results demonstrate that CAT-LLM, combined with
GPT-3.5-Turbo, achieves state-of-the-art performance, with a transfer accuracy
F1 score of 79.36% and a content preservation F1 score of 96.47% on the
"Fortress Besieged" dataset. These results highlight CAT-LLM's innovative
contributions to style transfer research, including its ability to preserve
content integrity while achieving precise and flexible style transfer across
diverse Chinese text domains. Building on these contributions, CAT-LLM presents
significant potential for advancing Chinese digital media and facilitating
automated content creation.

</details>


### [80] [Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport](https://arxiv.org/pdf/2406.12329)
*Minseok Choi, Daniel Rim, Dohyun Lee, Jaegul Choo*

Main category: cs.CL

TL;DR: The paper introduces Opt-Out, a method for entity-level unlearning in LLMs to remove sensitive user data, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: LLMs like ChatGPT inadvertently leak sensitive data, necessitating techniques to selectively erase information without full retraining.

Method: Proposes Opt-Out, an optimal transport-based method using Wasserstein distance for fine-grained unlearning, and introduces ELUDe dataset for evaluation.

Result: Opt-Out outperforms existing methods, enabling effective removal of entity-level data while preserving model performance.

Conclusion: Opt-Out sets a new standard for secure LLMs, allowing selective data removal without retraining.

Abstract: Instruction-following large language models (LLMs), such as ChatGPT, have
become widely popular among everyday users. However, these models inadvertently
disclose private, sensitive information to their users, underscoring the need
for machine unlearning techniques to remove selective information from the
models. While prior work has focused on forgetting small, random subsets of
training data at the instance-level, we argue that real-world scenarios often
require the removal of an entire user data, which may require a more careful
maneuver. In this study, we explore entity-level unlearning, which aims to
erase all knowledge related to a target entity while preserving the remaining
model capabilities. To address this, we introduce Opt-Out, an optimal
transport-based unlearning method that utilizes the Wasserstein distance from
the model's initial parameters to achieve more effective and fine-grained
unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe)
designed to evaluate entity-level unlearning. Our empirical results demonstrate
that Opt-Out surpasses existing methods, establishing a new standard for secure
and adaptable LLMs that can accommodate user data removal requests without the
need for full retraining.

</details>


### [81] [HIGHT: Hierarchical Graph Tokenization for Molecule-Language Alignment](https://arxiv.org/pdf/2406.14021)
*Yongqiang Chen, Quanming Yao, Juzheng Zhang, James Cheng, Yatao Bian*

Main category: cs.CL

TL;DR: HIGHT improves molecule-language alignment in LLMs by incorporating hierarchical molecular structures, reducing hallucination by 40%.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook hierarchical molecular structures, leading to poor alignment and hallucination.

Method: HIGHT uses a hierarchical graph tokenizer (atom, motif, molecular levels) and an augmented instruction tuning dataset.

Result: HIGHT reduces hallucination by 40% and improves performance in molecule-language tasks.

Conclusion: HIGHT effectively enhances molecular perception in LLMs by leveraging hierarchical structures.

Abstract: Recently, there has been a surge of interest in extending the success of
large language models (LLMs) from texts to molecules. Most existing approaches
adopt a graph neural network to represent a molecule as a series of node tokens
for molecule-language alignment, which, however, have overlooked the inherent
hierarchical structures in molecules. Notably, higher-order molecular
structures contain rich semantics of functional groups, which encode crucial
biochemical functionalities of the molecules. We show that neglecting the
hierarchical information in tokenization will lead to subpar molecule-language
alignment and severe hallucination. To address this limitation, we propose
HIerarchical GrapH Tokenization (HIGHT). HIGHT employs a hierarchical graph
tokenizer that encodes the hierarchy of atom, motif, and molecular levels of
informative tokens to improve the molecular perception of LLMs. HIGHT also
adopts an augmented instruction tuning dataset, enriched with the hierarchical
graph information, to further enhance the molecule-language alignment.
Extensive experiments on 14 real-world benchmarks verify the effectiveness of
HIGHT in reducing hallucination by 40%, and significant improvements in various
molecule-language downstream tasks. The project is available at https:
//higraphllm.github.io/.

</details>


### [82] [Banyan: Improved Representation Learning with Explicit Structure](https://arxiv.org/pdf/2407.17771)
*Mattia Opper, N. Siddharth*

Main category: cs.CL

TL;DR: Banyan is a model that combines hierarchical structure and efficient learning to outperform transformers in low-resource settings with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle in low-resource settings, while structured models lack performance. Banyan aims to bridge this gap.

Method: Uses an entangled hierarchical tree structure and diagonalized message passing.

Result: Outperforms larger transformer models with only 14 non-embedding parameters, excelling in low-resource scenarios.

Conclusion: Banyan offers an efficient, interpretable NLP solution for resource-constrained environments, especially for underrepresented languages.

Abstract: We present Banyan, a model that efficiently learns semantic representations
by leveraging explicit hierarchical structure. While transformers excel at
scale, they struggle in low-resource settings. Conversely recent structured
models have shown promise as efficient learners, but lack performance. Banyan
bridges this gap with two key innovations: an entangled hierarchical tree
structure and diagonalized message passing, enabling it to outperform larger
transformer models with just 14 non-embedding parameters. It excels in
low-resource settings, offering a viable alternative for under-represented
languages and highlighting its potential for efficient, interpretable NLP in
resource-constrained environments.

</details>


### [83] [A semantic embedding space based on large language models for modelling human beliefs](https://arxiv.org/pdf/2408.07237)
*Byunghwee Lee, Rachith Aiyappa, Yong-Yeol Ahn, Haewoon Kwak, Jisun An*

Main category: cs.CL

TL;DR: A method using LLMs and online debate data maps thousands of beliefs into a neural embedding space, revealing interconnectedness and polarization, and predicting new beliefs and cognitive dissonance.


<details>
  <summary>Details</summary>
Motivation: Understanding the nuanced interplay of beliefs and their influence on human actions and social connections.

Method: Leveraging online debate data and fine-tuned LLMs to map beliefs into a neural embedding space.

Result: The belief space captures interconnectedness and polarization, predicts new beliefs, and estimates cognitive dissonance.

Conclusion: LLMs and collective online records can provide insights into human belief formation principles.

Abstract: Beliefs form the foundation of human cognition and decision-making, guiding
our actions and social connections. A model encapsulating beliefs and their
interrelationships is crucial for understanding their influence on our actions.
However, research on belief interplay has often been limited to beliefs related
to specific issues and relied heavily on surveys. We propose a method to study
the nuanced interplay between thousands of beliefs by leveraging an online user
debate data and mapping beliefs onto a neural embedding space constructed using
a fine-tuned large language model (LLM). This belief space captures the
interconnectedness and polarization of diverse beliefs across social issues.
Our findings show that positions within this belief space predict new beliefs
of individuals and estimate cognitive dissonance based on the distance between
existing and new beliefs. This study demonstrates how LLMs, combined with
collective online records of human beliefs, can offer insights into the
fundamental principles that govern human belief formation.

</details>


### [84] [Where is the signal in tokenization space?](https://arxiv.org/pdf/2408.08541)
*Renato Lui Geh, Honghua Zhang, Kareem Ahmed, Benjie Wang, Guy Van den Broeck*

Main category: cs.CL

TL;DR: The paper explores non-canonical tokenizations in LLMs, proving computational hardness for finding the most likely tokenization and computing marginal probabilities. It shows marginal probabilities are often indistinguishable from canonical ones but reveals hidden signal in tokenization space, improving LLM benchmarks.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that canonical token sequences alone determine text probability in LLMs by investigating non-canonical tokenizations.

Method: Theoretical proofs on computational hardness for tokenization tasks and empirical evaluation of marginal probabilities and their impact on LLM performance.

Result: Marginal probabilities are often indistinguishable from canonical ones, but aggregating non-canonical tokenizations improves LLM benchmark performance.

Conclusion: Non-canonical tokenizations contain valuable signal, enhancing LLM performance when leveraged, despite computational challenges.

Abstract: Large Language Models (LLMs) are typically shipped with tokenizers that
deterministically encode text into so-called canonical token sequences, to
which the LLMs assign probability values. One common assumption is that the
probability of a piece of text is the probability of its canonical token
sequence. However, the tokenization of a string is not unique: e.g., the Llama2
tokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same
text. In this paper, we study non-canonical tokenizations. We prove that, given
a string, it is computationally hard to find the most likely tokenization for
an autoregressive LLM, as well as to compute the marginal probability over all
possible tokenizations. We then show how the marginal is, in most cases,
indistinguishable from the canonical probability. Surprisingly, we then
empirically demonstrate the existence of a significant amount of signal hidden
within tokenization space. Notably, by simply aggregating the probabilities of
non-canonical tokenizations, we achieve improvements across a range of LLM
evaluation benchmarks for a variety of architectures, including transformers
and state space models.

</details>


### [85] [Judgment of Learning: A Human Ability Beyond Generative Artificial Intelligence](https://arxiv.org/pdf/2410.13392)
*Markus Huff, Elanur Ulakçı*

Main category: cs.CL

TL;DR: LLMs like ChatGPT struggle with metacognition, specifically in predicting memory performance, unlike humans who accurately predict their own memory performance.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can align with human metacognitive abilities, particularly in predicting memory performance (judgments of learning, JOL).

Method: Tested humans and LLMs (GPT-3.5-turbo, GPT-4-turbo, GPT-4o) on garden-path sentences with fitting/unfitting contexts to compare JOL accuracy.

Result: Human JOL reliably predicted memory performance, but LLMs failed to match this accuracy, regardless of context.

Conclusion: LLMs lack metacognitive abilities in memory prediction, highlighting a need for improvements in self-monitoring for better AI integration in cognitive tasks.

Abstract: Large language models (LLMs) increasingly mimic human cognition in various
language-based tasks. However, their capacity for metacognition - particularly
in predicting memory performance - remains unexplored. Here, we introduce a
cross-agent prediction model to assess whether ChatGPT-based LLMs align with
human judgments of learning (JOL), a metacognitive measure where individuals
predict their own future memory performance. We tested humans and LLMs on pairs
of sentences, one of which was a garden-path sentence - a sentence that
initially misleads the reader toward an incorrect interpretation before
requiring reanalysis. By manipulating contextual fit (fitting vs. unfitting
sentences), we probed how intrinsic cues (i.e., relatedness) affect both LLM
and human JOL. Our results revealed that while human JOL reliably predicted
actual memory performance, none of the tested LLMs (GPT-3.5-turbo, GPT-4-turbo,
and GPT-4o) demonstrated comparable predictive accuracy. This discrepancy
emerged regardless of whether sentences appeared in fitting or unfitting
contexts. These findings indicate that, despite LLMs' demonstrated capacity to
model human cognition at the object-level, they struggle at the meta-level,
failing to capture the variability in individual memory predictions. By
identifying this shortcoming, our study underscores the need for further
refinements in LLMs' self-monitoring abilities, which could enhance their
utility in educational settings, personalized learning, and human-AI
interactions. Strengthening LLMs' metacognitive performance may reduce the
reliance on human oversight, paving the way for more autonomous and seamless
integration of AI into tasks requiring deeper cognitive awareness.

</details>


### [86] [The Impact of Inference Acceleration on Bias of LLMs](https://arxiv.org/pdf/2410.22118)
*Elisabeth Kirsten, Ivan Habernal, Vedant Nanda, Muhammad Bilal Zafar*

Main category: cs.CL

TL;DR: The paper examines how inference acceleration techniques in LLMs affect demographic bias in model outputs, revealing unpredictable and significant bias changes.


<details>
  <summary>Details</summary>
Motivation: While LLM inference acceleration strategies reduce cost and latency, their impact on demographic bias remains unexplored.

Method: The study evaluates bias changes in LLM outputs before and after applying acceleration techniques using diverse metrics.

Result: Acceleration strategies cause significant and unpredictable shifts in bias, varying by model and bias type.

Conclusion: Case-by-case bias evaluation is essential after applying inference acceleration to LLMs.

Abstract: Last few years have seen unprecedented advances in capabilities of Large
Language Models (LLMs). These advancements promise to benefit a vast array of
application domains. However, due to their immense size, performing inference
with LLMs is both costly and slow. Consequently, a plethora of recent work has
proposed strategies to enhance inference efficiency, e.g., quantization,
pruning, and caching. These acceleration strategies reduce the inference cost
and latency, often by several factors, while maintaining much of the predictive
performance measured via common benchmarks. In this work, we explore another
critical aspect of LLM performance: demographic bias in model generations due
to inference acceleration optimizations. Using a wide range of metrics, we
probe bias in model outputs from a number of angles. Analysis of outputs before
and after inference acceleration shows significant change in bias. Worryingly,
these bias effects are complex and unpredictable. A combination of an
acceleration strategy and bias type may show little bias change in one model
but may lead to a large effect in another. Our results highlight a need for
in-depth and case-by-case evaluation of model bias after it has been modified
to accelerate inference.

</details>


### [87] [Unveiling Topological Structures from Language: A Comprehensive Survey of Topological Data Analysis Applications in NLP](https://arxiv.org/pdf/2411.10298)
*Adaku Uchendu, Thai Le*

Main category: cs.CL

TL;DR: The paper surveys 95 papers applying Topological Data Analysis (TDA) in NLP, categorizing them into theoretical and non-theoretical approaches, and discusses challenges in the field.


<details>
  <summary>Details</summary>
Motivation: To address ML challenges like data imbalance and noise by exploring TDA's potential in NLP, despite its limited adoption compared to other domains.

Method: Comprehensive survey of 95 papers, categorized into theoretical (explaining linguistic phenomena topologically) and non-theoretical (combining TDA with ML features) approaches.

Result: Identifies two main approaches to TDA in NLP and highlights ongoing challenges in the field.

Conclusion: TDA shows promise for NLP but faces unresolved questions; resources are provided for further exploration.

Abstract: The surge of data available on the internet has led to the adoption of
various computational methods to analyze and extract valuable insights from
this wealth of information. Among these, the field of Machine Learning (ML) has
thrived by leveraging data to extract meaningful insights. However, ML
techniques face notable challenges when dealing with real-world data, often due
to issues of imbalance, noise, insufficient labeling, and high dimensionality.
To address these limitations, some researchers advocate for the adoption of
Topological Data Analysis (TDA), a statistical approach that discerningly
captures the intrinsic shape of data despite noise. Despite its potential, TDA
has not gained as much traction within the Natural Language Processing (NLP)
domain compared to structurally distinct areas like computer vision.
Nevertheless, a dedicated community of researchers has been exploring the
application of TDA in NLP, yielding 95 papers we comprehensively survey in this
paper. Our findings categorize these efforts into theoretical and
non-theoretical approaches. Theoretical approaches aim to explain linguistic
phenomena from a topological viewpoint, while non-theoretical approaches merge
TDA with ML features, utilizing diverse numerical representation techniques. We
conclude by exploring the challenges and unresolved questions that persist in
this niche field. Resources and a list of papers on this topic can be found at:
https://github.com/AdaUchendu/AwesomeTDA4NLP.

</details>


### [88] [The Synergy of LLMs & RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data](https://arxiv.org/pdf/2412.06877)
*Thomas Pouplin, Katarzyna Kobalczyk, Hao Sun, Mihaela van der Schaar*

Main category: cs.CL

TL;DR: TEDUO is a novel pipeline for offline language-conditioned policy learning in symbolic environments, leveraging LLMs for data augmentation and generalization, outperforming traditional RL and standalone LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of autonomous agents performing complex tasks with scarce labeled data and poor generalization in unseen scenarios.

Method: TEDUO uses LLMs to augment unlabeled datasets and as generalizable instruction-following agents for offline policy learning.

Result: Empirical results show TEDUO achieves data-efficient learning and robust performance on unseen tasks.

Conclusion: TEDUO effectively bridges the gap in generalization and data efficiency for language-conditioned policies.

Abstract: Developing autonomous agents capable of performing complex, multi-step
decision-making tasks specified in natural language remains a significant
challenge, particularly in realistic settings where labeled data is scarce and
real-time experimentation is impractical. Existing reinforcement learning (RL)
approaches often struggle to generalize to unseen goals and states, limiting
their applicability. In this paper, we introduce TEDUO, a novel training
pipeline for offline language-conditioned policy learning in symbolic
environments. Unlike conventional methods, TEDUO operates on readily available,
unlabeled datasets and addresses the challenge of generalization to previously
unseen goals and states. Our approach harnesses large language models (LLMs) in
a dual capacity: first, as automatization tools augmenting offline datasets
with richer annotations, and second, as generalizable instruction-following
agents. Empirical results demonstrate that TEDUO achieves data-efficient
learning of robust language-conditioned policies, accomplishing tasks beyond
the reach of conventional RL frameworks or out-of-the-box LLMs alone.

</details>


### [89] [Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning](https://arxiv.org/pdf/2412.13540)
*Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, Min Zhang*

Main category: cs.CL

TL;DR: VGCure is a benchmark to evaluate LVLMs' graph understanding and reasoning, revealing their weaknesses. A structure-aware fine-tuning framework is proposed to improve their performance.


<details>
  <summary>Details</summary>
Motivation: Recent studies highlight LVLMs' limitations in handling visual graphs, prompting the need to understand and address these shortcomings.

Method: VGCure benchmark with 22 tasks evaluates LVLMs. A structure-aware fine-tuning framework with three self-supervised learning tasks is introduced.

Result: LVLMs perform poorly in basic graph tasks, especially with relational or complex structures. The proposed method improves performance and robustness.

Conclusion: The structure-aware fine-tuning framework effectively enhances LVLMs' graph understanding and reasoning capabilities.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across diverse tasks. Despite great success, recent studies show that LVLMs
encounter substantial limitations when engaging with visual graphs. To study
the reason behind these limitations, we propose VGCure, a comprehensive
benchmark covering 22 tasks for examining the fundamental graph understanding
and reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs
reveal that LVLMs are weak in basic graph understanding and reasoning tasks,
particularly those concerning relational or structurally complex information.
Based on this observation, we propose a structure-aware fine-tuning framework
to enhance LVLMs with structure learning abilities through three
self-supervised learning tasks. Experiments validate the effectiveness of our
method in improving LVLMs' performance on fundamental and downstream graph
learning tasks, as well as enhancing their robustness against complex visual
graphs.

</details>


### [90] [Reasoning Through Execution: Unifying Process and Outcome Rewards for Code Generation](https://arxiv.org/pdf/2412.15118)
*Zhuohao Yu, Weizheng Gu, Yidong Wang, Xingru Jiang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang*

Main category: cs.CL

TL;DR: ORPS combines process and outcome supervision using executable verification to improve LLM code generation, achieving 26.9% higher correctness and 42.2% better efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of traditional process and outcome supervision in complex programming tasks by integrating verifiable outcomes with structured reasoning.

Method: Introduces Outcome Refining Process Supervision (ORPS), a tree-structured search framework with executable verification, self-critique mechanisms, and runtime feedback.

Result: Experiments show 26.9% higher correctness and 42.2% improved code efficiency across 5 models and 3 benchmarks.

Conclusion: ORPS effectively bridges the gap in complex code generation, offering a scalable solution by combining verifiable outcomes with structured reasoning.

Abstract: Large Language Models excel at code generation yet struggle with complex
programming tasks that demand sophisticated reasoning. To bridge this gap,
traditional process supervision relies on learned reward models requiring
costly training data and suffering from reward misalignment, while outcome
supervision fails for complex tasks needing coordinated intermediate steps. We
introduce Outcome Refining Process Supervision, which unifies process and
outcome supervision by leveraging executable verification: a tree-structured
search framework generates strategic alternatives, profiles execution metrics,
and scores candidates via self-critique mechanisms that integrate runtime
feedback with reasoning. Experiments across 5 models and 3 benchmarks show
consistent gains, with 26.9% higher correctness and 42.2% improved code
efficiency. The results demonstrate that ORPS enables LLMs to overcome local
optima in code generation, suggesting a promising direction for combining
verifiable outcomes with structured reasoning to tackle complex challenges. We
open-source at: https://github.com/zhuohaoyu/ORPS

</details>


### [91] [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/pdf/2412.17451)
*Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, Junxian He*

Main category: cs.CL

TL;DR: The paper introduces M-STAR, a framework for self-evolving training in multimodal reasoning, addressing performance saturation and optimizing design principles inspired by RL.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality chain-of-thought data and underexplored effectiveness of self-evolving training in multimodal reasoning motivated this work.

Method: The study reframes self-evolving training through RL, focusing on Training Method, Reward Model, and Prompt Variation, and introduces an automatic balancing mechanism.

Result: M-STAR achieves consistent performance gains across models of varying sizes and benchmarks, mitigating saturation.

Conclusion: The proposed framework and insights significantly enhance multimodal reasoning capabilities, with resources made publicly available.

Abstract: Self-evolving trainin--where models iteratively learn from their own
outputs--has emerged as a key approach for complex reasoning tasks, addressing
the scarcity of high-quality chain-of-thought data. However, its effectiveness
in multimodal reasoning, a domain more intricate than text-only reasoning,
remains underexplored, and the understanding of critical factors in this
training paradigm remains limited. Furthermore, a central challenge for this
training method is performance saturation, which impedes further improvements
and scalability. Inspired by reinforcement learning (RL), in this paper, we
reframe self-evolving training for multimodal reasoning through the lens of RL,
identifying three pivotal factors: Training Method, Reward Model, and Prompt
Variation. Through systematic analysis, we establish relatively optimal design
principles that significantly enhance multimodal reasoning capabilities.
Moreover, delving deeper into training dynamics, we uncover the roots of
saturation and propose a new automatic balancing mechanism to mitigate this
limitation. Building on these insights, we propose M-STAR (Multimodal
Self-evolving Training for Reasoning), a framework that achieves consistent
performance gains across models of varying sizes and diverse benchmarks. All
resources are made publicly available at https://mstar-lmm.github.io.

</details>


### [92] [ResearchTown: Simulator of Human Research Community](https://arxiv.org/pdf/2412.17767)
*Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, Tao Feng, Jiaxuan You*

Main category: cs.CL

TL;DR: ResearchTown is a multi-agent framework simulating human research communities using LLMs, modeling researchers and papers as nodes in a graph and evaluating simulation quality with ResearchBench.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can simulate human research communities, enhancing understanding of idea brainstorming and enabling automatic scientific discovery.

Method: Proposes ResearchTown, a framework representing researchers and papers as nodes in an agent-data graph, with TextGNN modeling research activities as message-passing processes.

Result: ResearchTown realistically simulates collaborative activities, maintains robustness with diverse inputs, and generates interdisciplinary research ideas.

Conclusion: ResearchTown demonstrates potential for simulating research communities and inspiring novel scientific insights.

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in
scientific domains, yet a fundamental question remains unanswered: Can we
simulate human research communities with LLMs? Addressing this question can
deepen our understanding of the processes behind idea brainstorming and inspire
the automatic discovery of novel scientific insights. In this work, we propose
ResearchTown, a multi-agent framework for research community simulation. Within
this framework, the human research community is simplified as an agent-data
graph, where researchers and papers are represented as agent-type and data-type
nodes, respectively, and connected based on their collaboration relationships.
We also introduce TextGNN, a text-based inference framework that models various
research activities (e.g., paper reading, paper writing, and review writing) as
special forms of a unified message-passing process on the agent-data graph. To
evaluate the quality of the research community simulation, we present
ResearchBench, a benchmark that uses a node-masking prediction task for
scalable and objective assessment based on similarity. Our experiments reveal
three key findings: (1) ResearchTown can provide a realistic simulation of
collaborative research activities, including paper writing and review writing;
(2) ResearchTown can maintain robust simulation with multiple researchers and
diverse papers; (3) ResearchTown can generate interdisciplinary research ideas
that potentially inspire pioneering research directions.

</details>


### [93] [MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models](https://arxiv.org/pdf/2501.00316)
*Mahir Labib Dihan, Md Tanvir Hassan, Md Tanvir Parvez, Md Hasebul Hasan, Md Almash Alam, Muhammad Aamir Cheema, Mohammed Eunus Ali, Md Rizwan Parvez*

Main category: cs.CL

TL;DR: MapEval is a benchmark for evaluating foundation models on geospatial reasoning tasks, revealing significant gaps in performance compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored capabilities of foundation models in map-based reasoning and provide a comprehensive evaluation framework.

Method: Introduces MapEval, a benchmark with 700 multiple-choice questions across textual, API-based, and visual reasoning tasks, covering 180 cities and 54 countries.

Result: None of the 30 evaluated models (including top ones like GPT-4o) surpassed 67% accuracy, with open-source models performing worse and all lagging behind humans by over 20%.

Conclusion: The results highlight critical gaps in spatial reasoning, emphasizing the need for improved geospatial AI to enhance real-world navigation capabilities.

Abstract: Recent advancements in foundation models have improved autonomous tool usage
and reasoning, but their capabilities in map-based reasoning remain
underexplored. To address this, we introduce MapEval, a benchmark designed to
assess foundation models across three distinct tasks - textual, API-based, and
visual reasoning - through 700 multiple-choice questions spanning 180 cities
and 54 countries, covering spatial relationships, navigation, travel planning,
and real-world map interactions. Unlike prior benchmarks that focus on simple
location queries, MapEval requires models to handle long-context reasoning, API
interactions, and visual map analysis, making it the most comprehensive
evaluation framework for geospatial AI. On evaluation of 30 foundation models,
including Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro, none surpass 67%
accuracy, with open-source models performing significantly worse and all models
lagging over 20% behind human performance. These results expose critical gaps
in spatial inference, as models struggle with distances, directions, route
planning, and place-specific reasoning, highlighting the need for better
geospatial AI to bridge the gap between foundation models and real-world
navigation. All the resources are available at: https://mapeval.github.io/.

</details>


### [94] [GRASP: Replace Redundant Layers with Adaptive Singular Parameters for Efficient Model Compression](https://arxiv.org/pdf/2501.00339)
*Kainan Liu, Yong Zhang, Ning Cheng, Zhitao Li, Shaojun Wang, Jing Xiao*

Main category: cs.CL

TL;DR: GRASP is a gradient-based compression framework for LLMs that retains critical singular values, outperforming existing methods with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Indiscriminate layer pruning in LLMs causes performance degradation; GRASP aims to preserve critical parameters for efficient compression.

Method: GRASP uses gradient-based attribution on a calibration dataset to identify and retain sensitivity-aware singular values, replacing redundant layers minimally.

Result: GRASP achieves 90% of the original model's performance at a 20% compression ratio, outperforming other methods.

Conclusion: GRASP offers an effective balance between model compression and performance retention in LLMs.

Abstract: Recent studies have demonstrated that many layers are functionally redundant
in large language models (LLMs), enabling model compression by removing these
layers to reduce inference cost. While such approaches can improve efficiency,
indiscriminate layer pruning often results in significant performance
degradation. In this paper, we propose GRASP (Gradient-based Retention of
Adaptive Singular Parameters), a novel compression framework that mitigates
this issue by preserving sensitivity-aware singular values. Unlike direct layer
pruning, GRASP leverages gradient-based attribution on a small calibration
dataset to adaptively identify and retain critical singular components. By
replacing redundant layers with only a minimal set of parameters, GRASP
achieves efficient compression while maintaining strong performance with
minimal overhead. Experiments across multiple LLMs show that GRASP consistently
outperforms existing compression methods, achieving 90% of the original model's
performance under a 20% compression ratio.

</details>


### [95] [Emergent Response Planning in LLMs](https://arxiv.org/pdf/2502.06258)
*Zhichen Dong, Zhanhui Zhou, Zhixuan Liu, Chao Yang, Chaochao Lu*

Main category: cs.CL

TL;DR: LLMs exhibit emergent planning behaviors, encoding future outputs in hidden representations, which can be probed for global response attributes like structure, content, and behavior.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs, despite being trained for next-token prediction, inherently plan ahead in their hidden representations.

Method: Probing LLM prompt representations to identify encoded global attributes of responses (structure, content, behavior) and analyzing scaling and evolution during generation.

Result: LLMs encode future outputs beyond the next token, with planning behaviors scaling with model size and evolving during generation.

Conclusion: The findings suggest LLMs inherently plan ahead, offering potential for improving transparency and control in generation tasks.

Abstract: In this work, we argue that large language models (LLMs), though trained to
predict only the next token, exhibit emergent planning behaviors:
$\textbf{their hidden representations encode future outputs beyond the next
token}$. Through simple probing, we demonstrate that LLM prompt representations
encode global attributes of their entire responses, including
$\textit{structure attributes}$ (e.g., response length, reasoning steps),
$\textit{content attributes}$ (e.g., character choices in storywriting,
multiple-choice answers at the end of response), and $\textit{behavior
attributes}$ (e.g., answer confidence, factual consistency). In addition to
identifying response planning, we explore how it scales with model size across
tasks and how it evolves during generation. The findings that LLMs plan ahead
for the future in their hidden representations suggest potential applications
for improving transparency and generation control.

</details>


### [96] [DPO-Shift: Shifting the Distribution of Direct Preference Optimization](https://arxiv.org/pdf/2502.07599)
*Xiliang Yang, Feng Jiang, Qianen Zhang, Lei Zhao, Xiao Li*

Main category: cs.CL

TL;DR: DPO-Shift is introduced to address likelihood displacement in DPO by controllably shifting chosen response probabilities, balancing improvement in chosen probability with reward margin trade-offs, and outperforming DPO in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To mitigate the likelihood displacement issue in DPO, where chosen response probabilities decrease during training.

Method: Introduces DPO-Shift to shift the distribution of chosen probabilities, supported by theoretical analysis and experiments.

Result: DPO-Shift shows a trade-off between chosen probability improvement and reward margin, and outperforms DPO in tasks like MT-Bench and win rate experiments.

Conclusion: DPO-Shift effectively mitigates DPO's likelihood displacement with a simple, theoretically grounded solution.

Abstract: Direct Preference Optimization (DPO) and its variants have become
increasingly popular for aligning language models with human preferences. These
methods aim to teach models to better distinguish between chosen (or preferred)
and rejected (or dispreferred) responses. However, prior research has
identified that the probability of chosen responses often decreases during
training, and this phenomenon is known as likelihood displacement. To tackle
this challenge, in this work we introduce DPO-Shift to controllably shift the
distribution of the chosen probability. Then, we show that DPO-Shift exhibits a
fundamental trade-off between improving the chosen probability and sacrificing
the reward margin, as supported by both theoretical analysis and experimental
validation. Furthermore, we demonstrate the superiority of DPO-Shift over DPO
on downstream tasks such as MT-Bench and a designed win rate experiment. We
believe this study shows that the likelihood displacement issue of DPO can be
effectively mitigated with a simple, theoretically grounded solution. Our code
is available at https://github.com/Meaquadddd/DPO-Shift.

</details>


### [97] [Lost in the Passage: Passage-level In-context Learning Does Not Necessarily Need a "Passage"](https://arxiv.org/pdf/2502.10634)
*Hao Sun, Chenming Tang, Gengyang Li, Yunfang Wu*

Main category: cs.CL

TL;DR: LLMs struggle to learn intrinsic relationships between demonstration passages and outputs in passage-level long-context ICL, performing better with meaningless shorter passages.


<details>
  <summary>Details</summary>
Motivation: To investigate why LLMs fail to leverage demonstration passages effectively in long-context ICL for generation tasks.

Method: Experiments with various LLMs on single-document QA and distractor generation, analyzing attention and information flow.

Result: Meaningless shorter passages outperform full passages; LLMs ignore passages in prompts.

Conclusion: Current compression methods for long-context tasks are ineffective for passage-level ICL due to LLMs' poor utilization of passages.

Abstract: By simply incorporating demonstrations into the context, in-context learning
(ICL) enables large language models (LLMs) to yield awesome performance on many
tasks. In this study, we focus on passage-level long-context ICL for generation
tasks and find that LLMs cannot learn the intrinsic relationship between the
demonstration passage and the generation output. We conduct experiments with
different LLMs on two typical generation tasks including single-document
question answering and distractor generation, demonstrating that even a
completely meaningless demonstration passage with 1/4 length achieves much
better performance than the original full passage. Analysis via attention and
information flow reveals that LLMs pay little attention to passages compared to
other components in the prompt and little information flows from the passage to
other parts of the demonstration, which further confirms our finding.
Additionally, experiments on context compression indicate that compression
approaches proven effective on other long-context tasks are not suitable for
passage-level ICL, since simply using shorter meaningless demonstration
passages already achieves competitive performance.

</details>


### [98] [Towards Effective Extraction and Evaluation of Factual Claims](https://arxiv.org/pdf/2502.10855)
*Dasha Metropolitansky, Jonathan Larson*

Main category: cs.CL

TL;DR: The paper proposes a framework for evaluating claim extraction methods in fact-checking, introduces Claimify (an LLM-based method), and shows its superiority over existing methods.


<details>
  <summary>Details</summary>
Motivation: The lack of a standardized evaluation framework for claim extraction methods in fact-checking hinders assessment and comparison.

Method: The authors propose an evaluation framework and introduce Claimify, an LLM-based claim extraction method, with novel metrics for coverage and decontextualization.

Result: Claimify outperforms existing methods, particularly in handling ambiguity and extracting high-confidence claims.

Conclusion: The framework and Claimify advance the field by providing scalable, replicable, and automated evaluation tools for claim extraction in fact-checking.

Abstract: A common strategy for fact-checking long-form content generated by Large
Language Models (LLMs) is extracting simple claims that can be verified
independently. Since inaccurate or incomplete claims compromise fact-checking
results, ensuring claim quality is critical. However, the lack of a
standardized evaluation framework impedes assessment and comparison of claim
extraction methods. To address this gap, we propose a framework for evaluating
claim extraction in the context of fact-checking along with automated,
scalable, and replicable methods for applying this framework, including novel
approaches for measuring coverage and decontextualization. We also introduce
Claimify, an LLM-based claim extraction method, and demonstrate that it
outperforms existing methods under our evaluation framework. A key feature of
Claimify is its ability to handle ambiguity and extract claims only when there
is high confidence in the correct interpretation of the source text.

</details>


### [99] [TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking](https://arxiv.org/pdf/2502.11187)
*Shahriar Kabir Nahin, Rabindra Nath Nandi, Sagor Sarker, Quazi Sarwar Muhtaseem, Md Kowsher, Apu Chandraw Shill, Md Ibrahim, Mehadi Hasan Menon, Tareq Al Muntasir, Firoj Alam*

Main category: cs.CL

TL;DR: TituLLMs are the first large pretrained Bangla LLMs (1b and 3b parameters), trained on ~37B tokens. They outperform multilingual versions but highlight language adaptation challenges. Models and datasets are publicly available.


<details>
  <summary>Details</summary>
Motivation: Address the lack of large pretrained Bangla LLMs and benchmarking datasets, enabling research and adoption for low-resource languages.

Method: Collected ~37B tokens for pretraining, extended Llama-3.2 tokenizer for Bangla, and developed five benchmarking datasets.

Result: TituLLMs outperform initial multilingual versions, though language adaptation complexities exist.

Conclusion: The work provides a foundation for adapting multilingual models to low-resource languages, with publicly available models and datasets.

Abstract: In this paper, we present TituLLMs, the first large pretrained Bangla LLMs,
available in 1b and 3b parameter sizes. Due to computational constraints during
both training and inference, we focused on smaller models. To train TituLLMs,
we collected a pretraining dataset of approximately ~37 billion tokens. We
extended the Llama-3.2 tokenizer to incorporate language- and culture-specific
knowledge, which also enables faster training and inference. There was a lack
of benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we
developed five benchmarking datasets. We benchmarked various LLMs, including
TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual
versions. However, this is not always the case, highlighting the complexities
of language adaptation. Our work lays the groundwork for adapting existing
multilingual open models to other low-resource languages. To facilitate broader
adoption and further research, we have made the TituLLMs models and
benchmarking datasets publicly available
(https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).

</details>


### [100] [On the Query Complexity of Verifier-Assisted Language Generation](https://arxiv.org/pdf/2502.12123)
*Edoardo Botta, Yuchen Li, Aashay Mehta, Jordan T. Ash, Cyril Zhang, Andrej Risteski*

Main category: cs.CL

TL;DR: The paper introduces a mathematical framework for constrained generation using a pre-trained language model and a verifier, showing verifiers can make intractable problems tractable. Simple algorithms like tokenwise rejection sampling benefit significantly from verifiers, and a modified version with backtracking outperforms baselines in efficiency, accuracy, and diversity.


<details>
  <summary>Details</summary>
Motivation: To address the poorly understood quality-efficiency trade-offs in inference-time algorithms for constrained generation tasks, the paper aims to formalize the role of verifiers in simplifying intractable problems.

Method: Develops a mathematical framework combining a pre-trained language model generator oracle and a process verifier. Tests simple algorithms like tokenwise rejection sampling and introduces a modified version with backtracking.

Result: Demonstrates that verifiers can transform intractable problems into tractable ones. The modified tokenwise rejection sampling with backtracking outperforms baselines in computational efficiency, accuracy, and diversity.

Conclusion: Verifiers significantly enhance constrained generation tasks, and simple algorithms with verifier access can achieve robust improvements over traditional methods.

Abstract: Recently, a plethora of works have proposed inference-time algorithms (e.g.
best-of-n), which incorporate verifiers to assist the generation process. Their
quality-efficiency trade-offs have been empirically benchmarked on a variety of
constrained generation tasks, but the algorithmic design landscape is still
largely poorly understood. In this paper, we develop a mathematical framework
for reasoning about constrained generation using a pre-trained language model
generator oracle and a process verifier--which can decide whether a prefix can
be extended to a string which satisfies the constraints of choice. We show that
even in very simple settings, access to a verifier can render an intractable
problem (information-theoretically or computationally) to a tractable one. In
fact, we show even simple algorithms, like tokenwise rejection sampling, can
enjoy significant benefits from access to a verifier. Empirically, we show that
a natural modification of tokenwise rejection sampling, in which the sampler is
allowed to "backtrack" (i.e., erase the final few generated tokens) has robust
and substantive benefits over natural baselines (e.g. (blockwise) rejection
sampling, nucleus sampling)--both in terms of computational efficiency,
accuracy and diversity.

</details>


### [101] [The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text](https://arxiv.org/pdf/2502.14921)
*Matthieu Meeus, Lukas Wutschitz, Santiago Zanella-Béguelin, Shruti Tople, Reza Shokri*

Main category: cs.CL

TL;DR: The paper investigates privacy risks in synthetic data generated by LLMs, showing that membership inference attacks (MIAs) can leak training data information. It also improves canary design for better privacy auditing.


<details>
  <summary>Details</summary>
Motivation: To understand and address privacy risks in synthetic data generated by LLMs, as current methods may underestimate leakage.

Method: Designs MIAs targeting LLM training data and introduces in-distribution canaries with high-perplexity suffixes for better privacy auditing.

Result: MIAs successfully leak training data information, and the new canary design improves detection of privacy risks in synthetic data.

Conclusion: Synthetic data from LLMs can leak training data, and improved canary methods enhance privacy risk assessment.

Abstract: How much information about training samples can be leaked through synthetic
data generated by Large Language Models (LLMs)? Overlooking the subtleties of
information flow in synthetic data generation pipelines can lead to a false
sense of privacy. In this paper, we assume an adversary has access to some
synthetic data generated by a LLM. We design membership inference attacks
(MIAs) that target the training data used to fine-tune the LLM that is then
used to synthesize data. The significant performance of our MIA shows that
synthetic data leak information about the training data. Further, we find that
canaries crafted for model-based MIAs are sub-optimal for privacy auditing when
only synthetic data is released. Such out-of-distribution canaries have limited
influence on the model's output when prompted to generate useful,
in-distribution synthetic data, which drastically reduces their effectiveness.
To tackle this problem, we leverage the mechanics of auto-regressive models to
design canaries with an in-distribution prefix and a high-perplexity suffix
that leave detectable traces in synthetic data. This enhances the power of
data-based MIAs and provides a better assessment of the privacy risks of
releasing synthetic data generated by LLMs.

</details>


### [102] [MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models](https://arxiv.org/pdf/2502.16671)
*Hengzhi Li, Megan Tjandrasuwita, Yi R. Fung, Armando Solar-Lezama, Paul Pu Liang*

Main category: cs.CL

TL;DR: The paper introduces MimeQA, a dataset for evaluating nonverbal social reasoning in AI, highlighting the limitations of current language-dominant approaches.


<details>
  <summary>Details</summary>
Motivation: To address the gap in AI's nonverbal social understanding by leveraging mime videos, which are rich in nonverbal interactions.

Method: Developed MimeQA, a dataset of 8 hours of mime videos with 806 annotated QA pairs, and evaluated state-of-the-art video large language models (vLLMs).

Result: vLLMs scored 20-30% accuracy on MimeQA, significantly lower than humans (86%), due to poor grounding of imagined objects and over-reliance on text prompts.

Conclusion: The work underscores the need for AI models to better interpret nonverbal interactions and aims to inspire future research in socially intelligent AI.

Abstract: As AI becomes more closely integrated with peoples' daily activities,
socially intelligent AI that can understand and interact seamlessly with humans
in daily lives is increasingly important. However, current works in AI social
reasoning all rely on language-only or language-dominant approaches to
benchmark and training models, resulting in systems that are improving in
verbal communication but struggle with nonverbal social understanding. To
address this limitation, we tap into a novel data source rich in nonverbal
social interactions -- mime videos. Mimes refer to the art of expression
through gesture and movement without spoken words, which presents unique
challenges and opportunities in interpreting nonverbal social communication. We
contribute a new dataset called MimeQA, obtained by sourcing 8 hours of videos
clips from YouTube and developing a comprehensive video question-answering
benchmark comprising 806 carefully annotated and verified question-answer
pairs, designed to probe nonverbal social reasoning capabilities. Using MimeQA,
we evaluate state-of-the-art video large language models (vLLMs) and find that
they achieve low overall accuracy, ranging from 20-30%, while humans score 86%.
Our analysis reveals that vLLMs often fail to ground imagined objects and
over-rely on the text prompt while ignoring subtle nonverbal interactions. We
hope to inspire future work in AI models that embody true social intelligence
capable of interpreting non-verbal human interactions.

</details>


### [103] [Improving Customer Service with Automatic Topic Detection in User Emails](https://arxiv.org/pdf/2502.19115)
*Bojana Bašaragin, Darija Medvecki, Gorana Gojić, Milena Oparnica, Dragiša Mišković*

Main category: cs.CL

TL;DR: A novel NLP pipeline using BERTopic for automated email topic detection and labeling improves customer service efficiency at Telekom Srbija, achieving high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: To enhance customer service efficiency by automating email topic detection and labeling, particularly for Serbian, a low-resourced and morphologically rich language.

Method: The pipeline uses BERTopic for unsupervised topic modeling, followed by preprocessing and postprocessing steps to classify emails into 12 topics and additional labels.

Result: Achieved a weighted average processing time of 0.041 seconds per email and a weighted average F1 score of 0.96.

Conclusion: The system successfully streamlines customer service operations and is adaptable to other languages.

Abstract: This study introduces a novel natural language processing pipeline that
enhances customer service efficiency at Telekom Srbija, a leading Serbian
telecommunications company, through automated email topic detection and
labeling. Central to the pipeline is BERTopic, a modular framework that allows
unsupervised topic modeling. After a series of preprocessing and postprocessing
steps, we assign one of 12 topics and several additional labels to incoming
emails, allowing customer service to filter and access them through a
custom-made application. While applied to Serbian, the methodology is
conceptually language-agnostic and can be readily adapted to other languages,
particularly those that are low-resourced and morphologically rich. The system
performance was evaluated by assessing the speed and correctness of the
automatically assigned topics, with a weighted average processing time of 0.041
seconds per email and a weighted average F1 score of 0.96. The system now
operates in the company's production environment, streamlining customer service
operations through automated email classification.

</details>


### [104] [Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models](https://arxiv.org/pdf/2502.20332)
*Yukang Yang, Declan Campbell, Kaixuan Huang, Mengdi Wang, Jonathan Cohen, Taylor Webb*

Main category: cs.CL

TL;DR: The paper investigates the internal mechanisms of LLMs for abstract reasoning, identifying a symbolic architecture involving symbol abstraction, symbolic induction, and retrieval.


<details>
  <summary>Details</summary>
Motivation: To understand the robustness and mechanisms behind emergent reasoning in LLMs, bridging the debate between symbolic and neural network approaches.

Method: Analyzes LLM layers to identify three computational steps: symbol abstraction, symbolic induction, and retrieval.

Result: Found an emergent symbolic architecture in LLMs, suggesting symbolic mechanisms underlie their reasoning.

Conclusion: Emergent reasoning in neural networks relies on symbolic mechanisms, resolving the symbolic vs. neural debate.

Abstract: Many recent studies have found evidence for emergent reasoning capabilities
in large language models (LLMs), but debate persists concerning the robustness
of these capabilities, and the extent to which they depend on structured
reasoning mechanisms. To shed light on these issues, we study the internal
mechanisms that support abstract reasoning in LLMs. We identify an emergent
symbolic architecture that implements abstract reasoning via a series of three
computations. In early layers, symbol abstraction heads convert input tokens to
abstract variables based on the relations between those tokens. In intermediate
layers, symbolic induction heads perform sequence induction over these abstract
variables. Finally, in later layers, retrieval heads predict the next token by
retrieving the value associated with the predicted abstract variable. These
results point toward a resolution of the longstanding debate between symbolic
and neural network approaches, suggesting that emergent reasoning in neural
networks depends on the emergence of symbolic mechanisms.

</details>


### [105] [GraphCheck: Multi-Path Fact-Checking with Entity-Relationship Graphs](https://arxiv.org/pdf/2502.20785)
*Hyewon Jeon, Jay-Yoon Lee*

Main category: cs.CL

TL;DR: GraphCheck and DP-GraphCheck improve automated fact-checking by using entity-relationship graphs and adaptive reasoning, excelling in complex claims while maintaining efficiency for simpler ones.


<details>
  <summary>Details</summary>
Motivation: Verifying complex claims requiring multi-hop reasoning is challenging in automated fact-checking.

Method: Proposes GraphCheck for structured verification via entity-relationship graphs and DP-GraphCheck, which adaptively selects reasoning strategies.

Result: Outperforms existing methods on HOVER and EX-FEVER datasets, especially for multi-hop claims.

Conclusion: The framework is versatile and generalizes well, enhancing both accuracy and efficiency in fact-checking.

Abstract: Automated fact-checking aims to assess the truthfulness of textual claims
based on relevant evidence. However, verifying complex claims that require
multi-hop reasoning remains a significant challenge. We propose GraphCheck, a
novel framework that transforms claims into entity-relationship graphs for
structured and systematic verification. By explicitly modeling both explicit
and latent entities and exploring multiple reasoning paths, GraphCheck improves
verification robustness. While GraphCheck excels in complex scenarios, it may
be unnecessarily elaborate for simpler claims. To address this, we introduce
DP-GraphCheck, a variant that employs a lightweight strategy selector to
adaptively choose between direct prompting and GraphCheck. This selective
mechanism improves both accuracy and efficiency by applying the appropriate
level of reasoning to each claim. Experiments on the HOVER and EX-FEVER
datasets demonstrate that our approach outperforms existing methods,
particularly on multi-hop claims. Moreover, the strategy selection mechanism in
DP-GraphCheck generalizes well to other fact-checking pipelines, highlighting
the versatility of our framework.

</details>


### [106] [Adversarial Tokenization](https://arxiv.org/pdf/2503.02174)
*Renato Lui Geh, Zilei Shao, Guy Van den Broeck*

Main category: cs.CL

TL;DR: The paper explores how adversarial tokenization can bypass LLM safety measures by using alternative tokenizations of harmful strings, revealing a new vulnerability in subword models.


<details>
  <summary>Details</summary>
Motivation: The study investigates whether LLMs, trained on a single tokenization, can understand alternative tokenizations and whether this can be exploited to evade safety restrictions.

Method: The authors empirically test adversarial tokenization across three state-of-the-art LLMs and adversarial datasets, comparing its effectiveness to existing adversarial approaches.

Result: Adversarial tokenization proves effective in bypassing safety measures without altering the text, outperforming some existing adversarial methods.

Conclusion: The findings highlight a previously unknown vulnerability in LLMs, emphasizing the need for robust defenses against adversarial tokenization.

Abstract: Current LLM pipelines account for only one possible tokenization for a given
string, ignoring exponentially many alternative tokenizations during training
and inference. For example, the standard Llama3 tokenization of penguin is
[p,enguin], yet [peng,uin] is another perfectly valid alternative. In this
paper, we show that despite LLMs being trained solely on one tokenization, they
still retain semantic understanding of other tokenizations, raising questions
about their implications in LLM safety. Put succinctly, we answer the following
question: can we adversarially tokenize an obviously malicious string to evade
safety and alignment restrictions? We show that not only is adversarial
tokenization an effective yet previously neglected axis of attack, but it is
also competitive against existing state-of-the-art adversarial approaches
without changing the text of the harmful request. We empirically validate this
exploit across three state-of-the-art LLMs and adversarial datasets, revealing
a previously unknown vulnerability in subword models.

</details>


### [107] [TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge](https://arxiv.org/pdf/2503.04381)
*Cheng-Han Chiang, Hung-yi Lee, Michal Lukasik*

Main category: cs.CL

TL;DR: TRACT combines CoT reasoning with regression-aware fine-tuning for LLM-as-a-judge tasks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect numerical score prediction or lack CoT reasoning, limiting LLM-as-a-judge performance.

Method: TRACT uses two-stage fine-tuning: first for CoT generation, then combining CE loss for CoT and regression loss for scores.

Result: TRACT significantly outperforms existing methods on four datasets and two LLMs.

Conclusion: TRACT's integration of CoT and regression-aware training improves LLM-as-a-judge performance, validated by ablation studies.

Abstract: The LLM-as-a-judge paradigm uses large language models (LLMs) for automated
text evaluation, where a numerical assessment is assigned by an LLM to the
input text following scoring rubrics. Existing methods for LLM-as-a-judge use
cross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of
score prediction. Recent work addresses numerical prediction limitations of LLM
fine-tuning through regression-aware fine-tuning, which, however, does not
consider chain-of-thought (CoT) reasoning for score prediction. In this paper,
we introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method
combining CoT reasoning with regression-aware training. TRACT consists of two
stages: first, seed LLM is fine-tuned to generate CoTs, which serve as
supervision for the second stage fine-tuning. The training objective of TRACT
combines the CE loss for learning the CoT reasoning capabilities, and the
regression-aware loss for the score prediction. Experiments across four
LLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms
existing methods. Extensive ablation studies validate the importance of each
component in TRACT.

</details>


### [108] [DiMA: An LLM-Powered Ride-Hailing Assistant at DiDi](https://arxiv.org/pdf/2503.04768)
*Yansong Ning, Shuowei Cai, Wei Li, Jun Fang, Naiqiang Tan, Hua Chai, Hao Liu*

Main category: cs.CL

TL;DR: DiMA, an LLM-powered ride-hailing assistant, enhances urban transportation with spatiotemporal-aware planning and cost-effective dialogue, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve ride-hailing services by providing seamless, natural, and efficient conversational interactions under dynamic urban conditions.

Method: Proposes a spatiotemporal-aware order planning module, a cost-effective dialogue system, and continual fine-tuning using real-world and simulated data.

Result: Achieves 93% accuracy in order planning and 92% in response generation, with significant improvements over state-of-the-art frameworks.

Conclusion: DiMA is an effective, efficient, and intelligent assistant for ride-hailing services, validated by real-world deployment and offline experiments.

Abstract: On-demand ride-hailing services like DiDi, Uber, and Lyft have transformed
urban transportation, offering unmatched convenience and flexibility. In this
paper, we introduce DiMA, an LLM-powered ride-hailing assistant deployed in
DiDi Chuxing. Its goal is to provide seamless ride-hailing services and beyond
through a natural and efficient conversational interface under dynamic and
complex spatiotemporal urban contexts. To achieve this, we propose a
spatiotemporal-aware order planning module that leverages external tools for
precise spatiotemporal reasoning and progressive order planning. Additionally,
we develop a cost-effective dialogue system that integrates multi-type dialog
repliers with cost-aware LLM configurations to handle diverse conversation
goals and trade-off response quality and latency. Furthermore, we introduce a
continual fine-tuning scheme that utilizes real-world interactions and
simulated dialogues to align the assistant's behavior with human preferred
decision-making processes. Since its deployment in the DiDi application, DiMA
has demonstrated exceptional performance, achieving 93% accuracy in order
planning and 92% in response generation during real-world interactions. Offline
experiments further validate DiMA capabilities, showing improvements of up to
70.23% in order planning and 321.27% in response generation compared to three
state-of-the-art agent frameworks, while reducing latency by $0.72\times$ to
$5.47\times$. These results establish DiMA as an effective, efficient, and
intelligent mobile assistant for ride-hailing services.

</details>


### [109] [Taming Knowledge Conflicts in Language Models](https://arxiv.org/pdf/2503.10996)
*Gaotang Li, Yuzhong Chen, Hanghang Tong*

Main category: cs.CL

TL;DR: The paper introduces JuICE, a method to address knowledge conflicts in LMs by mitigating superposition effects in attention heads without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Language Models (LMs) face conflicts when parametric memory clashes with contextual knowledge, attributed to attention heads. This study challenges the assumption of exclusive roles for memory and context heads.

Method: Proposes JuICE, a test-time intervention method that identifies reliable attention heads and uses a dual-run approach to steer LMs toward either parametric beliefs or contextual knowledge.

Result: JuICE achieves state-of-the-art performance across 11 datasets and 6 model architectures, showing robust generalization and consistent improvement.

Conclusion: The study theoretically analyzes knowledge conflict and superposition effects, validating JuICE's effectiveness in addressing these challenges.

Abstract: Language Models (LMs) often encounter knowledge conflicts when parametric
memory contradicts contextual knowledge. Previous works attribute this conflict
to the interplay between "memory heads" and "context heads", attention heads
assumed to promote either memory or context exclusively. In this study, we go
beyond this fundamental assumption by uncovering a critical phenomenon we term
the superposition of contextual information and parametric memory, where highly
influential attention heads simultaneously contribute to both memory and
context. Building upon this insight, we propose Just Run Twice (JuICE), a
test-time attention intervention method that steers LMs toward either
parametric beliefs or contextual knowledge without requiring fine-tuning. JuICE
identifies a set of reliable attention heads and leverages a dual-run approach
to mitigate the superposition effects. Extensive experiments across 11 datasets
and 6 model architectures demonstrate that JuICE sets the new state-of-the-art
performance and robust generalization, achieving significant and consistent
improvement across different domains under various conflict types. Finally, we
theoretically analyze knowledge conflict and the superposition of contextual
information and parametric memory in attention heads, which further elucidates
the effectiveness of JuICE in these settings. Our code is available at
https://github.com/GaotangLi/JUICE.

</details>


### [110] [Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning](https://arxiv.org/pdf/2504.05632)
*Sanchit Kabra, Akshita Jha, Chandan K. Reddy*

Main category: cs.CL

TL;DR: Improved reasoning in large language models reduces stereotypical bias, and ReGiFT fine-tuning enhances fairness without fairness-specific supervision.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between reasoning abilities and fairness in language models, and whether better reasoning mitigates harmful stereotypes.

Method: Evaluated multiple LLMs, introduced ReGiFT (Reasoning Guided Fine-Tuning) to infuse reasoning traces into models lacking such capabilities.

Result: Larger models with stronger reasoning showed lower bias; ReGiFT improved fairness and outperformed advanced reasoning models.

Conclusion: Enhancing reasoning is an effective, fairness-agnostic way to reduce stereotypical bias.

Abstract: Recent advances in large-scale generative language models have shown that
reasoning capabilities can significantly improve model performance across a
variety of tasks. However, the impact of reasoning on a model's ability to
mitigate stereotypical responses remains largely underexplored. In this work,
we investigate the crucial relationship between a model's reasoning ability and
fairness, and ask whether improved reasoning capabilities can mitigate harmful
stereotypical responses, especially those arising due to shallow or flawed
reasoning. We conduct a comprehensive evaluation of multiple open-source LLMs,
and find that larger models with stronger reasoning abilities exhibit
substantially lower stereotypical bias on existing fairness benchmarks.
Building on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning,
a novel approach that extracts structured reasoning traces from advanced
reasoning models and infuses them into models that lack such capabilities. We
use only general-purpose reasoning and do not require any fairness-specific
supervision for bias mitigation. Notably, we see that models fine-tuned using
ReGiFT not only improve fairness relative to their non-reasoning counterparts
but also outperform advanced reasoning models on fairness benchmarks. We also
analyze how variations in the correctness of the reasoning traces and their
length influence model fairness and their overall performance. Our findings
highlight that enhancing reasoning capabilities is an effective,
fairness-agnostic strategy for mitigating stereotypical bias caused by
reasoning flaws.

</details>


### [111] [Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations](https://arxiv.org/pdf/2504.13816)
*Chenghao Xiao, Hou Pong Chan, Hao Zhang, Mahani Aljunied, Lidong Bing, Noura Al Moubayed, Yu Rong*

Main category: cs.CL

TL;DR: This paper analyzes how LLMs recognize knowledge boundaries across languages, revealing key findings about their internal representations and proposing methods to reduce hallucination risks.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' knowledge boundaries is crucial to prevent hallucination, but prior research has focused mainly on English. This study extends the analysis to multiple languages.

Method: The study probes LLMs' internal representations when processing known and unknown questions in various languages. It also proposes a training-free alignment method and evaluates fine-tuning on bilingual question pairs.

Result: Key findings include: 1) Knowledge boundaries are encoded in middle to middle-upper layers; 2) Language differences follow a linear structure; 3) Fine-tuning enhances cross-lingual boundary recognition.

Conclusion: The study provides insights into LLMs' cross-lingual knowledge boundaries and introduces a multilingual evaluation suite to support future research.

Abstract: While understanding the knowledge boundaries of LLMs is crucial to prevent
hallucination, research on the knowledge boundaries of LLMs has predominantly
focused on English. In this work, we present the first study to analyze how
LLMs recognize knowledge boundaries across different languages by probing their
internal representations when processing known and unknown questions in
multiple languages. Our empirical studies reveal three key findings: 1) LLMs'
perceptions of knowledge boundaries are encoded in the middle to middle-upper
layers across different languages. 2) Language differences in knowledge
boundary perception follow a linear structure, which motivates our proposal of
a training-free alignment method that effectively transfers knowledge boundary
perception ability across languages, thereby helping reduce hallucination risk
in low-resource languages; 3) Fine-tuning on bilingual question pair
translation further enhances LLMs' recognition of knowledge boundaries across
languages. Given the absence of standard testbeds for cross-lingual knowledge
boundary analysis, we construct a multilingual evaluation suite comprising
three representative types of knowledge boundary data. Our code and datasets
are publicly available at
https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.

</details>


### [112] [Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents](https://arxiv.org/pdf/2504.18839)
*Abdellah Ghassel, Xianzhi Li, Xiaodan Zhu*

Main category: cs.CL

TL;DR: The paper introduces a 'Detect, Explain, Escalate' framework to manage dialogue breakdowns in LLMs, using a fine-tuned 8B-parameter model for detection and explanation, and advanced prompting for escalation, reducing costs by 54%.


<details>
  <summary>Details</summary>
Motivation: Addressing the susceptibility of LLMs to conversational breakdowns to enhance user trust and ensure low-carbon operation.

Method: Fine-tunes a compact 8B-parameter model for real-time breakdown detection and explanation, and evaluates frontier LLMs with advanced prompting for escalation.

Result: Improves accuracy by 7% over baseline, reduces inference costs by 54%, and achieves state-of-the-art results on benchmarks.

Conclusion: The framework offers a scalable, efficient, and interpretable solution for robust conversational AI, with publicly released code and models.

Abstract: While Large Language Models (LLMs) are transforming numerous applications,
their susceptibility to conversational breakdowns remains a critical challenge
undermining user trust. This paper introduces a "Detect, Explain, Escalate"
framework to manage dialogue breakdowns in LLM-powered agents, emphasizing
low-carbon operation. Our approach integrates two key strategies: (1) We
fine-tune a compact 8B-parameter model, augmented with teacher-generated
reasoning traces, which serves as an efficient real-time breakdown 'detector'
and 'explainer'. This model demonstrates robust classification and calibration
on English and Japanese dialogues, and generalizes well to the BETOLD dataset,
improving accuracy by 7% over its baseline. (2) We systematically evaluate
frontier LLMs using advanced prompting (few-shot, chain-of-thought, analogical
reasoning) for high-fidelity breakdown assessment. These are integrated into an
'escalation' architecture where our efficient detector defers to larger models
only when necessary, substantially reducing operational costs and energy
consumption. Our fine-tuned model and prompting strategies establish new
state-of-the-art results on dialogue breakdown detection benchmarks,
outperforming specialized classifiers and significantly narrowing the
performance gap to larger proprietary models. The proposed monitor-escalate
pipeline reduces inference costs by 54%, offering a scalable, efficient, and
more interpretable solution for robust conversational AI in high-impact
domains. Code and models will be publicly released.

</details>


### [113] [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/pdf/2504.19565)
*Meng Xiao, Xunxin Cai, Qingqing Long, Chengrui Wang, Yuanchun Zhou, Hengshu Zhu*

Main category: cs.CL

TL;DR: A knowledge-driven, multi-agent framework distills high-quality biomedical datasets for LLM training, outperforming proprietary models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of quality annotated biomedical corpora for effective LLM training.

Method: Collaborative multi-agent architecture guided by MeSH hierarchy to autonomously extract and refine domain-specific QA pairs.

Result: LLMs trained on distilled datasets outperform proprietary models (e.g., GPT-4) in biomedical QA tasks.

Conclusion: Multi-agent collaboration effectively enhances biomedical LLM training, validated by experimental results.

Abstract: Corpus distillation for biomedical large language models (LLMs) seeks to
address the pressing challenge of insufficient quantity and quality in
open-source annotated scientific corpora, which remains a bottleneck for
effective LLM training in biomedical research. This paper proposes a
knowledge-driven, agentic framework for scientific corpus distillation,
tailored explicitly for LLM training in the biomedical domain, addressing the
challenge posed by the complex hierarchy of biomedical knowledge. Central to
our approach is a collaborative multi-agent architecture, where specialized
agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in
concert to autonomously extract, synthesize, and self-evaluate high-quality
textual data from vast scientific literature. This agentic framework
collectively generates and refines domain-specific question-answer pairs,
ensuring comprehensive coverage and consistency with biomedical ontologies
while minimizing manual involvement. Extensive experimental results show that
language models trained on our multi-agent distilled datasets achieve notable
improvements in biomedical question-answering tasks, outperforming both strong
life sciences LLM baselines and advanced proprietary models. Notably, our
AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and
Med-PaLM-2, despite their larger scale. Detailed ablation studies and case
analyses further validate the effectiveness and synergy of each agent within
the framework, highlighting the potential of multi-agent collaboration in
biomedical LLM training.

</details>


### [114] [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/pdf/2505.14804)
*Maxence Verhaverbeke, Julie A. Gramaccia, Richard Khoury*

Main category: cs.CL

TL;DR: The paper introduces an automated pipeline for extracting 5W1H (who, what, when, where, why, how) information from French news articles, matching GPT-4o's performance.


<details>
  <summary>Details</summary>
Motivation: 5W1H questions are essential for tasks like summarization and news aggregation, but automated extraction for French news was lacking.

Method: Developed an extraction pipeline and evaluated it on a corpus of 250 Quebec news articles annotated by humans.

Result: The pipeline performs comparably to GPT-4o in extracting 5W1H information.

Conclusion: The pipeline is effective for automated 5W1H extraction from French news, offering a viable alternative to large language models.

Abstract: The 5W1H questions -- who, what, when, where, why and how -- are commonly
used in journalism to ensure that an article describes events clearly and
systematically. Answering them is a crucial prerequisites for tasks such as
summarization, clustering, and news aggregation. In this paper, we design the
first automated extraction pipeline to get 5W1H information from French news
articles. To evaluate the performance of our algorithm, we also create a corpus
of 250 Quebec news articles with 5W1H answers marked by four human annotators.
Our results demonstrate that our pipeline performs as well in this task as the
large language model GPT-4o.

</details>


### [115] [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/pdf/2505.18223)
*Hanyu Li, Haoyu Liu, Tingyu Zhu, Tianyu Guo, Zeyu Zheng, Xiaotie Deng, Michael I. Jordan*

Main category: cs.CL

TL;DR: IDA-Bench evaluates LLM agents in multi-round interactive data analysis tasks, revealing their limitations compared to single-turn benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook the iterative nature of data analysis, where expert decisions evolve with deeper insights.

Method: Introduces IDA-Bench, derived from Kaggle notebooks, simulating multi-round interactions via LLM-simulated users and comparing agent outputs to human baselines.

Result: State-of-the-art coding agents succeed in <50% of tasks, exposing limitations in multi-round scenarios.

Conclusion: Highlights the need to enhance LLMs' multi-round capabilities for reliable data analysis agents, balancing instruction following and reasoning.

Abstract: Large Language Models (LLMs) show promise as data analysis agents, but
existing benchmarks overlook the iterative nature of the field, where experts'
decisions evolve with deeper insights of the dataset. To address this, we
introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round
interactive scenarios. Derived from complex Kaggle notebooks, tasks are
presented as sequential natural language instructions by an LLM-simulated user.
Agent performance is judged by comparing its final numerical output to the
human-derived baseline. Initial results show that even state-of-the-art coding
agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting
limitations not evident in single-turn tests. This work underscores the need to
improve LLMs' multi-round capabilities for building more reliable data analysis
agents, highlighting the necessity of achieving a balance between instruction
following and reasoning.

</details>


### [116] [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators](https://arxiv.org/pdf/2505.22777)
*John Mendonça, Alon Lavie, Isabel Trancoso*

Main category: cs.CL

TL;DR: MEDAL is an automated multi-agent framework for creating diverse, multilingual dialogue benchmarks to address limitations in current chatbot evaluation datasets.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarking datasets for chatbots are static, outdated, and lack multilingual coverage, hindering accurate performance evaluation.

Method: MEDAL uses state-of-the-art LLMs to generate multilingual dialogues, evaluates them with GPT-4.1, and curates a new benchmark with human annotations.

Result: The framework reveals cross-lingual performance gaps and shows LLMs struggle with nuanced issues like empathy and reasoning.

Conclusion: MEDAL provides a more representative benchmark for chatbot evaluation, highlighting current LLM limitations in nuanced dialogue assessment.

Abstract: As the capabilities of chatbots and their underlying LLMs continue to
dramatically improve, evaluating their performance has increasingly become a
major blocker to their further development. A major challenge is the available
benchmarking datasets, which are largely static, outdated, and lacking in
multilingual coverage, limiting their ability to capture subtle linguistic and
cultural variations. This paper introduces MEDAL, an automated multi-agent
framework for generating, evaluating, and curating more representative and
diverse open-domain dialogue evaluation benchmarks. Our approach leverages
several state-of-the-art LLMs to generate user-chatbot multilingual dialogues,
conditioned on varied seed contexts. A strong LLM (GPT-4.1) is then used for a
multidimensional analysis of the performance of the chatbots, uncovering
noticeable cross-lingual performance differences. Guided by this large-scale
evaluation, we curate a new meta-evaluation multilingual benchmark and
human-annotate samples with nuanced quality judgments. This benchmark is then
used to assess the ability of several reasoning and non-reasoning LLMs to act
as evaluators of open-domain dialogues. We find that current LLMs struggle to
detect nuanced issues, particularly those involving empathy and reasoning.

</details>


### [117] [LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews](https://arxiv.org/pdf/2505.24757)
*Christian Jaumann, Andreas Wiedholz, Annemarie Friedrich*

Main category: cs.CL

TL;DR: LGAR, a zero-shot LLM-based abstract ranker, outperforms QA-based methods by 5-10 pp. in mean average precision for systematic literature reviews.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of scientific literature makes tracking state-of-the-art challenging, and existing abstract screening methods have limitations like binary classification focus or error propagation in QA-based ranking.

Method: Proposes LGAR, combining an LLM-based graded relevance scorer and a dense re-ranker, evaluated on manually extracted criteria from 57 SLRs.

Result: LGAR outperforms existing QA-based methods by 5-10 pp. in mean average precision.

Conclusion: LGAR offers a principled, effective solution for abstract screening in SLRs, with publicly available code and data.

Abstract: The scientific literature is growing rapidly, making it hard to keep track of
the state-of-the-art. Systematic literature reviews (SLRs) aim to identify and
evaluate all relevant papers on a topic. After retrieving a set of candidate
papers, the abstract screening phase determines initial relevance. To date,
abstract screening methods using large language models (LLMs) focus on binary
classification settings; existing question answering (QA) based ranking
approaches suffer from error propagation. LLMs offer a unique opportunity to
evaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks
do not provide them exhaustively. We manually extract these criteria as well as
research questions for 57 SLRs, mostly in the medical domain, enabling
principled comparisons between approaches. Moreover, we propose LGAR, a
zero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance
scorer and a dense re-ranker. Our extensive experiments show that LGAR
outperforms existing QA-based methods by 5-10 pp. in mean average precision.
Our code and data is publicly available.

</details>


### [118] [Data Swarms: Optimizable Generation of Synthetic Evaluation Data](https://arxiv.org/pdf/2506.00741)
*Shangbin Feng, Yike Wang, Weijia Shi, Yulia Tsvetkov*

Main category: cs.CL

TL;DR: Data Swarms is an algorithm for optimizing synthetic evaluation data for LLMs, using particle swarm optimization to improve data generators and adversarial swarms for co-evolving data and models.


<details>
  <summary>Details</summary>
Motivation: To enhance quantitative desiderata of LLM evaluation by generating synthetic data that reflects desired properties (e.g., difficulty) and improves model robustness.

Method: Train a swarm of data generators, define evaluation objectives, use particle swarm optimization to optimize generators, and extend to adversarial swarms for co-evolution.

Result: Outperforms eight baselines across five objectives; adversarial swarms improve robustness and generalization.

Conclusion: Data Swarms effectively optimizes multiple evaluation objectives and generalizes to unseen LLMs.

Abstract: We propose Data Swarms, an algorithm to optimize the generation of synthetic
evaluation data and advance quantitative desiderata of LLM evaluation. We first
train a swarm of initial data generators using existing data, and define
various evaluation objectives to reflect the desired properties of evaluation
(e.g., generate more difficult problems for the evaluated models) and
quantitatively evaluate data generators. We then employ particle swarm
optimization to optimize the swarm of data generators, where they
collaboratively search through the model parameter space to find new generators
that advance these objectives. We further extend it to Adversarial Swarms,
where the data generator swarm generates harder data while the test taker model
swarm learns from such data, co-evolving dynamically for better data and models
simultaneously. Extensive experiments demonstrate that Data Swarms outperforms
eight data generation baselines across five evaluation objectives, while
Adversarial Swarms produce more robust learning of synthetic data and stronger
generalization. Further analysis reveals that Data Swarms successfully
optimizes compositions of multiple evaluation objectives and generalizes to new
off-the-shelf LLMs, unseen at optimization time.

</details>


### [119] [GuessBench: Sensemaking Multimodal Creativity in the Wild](https://arxiv.org/pdf/2506.00814)
*Zifeng Zhu, Shangbin Feng, Herun Wan, Ningnan Wang, Minnan Luo, Yulia Tsvetkov*

Main category: cs.CL

TL;DR: GuessBench is a benchmark for evaluating Vision Language Models (VLMs) on human creativity, using data from a Minecraft minigame. It highlights challenges in creativity modeling and performance gaps between models.


<details>
  <summary>Details</summary>
Motivation: To assess VLMs' ability to model noisy and pluralistic human creativity in real-world scenarios, leveraging gameplay data for a robust evaluation.

Method: Curated 1500 images and designed 2000 problems from 'Guess the Build' Minecraft minigame, testing VLMs with varying hints and settings. Evaluated six VLMs and five reasoning approaches.

Result: State-of-the-art GPT-4o fails on 34% of instances, with a large performance gap (13.87% vs. 53.93%) between open and API models. Fine-tuning improves visual perception by 15.36%.

Conclusion: GuessBench is a challenging benchmark for creativity modeling, revealing performance disparities and biases tied to concept frequency and cultural contexts.

Abstract: We propose GuessBench, a novel benchmark that evaluates Vision Language
Models (VLMs) on modeling the pervasive, noisy, and pluralistic human
creativity. GuessBench sources data from "Guess the Build", an online
multiplayer Minecraft minigame where one player constructs a Minecraft build
given a concept (e.g. caterpillar) and others try to guess it with natural
language hints, presenting a pristine testbed for sensemaking creativity in the
wild with VLMs acting as guessers. We curate 1500 images from the actual
gameplay and design 2000 problems spanning static and dynamic image settings,
natural language hints of varying completeness, and more. Extensive experiments
with six open/API VLMs and five reasoning enhancement approaches demonstrate
that GuessBench presents a uniquely challenging task in creativity modeling:
even the start-of-the-art GPT-4o is incorrect on 34% of instances, while we
observe a huge performance gap (13.87% vs. 53.93% on average) between open and
API models. When used as a resource to improve VLMs, fine-tuning on the
reasoning traces for GuessBench problems improves visual perception tasks by
15.36% on average. Further analysis reveals that VLM performance in creativity
sensemaking correlates with the frequency of the concept in training data,
while the accuracy drops sharply for concepts in underrepresented cultural
contexts and low-resource languages.

</details>


### [120] [LLM in the Loop: Creating the ParaDeHate Dataset for Hate Speech Detoxification](https://arxiv.org/pdf/2506.01484)
*Shuzhou Yuan, Ercong Nie, Lukas Kouba, Ashish Yashwanth Kangen, Helmut Schmid, Hinrich Schütze, Michael Färber*

Main category: cs.CL

TL;DR: The paper proposes an LLM-in-the-loop pipeline using GPT-4o-mini for automated detoxification, creating ParaDeHate, a large-scale dataset for hate speech detoxification, and shows improved performance of models like BART when fine-tuned on it.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality parallel datasets for detoxification, especially for hate speech, due to the cost and sensitivity of human annotation.

Method: A novel LLM-in-the-loop pipeline leveraging GPT-4o-mini to automate detoxification, replicating the ParaDetox pipeline and constructing the ParaDeHate dataset.

Result: Models like BART, fine-tuned on ParaDeHate, achieve better performance in style accuracy, content preservation, and fluency.

Conclusion: LLM-generated detoxification text is a scalable and effective alternative to human annotation.

Abstract: Detoxification, the task of rewriting harmful language into non-toxic text,
has become increasingly important amid the growing prevalence of toxic content
online. However, high-quality parallel datasets for detoxification, especially
for hate speech, remain scarce due to the cost and sensitivity of human
annotation. In this paper, we propose a novel LLM-in-the-loop pipeline
leveraging GPT-4o-mini for automated detoxification. We first replicate the
ParaDetox pipeline by replacing human annotators with an LLM and show that the
LLM performs comparably to human annotation. Building on this, we construct
ParaDeHate, a large-scale parallel dataset specifically for hatespeech
detoxification. We release ParaDeHate as a benchmark of over 8K hate/non-hate
text pairs and evaluate a wide range of baseline methods. Experimental results
show that models such as BART, fine-tuned on ParaDeHate, achieve better
performance in style accuracy, content preservation, and fluency, demonstrating
the effectiveness of LLM-generated detoxification text as a scalable
alternative to human annotation.

</details>


### [121] [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/pdf/2506.01723)
*Soyoung Oh, Xinting Huang, Mathis Pink, Michael Hahn, Vera Demberg*

Main category: cs.CL

TL;DR: The paper investigates how a transformer model processes idioms, identifying three steps: retrieving figurative meaning early, suppressing literal interpretation, and maintaining parallel paths for both meanings.


<details>
  <summary>Details</summary>
Motivation: Idioms challenge language models due to their non-compositional meanings. Understanding how models handle this duality is key to improving their interpretability.

Method: Mechanistic interpretability tools are used to analyze how a pretrained transformer (LLama3.2-1B-base) processes idioms, focusing on attention and MLP sublayers.

Result: The model retrieves figurative meaning early, suppresses literal interpretation via specific attention heads, and maintains parallel paths for both meanings.

Conclusion: The study provides mechanistic evidence for idiom comprehension in transformers, highlighting dual-path processing.

Abstract: Idioms present a unique challenge for language models due to their
non-compositional figurative meanings, which often strongly diverge from the
idiom's literal interpretation. This duality requires a model to learn
representing and deciding between the two meanings to interpret an idiom in a
figurative sense, or literally. In this paper, we employ tools from mechanistic
interpretability to trace how a large pretrained causal transformer
(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom
processing: First, the idiom's figurative meaning is retrieved in early
attention and MLP sublayers. We identify specific attention heads which boost
the figurative meaning of the idiom while suppressing the idiom's literal
interpretation. The model subsequently represents the figurative representation
through an intermediate path. Meanwhile, a parallel bypass route forwards
literal interpretation, ensuring that a both reading remain available. Overall,
our findings provide a mechanistic evidence for idiom comprehension in an
autoregressive transformer.

</details>


### [122] [Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor](https://arxiv.org/pdf/2506.01819)
*Mohammadamin Shafiei, Hamidreza Saffari*

Main category: cs.CL

TL;DR: The paper addresses the gap in aligning LLMs with professional workplace humor, creating a dataset for evaluation, and finds LLMs struggle with judging humor appropriateness.


<details>
  <summary>Details</summary>
Motivation: Humor, especially in professional settings, is overlooked in AI alignment efforts despite its importance in daily tasks.

Method: Developed a dataset of professional humor statements with features for appropriateness and evaluated five LLMs.

Result: LLMs often fail to accurately judge the appropriateness of professional humor.

Conclusion: There's a need for improved alignment of LLMs with nuanced human values like humor in professional contexts.

Abstract: With the recent advances in Artificial Intelligence (AI) and Large Language
Models (LLMs), the automation of daily tasks, like automatic writing, is
getting more and more attention. Hence, efforts have focused on aligning LLMs
with human values, yet humor, particularly professional industrial humor used
in workplaces, has been largely neglected. To address this, we develop a
dataset of professional humor statements along with features that determine the
appropriateness of each statement. Our evaluation of five LLMs shows that LLMs
often struggle to judge the appropriateness of humor accurately.

</details>


### [123] [Identifying Reliable Evaluation Metrics for Scientific Text Revision](https://arxiv.org/pdf/2506.04772)
*Léane Jourdan, Florian Boudin, Richard Dufour, Nicolas Hernandez*

Main category: cs.CL

TL;DR: The paper critiques traditional text revision metrics like ROUGE and BERTScore, proposing a hybrid approach combining LLM-as-a-judge and domain-specific metrics for better evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics fail to capture meaningful improvements in scientific writing revisions, prompting the need for better evaluation methods aligned with human judgments.

Method: The study includes manual annotation of revision quality, exploration of reference-free metrics, and analysis of LLM-as-a-judge approaches with and without gold references.

Result: LLMs excel at assessing instruction-following but lack correctness evaluation, while domain-specific metrics offer complementary insights. A hybrid approach proves most reliable.

Conclusion: A hybrid method combining LLM-as-a-judge and task-specific metrics provides the most accurate assessment of revision quality in scientific writing.

Abstract: Evaluating text revision in scientific writing remains a challenge, as
traditional metrics such as ROUGE and BERTScore primarily focus on similarity
rather than capturing meaningful improvements. In this work, we analyse and
identify the limitations of these metrics and explore alternative evaluation
methods that better align with human judgments. We first conduct a manual
annotation study to assess the quality of different revisions. Then, we
investigate reference-free evaluation metrics from related NLP domains.
Additionally, we examine LLM-as-a-judge approaches, analysing their ability to
assess revisions with and without a gold reference. Our results show that LLMs
effectively assess instruction-following but struggle with correctness, while
domain-specific metrics provide complementary insights. We find that a hybrid
approach combining LLM-as-a-judge evaluation and task-specific metrics offers
the most reliable assessment of revision quality.

</details>


### [124] [Does It Make Sense to Speak of Introspection in Large Language Models?](https://arxiv.org/pdf/2506.05068)
*Iulia M. Comsa, Murray Shanahan*

Main category: cs.CL

TL;DR: The paper examines whether LLMs' self-reports can be considered introspection, critiquing two examples—one invalid and one minimally valid.


<details>
  <summary>Details</summary>
Motivation: To explore the interpretation of LLMs' self-reports and their relation to introspection, given their linguistic fluency.

Method: Critique of two examples of LLM self-reports: one on creative writing (invalid) and another on inferring its temperature parameter (minimally valid).

Result: The first example is deemed invalid for introspection, while the second is a minimal example, likely without consciousness.

Conclusion: Introspection can be minimally applied to LLMs, but without implying consciousness.

Abstract: Large language models (LLMs) exhibit compelling linguistic behaviour, and
sometimes offer self-reports, that is to say statements about their own nature,
inner workings, or behaviour. In humans, such reports are often attributed to a
faculty of introspection and are typically linked to consciousness. This raises
the question of how to interpret self-reports produced by LLMs, given their
increasing linguistic fluency and cognitive capabilities. To what extent (if
any) can the concept of introspection be meaningfully applied to LLMs? Here, we
present and critique two examples of apparent introspective self-report from
LLMs. In the first example, an LLM attempts to describe the process behind its
own "creative" writing, and we argue this is not a valid example of
introspection. In the second example, an LLM correctly infers the value of its
own temperature parameter, and we argue that this can be legitimately
considered a minimal example of introspection, albeit one that is (presumably)
not accompanied by conscious experience.

</details>


### [125] [Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective](https://arxiv.org/pdf/2506.05166)
*Bhavik Chandna, Zubair Bashir, Procheta Sen*

Main category: cs.CL

TL;DR: The paper analyzes bias in LLMs like GPT-2 and Llama2 using mechanistic interpretability, identifying localized bias-related components and showing their impact on model behavior and other NLP tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how social, demographic, and gender biases are structurally represented in LLMs and how these biases can be mitigated.

Method: Mechanistic interpretability approach with systematic ablations to identify and analyze bias-related components in models.

Result: Bias-related computations are localized in a few layers, vary with fine-tuning, and their removal affects both bias and other NLP tasks.

Conclusion: Bias in LLMs is structurally localized, and mitigating it impacts broader model functionality, highlighting trade-offs in debiasing efforts.

Abstract: Large Language Models (LLMs) are known to exhibit social, demographic, and
gender biases, often as a consequence of the data on which they are trained. In
this work, we adopt a mechanistic interpretability approach to analyze how such
biases are structurally represented within models such as GPT-2 and Llama2.
Focusing on demographic and gender biases, we explore different metrics to
identify the internal edges responsible for biased behavior. We then assess the
stability, localization, and generalizability of these components across
dataset and linguistic variations. Through systematic ablations, we demonstrate
that bias-related computations are highly localized, often concentrated in a
small subset of layers. Moreover, the identified components change across
fine-tuning settings, including those unrelated to bias. Finally, we show that
removing these components not only reduces biased outputs but also affects
other NLP tasks, such as named entity recognition and linguistic acceptability
judgment because of the sharing of important components with these tasks.

</details>


### [126] [ECoRAG: Evidentiality-guided Compression for Long Context RAG](https://arxiv.org/pdf/2506.05167)
*Yeonseok Jeong, Jinsu Kim, Dohyeon Lee, Seung-won Hwang*

Main category: cs.CL

TL;DR: ECoRAG improves LLM-based ODQA by compressing retrieved documents based on evidentiality, enhancing performance and cost-efficiency.


<details>
  <summary>Details</summary>
Motivation: Prior compression methods in RAG overlook filtering non-evidential information, limiting LLM performance.

Method: Proposes ECoRAG, which compresses documents based on evidentiality and retrieves more if evidence is insufficient.

Result: ECoRAG outperforms existing methods in ODQA tasks, reducing latency and token usage.

Conclusion: ECoRAG is a cost-efficient solution for improving LLM performance in RAG-based ODQA.

Abstract: Large Language Models (LLMs) have shown remarkable performance in Open-Domain
Question Answering (ODQA) by leveraging external documents through
Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer
context, context compression is necessary. However, prior compression methods
do not focus on filtering out non-evidential information, which limit the
performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or
ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved
documents based on evidentiality, ensuring whether answer generation is
supported by the correct evidence. As an additional step, ECoRAG reflects
whether the compressed content provides sufficient evidence, and if not,
retrieves more until sufficient. Experiments show that ECoRAG improves LLM
performance on ODQA tasks, outperforming existing compression methods.
Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency
but also minimizes token usage by retaining only the necessary information to
generate the correct answer. Code is available at
https://github.com/ldilab/ECoRAG.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [127] [Can ChatGPT Perform Image Splicing Detection? A Preliminary Study](https://arxiv.org/pdf/2506.05358)
*Souradip Nath*

Main category: cs.CV

TL;DR: GPT-4V shows strong zero-shot performance (85% accuracy) in detecting image splicing, with CoT prompting being the most effective. It leverages both visual artifacts and contextual knowledge, though it trails specialized models.


<details>
  <summary>Details</summary>
Motivation: To explore GPT-4V's out-of-the-box capabilities in image forensics, specifically for detecting image splicing, without task-specific fine-tuning.

Method: Evaluated GPT-4V using Zero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT) prompting on a subset of the CASIA v2.0 splicing dataset.

Result: Achieved competitive accuracy (85%) in zero-shot settings, with CoT providing the best balance. The model used visual and contextual reasoning.

Conclusion: GPT-4V is a flexible tool for image forensics due to its generalizability and interpretability, though it lags behind specialized models.

Abstract: Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning
across text and image modalities, showing promise in a variety of complex
vision-language tasks. In this preliminary study, we investigate the
out-of-the-box capabilities of GPT-4V in the domain of image forensics,
specifically, in detecting image splicing manipulations. Without any
task-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:
Zero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a
curated subset of the CASIA v2.0 splicing dataset.
  Our results show that GPT-4V achieves competitive detection performance in
zero-shot settings (more than 85% accuracy), with CoT prompting yielding the
most balanced trade-off across authentic and spliced images. Qualitative
analysis further reveals that the model not only detects low-level visual
artifacts but also draws upon real-world contextual knowledge such as object
scale, semantic consistency, and architectural facts, to identify implausible
composites. While GPT-4V lags behind specialized state-of-the-art splicing
detection models, its generalizability, interpretability, and encyclopedic
reasoning highlight its potential as a flexible tool in image forensics.

</details>


### [128] [CarboNeXT and CarboFormer: Dual Semantic Segmentation Architectures for Detecting and Quantifying Carbon Dioxide Emissions Using Optical Gas Imaging](https://arxiv.org/pdf/2506.05360)
*Taminul Islam, Toqi Tahamid Sarker, Mohamed G Embaby, Khaled R Ahmed, Amer AbuGhazaleh*

Main category: cs.CV

TL;DR: CarboNeXT is a semantic segmentation framework for detecting and quantifying CO$_2$ emissions using Optical Gas Imaging, outperforming state-of-the-art methods with high accuracy and real-time performance.


<details>
  <summary>Details</summary>
Motivation: CO$_2$ emissions are critical for environmental and industrial monitoring, especially in livestock management. Current methods lack robustness for diverse applications.

Method: Integrates a multi-scale context aggregation network with UPerHead and auxiliary FCN components, tested on two novel datasets (CCR and RTA).

Result: Achieves 88.46% mIoU on CCR and 92.95% mIoU on RTA, with 60.95 FPS. CarboFormer, a lightweight variant, offers 84.68 FPS and competitive performance.

Conclusion: CarboNeXT and CarboFormer provide robust, real-time tools for CO$_2$ emission analysis, advancing environmental sensing and precision livestock management.

Abstract: Carbon dioxide (CO$_2$) emissions are critical indicators of both
environmental impact and various industrial processes, including livestock
management. We introduce CarboNeXT, a semantic segmentation framework for
Optical Gas Imaging (OGI), designed to detect and quantify CO$_2$ emissions
across diverse applications. Our approach integrates a multi-scale context
aggregation network with UPerHead and auxiliary FCN components to effectively
model both local details and global relationships in gas plume imagery. We
contribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR)
dataset, which simulates gas leaks with systematically varied flow rates
(10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions
from dairy cow rumen fluid in vitro experiments. Extensive evaluations
demonstrate that CarboNeXT outperforms state-of-the-art methods, achieving
88.46% mIoU on CCR and 92.95% mIoU on RTA, with particular effectiveness in
challenging low-flow scenarios. The model operates at 60.95 FPS, enabling
real-time monitoring applications. Additionally, we propose CarboFormer, a
lightweight variant with only 5.07M parameters that achieves 84.68 FPS, with
competitive performance of 84.88% mIoU on CCR and 92.98% on RTA, making it
suitable for resource-constrained platforms such as programmable drones. Our
work advances both environmental sensing and precision livestock management by
providing robust tools for CO$_2$ emission analysis, with a specific focus on
livestock applications.

</details>


### [129] [Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching](https://arxiv.org/pdf/2506.05361)
*Tinglin Huang, Tianyu Liu, Mehrtash Babadi, Wengong Jin, Rex Ying*

Main category: cs.CV

TL;DR: STFlow is a flow matching generative model for spatial transcriptomics (ST) that addresses limitations of prior methods by modeling cell-cell interactions and enabling efficient whole-slide processing.


<details>
  <summary>Details</summary>
Motivation: Overcome the low throughput and specialized facility requirements of ST by predicting ST from histology images, while addressing prior methods' lack of cell-cell interaction modeling and memory constraints.

Method: Proposes STFlow, a flow matching generative model that models joint gene expression distribution and uses a slide-level encoder with local spatial attention for efficient processing.

Result: STFlow outperforms state-of-the-art baselines, achieving over 18% relative improvement on HEST-1k and STImage-1K4M benchmarks.

Conclusion: STFlow effectively addresses key limitations in ST prediction, offering a scalable and accurate solution for bridging histology and gene expression data.

Abstract: Spatial transcriptomics (ST) has emerged as a powerful technology for
bridging histology imaging with gene expression profiling. However, its
application has been limited by low throughput and the need for specialized
experimental facilities. Prior works sought to predict ST from whole-slide
histology images to accelerate this process, but they suffer from two major
limitations. First, they do not explicitly model cell-cell interaction as they
factorize the joint distribution of whole-slide ST data and predict the gene
expression of each spot independently. Second, their encoders struggle with
memory constraints due to the large number of spots (often exceeding 10,000) in
typical ST datasets. Herein, we propose STFlow, a flow matching generative
model that considers cell-cell interaction by modeling the joint distribution
of gene expression of an entire slide. It also employs an efficient slide-level
encoder with local spatial attention, enabling whole-slide processing without
excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M
benchmarks, STFlow substantially outperforms state-of-the-art baselines and
achieves over 18% relative improvements over the pathology foundation models.

</details>


### [130] [Seed Selection for Human-Oriented Image Reconstruction via Guided Diffusion](https://arxiv.org/pdf/2506.05363)
*Yui Tatsumi, Ziyue Zeng, Hiroshi Watanabe*

Main category: cs.CV

TL;DR: Proposes a seed selection method for diffusion-based scalable image coding to improve quality without extra bitrate by choosing optimal seeds from multiple candidates.


<details>
  <summary>Details</summary>
Motivation: Existing methods use a single random seed, leading to suboptimal image quality. The goal is to enhance quality without increasing bitrate.

Method: Selects optimal seed from multiple candidates using intermediate outputs from early reverse diffusion steps to reduce computational cost.

Result: Outperforms baseline across multiple metrics.

Conclusion: The proposed seed selection method effectively improves image quality without additional bitrate.

Abstract: Conventional methods for scalable image coding for humans and machines
require the transmission of additional information to achieve scalability. A
recent diffusion-based method avoids this by generating human-oriented images
from machine-oriented images without extra bitrate. This method, however, uses
a single random seed, which may lead to suboptimal image quality. In this
paper, we propose a seed selection method that identifies the optimal seed from
multiple candidates to improve image quality without increasing the bitrate. To
reduce computational cost, the selection is performed based on intermediate
outputs obtained from early steps of the reverse diffusion process.
Experimental results demonstrate that our method outperforms the baseline
across multiple metrics.

</details>


### [131] [Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment](https://arxiv.org/pdf/2506.05384)
*Zhuoxuan Cai, Jian Zhang, Xinbin Yuan, Pengtao Jiang, Wenxiang Chen, Bowen Tang, Lujian Yao, Qiyuan Wang, Jinwen Chen, Bo Li*

Main category: cs.CV

TL;DR: A unified two-stage training framework (Q-Ponder) improves visual quality assessment in MLLMs by jointly optimizing scoring accuracy and reasoning consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM approaches treat quality scoring and reasoning as separate tasks, creating a trade-off between accuracy and interpretability. This limits their potential in visual quality assessment.

Method: Proposes a two-stage framework: 1) Cold-start stage with expert-designed prompts and cross-entropy loss. 2) Reinforcement learning fine-tuning with Group Relative Policy Optimization (GRPO) for joint optimization.

Result: Q-Ponder achieves SOTA performance (6.5% higher SRCC) and outperforms description-based models in accuracy and reasonableness.

Conclusion: The unified framework enhances both scoring and reasoning, demonstrating generalization potential across diverse tasks.

Abstract: Recent studies demonstrate that multimodal large language models (MLLMs) can
proficiently evaluate visual quality through interpretable assessments.
However, existing approaches typically treat quality scoring and reasoning
descriptions as separate tasks with disjoint optimization objectives, leading
to a trade-off: models adept at quality reasoning descriptions struggle with
precise score regression, while score-focused models lack interpretability.
This limitation hinders the full potential of MLLMs in visual quality
assessment, where accuracy and interpretability should be mutually reinforcing.
To address this, we propose a unified two-stage training framework comprising a
cold-start stage and a reinforcement learning-based fine-tuning stage.
Specifically, in the first stage, we distill high-quality data from a teacher
model through expert-designed prompts, initializing reasoning capabilities via
cross-entropy loss supervision. In the second stage, we introduce a novel
reward with Group Relative Policy Optimization (GRPO) to jointly optimize
scoring accuracy and reasoning consistency. We designate the models derived
from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show
that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score
regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain
datasets. Furthermore, Q-Ponder significantly outperforms description-based
SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in
description accuracy and reasonableness, demonstrating the generalization
potential over diverse tasks.

</details>


### [132] [Text2Stereo: Repurposing Stable Diffusion for Stereo Generation with Consistency Rewards](https://arxiv.org/pdf/2506.05367)
*Aakash Garg, Libing Zeng, Andrii Tsarov, Nima Khademi Kalantari*

Main category: cs.CV

TL;DR: A diffusion-based method for generating stereo images from text prompts, leveraging Stable Diffusion and fine-tuning for improved stereo consistency and alignment.


<details>
  <summary>Details</summary>
Motivation: Stereo image datasets with large baselines are scarce, making training diffusion models from scratch impractical.

Method: Fine-tune Stable Diffusion on stereo datasets, enhance with prompt alignment and stereo consistency reward functions.

Result: Generates high-quality stereo images, outperforming existing methods.

Conclusion: The approach effectively adapts diffusion models for stereo image generation with superior results.

Abstract: In this paper, we propose a novel diffusion-based approach to generate stereo
images given a text prompt. Since stereo image datasets with large baselines
are scarce, training a diffusion model from scratch is not feasible. Therefore,
we propose leveraging the strong priors learned by Stable Diffusion and
fine-tuning it on stereo image datasets to adapt it to the task of stereo
generation. To improve stereo consistency and text-to-image alignment, we
further tune the model using prompt alignment and our proposed stereo
consistency reward functions. Comprehensive experiments demonstrate the
superiority of our approach in generating high-quality stereo images across
diverse scenarios, outperforming existing methods.

</details>


### [133] [TriPSS: A Tri-Modal Keyframe Extraction Framework Using Perceptual, Structural, and Semantic Representations](https://arxiv.org/pdf/2506.05395)
*Mert Can Cakmak, Nitin Agarwal, Diwash Poudel*

Main category: cs.CV

TL;DR: TriPSS is a tri-modal framework for keyframe extraction, combining color, structural, and semantic cues via PCA and HDBSCAN, achieving state-of-the-art performance on TVSum20 and SumMe datasets.


<details>
  <summary>Details</summary>
Motivation: Efficient keyframe extraction is challenging due to the need to capture rich video content, motivating the integration of multi-modal cues for better summarization and retrieval.

Method: TriPSS integrates CIELAB color features, ResNet-50 structural embeddings, and Llama-3.2-11B-Vision-Instruct captions, fusing them with PCA and clustering via HDBSCAN, followed by refinement.

Result: TriPSS outperforms traditional unimodal and previous multi-modal methods on TVSum20 and SumMe datasets, demonstrating superior performance.

Conclusion: TriPSS sets a new benchmark for video content understanding by effectively capturing nuanced visual and semantic information.

Abstract: Efficient keyframe extraction is critical for effective video summarization
and retrieval, yet capturing the complete richness of video content remains
challenging. In this work, we present TriPSS, a novel tri-modal framework that
effectively integrates perceptual cues from color features in the CIELAB space,
deep structural embeddings derived from ResNet-50, and semantic context from
frame-level captions generated by Llama-3.2-11B-Vision-Instruct. By fusing
these diverse modalities using principal component analysis, TriPSS constructs
robust multi-modal embeddings that enable adaptive segmentation of video
content via HDBSCAN clustering. A subsequent refinement stage incorporating
quality assessment and duplicate filtering ensures that the final keyframe set
is both concise and semantically rich. Comprehensive evaluations on benchmark
datasets TVSum20 and SumMe demonstrate that TriPSS achieves state-of-the-art
performance, substantially outperforming traditional unimodal and previous
multi-modal methods. These results underscore TriPSS's ability to capture
nuanced visual and semantic information, thereby setting a new benchmark for
video content understanding in large-scale retrieval scenarios.

</details>


### [134] [Speaking images. A novel framework for the automated self-description of artworks](https://arxiv.org/pdf/2506.05368)
*Valentine Bernasconi, Gustavo Marfia*

Main category: cs.CV

TL;DR: A new framework uses generative AI to create self-explaining videos of digitized artworks, addressing cultural biases and educational potential.


<details>
  <summary>Details</summary>
Motivation: To innovate access to digital cultural collections by exploring the malleability of digital images and their contemporary interpretations.

Method: Utilizes open-source large-language, face detection, text-to-speech, and audio-to-animation models to animate artworks into explanatory videos.

Result: Automated production of short videos where artwork characters explain their content, questioning cultural biases and educational uses.

Conclusion: The framework highlights the potential and challenges of generative AI in art history and cultural heritage education.

Abstract: Recent breakthroughs in generative AI have opened the door to new research
perspectives in the domain of art and cultural heritage, where a large number
of artifacts have been digitized. There is a need for innovation to ease the
access and highlight the content of digital collections. Such innovations
develop into creative explorations of the digital image in relation to its
malleability and contemporary interpretation, in confrontation to the original
historical object. Based on the concept of the autonomous image, we propose a
new framework towards the production of self-explaining cultural artifacts
using open-source large-language, face detection, text-to-speech and
audio-to-animation models. The goal is to start from a digitized artwork and to
automatically assemble a short video of the latter where the main character
animates to explain its content. The whole process questions cultural biases
encapsulated in large-language models, the potential of digital images and
deepfakes of artworks for educational purposes, along with concerns of the
field of art history regarding such creative diversions.

</details>


### [135] [SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing](https://arxiv.org/pdf/2506.05414)
*Mingfei Chen, Zijun Cui, Xiulong Liu, Jinlin Xiang, Caleb Zheng, Jingyuan Li, Eli Shlizerman*

Main category: cs.CV

TL;DR: SAVVY-Bench is the first benchmark for 3D spatial reasoning in dynamic scenes with spatial audio, and SAVVY, a training-free pipeline, improves AV-LLMs' performance in this task.


<details>
  <summary>Details</summary>
Motivation: Existing AV-LLMs and benchmarks focus on static or 2D scenes, leaving 3D spatial reasoning in dynamic audio-visual environments unexplored.

Method: SAVVY uses a two-stage pipeline: (i) Egocentric Spatial Tracks Estimation for object tracking and (ii) Dynamic Global Map Construction for unified trajectory mapping.

Result: SAVVY significantly boosts AV-LLMs' performance, setting a new standard for dynamic 3D spatial reasoning.

Conclusion: SAVVY-Bench and SAVVY advance the field by addressing dynamic 3D spatial reasoning, offering a robust framework for future research.

Abstract: 3D spatial reasoning in dynamic, audio-visual environments is a cornerstone
of human cognition yet remains largely unexplored by existing Audio-Visual
Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on
static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D
spatial reasoning in dynamic scenes with synchronized spatial audio.
SAVVY-Bench is comprised of thousands of relationships involving static and
moving objects, and requires fine-grained temporal grounding, consistent 3D
localization, and multi-modal annotation. To tackle this challenge, we propose
SAVVY, a novel training-free reasoning pipeline that consists of two stages:
(i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as
other audio-visual methods to track the trajectories of key objects related to
the query using both visual and spatial audio cues, and (ii) Dynamic Global Map
Construction, which aggregates multi-modal queried object trajectories and
converts them into a unified global dynamic map. Using the constructed map, a
final QA answer is obtained through a coordinate transformation that aligns the
global map with the queried viewpoint. Empirical evaluation demonstrates that
SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a
new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.

</details>


### [136] [MR.NAVI: Mixed-Reality Navigation Assistant for the Visually Impaired](https://arxiv.org/pdf/2506.05369)
*Nicolas Pfitzer, Yifan Zhou, Marco Poggensee, Defne Kurtulus, Bessie Dominguez-Dager, Mihai Dusmanu, Marc Pollefeys, Zuria Bauer*

Main category: cs.CV

TL;DR: MR.NAVI is a mixed reality system aiding visually impaired users with real-time scene understanding and audio feedback, combining computer vision and NLP for navigation and obstacle avoidance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by over 43 million severely visually impaired individuals in navigating unfamiliar environments.

Method: Uses computer vision (MobileNet for object detection, RANSAC and DBSCAN for obstacle avoidance) and NLP for contextual feedback. Integrates with public transit APIs for navigation.

Result: User studies demonstrated promising usability and effectiveness in scene description and navigation.

Conclusion: MR.NAVI effectively enhances spatial awareness and navigation for visually impaired users.

Abstract: Over 43 million people worldwide live with severe visual impairment, facing
significant challenges in navigating unfamiliar environments. We present
MR.NAVI, a mixed reality system that enhances spatial awareness for visually
impaired users through real-time scene understanding and intuitive audio
feedback. Our system combines computer vision algorithms for object detection
and depth estimation with natural language processing to provide contextual
scene descriptions, proactive collision avoidance, and navigation instructions.
The distributed architecture processes sensor data through MobileNet for object
detection and employs RANSAC-based floor detection with DBSCAN clustering for
obstacle avoidance. Integration with public transit APIs enables navigation
with public transportation directions. Through our experiments with user
studies, we evaluated both scene description and navigation functionalities in
unfamiliar environments, showing promising usability and effectiveness.

</details>


### [137] [A Compendium of Autonomous Navigation using Object Detection and Tracking in Unmanned Aerial Vehicles](https://arxiv.org/pdf/2506.05378)
*Mohit Arora, Pratyush Shukla, Shivali Chopra*

Main category: cs.CV

TL;DR: The paper reviews UAV autonomy challenges and solutions using computer vision for real-time object detection and tracking, with applications in fields like disaster management and surveillance.


<details>
  <summary>Details</summary>
Motivation: UAVs face operational challenges like signal quality, real-time processing, and data security. Autonomous navigation via computer vision can address these issues.

Method: The paper reviews various algorithms for object detection and tracking in UAVs, leveraging computer vision and deep learning.

Result: Autonomous UAVs using computer vision can improve real-time processing and expand applications in critical fields.

Conclusion: Computer vision-based autonomy enhances UAV capabilities, making them more reliable for diverse applications.

Abstract: Unmanned Aerial Vehicles (UAVs) are one of the most revolutionary inventions
of 21st century. At the core of a UAV lies the central processing system that
uses wireless signals to control their movement. The most popular UAVs are
quadcopters that use a set of four motors, arranged as two on either side with
opposite spin. An autonomous UAV is called a drone. Drones have been in service
in the US army since the 90's for covert missions critical to national
security. It would not be wrong to claim that drones make up an integral part
of the national security and provide the most valuable service during
surveillance operations. While UAVs are controlled using wireless signals,
there reside some challenges that disrupt the operation of such vehicles such
as signal quality and range, real time processing, human expertise, robust
hardware and data security. These challenges can be solved by programming UAVs
to be autonomous, using object detection and tracking, through Computer Vision
algorithms. Computer Vision is an interdisciplinary field that seeks the use of
deep learning to gain a high-level understanding of digital images and videos
for the purpose of automating the task of human visual system. Using computer
vision, algorithms for detecting and tracking various objects can be developed
suitable to the hardware so as to allow real time processing for immediate
judgement. This paper attempts to review the various approaches several authors
have proposed for the purpose of autonomous navigation of UAVs by through
various algorithms of object detection and tracking in real time, for the
purpose of applications in various fields such as disaster management, dense
area exploration, traffic vehicle surveillance etc.

</details>


### [138] [DVD: A Comprehensive Dataset for Advancing Violence Detection in Real-World Scenarios](https://arxiv.org/pdf/2506.05372)
*Dimitrios Kollias, Damith C. Senadeera, Jianian Zheng, Kaushal K. K. Yadav, Greg Slabaugh, Muhammad Awais, Xiaoyun Yang*

Main category: cs.CV

TL;DR: A new large-scale, frame-level annotated violence detection database (DVD) is introduced to address limitations in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing violence detection databases lack diversity, scale, and detailed annotations, limiting model generalization.

Method: The paper introduces DVD, a dataset with 500 videos (2.7M frames), frame-level annotations, diverse environments, lighting conditions, and rich metadata.

Result: DVD provides a comprehensive resource for violence detection, capturing real-world complexities.

Conclusion: DVD addresses key challenges in violence detection research by offering a diverse, well-annotated dataset.

Abstract: Violence Detection (VD) has become an increasingly vital area of research.
Existing automated VD efforts are hindered by the limited availability of
diverse, well-annotated databases. Existing databases suffer from coarse
video-level annotations, limited scale and diversity, and lack of metadata,
restricting the generalization of models. To address these challenges, we
introduce DVD, a large-scale (500 videos, 2.7M frames), frame-level annotated
VD database with diverse environments, varying lighting conditions, multiple
camera sources, complex social interactions, and rich metadata. DVD is designed
to capture the complexities of real-world violent events.

</details>


### [139] [Talk2SAM: Text-Guided Semantic Enhancement for Complex-Shaped Object Segmentation](https://arxiv.org/pdf/2506.05396)
*Luka Vetoshkin, Dmitry Yudin*

Main category: cs.CV

TL;DR: Talk2SAM improves segmentation of complex objects by integrating textual guidance with SAM-HQ, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Current segmentation models struggle with thin structures and fine boundaries, limiting their effectiveness for complex objects.

Method: Talk2SAM uses CLIP-based embeddings from text prompts, projected into DINO feature space, to guide SAM-HQ for better segmentation.

Result: Outperforms SAM-HQ by up to +5.9% IoU and +8.3% boundary IoU on benchmarks like BIG, ThinObject5K, and DIS5K.

Conclusion: Natural language guidance enhances segmentation precision, especially for challenging cases, and offers user-controllable segmentation.

Abstract: Segmenting objects with complex shapes, such as wires, bicycles, or
structural grids, remains a significant challenge for current segmentation
models, including the Segment Anything Model (SAM) and its high-quality variant
SAM-HQ. These models often struggle with thin structures and fine boundaries,
leading to poor segmentation quality. We propose Talk2SAM, a novel approach
that integrates textual guidance to improve segmentation of such challenging
objects. The method uses CLIP-based embeddings derived from user-provided text
prompts to identify relevant semantic regions, which are then projected into
the DINO feature space. These features serve as additional prompts for SAM-HQ,
enhancing its ability to focus on the target object. Beyond improving
segmentation accuracy, Talk2SAM allows user-controllable segmentation, enabling
disambiguation of objects within a single bounding box based on textual input.
We evaluate our approach on three benchmarks: BIG, ThinObject5K, and DIS5K.
Talk2SAM consistently outperforms SAM-HQ, achieving up to +5.9\% IoU and +8.3\%
boundary IoU improvements. Our results demonstrate that incorporating natural
language guidance provides a flexible and effective means for precise object
segmentation, particularly in cases where traditional prompt-based methods
fail. The source code is available on GitHub:
https://github.com/richlukich/Talk2SAM

</details>


### [140] [State Estimation and Control of Dynamic Systems from High-Dimensional Image Data](https://arxiv.org/pdf/2506.05375)
*Ashik E Rasul, Hyung-Jin Yoon*

Main category: cs.CV

TL;DR: A neural architecture combining CNNs and GRUs for state estimation in dynamic systems, enabling reinforcement learning without ground-truth states.


<details>
  <summary>Details</summary>
Motivation: Accurate state estimation is crucial for policy design but often impractical, necessitating alternative methods.

Method: Integrates CNNs for spatial features and GRUs for temporal modeling to learn state representations from image-action sequences, then trains a DQN agent.

Result: Achieves real-time, accurate estimation and control without ground-truth states, with a methodology to evaluate state accuracy.

Conclusion: The proposed method effectively learns state representations, improving policy performance and control stability.

Abstract: Accurate state estimation is critical for optimal policy design in dynamic
systems. However, obtaining true system states is often impractical or
infeasible, complicating the policy learning process. This paper introduces a
novel neural architecture that integrates spatial feature extraction using
convolutional neural networks (CNNs) and temporal modeling through gated
recurrent units (GRUs), enabling effective state representation from sequences
of images and corresponding actions. These learned state representations are
used to train a reinforcement learning agent with a Deep Q-Network (DQN).
Experimental results demonstrate that our proposed approach enables real-time,
accurate estimation and control without direct access to ground-truth states.
Additionally, we provide a quantitative evaluation methodology for assessing
the accuracy of the learned states, highlighting their impact on policy
performance and control stability.

</details>


### [141] [U-NetMN and SegNetMN: Modified U-Net and SegNet models for bimodal SAR image segmentation](https://arxiv.org/pdf/2506.05444)
*Marwane Kzadri, Franco Alberto Cardillo, Nanée Chahinian, Carole Delenne, Renaud Hostache, Jamal Riffi*

Main category: cs.CV

TL;DR: Mode normalization improves convergence speed and stability in SAR image segmentation models like U-Net and SegNet.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for SAR image segmentation struggle with convergence and stability due to complex data distributions.

Method: Evaluated mode normalization's impact on U-Net and SegNet, integrating it to enhance convergence without sacrificing performance.

Result: Mode normalization significantly speeds up convergence and increases model stability across different zones.

Conclusion: Normalization enhances computational efficiency and generalization in SAR image segmentation.

Abstract: Segmenting Synthetic Aperture Radar (SAR) images is crucial for many remote
sensing applications, particularly water body detection. However, deep
learning-based segmentation models often face challenges related to convergence
speed and stability, mainly due to the complex statistical distribution of this
type of data. In this study, we evaluate the impact of mode normalization on
two widely used semantic segmentation models, U-Net and SegNet. Specifically,
we integrate mode normalization, to reduce convergence time while maintaining
the performance of the baseline models. Experimental results demonstrate that
mode normalization significantly accelerates convergence. Furthermore,
cross-validation results indicate that normalized models exhibit increased
stability in different zones. These findings highlight the effectiveness of
normalization in improving computational efficiency and generalization in SAR
image segmentation.

</details>


### [142] [GroMo: Plant Growth Modeling with Multiview Images](https://arxiv.org/pdf/2503.06608)
*Ruchi Bhatt, Shreya Bansal, Amanpreet Chander, Rupinder Kaur, Malya Singh, Mohan Kankanhalli, Abdulmotaleb El Saddik, Mukesh Kumar Saini*

Main category: cs.CV

TL;DR: The paper introduces the GroMo challenge for plant age prediction and leaf count estimation using the GroMo25 dataset, featuring multiview images of four crops. A Multiview Vision Transformer (MVVT) model is proposed, achieving MAEs of 7.74 and 5.52 for the tasks, respectively.


<details>
  <summary>Details</summary>
Motivation: To advance plant phenotyping research and support precision agriculture by developing methods for accurate plant growth tracking.

Method: Proposes the MVVT model for analyzing multiview images of crops (radish, okra, wheat, mustard) to predict age and estimate leaf count.

Result: MVVT achieves an average MAE of 7.74 for age prediction and 5.52 for leaf count estimation.

Conclusion: The GroMo Challenge and MVVT model contribute to plant growth modeling, with potential applications in agriculture and phenotyping.

Abstract: Understanding plant growth dynamics is essential for applications in
agriculture and plant phenotyping. We present the Growth Modelling (GroMo)
challenge, which is designed for two primary tasks: (1) plant age prediction
and (2) leaf count estimation, both essential for crop monitoring and precision
agriculture. For this challenge, we introduce GroMo25, a dataset with images of
four crops: radish, okra, wheat, and mustard. Each crop consists of multiple
plants (p1, p2, ..., pn) captured over different days (d1, d2, ..., dm) and
categorized into five levels (L1, L2, L3, L4, L5). Each plant is captured from
24 different angles with a 15-degree gap between images. Participants are
required to perform both tasks for all four crops with these multiview images.
We proposed a Multiview Vision Transformer (MVVT) model for the GroMo challenge
and evaluated the crop-wise performance on GroMo25. MVVT reports an average MAE
of 7.74 for age prediction and an MAE of 5.52 for leaf count. The GroMo
Challenge aims to advance plant phenotyping research by encouraging innovative
solutions for tracking and predicting plant growth. The GitHub repository is
publicly available at
https://github.com/mriglab/GroMo-Plant-Growth-Modeling-with-Multiview-Images.

</details>


### [143] [An Independent Discriminant Network Towards Identification of Counterfeit Images and Videos](https://arxiv.org/pdf/2506.05377)
*Shayantani Kar, B. Shresth Bhimrajka, Aditya Kumar, Sahil Gupta, Sourav Ghosh, Subhamita Mukherjee, Shauvik Paul*

Main category: cs.CV

TL;DR: The paper addresses the challenge of detecting GAN-generated counterfeit images and videos using a discriminant network based on InceptionResNetV2, proposing a platform for forensic detection.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of manipulated images and videos online creates misleading evidence and false information, necessitating advanced detection methods.

Method: A discriminant network using a convolutional neural network (InceptionResNetV2) is developed to identify GAN-generated content.

Result: The proposed method effectively detects forged images and videos, aiding forensic analysis.

Conclusion: The work offers a practical solution for identifying counterfeit media, supporting criminal activity detection.

Abstract: Rapid spread of false images and videos on online platforms is an emerging
problem. Anyone may add, delete, clone or modify people and entities from an
image using various editing software which are readily available. This
generates false and misleading proof to hide the crime. Now-a-days, these false
and counterfeit images and videos are flooding on the internet. These spread
false information. Many methods are available in literature for detecting those
counterfeit contents but new methods of counterfeiting are also evolving.
Generative Adversarial Networks (GAN) are observed to be one effective method
as it modifies the context and definition of images producing plausible results
via image-to-image translation. This work uses an independent discriminant
network that can identify GAN generated image or video. A discriminant network
has been created using a convolutional neural network based on
InceptionResNetV2. The article also proposes a platform where users can detect
forged images and videos. This proposed work has the potential to help the
forensics domain to detect counterfeit videos and hidden criminal evidence
towards the identification of criminal activities.

</details>


### [144] [Aerial Multi-View Stereo via Adaptive Depth Range Inference and Normal Cues](https://arxiv.org/pdf/2506.05655)
*Yimei Liu, Yakun Ju, Yuan Rao, Hao Fan, Junyu Dong, Feng Gao, Qian Du*

Main category: cs.CV

TL;DR: ADR-MVS improves multi-view stereo for aerial images by using adaptive depth ranges and monocular cues, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MVS methods overlook differences in aerial vs. close-range settings, leading to inaccurate depth estimation.

Method: ADR-MVS integrates monocular geometric cues, adaptive depth range prediction, and normal-guided cost aggregation for precise depth regression.

Result: ADR-MVS achieves state-of-the-art performance on WHU, LuoJia-MVS, and München datasets with superior computational efficiency.

Conclusion: ADR-MVS effectively addresses aerial image challenges and outperforms existing techniques in accuracy and complexity.

Abstract: Three-dimensional digital urban reconstruction from multi-view aerial images
is a critical application where deep multi-view stereo (MVS) methods outperform
traditional techniques. However, existing methods commonly overlook the key
differences between aerial and close-range settings, such as varying depth
ranges along epipolar lines and insensitive feature-matching associated with
low-detailed aerial images. To address these issues, we propose an Adaptive
Depth Range MVS (ADR-MVS), which integrates monocular geometric cues to improve
multi-view depth estimation accuracy. The key component of ADR-MVS is the depth
range predictor, which generates adaptive range maps from depth and normal
estimates using cross-attention discrepancy learning. In the first stage, the
range map derived from monocular cues breaks through predefined depth
boundaries, improving feature-matching discriminability and mitigating
convergence to local optima. In later stages, the inferred range maps are
progressively narrowed, ultimately aligning with the cascaded MVS framework for
precise depth regression. Moreover, a normal-guided cost aggregation operation
is specially devised for aerial stereo images to improve geometric awareness
within the cost volume. Finally, we introduce a normal-guided depth refinement
module that surpasses existing RGB-guided techniques. Experimental results
demonstrate that ADR-MVS achieves state-of-the-art performance on the WHU,
LuoJia-MVS, and M\"unchen datasets, while exhibits superior computational
complexity.

</details>


### [145] [Can Vision Transformers with ResNet's Global Features Fairly Authenticate Demographic Faces?](https://arxiv.org/pdf/2506.05383)
*Abu Sufian, Marco Leo, Cosimo Distante, Anirudha Ghosh, Debaditya Barman*

Main category: cs.CV

TL;DR: The paper explores using Vision Transformers (ViT) and ResNet for fair biometric face authentication across demographics, testing performance with few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Addressing fairness and generalization challenges in face authentication across diverse demographic groups.

Method: Combined pre-trained ViT and ResNet features, trained on custom datasets, and designed a few-shot prototype network for testing.

Result: Microsoft Swin Transformer outperformed other ViT models, with performance improving as support set size increased.

Conclusion: The approach demonstrates potential for fair face authentication, with code and data made available for further research.

Abstract: Biometric face authentication is crucial in computer vision, but ensuring
fairness and generalization across demographic groups remains a big challenge.
Therefore, we investigated whether Vision Transformer (ViT) and ResNet,
leveraging pre-trained global features, can fairly authenticate different
demographic faces while relying minimally on local features. In this
investigation, we used three pre-trained state-of-the-art (SOTA) ViT foundation
models from Facebook, Google, and Microsoft for global features as well as
ResNet-18. We concatenated the features from ViT and ResNet, passed them
through two fully connected layers, and trained on customized face image
datasets to capture the local features. Then, we designed a novel few-shot
prototype network with backbone features embedding. We also developed new
demographic face image support and query datasets for this empirical study. The
network's testing was conducted on this dataset in one-shot, three-shot, and
five-shot scenarios to assess how performance improves as the size of the
support set increases. We observed results across datasets with varying
races/ethnicities, genders, and age groups. The Microsoft Swin Transformer
backbone performed better among the three SOTA ViT for this task. The code and
data are available at: https://github.com/Sufianlab/FairVitBio.

</details>


### [146] [Attention-based transformer models for image captioning across languages: An in-depth survey and evaluation](https://arxiv.org/pdf/2506.05399)
*Israa A. Albadarneh, Bassam H. Hammo, Omar S. Al-Kadi*

Main category: cs.CV

TL;DR: A survey on attention-based transformer models for image captioning, covering methodologies, datasets, evaluation metrics, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in comprehensive analysis of attention-based transformer models for multilingual image captioning.

Method: Reviews and categorizes models into transformer-based, deep learning-based, and hybrid approaches, and discusses datasets and metrics.

Result: Identifies limitations like semantic inconsistencies, data scarcity in non-English languages, and reasoning gaps.

Conclusion: Highlights future research directions and serves as a reference for advancing attention-based image captioning.

Abstract: Image captioning involves generating textual descriptions from input images,
bridging the gap between computer vision and natural language processing.
Recent advancements in transformer-based models have significantly improved
caption generation by leveraging attention mechanisms for better scene
understanding. While various surveys have explored deep learning-based
approaches for image captioning, few have comprehensively analyzed
attention-based transformer models across multiple languages. This survey
reviews attention-based image captioning models, categorizing them into
transformer-based, deep learning-based, and hybrid approaches. It explores
benchmark datasets, discusses evaluation metrics such as BLEU, METEOR, CIDEr,
and ROUGE, and highlights challenges in multilingual captioning. Additionally,
this paper identifies key limitations in current models, including semantic
inconsistencies, data scarcity in non-English languages, and limitations in
reasoning ability. Finally, we outline future research directions, such as
multimodal learning, real-time applications in AI-powered assistants,
healthcare, and forensic analysis. This survey serves as a comprehensive
reference for researchers aiming to advance the field of attention-based image
captioning.

</details>


### [147] [AD-EE: Early Exiting for Fast and Reliable Vision-Language Models in Autonomous Driving](https://arxiv.org/pdf/2506.05404)
*Lianming Huang, Haibo Hu, Yufei Cui, Jiacheng Zuo, Shangyu Wu, Nan Guan, Chun Jason Xue*

Main category: cs.CV

TL;DR: AD-EE, an Early Exit framework for Vision-Language Models (VLMs) in autonomous driving, reduces latency by up to 57.58% and improves object detection accuracy by up to 44%.


<details>
  <summary>Details</summary>
Motivation: High latency and computational overhead in VLMs hinder real-time autonomous driving applications, especially due to over-inference.

Method: Proposes AD-EE, an Early Exit framework using domain characteristics and causal inference to identify optimal exit layers.

Result: Tested on Waymo, CODA, and Autoware Universe, AD-EE reduces latency by up to 57.58% and improves accuracy by up to 44%.

Conclusion: AD-EE effectively addresses inefficiencies in VLMs for autonomous driving, enhancing real-time performance and accuracy.

Abstract: With the rapid advancement of autonomous driving, deploying Vision-Language
Models (VLMs) to enhance perception and decision-making has become increasingly
common. However, the real-time application of VLMs is hindered by high latency
and computational overhead, limiting their effectiveness in time-critical
driving scenarios. This challenge is particularly evident when VLMs exhibit
over-inference, continuing to process unnecessary layers even after confident
predictions have been reached. To address this inefficiency, we propose AD-EE,
an Early Exit framework that incorporates domain characteristics of autonomous
driving and leverages causal inference to identify optimal exit layers. We
evaluate our method on large-scale real-world autonomous driving datasets,
including Waymo and the corner-case-focused CODA, as well as on a real vehicle
running the Autoware Universe platform. Extensive experiments across multiple
VLMs show that our method significantly reduces latency, with maximum
improvements reaching up to 57.58%, and enhances object detection accuracy,
with maximum gains of up to 44%.

</details>


### [148] [A VLM-based Method for Visual Anomaly Detection in Robotic Scientific Laboratories](https://arxiv.org/pdf/2506.05405)
*Shiwei Lin, Chenxu Wang, Xiaozhen Ding, Yi Wang, Boyuan Du, Lei Song, Chenggang Wang, Huaping Liu*

Main category: cs.CV

TL;DR: A VLM-based visual reasoning approach for anomaly detection in scientific workflows, evaluated with progressively informative prompts, shows improved accuracy with more contextual information.


<details>
  <summary>Details</summary>
Motivation: To ensure stability and safety in robot scientific laboratories by detecting visual anomalies timely.

Method: Proposes a VLM-based visual reasoning approach with four prompt configurations for varying supervision levels, tested on a tailored benchmark.

Result: Detection accuracy improves with more contextual information; real-world validations confirm effectiveness.

Conclusion: The approach provides a data-driven foundation and evaluation framework for vision anomaly detection in scientific workflows.

Abstract: In robot scientific laboratories, visual anomaly detection is important for
the timely identification and resolution of potential faults or deviations. It
has become a key factor in ensuring the stability and safety of experimental
processes. To address this challenge, this paper proposes a VLM-based visual
reasoning approach that supports different levels of supervision through four
progressively informative prompt configurations. To systematically evaluate its
effectiveness, we construct a visual benchmark tailored for process anomaly
detection in scientific workflows. Experiments on two representative
vision-language models show that detection accuracy improves as more contextual
information is provided, confirming the effectiveness and adaptability of the
proposed reasoning approach for process anomaly detection in scientific
workflows. Furthermore, real-world validations at selected experimental steps
confirm that first-person visual observation can effectively identify
process-level anomalies. This work provides both a data-driven foundation and
an evaluation framework for vision anomaly detection in scientific experiment
workflows.

</details>


### [149] [Object-level Self-Distillation for Vision Pretraining](https://arxiv.org/pdf/2506.05409)
*Çağlar Hızlı, Çağatay Yıldız, Pekka Marttinen*

Main category: cs.CV

TL;DR: ODIS introduces object-level self-distillation for vision pretraining, improving representations by focusing on individual objects rather than whole images.


<details>
  <summary>Details</summary>
Motivation: Current methods assume single-object images, limiting scalability to complex scenes. ODIS addresses this by isolating object-specific regions.

Method: Uses object-aware cropping and masked attention to guide transformers toward meaningful content, simplifying scene-level tasks into object-level sub-tasks.

Result: Achieves 82.6% k-NN accuracy on ImageNet1k with ViT-Large, demonstrating improved visual representations.

Conclusion: ODIS enhances pretraining by shifting focus to object-level granularity, improving performance on complex datasets.

Abstract: State-of-the-art vision pretraining methods rely on image-level
self-distillation from object-centric datasets such as ImageNet, implicitly
assuming each image contains a single object. This assumption does not always
hold: many ImageNet images already contain multiple objects. Further, it limits
scalability to scene-centric datasets that better mirror real-world complexity.
We address these challenges by introducing Object-level Self-DIStillation
(ODIS), a pretraining approach that shifts the self-distillation granularity
from whole images to individual objects. Using object-aware cropping and masked
attention, ODIS isolates object-specific regions, guiding the transformer
toward semantically meaningful content and transforming a noisy, scene-level
task into simpler object-level sub-tasks. We show that this approach improves
visual representations both at the image and patch levels. Using masks at
inference time, our method achieves an impressive $82.6\%$ $k$-NN accuracy on
ImageNet1k with ViT-Large.

</details>


### [150] [Can Vision Language Models Infer Human Gaze Direction? A Controlled Study](https://arxiv.org/pdf/2506.05412)
*Zory Zhang, Pinyuan Feng, Bingyang Wang, Tianwei Zhao, Suyang Yu, Qingying Gao, Hokin Deng, Ziqiao Ma, Yijiang Li, Dezhi Luo*

Main category: cs.CV

TL;DR: Most VLMs perform poorly at gaze-referential inference, unlike humans who excel. Top-tier VLMs show some capability but are limited by task difficulty.


<details>
  <summary>Details</summary>
Motivation: To assess VLMs' ability to infer gaze direction, a key skill for natural human-AI interaction, and compare it to human performance.

Method: Evaluated 111 VLMs using manipulated photos, compared results with 65 humans, and analyzed behaviors with mixed-effects models.

Result: 94 VLMs performed no better than random guessing, while humans achieved near-ceiling accuracy. Top-tier VLMs showed limited but non-random performance.

Conclusion: VLMs lack robust gaze inference capability, limiting natural human-AI interaction, though potential exists for improvement.

Abstract: Gaze-referential inference--the ability to infer what others are looking
at--is a critical component of a theory of mind that underpins natural human-AI
interaction. In a controlled study, we evaluated this skill across 111 Vision
Language Models (VLMs) using photos taken with manipulated difficulty and
variability, comparing performance with that of human participants (N = 65),
and analyzed behaviors using mixed-effects models. We found that 94 of the 111
VLMs failed to do better than random guessing, while humans achieved
near-ceiling accuracy. VLMs even respond with each choice almost equally
frequently. Are they randomly guessing? Although most VLMs struggle, when we
zoom in on five of the top-tier VLMs with above-chance performance, we find
that their performance declined with increasing task difficulty but varied only
slightly across different prompts and scene objects. These behavioral features
cannot be explained by considering them as random guessers. Instead, they
likely use a combination of heuristics and guessing such that their performance
is subject to the task difficulty but robust to perceptual variations. This
suggests that VLMs, lacking gaze inference capability, have yet to become
technologies that can naturally interact with humans, but the potential
remains.

</details>


### [151] [Better STEP, a format and dataset for boundary representation](https://arxiv.org/pdf/2506.05417)
*Nafiseh Izadyar, Sai Chandra Madduri, Teseo Schneider*

Main category: cs.CV

TL;DR: The paper introduces an HDF5-based alternative to STEP format for CAD B-rep data, enabling easier integration into learning pipelines without costly CAD kernel licenses.


<details>
  <summary>Details</summary>
Motivation: STEP format requires CAD kernels, limiting scalability and usage in learning pipelines due to licensing costs.

Method: Developed an open-source HDF5-based format and library for processing CAD data, including sampling and curvature functionalities.

Result: Successfully converted Fusion 360 and ABC datasets, validated through four use cases (normal estimation, denoising, etc.).

Conclusion: The HDF5 format is effective for CAD data, offering scalability and compatibility with learning pipelines.

Abstract: Boundary representation (B-rep) generated from computer-aided design (CAD) is
widely used in industry, with several large datasets available. However, the
data in these datasets is represented in STEP format, requiring a CAD kernel to
read and process it. This dramatically limits their scope and usage in large
learning pipelines, as it constrains the possibility of deploying them on
computing clusters due to the high cost of per-node licenses.
  This paper introduces an alternative format based on the open, cross-platform
format HDF5 and a corresponding dataset for STEP files, paired with an
open-source library to query and process them. Our Python package also provides
standard functionalities such as sampling, normals, and curvature to ease
integration in existing pipelines.
  To demonstrate the effectiveness of our format, we converted the Fusion 360
dataset and the ABC dataset. We developed four standard use cases (normal
estimation, denoising, surface reconstruction, and segmentation) to assess the
integrity of the data and its compliance with the original STEP files.

</details>


### [152] [Self-Predictive Dynamics for Generalization of Vision-based Reinforcement Learning](https://arxiv.org/pdf/2506.05418)
*Kyungsoo Kim, Jeongsoo Ha, Yusung Kim*

Main category: cs.CV

TL;DR: SPD improves vision-based RL by extracting task-relevant features from images with distractions, using weak and strong augmentations to predict transitions.


<details>
  <summary>Details</summary>
Motivation: Handling task-irrelevant distractions (e.g., shadows, clouds) in vision-based RL, especially when unseen during training, requires robust feature extraction.

Method: SPD uses weak and strong augmentations in parallel to learn representations by predicting inverse and forward transitions between augmented versions.

Result: SPD outperforms prior methods in MuJoCo visual control and CARLA driving tasks, showing strong generalization to unseen observations.

Conclusion: SPD effectively extracts task-relevant features, enhancing generalization in vision-based RL with distractions.

Abstract: Vision-based reinforcement learning requires efficient and robust
representations of image-based observations, especially when the images contain
distracting (task-irrelevant) elements such as shadows, clouds, and light. It
becomes more important if those distractions are not exposed during training.
We design a Self-Predictive Dynamics (SPD) method to extract task-relevant
features efficiently, even in unseen observations after training. SPD uses weak
and strong augmentations in parallel, and learns representations by predicting
inverse and forward transitions across the two-way augmented versions. In a set
of MuJoCo visual control tasks and an autonomous driving task (CARLA), SPD
outperforms previous studies in complex observations, and significantly
improves the generalization performance for unseen observations. Our code is
available at https://github.com/unigary/SPD.

</details>


### [153] [Dream to Generalize: Zero-Shot Model-Based Reinforcement Learning for Unseen Visual Distractions](https://arxiv.org/pdf/2506.05419)
*Jeongsoo Ha, Kyungsoo Kim, Yusung Kim*

Main category: cs.CV

TL;DR: Dr. G is a self-supervised MBRL method using dual contrastive learning and a recurrent state inverse dynamics model to improve robustness against visual distractions, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: MBRL struggles with visual distractions in real-world scenarios, limiting its generalization.

Method: Dr. G employs dual contrastive learning for feature capture and a recurrent state inverse dynamics model for temporal understanding.

Result: Dr. G improves performance by 117% and 14% over prior works in tested environments.

Conclusion: Dr. G enhances MBRL robustness against visual distractions, demonstrating strong zero-shot generalization.

Abstract: Model-based reinforcement learning (MBRL) has been used to efficiently solve
vision-based control tasks in highdimensional image observations. Although
recent MBRL algorithms perform well in trained observations, they fail when
faced with visual distractions in observations. These task-irrelevant
distractions (e.g., clouds, shadows, and light) may be constantly present in
real-world scenarios. In this study, we propose a novel self-supervised method,
Dream to Generalize (Dr. G), for zero-shot MBRL. Dr. G trains its encoder and
world model with dual contrastive learning which efficiently captures
task-relevant features among multi-view data augmentations. We also introduce a
recurrent state inverse dynamics model that helps the world model to better
understand the temporal structure. The proposed methods can enhance the
robustness of the world model against visual distractions. To evaluate the
generalization performance, we first train Dr. G on simple backgrounds and then
test it on complex natural video backgrounds in the DeepMind Control suite, and
the randomizing environments in Robosuite. Dr. G yields a performance
improvement of 117% and 14% over prior works, respectively. Our code is
open-sourced and available at https://github.com/JeongsooHa/DrG.git

</details>


### [154] [Enhancing pretraining efficiency for medical image segmentation via transferability metrics](https://arxiv.org/pdf/2410.18677)
*Gábor Hidy, Bence Bakos, András Lukács*

Main category: cs.CV

TL;DR: Shorter pretraining on ImageNet often improves medical image segmentation performance. A novel contrastive learning-based transferability metric is introduced to optimize pretraining for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of scarce labeled data in medical image segmentation by improving pretraining efficiency and effectiveness.

Method: Examined 300+ combinations of models, datasets, and training methods. Introduced a contrastive learning-based transferability metric to optimize pretraining.

Result: Shorter pretraining often yields better downstream performance. The new metric effectively identifies optimal pretraining weights.

Conclusion: The proposed transferability metric reduces pretraining time and enhances segmentation performance, proving ImageNet accuracy is a poor downstream indicator.

Abstract: In medical image segmentation tasks, the scarcity of labeled training data
poses a significant challenge when training deep neural networks. When using
U-Net-style architectures, it is common practice to address this problem by
pretraining the encoder part on a large general-purpose dataset like ImageNet.
However, these methods are resource-intensive and do not guarantee improved
performance on the downstream task. In this paper we investigate a variety of
training setups on medical image segmentation datasets, using
ImageNet-pretrained models. By examining over 300 combinations of models,
datasets, and training methods, we find that shorter pretraining often leads to
better results on the downstream task, providing additional proof to the
well-known fact that the accuracy of the model on ImageNet is a poor indicator
for downstream performance. As our main contribution, we introduce a novel
transferability metric, based on contrastive learning, that measures how
robustly a pretrained model is able to represent the target data. In contrast
to other transferability scores, our method is applicable to the case of
transferring from ImageNet classification to medical image segmentation. We
apply our robustness score by measuring it throughout the pretraining phase to
indicate when the model weights are optimal for downstream transfer. This
reduces pretraining time and improves results on the target task.

</details>


### [155] [Self-supervised One-Stage Learning for RF-based Multi-Person Pose Estimation](https://arxiv.org/pdf/2506.05420)
*Seunghwan Shin, Yusung Kim*

Main category: cs.CV

TL;DR: A lightweight one-stage MPPE model using raw RF signals with sub-grouping and multi-head attention outperforms previous methods, achieving up to 15% higher accuracy. A novel SSL method further enhances performance in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing RF-based MPPE methods are either computationally intensive or lack accuracy. This paper aims to improve efficiency and performance by leveraging raw RF signals with a simpler, more effective approach.

Method: The model sub-groups RF signals, processes them with a shared single-layer CNN and multi-head attention, and introduces a self-supervised learning method for masked data prediction.

Result: Empirical results show a 15% improvement in MPPE accuracy (PCKh@0.5) over previous methods, with significant gains in new locations or obstructed scenarios.

Conclusion: The proposed model and SSL method offer a lightweight, accurate solution for RF-based MPPE, with open-source code and dataset available.

Abstract: In the field of Multi-Person Pose Estimation (MPPE), Radio Frequency
(RF)-based methods can operate effectively regardless of lighting conditions
and obscured line-of-sight situations. Existing RF-based MPPE methods typically
involve either 1) converting RF signals into heatmap images through complex
preprocessing, or 2) applying a deep embedding network directly to raw RF
signals. The first approach, while delivering decent performance, is
computationally intensive and time-consuming. The second method, though simpler
in preprocessing, results in lower MPPE accuracy and generalization
performance. This paper proposes an efficient and lightweight one-stage MPPE
model based on raw RF signals. By sub-grouping RF signals and embedding them
using a shared single-layer CNN followed by multi-head attention, this model
outperforms previous methods that embed all signals at once through a large and
deep CNN. Additionally, we propose a new self-supervised learning (SSL) method
that takes inputs from both one unmasked subgroup and the remaining masked
subgroups to predict the latent representations of the masked data. Empirical
results demonstrate that our model improves MPPE accuracy by up to 15 in
PCKh@0.5 compared to previous methods using raw RF signals. Especially, the
proposed SSL method has shown to significantly enhance performance improvements
when placed in new locations or in front of obstacles at RF antennas,
contributing to greater performance gains as the number of people increases.
Our code and dataset is open at Github. https://github.com/sshnan7/SOSPE .

</details>


### [156] [Fréchet Radiomic Distance (FRD): A Versatile Metric for Comparing Medical Imaging Datasets](https://arxiv.org/pdf/2412.01496)
*Nicholas Konz, Richard Osuala, Preeti Verma, Yuwen Chen, Hanxue Gu, Haoyu Dong, Yaqian Chen, Andrew Marshall, Lidia Garrucho, Kaisar Kushibar, Daniel M. Lang, Gene S. Kim, Lars J. Grimm, John M. Lewin, James S. Duncan, Julia A. Schnabel, Oliver Diaz, Karim Lekadir, Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: The paper introduces FRD (Fréchet Radiomic Distance), a perceptual metric for medical images, outperforming existing metrics in tasks like OOD detection and image generation evaluation.


<details>
  <summary>Details</summary>
Motivation: Current metrics for comparing medical image distributions are either task-biased or insufficient for anatomical features, necessitating a specialized solution.

Method: FRD uses standardized, clinically meaningful features to evaluate image distributions, tested across various medical imaging applications.

Result: FRD excels in OOD detection, image translation evaluation, and generation tasks, offering stability, interpretability, and correlation with radiologist assessments.

Conclusion: FRD is a robust, interpretable metric for medical image analysis, supported by extensive experiments and an open codebase for future research.

Abstract: Determining whether two sets of images belong to the same or different
distributions or domains is a crucial task in modern medical image analysis and
deep learning; for example, to evaluate the output quality of image generative
models. Currently, metrics used for this task either rely on the (potentially
biased) choice of some downstream task, such as segmentation, or adopt
task-independent perceptual metrics (e.g., Fr\'echet Inception Distance/FID)
from natural imaging, which we show insufficiently capture anatomical features.
To this end, we introduce a new perceptual metric tailored for medical images,
FRD (Fr\'echet Radiomic Distance), which utilizes standardized, clinically
meaningful, and interpretable image features. We show that FRD is superior to
other image distribution metrics for a range of medical imaging applications,
including out-of-domain (OOD) detection, the evaluation of image-to-image
translation (by correlating more with downstream task performance as well as
anatomical consistency and realism), and the evaluation of unconditional image
generation. Moreover, FRD offers additional benefits such as stability and
computational efficiency at low sample sizes, sensitivity to image corruptions
and adversarial attacks, feature interpretability, and correlation with
radiologist-perceived image quality. Additionally, we address key gaps in the
literature by presenting an extensive framework for the multifaceted evaluation
of image similarity metrics in medical imaging -- including the first
large-scale comparative study of generative models for medical image
translation -- and release an accessible codebase to facilitate future
research. Our results are supported by thorough experiments spanning a variety
of datasets, modalities, and downstream tasks, highlighting the broad potential
of FRD for medical image analysis.

</details>


### [157] [SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning](https://arxiv.org/pdf/2506.05425)
*Fanqi Kong, Weiqin Zu, Xinyu Chen, Yaodong Yang, Song-Chun Zhu, Xue Feng*

Main category: cs.CV

TL;DR: SIV-Bench is a new video benchmark for evaluating Multimodal Large Language Models (MLLMs) in social scene understanding, reasoning, and dynamics prediction, revealing strengths and limitations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of human social interaction complexity for AI, by evaluating MLLMs' capabilities in social contexts.

Method: Introduces SIV-Bench with 2,792 video clips and 8,792 QA pairs, collected from TikTok and YouTube, covering diverse genres and cultural backgrounds.

Result: MLLMs excel in Social Scene Understanding but struggle with Social State Reasoning and Dynamics Prediction, especially Relation Inference.

Conclusion: SIV-Bench highlights the need for improved social intelligence in AI, emphasizing the role of dialogue in comprehension.

Abstract: The rich and multifaceted nature of human social interaction, encompassing
multimodal cues, unobservable relations and mental states, and dynamical
behavior, presents a formidable challenge for artificial intelligence. To
advance research in this area, we introduce SIV-Bench, a novel video benchmark
for rigorously evaluating the capabilities of Multimodal Large Language Models
(MLLMs) across Social Scene Understanding (SSU), Social State Reasoning (SSR),
and Social Dynamics Prediction (SDP). SIV-Bench features 2,792 video clips and
8,792 meticulously generated question-answer pairs derived from a human-LLM
collaborative pipeline. It is originally collected from TikTok and YouTube,
covering a wide range of video genres, presentation styles, and linguistic and
cultural backgrounds. It also includes a dedicated setup for analyzing the
impact of different textual cues-original on-screen text, added dialogue, or no
text. Our comprehensive experiments on leading MLLMs reveal that while models
adeptly handle SSU, they significantly struggle with SSR and SDP, where
Relation Inference (RI) is an acute bottleneck, as further examined in our
analysis. Our study also confirms the critical role of transcribed dialogue in
aiding comprehension of complex social interactions. By systematically
identifying current MLLMs' strengths and limitations, SIV-Bench offers crucial
insights to steer the development of more socially intelligent AI. The dataset
and code are available at https://kfq20.github.io/sivbench/.

</details>


### [158] [Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum Learning Approach](https://arxiv.org/pdf/2502.01940)
*Mohammed Alsakabi, Aidan Erickson, John M. Dolan, Ozan K. Tonguz*

Main category: cs.CV

TL;DR: A cost-effective method integrates DNN 4D radar and RGB images for denser depth maps in AVs, using a novel pixel positional encoding algorithm. It outperforms SOTA by 27.95% in UCD.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional radar detectors in complex vehicular environments by leveraging high-resolution camera images.

Method: Introduces a pixel positional encoding algorithm, transforms radar and RGB images into a unified subspace (Spatial Spectrum), and develops tailored spectrum estimation algorithms and a training framework.

Result: Outperforms SOTA by 27.95% in Unidirectional Chamfer Distance (UCD).

Conclusion: The approach effectively sharpens radar output and enhances depth map generation for AVs.

Abstract: We present a cost-effective new approach for generating denser depth maps for
Autonomous Driving (AD) and Autonomous Vehicles (AVs) by integrating the images
obtained from deep neural network (DNN) 4D radar detectors with conventional
camera RGB images. Our approach introduces a novel pixel positional encoding
algorithm inspired by Bartlett's spatial spectrum estimation technique. This
algorithm transforms both radar depth maps and RGB images into a unified pixel
image subspace called the Spatial Spectrum, facilitating effective learning
based on their similarities and differences. Our method effectively leverages
high-resolution camera images to train radar depth map generative models,
addressing the limitations of conventional radar detectors in complex vehicular
environments, thus sharpening the radar output. We develop spectrum estimation
algorithms tailored for radar depth maps and RGB images, a comprehensive
training framework for data-driven generative models, and a camera-radar
deployment scheme for AV operation. Our results demonstrate that our approach
also outperforms the state-of-the-art (SOTA) by 27.95% in terms of
Unidirectional Chamfer Distance (UCD).

</details>


### [159] [Coordinated Robustness Evaluation Framework for Vision-Language Models](https://arxiv.org/pdf/2506.05429)
*Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, Soumyendu Sarkar*

Main category: cs.CV

TL;DR: A surrogate model generates adversarial perturbations for vision-language models, outperforming existing attacks and compromising robustness.


<details>
  <summary>Details</summary>
Motivation: Vision-language models lack robustness against perturbations, requiring evaluation of inter-modal dependencies.

Method: Train a surrogate model to generate joint representations and coordinated adversarial attacks for text and image modalities.

Result: The strategy outperforms multi-modal and single-modality attacks, compromising models like instruct-BLIP and ViLT.

Conclusion: The proposed approach effectively evaluates and challenges the robustness of vision-language models.

Abstract: Vision-language models, which integrate computer vision and natural language
processing capabilities, have demonstrated significant advancements in tasks
such as image captioning and visual question and answering. However, similar to
traditional models, they are susceptible to small perturbations, posing a
challenge to their robustness, particularly in deployment scenarios. Evaluating
the robustness of these models requires perturbations in both the vision and
language modalities to learn their inter-modal dependencies. In this work, we
train a generic surrogate model that can take both image and text as input and
generate joint representation which is further used to generate adversarial
perturbations for both the text and image modalities. This coordinated attack
strategy is evaluated on the visual question and answering and visual reasoning
datasets using various state-of-the-art vision-language models. Our results
indicate that the proposed strategy outperforms other multi-modal attacks and
single-modality attacks from the recent literature. Our results demonstrate
their effectiveness in compromising the robustness of several state-of-the-art
pre-trained multi-modal models such as instruct-BLIP, ViLT and others.

</details>


### [160] [Category Query Learning for Human-Object Interaction Classification](https://arxiv.org/pdf/2303.14005)
*Chi Xie, Fangao Zeng, Yue Hu, Shuang Liang, Yichen Wei*

Main category: cs.CV

TL;DR: Proposes category query learning for human-object interaction (HOI) classification, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of HOI classification by learning explicit category queries, inspired by multi-label image classification.

Method: Uses category queries converted to image-specific representations via a transformer decoder, trained with an auxiliary classification task.

Result: Achieves state-of-the-art performance on two benchmarks when integrated with three HOI baselines.

Conclusion: The method is simple, general, and effective for HOI classification.

Abstract: Unlike most previous HOI methods that focus on learning better human-object
features, we propose a novel and complementary approach called category query
learning. Such queries are explicitly associated to interaction categories,
converted to image specific category representation via a transformer decoder,
and learnt via an auxiliary image-level classification task. This idea is
motivated by an earlier multi-label image classification method, but is for the
first time applied for the challenging human-object interaction classification
task. Our method is simple, general and effective. It is validated on three
representative HOI baselines and achieves new state-of-the-art results on two
benchmarks.

</details>


### [161] [Application of convolutional neural networks in image super-resolution](https://arxiv.org/pdf/2506.02604)
*Chunwei Tian, Mingjian Song, Wangmeng Zuo, Bo Du, Yanning Zhang, Shichao Zhang*

Main category: cs.CV

TL;DR: The paper summarizes and compares CNN-based interpolation methods for image super-resolution, highlighting their differences, performance, and potential research directions.


<details>
  <summary>Details</summary>
Motivation: There is a lack of literature summarizing the relations and differences among CNN-based methods for image super-resolution, making such a review crucial for understanding their performance and applicability.

Method: The paper reviews CNN principles and compares various interpolation methods (bicubic, nearest neighbor, bilinear, transposed convolution, sub-pixel layer, meta up-sampling) through experiments.

Result: The study provides a performance comparison of different CNN-based interpolation methods, identifying their strengths and weaknesses.

Conclusion: The paper concludes with potential research points and drawbacks, aiding future developments in CNN-based image super-resolution.

Abstract: Due to strong learning abilities of convolutional neural networks (CNNs),
they have become mainstream methods for image super-resolution. However, there
are big differences of different deep learning methods with different types.
There is little literature to summarize relations and differences of different
methods in image super-resolution. Thus, summarizing these literatures are
important, according to loading capacity and execution speed of devices. This
paper first introduces principles of CNNs in image super-resolution, then
introduces CNNs based bicubic interpolation, nearest neighbor interpolation,
bilinear interpolation, transposed convolution, sub-pixel layer, meta
up-sampling for image super-resolution to analyze differences and relations of
different CNNs based interpolations and modules, and compare performance of
these methods by experiments. Finally, this paper gives potential research
points and drawbacks and summarizes the whole paper, which can facilitate
developments of CNNs in image super-resolution.

</details>


### [162] [Robustness Evaluation for Video Models with Reinforcement Learning](https://arxiv.org/pdf/2506.05431)
*Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, Soumyendu Sarkar*

Main category: cs.CV

TL;DR: A multi-agent reinforcement learning approach is proposed to evaluate video classification model robustness by generating fine, imperceptible perturbations in sensitive spatial and temporal regions.


<details>
  <summary>Details</summary>
Motivation: Evaluating video classification model robustness is complex due to the temporal dimension, requiring minimal perturbations for misclassification.

Method: Uses multi-agent reinforcement learning (spatial and temporal) to cooperatively identify and perturb sensitive regions while maintaining temporal coherence.

Result: Outperforms state-of-the-art on Lp metric and average queries, with custom distortion types for relevant robustness evaluation.

Conclusion: The method effectively evaluates robustness on popular video action recognition models and datasets (HMDB-51, UCF-101).

Abstract: Evaluating the robustness of Video classification models is very challenging,
specifically when compared to image-based models. With their increased temporal
dimension, there is a significant increase in complexity and computational
cost. One of the key challenges is to keep the perturbations to a minimum to
induce misclassification. In this work, we propose a multi-agent reinforcement
learning approach (spatial and temporal) that cooperatively learns to identify
the given video's sensitive spatial and temporal regions. The agents consider
temporal coherence in generating fine perturbations, leading to a more
effective and visually imperceptible attack. Our method outperforms the
state-of-the-art solutions on the Lp metric and the average queries. Our method
enables custom distortion types, making the robustness evaluation more relevant
to the use case. We extensively evaluate 4 popular models for video action
recognition on two popular datasets, HMDB-51 and UCF-101.

</details>


### [163] [LLMs Can Compensate for Deficiencies in Visual Representations](https://arxiv.org/pdf/2506.05439)
*Sho Takishita, Jay Gala, Abdelrahman Mohamed, Kentaro Inui, Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: CLIP-based VLMs compensate for weak visual features with strong language backbones, enabling performance recovery in reduced visual contextualization scenarios.


<details>
  <summary>Details</summary>
Motivation: To understand if the language backbone in VLMs compensates for limitations in CLIP-based vision encoders.

Method: Controlled self-attention ablations on three CLIP-based VLMs using a probing task.

Result: CLIP visual representations provide semantic info, but language decoders can compensate for visual deficiencies.

Conclusion: VLMs dynamically balance visual and language processing, suggesting future architectures can leverage language decoders more.

Abstract: Many vision-language models (VLMs) that prove very effective at a range of
multimodal task, build on CLIP-based vision encoders, which are known to have
various limitations. We investigate the hypothesis that the strong language
backbone in VLMs compensates for possibly weak visual features by
contextualizing or enriching them. Using three CLIP-based VLMs, we perform
controlled self-attention ablations on a carefully designed probing task. Our
findings show that despite known limitations, CLIP visual representations offer
ready-to-read semantic information to the language decoder. However, in
scenarios of reduced contextualization in the visual representations, the
language decoder can largely compensate for the deficiency and recover
performance. This suggests a dynamic division of labor in VLMs and motivates
future architectures that offload more visual processing to the language
decoder.

</details>


### [164] [BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models](https://arxiv.org/pdf/2506.05440)
*Ludovic Arnould, Salim Khazem, Hugues Ali Mehenni*

Main category: cs.CV

TL;DR: The paper introduces a new evaluation method for Visual Language Models (VLMs) using synthetic images to systematically test and analyze perception failures, moving beyond traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VLM benchmarks are costly, prone to information leakage, and fail to pinpoint whether model errors arise from perception, reasoning, or general knowledge gaps.

Method: The proposed method uses procedurally generated synthetic images with controlled variations to isolate and test specific visual perception capabilities in VLMs.

Result: The approach enables fine-grained failure analysis and targeted assessment of VLM capabilities, providing clearer insights into model limitations.

Conclusion: The synthetic image-based diagnostic method offers a more interpretable and systematic way to evaluate VLMs, addressing shortcomings of traditional benchmarks.

Abstract: Visual Language Models (VLMs) are now sufficiently advanced to support a
broad range of applications, including answering complex visual questions, and
are increasingly expected to interact with images in varied ways. To evaluate
them, current benchmarks often focus on specific domains (e.g., reading
charts), constructing datasets of annotated real images paired with pre-defined
Multiple Choice Questions (MCQs) to report aggregate accuracy scores. However,
such benchmarks entail high annotation costs, risk information leakage, and do
not clarify whether failures stem from limitations in visual perception,
reasoning, or general knowledge. We propose a new evaluation methodology,
inspired by ophthalmologic diagnostics, leveraging procedural generation of
synthetic images to obtain control over visual attributes and precisely reveal
perception failures in VLMs. Specifically, we build collections of images with
gradually more challenging variations in the content of interest (e.g., number
of objects in a counting task) while holding other visual parameters constant.
This diagnostic allows systematic stress testing and fine-grained failure
analysis, shifting the focus from coarse benchmarking toward targeted and
interpretable assessment of VLM capabilities. Our code is available at
https://github.com/byoeval/BYO-EVAL.

</details>


### [165] [Structured Labeling Enables Faster Vision-Language Models for End-to-End Autonomous Driving](https://arxiv.org/pdf/2506.05442)
*Hao Jiang, Chuan Hu, Yukang Shi, Yuan He, Ke Wang, Xi Zhang, Zhipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces NuScenes-S, a structured dataset, and FastDrive, a compact VLM, to address gaps in current VLMs for autonomous driving, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current VLMs for autonomous driving suffer from loosely formatted datasets and high computational costs, limiting real-world deployment.

Method: Developed NuScenes-S, a structured dataset, and FastDrive, a compact VLM with 0.9B parameters, for efficient and machine-friendly autonomous driving.

Result: FastDrive improves decision-making accuracy by 20% and achieves a 10x speedup in inference compared to larger VLMs.

Conclusion: Structured datasets and compact VLMs like FastDrive enhance autonomous driving performance and efficiency, with scene annotations proving critical for decision-making.

Abstract: Vision-Language Models (VLMs) offer a promising approach to end-to-end
autonomous driving due to their human-like reasoning capabilities. However,
troublesome gaps remains between current VLMs and real-world autonomous driving
applications. One major limitation is that existing datasets with loosely
formatted language descriptions are not machine-friendly and may introduce
redundancy. Additionally, high computational cost and massive scale of VLMs
hinder the inference speed and real-world deployment. To bridge the gap, this
paper introduces a structured and concise benchmark dataset, NuScenes-S, which
is derived from the NuScenes dataset and contains machine-friendly structured
representations. Moreover, we present FastDrive, a compact VLM baseline with
0.9B parameters. In contrast to existing VLMs with over 7B parameters and
unstructured language processing(e.g., LLaVA-1.5), FastDrive understands
structured and concise descriptions and generates machine-friendly driving
decisions with high efficiency. Extensive experiments show that FastDrive
achieves competitive performance on structured dataset, with approximately 20%
accuracy improvement on decision-making tasks, while surpassing massive
parameter baseline in inference speed with over 10x speedup. Additionally,
ablation studies further focus on the impact of scene annotations (e.g.,
weather, time of day) on decision-making tasks, demonstrating their importance
on decision-making tasks in autonomous driving.

</details>


### [166] [Degradation-Aware Image Enhancement via Vision-Language Classification](https://arxiv.org/pdf/2506.05450)
*Jie Cai, Kangning Yang, Jiaming Ding, Lan Fu, Ling Ouyang, Jiang Li, Jinglin Shen, Zibo Meng*

Main category: cs.CV

TL;DR: A novel framework uses a Vision-Language Model (VLM) to classify degraded images into four categories and applies targeted restoration for improved visual quality.


<details>
  <summary>Details</summary>
Motivation: Image degradation affects visual quality and downstream tasks, necessitating automated solutions.

Method: The VLM classifies images into degradation types (A-D), followed by targeted restoration using specialized models.

Result: The approach accurately classifies degradations and enhances image quality via tailored restoration.

Conclusion: The method offers a scalable, automated solution for real-world image enhancement using VLMs and advanced restoration techniques.

Abstract: Image degradation is a prevalent issue in various real-world applications,
affecting visual quality and downstream processing tasks. In this study, we
propose a novel framework that employs a Vision-Language Model (VLM) to
automatically classify degraded images into predefined categories. The VLM
categorizes an input image into one of four degradation types: (A)
super-resolution degradation (including noise, blur, and JPEG compression), (B)
reflection artifacts, (C) motion blur, or (D) no visible degradation
(high-quality image). Once classified, images assigned to categories A, B, or C
undergo targeted restoration using dedicated models tailored for each specific
degradation type. The final output is a restored image with improved visual
quality. Experimental results demonstrate the effectiveness of our approach in
accurately classifying image degradations and enhancing image quality through
specialized restoration models. Our method presents a scalable and automated
solution for real-world image enhancement tasks, leveraging the capabilities of
VLMs in conjunction with state-of-the-art restoration techniques.

</details>


### [167] [Towards Reliable Identification of Diffusion-based Image Manipulations](https://arxiv.org/pdf/2506.05466)
*Alex Costanzino, Woody Bayliss, Juil Sock, Marc Gorriz Blanch, Danijela Horak, Ivan Laptev, Philip Torr, Fabio Pizzati*

Main category: cs.CV

TL;DR: RADAR is a novel method for detecting and localizing image manipulations, especially those made by diffusion models, using multimodal features and contrastive loss. It outperforms existing methods and is evaluated on a new benchmark, BBC-PAIR.


<details>
  <summary>Details</summary>
Motivation: The rise of high-quality image manipulation via diffusion models necessitates reliable detection tools to combat misuse.

Method: RADAR combines features from different image modalities and uses an auxiliary contrastive loss to isolate manipulated patches.

Result: RADAR achieves superior accuracy and generalizes well across 28 diffusion models, outperforming state-of-the-art methods.

Conclusion: RADAR is an effective solution for detecting diffusion-based image edits, with code and data made publicly available.

Abstract: Changing facial expressions, gestures, or background details may dramatically
alter the meaning conveyed by an image. Notably, recent advances in diffusion
models greatly improve the quality of image manipulation while also opening the
door to misuse. Identifying changes made to authentic images, thus, becomes an
important task, constantly challenged by new diffusion-based editing tools. To
this end, we propose a novel approach for ReliAble iDentification of inpainted
AReas (RADAR). RADAR builds on existing foundation models and combines features
from different image modalities. It also incorporates an auxiliary contrastive
loss that helps to isolate manipulated image patches. We demonstrate these
techniques to significantly improve both the accuracy of our method and its
generalisation to a large number of diffusion models. To support realistic
evaluation, we further introduce BBC-PAIR, a new comprehensive benchmark, with
images tampered by 28 diffusion models. Our experiments show that RADAR
achieves excellent results, outperforming the state-of-the-art in detecting and
localising image edits made by both seen and unseen diffusion models. Our code,
data and models will be publicly available at alex-costanzino.github.io/radar.

</details>


### [168] [S2GO: Streaming Sparse Gaussian Occupancy Prediction](https://arxiv.org/pdf/2506.05473)
*Jinhyung Park, Yihan Hu, Chensheng Peng, Wenzhao Zheng, Kris Kitani, Wei Zhan*

Main category: cs.CV

TL;DR: S2GO introduces a query-based 3D representation for occupancy prediction, outperforming dense methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Dense voxel or Gaussian representations are slow and inflexible for dynamic driving scenes.

Method: Uses compact 3D queries propagated over time, decoded into semantic Gaussians, with a denoising rendering objective.

Result: Achieves state-of-the-art performance on nuScenes and KITTI, with 1.5 IoU improvement and 5.9x faster inference.

Conclusion: Query-based representations are efficient and effective for 3D occupancy prediction in dynamic scenes.

Abstract: Despite the demonstrated efficiency and performance of sparse query-based
representations for perception, state-of-the-art 3D occupancy prediction
methods still rely on voxel-based or dense Gaussian-based 3D representations.
However, dense representations are slow, and they lack flexibility in capturing
the temporal dynamics of driving scenes. Distinct from prior work, we instead
summarize the scene into a compact set of 3D queries which are propagated
through time in an online, streaming fashion. These queries are then decoded
into semantic Gaussians at each timestep. We couple our framework with a
denoising rendering objective to guide the queries and their constituent
Gaussians in effectively capturing scene geometry. Owing to its efficient,
query-based representation, S2GO achieves state-of-the-art performance on the
nuScenes and KITTI occupancy benchmarks, outperforming prior art (e.g.,
GaussianWorld) by 1.5 IoU with 5.9x faster inference.

</details>


### [169] [OpenRR-5k: A Large-Scale Benchmark for Reflection Removal in the Wild](https://arxiv.org/pdf/2506.05482)
*Jie Cai, Kangning Yang, Ling Ouyang, Lan Fu, Jiaming Ding, Jinglin Shen, Zibo Meng*

Main category: cs.CV

TL;DR: The paper introduces a large-scale dataset for Single Image Reflection Removal (SIRR), featuring 5,300 pixel-aligned image pairs, and validates its utility with a U-Net model and five metrics.


<details>
  <summary>Details</summary>
Motivation: Existing reflection removal methods lack large-scale, high-quality datasets, limiting their effectiveness.

Method: A novel benchmark dataset (5,300 images) is created, divided into training, validation, and test sets. A U-Net model is trained and evaluated using five metrics.

Result: The dataset is validated with a U-Net model, showing practical performance on real-world images.

Conclusion: The dataset and code are released to advance research in reflection removal.

Abstract: Removing reflections is a crucial task in computer vision, with significant
applications in photography and image enhancement. Nevertheless, existing
methods are constrained by the absence of large-scale, high-quality, and
diverse datasets. In this paper, we present a novel benchmark for Single Image
Reflection Removal (SIRR). We have developed a large-scale dataset containing
5,300 high-quality, pixel-aligned image pairs, each consisting of a reflection
image and its corresponding clean version. Specifically, the dataset is divided
into two parts: 5,000 images are used for training, and 300 images are used for
validation. Additionally, we have included 100 real-world testing images
without ground truth (GT) to further evaluate the practical performance of
reflection removal methods. All image pairs are precisely aligned at the pixel
level to guarantee accurate supervision. The dataset encompasses a broad
spectrum of real-world scenarios, featuring various lighting conditions, object
types, and reflection patterns, and is segmented into training, validation, and
test sets to facilitate thorough evaluation. To validate the usefulness of our
dataset, we train a U-Net-based model and evaluate it using five widely-used
metrics, including PSNR, SSIM, LPIPS, DISTS, and NIQE. We will release both the
dataset and the code on https://github.com/caijie0620/OpenRR-5k to facilitate
future research in this field.

</details>


### [170] [A Neural Network Model of Spatial and Feature-Based Attention](https://arxiv.org/pdf/2506.05487)
*Ruoyang Hu, Robert A. Jacobs*

Main category: cs.CV

TL;DR: A neural network model mimicking human visual attention shows emergent patterns similar to spatial and feature-based attention, suggesting its potential for studying human cognition.


<details>
  <summary>Details</summary>
Motivation: To explore how top-down information influences visual processing and to design a neural network model inspired by human visual attention.

Method: The model consists of two networks: one for basic task processing and another for contextual guidance via attention. Training and visualization of attention responses were conducted.

Result: The model's attention patterns resembled human spatial and feature-based attention, indicating a parallel between human and artificial attention mechanisms.

Conclusion: The similarity suggests neural network models can be a valuable tool for studying human cognition, particularly visual attention.

Abstract: Visual attention is a mechanism closely intertwined with vision and memory.
Top-down information influences visual processing through attention. We
designed a neural network model inspired by aspects of human visual attention.
This model consists of two networks: one serves as a basic processor performing
a simple task, while the other processes contextual information and guides the
first network through attention to adapt to more complex tasks. After training
the model and visualizing the learned attention response, we discovered that
the model's emergent attention patterns corresponded to spatial and
feature-based attention. This similarity between human visual attention and
attention in computer vision suggests a promising direction for studying human
cognition using neural network models.

</details>


### [171] [Implicit Neural Representation for Video Restoration](https://arxiv.org/pdf/2506.05488)
*Mary Aiyetigbo, Wanqi Yuan, Feng Luo, Nianyi Li*

Main category: cs.CV

TL;DR: VR-INR is a video restoration method using Implicit Neural Representations, trained on a single upscaling factor but generalizing to arbitrary scales and zero-shot denoising.


<details>
  <summary>Details</summary>
Motivation: Existing video restoration methods lack flexibility for unseen scales or degradations, limiting their practical use.

Method: VR-INR uses hierarchical spatial-temporal-texture encoding and multi-resolution implicit hash encoding for adaptive decoding.

Result: VR-INR outperforms state-of-the-art methods in handling unseen scales and noise, with high-quality reconstructions.

Conclusion: VR-INR offers a flexible and effective solution for video restoration, generalizing beyond its training distribution.

Abstract: High-resolution (HR) videos play a crucial role in many computer vision
applications. Although existing video restoration (VR) methods can
significantly enhance video quality by exploiting temporal information across
video frames, they are typically trained for fixed upscaling factors and lack
the flexibility to handle scales or degradations beyond their training
distribution. In this paper, we introduce VR-INR, a novel video restoration
approach based on Implicit Neural Representations (INRs) that is trained only
on a single upscaling factor ($\times 4$) but generalizes effectively to
arbitrary, unseen super-resolution scales at test time. Notably, VR-INR also
performs zero-shot denoising on noisy input, despite never having seen noisy
data during training. Our method employs a hierarchical
spatial-temporal-texture encoding framework coupled with multi-resolution
implicit hash encoding, enabling adaptive decoding of high-resolution and
noise-suppressed frames from low-resolution inputs at any desired
magnification. Experimental results show that VR-INR consistently maintains
high-quality reconstructions at unseen scales and noise during training,
significantly outperforming state-of-the-art approaches in sharpness, detail
preservation, and denoising efficacy.

</details>


### [172] [F2T2-HiT: A U-Shaped FFT Transformer and Hierarchical Transformer for Reflection Removal](https://arxiv.org/pdf/2506.05489)
*Jie Cai, Kangning Yang, Ling Ouyang, Lan Fu, Jiaming Ding, Huiming Sun, Chiu Man Ho, Zibo Meng*

Main category: cs.CV

TL;DR: The paper introduces F2T2-HiT, a Transformer-based architecture for Single Image Reflection Removal (SIRR), combining FFT and Hierarchical Transformer blocks in a UNet framework to handle complex reflections.


<details>
  <summary>Details</summary>
Motivation: Reflections in images degrade quality, and existing methods struggle with varied reflection patterns. The paper aims to address this challenge.

Method: Proposes F2T2-HiT, integrating FFT Transformer blocks for global frequency analysis and Hierarchical Transformer blocks for multi-scale feature extraction within a UNet.

Result: Achieves state-of-the-art performance on three public datasets, demonstrating effectiveness in reflection removal.

Conclusion: F2T2-HiT is a robust solution for SIRR, handling diverse reflection patterns effectively.

Abstract: Single Image Reflection Removal (SIRR) technique plays a crucial role in
image processing by eliminating unwanted reflections from the background. These
reflections, often caused by photographs taken through glass surfaces, can
significantly degrade image quality. SIRR remains a challenging problem due to
the complex and varied reflections encountered in real-world scenarios. These
reflections vary significantly in intensity, shapes, light sources, sizes, and
coverage areas across the image, posing challenges for most existing methods to
effectively handle all cases. To address these challenges, this paper
introduces a U-shaped Fast Fourier Transform Transformer and Hierarchical
Transformer (F2T2-HiT) architecture, an innovative Transformer-based design for
SIRR. Our approach uniquely combines Fast Fourier Transform (FFT) Transformer
blocks and Hierarchical Transformer blocks within a UNet framework. The FFT
Transformer blocks leverage the global frequency domain information to
effectively capture and separate reflection patterns, while the Hierarchical
Transformer blocks utilize multi-scale feature extraction to handle reflections
of varying sizes and complexities. Extensive experiments conducted on three
publicly available testing datasets demonstrate state-of-the-art performance,
validating the effectiveness of our approach.

</details>


### [173] [FocusDiff: Advancing Fine-Grained Text-Image Alignment for Autoregressive Visual Generation through RL](https://arxiv.org/pdf/2506.05501)
*Kaihang Pan, Wendong Bu, Yuruo Wu, Yang Wu, Kai Shen, Yunfei Li, Hang Zhao, Juncheng Li, Siliang Tang, Yueting Zhuang*

Main category: cs.CV

TL;DR: FocusDiff improves fine-grained text-image alignment in autoregressive text-to-image generation by emphasizing subtle semantic differences between similar pairs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with fine-grained text-image alignment, failing precise control over visual tokens, as revealed by the PairComp benchmark.

Method: Proposes FocusDiff, using a new dataset of paired texts/images with similar expressions but distinct local semantics, and a reinforcement learning algorithm to highlight fine-grained differences.

Result: Achieves state-of-the-art performance on text-to-image benchmarks and significantly outperforms prior methods on PairComp.

Conclusion: FocusDiff effectively addresses fine-grained alignment issues, enabling more precise control in text-to-image generation.

Abstract: Recent studies extend the autoregression paradigm to text-to-image
generation, achieving performance comparable to diffusion models. However, our
new PairComp benchmark -- featuring test cases of paired prompts with similar
syntax but different fine-grained semantics -- reveals that existing models
struggle with fine-grained text-image alignment thus failing to realize precise
control over visual tokens. To address this, we propose FocusDiff, which
enhances fine-grained text-image semantic alignment by focusing on subtle
differences between similar text-image pairs. We construct a new dataset of
paired texts and images with similar overall expressions but distinct local
semantics, further introducing a novel reinforcement learning algorithm to
emphasize such fine-grained semantic differences for desired image generation.
Our approach achieves state-of-the-art performance on existing text-to-image
benchmarks and significantly outperforms prior methods on PairComp.

</details>


### [174] [MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning](https://arxiv.org/pdf/2506.05523)
*Zikui Cai, Andrew Wang, Anirudh Satheesh, Ankit Nakhawa, Hyunwoo Jae, Keenan Powell, Minghui Liu, Neel Jay, Sungbin Oh, Xiyao Wang, Yongyuan Liang, Tom Goldstein, Furong Huang*

Main category: cs.CV

TL;DR: MORSE-500 is a new video benchmark addressing limitations in current multimodal reasoning benchmarks by focusing on temporal complexity, diverse reasoning skills, and scalability.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack temporal dynamics, narrow focus on math, and saturate quickly, limiting progress in multimodal reasoning research.

Method: MORSE-500 includes 500 scripted video clips with questions across six reasoning categories, generated using Python scripts, generative models, and real footage.

Result: State-of-the-art models show significant performance gaps, especially in abstract and planning tasks.

Conclusion: MORSE-500 is designed to evolve and support future research, with released resources for reproducibility.

Abstract: Despite rapid advances in vision-language models (VLMs), current benchmarks
for multimodal reasoning fall short in three key dimensions. First, they
overwhelmingly rely on static images, failing to capture the temporal
complexity of real-world environments. Second, they narrowly focus on
mathematical problem-solving, neglecting the broader spectrum of reasoning
skills -- including abstract, physical, planning, spatial, and temporal
capabilities -- required for robust multimodal intelligence. Third, many
benchmarks quickly saturate, offering limited headroom for diagnosing failure
modes or measuring continued progress. We introduce MORSE-500 (Multimodal
Reasoning Stress-test Environment), a video benchmark composed of 500 fully
scripted clips with embedded questions spanning six complementary reasoning
categories. Each instance is programmatically generated using deterministic
Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and
curated real footage. This script-driven design allows fine-grained control
over visual complexity, distractor density, and temporal dynamics -- enabling
difficulty to be scaled systematically as models improve. Unlike static
benchmarks that become obsolete once saturated, MORSE-500 is built to evolve:
its controllable generation pipeline supports the creation of arbitrarily
challenging new instances, making it ideally suited for stress-testing
next-generation models. Initial experiments with state-of-the-art systems --
including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest
available at the time, alongside strong open-source models -- reveal
substantial performance gaps across all categories, with particularly large
deficits in abstract and planning tasks. We release the full dataset,
generation scripts, and evaluation harness to support transparent,
reproducible, and forward-looking multimodal reasoning research.

</details>


### [175] [Personalized Interpretability -- Interactive Alignment of Prototypical Parts Networks](https://arxiv.org/pdf/2506.05533)
*Tomasz Michalski, Adam Wróbel, Andrea Bontempelli, Jakub Luśtyk, Mikolaj Kniejski, Stefano Teso, Andrea Passerini, Bartosz Zieliński, Dawid Rymarczyk*

Main category: cs.CV

TL;DR: YoursProtoP introduces an interactive strategy to personalize visual concepts in interpretable neural networks, addressing concept inconsistency and user preferences while maintaining model accuracy.


<details>
  <summary>Details</summary>
Motivation: Current concept-based interpretable neural networks suffer from concept inconsistency and lack mechanisms for user feedback, misaligning model reasoning with human understanding.

Method: YoursProtoP adapts and splits prototypical parts (visual concepts) based on user supervision to align with user preferences.

Result: Experiments on synthetic (FunnyBirds) and real-world datasets (CUB, CARS, PETS) show YoursProtoP achieves concept consistency without accuracy loss.

Conclusion: YoursProtoP effectively bridges the gap between model explanations and human understanding by incorporating user feedback.

Abstract: Concept-based interpretable neural networks have gained significant attention
due to their intuitive and easy-to-understand explanations based on case-based
reasoning, such as "this bird looks like those sparrows". However, a major
limitation is that these explanations may not always be comprehensible to users
due to concept inconsistency, where multiple visual features are
inappropriately mixed (e.g., a bird's head and wings treated as a single
concept). This inconsistency breaks the alignment between model reasoning and
human understanding. Furthermore, users have specific preferences for how
concepts should look, yet current approaches provide no mechanism for
incorporating their feedback. To address these issues, we introduce
YoursProtoP, a novel interactive strategy that enables the personalization of
prototypical parts - the visual concepts used by the model - according to user
needs. By incorporating user supervision, YoursProtoP adapts and splits
concepts used for both prediction and explanation to better match the user's
preferences and understanding. Through experiments on both the synthetic
FunnyBirds dataset and a real-world scenario using the CUB, CARS, and PETS
datasets in a comprehensive user study, we demonstrate the effectiveness of
YoursProtoP in achieving concept consistency without compromising the accuracy
of the model.

</details>


### [176] [FRAME: Pre-Training Video Feature Representations via Anticipation and Memory](https://arxiv.org/pdf/2506.05543)
*Sethuraman TV, Savya Khosla, Vignesh Srinivasakumar, Jiahui Huang, Seoung Wug Oh, Simon Jenni, Derek Hoiem, Joon-Young Lee*

Main category: cs.CV

TL;DR: FRAME is a self-supervised video frame encoder designed for dense video understanding, outperforming image and video models on dense prediction tasks while maintaining temporal and spatial coherence.


<details>
  <summary>Details</summary>
Motivation: Existing image encoders lack temporal awareness, and video models underperform on dense prediction tasks. FRAME bridges this gap by leveraging image-based models for dense video understanding.

Method: FRAME predicts current and future DINO patch features from past and present RGB frames, ensuring spatially precise and temporally coherent representations. It also aligns its class token with CLIP's semantic space for language-driven tasks.

Result: FRAME outperforms image encoders and existing self-supervised video models across six dense prediction tasks on seven datasets.

Conclusion: FRAME is a versatile and compact video encoder that excels in dense video understanding and supports language-driven tasks, making it suitable for various downstream applications.

Abstract: Dense video prediction tasks, such as object tracking and semantic
segmentation, require video encoders that generate temporally consistent,
spatially dense features for every frame. However, existing approaches fall
short: image encoders like DINO or CLIP lack temporal awareness, while video
models such as VideoMAE underperform compared to image encoders on dense
prediction tasks. We address this gap with FRAME, a self-supervised video frame
encoder tailored for dense video understanding. FRAME learns to predict current
and future DINO patch features from past and present RGB frames, leading to
spatially precise and temporally coherent representations. To our knowledge,
FRAME is the first video encoder to leverage image-based models for dense
prediction while outperforming them on tasks requiring fine-grained visual
correspondence. As an auxiliary capability, FRAME aligns its class token with
CLIP's semantic space, supporting language-driven tasks such as video
classification. We evaluate FRAME across six dense prediction tasks on seven
datasets, where it consistently outperforms image encoders and existing
self-supervised video models. Despite its versatility, FRAME maintains a
compact architecture suitable for a range of downstream applications.

</details>


### [177] [Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos](https://arxiv.org/pdf/2506.05546)
*Vadim Tschernezki, Diane Larlus, Andrea Vedaldi, Iro Laina*

Main category: cs.CV

TL;DR: The paper explores improving dynamic segmentation in 3D by fusing 2D motion predictions into layered radiance fields, addressing challenges with test-time refinement, and outperforming 2D baselines.


<details>
  <summary>Details</summary>
Motivation: 3D techniques often fail for dynamic phenomena, despite their success in static scenes. The paper aims to bridge this gap by enhancing 3D methods for dynamic segmentation.

Method: Proposes Layered Motion Fusion to integrate 2D motion segmentation into 3D layered radiance fields, using test-time refinement to handle data complexity.

Result: The 3D model's segmentation predictions significantly outperform the 2D baseline, showing 3D techniques can enhance dynamic scene analysis.

Conclusion: 3D methods can effectively improve 2D analysis for dynamic phenomena, even in challenging real-world settings.

Abstract: Computer vision is largely based on 2D techniques, with 3D vision still
relegated to a relatively narrow subset of applications. However, by building
on recent advances in 3D models such as neural radiance fields, some authors
have shown that 3D techniques can at last improve outputs extracted from
independent 2D views, by fusing them into 3D and denoising them. This is
particularly helpful in egocentric videos, where the camera motion is
significant, but only under the assumption that the scene itself is static. In
fact, as shown in the recent analysis conducted by EPIC Fields, 3D techniques
are ineffective when it comes to studying dynamic phenomena, and, in
particular, when segmenting moving objects. In this paper, we look into this
issue in more detail. First, we propose to improve dynamic segmentation in 3D
by fusing motion segmentation predictions from a 2D-based model into layered
radiance fields (Layered Motion Fusion). However, the high complexity of long,
dynamic videos makes it challenging to capture the underlying geometric
structure, and, as a result, hinders the fusion of motion cues into the
(incomplete) scene geometry. We address this issue through test-time
refinement, which helps the model to focus on specific frames, thereby reducing
the data complexity. This results in a synergy between motion fusion and the
refinement, and in turn leads to segmentation predictions of the 3D model that
surpass the 2D baseline by a large margin. This demonstrates that 3D techniques
can enhance 2D analysis even for dynamic phenomena in a challenging and
realistic setting.

</details>


### [178] [When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding](https://arxiv.org/pdf/2506.05551)
*Yan Shu, Hangui Lin, Yexin Liu, Yan Zhang, Gangyan Zeng, Yan Li, Yu Zhou, Ser-Nam Lim, Harry Yang, Nicu Sebe*

Main category: cs.CV

TL;DR: The paper addresses semantic hallucination in Large Multimodal Models (LMMs) when processing ambiguous or non-semantic scene text. It proposes a training-free framework (ZoomText and Grounded Layer Correction) to mitigate this issue and introduces TextHalu-Bench for evaluation.


<details>
  <summary>Details</summary>
Motivation: LMMs struggle with visually ambiguous or non-semantic scene text, often producing incorrect yet plausible answers (semantic hallucination). The study aims to understand and mitigate this issue.

Method: The framework includes ZoomText (coarse-to-fine text region identification) and Grounded Layer Correction (leveraging internal representations from less hallucination-prone layers).

Result: The method effectively reduces semantic hallucination and performs well on scene text spotting and understanding benchmarks.

Conclusion: The proposed framework successfully mitigates semantic hallucination in LMMs without additional training, validated by the TextHalu-Bench benchmark.

Abstract: Large Multimodal Models (LMMs) have achieved impressive progress in visual
perception and reasoning. However, when confronted with visually ambiguous or
non-semantic scene text, they often struggle to accurately spot and understand
the content, frequently generating semantically plausible yet visually
incorrect answers, which we refer to as semantic hallucination. In this work,
we investigate the underlying causes of semantic hallucination and identify a
key finding: Transformer layers in LLM with stronger attention focus on scene
text regions are less prone to producing semantic hallucinations. Thus, we
propose a training-free semantic hallucination mitigation framework comprising
two key components: (1) ZoomText, a coarse-to-fine strategy that identifies
potential text regions without external detectors; and (2) Grounded Layer
Correction, which adaptively leverages the internal representations from layers
less prone to hallucination to guide decoding, correcting hallucinated outputs
for non-semantic samples while preserving the semantics of meaningful ones. To
enable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over
1,730 samples spanning both semantic and non-semantic cases, with manually
curated question-answer pairs designed to probe model hallucinations. Extensive
experiments demonstrate that our method not only effectively mitigates semantic
hallucination but also achieves strong performance on public benchmarks for
scene text spotting and understanding.

</details>


### [179] [EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh](https://arxiv.org/pdf/2506.05554)
*Tao Hu, Haoyang Peng, Xiao Liu, Yuewen Ma*

Main category: cs.CV

TL;DR: EX-4D introduces a Depth Watertight Mesh representation and simulated masking for high-quality, camera-controllable video generation from monocular input, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with geometric inconsistencies and occlusion artifacts in extreme viewpoints, degrading visual quality.

Method: Uses Depth Watertight Mesh for geometric consistency, simulated masking for training data, and a LoRA-based video diffusion adapter for synthesis.

Result: Outperforms state-of-the-art methods in physical consistency and extreme-view quality.

Conclusion: EX-4D enables practical 4D video generation with improved quality and consistency.

Abstract: Generating high-quality camera-controllable videos from monocular input is a
challenging task, particularly under extreme viewpoint. Existing methods often
struggle with geometric inconsistencies and occlusion artifacts in boundaries,
leading to degraded visual quality. In this paper, we introduce EX-4D, a novel
framework that addresses these challenges through a Depth Watertight Mesh
representation. The representation serves as a robust geometric prior by
explicitly modeling both visible and occluded regions, ensuring geometric
consistency in extreme camera pose. To overcome the lack of paired multi-view
datasets, we propose a simulated masking strategy that generates effective
training data only from monocular videos. Additionally, a lightweight
LoRA-based video diffusion adapter is employed to synthesize high-quality,
physically consistent, and temporally coherent videos. Extensive experiments
demonstrate that EX-4D outperforms state-of-the-art methods in terms of
physical consistency and extreme-view quality, enabling practical 4D video
generation.

</details>


### [180] [On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images](https://arxiv.org/pdf/2506.05558)
*Andreas Meuleman, Ishaan Shah, Alexandre Lanvin, Bernhard Kerbl, George Drettakis*

Main category: cs.CV

TL;DR: A fast on-the-fly method for camera pose estimation and 3D Gaussian Splatting (3DGS) training, handling wide baselines and large scenes efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Structure from Motion and 3DGS optimization are slow, while SLAM with 3DGS struggles with wide baselines and large scenes. The goal is real-time processing post-capture.

Method: Combines fast initial pose estimation with GPU-friendly mini bundle adjustment and direct sampling of Gaussian primitives. Uses incremental spawning, scalable radiance field construction, and clustering for large scenes.

Result: Achieves on-the-fly processing for diverse capture scenarios and scene sizes, maintaining competitive speed and image quality.

Conclusion: The method efficiently handles large-scale scenes and wide baselines, offering real-time performance without compromising quality.

Abstract: Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy
reconstruction from photos, enabling free-viewpoint navigation. Nonetheless,
pose estimation using Structure from Motion and 3DGS optimization can still
each take between minutes and hours of computation after capture is complete.
SLAM methods combined with 3DGS are fast but struggle with wide camera
baselines and large scenes. We present an on-the-fly method to produce camera
poses and a trained 3DGS immediately after capture. Our method can handle dense
and wide-baseline captures of ordered photo sequences and large-scale scenes.
To do this, we first introduce fast initial pose estimation, exploiting learned
features and a GPU-friendly mini bundle adjustment. We then introduce direct
sampling of Gaussian primitive positions and shapes, incrementally spawning
primitives where required, significantly accelerating training. These two
efficient steps allow fast and robust joint optimization of poses and Gaussian
primitives. Our incremental approach handles large-scale scenes by introducing
scalable radiance field construction, progressively clustering 3DGS primitives,
storing them in anchors, and offloading them from the GPU. Clustered primitives
are progressively merged, keeping the required scale of 3DGS at any viewpoint.
We evaluate our solution on a variety of datasets and show that our solution
can provide on-the-fly processing of all the capture scenarios and scene sizes
we target while remaining competitive with other methods that only handle
specific capture styles or scene sizes in speed, image quality, or both.

</details>


### [181] [VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for Occupancy and Flow Prediction](https://arxiv.org/pdf/2506.05563)
*Ziyue Zhu, Shenlong Wang, Jin Xie, Jiang-jiang Liu, Jingdong Wang, Jian Yang*

Main category: cs.CV

TL;DR: VoxelSplat is a novel regularization framework for camera-based occupancy prediction, improving 3D semantics and scene flow estimation using 3D Gaussian Splatting and 2D projection supervision.


<details>
  <summary>Details</summary>
Motivation: Address challenges like occlusions and unbalanced dynamic environments in 3D semantics and scene flow prediction.

Method: Uses 3D Gaussian Splatting for enhanced semantics supervision via 2D projection and self-supervised scene flow learning.

Result: Improves accuracy of semantic occupancy and scene flow estimation without increasing inference time.

Conclusion: VoxelSplat effectively enhances performance and is compatible with existing occupancy models.

Abstract: Recent advancements in camera-based occupancy prediction have focused on the
simultaneous prediction of 3D semantics and scene flow, a task that presents
significant challenges due to specific difficulties, e.g., occlusions and
unbalanced dynamic environments. In this paper, we analyze these challenges and
their underlying causes. To address them, we propose a novel regularization
framework called VoxelSplat. This framework leverages recent developments in 3D
Gaussian Splatting to enhance model performance in two key ways: (i) Enhanced
Semantics Supervision through 2D Projection: During training, our method
decodes sparse semantic 3D Gaussians from 3D representations and projects them
onto the 2D camera view. This provides additional supervision signals in the
camera-visible space, allowing 2D labels to improve the learning of 3D
semantics. (ii) Scene Flow Learning: Our framework uses the predicted scene
flow to model the motion of Gaussians, and is thus able to learn the scene flow
of moving objects in a self-supervised manner using the labels of adjacent
frames. Our method can be seamlessly integrated into various existing occupancy
models, enhancing performance without increasing inference time. Extensive
experiments on benchmark datasets demonstrate the effectiveness of VoxelSplat
in improving the accuracy of both semantic occupancy and scene flow estimation.
The project page and codes are available at
https://zzy816.github.io/VoxelSplat-Demo/.

</details>


### [182] [TissUnet: Improved Extracranial Tissue and Cranium Segmentation for Children through Adulthood](https://arxiv.org/pdf/2506.05660)
*Markian Mandzak, Elvira Yang, Anna Zapaishchykova, Yu-Hui Chen, Lucas Heilbroner, John Zielke, Divyanshu Tak, Reza Mojahed-Yazdi, Francesca Romana Mussa, Zezhong Ye, Sridhar Vajapeyam, Viviana Benitez, Ralph Salloum, Susan N. Chi, Houman Sotoudeh, Jakob Seidlitz, Sabine Mueller, Hugo J. W. L. Aerts, Tina Y. Poussaint, Benjamin H. Kann*

Main category: cs.CV

TL;DR: TissUnet is a deep learning model for segmenting extracranial tissues (skull bone, fat, muscle) from brain MRI, validated across diverse datasets and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Extracranial tissues in brain MRI are underutilized for health assessment, lacking validated tools for diverse populations and pathologies.

Method: TissUnet was trained on 155 MRI-CT pairs and validated on nine datasets, including healthy and tumor cases, using Dice coefficients and expert annotations.

Result: Achieved median Dice of 0.79-0.83, outperforming state-of-the-art, with 89% acceptance rate in adjudication and excellent performance in blinded reviews.

Conclusion: TissUnet provides fast, accurate extracranial tissue segmentation, enabling large-scale studies on health and treatment effects using standard MRI.

Abstract: Extracranial tissues visible on brain magnetic resonance imaging (MRI) may
hold significant value for characterizing health conditions and clinical
decision-making, yet they are rarely quantified. Current tools have not been
widely validated, particularly in settings of developing brains or underlying
pathology. We present TissUnet, a deep learning model that segments skull bone,
subcutaneous fat, and muscle from routine three-dimensional T1-weighted MRI,
with or without contrast enhancement. The model was trained on 155 paired
MRI-computed tomography (CT) scans and validated across nine datasets covering
a wide age range and including individuals with brain tumors. In comparison to
AI-CT-derived labels from 37 MRI-CT pairs, TissUnet achieved a median Dice
coefficient of 0.79 [IQR: 0.77-0.81] in a healthy adult cohort. In a second
validation using expert manual annotations, median Dice was 0.83 [IQR:
0.83-0.84] in healthy individuals and 0.81 [IQR: 0.78-0.83] in tumor cases,
outperforming previous state-of-the-art method. Acceptability testing resulted
in an 89% acceptance rate after adjudication by a tie-breaker(N=108 MRIs), and
TissUnet demonstrated excellent performance in the blinded comparative review
(N=45 MRIs), including both healthy and tumor cases in pediatric populations.
TissUnet enables fast, accurate, and reproducible segmentation of extracranial
tissues, supporting large-scale studies on craniofacial morphology, treatment
effects, and cardiometabolic risk using standard brain T1w MRI.

</details>


### [183] [PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers](https://arxiv.org/pdf/2506.05573)
*Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: PartCrafter is a novel 3D generative model that synthesizes multiple semantically meaningful 3D meshes from a single RGB image, outperforming existing methods by using a unified, compositional architecture.


<details>
  <summary>Details</summary>
Motivation: Existing methods either produce monolithic 3D shapes or rely on two-stage pipelines, limiting their ability to generate part-aware 3D meshes from single images.

Method: PartCrafter uses a pretrained 3D mesh diffusion transformer (DiT) and introduces a compositional latent space and hierarchical attention mechanism for part-aware generation.

Result: The model outperforms existing approaches in generating decomposable 3D meshes, even for parts not visible in input images.

Conclusion: PartCrafter demonstrates the strength of part-aware generative priors for 3D understanding and synthesis, with code and data to be released.

Abstract: We introduce PartCrafter, the first structured 3D generative model that
jointly synthesizes multiple semantically meaningful and geometrically distinct
3D meshes from a single RGB image. Unlike existing methods that either produce
monolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an
image and then reconstructing each segment, PartCrafter adopts a unified,
compositional generation architecture that does not rely on pre-segmented
inputs. Conditioned on a single image, it simultaneously denoises multiple 3D
parts, enabling end-to-end part-aware generation of both individual objects and
complex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh
diffusion transformer (DiT) trained on whole objects, inheriting the pretrained
weights, encoder, and decoder, and introduces two key innovations: (1) A
compositional latent space, where each 3D part is represented by a set of
disentangled latent tokens; (2) A hierarchical attention mechanism that enables
structured information flow both within individual parts and across all parts,
ensuring global coherence while preserving part-level detail during generation.
To support part-level supervision, we curate a new dataset by mining part-level
annotations from large-scale 3D object datasets. Experiments show that
PartCrafter outperforms existing approaches in generating decomposable 3D
meshes, including parts that are not directly visible in input images,
demonstrating the strength of part-aware generative priors for 3D understanding
and synthesis. Code and training data will be released.

</details>


### [184] [DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models](https://arxiv.org/pdf/2506.05667)
*Yuhan Hao, Zhengning Li, Lei Sun, Weilong Wang, Naixin Yi, Sheng Song, Caihong Qin, Mofan Zhou, Yifei Zhan, Peng Jia, Xianpeng Lang*

Main category: cs.CV

TL;DR: DriveAction is a new benchmark for Vision-Language-Action (VLA) models in autonomous driving, addressing gaps in scenario diversity, action-level annotation, and human-aligned evaluation. It uses real-world data and a tree-structured framework for comprehensive assessment.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack diversity, reliable action labels, and human-aligned evaluation, limiting VLA model development.

Method: DriveAction includes 16,185 QA pairs from 2,610 scenarios, using real-world data and user-driven action labels. It employs a tree-structured evaluation framework linking vision, language, and action tasks.

Result: VLMs need both vision and language inputs for accurate action prediction; accuracy drops significantly without either. The benchmark identifies model bottlenecks robustly.

Conclusion: DriveAction provides a rigorous foundation for improving human-like decisions in autonomous driving, offering new insights and reliable evaluation.

Abstract: Vision-Language-Action (VLA) models have advanced autonomous driving, but
existing benchmarks still lack scenario diversity, reliable action-level
annotation, and evaluation protocols aligned with human preferences. To address
these limitations, we introduce DriveAction, the first action-driven benchmark
specifically designed for VLA models, comprising 16,185 QA pairs generated from
2,610 driving scenarios. DriveAction leverages real-world driving data
proactively collected by users of production-level autonomous vehicles to
ensure broad and representative scenario coverage, offers high-level discrete
action labels collected directly from users' actual driving operations, and
implements an action-rooted tree-structured evaluation framework that
explicitly links vision, language, and action tasks, supporting both
comprehensive and task-specific assessment. Our experiments demonstrate that
state-of-the-art vision-language models (VLMs) require both vision and language
guidance for accurate action prediction: on average, accuracy drops by 3.3%
without vision input, by 4.1% without language input, and by 8.0% without
either. Our evaluation supports precise identification of model bottlenecks
with robust and consistent results, thus providing new insights and a rigorous
foundation for advancing human-like decisions in autonomous driving.

</details>


### [185] [UniRes: Universal Image Restoration for Complex Degradations](https://arxiv.org/pdf/2506.05599)
*Mo Zhou, Keren Ye, Mauricio Delbracio, Peyman Milanfar, Vishal M. Patel, Hossein Talebi*

Main category: cs.CV

TL;DR: UniRes, a diffusion-based framework, addresses complex image degradations by combining specialized models, improving restoration for in-the-wild data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with generalization to real-world degradations, which are often complex mixtures of known types.

Method: Proposes UniRes, a flexible diffusion-based framework that integrates specialized models during diffusion sampling for end-to-end restoration.

Result: Outperforms existing methods on complex-degradation datasets, showing consistent gains in qualitative and quantitative metrics.

Conclusion: UniRes effectively handles complex degradations, offering flexibility and improved performance for real-world image restoration.

Abstract: Real-world image restoration is hampered by diverse degradations stemming
from varying capture conditions, capture devices and post-processing pipelines.
Existing works make improvements through simulating those degradations and
leveraging image generative priors, however generalization to in-the-wild data
remains an unresolved problem. In this paper, we focus on complex degradations,
i.e., arbitrary mixtures of multiple types of known degradations, which is
frequently seen in the wild. A simple yet flexible diffusionbased framework,
named UniRes, is proposed to address such degradations in an end-to-end manner.
It combines several specialized models during the diffusion sampling steps,
hence transferring the knowledge from several well-isolated restoration tasks
to the restoration of complex in-the-wild degradations. This only requires
well-isolated training data for several degradation types. The framework is
flexible as extensions can be added through a unified formulation, and the
fidelity-quality trade-off can be adjusted through a new paradigm. Our proposed
method is evaluated on both complex-degradation and single-degradation image
restoration datasets. Extensive qualitative and quantitative experimental
results show consistent performance gain especially for images with complex
degradations.

</details>


### [186] [Controlled Data Rebalancing in Multi-Task Learning for Real-World Image Super-Resolution](https://arxiv.org/pdf/2506.05607)
*Shuchen Lin, Mingtao Feng, Weisheng Dong, Fangfang Wu, Jianqiao Luo, Yaonan Wang, Guangming Shi*

Main category: cs.CV

TL;DR: The paper proposes a multi-task learning approach for Real-SR, addressing task imbalance through improved task definition, imbalance quantification, and adaptive data rebalancing, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: Real-SR is challenging due to complex degradation patterns. Existing methods assume broad degradation spaces, but this work focuses on balancing SR networks' handling of different degradation patterns within a fixed space.

Method: The method frames Real-SR as a multi-task learning problem, introducing a task definition framework, a focal loss-based weighting mechanism, and adaptive data rebalancing to manage task imbalance.

Result: The approach achieves consistent superiority across all degradation tasks, as shown in extensive experiments.

Conclusion: The proposed paradigm effectively balances degradation handling in Real-SR, outperforming existing methods.

Abstract: Real-world image super-resolution (Real-SR) is a challenging problem due to
the complex degradation patterns in low-resolution images. Unlike approaches
that assume a broadly encompassing degradation space, we focus specifically on
achieving an optimal balance in how SR networks handle different degradation
patterns within a fixed degradation space. We propose an improved paradigm that
frames Real-SR as a data-heterogeneous multi-task learning problem, our work
addresses task imbalance in the paradigm through coordinated advancements in
task definition, imbalance quantification, and adaptive data rebalancing.
Specifically, we introduce a novel task definition framework that segments the
degradation space by setting parameter-specific boundaries for degradation
operators, effectively reducing the task quantity while maintaining task
discrimination. We then develop a focal loss based multi-task weighting
mechanism that precisely quantifies task imbalance dynamics during model
training. Furthermore, to prevent sporadic outlier samples from dominating the
gradient optimization of the shared multi-task SR model, we strategically
convert the quantified task imbalance into controlled data rebalancing through
deliberate regulation of task-specific training volumes. Extensive quantitative
and qualitative experiments demonstrate that our method achieves consistent
superiority across all degradation tasks.

</details>


### [187] [Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection](https://arxiv.org/pdf/2506.05651)
*Shanmukha Vellamcheti, Sanjoy Kundu, Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: The paper introduces an iterative visual grounding framework using LLMs for open-world visual relationship detection, outperforming baselines on unseen predicates.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of fixed predicate sets in VRD models and enable generalization to novel interactions by grounding unannotated relationships.

Method: An expectation-maximization inspired framework alternating between LLM-generated scene graphs and visual model training for alignment.

Result: Achieves mean recall (mR@50) of 15.9, 13.1, and 11.7 on seen, unseen, and mixed predicate sets, outperforming baselines.

Conclusion: Grounded LLM priors show promise for scalable open-world visual understanding.

Abstract: Understanding relationships between objects is central to visual
intelligence, with applications in embodied AI, assistive systems, and scene
understanding. Yet, most visual relationship detection (VRD) models rely on a
fixed predicate set, limiting their generalization to novel interactions. A key
challenge is the inability to visually ground semantically plausible, but
unannotated, relationships hypothesized from external knowledge. This work
introduces an iterative visual grounding framework that leverages large
language models (LLMs) as structured relational priors. Inspired by
expectation-maximization (EM), our method alternates between generating
candidate scene graphs from detected objects using an LLM (expectation) and
training a visual model to align these hypotheses with perceptual evidence
(maximization). This process bootstraps relational understanding beyond
annotated data and enables generalization to unseen predicates. Additionally,
we introduce a new benchmark for open-world VRD on Visual Genome with 21
held-out predicates and evaluate under three settings: seen, unseen, and mixed.
Our model outperforms LLM-only, few-shot, and debiased baselines, achieving
mean recall (mR@50) of 15.9, 13.1, and 11.7 on predicate classification on
these three sets. These results highlight the promise of grounded LLM priors
for scalable open-world visual understanding.

</details>


### [188] [Pts3D-LLM: Studying the Impact of Token Structure for 3D Scene Understanding With Large Language Models](https://arxiv.org/pdf/2506.05689)
*Hugues Thomas, Chen Chen, Jian Zhang*

Main category: cs.CV

TL;DR: The paper studies 3D token structures for MLLMs, comparing video-based and point-based representations. It introduces a novel method using 3D point cloud features, showing improved performance and competitive results with video-based structures.


<details>
  <summary>Details</summary>
Motivation: Effectively representing 3D scenes for MLLMs is challenging, as existing methods rely on 2D image features and varied tokenization. This work aims to improve 3D understanding by exploring and comparing token structures.

Method: The study systematically compares video-based and point-based 3D token structures. A novel approach enriches visual tokens using 3D point cloud features from a pretrained Point Transformer V3 encoder.

Result: Merging explicit 3D features boosts performance, and point-based structures can rival video-based ones with clever sampling and ordering. State-of-the-art results are achieved on 3D benchmarks.

Conclusion: The analysis of token structures is a key contribution, alongside transparent reporting of results. The findings highlight the importance of robust 3D representations for MLLMs.

Abstract: Effectively representing 3D scenes for Multimodal Large Language Models
(MLLMs) is crucial yet challenging. Existing approaches commonly only rely on
2D image features and use varied tokenization approaches. This work presents a
rigorous study of 3D token structures, systematically comparing video-based and
point-based representations while maintaining consistent model backbones and
parameters. We propose a novel approach that enriches visual tokens by
incorporating 3D point cloud features from a Sonata pretrained Point
Transformer V3 encoder. Our experiments demonstrate that merging explicit 3D
features significantly boosts performance. Furthermore, we show that
point-based token structures can rival video-based ones when the points are
cleverly sampled and ordered. Our best models from both structures achieve
state-of-the-art results on multiple 3D understanding benchmarks. We emphasize
our analysis of token structures as a key contribution, alongside transparent
reporting of results averaged over multiple seeds, a practice we believe is
vital for robust progress in the field.

</details>


### [189] [MoralCLIP: Contrastive Alignment of Vision-and-Language Representations with Moral Foundations Theory](https://arxiv.org/pdf/2506.05696)
*Ana Carolina Condez, Diogo Tavares, João Magalhães*

Main category: cs.CV

TL;DR: MoralCLIP extends vision-language models with moral grounding using Moral Foundations Theory, improving moral content understanding.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models lack moral reasoning, a key aspect of human cognition.

Method: MoralCLIP integrates visual and textual moral cues into a unified embedding space, using a multi-label dataset and moral data augmentation.

Result: Explicit moral supervision enhances unimodal and multimodal moral understanding.

Conclusion: MoralCLIP lays groundwork for morally-aware AI systems aligned with human values.

Abstract: Recent advances in vision-language models have enabled rich semantic
understanding across modalities. However, these encoding methods lack the
ability to interpret or reason about the moral dimensions of content-a crucial
aspect of human cognition. In this paper, we address this gap by introducing
MoralCLIP, a novel embedding representation method that extends multimodal
learning with explicit moral grounding based on Moral Foundations Theory (MFT).
Our approach integrates visual and textual moral cues into a unified embedding
space, enabling cross-modal moral alignment. MoralCLIP is grounded on the
multi-label dataset Social-Moral Image Database to identify co-occurring moral
foundations in visual content. For MoralCLIP training, we design a moral data
augmentation strategy to scale our annotated dataset to 15,000 image-text pairs
labeled with MFT-aligned dimensions. Our results demonstrate that explicit
moral supervision improves both unimodal and multimodal understanding of moral
content, establishing a foundation for morally-aware AI systems capable of
recognizing and aligning with human moral values.

</details>


### [190] [Token Transforming: A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration](https://arxiv.org/pdf/2506.05709)
*Fanhu Zeng, Deli Yu, Zhenglun Kong, Hao Tang*

Main category: cs.CV

TL;DR: The paper proposes a many-to-many Token Transforming framework for compressing vision transformers, unifying existing token reduction methods and minimizing information loss without post-training.


<details>
  <summary>Details</summary>
Motivation: Current token reduction methods (pruning/merging) cause significant information loss and require post-training, prompting a need for a more efficient and training-free approach.

Method: The authors introduce a token matrix transformation framework, generalizing existing methods, and propose a many-to-many Token Transforming approach to retain maximum information.

Result: The framework reduces FLOPs by 40%, accelerates DeiT-S by 1.5x with only a 0.1% accuracy drop, and shows consistent improvements in dense prediction tasks.

Conclusion: The proposed method offers superior computation-performance trade-offs, significant budget reduction, and inference acceleration across various tasks.

Abstract: Vision transformers have been widely explored in various vision tasks. Due to
heavy computational cost, much interest has aroused for compressing vision
transformer dynamically in the aspect of tokens. Current methods mainly pay
attention to token pruning or merging to reduce token numbers, in which tokens
are compressed exclusively, causing great information loss and therefore
post-training is inevitably required to recover the performance. In this paper,
we rethink token reduction and unify the process as an explicit form of token
matrix transformation, in which all existing methods are constructing special
forms of matrices within the framework. Furthermore, we propose a many-to-many
Token Transforming framework that serves as a generalization of all existing
methods and reserves the most information, even enabling training-free
acceleration. We conduct extensive experiments to validate our framework.
Specifically, we reduce 40% FLOPs and accelerate DeiT-S by $\times$1.5 with
marginal 0.1% accuracy drop. Furthermore, we extend the method to dense
prediction tasks including segmentation, object detection, depth estimation,
and language model generation. Results demonstrate that the proposed method
consistently achieves substantial improvements, offering a better
computation-performance trade-off, impressive budget reduction and inference
acceleration.

</details>


### [191] [You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping](https://arxiv.org/pdf/2506.05719)
*Jingshun Huang, Haitao Lin, Tianyu Wang, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue*

Main category: cs.CV

TL;DR: YOEO is a single-stage method for category-level pose estimation of articulated objects, improving efficiency and performance over multi-stage pipelines.


<details>
  <summary>Details</summary>
Motivation: Existing multi-stage pipelines for pose estimation are computationally expensive and inefficient for real-time robotic tasks.

Method: YOEO uses a unified network for instance segmentation and NPCS representation, combined with clustering and alignment techniques.

Result: Experiments on GAPart dataset show YOEO's effectiveness, with real-time performance at 200Hz in real-world robotic tasks.

Conclusion: YOEO offers a practical, efficient solution for real-time pose estimation in robotic manipulation.

Abstract: This paper addresses the problem of category-level pose estimation for
articulated objects in robotic manipulation tasks. Recent works have shown
promising results in estimating part pose and size at the category level.
However, these approaches primarily follow a complex multi-stage pipeline that
first segments part instances in the point cloud and then estimates the
Normalized Part Coordinate Space (NPCS) representation for 6D poses. These
approaches suffer from high computational costs and low performance in
real-time robotic tasks. To address these limitations, we propose YOEO, a
single-stage method that simultaneously outputs instance segmentation and NPCS
representations in an end-to-end manner. We use a unified network to generate
point-wise semantic labels and centroid offsets, allowing points from the same
part instance to vote for the same centroid. We further utilize a clustering
algorithm to distinguish points based on their estimated centroid distances.
Finally, we first separate the NPCS region of each instance. Then, we align the
separated regions with the real point cloud to recover the final pose and size.
Experimental results on the GAPart dataset demonstrate the pose estimation
capabilities of our proposed single-shot method. We also deploy our
synthetically-trained model in a real-world setting, providing real-time visual
feedback at 200Hz, enabling a physical Kinova robot to interact with unseen
articulated objects. This showcases the utility and effectiveness of our
proposed method.

</details>


### [192] [Investigating the Relationship between Weighted Figure of Merit and Rosin's Measure](https://arxiv.org/pdf/2506.05749)
*Bimal Kumar Ray*

Main category: cs.CV

TL;DR: The paper investigates the relationship between weighted figure of merit and Rosin's measure for polygonal approximation, concluding they are independent and not substitutable.


<details>
  <summary>Details</summary>
Motivation: To determine if weighted figure of merit can replace Rosin's measure for comparing sub-optimal polygonal approximation schemes.

Method: Theoretical analysis, experimental investigation, and statistical analysis (Pearson's correlation coefficient) were used.

Result: The two measures are theoretically independent, experimentally uncorrelated, and cannot substitute each other.

Conclusion: Weighted figure of merit cannot replace Rosin's measure for comparing sub-optimal schemes.

Abstract: Many studies had been conducted to solve the problem of approximating a
digital boundary by piece straight-line segments for further processing
required in computer vision applications. The authors of these studies compared
their schemes to determine the best one. The initial measure used to assess the
goodness of a polygonal approximation was figure of merit. Later, it was
pointed out that this measure was not an appropriate metric for a valid reason
and this is why Rosin - through mathematical analysis - introduced a measure
called merit. However, this measure involves optimal scheme of polygonal
approximation and so it is time-consuming to compute it to assess the goodness
of an approximation. This led many researchers to use weighted figure of merit
as a substitute for Rosin's measure to compare among sub-optimal schemes. An
attempt is made in this communication to investigate whether the two measures -
weighted figure of merit and Rosin's measure - are related so that one can be
used instead of the other and towards this end theoretical analysis,
experimental investigation and statistical analysis are carried out. The
mathematical formula for weighted figure of merit and Rosin's measure are
analyzed and through proof of theorems it is found that the two measures are
independent of each other theoretically. The graphical analysis of experiments
carried out using public dataset supports theoretical analysis. The statistical
analysis using Pearson's correlation coefficient also establishes that the two
measures are uncorrelated. This analysis leads one to conclude that if a
sub-optimal scheme is found to be better (worse) than some other sub-optimal
scheme as indicated by Rosin's measure then the same conclusion cannot be drawn
using weighted figure of merit and so one cannot use weighted figure of merit
instead of Rosin's measure.

</details>


### [193] [Where Is The Ball: 3D Ball Trajectory Estimation From 2D Monocular Tracking](https://arxiv.org/pdf/2506.05763)
*Puntawat Ponglertnapakorn, Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: A method for 3D ball trajectory estimation from 2D tracking sequences using an LSTM-based pipeline with a novel canonical 3D representation, achieving state-of-the-art performance and real-world generalization.


<details>
  <summary>Details</summary>
Motivation: To address the ambiguity in estimating 3D trajectories from 2D sequences and enable applications in sports analysis and virtual replay.

Method: An LSTM-based pipeline with a canonical 3D representation independent of camera location, intermediate representations for invariance and reprojection consistency, trained on synthetic data.

Result: State-of-the-art performance on synthetic and real datasets, with generalization to real-world scenarios.

Conclusion: The method is effective for 3D trajectory estimation from 2D sequences, even when trained on simulated data, and has practical applications.

Abstract: We present a method for 3D ball trajectory estimation from a 2D tracking
sequence. To overcome the ambiguity in 3D from 2D estimation, we design an
LSTM-based pipeline that utilizes a novel canonical 3D representation that is
independent of the camera's location to handle arbitrary views and a series of
intermediate representations that encourage crucial invariance and reprojection
consistency. We evaluated our method on four synthetic and three real datasets
and conducted extensive ablation studies on our design choices. Despite
training solely on simulated data, our method achieves state-of-the-art
performance and can generalize to real-world scenarios with multiple
trajectories, opening up a range of applications in sport analysis and virtual
replay. Please visit our page: https://where-is-the-ball.github.io.

</details>


### [194] [Do Large Vision-Language Models Distinguish between the Actual and Apparent Features of Illusions?](https://arxiv.org/pdf/2506.05765)
*Taiga Shinozaki, Tomoki Doi, Satoshi Nishida, Hitomi Yanaka*

Main category: cs.CV

TL;DR: The paper investigates whether large vision language models (LVLMs) can discern genuine and fake visual illusions, revealing that their responses may rely on prior knowledge rather than true visual understanding.


<details>
  <summary>Details</summary>
Motivation: To address ambiguities in assessing machine cognition of visual illusions by distinguishing genuine and fake illusions.

Method: Introduces a VQA dataset with genuine and fake illusions, evaluating LVLMs' performance in discerning actual vs. apparent features.

Result: LVLMs predict similar answers for both genuine and fake illusions, suggesting reliance on prior knowledge, not visual understanding.

Conclusion: The study highlights limitations in LVLMs' visual cognition, emphasizing the need for deeper evaluation of machine understanding.

Abstract: Humans are susceptible to optical illusions, which serve as valuable tools
for investigating sensory and cognitive processes. Inspired by human vision
studies, research has begun exploring whether machines, such as large vision
language models (LVLMs), exhibit similar susceptibilities to visual illusions.
However, studies often have used non-abstract images and have not distinguished
actual and apparent features, leading to ambiguous assessments of machine
cognition. To address these limitations, we introduce a visual question
answering (VQA) dataset, categorized into genuine and fake illusions, along
with corresponding control images. Genuine illusions present discrepancies
between actual and apparent features, whereas fake illusions have the same
actual and apparent features even though they look illusory due to the similar
geometric configuration. We evaluate the performance of LVLMs for genuine and
fake illusion VQA tasks and investigate whether the models discern actual and
apparent features. Our findings indicate that although LVLMs may appear to
recognize illusions by correctly answering questions about both feature types,
they predict the same answers for both Genuine Illusion and Fake Illusion VQA
questions. This suggests that their responses might be based on prior knowledge
of illusions rather than genuine visual understanding. The dataset is available
at https://github.com/ynklab/FILM

</details>


### [195] [Robust sensor fusion against on-vehicle sensor staleness](https://arxiv.org/pdf/2506.05780)
*Meng Fan, Yifan Zuo, Patrick Blaes, Harley Montgomery, Subhasis Das*

Main category: cs.CV

TL;DR: A novel, model-agnostic approach improves sensor fusion in autonomous vehicles by addressing sensor staleness with timestamp offsets and data augmentation, maintaining performance under stale conditions.


<details>
  <summary>Details</summary>
Motivation: Sensor staleness in autonomous vehicles causes temporal misalignment, degrading trajectory predictions and safety-critical perception systems.

Method: Introduces per-point timestamp offsets for LiDAR/radar relative to cameras and a data augmentation strategy to simulate realistic staleness patterns.

Result: The method maintains consistent performance under stale conditions, unlike conventional models which regress significantly.

Conclusion: The approach effectively mitigates sensor staleness, enhancing robustness in multi-sensor fusion for autonomous vehicles.

Abstract: Sensor fusion is crucial for a performant and robust Perception system in
autonomous vehicles, but sensor staleness, where data from different sensors
arrives with varying delays, poses significant challenges. Temporal
misalignment between sensor modalities leads to inconsistent object state
estimates, severely degrading the quality of trajectory predictions that are
critical for safety. We present a novel and model-agnostic approach to address
this problem via (1) a per-point timestamp offset feature (for LiDAR and radar
both relative to camera) that enables fine-grained temporal awareness in sensor
fusion, and (2) a data augmentation strategy that simulates realistic sensor
staleness patterns observed in deployed vehicles. Our method is integrated into
a perspective-view detection model that consumes sensor data from multiple
LiDARs, radars and cameras. We demonstrate that while a conventional model
shows significant regressions when one sensor modality is stale, our approach
reaches consistently good performance across both synchronized and stale
conditions.

</details>


### [196] [GazeNLQ @ Ego4D Natural Language Queries Challenge 2025](https://arxiv.org/pdf/2506.05782)
*Wei-Cheng Lin, Chih-Ming Lien, Chen Lo, Chia-Hung Yeh*

Main category: cs.CV

TL;DR: GazeNLQ leverages gaze data to improve video segment retrieval for natural language queries, achieving notable accuracy scores.


<details>
  <summary>Details</summary>
Motivation: Gaze reflects visual attention and human intention, making it valuable for enhancing video retrieval tasks.

Method: Uses contrastive learning for gaze estimation from video, then augments video representations with gaze data.

Result: Achieves R1@IoU0.3 and R1@IoU0.5 scores of 27.82 and 18.68, respectively.

Conclusion: GazeNLQ effectively improves localization accuracy by integrating gaze cues.

Abstract: This report presents our solution to the Ego4D Natural Language Queries (NLQ)
Challenge at CVPR 2025. Egocentric video captures the scene from the wearer's
perspective, where gaze serves as a key non-verbal communication cue that
reflects visual attention and offer insights into human intention and
cognition. Motivated by this, we propose a novel approach, GazeNLQ, which
leverages gaze to retrieve video segments that match given natural language
queries. Specifically, we introduce a contrastive learning-based pretraining
strategy for gaze estimation directly from video. The estimated gaze is used to
augment video representations within proposed model, thereby enhancing
localization accuracy. Experimental results show that GazeNLQ achieves
R1@IoU0.3 and R1@IoU0.5 scores of 27.82 and 18.68, respectively. Our code is
available at https://github.com/stevenlin510/GazeNLQ.

</details>


### [197] [EASG-Bench: Video Q&A Benchmark with Egocentric Action Scene Graphs](https://arxiv.org/pdf/2506.05787)
*Ivan Rodin, Tz-Ying Wu, Kyle Min, Sharath Nittur Sridhar, Antonino Furnari, Subarna Tripathi, Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: EASG-Bench is a QA benchmark for egocentric videos using dynamic scene graphs, revealing performance gaps in language-only and video-LLMs, especially in temporal ordering questions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for spatio-temporally grounded QA in egocentric videos and evaluate model performance.

Method: Created QA pairs from dynamic scene graphs and evaluated language-only and video-LLMs systematically.

Result: Identified a performance gap, particularly in temporal ordering questions, highlighting a research gap in long-context video understanding.

Conclusion: EASG-Bench and code are released to promote reproducibility and further research in video understanding.

Abstract: We introduce EASG-Bench, a question-answering benchmark for egocentric videos
where the question-answering pairs are created from spatio-temporally grounded
dynamic scene graphs capturing intricate relationships among actors, actions,
and objects. We propose a systematic evaluation framework and evaluate several
language-only and video large language models (video-LLMs) on this benchmark.
We observe a performance gap in language-only and video-LLMs, especially on
questions focusing on temporal ordering, thus identifying a research gap in the
area of long-context video understanding. To promote the reproducibility of our
findings and facilitate further research, the benchmark and accompanying code
are available at the following GitHub page:
https://github.com/fpv-iplab/EASG-bench.

</details>


### [198] [FuseUNet: A Multi-Scale Feature Fusion Method for U-like Networks](https://arxiv.org/pdf/2506.05821)
*Quansong He, Xiangde Min, Kaishen Wang, Tao He*

Main category: cs.CV

TL;DR: The paper proposes a novel multi-scale feature fusion method for UNet-based medical image segmentation, addressing limitations in skip connections by treating them as discrete nodes and using an adaptive ODE method.


<details>
  <summary>Details</summary>
Motivation: Current UNet skip connections lack effective multi-scale feature interaction and rely on inefficient operations, limiting performance despite encoder/decoder improvements.

Method: The method reimagines UNet decoding as solving an initial value problem, using an adaptive ODE approach for multi-scale feature fusion, independent of encoder/decoder architectures.

Result: Experiments on multiple datasets (ACDC, KiTS2023, MSD brain tumor, ISIC2017/2018) show improved feature utilization, fewer parameters, and maintained high performance.

Conclusion: The proposed method enhances UNet skip connections, offering a flexible and efficient solution for medical image segmentation.

Abstract: Medical image segmentation is a critical task in computer vision, with UNet
serving as a milestone architecture. The typical component of UNet family is
the skip connection, however, their skip connections face two significant
limitations: (1) they lack effective interaction between features at different
scales, and (2) they rely on simple concatenation or addition operations, which
constrain efficient information integration. While recent improvements to UNet
have focused on enhancing encoder and decoder capabilities, these limitations
remain overlooked. To overcome these challenges, we propose a novel multi-scale
feature fusion method that reimagines the UNet decoding process as solving an
initial value problem (IVP), treating skip connections as discrete nodes. By
leveraging principles from the linear multistep method, we propose an adaptive
ordinary differential equation method to enable effective multi-scale feature
fusion. Our approach is independent of the encoder and decoder architectures,
making it adaptable to various U-Net-like networks. Experiments on ACDC,
KiTS2023, MSD brain tumor, and ISIC2017/2018 skin lesion segmentation datasets
demonstrate improved feature utilization, reduced network parameters, and
maintained high performance. The code is available at
https://github.com/nayutayuki/FuseUNet.

</details>


### [199] [LLIA -- Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven Portrait Video Generation with Diffusion Models](https://arxiv.org/pdf/2506.05806)
*Haojie Yu, Zhaonian Wang, Yihan Pan, Meng Cheng, Hao Yang, Chao Wang, Tao Xie, Xiaoming Xu, Xiaoming Wei, Xunliang Cai*

Main category: cs.CV

TL;DR: A novel audio-driven portrait video generation framework using diffusion models achieves real-time performance with low latency and high fidelity.


<details>
  <summary>Details</summary>
Motivation: Overcome the computational constraints of diffusion models for real-time interactive avatar applications.

Method: Proposes variable-length video generation, consistency model training, model quantization, pipeline parallelism, and a new inference strategy for stability.

Result: Achieves up to 78 FPS at 384x384 and 45 FPS at 512x512 with low latency (140 ms and 215 ms).

Conclusion: The framework enables fluid, authentic two-way communication with fine-grained facial expression control.

Abstract: Diffusion-based models have gained wide adoption in the virtual human
generation due to their outstanding expressiveness. However, their substantial
computational requirements have constrained their deployment in real-time
interactive avatar applications, where stringent speed, latency, and duration
requirements are paramount. We present a novel audio-driven portrait video
generation framework based on the diffusion model to address these challenges.
Firstly, we propose robust variable-length video generation to reduce the
minimum time required to generate the initial video clip or state transitions,
which significantly enhances the user experience. Secondly, we propose a
consistency model training strategy for Audio-Image-to-Video to ensure
real-time performance, enabling a fast few-step generation. Model quantization
and pipeline parallelism are further employed to accelerate the inference
speed. To mitigate the stability loss incurred by the diffusion process and
model quantization, we introduce a new inference strategy tailored for
long-duration video generation. These methods ensure real-time performance and
low latency while maintaining high-fidelity output. Thirdly, we incorporate
class labels as a conditional input to seamlessly switch between speaking,
listening, and idle states. Lastly, we design a novel mechanism for
fine-grained facial expression control to exploit our model's inherent
capacity. Extensive experiments demonstrate that our approach achieves
low-latency, fluid, and authentic two-way communication. On an NVIDIA RTX
4090D, our model achieves a maximum of 78 FPS at a resolution of 384x384 and 45
FPS at a resolution of 512x512, with an initial video generation latency of 140
ms and 215 ms, respectively.

</details>


### [200] [NTIRE 2025 Challenge on HR Depth from Images of Specular and Transparent Surfaces](https://arxiv.org/pdf/2506.05815)
*Pierluigi Zama Ramirez, Fabio Tosi, Luigi Di Stefano, Radu Timofte, Alex Costanzino, Matteo Poggi, Samuele Salti, Stefano Mattoccia, Zhe Zhang, Yang Yang, Wu Chen, Anlong Ming, Mingshuai Zhao, Mengying Yu, Shida Gao, Xiangfeng Wang, Feng Xue, Jun Shi, Yong Yang, Yong A, Yixiang Jin, Dingzhe Li, Aryan Shukla, Liam Frija-Altarac, Matthew Toews, Hui Geng, Tianjiao Wan, Zijian Gao, Qisheng Xu, Kele Xu, Zijian Zang, Jameer Babu Pinjari, Kuldeep Purohit, Mykola Lavreniuk, Jing Cao, Shenyi Li, Kui Jiang, Junjun Jiang, Yong Huang*

Main category: cs.CV

TL;DR: The NTIRE 2025 challenge focused on high-resolution depth estimation from specular and transparent surfaces, addressing key issues in the field.


<details>
  <summary>Details</summary>
Motivation: To advance research in depth estimation, particularly for high-resolution and non-Lambertian surfaces.

Method: Two tracks were proposed: stereo and single-image depth estimation, with 177 registered participants.

Result: 4 teams submitted models for each track in the final testing stage.

Conclusion: The challenge successfully addressed critical issues in depth estimation and engaged significant participation.

Abstract: This paper reports on the NTIRE 2025 challenge on HR Depth From images of
Specular and Transparent surfaces, held in conjunction with the New Trends in
Image Restoration and Enhancement (NTIRE) workshop at CVPR 2025. This challenge
aims to advance the research on depth estimation, specifically to address two
of the main open issues in the field: high-resolution and non-Lambertian
surfaces. The challenge proposes two tracks on stereo and single-image depth
estimation, attracting about 177 registered participants. In the final testing
stage, 4 and 4 participating teams submitted their models and fact sheets for
the two tracks.

</details>


### [201] [DeformCL: Learning Deformable Centerline Representation for Vessel Extraction in 3D Medical Image](https://arxiv.org/pdf/2506.05820)
*Ziwei Zhao, Zhixing Zhang, Yuhang Liu, Zhao Zhang, Haojun Yu, Dong Wang, Liwei Wang*

Main category: cs.CV

TL;DR: DeformCL introduces a continuous representation for 3D blood vessel extraction using deformable centerlines, improving connectivity, noise robustness, and interaction over traditional discrete methods.


<details>
  <summary>Details</summary>
Motivation: Existing discrete representations (e.g., masks) in 3D medical imaging often lead to fragmented vessel structures due to per-pixel classification limitations.

Method: DeformCL uses deformable centerlines with connected nodes and edges to represent vessels continuously, supported by a cascaded training pipeline.

Result: Experiments on four datasets show DeformCL's superiority in accuracy and robustness, with clinical validation via curved planar reformation images.

Conclusion: DeformCL outperforms traditional methods, offering a clinically significant and effective solution for 3D vessel segmentation.

Abstract: In the field of 3D medical imaging, accurately extracting and representing
the blood vessels with curvilinear structures holds paramount importance for
clinical diagnosis. Previous methods have commonly relied on discrete
representation like mask, often resulting in local fractures or scattered
fragments due to the inherent limitations of the per-pixel classification
paradigm. In this work, we introduce DeformCL, a new continuous representation
based on Deformable Centerlines, where centerline points act as nodes connected
by edges that capture spatial relationships. Compared with previous
representations, DeformCL offers three key advantages: natural connectivity,
noise robustness, and interaction facility. We present a comprehensive training
pipeline structured in a cascaded manner to fully exploit these favorable
properties of DeformCL. Extensive experiments on four 3D vessel segmentation
datasets demonstrate the effectiveness and superiority of our method.
Furthermore, the visualization of curved planar reformation images validates
the clinical significance of the proposed framework. We release the code in
https://github.com/barry664/DeformCL

</details>


### [202] [High Throughput Event Filtering: The Interpolation-based DIF Algorithm Hardware Architecture](https://arxiv.org/pdf/2506.05825)
*Marcin Kowalczyk, Tomasz Kryjak*

Main category: cs.CV

TL;DR: The paper proposes a hardware architecture for the DIF filter to address noise in event vision sensors, achieving high throughput and comparable performance to state-of-the-art solutions.


<details>
  <summary>Details</summary>
Motivation: Noise in event vision sensors varies with factors like illumination and temperature, necessitating efficient filtering solutions.

Method: The authors designed and implemented the DIF filter on an FPGA chip and evaluated it using a new high-resolution event dataset.

Result: The architecture achieved 403.39 MEPS (1280x720) and 428.45 MEPS (640x480) throughput, with AUROC scores of 0.844-0.999.

Conclusion: The DIF filter offers high throughput and robust performance across noise levels, comparable to existing solutions.

Abstract: In recent years, there has been rapid development in the field of event
vision. It manifests itself both on the technical side, as better and better
event sensors are available, and on the algorithmic side, as more and more
applications of this technology are proposed and scientific papers are
published. However, the data stream from these sensors typically contains a
significant amount of noise, which varies depending on factors such as the
degree of illumination in the observed scene or the temperature of the sensor.
We propose a hardware architecture of the Distance-based Interpolation with
Frequency Weights (DIF) filter and implement it on an FPGA chip. To evaluate
the algorithm and compare it with other solutions, we have prepared a new
high-resolution event dataset, which we are also releasing to the community.
Our architecture achieved a throughput of 403.39 million events per second
(MEPS) for a sensor resolution of 1280 x 720 and 428.45 MEPS for a resolution
of 640 x 480. The average values of the Area Under the Receiver Operating
Characteristic (AUROC) index ranged from 0.844 to 0.999, depending on the
dataset, which is comparable to the state-of-the-art filtering solutions, but
with much higher throughput and better operation over a wide range of noise
levels.

</details>


### [203] [Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025](https://arxiv.org/pdf/2506.05856)
*Yuqian Fu, Runze Wang, Yanwei Fu, Danda Pani Paudel, Luc Van Gool*

Main category: cs.CV

TL;DR: A cross-view multi-modal object segmentation method for the Ego-Exo4D Correspondence Challenges 2025, combining visual masks and textual descriptions, achieving second place on the benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting corresponding object masks across different perspectives (ego and exo views) by leveraging multimodal data.

Method: Proposes a multimodal condition fusion module for enhanced object localization and a cross-view object alignment module to ensure consistency across perspectives.

Result: Ranked second on the Ego-Exo4D object correspondence benchmark.

Conclusion: The method effectively improves robustness to viewpoint changes and demonstrates strong performance in cross-view object segmentation.

Abstract: In this report, we present a cross-view multi-modal object segmentation
approach for the object correspondence task in the Ego-Exo4D Correspondence
Challenges 2025. Given object queries from one perspective (e.g., ego view),
the goal is to predict the corresponding object masks in another perspective
(e.g., exo view). To tackle this task, we propose a multimodal condition fusion
module that enhances object localization by leveraging both visual masks and
textual descriptions as segmentation conditions. Furthermore, to address the
visual domain gap between ego and exo views, we introduce a cross-view object
alignment module that enforces object-level consistency across perspectives,
thereby improving the model's robustness to viewpoint changes. Our proposed
method ranked second on the leaderboard of the large-scale Ego-Exo4D object
correspondence benchmark. Code will be made available at
https://github.com/lovelyqian/ObjectRelator.

</details>


### [204] [FontAdapter: Instant Font Adaptation in Visual Text Generation](https://arxiv.org/pdf/2506.05843)
*Myungkyu Koo, Subin Kim, Sangkyung Kwak, Jaehyun Nam, Seojin Kim, Jinwoo Shin*

Main category: cs.CV

TL;DR: FontAdapter enables fast, high-quality font customization for unseen fonts using a two-stage curriculum learning approach, avoiding costly fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adapting unseen fonts are slow and impractical for real-time use, requiring fine-tuning with predefined fonts.

Method: FontAdapter uses a two-stage curriculum: first learning font attributes from isolated glyphs, then integrating styles into natural backgrounds. Synthetic datasets support each stage.

Result: FontAdapter achieves robust font customization without fine-tuning, supporting text editing, style blending, and cross-lingual transfer.

Conclusion: FontAdapter is a versatile, efficient framework for real-time font customization tasks.

Abstract: Text-to-image diffusion models have significantly improved the seamless
integration of visual text into diverse image contexts. Recent approaches
further improve control over font styles through fine-tuning with predefined
font dictionaries. However, adapting unseen fonts outside the preset is
computationally expensive, often requiring tens of minutes, making real-time
customization impractical. In this paper, we present FontAdapter, a framework
that enables visual text generation in unseen fonts within seconds, conditioned
on a reference glyph image. To this end, we find that direct training on font
datasets fails to capture nuanced font attributes, limiting generalization to
new glyphs. To overcome this, we propose a two-stage curriculum learning
approach: FontAdapter first learns to extract font attributes from isolated
glyphs and then integrates these styles into diverse natural backgrounds. To
support this two-stage training scheme, we construct synthetic datasets
tailored to each stage, leveraging large-scale online fonts effectively.
Experiments demonstrate that FontAdapter enables high-quality, robust font
customization across unseen fonts without additional fine-tuning during
inference. Furthermore, it supports visual text editing, font style blending,
and cross-lingual font transfer, positioning FontAdapter as a versatile
framework for font customization tasks.

</details>


### [205] [ChronoTailor: Harnessing Attention Guidance for Fine-Grained Video Virtual Try-On](https://arxiv.org/pdf/2506.05858)
*Jinjuan Wang, Wenzhang Sun, Ming Li, Yun Zheng, Fanyao Li, Zhulin Tao, Donglin Di, Hao Li, Wei Chen, Xianglin Huang*

Main category: cs.CV

TL;DR: ChronoTailor is a diffusion-based framework for video virtual try-on, ensuring temporal consistency and fine-grained garment details through spatio-temporal attention and multi-scale feature integration.


<details>
  <summary>Details</summary>
Motivation: Existing video virtual try-on methods struggle with maintaining continuity and garment detail reproduction, prompting the need for a more robust solution.

Method: ChronoTailor uses region-aware spatial guidance, attention-driven temporal feature fusion, multi-scale garment features, and garment-pose alignment to enhance performance.

Result: The framework outperforms previous methods in maintaining spatio-temporal continuity and preserving garment details, validated by extensive experiments.

Conclusion: ChronoTailor advances video virtual try-on by addressing continuity and detail challenges, supported by the new StyleDress dataset.

Abstract: Video virtual try-on aims to seamlessly replace the clothing of a person in a
source video with a target garment. Despite significant progress in this field,
existing approaches still struggle to maintain continuity and reproduce garment
details. In this paper, we introduce ChronoTailor, a diffusion-based framework
that generates temporally consistent videos while preserving fine-grained
garment details. By employing a precise spatio-temporal attention mechanism to
guide the integration of fine-grained garment features, ChronoTailor achieves
robust try-on performance. First, ChronoTailor leverages region-aware spatial
guidance to steer the evolution of spatial attention and employs an
attention-driven temporal feature fusion mechanism to generate more continuous
temporal features. This dual approach not only enables fine-grained local
editing but also effectively mitigates artifacts arising from video dynamics.
Second, ChronoTailor integrates multi-scale garment features to preserve
low-level visual details and incorporates a garment-pose feature alignment to
ensure temporal continuity during dynamic motion. Additionally, we collect
StyleDress, a new dataset featuring intricate garments, varied environments,
and diverse poses, offering advantages over existing public datasets, and will
be publicly available for research. Extensive experiments show that
ChronoTailor maintains spatio-temporal continuity and preserves garment details
during motion, significantly outperforming previous methods.

</details>


### [206] [Improved Allergy Wheal Detection for the Skin Prick Automated Test Device](https://arxiv.org/pdf/2506.05862)
*Rembert Daems, Sven Seys, Valérie Hox, Adam Chaker, Glynnis De Greve, Winde Lemmens, Anne-Lise Poirrier, Eline Beckers, Zuzana Diamant, Carmen Dierickx, Peter W. Hellings, Caroline Huart, Claudia Jerin, Mark Jorissen, Hanne Oscé, Karolien Roux, Mark Thompson, Sophie Tombu, Saartje Uyttebroek, Andrzej Zarowski, Senne Gorris, Laura Van Gerven, Dirk Loeckx, Thomas Demeester*

Main category: cs.CV

TL;DR: The paper introduces an automated method (SPAT) for detecting and delineating allergy wheals using 32 images under distinct lighting conditions, outperforming single-image methods.


<details>
  <summary>Details</summary>
Motivation: To improve consistency and accuracy in diagnosing inhalant allergies by leveraging multiple images under varied lighting conditions.

Method: Combines a neural network for pixel-level wheal segmentation with an algorithmic approach for detection and delineation, using manually annotated data from 868 patients.

Result: The method, tested on 217 patients, shows higher accuracy with 32 SPAT images compared to single-image baselines.

Conclusion: Using multiple images under varied lighting significantly enhances diagnostic accuracy for inhalant allergies.

Abstract: Background: The skin prick test (SPT) is the gold standard for diagnosing
sensitization to inhalant allergies. The Skin Prick Automated Test (SPAT)
device was designed for increased consistency in test results, and captures 32
images to be jointly used for allergy wheal detection and delineation, which
leads to a diagnosis.
  Materials and Methods: Using SPAT data from $868$ patients with suspected
inhalant allergies, we designed an automated method to detect and delineate
wheals on these images. To this end, $10,416$ wheals were manually annotated by
drawing detailed polygons along the edges. The unique data-modality of the SPAT
device, with $32$ images taken under distinct lighting conditions, requires a
custom-made approach. Our proposed method consists of two parts: a neural
network component that segments the wheals on the pixel level, followed by an
algorithmic and interpretable approach for detecting and delineating the
wheals.
  Results: We evaluate the performance of our method on a hold-out validation
set of $217$ patients. As a baseline we use a single conventionally lighted
image per SPT as input to our method.
  Conclusion: Using the $32$ SPAT images under various lighting conditions
offers a considerably higher accuracy than a single image in conventional,
uniform light.

</details>


### [207] [CryoFastAR: Fast Cryo-EM Ab Initio Reconstruction Made Easy](https://arxiv.org/pdf/2506.05864)
*Jiakai Zhang, Shouchen Zhou, Haizhao Dai, Xinhang Liu, Peihao Wang, Zhiwen Fan, Yuan Pei, Jingyi Yu*

Main category: cs.CV

TL;DR: CryoFastAR is a geometric foundation model for fast pose estimation in cryo-EM, outperforming traditional iterative methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Pose estimation in cryo-EM is slow due to low SNR and CTF distortions; CryoFastAR aims to address this with direct prediction.

Method: Integrates multi-view features, trains on simulated cryo-EM data with realistic noise, and uses progressive training for stability.

Result: Achieves comparable quality to iterative methods while significantly speeding up inference on synthetic and real datasets.

Conclusion: CryoFastAR offers a faster, robust solution for cryo-EM pose estimation, advancing 3D reconstruction in scientific imaging.

Abstract: Pose estimation from unordered images is fundamental for 3D reconstruction,
robotics, and scientific imaging. Recent geometric foundation models, such as
DUSt3R, enable end-to-end dense 3D reconstruction but remain underexplored in
scientific imaging fields like cryo-electron microscopy (cryo-EM) for
near-atomic protein reconstruction. In cryo-EM, pose estimation and 3D
reconstruction from unordered particle images still depend on time-consuming
iterative optimization, primarily due to challenges such as low signal-to-noise
ratios (SNR) and distortions from the contrast transfer function (CTF). We
introduce CryoFastAR, the first geometric foundation model that can directly
predict poses from Cryo-EM noisy images for Fast ab initio Reconstruction. By
integrating multi-view features and training on large-scale simulated cryo-EM
data with realistic noise and CTF modulations, CryoFastAR enhances pose
estimation accuracy and generalization. To enhance training stability, we
propose a progressive training strategy that first allows the model to extract
essential features under simpler conditions before gradually increasing
difficulty to improve robustness. Experiments show that CryoFastAR achieves
comparable quality while significantly accelerating inference over traditional
iterative approaches on both synthetic and real datasets.

</details>


### [208] [HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios](https://arxiv.org/pdf/2506.05883)
*Daming Wang, Yuhao Song, Zijian He, Kangliang Chen, Xing Pan, Lu Deng, Weihao Gu*

Main category: cs.CV

TL;DR: HaoMo Vision-Language Model (HMVLM) is a driving framework with a fast-slow architecture, achieving high performance in the Waymo E2E Driving Challenge.


<details>
  <summary>Details</summary>
Motivation: To improve autonomous driving by combining fast low-level control with slow high-level planning using vision-language models.

Method: Uses selective five-view prompting, multi-stage chain-of-thought prompting, and spline-based trajectory post-processing.

Result: Achieved a Rater Feedback Score of 7.7367, securing 2nd place in the 2025 Waymo Challenge, surpassing the baseline by 2.77%.

Conclusion: HMVLM demonstrates the effectiveness of vision-language models in autonomous driving, balancing latency and decision-making.

Abstract: We present HaoMo Vision-Language Model (HMVLM), an end-to-end driving
framework that implements the slow branch of a cognitively inspired fast-slow
architecture. A fast controller outputs low-level steering, throttle, and brake
commands, while a slow planner-a large vision-language model-generates
high-level intents such as "yield to pedestrian" or "merge after the truck"
without compromising latency. HMVLM introduces three upgrades: (1) selective
five-view prompting with an embedded 4s history of ego kinematics, (2)
multi-stage chain-of-thought (CoT) prompting that enforces a Scene
Understanding -> Driving Decision -> Trajectory Inference reasoning flow, and
(3) spline-based trajectory post-processing that removes late-stage jitter and
sharp turns. Trained on the Waymo Open Dataset, these upgrades enable HMVLM to
achieve a Rater Feedback Score (RFS) of 7.7367, securing 2nd place in the 2025
Waymo Vision-based End-to-End (E2E) Driving Challenge and surpassing the public
baseline by 2.77%.

</details>


### [209] [Domain-RAG: Retrieval-Guided Compositional Image Generation for Cross-Domain Few-Shot Object Detection](https://arxiv.org/pdf/2506.05872)
*Yu Li, Xingyu Qiu, Yuqian Fu, Jie Chen, Tianwen Qian, Xu Zheng, Danda Pani Paudel, Yanwei Fu, Xuanjing Huang, Luc Van Gool, Yu-Gang Jiang*

Main category: cs.CV

TL;DR: Proposes Domain-RAG, a retrieval-guided framework for Cross-Domain Few-Shot Object Detection (CD-FSOD), generating domain-aligned images without training.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of preserving object category and domain alignment in CD-FSOD, where existing methods like copy-paste or text-to-image generation fall short.

Method: Domain-RAG uses domain-aware background retrieval, domain-guided background generation, and foreground-background composition to create realistic, domain-consistent images.

Result: Achieves state-of-the-art performance in CD-FSOD, remote sensing FSOD, and camouflaged FSOD without additional training.

Conclusion: Domain-RAG is a training-free, effective solution for generating high-quality, domain-aligned samples in CD-FSOD.

Abstract: Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel objects
with only a handful of labeled samples from previously unseen domains. While
data augmentation and generative methods have shown promise in few-shot
learning, their effectiveness for CD-FSOD remains unclear due to the need for
both visual realism and domain alignment. Existing strategies, such as
copy-paste augmentation and text-to-image generation, often fail to preserve
the correct object category or produce backgrounds coherent with the target
domain, making them non-trivial to apply directly to CD-FSOD. To address these
challenges, we propose Domain-RAG, a training-free, retrieval-guided
compositional image generation framework tailored for CD-FSOD. Domain-RAG
consists of three stages: domain-aware background retrieval, domain-guided
background generation, and foreground-background composition. Specifically, the
input image is first decomposed into foreground and background regions. We then
retrieve semantically and stylistically similar images to guide a generative
model in synthesizing a new background, conditioned on both the original and
retrieved contexts. Finally, the preserved foreground is composed with the
newly generated domain-aligned background to form the generated image. Without
requiring any additional supervision or training, Domain-RAG produces
high-quality, domain-consistent samples across diverse tasks, including
CD-FSOD, remote sensing FSOD, and camouflaged FSOD. Extensive experiments show
consistent improvements over strong baselines and establish new
state-of-the-art results. Codes will be released upon acceptance.

</details>


### [210] [Unleashing the Potential of Consistency Learning for Detecting and Grounding Multi-Modal Media Manipulation](https://arxiv.org/pdf/2506.05890)
*Yiheng Li, Yang Yang, Zichang Tan, Huan Liu, Weihua Chen, Xu Zhou, Zhen Lei*

Main category: cs.CV

TL;DR: Proposes CSCL, a method for detecting fake news by enhancing fine-grained perception of forgery in multi-modal media using contextual-semantic consistency learning.


<details>
  <summary>Details</summary>
Motivation: Current methods lack fine-grained consistency analysis, leading to unreliable forgery detection in multi-modal media.

Method: Uses two branches (image and text) with cascaded decoders (CCD and SCD) to capture within-modality contextual and across-modality semantic consistency.

Result: Achieves state-of-the-art performance on DGM4 datasets, especially in grounding manipulated content.

Conclusion: CSCL effectively improves forgery detection by leveraging fine-grained consistency learning.

Abstract: To tackle the threat of fake news, the task of detecting and grounding
multi-modal media manipulation DGM4 has received increasing attention. However,
most state-of-the-art methods fail to explore the fine-grained consistency
within local content, usually resulting in an inadequate perception of detailed
forgery and unreliable results. In this paper, we propose a novel approach
named Contextual-Semantic Consistency Learning (CSCL) to enhance the
fine-grained perception ability of forgery for DGM4. Two branches for image and
text modalities are established, each of which contains two cascaded decoders,
i.e., Contextual Consistency Decoder (CCD) and Semantic Consistency Decoder
(SCD), to capture within-modality contextual consistency and across-modality
semantic consistency, respectively. Both CCD and SCD adhere to the same
criteria for capturing fine-grained forgery details. To be specific, each
module first constructs consistency features by leveraging additional
supervision from the heterogeneous information of each token pair. Then, the
forgery-aware reasoning or aggregating is adopted to deeply seek forgery cues
based on the consistency features. Extensive experiments on DGM4 datasets prove
that CSCL achieves new state-of-the-art performance, especially for the results
of grounding manipulated content. Codes and weights are avaliable at
https://github.com/liyih/CSCL.

</details>


### [211] [Rethinking Semi-supervised Segmentation Beyond Accuracy: Reliability and Robustness](https://arxiv.org/pdf/2506.05917)
*Steven Landgraf, Markus Hillemann, Markus Ulrich*

Main category: cs.CV

TL;DR: The paper introduces the Reliable Segmentation Score (RSS) to evaluate semi-supervised segmentation models holistically, addressing gaps in current protocols that overlook reliability and robustness.


<details>
  <summary>Details</summary>
Motivation: Current evaluation protocols for semi-supervised segmentation focus only on accuracy, neglecting reliability and robustness, which are crucial for safety-critical applications like autonomous driving.

Method: The authors propose RSS, a novel metric combining accuracy, calibration, and uncertainty quality via a harmonic mean, penalizing deficiencies in any component.

Result: Evaluations show semi-supervised methods often sacrifice reliability for accuracy, with UniMatchV2 demonstrating robustness but still facing reliability issues.

Conclusion: The paper advocates for holistic metrics like RSS to better align research with real-world deployment needs.

Abstract: Semantic segmentation is critical for scene understanding but demands costly
pixel-wise annotations, attracting increasing attention to semi-supervised
approaches to leverage abundant unlabeled data. While semi-supervised
segmentation is often promoted as a path toward scalable, real-world
deployment, it is astonishing that current evaluation protocols exclusively
focus on segmentation accuracy, entirely overlooking reliability and
robustness. These qualities, which ensure consistent performance under diverse
conditions (robustness) and well-calibrated model confidences as well as
meaningful uncertainties (reliability), are essential for safety-critical
applications like autonomous driving, where models must handle unpredictable
environments and avoid sudden failures at all costs. To address this gap, we
introduce the Reliable Segmentation Score (RSS), a novel metric that combines
predictive accuracy, calibration, and uncertainty quality measures via a
harmonic mean. RSS penalizes deficiencies in any of its components, providing
an easy and intuitive way of holistically judging segmentation models.
Comprehensive evaluations of UniMatchV2 against its predecessor and a
supervised baseline show that semi-supervised methods often trade reliability
for accuracy. While out-of-domain evaluations demonstrate UniMatchV2's
robustness, they further expose persistent reliability shortcomings. We
advocate for a shift in evaluation protocols toward more holistic metrics like
RSS to better align semi-supervised learning research with real-world
deployment needs.

</details>


### [212] [Query Nearby: Offset-Adjusted Mask2Former enhances small-organ segmentation](https://arxiv.org/pdf/2506.05897)
*Xin Zhang, Dongdong Meng, Sheng Li*

Main category: cs.CV

TL;DR: The paper proposes a modified Mask2Former model with deformable attention and offset adjustment strategies to improve medical image segmentation, achieving SOTA performance on mid-sized and small organs.


<details>
  <summary>Details</summary>
Motivation: Medical segmentation is challenging, especially for small organs, and transformer-based models are resource-intensive. The goal is to enhance segmentation accuracy while reducing computational demands.

Method: The authors used Mask2Former with deformable attention, offset adjustment strategies, a 4th feature map for organ location, and an FCN-based auxiliary head trained with Dice loss.

Result: The model achieves SOTA performance on HaNSeg and SegRap2023 datasets, particularly for mid-sized and small organs.

Conclusion: The proposed approach effectively addresses the limitations of transformer-based models in medical segmentation, improving accuracy and efficiency.

Abstract: Medical segmentation plays an important role in clinical applications like
radiation therapy and surgical guidance, but acquiring clinically acceptable
results is difficult. In recent years, progress has been witnessed with the
success of utilizing transformer-like models, such as combining the attention
mechanism with CNN. In particular, transformer-based segmentation models can
extract global information more effectively, compensating for the drawbacks of
CNN modules that focus on local features. However, utilizing transformer
architecture is not easy, because training transformer-based models can be
resource-demanding. Moreover, due to the distinct characteristics in the
medical field, especially when encountering mid-sized and small organs with
compact regions, their results often seem unsatisfactory. For example, using
ViT to segment medical images directly only gives a DSC of less than 50\%,
which is far lower than the clinically acceptable score of 80\%. In this paper,
we used Mask2Former with deformable attention to reduce computation and
proposed offset adjustment strategies to encourage sampling points within the
same organs during attention weights computation, thereby integrating compact
foreground information better. Additionally, we utilized the 4th feature map in
Mask2Former to provide a coarse location of organs, and employed an FCN-based
auxiliary head to help train Mask2Former more quickly using Dice loss. We show
that our model achieves SOTA (State-of-the-Art) performance on the HaNSeg and
SegRap2023 datasets, especially on mid-sized and small organs.Our code is
available at link
https://github.com/earis/Offsetadjustment\_Background-location\_Decoder\_Mask2former.

</details>


### [213] [FADE: Frequency-Aware Diffusion Model Factorization for Video Editing](https://arxiv.org/pdf/2506.05934)
*Yixuan Zhu, Haolin Wang, Shilin Ma, Wenliang Zhao, Yansong Tang, Lei Chen, Jie Zhou*

Main category: cs.CV

TL;DR: FADE introduces a training-free video editing method using frequency-aware factorization and spectrum-guided modulation for high-quality, temporally coherent results.


<details>
  <summary>Details</summary>
Motivation: Current video diffusion models struggle with efficient editing due to computational demands, and image-based methods fail to handle video dynamics.

Method: Analyzes attention patterns in pre-trained models, proposes factorization for component optimization, and uses spectrum-guided modulation for refined sampling.

Result: Produces high-quality, realistic, and temporally coherent edits, outperforming conventional methods.

Conclusion: FADE effectively leverages video priors for efficient and versatile editing without additional training.

Abstract: Recent advancements in diffusion frameworks have significantly enhanced video
editing, achieving high fidelity and strong alignment with textual prompts.
However, conventional approaches using image diffusion models fall short in
handling video dynamics, particularly for challenging temporal edits like
motion adjustments. While current video diffusion models produce high-quality
results, adapting them for efficient editing remains difficult due to the heavy
computational demands that prevent the direct application of previous image
editing techniques. To overcome these limitations, we introduce FADE, a
training-free yet highly effective video editing approach that fully leverages
the inherent priors from pre-trained video diffusion models via frequency-aware
factorization. Rather than simply using these models, we first analyze the
attention patterns within the video model to reveal how video priors are
distributed across different components. Building on these insights, we propose
a factorization strategy to optimize each component's specialized role.
Furthermore, we devise spectrum-guided modulation to refine the sampling
trajectory with frequency domain cues, preventing information leakage and
supporting efficient, versatile edits while preserving the basic spatial and
temporal structure. Extensive experiments on real-world videos demonstrate that
our method consistently delivers high-quality, realistic and temporally
coherent editing results both qualitatively and quantitatively. Code is
available at https://github.com/EternalEvan/FADE .

</details>


### [214] [MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation](https://arxiv.org/pdf/2506.05952)
*Dongjie Fu, Tengjiao Sun, Pengcheng Fang, Xiaohao Cai, Hansung Kim*

Main category: cs.CV

TL;DR: MOGO is a novel autoregressive framework for efficient, real-time 3D motion generation, combining MoSA-VQ for compact motion representation and RQHC-Transformer for low-latency token generation. It outperforms state-of-the-art methods in quality and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based motion generation methods struggle with balancing high fidelity, real-time responsiveness, and scalability.

Method: MOGO uses MoSA-VQ for hierarchical motion discretization and RQHC-Transformer for efficient token generation in one pass, with a text alignment mechanism for better semantic control.

Result: MOGO achieves competitive or superior motion quality on HumanML3D, KIT-ML, and CMP datasets, with improved real-time performance and zero-shot generalization.

Conclusion: MOGO addresses key challenges in motion generation, offering high fidelity, real-time capability, and scalability, making it a promising solution for practical applications.

Abstract: Recent advances in transformer-based text-to-motion generation have led to
impressive progress in synthesizing high-quality human motion. Nevertheless,
jointly achieving high fidelity, streaming capability, real-time
responsiveness, and scalability remains a fundamental challenge. In this paper,
we propose MOGO (Motion Generation with One-pass), a novel autoregressive
framework tailored for efficient and real-time 3D motion generation. MOGO
comprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual
vector quantization module that hierarchically discretizes motion sequences
with learnable scaling to produce compact yet expressive representations; and
(2) RQHC-Transformer, a residual quantized hierarchical causal transformer that
generates multi-layer motion tokens in a single forward pass, significantly
reducing inference latency. To enhance semantic fidelity, we further introduce
a text condition alignment mechanism that improves motion decoding under
textual control. Extensive experiments on benchmark datasets including
HumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or
superior generation quality compared to state-of-the-art transformer-based
methods, while offering substantial improvements in real-time performance,
streaming generation, and generalization under zero-shot settings.

</details>


### [215] [Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments](https://arxiv.org/pdf/2506.05965)
*Mingrui Li, Yiming Zhou, Hongxing Zhou, Xinggang Hu, Florian Roemer, Hongyu Wang, Ahmad Osman*

Main category: cs.CV

TL;DR: Dy3DGS-SLAM is a novel 3D Gaussian Splatting SLAM method for dynamic scenes using monocular RGB input, outperforming existing RGB-D methods.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF-based SLAM methods struggle with dynamic environments and rely on RGB-D inputs, limiting their applicability.

Method: The approach fuses optical flow and depth masks probabilistically, introduces a motion loss for pose estimation, and uses rendering loss for mapping to handle dynamic objects.

Result: Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic environments, matching or surpassing RGB-D methods.

Conclusion: The method successfully addresses dynamic scene challenges with monocular RGB input, offering improved performance and versatility.

Abstract: Current Simultaneous Localization and Mapping (SLAM) methods based on Neural
Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static
3D scenes but struggle with tracking and reconstruction in dynamic
environments, such as real-world scenes with moving elements. Existing
NeRF-based SLAM approaches addressing dynamic challenges typically rely on
RGB-D inputs, with few methods accommodating pure RGB input. To overcome these
limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS)
SLAM method for dynamic scenes using monocular RGB input. To address dynamic
interference, we fuse optical flow masks and depth masks through a
probabilistic model to obtain a fused dynamic mask. With only a single network
iteration, this can constrain tracking scales and refine rendered geometry.
Based on the fused dynamic mask, we designed a novel motion loss to constrain
the pose estimation network for tracking. In mapping, we use the rendering loss
of dynamic pixels, color, and depth to eliminate transient interference and
occlusion caused by dynamic objects. Experimental results demonstrate that
Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic
environments, outperforming or matching existing RGB-D methods.

</details>


### [216] [Domain Adaptation in Agricultural Image Analysis: A Comprehensive Review from Shallow Models to Deep Learning](https://arxiv.org/pdf/2506.05972)
*Xing Hu, Siyuan Chen, Dawei Zhang*

Main category: cs.CV

TL;DR: The paper reviews Domain Adaptation (DA) techniques to address domain shifts in agricultural image analysis, enhancing cross-domain transferability for tasks like crop health monitoring and pest detection.


<details>
  <summary>Details</summary>
Motivation: Domain shifts due to environmental differences, crop types, and data acquisition methods limit model generalization in agriculture. DA can mitigate these challenges.

Method: Systematic review of DA methods, categorized into shallow and deep learning models (supervised, semi-supervised, unsupervised), with emphasis on adversarial learning.

Result: DA improves performance in crop health monitoring, pest detection, and fruit recognition across regions and seasons. Key public datasets are analyzed.

Conclusion: The review provides a framework for researchers, highlighting gaps and supporting DA advancement in agricultural image analysis.

Abstract: With the increasing use of computer vision in agriculture, image analysis has
become crucial for tasks like crop health monitoring and pest detection.
However, significant domain shifts between source and target domains-due to
environmental differences, crop types, and data acquisition methods-pose
challenges. These domain gaps limit the ability of models to generalize across
regions, seasons, and complex agricultural environments. This paper explores
how Domain Adaptation (DA) techniques can address these challenges, focusing on
their role in enhancing the cross-domain transferability of agricultural image
analysis. DA has gained attention in agricultural vision tasks due to its
potential to mitigate domain heterogeneity. The paper systematically reviews
recent advances in DA for agricultural imagery, particularly its practical
applications in complex agricultural environments. We examine the key drivers
for adopting DA in agriculture, such as limited labeled data, weak model
transferability, and dynamic environmental conditions. We also discuss its use
in crop health monitoring, pest detection, and fruit recognition, highlighting
improvements in performance across regions and seasons. The paper categorizes
DA methods into shallow and deep learning models, with further divisions into
supervised, semi-supervised, and unsupervised approaches. A special focus is
given to adversarial learning-based DA methods, which have shown great promise
in challenging agricultural scenarios. Finally, we review key public datasets
in agricultural imagery, analyzing their value and limitations in DA research.
This review provides a comprehensive framework for researchers, offering
insights into current research gaps and supporting the advancement of DA
methods in agricultural image analysis.

</details>


### [217] [MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks](https://arxiv.org/pdf/2506.05982)
*Zonglin Wu, Yule Xue, Xin Wei, Yiren Song*

Main category: cs.CV

TL;DR: The paper introduces MCA-Bench, a unified benchmark for evaluating CAPTCHA security across diverse modalities, revealing vulnerabilities and proposing design principles.


<details>
  <summary>Details</summary>
Motivation: The lack of a standardized, large-scale, multimodal benchmark for CAPTCHA security evaluation motivates the creation of MCA-Bench.

Method: MCA-Bench integrates various CAPTCHA types into a single protocol, using a shared vision-language model to fine-tune cracking agents for cross-modal assessment.

Result: Experiments show MCA-Bench effectively maps CAPTCHA vulnerabilities and analyzes the interplay of challenge complexity, interaction depth, and solvability.

Conclusion: The study proposes design principles and identifies open challenges, aiming to improve CAPTCHA security and benchmarking.

Abstract: As automated attack techniques rapidly advance, CAPTCHAs remain a critical
defense mechanism against malicious bots. However, existing CAPTCHA schemes
encompass a diverse range of modalities -- from static distorted text and
obfuscated images to interactive clicks, sliding puzzles, and logic-based
questions -- yet the community still lacks a unified, large-scale, multimodal
benchmark to rigorously evaluate their security robustness. To address this
gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking
suite that integrates heterogeneous CAPTCHA types into a single evaluation
protocol. Leveraging a shared vision-language model backbone, we fine-tune
specialized cracking agents for each CAPTCHA category, enabling consistent,
cross-modal assessments. Extensive experiments reveal that MCA-Bench
effectively maps the vulnerability spectrum of modern CAPTCHA designs under
varied attack settings, and crucially offers the first quantitative analysis of
how challenge complexity, interaction depth, and model solvability interrelate.
Based on these findings, we propose three actionable design principles and
identify key open challenges, laying the groundwork for systematic CAPTCHA
hardening, fair benchmarking, and broader community collaboration. Datasets and
code are available online.

</details>


### [218] [Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models](https://arxiv.org/pdf/2506.06006)
*Yifu Qiu, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti*

Main category: cs.CV

TL;DR: Fine-tuning vision-and-language foundation models for dynamics models is easier than for world models, and dynamics models can bootstrap world models via synthetic data and inference-time verification, achieving competitive performance in action-centric image editing.


<details>
  <summary>Details</summary>
Motivation: To assess whether vision-and-language foundation models can realistically model world dynamics (observation-action-observation) and dynamics (observation-observation-action) when actions are language-based.

Method: Fine-tuning foundation models for dynamics models, then using them to bootstrap world models through weakly supervised learning from synthetic data and inference-time verification. A new objective weights image tokens by importance.

Result: The best model achieves competitive performance in action-centric image editing, improving by 15% on real-world subsets and excelling in human evaluations.

Conclusion: Dynamics models can effectively bootstrap world models, leading to improved performance in tasks like action-centric image editing.

Abstract: To what extent do vision-and-language foundation models possess a realistic
world model (observation $\times$ action $\rightarrow$ observation) and a
dynamics model (observation $\times$ observation $\rightarrow$ action), when
actions are expressed through language? While open-source foundation models
struggle with both, we find that fine-tuning them to acquire a dynamics model
through supervision is significantly easier than acquiring a world model. In
turn, dynamics models can be used to bootstrap world models through two main
strategies: 1) weakly supervised learning from synthetic data and 2) inference
time verification. Firstly, the dynamics model can annotate actions for
unlabelled pairs of video frame observations to expand the training data. We
further propose a new objective, where image tokens in observation pairs are
weighted by their importance, as predicted by a recognition model. Secondly,
the dynamics models can assign rewards to multiple samples of the world model
to score them, effectively guiding search at inference time. We evaluate the
world models resulting from both strategies through the task of action-centric
image editing on Aurora-Bench. Our best model achieves a performance
competitive with state-of-the-art image editing models, improving on them by a
margin of $15\%$ on real-world subsets according to GPT4o-as-judge, and
achieving the best average human evaluation across all subsets of Aurora-Bench.

</details>


### [219] [Enhancing Orthopox Image Classification Using Hybrid Machine Learning and Deep Learning Models](https://arxiv.org/pdf/2506.06007)
*Alejandro Puente-Castro, Enrique Fernandez-Blanco, Daniel Rivero, Andres Molares-Ulloa*

Main category: cs.CV

TL;DR: A hybrid ML and pretrained DL approach improves Orthopoxvirus classification from medical images, offering efficiency and robustness without data augmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional diagnostic methods for Orthopoxvirus infections are slow and require expertise, with limited and biased datasets. Automated, scalable solutions are needed.

Method: Combines Machine Learning models with pretrained Deep Learning models for deep feature extraction, avoiding data augmentation.

Result: Achieves excellent classification performance and efficiency, with strong generalization and robustness.

Conclusion: The approach is scalable and interpretable, suitable for real-world clinical use.

Abstract: Orthopoxvirus infections must be accurately classified from medical pictures
for an easy and early diagnosis and epidemic prevention. The necessity for
automated and scalable solutions is highlighted by the fact that traditional
diagnostic techniques can be time-consuming and require expert interpretation
and there are few and biased data sets of the different types of Orthopox. In
order to improve classification performance and lower computational costs, a
hybrid strategy is put forth in this paper that uses Machine Learning models
combined with pretrained Deep Learning models to extract deep feature
representations without the need for augmented data. The findings show that
this feature extraction method, when paired with other methods in the
state-of-the-art, produces excellent classification outcomes while preserving
training and inference efficiency. The proposed approach demonstrates strong
generalization and robustness across multiple evaluation settings, offering a
scalable and interpretable solution for real-world clinical deployment.

</details>


### [220] [Restereo: Diffusion stereo video generation and restoration](https://arxiv.org/pdf/2506.06023)
*Xingchang Huang, Ashish Kumar Singh, Florian Dubost, Cristina Nader Vasconcelos, Sakar Khattar, Liang Shi, Christian Theobalt, Cengiz Oztireli, Gurprit Singh*

Main category: cs.CV

TL;DR: A new pipeline for stereo video generation and enhancement, outperforming existing methods on low-resolution inputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on high-quality monocular videos, lacking solutions for low-quality inputs and consistent stereo enhancement.

Method: Fine-tuning on degraded data for restoration and conditioning on warped masks for consistent stereo generation.

Result: Outperforms existing methods in stereo video generation from low-resolution inputs.

Conclusion: The approach effectively combines stereo generation and restoration, applicable to real-world low-quality videos.

Abstract: Stereo video generation has been gaining increasing attention with recent
advancements in video diffusion models. However, most existing methods focus on
generating 3D stereoscopic videos from monocular 2D videos. These approaches
typically assume that the input monocular video is of high quality, making the
task primarily about inpainting occluded regions in the warped video while
preserving disoccluded areas. In this paper, we introduce a new pipeline that
not only generates stereo videos but also enhances both left-view and
right-view videos consistently with a single model. Our approach achieves this
by fine-tuning the model on degraded data for restoration, as well as
conditioning the model on warped masks for consistent stereo generation. As a
result, our method can be fine-tuned on a relatively small synthetic stereo
video datasets and applied to low-quality real-world videos, performing both
stereo video generation and restoration. Experiments demonstrate that our
method outperforms existing approaches both qualitatively and quantitatively in
stereo video generation from low-resolution inputs.

</details>


### [221] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/pdf/2506.06035)
*Shiyi Zhang, Dong Liang, Hairong Zheng, Yihang Zhou*

Main category: cs.CV

TL;DR: HAVIR, a novel method using two adapters (AutoKL and CLIP), effectively reconstructs complex visual stimuli from fMRI by combining topological and semantic information, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: The challenge of accurately reconstructing highly complex visual stimuli from fMRI due to their density, diversity, and multifaceted semantics.

Method: HAVIR employs two adapters: AutoKL for topological structures and CLIP for semantic information, fused by Versatile Diffusion. CLIP Adapter is trained with text captions and synthesized images.

Result: HAVIR successfully reconstructs structural and semantic features of visual stimuli, even in complex scenarios, surpassing current models.

Conclusion: HAVIR addresses the limitations of existing methods by integrating complementary representations, achieving superior reconstruction of complex visual stimuli.

Abstract: Reconstructing visual information from brain activity bridges the gap between
neuroscience and computer vision. Even though progress has been made in
decoding images from fMRI using generative models, a challenge remains in
accurately recovering highly complex visual stimuli. This difficulty stems from
their elemental density and diversity, sophisticated spatial structures, and
multifaceted semantic information.
  To address these challenges, we propose HAVIR that contains two adapters: (1)
The AutoKL Adapter transforms fMRI voxels into a latent diffusion prior,
capturing topological structures; (2) The CLIP Adapter converts the voxels to
CLIP text and image embeddings, containing semantic information. These
complementary representations are fused by Versatile Diffusion to generate the
final reconstructed image. To extract the most essential semantic information
from complex scenarios, the CLIP Adapter is trained with text captions
describing the visual stimuli and their corresponding semantic images
synthesized from these captions. The experimental results demonstrate that
HAVIR effectively reconstructs both structural features and semantic
information of visual stimuli even in complex scenarios, outperforming existing
models.

</details>


### [222] [O-MaMa @ EgoExo4D Correspondence Challenge: Learning Object Mask Matching between Egocentric and Exocentric Views](https://arxiv.org/pdf/2506.06026)
*Lorenzo Mur-Labadia, Maria Santos-Villafranca, Alejandro Perez-Yus, Jesus Bermudez-Cameo, Ruben Martinez-Cantin, Jose J. Guerrero*

Main category: cs.CV

TL;DR: The paper redefines cross-image segmentation as a mask matching task, introducing a method with four components: Mask-Context Encoder, Cross-Attention, Mask Matching loss, and Hard Negative Mining.


<details>
  <summary>Details</summary>
Motivation: To improve segmentation of specific objects across different views by treating it as a mask matching problem.

Method: 1. Mask-Context Encoder for object-level features. 2. Cross-Attention for multi-perspective fusion. 3. Mask Matching loss for feature alignment. 4. Hard Negative Mining for better differentiation.

Result: Not explicitly stated, but the method aims to enhance discriminative object-level representations and cross-view feature alignment.

Conclusion: The proposed approach reframes cross-image segmentation, leveraging advanced techniques for improved performance.

Abstract: The goal of the correspondence task is to segment specific objects across
different views. This technical report re-defines cross-image segmentation by
treating it as a mask matching task. Our method consists of: (1) A Mask-Context
Encoder that pools dense DINOv2 semantic features to obtain discriminative
object-level representations from FastSAM mask candidates, (2) an
Ego$\leftrightarrow$Exo Cross-Attention that fuses multi-perspective
observations, (3) a Mask Matching contrastive loss that aligns cross-view
features in a shared latent space, and (4) a Hard Negative Adjacent Mining
strategy to encourage the model to better differentiate between nearby objects.

</details>


### [223] [Sample-Specific Noise Injection For Diffusion-Based Adversarial Purification](https://arxiv.org/pdf/2506.06027)
*Yuhao Sun, Jiacheng Zhang, Zesheng Ye, Chaowei Xiao, Feng Liu*

Main category: cs.CV

TL;DR: The paper introduces SSNI, a framework for adaptive noise injection in diffusion-based purification, improving accuracy and robustness by adjusting noise levels per sample based on score norms.


<details>
  <summary>Details</summary>
Motivation: Existing DBP methods use a fixed noise level for all samples, but the paper finds that optimal noise levels vary per sample, motivating a sample-specific approach.

Method: SSNI uses a pre-trained score network to estimate deviations from clean data (score norms) and adaptively adjusts noise levels for each sample.

Result: SSNI improves accuracy and robustness on CIFAR-10 and ImageNet-1K when integrated with existing DBP methods.

Conclusion: Sample-specific noise injection is necessary for DBP methods, and SSNI effectively achieves this, enhancing performance.

Abstract: Diffusion-based purification (DBP) methods aim to remove adversarial noise
from the input sample by first injecting Gaussian noise through a forward
diffusion process, and then recovering the clean example through a reverse
generative process. In the above process, how much Gaussian noise is injected
to the input sample is key to the success of DBP methods, which is controlled
by a constant noise level $t^*$ for all samples in existing methods. In this
paper, we discover that an optimal $t^*$ for each sample indeed could be
different. Intuitively, the cleaner a sample is, the less the noise it should
be injected, and vice versa. Motivated by this finding, we propose a new
framework, called Sample-specific Score-aware Noise Injection (SSNI).
Specifically, SSNI uses a pre-trained score network to estimate how much a data
point deviates from the clean data distribution (i.e., score norms). Then,
based on the magnitude of score norms, SSNI applies a reweighting function to
adaptively adjust $t^*$ for each sample, achieving sample-specific noise
injections. Empirically, incorporating our framework with existing DBP methods
results in a notable improvement in both accuracy and robustness on CIFAR-10
and ImageNet-1K, highlighting the necessity to allocate distinct noise levels
to different samples in DBP methods. Our code is available at:
https://github.com/tmlr-group/SSNI.

</details>


### [224] [Tensor-to-Tensor Models with Fast Iterated Sum Features](https://arxiv.org/pdf/2506.06041)
*Joscha Diehl, Rasheed Ibraheem, Leonard Schmitz, Yue Wu*

Main category: cs.CV

TL;DR: A novel tensor-to-tensor layer (FIS) with linear cost is proposed for processing high-dimensional data, achieving competitive performance in classification and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: High-dimensional data (e.g., images, tensors) requires efficient subquadratic processing layers, which current methods lack.

Method: Uses 'corner trees' and a multiparameter generalization of iterated integrals to create the FIS layer, integrating it into neural networks.

Result: FIS matches a larger ResNet's accuracy (0.1% difference) with fewer parameters and operations, and achieves 97.3% AUROC in anomaly detection.

Conclusion: FIS is a scalable, efficient layer for high-dimensional data, suitable for diverse tasks like classification and anomaly detection.

Abstract: Data in the form of images or higher-order tensors is ubiquitous in modern
deep learning applications. Owing to their inherent high dimensionality, the
need for subquadratic layers processing such data is even more pressing than
for sequence data. We propose a novel tensor-to-tensor layer with linear cost
in the input size, utilizing the mathematical gadget of ``corner trees'' from
the field of permutation counting. In particular, for order-two tensors, we
provide an image-to-image layer that can be plugged into image processing
pipelines. On the one hand, our method can be seen as a higher-order
generalization of state-space models. On the other hand, it is based on a
multiparameter generalization of the signature of iterated integrals (or sums).
The proposed tensor-to-tensor concept is used to build a neural network layer
called the Fast Iterated Sums (FIS) layer which integrates seamlessly with
other layer types. We demonstrate the usability of the FIS layer with both
classification and anomaly detection tasks. By replacing some layers of a
smaller ResNet architecture with FIS, a similar accuracy (with a difference of
only 0.1\%) was achieved in comparison to a larger ResNet while reducing the
number of trainable parameters and multi-add operations. The FIS layer was also
used to build an anomaly detection model that achieved an average AUROC of
97.3\% on the texture images of the popular MVTec AD dataset. The processing
and modelling codes are publicly available at
https://github.com/diehlj/fast-iterated-sums.

</details>


### [225] [SDS-Net: Shallow-Deep Synergism-detection Network for infrared small target detection](https://arxiv.org/pdf/2506.06042)
*Taoran Yue, Xiaojin Lu, Jiaxi Cai, Yuanping Chen, Shibing Chu*

Main category: cs.CV

TL;DR: SDS-Net improves IRSTD by modeling shallow-deep feature synergy and adaptive fusion, outperforming state-of-the-art methods with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing CNN-based IRSTD methods inefficiently handle shallow-deep feature collaboration and lack systematic modeling of feature hierarchies, limiting performance and increasing computational costs.

Method: Proposes SDS-Net with a dual-branch architecture for structural and semantic feature modeling, plus an adaptive fusion module for dynamic cross-layer feature correlation.

Result: Outperforms state-of-the-art methods on three datasets (NUAA-SIRST, NUDT-SIRST, IRSTD-1K) with high precision and efficiency.

Conclusion: SDS-Net achieves superior IRSTD performance with low computational complexity, offering broad application potential.

Abstract: Current CNN-based infrared small target detection(IRSTD) methods generally
overlook the heterogeneity between shallow and deep features, leading to
inefficient collaboration between shallow fine grained structural information
and deep high-level semantic representations. Additionally, the dependency
relationships and fusion mechanisms across different feature hierarchies lack
systematic modeling, which fails to fully exploit the complementarity of
multilevel features. These limitations hinder IRSTD performance while incurring
substantial computational costs. To address these challenges, this paper
proposes a shallow-deep synergistic detection network (SDS-Net) that
efficiently models multilevel feature representations to increase both the
detection accuracy and computational efficiency in IRSTD tasks. SDS-Net
introduces a dual-branch architecture that separately models the structural
characteristics and semantic properties of features, effectively preserving
shallow spatial details while capturing deep semantic representations, thereby
achieving high-precision detection with significantly improved inference speed.
Furthermore, the network incorporates an adaptive feature fusion module to
dynamically model cross-layer feature correlations, enhancing overall feature
collaboration and representation capability. Comprehensive experiments on three
public datasets (NUAA-SIRST, NUDT-SIRST, and IRSTD-1K) demonstrate that SDS-Net
outperforms state-of-the-art IRSTD methods while maintaining low computational
complexity and high inference efficiency, showing superior detection
performance and broad application prospects. Our code will be made public at
https://github.com/PhysiLearn/SDS-Net.

</details>


### [226] [Full Conformal Adaptation of Medical Vision-Language Models](https://arxiv.org/pdf/2506.06076)
*Julio Silva-Rodríguez, Leo Fillioux, Paul-Henry Cournède, Maria Vakalopoulou, Stergios Christodoulidis, Ismail Ben Ayed, Jose Dolz*

Main category: cs.CV

TL;DR: The paper explores the reliability of vision-language models (VLMs) in medical image analysis under the split conformal prediction (SCP) framework, proposing full conformal adaptation and SS-Text to improve efficiency and maintain coverage guarantees.


<details>
  <summary>Details</summary>
Motivation: VLMs are widely used in medical image analysis, but their reliability under SCP remains unaddressed, especially with few-shot transfer learning pipelines.

Method: Proposes full conformal adaptation for joint adaptation and conformalization of VLMs, along with SS-Text, a training-free linear probe solver.

Result: Experiments show consistent improvements of up to 27% in set efficiency while maintaining coverage guarantees.

Conclusion: The framework enhances VLM reliability in medical applications without additional data requirements.

Abstract: Vision-language models (VLMs) pre-trained at large scale have shown
unprecedented transferability capabilities and are being progressively
integrated into medical image analysis. Although its discriminative potential
has been widely explored, its reliability aspect remains overlooked. This work
investigates their behavior under the increasingly popular split conformal
prediction (SCP) framework, which theoretically guarantees a given error level
on output sets by leveraging a labeled calibration set. However, the zero-shot
performance of VLMs is inherently limited, and common practice involves
few-shot transfer learning pipelines, which cannot absorb the rigid
exchangeability assumptions of SCP. To alleviate this issue, we propose full
conformal adaptation, a novel setting for jointly adapting and conformalizing
pre-trained foundation models, which operates transductively over each test
data point using a few-shot adaptation set. Moreover, we complement this
framework with SS-Text, a novel training-free linear probe solver for VLMs that
alleviates the computational cost of such a transductive approach. We provide
comprehensive experiments using 3 different modality-specialized medical VLMs
and 9 adaptation tasks. Our framework requires exactly the same data as SCP,
and provides consistent relative improvements of up to 27% on set efficiency
while maintaining the same coverage guarantees.

</details>


### [227] [CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval](https://arxiv.org/pdf/2506.06144)
*David Wan, Han Wang, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal*

Main category: cs.CV

TL;DR: CLaMR is a multimodal retriever for video content, dynamically selecting relevant modalities (video frames, speech, text, metadata) to improve retrieval performance. It outperforms single- and multi-modality baselines, validated on datasets like MultiVENT 2.0++ and MSRVTT.


<details>
  <summary>Details</summary>
Motivation: Current retrieval systems treat video modalities independently, leading to noisy results. A dynamic, joint approach is needed for better relevance scoring.

Method: CLaMR jointly encodes 4 modalities with a unified backbone, trained using a synthetic dataset (MultiVENT 2.0++) and a modality-aware loss.

Result: CLaMR improves nDCG@10 by 25.6 over single-modality and 35.4 over multi-modality retrievers. It also boosts performance in downstream tasks like long-video QA.

Conclusion: CLaMR demonstrates the effectiveness of dynamic modality selection and joint encoding for multimodal video retrieval, outperforming existing methods.

Abstract: Online video web content is richly multimodal: a single video blends vision,
speech, ambient audio, and on-screen text. Retrieval systems typically treat
these modalities as independent retrieval sources, which can lead to noisy and
subpar retrieval. We explore multimodal video content retrieval, where
relevance can be scored from one particular modality or jointly across multiple
modalities simultaneously. Consequently, an effective retriever must
dynamically choose which modality (or set of modalities) best addresses the
query. We introduce CLaMR, a multimodal, late-interaction retriever that
jointly indexes 4 modalities: video frames, transcribed speech, on-screen text,
and metadata. CLaMR jointly encodes all modalities with a unified multimodal
backbone for improved contextualization and is trained to enhance dynamic
modality selection via two key innovations. First, given the lack of training
data for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale
synthetic training dataset built on MultiVENT 2.0 (event-centric videos in
various languages paired with queries) with modality-targeted queries. Next, we
propose a modality-aware loss that jointly trains according to a standard
contrastive objective alongside an objective for learning correct modality
usage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation
strategies, such as averaging similarities for baseline retrievers, degrade
performance by introducing noise from irrelevant modalities. In contrast, CLaMR
consistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR
improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4
over the best multi-modality retriever. We illustrate CLaMR's downstream
utility on long-video QA, retrieving relevant frames and obtaining a 3.50%
boost over LanguageBind on Video-MME and 1.42% over dense sampling on
LongVideoBench.

</details>


### [228] [WisWheat: A Three-Tiered Vision-Language Dataset for Wheat Management](https://arxiv.org/pdf/2506.06084)
*Bowen Yuan, Selena Song, Javier Fernandez, Yadan Luo, Mahsa Baktashmotlagh, Zijian Wang*

Main category: cs.CV

TL;DR: WisWheat dataset enhances VLMs for wheat management, improving accuracy in stress and growth stage tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional wheat management is labor-intensive and subjective; VLMs lack domain-specific knowledge for accurate recommendations.

Method: Proposes WisWheat, a three-layered dataset (pretraining, quantitative, instruction fine-tuning) to adapt VLMs for wheat tasks.

Result: Fine-tuned Qwen2.5 7B achieves 79.2% and 84.6% accuracy in stress and growth stage tasks, outperforming GPT-4o.

Conclusion: WisWheat significantly improves VLM performance for wheat management, offering scalable, data-driven solutions.

Abstract: Wheat management strategies play a critical role in determining yield.
Traditional management decisions often rely on labour-intensive expert
inspections, which are expensive, subjective and difficult to scale. Recently,
Vision-Language Models (VLMs) have emerged as a promising solution to enable
scalable, data-driven management support. However, due to a lack of
domain-specific knowledge, directly applying VLMs to wheat management tasks
results in poor quantification and reasoning capabilities, ultimately producing
vague or even misleading management recommendations. In response, we propose
WisWheat, a wheat-specific dataset with a three-layered design to enhance VLM
performance on wheat management tasks: (1) a foundational pretraining dataset
of 47,871 image-caption pairs for coarsely adapting VLMs to wheat morphology;
(2) a quantitative dataset comprising 7,263 VQA-style image-question-answer
triplets for quantitative trait measuring tasks; and (3) an Instruction
Fine-tuning dataset with 4,888 samples targeting biotic and abiotic stress
diagnosis and management plan for different phenological stages. Extensive
experimental results demonstrate that fine-tuning open-source VLMs (e.g.,
Qwen2.5 7B) on our dataset leads to significant performance improvements.
Specifically, the Qwen2.5 VL 7B fine-tuned on our wheat instruction dataset
achieves accuracy scores of 79.2% and 84.6% on wheat stress and growth stage
conversation tasks respectively, surpassing even general-purpose commercial
models such as GPT-4o by a margin of 11.9% and 34.6%.

</details>


### [229] [Feedback Guidance of Diffusion Models](https://arxiv.org/pdf/2506.06085)
*Koulischer Felix, Handke Florian, Deleu Johannes, Demeester Thomas, Ambrogioni Luca*

Main category: cs.CV

TL;DR: FBG improves conditional diffusion models by dynamically adjusting guidance based on sample needs, outperforming CFG and matching LIG in performance.


<details>
  <summary>Details</summary>
Motivation: CFG's constant guidance harms diversity and induces memorization, prompting the need for adaptive guidance.

Method: FBG uses a state-dependent coefficient to self-regulate guidance, derived from a linear corruption assumption of the conditional distribution.

Result: FBG outperforms CFG on ImageNet512x512 and adapts guidance for complex prompts in text-to-image generation.

Conclusion: FBG offers a mathematically grounded, adaptive alternative to fixed guidance schemes like CFG and LIG.

Abstract: While Classifier-Free Guidance (CFG) has become standard for improving sample
fidelity in conditional diffusion models, it can harm diversity and induce
memorization by applying constant guidance regardless of whether a particular
sample needs correction. We propose FeedBack Guidance (FBG), which uses a
state-dependent coefficient to self-regulate guidance amounts based on need.
Our approach is derived from first principles by assuming the learned
conditional distribution is linearly corrupted by the unconditional
distribution, contrasting with CFG's implicit multiplicative assumption. Our
scheme relies on feedback of its own predictions about the conditional signal
informativeness to adapt guidance dynamically during inference, challenging the
view of guidance as a fixed hyperparameter. The approach is benchmarked on
ImageNet512x512, where it significantly outperforms Classifier-Free Guidance
and is competitive to Limited Interval Guidance (LIG) while benefitting from a
strong mathematical framework. On Text-To-Image generation, we demonstrate
that, as anticipated, our approach automatically applies higher guidance scales
for complex prompts than for simpler ones and that it can be easily combined
with existing guidance schemes such as CFG or LIG.

</details>


### [230] [VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning](https://arxiv.org/pdf/2506.06097)
*Zikang Wang, Boyu Chen, Zhengrong Yue, Yi Wang, Yu Qiao, Limin Wang, Yali Wang*

Main category: cs.CV

TL;DR: VideoChat-A1 introduces a chain-of-shot reasoning paradigm for long video understanding, outperforming existing methods by focusing on relevant shots and mimicking human thinking.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with long videos due to redundant or noisy context retrieval, lacking deep understanding of relevant shots.

Method: VideoChat-A1 uses a chain-of-shot reasoning paradigm to progressively select and analyze relevant shots in a coarse-to-fine manner.

Result: Achieves state-of-the-art performance on benchmarks (77.0 on VideoMME, 70.1 on EgoSchema), outperforming baselines by up to 10.8% and 6.2%.

Conclusion: VideoChat-A1 offers competitive accuracy with reduced input frames and inference time, advancing long video understanding.

Abstract: The recent advance in video understanding has been driven by multimodal large
language models (MLLMs). But these MLLMs are good at analyzing short videos,
while suffering from difficulties in understanding videos with a longer
context. To address this difficulty, several agent paradigms have recently been
proposed, using MLLMs as agents for retrieving extra contextual knowledge in a
long video. However, most existing agents ignore the key fact that a long video
is composed with multiple shots, i.e., to answer the user question from a long
video, it is critical to deeply understand its relevant shots like human.
Without such insight, these agents often mistakenly find redundant even noisy
temporal context, restricting their capacity for long video understanding. To
fill this gap, we propose VideoChat-A1, a novel long video agent paradigm.
Different from the previous works, our VideoChat-A1 can deeply think with long
videos, via a distinct chain-of-shot reasoning paradigm. More specifically, it
can progressively select the relevant shots of user question, and look into
these shots in a coarse-to-fine partition. By multi-modal reasoning along the
shot chain, VideoChat-A1 can effectively mimic step-by-step human thinking
process, allowing to interactively discover preferable temporal context for
thoughtful understanding in long videos. Extensive experiments show that, our
VideoChat-A1 achieves the state-of-the-art performance on the mainstream long
video QA benchmarks, e.g., it achieves 77.0 on VideoMME and 70.1 on EgoSchema,
outperforming its strong baselines (e.g., Intern2.5VL-8B and
InternVideo2.5-8B), by up to 10.8\% and 6.2\%. Compared to leading close-source
GPT-4o and Gemini 1.5 Pro, VideoChat-A1 offers competitive accuracy, but with
7\% input frames and 12\% inference time on average.

</details>


### [231] [Bidirectional Image-Event Guided Low-Light Image Enhancement](https://arxiv.org/pdf/2506.06120)
*Zhanwen Liu, Huanna Song, Yang Wang, Nan Yang, Shangyu Xie, Yisheng An, Xiangmo Zhao*

Main category: cs.CV

TL;DR: Proposes BiLIE, a framework for low-light image enhancement using event cameras, addressing noise and structural issues, and introduces a new dataset (RELIE).


<details>
  <summary>Details</summary>
Motivation: Traditional cameras struggle in low-light, and existing event-guided methods ignore noise and structural problems.

Method: Uses frequency high-pass filtering (EFE module) and Bidirectional Cross Attention Fusion (BCAF) to enhance edges and reduce noise.

Result: Outperforms state-of-the-art by 0.96dB in PSNR and 0.03 in LPIPS.

Conclusion: BiLIE effectively enhances low-light images by addressing noise and structural issues, supported by a new high-quality dataset.

Abstract: Under extreme low-light conditions, traditional frame-based cameras, due to
their limited dynamic range and temporal resolution, face detail loss and
motion blur in captured images. To overcome this bottleneck, researchers have
introduced event cameras and proposed event-guided low-light image enhancement
algorithms. However, these methods neglect the influence of global
low-frequency noise caused by dynamic lighting conditions and local structural
discontinuities in sparse event data. To address these issues, we propose an
innovative Bidirectional guided Low-light Image Enhancement framework (BiLIE).
Specifically, to mitigate the significant low-frequency noise introduced by
global illumination step changes, we introduce the frequency high-pass
filtering-based Event Feature Enhancement (EFE) module at the event
representation level to suppress the interference of low-frequency information,
and preserve and highlight the high-frequency edges.Furthermore, we design a
Bidirectional Cross Attention Fusion (BCAF) mechanism to acquire high-frequency
structures and edges while suppressing structural discontinuities and local
noise introduced by sparse event guidance, thereby generating smoother fused
representations.Additionally, considering the poor visual quality and color
bias in existing datasets, we provide a new dataset (RELIE), with high-quality
ground truth through a reliable enhancement scheme. Extensive experimental
results demonstrate that our proposed BiLIE outperforms state-of-the-art
methods by 0.96dB in PSNR and 0.03 in LPIPS.

</details>


### [232] [Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding](https://arxiv.org/pdf/2506.06275)
*Emmanouil Zaranis, António Farinhas, Saul Santos, Beatriz Canaverde, Miguel Moura Ramos, Aditya K Surikuchi, André Viveiros, Baohao Liao, Elena Bueno-Benito, Nithin Sivakumaran, Pavlo Vasylenko, Shoubin Yu, Sonal Sannigrahi, Wafaa Mohammed, Ben Peters, Danae Sánchez Villegas, Elias Stengel-Eskin, Giuseppe Attanasio, Jaehong Yoon, Stella Frank, Alessandro Suglia, Chrysoula Zerva, Desmond Elliott, Mariella Dimiccoli, Mohit Bansal, Oswald Lanz, Raffaella Bernardi, Raquel Fernández, Sandro Pezzelle, Vlad Niculae, André F. T. Martins*

Main category: cs.CV

TL;DR: The paper introduces MF$^2$, a benchmark for evaluating VLMs' ability to comprehend and recall key narrative details from full-length movies, highlighting current models' shortcomings compared to humans.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for VLMs focus on superficial details or semi-automated questions, failing to assess deep comprehension of long-form video content like movies.

Method: MF$^2$ includes 50+ full-length movies with manually crafted claim pairs (true/false) targeting core narrative elements, evaluated via binary claim identification.

Result: State-of-the-art VLMs perform significantly worse than humans, showing their inability to retain and reason over critical narrative information.

Conclusion: MF$^2$ exposes VLMs' limitations in holistic video understanding, emphasizing the gap between human and model capabilities in narrative comprehension.

Abstract: Despite recent progress in vision-language models (VLMs), holistic
understanding of long-form video content remains a significant challenge,
partly due to limitations in current benchmarks. Many focus on peripheral,
``needle-in-a-haystack'' details, encouraging context-insensitive retrieval
over deep comprehension. Others rely on large-scale, semi-automatically
generated questions (often produced by language models themselves) that are
easier for models to answer but fail to reflect genuine understanding. In this
paper, we introduce MF$^2$, a new benchmark for evaluating whether models can
comprehend, consolidate, and recall key narrative information from full-length
movies (50-170 minutes long). MF$^2$ includes over 50 full-length,
open-licensed movies, each paired with manually constructed sets of claim pairs
-- one true (fact) and one plausible but false (fib), totalling over 850 pairs.
These claims target core narrative elements such as character motivations and
emotions, causal chains, and event order, and refer to memorable moments that
humans can recall without rewatching the movie. Instead of multiple-choice
formats, we adopt a binary claim evaluation protocol: for each pair, models
must correctly identify both the true and false claims. This reduces biases
like answer ordering and enables a more precise assessment of reasoning. Our
experiments demonstrate that both open-weight and closed state-of-the-art
models fall well short of human performance, underscoring the relative ease of
the task for humans and their superior ability to retain and reason over
critical narrative information -- an ability current VLMs lack.

</details>


### [233] [CCLSTM: Coupled Convolutional Long-Short Term Memory Network for Occupancy Flow Forecasting](https://arxiv.org/pdf/2506.06128)
*Peter Lengyel*

Main category: cs.CV

TL;DR: CCLSTM, a lightweight convolutional LSTM architecture, achieves state-of-the-art performance in predicting dynamic agent states without relying on vectorized inputs or transformers.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing methods that depend on high-quality vectorized inputs and computationally intensive transformer architectures.

Method: Proposes Coupled Convolutional LSTM (CCLSTM), a lightweight, end-to-end trainable architecture using convolutional operations to capture temporal dynamics and spatial occupancy-flow correlations.

Result: Achieves state-of-the-art performance on occupancy flow metrics and ranks 1st in all metrics on the 2024 Waymo Occupancy and Flow Prediction Challenge.

Conclusion: CCLSTM offers a practical and efficient solution for predicting dynamic agent states, outperforming existing methods.

Abstract: Predicting future states of dynamic agents is a fundamental task in
autonomous driving. An expressive representation for this purpose is Occupancy
Flow Fields, which provide a scalable and unified format for modeling motion,
spatial extent, and multi-modal future distributions. While recent methods have
achieved strong results using this representation, they often depend on
high-quality vectorized inputs, which are unavailable or difficult to generate
in practice, and the use of transformer-based architectures, which are
computationally intensive and costly to deploy. To address these issues, we
propose \textbf{Coupled Convolutional LSTM (CCLSTM)}, a lightweight, end-to-end
trainable architecture based solely on convolutional operations. Without
relying on vectorized inputs or self-attention mechanisms, CCLSTM effectively
captures temporal dynamics and spatial occupancy-flow correlations using a
compact recurrent convolutional structure. Despite its simplicity, CCLSTM
achieves state-of-the-art performance on occupancy flow metrics and, as of this
submission, ranks \(1^{\text{st}}\) in all metrics on the 2024 Waymo Occupancy
and Flow Prediction Challenge leaderboard.

</details>


### [234] [GenIR: Generative Visual Feedback for Mental Image Retrieval](https://arxiv.org/pdf/2506.06220)
*Diji Yang, Minghao Liu, Chung-Hsiang Lo, Yi Zhang, James Davis*

Main category: cs.CV

TL;DR: The paper introduces Mental Image Retrieval (MIR), a multi-round interactive task for refining image searches based on mental images, and proposes GenIR, a generative retrieval method using diffusion models for clear feedback.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image retrieval models lack support for multi-round interactions, which better reflect real-world human search behavior involving mental images.

Method: GenIR employs diffusion-based image generation to provide explicit visual feedback, aiding users in refining queries iteratively. A pipeline for generating a multi-round MIR dataset is also introduced.

Result: GenIR outperforms existing interactive methods in the MIR scenario, demonstrating its effectiveness.

Conclusion: The work establishes MIR as a new task, provides a dataset, and introduces GenIR, paving the way for future research in interactive image retrieval.

Abstract: Vision-language models (VLMs) have shown strong performance on text-to-image
retrieval benchmarks. However, bridging this success to real-world applications
remains a challenge. In practice, human search behavior is rarely a one-shot
action. Instead, it is often a multi-round process guided by clues in mind,
that is, a mental image ranging from vague recollections to vivid mental
representations of the target image. Motivated by this gap, we study the task
of Mental Image Retrieval (MIR), which targets the realistic yet underexplored
setting where users refine their search for a mentally envisioned image through
multi-round interactions with an image search engine. Central to successful
interactive retrieval is the capability of machines to provide users with
clear, actionable feedback; however, existing methods rely on indirect or
abstract verbal feedback, which can be ambiguous, misleading, or ineffective
for users to refine the query. To overcome this, we propose GenIR, a generative
multi-round retrieval paradigm leveraging diffusion-based image generation to
explicitly reify the AI system's understanding at each round. These synthetic
visual representations provide clear, interpretable feedback, enabling users to
refine their queries intuitively and effectively. We further introduce a fully
automated pipeline to generate a high-quality multi-round MIR dataset.
Experimental results demonstrate that GenIR significantly outperforms existing
interactive methods in the MIR scenario. This work establishes a new task with
a dataset and an effective generative retrieval method, providing a foundation
for future research in this direction.

</details>


### [235] [A Novel Large-scale Crop Dataset and Dual-stream Transformer Method for Fine-grained Hierarchical Crop Classification from Integrated Hyperspectral EnMAP Data and Multispectral Sentinel-2 Time Series](https://arxiv.org/pdf/2506.06155)
*Wenyuan Li, Shunlin Liang, Yuxiang Zhang, Liqin Liu, Keyan Chen, Yongzhe Chen, Han Ma, Jianglei Xu, Yichuan Ma, Shikang Guan, Zhenwei Shi*

Main category: cs.CV

TL;DR: The paper introduces H2Crop, a hierarchical hyperspectral crop dataset combining EnMAP hyperspectral data and Sentinel-2 time series, and proposes a dual-stream Transformer for fine-grained crop classification, achieving improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Fine-grained crop classification is essential for precision agriculture and food security but is hindered by challenges in hyperspectral data acquisition and annotation costs.

Method: A dual-stream Transformer architecture processes hyperspectral EnMAP data (spectral-spatial Transformer) and Sentinel-2 time series (temporal Swin Transformer), with hierarchical fusion for multi-level classification.

Result: Adding hyperspectral data improves F1-scores by 4.2% on average (up to 6.3%), outperforming existing deep learning methods.

Conclusion: The H2Crop dataset and proposed method advance fine-grained crop classification, demonstrating the value of hyperspectral data in agricultural remote sensing.

Abstract: Fine-grained crop classification is crucial for precision agriculture and
food security monitoring. It requires simultaneous capture of both phenological
dynamics (obtained from multi-temporal satellite data like Sentinel-2) and
subtle spectral variations (demanding nanometer-scale spectral resolution from
hyperspectral imagery). Research combining these two modalities remains scarce
currently due to challenges in hyperspectral data acquisition and crop types
annotation costs. To address these issues, we construct a hierarchical
hyperspectral crop dataset (H2Crop) by integrating 30m-resolution EnMAP
hyperspectral data with Sentinel-2 time series. With over one million annotated
field parcels organized in a four-tier crop taxonomy, H2Crop establishes a
vital benchmark for fine-grained agricultural crop classification and
hyperspectral image processing. We propose a dual-stream Transformer
architecture that synergistically processes these modalities. It coordinates
two specialized pathways: a spectral-spatial Transformer extracts fine-grained
signatures from hyperspectral EnMAP data, while a temporal Swin Transformer
extracts crop growth patterns from Sentinel-2 time series. The designed
hierarchy classification heads with hierarchical fusion then simultaneously
delivers multi-level classification across all taxonomic tiers. Experiments
demonstrate that adding hyperspectral EnMAP data to Sentinel-2 time series
yields a 4.2% average F1-scores improvement (peaking at 6.3%). Extensive
comparisons also confirming our method's higher accuracy over existing deep
learning approaches for crop type classification and the consistent benefits of
hyperspectral data across varying temporal windows and crop change scenarios.
Codes and dataset will be available at https://github.com/flyakon/H2Crop and
www.glass.hku.hk
  Keywords: Crop type classification, precision agriculture, remote sensing,
deep learning, hyperspectral data, Sentinel-2 time series, fine-grained crops

</details>


### [236] [Technical Report for Egocentric Mistake Detection for the HoloAssist Challenge](https://arxiv.org/pdf/2506.06174)
*Constantin Patsch, Marsil Zakour, Yuankai Wu, Eckehard Steinbach*

Main category: cs.CV

TL;DR: The paper introduces an online mistake detection framework for real-time video analysis, addressing both procedural and execution errors, and uses an LLM for feedback. It ranks second on the HoloAssist benchmark.


<details>
  <summary>Details</summary>
Motivation: Real-time mistake detection is crucial in domains like industrial automation and education, but existing methods focus only on procedural errors, limiting real-world applicability.

Method: The proposed framework detects procedural and execution errors (e.g., motor slips) and employs an LLM to generate explanatory feedback.

Result: The approach ranks second on the HoloAssist benchmark, demonstrating its effectiveness.

Conclusion: The framework successfully broadens error detection capabilities and provides actionable feedback, proving useful for real-world applications.

Abstract: In this report, we address the task of online mistake detection, which is
vital in domains like industrial automation and education, where real-time
video analysis allows human operators to correct errors as they occur. While
previous work focuses on procedural errors involving action order, broader
error types must be addressed for real-world use. We introduce an online
mistake detection framework that handles both procedural and execution errors
(e.g., motor slips or tool misuse). Upon detecting an error, we use a large
language model (LLM) to generate explanatory feedback. Experiments on the
HoloAssist benchmark confirm the effectiveness of our approach, where our
approach is placed second on the mistake detection task.

</details>


### [237] [SatelliteFormula: Multi-Modal Symbolic Regression from Remote Sensing Imagery for Physics Discovery](https://arxiv.org/pdf/2506.06176)
*Zhenyu Yu, Mohd. Yamani Idna Idris, Pei Wang, Yuelong Xia, Fei Ma, Rizwan Qureshi*

Main category: cs.CV

TL;DR: SatelliteFormula is a symbolic regression framework for deriving interpretable expressions from multi-spectral imagery, combining Vision Transformers and physics-guided constraints.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional empirical indices and black-box models in handling high-dimensional multi-spectral data while ensuring interpretability and physical consistency.

Method: Integrates a Vision Transformer-based encoder for feature extraction with physics-guided constraints and a symbolic optimizer to balance accuracy and physical plausibility.

Result: Outperforms state-of-the-art baselines in performance, stability, and generalization on benchmark datasets and remote sensing tasks.

Conclusion: SatelliteFormula bridges data-driven learning and physical understanding, enabling interpretable modeling of environmental variables.

Abstract: We propose SatelliteFormula, a novel symbolic regression framework that
derives physically interpretable expressions directly from multi-spectral
remote sensing imagery. Unlike traditional empirical indices or black-box
learning models, SatelliteFormula combines a Vision Transformer-based encoder
for spatial-spectral feature extraction with physics-guided constraints to
ensure consistency and interpretability. Existing symbolic regression methods
struggle with the high-dimensional complexity of multi-spectral data; our
method addresses this by integrating transformer representations into a
symbolic optimizer that balances accuracy and physical plausibility. Extensive
experiments on benchmark datasets and remote sensing tasks demonstrate superior
performance, stability, and generalization compared to state-of-the-art
baselines. SatelliteFormula enables interpretable modeling of complex
environmental variables, bridging the gap between data-driven learning and
physical understanding.

</details>


### [238] [Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models](https://arxiv.org/pdf/2506.06242)
*Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu*

Main category: cs.CV

TL;DR: The paper introduces the Visual Graph Arena (VGA) to evaluate AI's ability for visual abstraction, revealing significant gaps compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Address the lack of 'conceptualization' in AI models—recognizing and reasoning about concepts despite visual variations.

Method: VGA uses six graph-based tasks with diverse layouts to test representation-invariant reasoning.

Result: Humans performed near-perfectly, while AI models failed on isomorphism detection and showed limited success in path/cycle tasks.

Conclusion: VGA highlights AI's limitations in visual understanding and provides a framework for improving human-like conceptualization.

Abstract: Recent advancements in multimodal large language models have driven
breakthroughs in visual question answering. Yet, a critical gap persists,
`conceptualization'-the ability to recognize and reason about the same concept
despite variations in visual form, a basic ability of human reasoning. To
address this challenge, we introduce the Visual Graph Arena (VGA), a dataset
featuring six graph-based tasks designed to evaluate and improve AI systems'
capacity for visual abstraction. VGA uses diverse graph layouts (e.g.,
Kamada-Kawai vs. planar) to test reasoning independent of visual form.
Experiments with state-of-the-art vision models and multimodal LLMs reveal a
striking divide: humans achieved near-perfect accuracy across tasks, while
models totally failed on isomorphism detection and showed limited success in
path/cycle tasks. We further identify behavioral anomalies suggesting
pseudo-intelligent pattern matching rather than genuine understanding. These
findings underscore fundamental limitations in current AI models for visual
understanding. By isolating the challenge of representation-invariant
reasoning, the VGA provides a framework to drive progress toward human-like
conceptualization in AI visual models. The Visual Graph Arena is available at:
\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}

</details>


### [239] [STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving](https://arxiv.org/pdf/2506.06218)
*Christian Fruhwirth-Reisinger, Dušan Malić, Wei Lin, David Schinagl, Samuel Schulter, Horst Possegger*

Main category: cs.CV

TL;DR: STSBench is a framework for benchmarking vision-language models (VLMs) in autonomous driving, focusing on spatio-temporal reasoning. It includes STSnu, a benchmark for evaluating VLMs on complex traffic scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack evaluation of VLMs' spatio-temporal reasoning in autonomous driving, a critical capability for real-world applications.

Method: STSBench mines traffic scenarios from datasets, verifies them via a user interface, and generates multiple-choice questions for evaluation. STSnu applies this to the NuScenes dataset.

Result: STSnu includes 43 scenarios and 971 questions, revealing shortcomings in current VLMs' reasoning about traffic dynamics.

Conclusion: STSBench fills a gap in spatio-temporal evaluation, urging architectural improvements for robust VLMs in autonomous driving.

Abstract: We introduce STSBench, a scenario-based framework to benchmark the holistic
understanding of vision-language models (VLMs) for autonomous driving. The
framework automatically mines pre-defined traffic scenarios from any dataset
using ground-truth annotations, provides an intuitive user interface for
efficient human verification, and generates multiple-choice questions for model
evaluation. Applied to the NuScenes dataset, we present STSnu, the first
benchmark that evaluates the spatio-temporal reasoning capabilities of VLMs
based on comprehensive 3D perception. Existing benchmarks typically target
off-the-shelf or fine-tuned VLMs for images or videos from a single viewpoint
and focus on semantic tasks such as object recognition, dense captioning, risk
assessment, or scene understanding. In contrast, STSnu evaluates driving expert
VLMs for end-to-end driving, operating on videos from multi-view cameras or
LiDAR. It specifically assesses their ability to reason about both ego-vehicle
actions and complex interactions among traffic participants, a crucial
capability for autonomous vehicles. The benchmark features 43 diverse scenarios
spanning multiple views and frames, resulting in 971 human-verified
multiple-choice questions. A thorough evaluation uncovers critical shortcomings
in existing models' ability to reason about fundamental traffic dynamics in
complex environments. These findings highlight the urgent need for
architectural advances that explicitly model spatio-temporal reasoning. By
addressing a core gap in spatio-temporal evaluation, STSBench enables the
development of more robust and explainable VLMs for autonomous driving.

</details>


### [240] [Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study](https://arxiv.org/pdf/2506.06232)
*Leon Mayer, Tim Rädsch, Dominik Michael, Lucas Luttner, Amine Yamlahi, Evangelia Christodoulou, Patrick Godau, Marcel Knopp, Annika Reinke, Fiona Kolbinger, Lena Maier-Hein*

Main category: cs.CV

TL;DR: VLMs show promise for basic endoscopic tasks but struggle with medical knowledge; specialized medical VLMs underperform generalist models, indicating a need for further optimization.


<details>
  <summary>Details</summary>
Motivation: Assess the capabilities of Vision Language Models (VLMs) in endoscopic tasks, particularly laparoscopic surgery, to understand their potential and limitations.

Method: Evaluated multiple state-of-the-art VLMs on diverse surgical datasets with human annotations, addressing three key research questions about task performance.

Result: VLMs perform well on basic tasks like object counting but falter with medical knowledge; specialized medical VLMs lag behind generalist models.

Conclusion: Specialized VLMs need optimization for surgical environments; the study guides future development of endoscopic AI systems.

Abstract: While traditional computer vision models have historically struggled to
generalize to endoscopic domains, the emergence of foundation models has shown
promising cross-domain performance. In this work, we present the first
large-scale study assessing the capabilities of Vision Language Models (VLMs)
for endoscopic tasks with a specific focus on laparoscopic surgery. Using a
diverse set of state-of-the-art models, multiple surgical datasets, and
extensive human reference annotations, we address three key research questions:
(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can
they handle advanced frame-based endoscopic scene understanding tasks? and (3)
How do specialized medical VLMs compare to generalist models in this context?
Our results reveal that VLMs can effectively perform basic surgical perception
tasks, such as object counting and localization, with performance levels
comparable to general domain tasks. However, their performance deteriorates
significantly when the tasks require medical knowledge. Notably, we find that
specialized medical VLMs currently underperform compared to generalist models
across both basic and advanced surgical tasks, suggesting that they are not yet
optimized for the complexity of surgical environments. These findings highlight
the need for further advancements to enable VLMs to handle the unique
challenges posed by surgery. Overall, our work provides important insights for
the development of next-generation endoscopic AI systems and identifies key
areas for improvement in medical visual language models.

</details>


### [241] [Optimizing Cloud-to-GPU Throughput for Deep Learning With Earth Observation Data](https://arxiv.org/pdf/2506.06235)
*Akram Zaytar, Caleb Robinson, Girmaw Abebe Tadesse, Tammy Glazer, Gilles Hacheme, Anthony Ortiz, Rahul M Dodhia, Juan M Lavista Ferres*

Main category: cs.CV

TL;DR: Optimized GeoTIFF loading for deep learning on petabyte-scale Earth observation data, improving throughput and GPU utilization.


<details>
  <summary>Details</summary>
Motivation: Standard PyTorch data loaders underutilize GPUs when streaming GeoTIFF files from cloud storage, necessitating optimization.

Method: Benchmarked GeoTIFF loading throughput, tested configurations (tile-aligned reads, worker threads), and used Bayesian optimization for optimal settings.

Result: 20x remote and 4x local throughput gains; matched local training accuracy with optimized remote loading, improved GPU utilization (85-95% vs. 0-30%).

Conclusion: Optimized configurations significantly enhance performance and GPU utilization for EO data training, with code publicly available.

Abstract: Training deep learning models on petabyte-scale Earth observation (EO) data
requires separating compute resources from data storage. However, standard
PyTorch data loaders cannot keep modern GPUs utilized when streaming GeoTIFF
files directly from cloud storage. In this work, we benchmark GeoTIFF loading
throughput from both cloud object storage and local SSD, systematically testing
different loader configurations and data parameters. We focus on tile-aligned
reads and worker thread pools, using Bayesian optimization to find optimal
settings for each storage type. Our optimized configurations increase remote
data loading throughput by 20x and local throughput by 4x compared to default
settings. On three public EO benchmarks, models trained with optimized remote
loading achieve the same accuracy as local training within identical time
budgets. We improve validation IoU by 6-15% and maintain 85-95% GPU utilization
versus 0-30% with standard configurations. Code is publicly available at
https://github.com/microsoft/pytorch-cloud-geotiff-optimization

</details>


### [242] [Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence with Egocentric-Exocentric Vision](https://arxiv.org/pdf/2506.06253)
*Yuping He, Yifei Huang, Guo Chen, Lidong Lu, Baoqi Pei, Jilan Xu, Tong Lu, Yoichi Sato*

Main category: cs.CV

TL;DR: A survey on video understanding from egocentric and exocentric perspectives, reviewing applications, research tasks, advancements, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore the synergistic potential of integrating egocentric and exocentric perspectives in video understanding for human-like machine perception.

Method: Systematic review of recent advancements in three directions: leveraging egocentric data for exocentric understanding, using exocentric data for egocentric analysis, and joint learning frameworks.

Result: Identified key tasks, benchmark datasets, and current limitations, proposing future research directions.

Conclusion: The survey aims to inspire advancements in video understanding by unifying both perspectives, bringing machines closer to human-like perception.

Abstract: Perceiving the world from both egocentric (first-person) and exocentric
(third-person) perspectives is fundamental to human cognition, enabling rich
and complementary understanding of dynamic environments. In recent years,
allowing the machines to leverage the synergistic potential of these dual
perspectives has emerged as a compelling research direction in video
understanding. In this survey, we provide a comprehensive review of video
understanding from both exocentric and egocentric viewpoints. We begin by
highlighting the practical applications of integrating egocentric and
exocentric techniques, envisioning their potential collaboration across
domains. We then identify key research tasks to realize these applications.
Next, we systematically organize and review recent advancements into three main
research directions: (1) leveraging egocentric data to enhance exocentric
understanding, (2) utilizing exocentric data to improve egocentric analysis,
and (3) joint learning frameworks that unify both perspectives. For each
direction, we analyze a diverse set of tasks and relevant works. Additionally,
we discuss benchmark datasets that support research in both perspectives,
evaluating their scope, diversity, and applicability. Finally, we discuss
limitations in current works and propose promising future research directions.
By synthesizing insights from both perspectives, our goal is to inspire
advancements in video understanding and artificial intelligence, bringing
machines closer to perceiving the world in a human-like manner. A GitHub repo
of related works can be found at
https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.

</details>


### [243] [BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading](https://arxiv.org/pdf/2506.06271)
*Jonathan Schmidt, Simon Giebenhain, Matthias Niessner*

Main category: cs.CV

TL;DR: BecomingLit introduces a method for creating relightable, high-resolution head avatars using a low-cost light stage setup, a new dataset, and a hybrid neural shading approach, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The goal is to reconstruct relightable, high-resolution head avatars that can be rendered interactively from novel viewpoints, addressing limitations in current avatar creation and relighting techniques.

Method: The method involves a low-cost light stage setup for face capture, a dataset of multi-view sequences, a relightable avatar representation using 3D Gaussian primitives, and a hybrid neural shading approach combining neural diffuse BRDF with analytical specular terms.

Result: The approach outperforms state-of-the-art methods in relighting and reenactment, validated through extensive experiments.

Conclusion: BecomingLit successfully achieves high-quality relightable avatars with animation and control capabilities, demonstrating significant improvements over existing techniques.

Abstract: We introduce BecomingLit, a novel method for reconstructing relightable,
high-resolution head avatars that can be rendered from novel viewpoints at
interactive rates. Therefore, we propose a new low-cost light stage capture
setup, tailored specifically towards capturing faces. Using this setup, we
collect a novel dataset consisting of diverse multi-view sequences of numerous
subjects under varying illumination conditions and facial expressions. By
leveraging our new dataset, we introduce a new relightable avatar
representation based on 3D Gaussian primitives that we animate with a
parametric head model and an expression-dependent dynamics module. We propose a
new hybrid neural shading approach, combining a neural diffuse BRDF with an
analytical specular term. Our method reconstructs disentangled materials from
our dynamic light stage recordings and enables all-frequency relighting of our
avatars with both point lights and environment maps. In addition, our avatars
can easily be animated and controlled from monocular videos. We validate our
approach in extensive experiments on our dataset, where we consistently
outperform existing state-of-the-art methods in relighting and reenactment by a
significant margin.

</details>


### [244] [STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis](https://arxiv.org/pdf/2506.06276)
*Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng, Yuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel Angel Bautista, Josh Susskind, Shuangfei Zhai*

Main category: cs.CV

TL;DR: STARFlow is a scalable generative model using normalizing flows and autoregressive transformers for high-resolution image synthesis, achieving competitive performance with state-of-the-art diffusion models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling normalizing flows for high-resolution image synthesis while maintaining exact maximum likelihood training.

Method: Combines Transformer Autoregressive Flow (TARFlow) with architectural innovations like deep-shallow design, latent space modeling, and a guidance algorithm.

Result: Competitive performance in class- and text-conditional image generation, approaching diffusion models in quality.

Conclusion: STARFlow successfully scales normalizing flows to high resolutions, demonstrating their viability for large-scale generative tasks.

Abstract: We present STARFlow, a scalable generative model based on normalizing flows
that achieves strong performance in high-resolution image synthesis. The core
of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the
expressive power of normalizing flows with the structured modeling capabilities
of Autoregressive Transformers. We first establish the theoretical universality
of TARFlow for modeling continuous distributions. Building on this foundation,
we introduce several key architectural and algorithmic innovations to
significantly enhance scalability: (1) a deep-shallow design, wherein a deep
Transformer block captures most of the model representational capacity,
complemented by a few shallow Transformer blocks that are computationally
efficient yet substantially beneficial; (2) modeling in the latent space of
pretrained autoencoders, which proves more effective than direct pixel-level
modeling; and (3) a novel guidance algorithm that significantly boosts sample
quality. Crucially, our model remains an end-to-end normalizing flow, enabling
exact maximum likelihood training in continuous spaces without discretization.
STARFlow achieves competitive performance in both class-conditional and
text-conditional image generation tasks, approaching state-of-the-art diffusion
models in sample quality. To our knowledge, this work is the first successful
demonstration of normalizing flows operating effectively at this scale and
resolution.

</details>


### [245] [ExAct: A Video-Language Benchmark for Expert Action Analysis](https://arxiv.org/pdf/2506.06277)
*Han Yi, Yulu Pan, Feihong He, Xinyu Liu, Benjamin Zhang, Oluwatumininu Oguntola, Gedas Bertasius*

Main category: cs.CV

TL;DR: ExAct is a new video-language benchmark for expert-level understanding of skilled human activities, revealing a performance gap between VLMs and human experts.


<details>
  <summary>Details</summary>
Motivation: To develop a benchmark for evaluating nuanced, expert-level understanding of physical human skills in video-language models.

Method: ExAct includes 3,521 expert-curated video QA pairs across 6 domains, requiring fine-grained understanding.

Result: GPT-4o achieves 44.70% accuracy, significantly below human experts' 82.02%.

Conclusion: ExAct aids in developing VLMs for precise understanding of human skills; dataset and code are publicly available.

Abstract: We present ExAct, a new video-language benchmark for expert-level
understanding of skilled physical human activities. Our new benchmark contains
3521 expert-curated video question-answer pairs spanning 11 physical activities
in 6 domains: Sports, Bike Repair, Cooking, Health, Music, and Dance. ExAct
requires the correct answer to be selected from five carefully designed
candidate options, thus necessitating a nuanced, fine-grained, expert-level
understanding of physical human skills. Evaluating the recent state-of-the-art
VLMs on ExAct reveals a substantial performance gap relative to human expert
performance. Specifically, the best-performing GPT-4o model achieves only
44.70% accuracy, well below the 82.02% attained by trained human
specialists/experts. We believe that ExAct will be beneficial for developing
and evaluating VLMs capable of precise understanding of human skills in various
physical and procedural domains. Dataset and code are available at
https://texaser.github.io/exact_project_page/

</details>


### [246] [CoMemo: LVLMs Need Image Context with Image Memory](https://arxiv.org/pdf/2506.06279)
*Shi Liu, Weijie Su, Xizhou Zhu, Wenhai Wang, Jifeng Dai*

Main category: cs.CV

TL;DR: CoMemo introduces a dual-path architecture and RoPE-DHR positional encoding to address visual neglect and 2D structural loss in LVLMs, outperforming existing models on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs, built on LLMs, suffer from suboptimal multimodal processing due to attention neglect and poor 2D spatial preservation.

Method: Proposes CoMemo (dual-path architecture) and RoPE-DHR (positional encoding) to improve visual processing and spatial awareness.

Result: Superior performance on seven benchmarks, including long-context comprehension and visual question answering.

Conclusion: CoMemo effectively addresses key LVLM limitations, offering improved multimodal processing capabilities.

Abstract: Recent advancements in Large Vision-Language Models built upon Large Language
Models have established aligning visual features with LLM representations as
the dominant paradigm. However, inherited LLM architectural designs introduce
suboptimal characteristics for multimodal processing. First, LVLMs exhibit a
bimodal distribution in attention allocation, leading to the progressive
neglect of middle visual content as context expands. Second, conventional
positional encoding schemes fail to preserve vital 2D structural relationships
when processing dynamic high-resolution images. To address these limitations,
we propose CoMemo - a dual-path architecture that combines a Context image path
with an image Memory path for visual processing, effectively alleviating visual
information neglect. Additionally, we introduce RoPE-DHR, a novel positional
encoding mechanism that employs thumbnail-based positional aggregation to
maintain 2D spatial awareness while mitigating remote decay in extended
sequences. Evaluations across seven benchmarks,including long-context
comprehension, multi-image reasoning, and visual question answering,
demonstrate CoMemo's superior performance compared to conventional LVLM
architectures. Project page is available at
https://lalbj.github.io/projects/CoMemo/.

</details>


### [247] [TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation](https://arxiv.org/pdf/2506.06281)
*Muhammad Sohail Danish, Muhammad Akhtar Munir, Syed Roshaan Ali Shah, Muhammad Haris Khan, Rao Muhammad Anwer, Jorma Laaksonen, Fahad Shahbaz Khan, Salman Khan*

Main category: cs.CV

TL;DR: TerraFM is a scalable self-supervised learning model for Earth observation, leveraging Sentinel-1 and Sentinel-2 imagery with improved spatial and semantic coverage. It outperforms prior models on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in training data scale, geographical coverage, and spectral diversity for globally transferable representations in Earth observation.

Method: Uses self-supervised learning with Sentinel-1 and Sentinel-2 imagery, modality-specific patch embeddings, adaptive cross-attention fusion, local-global contrastive learning, and a dual-centering mechanism for long-tailed distributions.

Result: Achieves strong generalization on classification and segmentation tasks, outperforming prior models on GEO-Bench and Copernicus-Bench.

Conclusion: TerraFM demonstrates the effectiveness of scalable self-supervised learning for Earth observation, with publicly available code and models.

Abstract: Modern Earth observation (EO) increasingly leverages deep learning to harness
the scale and diversity of satellite imagery across sensors and regions. While
recent foundation models have demonstrated promising generalization across EO
tasks, many remain limited by the scale, geographical coverage, and spectral
diversity of their training data, factors critical for learning globally
transferable representations. In this work, we introduce TerraFM, a scalable
self-supervised learning model that leverages globally distributed Sentinel-1
and Sentinel-2 imagery, combined with large spatial tiles and land-cover aware
sampling to enrich spatial and semantic coverage. By treating sensing
modalities as natural augmentations in our self-supervised approach, we unify
radar and optical inputs via modality-specific patch embeddings and adaptive
cross-attention fusion. Our training strategy integrates local-global
contrastive learning and introduces a dual-centering mechanism that
incorporates class-frequency-aware regularization to address long-tailed
distributions in land cover.TerraFM achieves strong generalization on both
classification and segmentation tasks, outperforming prior models on GEO-Bench
and Copernicus-Bench. Our code and pretrained models are publicly available at:
https://github.com/mbzuai-oryx/TerraFM .

</details>


### [248] [Self-Supervised Generative-Contrastive Learning of Multi-Modal Euclidean Input for 3D Shape Latent Representations: A Dynamic Switching Approach](https://arxiv.org/pdf/2301.04612)
*Chengzhi Wu, Julius Pfrommer, Mingyuan Zhou, Jürgen Beyerer*

Main category: cs.CV

TL;DR: A neural architecture combines generative and contrastive learning for 3D shape representations, using dual encoders and a shared decoder with dynamic switching to avoid trivial solutions.


<details>
  <summary>Details</summary>
Motivation: To learn better latent representations of 3D shapes by integrating voxel grids and multi-view images, avoiding collapse in contrastive learning.

Method: Uses two encoders (voxel and multi-view) with a shared decoder, combining contrastive and reconstruction losses, and employs dynamic switching for cross-training.

Result: Improved latent representations with better reconstruction and classification performance due to implicit integration of additional input data.

Conclusion: The proposed self-supervised method effectively enhances 3D shape representation learning by combining generative and contrastive approaches.

Abstract: We propose a combined generative and contrastive neural architecture for
learning latent representations of 3D volumetric shapes. The architecture uses
two encoder branches for voxel grids and multi-view images from the same
underlying shape. The main idea is to combine a contrastive loss between the
resulting latent representations with an additional reconstruction loss. That
helps to avoid collapsing the latent representations as a trivial solution for
minimizing the contrastive loss. A novel dynamic switching approach is used to
cross-train two encoders with a shared decoder. The switching approach also
enables the stop gradient operation on a random branch. Further classification
experiments show that the latent representations learned with our
self-supervised method integrate more useful information from the additional
input data implicitly, thus leading to better reconstruction and classification
performance.

</details>


### [249] [Pseudo-labelling meets Label Smoothing for Noisy Partial Label Learning](https://arxiv.org/pdf/2402.04835)
*Darshana Saravanan, Naresh Manwani, Vineet Gandhi*

Main category: cs.CV

TL;DR: A framework for Noisy Partial Label Learning (NPLL) improves pseudo-label accuracy using weighted nearest neighbours and deep neural networks, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Weakly supervised learning is needed when perfect annotations are costly or require expertise, especially in fine-grained classification. NPLL extends Partial Label Learning by allowing noisy labels.

Method: The framework assigns pseudo-labels via weighted nearest neighbours, trains a deep neural network with label smoothing, and refines pseudo-labels using the classifier's features and predictions.

Result: State-of-the-art performance on seven datasets, with significant gains in fine-grained benchmarks and strong generalization in crowd-sourced datasets.

Conclusion: The proposed NPLL framework is effective and practical, demonstrating superior performance and generalization in weakly supervised settings.

Abstract: We motivate weakly supervised learning as an effective learning paradigm for
problems where curating perfectly annotated datasets is expensive and may
require domain expertise such as fine-grained classification. We focus on
Partial Label Learning (PLL), a weakly-supervised learning paradigm where each
training instance is paired with a set of candidate labels (partial label), one
of which is the true label. Noisy PLL (NPLL) relaxes this constraint by
allowing some partial labels to not contain the true label, enhancing the
practicality of the problem. Our work centres on NPLL and presents a framework
that initially assigns pseudo-labels to images by exploiting the noisy partial
labels through a weighted nearest neighbour algorithm. These pseudo-label and
image pairs are then used to train a deep neural network classifier with label
smoothing. The classifier's features and predictions are subsequently employed
to refine and enhance the accuracy of pseudo-labels. We perform thorough
experiments on seven datasets and compare against nine NPLL and PLL methods. We
achieve state-of-the-art results in all studied settings from the prior
literature, obtaining substantial gains in the simulated fine-grained
benchmarks. Further, we show the promising generalisation capability of our
framework in realistic, fine-grained, crowd-sourced datasets.

</details>


### [250] [LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models](https://arxiv.org/pdf/2406.05113)
*Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, Patrick Schramowski*

Main category: cs.CV

TL;DR: LlavaGuard is a VLM-based vision safeguard framework offering customizable safety tools, outperforming existing methods in accuracy and flexibility.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for reliable safety guardrails in large-scale data and models.

Method: Develops a safety taxonomy, preprocessing, augmentation, and training setup, using a human-annotated multimodal safety dataset.

Result: LlavaGuard models (0.5B-7B) excel in safety compliance evaluation and outperform state-of-the-art safeguards.

Conclusion: LlavaGuard is effective for large-scale dataset annotation and moderation, with its framework and resources made publicly available.

Abstract: This paper introduces LlavaGuard, a suite of VLM-based vision safeguards that
address the critical need for reliable guardrails in the era of large-scale
data and models. To this end, we establish a novel open framework, describing a
customizable safety taxonomy, data preprocessing, augmentation, and training
setup. For teaching a VLM safeguard on safety, we further create a multimodal
safety dataset with high-quality human expert annotations, where each image is
labeled with a safety rating, category, and rationale. We also employ advanced
augmentations to support context-specific assessments. The resulting LlavaGuard
models, ranging from 0.5B to 7B, serve as a versatile tool for evaluating the
safety compliance of visual content against flexible policies. In comprehensive
experiments, LlavaGuard outperforms both state-of-the-art safeguards and VLMs
in accuracy and in flexibly handling different policies. Additionally, we
demonstrate LlavaGuard's performance in two real-world applications:
large-scale dataset annotation and moderation of text-to-image models. We make
our entire framework, including the dataset, model weights, and training code.

</details>


### [251] [HilbertMamba: Local-Global Reciprocal Network for Uterine Fibroid Segmentation in Ultrasound Videos](https://arxiv.org/pdf/2407.05703)
*Huihui Xu, Yijun Yang, Angelica I Aviles-Rivero, Guang Yang, Jing Qin, Lei Zhu*

Main category: cs.CV

TL;DR: The paper introduces a new dataset (UFUV) for uterine fibroid segmentation and proposes LGRNet, a method combining local and global temporal context for improved segmentation.


<details>
  <summary>Details</summary>
Motivation: Early detection of uterine fibroids is vital for preventing malignant transformations, but existing methods lack efficient temporal context propagation.

Method: LGRNet uses Cyclic Neighborhood Propagation (CNP) for local context and Hilbert Selective Scan (HilbertSS) for global context, with reciprocal refinement.

Result: LGRNet outperforms state-of-the-art methods on UFUV and public VPS datasets.

Conclusion: LGRNet is effective and versatile for video segmentation tasks, with code and dataset publicly available.

Abstract: Regular screening and early discovery of uterine fibroid are crucial for
preventing potential malignant transformations and ensuring timely, life-saving
interventions. To this end, we collect and annotate the first ultrasound video
dataset with 100 videos for uterine fibroid segmentation (UFUV). We also
present Local-Global Reciprocal Network (LGRNet) to efficiently and effectively
propagate the long-term temporal context which is crucial to help distinguish
between uninformative noisy surrounding tissues and target lesion regions.
Specifically, the Cyclic Neighborhood Propagation (CNP) is introduced to
propagate the inter-frame local temporal context in a cyclic manner. Moreover,
to aggregate global temporal context, we first condense each frame into a set
of frame bottleneck queries and devise Hilbert Selective Scan (HilbertSS) to
both efficiently path connect each frame and preserve the locality bias. A
distribute layer is then utilized to disseminate back the global context for
reciprocal refinement. Extensive experiments on UFUV and three public Video
Polyp Segmentation (VPS) datasets demonstrate consistent improvements compared
to state-of-the-art segmentation methods, indicating the effectiveness and
versatility of LGRNet. Code, checkpoints, and dataset are available at
https://github.com/bio-mlhui/LGRNet

</details>


### [252] [SALVE: A 3D Reconstruction Benchmark of Wounds from Consumer-grade Videos](https://arxiv.org/pdf/2407.19652)
*Remi Chierchia, Leo Lebrat, David Ahmedt-Aristizabal, Olivier Salvado, Clinton Fookes, Rodrigo Santa Cruz*

Main category: cs.CV

TL;DR: The paper evaluates 3D wound reconstruction methods using consumer-grade videos, introducing the SALVE dataset and comparing photogrammetry and neural rendering approaches. Neural rendering shows promise for clinical use.


<details>
  <summary>Details</summary>
Motivation: Chronic wound management is a global challenge, and existing 2D methods are insufficient for 3D wound features. A thorough evaluation of 3D reconstruction methods is lacking.

Method: The study introduces the SALVE dataset with realistic wound phantom videos. It evaluates state-of-the-art 3D reconstruction methods, including photogrammetry and neural rendering.

Result: Photogrammetry lacks smooth surfaces for precise clinical measurements, while neural rendering shows potential for improving wound care practices.

Conclusion: Neural rendering approaches are promising for 3D wound reconstruction, advancing their clinical application. The SALVE dataset supports further research.

Abstract: Managing chronic wounds is a global challenge that can be alleviated by the
adoption of automatic systems for clinical wound assessment from consumer-grade
videos. While 2D image analysis approaches are insufficient for handling the 3D
features of wounds, existing approaches utilizing 3D reconstruction methods
have not been thoroughly evaluated. To address this gap, this paper presents a
comprehensive study on 3D wound reconstruction from consumer-grade videos.
Specifically, we introduce the SALVE dataset, comprising video recordings of
realistic wound phantoms captured with different cameras. Using this dataset,
we assess the accuracy and precision of state-of-the-art methods for 3D
reconstruction, ranging from traditional photogrammetry pipelines to advanced
neural rendering approaches. In our experiments, we observe that photogrammetry
approaches do not provide smooth surfaces suitable for precise clinical
measurements of wounds. Neural rendering approaches show promise in addressing
this issue, advancing the use of this technology in wound care practices. We
encourage the readers to visit the project page:
https://remichierchia.github.io/SALVE/.

</details>


### [253] [Explainable Concept Generation through Vision-Language Preference Learning for Understanding Neural Networks' Internal Representations](https://arxiv.org/pdf/2408.13438)
*Aditya Taparia, Som Sagar, Ransalu Senanayake*

Main category: cs.CV

TL;DR: The paper proposes a reinforcement learning-based method to automate concept image set creation for explaining neural networks, addressing the labor-intensive manual process.


<details>
  <summary>Details</summary>
Motivation: Manual collection of concept image sets for explaining neural networks is time-consuming and prone to missing important concepts.

Method: The authors frame concept image set creation as an image generation problem and introduce a reinforcement learning-based preference optimization (RLPO) algorithm to fine-tune a vision-language generative model.

Result: Experiments show the method efficiently and reliably generates diverse concepts that are hard to craft manually.

Conclusion: The proposed RLPO algorithm automates and improves concept-based explanations for neural networks, reducing manual effort.

Abstract: Understanding the inner representation of a neural network helps users
improve models. Concept-based methods have become a popular choice for
explaining deep neural networks post-hoc because, unlike most other explainable
AI techniques, they can be used to test high-level visual "concepts" that are
not directly related to feature attributes. For instance, the concept of
"stripes" is important to classify an image as a zebra. Concept-based
explanation methods, however, require practitioners to guess and manually
collect multiple candidate concept image sets, making the process
labor-intensive and prone to overlooking important concepts. Addressing this
limitation, in this paper, we frame concept image set creation as an image
generation problem. However, since naively using a standard generative model
does not result in meaningful concepts, we devise a reinforcement
learning-based preference optimization (RLPO) algorithm that fine-tunes a
vision-language generative model from approximate textual descriptions of
concepts. Through a series of experiments, we demonstrate our method's ability
to efficiently and reliably articulate diverse concepts that are otherwise
challenging to craft manually.

</details>


### [254] [VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters](https://arxiv.org/pdf/2408.17253)
*Mouxiang Chen, Lefei Shen, Zhuo Li, Xiaoyun Joy Wang, Jianling Sun, Chenghao Liu*

Main category: cs.CV

TL;DR: VisionTS leverages a visual masked autoencoder, pre-trained on ImageNet, for time series forecasting by reformulating it as an image reconstruction task, achieving state-of-the-art performance with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in cross-domain gaps and in-domain heterogeneity in existing TSF foundation models by exploring a novel approach using natural images.

Method: Reformulates time series forecasting as an image reconstruction task using a visual masked autoencoder pre-trained on ImageNet, without domain-specific adaptation.

Result: Achieves better zero-shot performance than existing models and state-of-the-art results with one-epoch fine-tuning.

Conclusion: Demonstrates intrinsic similarities between images and time series, suggesting visual models as a promising avenue for cross-modality TSF research.

Abstract: Foundation models have emerged as a promising approach in time series
forecasting (TSF). Existing approaches either repurpose large language models
(LLMs) or build large-scale time series datasets to develop TSF foundation
models for universal forecasting. However, these methods face challenges due to
the severe cross-domain gap or in-domain heterogeneity. This paper explores a
new road to building a TSF foundation model from rich, high-quality natural
images. Our key insight is that a visual masked autoencoder, pre-trained on the
ImageNet dataset, can naturally be a numeric series forecaster. By
reformulating TSF as an image reconstruction task, we bridge the gap between
image pre-training and TSF downstream tasks. Surprisingly, without further
adaptation in the time series domain, the proposed VisionTS could achieve
better zero-shot forecast performance than existing TSF foundation models. With
fine-tuning for one epoch, VisionTS could further improve the forecasting and
achieve state-of-the-art performance in most cases. Extensive experiments
reveal intrinsic similarities between images and real-world time series,
suggesting that visual models may offer a "free lunch" for TSF and highlight
the potential for future cross-modality research. Our code is publicly
available at https://github.com/Keytoyze/VisionTS.

</details>


### [255] [Flexiffusion: Segment-wise Neural Architecture Search for Flexible Denoising Schedule](https://arxiv.org/pdf/2409.17566)
*Hongtao Huang, Xiaojun Chang, Lina Yao*

Main category: cs.CV

TL;DR: Flexiffusion is a training-free NAS paradigm that accelerates diffusion models by optimizing generation steps and network structures, achieving significant speedups without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are computationally expensive due to sequential denoising steps. Existing NAS methods for diffusion are time-consuming. Flexiffusion aims to reduce search costs and improve efficiency.

Method: Flexiffusion partitions the generation process into segments with full, partial, and null steps, autonomously exploring flexible step combinations to minimize redundancy.

Result: Achieved speedups of 2.6× for LDM-4-G and 5.1× for Stable Diffusion V1.5 compared to SOTA, with verified performance across datasets.

Conclusion: Flexiffusion effectively reduces redundancy in diffusion models, offering a faster and more efficient alternative to existing methods.

Abstract: Diffusion models are cutting-edge generative models adept at producing
diverse, high-quality images. Despite their effectiveness, these models often
require significant computational resources owing to their numerous sequential
denoising steps and the significant inference cost of each step. Recently,
Neural Architecture Search (NAS) techniques have been employed to automatically
search for faster generation processes. However, NAS for diffusion is
inherently time-consuming as it requires estimating thousands of diffusion
models to search for the optimal one. In this paper, we introduce Flexiffusion,
a novel training-free NAS paradigm designed to accelerate diffusion models by
concurrently optimizing generation steps and network structures. Specifically,
we partition the generation process into isometric step segments, each
sequentially composed of a full step, multiple partial steps, and several null
steps. The full step computes all network blocks, while the partial step
involves part of the blocks, and the null step entails no computation.
Flexiffusion autonomously explores flexible step combinations for each segment,
substantially reducing search costs and enabling greater acceleration compared
to the state-of-the-art (SOTA) method for diffusion models. Our searched models
reported speedup factors of $2.6\times$ and $1.5\times$ for the original
LDM-4-G and the SOTA, respectively. The factors for Stable Diffusion V1.5 and
the SOTA are $5.1\times$ and $2.0\times$. We also verified the performance of
Flexiffusion on multiple datasets, and positive experiment results indicate
that Flexiffusion can effectively reduce redundancy in diffusion models.

</details>


### [256] [Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks](https://arxiv.org/pdf/2410.01744)
*Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Dong Yu, Meng Jiang*

Main category: cs.CV

TL;DR: Leopard is a multimodal large language model (MLLM) designed for tasks involving multiple text-rich images, addressing challenges like dataset scarcity and resolution balancing. It outperforms state-of-the-art models with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Text-rich images are common but challenging for MLLMs due to lack of datasets and resolution issues. Leopard aims to solve these problems.

Method: Leopard uses a curated dataset of 1M multimodal instructions and an adaptive high-resolution multi-image encoding module.

Result: Leopard outperforms models like Llama-3.2 and Qwen2-VL in text-rich, multi-image tasks, achieving high efficiency with 1.2M training instances.

Conclusion: Leopard demonstrates effectiveness in handling text-rich multi-image tasks, with open-sourced code and data for broader use.

Abstract: Text-rich images, where text serves as the central visual element guiding the
overall understanding, are prevalent in real-world applications, such as
presentation slides, scanned documents, and webpage snapshots. Tasks involving
multiple text-rich images are especially challenging, as they require not only
understanding the content of individual images but reasoning about
inter-relationships and logical flows across multiple visual inputs. Despite
the importance of these scenarios, current multimodal large language models
(MLLMs) struggle to handle such tasks due to two key challenges: (1) the
scarcity of high-quality instruction tuning datasets for text-rich multi-image
scenarios, and (2) the difficulty in balancing image resolution with visual
feature sequence length. To address these challenges, we propose Leopard, an
MLLM tailored for handling vision-language tasks involving multiple text-rich
images. First, we curated about one million high-quality multimodal
instruction-tuning data, tailored to text-rich, multi-image scenarios. Second,
we proposed an adaptive high-resolution multi-image encoding module to
dynamically optimize the allocation of visual sequence length based on the
original aspect ratios and resolutions of images. Experiments on a diverse set
of benchmarks reveal that our model consistently outperforms state-of-the-art
systems, such as Llama-3.2 and Qwen2-VL, in challenging text-rich, multi-image
evaluations. Remarkably, our approach achieves outstanding performance using
only 1.2M training instances, all of which are fully open-sourced,
demonstrating both high efficiency and effectiveness compared to models trained
on large-scale in-house data. Our code and data are available at
https://github.com/tencent-ailab/Leopard.

</details>


### [257] [In Search of Forgotten Domain Generalization](https://arxiv.org/pdf/2410.08258)
*Prasanna Mayilvahanan, Roland S. Zimmermann, Thaddäus Wiedemer, Evgenia Rusak, Attila Juhos, Matthias Bethge, Wieland Brendel*

Main category: cs.CV

TL;DR: The paper addresses OOD generalization challenges in foundation models, revealing that web-scale training data may create an illusion of OOD robustness. It introduces LAION-Natural and LAION-Rendition datasets to rigorously evaluate OOD performance and identifies optimal data mixing ratios for better generalization.


<details>
  <summary>Details</summary>
Motivation: To clarify OOD generalization in the era of web-scale datasets and foundation models, which risk test domain contamination, and to re-enable meaningful OOD robustness assessment.

Method: Created LAION-Natural and LAION-Rendition datasets, strictly OOD to ImageNet and DomainNet, and trained CLIP models on them. Explored mixing ratios of natural and rendition data.

Result: Found that much of CLIP's performance is due to in-domain examples, showing OOD challenges persist. Identified optimal data mixing ratios for cross-domain generalization.

Conclusion: OOD generalization challenges remain despite web-scale data, and careful dataset design is crucial for robust model evaluation and improvement.

Abstract: Out-of-Domain (OOD) generalization is the ability of a model trained on one
or more domains to generalize to unseen domains. In the ImageNet era of
computer vision, evaluation sets for measuring a model's OOD performance were
designed to be strictly OOD with respect to style. However, the emergence of
foundation models and expansive web-scale datasets has obfuscated this
evaluation process, as datasets cover a broad range of domains and risk test
domain contamination. In search of the forgotten domain generalization, we
create large-scale datasets subsampled from LAION -- LAION-Natural and
LAION-Rendition -- that are strictly OOD to corresponding ImageNet and
DomainNet test sets in terms of style. Training CLIP models on these datasets
reveals that a significant portion of their performance is explained by
in-domain examples. This indicates that the OOD generalization challenges from
the ImageNet era still prevail and that training on web-scale data merely
creates the illusion of OOD generalization. Furthermore, through a systematic
exploration of combining natural and rendition datasets in varying proportions,
we identify optimal mixing ratios for model generalization across these
domains. Our datasets and results re-enable meaningful assessment of OOD
robustness at scale -- a crucial prerequisite for improving model robustness.

</details>


### [258] [Modality-Fair Preference Optimization for Trustworthy MLLM Alignment](https://arxiv.org/pdf/2410.15334)
*Songtao Jiang, Yan Zhang, Ruizhe Chen, Tianxiang Hu, Yeying Jin, Qinglin He, Yang Feng, Jian Wu, Zuozhu Liu*

Main category: cs.CV

TL;DR: MFPO improves MLLM trustworthiness by aligning visual and textual modalities through a multimodal preference dataset, image reward loss, and iterative training.


<details>
  <summary>Details</summary>
Motivation: Addressing modality misalignment in MLLMs, which causes hallucination and undermines trustworthiness.

Method: Proposes MFPO with a multimodal preference dataset, image reward loss, and easy-to-hard iterative alignment.

Result: MFPO significantly enhances trustworthiness, enabling smaller models to match or surpass larger ones.

Conclusion: MFPO effectively aligns modalities, improving MLLM reliability in real-world applications.

Abstract: Multimodal large language models (MLLMs) have achieved remarkable success
across various tasks. However, separate training of visual and textual encoders
often results in a misalignment of the modality. Such misalignment may lead
models to generate content that is absent from the input image, a phenomenon
referred to as hallucination. These inaccuracies severely undermine the
trustworthiness of MLLMs in real-world applications. Despite attempts to
optimize text preferences to mitigate this issue, our initial investigation
indicates that the trustworthiness of MLLMs remains inadequate. Specifically,
these models tend to provide preferred answers even when the input image is
heavily distorted. Analysis of visual token attention also indicates that the
model focuses primarily on the surrounding context rather than the key object
referenced in the question. These findings highlight a misalignment between the
modalities, where answers inadequately leverage input images. Motivated by our
findings, we propose Modality-Fair Preference Optimization (MFPO), which
comprises three components: the construction of a multimodal preference dataset
in which dispreferred images differ from originals solely in key regions; an
image reward loss function encouraging the model to generate answers better
aligned with the input images; and an easy-to-hard iterative alignment strategy
to stabilize joint modality training. Extensive experiments on three
trustworthiness benchmarks demonstrate that MFPO significantly enhances the
trustworthiness of MLLMs. In particular, it enables the 7B models to attain
trustworthiness levels on par with, or even surpass, those of the 13B, 34B, and
larger models.

</details>


### [259] [CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP](https://arxiv.org/pdf/2410.23330)
*Tianyu Yang, Lisen Dai, Xiangqi Wang, Minhao Cheng, Yapeng Tian, Xiangliang Zhang*

Main category: cs.CV

TL;DR: CLIPErase is a novel method for machine unlearning in multimodal models like CLIP, effectively removing specific data associations without full retraining while preserving model performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored challenge of unlearning in multimodal models, particularly CLIP, to remove specific data associations without compromising performance.

Method: CLIPErase uses three modules: Forgetting Module (disrupts associations), Retention Module (preserves performance), and Consistency Module (maintains model consistency).

Result: Effective unlearning of designated associations in zero-shot tasks on CIFAR-100 and Flickr30K datasets, with retained model performance.

Conclusion: CLIPErase successfully tackles multimodal unlearning, balancing forgetting and retention without full retraining.

Abstract: Machine unlearning (MU) has gained significant attention as a means to remove
specific data from trained models without requiring a full retraining process.
While progress has been made in unimodal domains like text and image
classification, unlearning in multimodal models remains relatively
underexplored. In this work, we address the unique challenges of unlearning in
CLIP, a prominent multimodal model that aligns visual and textual
representations. We introduce CLIPErase, a novel approach that disentangles and
selectively forgets both visual and textual associations, ensuring that
unlearning does not compromise model performance. CLIPErase consists of three
key modules: a Forgetting Module that disrupts the associations in the forget
set, a Retention Module that preserves performance on the retain set, and a
Consistency Module that maintains consistency with the original model.
Extensive experiments on the CIFAR-100 and Flickr30K datasets across four CLIP
downstream tasks demonstrate that CLIPErase effectively forgets designated
associations in zero-shot tasks for multimodal samples, while preserving the
model's performance on the retain set after unlearning.

</details>


### [260] [From Prototypes to General Distributions: An Efficient Curriculum for Masked Image Modeling](https://arxiv.org/pdf/2411.10685)
*Jinhong Lin, Cheng-En Wu, Huanran Li, Jifan Zhang, Yu Hen Hu, Pedro Morgado*

Main category: cs.CV

TL;DR: A prototype-driven curriculum learning framework improves Masked Image Modeling (MIM) by addressing early-stage optimization challenges, enhancing training efficiency and representation quality.


<details>
  <summary>Details</summary>
Motivation: MIM's effectiveness is limited by early-stage optimization challenges where models struggle to learn complex distributions from partial observations before basic visual capabilities develop.

Method: Introduces a prototype-driven curriculum learning framework with a temperature-based annealing scheme to gradually expand the training distribution.

Result: Significantly improves training efficiency and representation quality on ImageNet-1K, requiring fewer epochs than standard Masked Auto-Encoding.

Conclusion: Controlling the order of training examples is crucial in self-supervised visual learning, offering a practical solution to early-stage MIM challenges.

Abstract: Masked Image Modeling (MIM) has emerged as a powerful self-supervised
learning paradigm for visual representation learning, enabling models to
acquire rich visual representations by predicting masked portions of images
from their visible regions. While this approach has shown promising results, we
hypothesize that its effectiveness may be limited by optimization challenges
during early training stages, where models are expected to learn complex image
distributions from partial observations before developing basic visual
processing capabilities. To address this limitation, we propose a
prototype-driven curriculum leagrning framework that structures the learning
process to progress from prototypical examples to more complex variations in
the dataset. Our approach introduces a temperature-based annealing scheme that
gradually expands the training distribution, enabling more stable and efficient
learning trajectories. Through extensive experiments on ImageNet-1K, we
demonstrate that our curriculum learning strategy significantly improves both
training efficiency and representation quality while requiring substantially
fewer training epochs compared to standard Masked Auto-Encoding. Our findings
suggest that carefully controlling the order of training examples plays a
crucial role in self-supervised visual learning, providing a practical solution
to the early-stage optimization challenges in MIM.

</details>


### [261] [Birth and Death of a Rose](https://arxiv.org/pdf/2412.05278)
*Chen Geng, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu*

Main category: cs.CV

TL;DR: A method for generating temporally evolving 3D object intrinsics (geometry, reflectance, texture) using pre-trained 2D diffusion models, ensuring temporal consistency with Neural Templates.


<details>
  <summary>Details</summary>
Motivation: To automate the creation of dynamic 3D objects (e.g., a blooming rose) without manual effort, leveraging pre-trained 2D models.

Method: Uses signals from pre-trained 2D diffusion models and Neural Templates for temporal-state-guided distillation to ensure consistency.

Result: Generates high-quality temporal object intrinsics, allowing controllable rendering from any viewpoint, lighting, or time.

Conclusion: The method successfully automates dynamic 3D object generation, offering flexibility and quality.

Abstract: We study the problem of generating temporal object intrinsics -- temporally
evolving sequences of object geometry, reflectance, and texture, such as a
blooming rose -- from pre-trained 2D foundation models. Unlike conventional 3D
modeling and animation techniques that require extensive manual effort and
expertise, we introduce a method that generates such assets with signals
distilled from pre-trained 2D diffusion models. To ensure the temporal
consistency of object intrinsics, we propose Neural Templates for
temporal-state-guided distillation, derived automatically from image features
from self-supervised learning. Our method can generate high-quality temporal
object intrinsics for several natural phenomena and enable the sampling and
controllable rendering of these dynamic objects from any viewpoint, under any
environmental lighting conditions, at any time of their lifespan. Project
website: https://chen-geng.com/rose4d

</details>


### [262] [Normalizing Flows are Capable Generative Models](https://arxiv.org/pdf/2412.06329)
*Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, Josh Susskind*

Main category: cs.CV

TL;DR: TarFlow, a Transformer-based NF model, outperforms previous methods in likelihood estimation and matches diffusion models in sample quality.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the untapped potential of Normalizing Flows (NFs) and propose a scalable, high-performance NF architecture.

Method: Introduces TarFlow, a Transformer-based variant of Masked Autoregressive Flows (MAFs), with techniques like noise augmentation, denoising, and guidance.

Result: Achieves state-of-the-art likelihood estimation and generates high-quality, diverse samples comparable to diffusion models.

Conclusion: TarFlow proves NFs are more powerful than believed, offering a competitive alternative to diffusion models.

Abstract: Normalizing Flows (NFs) are likelihood-based models for continuous inputs.
They have demonstrated promising results on both density estimation and
generative modeling tasks, but have received relatively little attention in
recent years. In this work, we demonstrate that NFs are more powerful than
previously believed. We present TarFlow: a simple and scalable architecture
that enables highly performant NF models. TarFlow can be thought of as a
Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of
a stack of autoregressive Transformer blocks on image patches, alternating the
autoregression direction between layers. TarFlow is straightforward to train
end-to-end, and capable of directly modeling and generating pixels. We also
propose three key techniques to improve sample quality: Gaussian noise
augmentation during training, a post training denoising procedure, and an
effective guidance method for both class-conditional and unconditional
settings. Putting these together, TarFlow sets new state-of-the-art results on
likelihood estimation for images, beating the previous best methods by a large
margin, and generates samples with quality and diversity comparable to
diffusion models, for the first time with a stand-alone NF model. We make our
code available at https://github.com/apple/ml-tarflow.

</details>


### [263] [Illusion3D: 3D Multiview Illusion with 2D Diffusion Priors](https://arxiv.org/pdf/2412.09625)
*Yue Feng, Vaibhav Sanjay, Spencer Lutz, Badour AlBahar, Songwei Ge, Jia-Bin Huang*

Main category: cs.CV

TL;DR: A method for generating 3D multiview illusions using text prompts or images, leveraging diffusion models and differentiable rendering.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional and recent methods in creating expressive and versatile multiview illusions.

Method: Uses a pre-trained text-to-image diffusion model to optimize textures and geometry of neural 3D representations via differentiable rendering.

Result: Produces diverse 3D illusions with distinct interpretations from multiple angles.

Conclusion: The approach is effective for generating high-quality 3D multiview illusions, expanding artistic and practical applications.

Abstract: Automatically generating multiview illusions is a compelling challenge, where
a single piece of visual content offers distinct interpretations from different
viewing perspectives. Traditional methods, such as shadow art and wire art,
create interesting 3D illusions but are limited to simple visual outputs (i.e.,
figure-ground or line drawing), restricting their artistic expressiveness and
practical versatility. Recent diffusion-based illusion generation methods can
generate more intricate designs but are confined to 2D images. In this work, we
present a simple yet effective approach for creating 3D multiview illusions
based on user-provided text prompts or images. Our method leverages a
pre-trained text-to-image diffusion model to optimize the textures and geometry
of neural 3D representations through differentiable rendering. When viewed from
multiple angles, this produces different interpretations. We develop several
techniques to improve the quality of the generated 3D multiview illusions. We
demonstrate the effectiveness of our approach through extensive experiments and
showcase illusion generation with diverse 3D forms.

</details>


### [264] [Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google Earth and Gaussian Splatting](https://arxiv.org/pdf/2501.00625)
*Kyle Gao, Liangzhi Li, Hongjie He, Dening Lu, Linlin Xu, Jonathan Li*

Main category: cs.CV

TL;DR: A pipeline combining SAM2+GroundingDINO, Gaussian Splatting, and mask refinement extracts 3D building meshes from text or click prompts without labeled data.


<details>
  <summary>Details</summary>
Motivation: To enable 3D mesh extraction of buildings using open-source models and minimal user input, eliminating the need for labeled datasets.

Method: Integrates SAM2+GroundingDINO for object segmentation, Gaussian Splatting for 3D scene representation, and mask refinement techniques for improved accuracy.

Result: A pipeline capable of generating 3D building meshes from prompts like names, addresses, or coordinates.

Conclusion: The method provides a practical, label-free solution for 3D building reconstruction using accessible tools and user-friendly prompts.

Abstract: Recently released open-source pre-trained foundational image segmentation and
object detection models (SAM2+GroundingDINO) allow for geometrically consistent
segmentation of objects of interest in multi-view 2D images. Users can use
text-based or click-based prompts to segment objects of interest without
requiring labeled training datasets. Gaussian Splatting allows for the learning
of the 3D representation of a scene's geometry and radiance based on 2D images.
Combining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and
our improvements in mask refinement based on morphological operations and
contour simplification, we created a pipeline to extract the 3D mesh of any
building based on its name, address, or geographic coordinates.

</details>


### [265] [ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think](https://arxiv.org/pdf/2501.01045)
*Tao Feng, Wei Li, Didi Zhu, Hangjie Yuan, Wendi Zheng, Dan Zhang, Jie Tang*

Main category: cs.CV

TL;DR: ZeroFlow is a benchmark for gradient-free optimization in continual learning, showing forward passes can mitigate forgetting.


<details>
  <summary>Details</summary>
Motivation: Gradient bans (lack of gradient access) hinder continual learning; ZeroFlow addresses this by evaluating gradient-free methods.

Method: ZeroFlow tests forward pass-based methods across algorithms, scenarios, and datasets.

Result: Forward passes alone can mitigate forgetting, with new principles and enhancements improving performance.

Conclusion: ZeroFlow advances forward-pass-based continual learning, offering tools and insights for gradient-free optimization.

Abstract: Backpropagation provides a generalized configuration for overcoming
catastrophic forgetting. Optimizers such as SGD and Adam are commonly used for
weight updates in continual learning and continual pre-training. However,
access to gradient information is not always feasible in practice due to
black-box APIs, hardware constraints, or non-differentiable systems, a
challenge we refer to as the gradient bans. To bridge this gap, we introduce
ZeroFlow, the first benchmark designed to evaluate gradient-free optimization
algorithms for overcoming forgetting. ZeroFlow examines a suite of forward
pass-based methods across various algorithms, forgetting scenarios, and
datasets. Our results show that forward passes alone can be sufficient to
mitigate forgetting. We uncover novel optimization principles that highlight
the potential of forward pass-based methods in mitigating forgetting, managing
task conflicts, and reducing memory demands. Additionally, we propose new
enhancements that further improve forgetting resistance using only forward
passes. This work provides essential tools and insights to advance the
development of forward-pass-based methods for continual learning.

</details>


### [266] [UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control](https://arxiv.org/pdf/2502.05749)
*Kaizhen Zhu, Mokai Pan, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang, Ye Shi*

Main category: cs.CV

TL;DR: UniDB introduces a unified diffusion bridge framework using Stochastic Optimal Control (SOC), improving detail preservation and output quality over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion bridge models using Doob's $h$-transform often produce blurred results and lack theoretical grounding.

Method: UniDB formulates the problem via SOC optimization, deriving a closed-form solution for the optimal controller, unifying existing models.

Result: UniDB outperforms existing methods, balancing control costs and terminal penalties for better detail preservation.

Conclusion: UniDB provides a versatile and superior framework for diffusion bridges, requiring minimal code changes for integration.

Abstract: Recent advances in diffusion bridge models leverage Doob's $h$-transform to
establish fixed endpoints between distributions, demonstrating promising
results in image translation and restoration tasks. However, these approaches
frequently produce blurred or excessively smoothed image details and lack a
comprehensive theoretical foundation to explain these shortcomings. To address
these limitations, we propose UniDB, a unified framework for diffusion bridges
based on Stochastic Optimal Control (SOC). UniDB formulates the problem through
an SOC-based optimization and derives a closed-form solution for the optimal
controller, thereby unifying and generalizing existing diffusion bridge models.
We demonstrate that existing diffusion bridges employing Doob's $h$-transform
constitute a special case of our framework, emerging when the terminal penalty
coefficient in the SOC cost function tends to infinity. By incorporating a
tunable terminal penalty coefficient, UniDB achieves an optimal balance between
control costs and terminal penalties, substantially improving detail
preservation and output quality. Notably, UniDB seamlessly integrates with
existing diffusion bridge models, requiring only minimal code modifications.
Extensive experiments across diverse image restoration tasks validate the
superiority and adaptability of the proposed framework. Our code is available
at https://github.com/UniDB-SOC/UniDB/.

</details>


### [267] [A Comprehensive Survey on Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/pdf/2502.14896)
*Changhoon Kim, Yanjun Qi*

Main category: cs.CV

TL;DR: A survey on concept erasure in Text-to-Image (T2I) models, categorizing methods, discussing challenges, and providing resources for future research.


<details>
  <summary>Details</summary>
Motivation: Address ethical and legal concerns of T2I models generating copyrighted, sensitive, or harmful content by proactively modifying models to prevent undesired outputs.

Method: Categorizes concept erasure into fine-tuning, closed-form solutions, and inference-time interventions; also covers adversarial attacks and defenses.

Result: Provides a structured overview of methods, datasets, metrics, and benchmarks for evaluating erasure effectiveness and model robustness.

Conclusion: Serves as a comprehensive resource for understanding concept erasure, its challenges, and future research directions.

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating
high-quality, diverse visual content from natural language prompts. However,
their ability to reproduce copyrighted styles, sensitive imagery, and harmful
content raises significant ethical and legal concerns. Concept erasure offers a
proactive alternative to external filtering by modifying T2I models to prevent
the generation of undesired content. In this survey, we provide a structured
overview of concept erasure, categorizing existing methods based on their
optimization strategies and the architectural components they modify. We
categorize concept erasure methods into fine-tuning for parameter updates,
closed-form solutions for efficient edits, and inference-time interventions for
content restriction without weight modification. Additionally, we explore
adversarial attacks that bypass erasure techniques and discuss emerging
defenses. To support further research, we consolidate key datasets, evaluation
metrics, and benchmarks for assessing erasure effectiveness and model
robustness. This survey serves as a comprehensive resource, offering insights
into the evolving landscape of concept erasure, its challenges, and future
directions.

</details>


### [268] [CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness](https://arxiv.org/pdf/2502.14914)
*Zhihang Liu, Chen-Wei Xie, Bin Wen, Feiwu Yu, Jixuan Chen, Pandeng Li, Boqiang Zhang, Nianzu Yang, Yinglu Li, Zuan Gao, Yun Zheng, Hongtao Xie*

Main category: cs.CV

TL;DR: CAPability is a multi-view benchmark for evaluating visual captioning in MLLMs, addressing outdated metrics with 12 dimensions and 11K annotated images/videos.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to assess detailed captions effectively due to outdated metrics and limited visual coverage.

Method: Introduces CAPability with precision, hit, and K¯T metrics, using QA pairs for evaluation across 12 dimensions.

Result: CAPability provides stable assessment of caption correctness and thoroughness, revealing MLLMs' strengths and weaknesses.

Conclusion: The benchmark guides future research to improve specific captioning capabilities in MLLMs.

Abstract: Visual captioning benchmarks have become outdated with the emergence of
modern multimodal large language models (MLLMs), as the brief ground-truth
sentences and traditional metrics fail to assess detailed captions effectively.
While recent benchmarks attempt to address this by focusing on keyword
extraction or object-centric evaluation, they remain limited to vague-view or
object-view analyses and incomplete visual element coverage. In this paper, we
introduce CAPability, a comprehensive multi-view benchmark for evaluating
visual captioning across 12 dimensions spanning six critical views. We curate
nearly 11K human-annotated images and videos with visual element annotations to
evaluate the generated captions. CAPability stably assesses both the
correctness and thoroughness of captions with \textit{precision} and
\textit{hit} metrics. By converting annotations to QA pairs, we further
introduce a heuristic metric, \textit{know but cannot tell} ($K\bar{T}$),
indicating a significant performance gap between QA and caption capabilities.
Our work provides a holistic analysis of MLLMs' captioning abilities, as we
identify their strengths and weaknesses across various dimensions, guiding
future research to enhance specific aspects of their capabilities.

</details>


### [269] [Feedforward Few-shot Species Range Estimation](https://arxiv.org/pdf/2502.14977)
*Christian Lange, Max Hamilton, Elijah Cole, Alexander Shepard, Samuel Heinrich, Angela Zhu, Subhransu Maji, Grant Van Horn, Oisin Mac Aodha*

Main category: cs.CV

TL;DR: A new few-shot species range estimation method is proposed to accurately predict species ranges from limited data, outperforming existing approaches in speed and performance.


<details>
  <summary>Details</summary>
Motivation: Accurate species range mapping is vital for understanding biodiversity impacts from climate change and habitat loss, but current data is limited for most species.

Method: The model uses spatial locations and optional metadata (text or images) to generate a species encoding for predicting ranges of unseen species.

Result: Achieves state-of-the-art performance on benchmarks with significantly reduced compute time.

Conclusion: The approach effectively addresses the challenge of estimating species ranges from sparse data, offering practical benefits for ecological research.

Abstract: Knowing where a particular species can or cannot be found on Earth is crucial
for ecological research and conservation efforts. By mapping the spatial ranges
of all species, we would obtain deeper insights into how global biodiversity is
affected by climate change and habitat loss. However, accurate range estimates
are only available for a relatively small proportion of all known species. For
the majority of the remaining species, we typically only have a small number of
records denoting the spatial locations where they have previously been
observed. We outline a new approach for few-shot species range estimation to
address the challenge of accurately estimating the range of a species from
limited data. During inference, our model takes a set of spatial locations as
input, along with optional metadata such as text or an image, and outputs a
species encoding that can be used to predict the range of a previously unseen
species in a feedforward manner. We evaluate our approach on two challenging
benchmarks, where we obtain state-of-the-art range estimation performance, in a
fraction of the compute time, compared to recent alternative approaches.

</details>


### [270] [On the Importance of Text Preprocessing for Multimodal Representation Learning and Pathology Report Generation](https://arxiv.org/pdf/2502.19285)
*Ruben T. Lucassen, Tijn van de Luijtgaarden, Sander P. J. Moonemans, Gerben E. Breimer, Willeke A. M. Blokx, Mitko Veta*

Main category: cs.CV

TL;DR: Training vision-language models on preprocessed pathology reports (excluding non-inferable data) reduces hallucination in generated reports but slightly worsens cross-modal retrieval performance compared to using full reports.


<details>
  <summary>Details</summary>
Motivation: To address hallucination in generated reports caused by training on pathology reports with non-inferable data (e.g., patient history).

Method: Compare models trained on full reports vs. preprocessed reports (only H&E slide descriptions) using BLIP-2 framework on a melanocytic lesion dataset. Evaluate via retrieval tasks and expert qualitative assessment.

Result: Preprocessing prevents hallucination in reports but full reports yield better cross-modal retrieval.

Conclusion: Trade-off exists between report quality (preprocessed) and retrieval performance (full reports).

Abstract: Vision-language models in pathology enable multimodal case retrieval and
automated report generation. Many of the models developed so far, however, have
been trained on pathology reports that include information which cannot be
inferred from paired whole slide images (e.g., patient history), potentially
leading to hallucinated sentences in generated reports. To this end, we
investigate how the selection of information from pathology reports for
vision-language modeling affects the quality of the multimodal representations
and generated reports. More concretely, we compare a model trained on full
reports against a model trained on preprocessed reports that only include
sentences describing the cell and tissue appearances based on the H&E-stained
slides. For the experiments, we built upon the BLIP-2 framework and used a
cutaneous melanocytic lesion dataset of 42,433 H&E-stained whole slide images
and 19,636 corresponding pathology reports. Model performance was assessed
using image-to-text and text-to-image retrieval, as well as qualitative
evaluation of the generated reports by an expert pathologist. Our results
demonstrate that text preprocessing prevents hallucination in report
generation. Despite the improvement in the quality of the generated reports,
training the vision-language model on full reports showed better cross-modal
retrieval performance.

</details>


### [271] [A novel non-convex minimax $p$-th order concave penalty function approach to low-rank tensor completion](https://arxiv.org/pdf/2502.19979)
*Hongbing Zhang, Bing Zheng*

Main category: cs.CV

TL;DR: The paper proposes a novel minimax $p$-th order concave penalty (MPCP) function to improve tensor recovery in low-rank tensor completion (LRTC), addressing limitations of the MCP method.


<details>
  <summary>Details</summary>
Motivation: The MCP non-convex relaxation method, while effective, inadequately penalizes small singular values, leading to inefficient tensor recovery.

Method: A new MPCP function and tensor $p$-th order $\tau$ norm are introduced for non-convex rank approximation, forming an MPCP-based LRTC model with theoretical convergence guarantees.

Result: Extensive experiments show the proposed method outperforms state-of-the-art techniques in visual quality and quantitative metrics.

Conclusion: The MPCP-based approach enhances tensor recovery performance, validated by superior experimental results.

Abstract: The low-rank tensor completion (LRTC) problem aims to reconstruct a tensor
from partial sample information, which has attracted significant interest in a
wide range of practical applications such as image processing and computer
vision. Among the various techniques employed for the LRTC problem, non-convex
relaxation methods have been widely studied for their effectiveness in handling
tensor singular values, which are crucial for accurate tensor recovery. While
the minimax concave penalty (MCP) non-convex relaxation method has achieved
promising results in tackling the LRTC problem and gained widely adopted, it
exhibits a notable limitation: insufficient penalty on small singular values
during the singular value handling process, resulting in inefficient tensor
recovery. To address this issue and enhance recovery performance, a novel
minimax $p$-th order concave penalty (MPCP) function is proposed. Based on this
novel function, a tensor $p$-th order $\tau$ norm is proposed as a non-convex
relaxation for tensor rank approximation, thereby establishing an MPCP-based
LRTC model. Furthermore, theoretical convergence guarantees are rigorously
established for the proposed method. Extensive numerical experiments conducted
on multiple real datasets demonstrate that the proposed method outperforms the
state-of-the-art methods in both visual quality and quantitative metrics.

</details>


### [272] [ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability](https://arxiv.org/pdf/2503.06542)
*Jianwen Sun, Yukang Feng, Chuanhao Li, Fanrui Zhang, Zizhen Li, Jiaxin Ai, Sizhuo Zhou, Yu Dai, Shenglin Zhang, Kaipeng Zhang*

Main category: cs.CV

TL;DR: ARMOR is a resource-efficient autoregressive framework that enhances multimodal large language models (MLLMs) for unified understanding and generation of interleaved text-image content.


<details>
  <summary>Details</summary>
Motivation: Existing unified multimodal models (UniMs) are computationally intensive and struggle with interleaved text-image generation. ARMOR aims to address these limitations efficiently.

Method: ARMOR extends MLLMs with an asymmetric encoder-decoder architecture, a curated interleaved dataset, and a 'what or how to generate' training algorithm in three stages.

Result: ARMOR successfully upgrades MLLMs to UniMs with strong image generation capabilities using minimal resources.

Conclusion: ARMOR provides a practical solution for unified multimodal understanding and generation, with promising results and efficient resource use.

Abstract: Unified multimodal understanding and generation have recently received much
attention in the area of vision and language. Existing UniMs are designed to
simultaneously learn both multimodal understanding and generation capabilities,
demanding substantial computational resources, and often struggle to generate
interleaved text-image. We present ARMOR, a resource-efficient and pure
autoregressive framework that achieves both understanding and generation by
fine-tuning existing multimodal large language models (MLLMs). Specifically,
ARMOR extends existing MLLMs from three perspectives: (1) For model
architecture, an asymmetric encoder-decoder architecture with a
forward-switching mechanism is introduced to unify embedding space integrating
textual and visual modalities for enabling natural text-image interleaved
generation with minimal computational overhead. (2) For training data, a
meticulously curated, high-quality interleaved dataset is collected for
fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how
to generate'' algorithm to empower existing MLLMs with multimodal generation
capabilities while preserving their multimodal understanding capabilities,
through three progressive training stages based on the collected dataset.
Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs
with promising image generation capabilities, using limited training resources.
Our code will be released soon at https://github.com/finyorko/armor.

</details>


### [273] [TT-Occ: Test-Time Compute for Self-Supervised Occupancy via Spatio-Temporal Gaussian Splatting](https://arxiv.org/pdf/2503.08485)
*Fengyi Zhang, Huitong Yang, Zheng Zhang, Zi Huang, Yadan Luo*

Main category: cs.CV

TL;DR: TT-Occ is a test-time 3D occupancy prediction framework using 3D Gaussians and vision foundation models for flexible, efficient scene understanding without retraining.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and inflexibility of current 3D occupancy prediction methods, which require extensive training and struggle with varying resolutions or new object categories.

Method: Lift-track-voxelize approach: lift geometry/semantics from VLMs to 3D Gaussians, track dynamic Gaussians, and voxelize at arbitrary resolutions.

Result: Validated on Occ3D and nuCraft benchmarks, showing generality and effectiveness with LiDAR and vision-centric variants.

Conclusion: TT-Occ provides a practical, flexible solution for 3D occupancy prediction without retraining, leveraging VLMs and 3D Gaussians.

Abstract: Self-supervised 3D occupancy prediction offers a promising solution for
understanding complex driving scenes without requiring costly 3D annotations.
However, training dense occupancy decoders to capture fine-grained geometry and
semantics can demand hundreds of GPU hours, and once trained, such models
struggle to adapt to varying voxel resolutions or novel object categories
without extensive retraining. To overcome these limitations, we propose a
practical and flexible test-time occupancy prediction framework termed TT-Occ.
Our method incrementally constructs, optimizes and voxelizes time-aware 3D
Gaussians from raw sensor streams by integrating vision foundation models
(VLMs) at runtime. The flexible nature of 3D Gaussians allows voxelization at
arbitrary user-specified resolutions, while the generalization ability of VLMs
enables accurate perception and open-vocabulary recognition, without any
network training or fine-tuning. Specifically, TT-Occ operates in a
lift-track-voxelize symphony: We first lift the geometry and semantics of
surrounding-view extracted from VLMs to instantiate Gaussians at 3D space;
Next, we track dynamic Gaussians while accumulating static ones to complete the
scene and enforce temporal consistency; Finally, we voxelize the optimized
Gaussians to generate occupancy prediction. Optionally, inherent noise in VLM
predictions and tracking is mitigated by periodically smoothing neighboring
Gaussians during optimization. To validate the generality and effectiveness of
our framework, we offer two variants: one LiDAR-based and one vision-centric,
and conduct extensive experiments on Occ3D and nuCraft benchmarks with varying
voxel resolutions. Code will be available at
https://github.com/Xian-Bei/TT-Occ.

</details>


### [274] [TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation](https://arxiv.org/pdf/2503.11423)
*Hongxiang Zhao, Xingchen Liu, Mutian Xu, Yiming Hao, Weikai Chen, Xiaoguang Han*

Main category: cs.CV

TL;DR: The paper introduces TASTE-Rob, a large-scale dataset for hand-object interaction videos, and a pose-refinement pipeline to improve video generation for robotic imitation learning.


<details>
  <summary>Details</summary>
Motivation: Existing datasets like Ego4D have inconsistent views and misaligned interactions, limiting their use for precise imitation learning.

Method: A Video Diffusion Model (VDM) is fine-tuned on TASTE-Rob, and a three-stage pose-refinement pipeline is introduced to enhance hand posture accuracy.

Result: The approach improves video quality and generalizable robotic manipulation, though occasional hand posture inconsistencies remain.

Conclusion: TASTE-Rob and the pose-refinement framework advance high-quality video generation for robotic tasks, with the dataset and code publicly available.

Abstract: We address key limitations in existing datasets and models for task-oriented
hand-object interaction video generation, a critical approach of generating
video demonstrations for robotic imitation learning. Current datasets, such as
Ego4D, often suffer from inconsistent view perspectives and misaligned
interactions, leading to reduced video quality and limiting their applicability
for precise imitation learning tasks. Towards this end, we introduce TASTE-Rob
-- a pioneering large-scale dataset of 100,856 ego-centric hand-object
interaction videos. Each video is meticulously aligned with language
instructions and recorded from a consistent camera viewpoint to ensure
interaction clarity. By fine-tuning a Video Diffusion Model (VDM) on TASTE-Rob,
we achieve realistic object interactions, though we observed occasional
inconsistencies in hand grasping postures. To enhance realism, we introduce a
three-stage pose-refinement pipeline that improves hand posture accuracy in
generated videos. Our curated dataset, coupled with the specialized
pose-refinement framework, provides notable performance gains in generating
high-quality, task-oriented hand-object interaction videos, resulting in
achieving superior generalizable robotic manipulation. The TASTE-Rob dataset is
publicly available to foster further advancements in the field, TASTE-Rob
dataset and source code will be made publicly available on our website
https://taste-rob.github.io.

</details>


### [275] [Imitating Radiological Scrolling: A Global-Local Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification](https://arxiv.org/pdf/2503.20652)
*Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel*

Main category: cs.CV

TL;DR: CT-Scroll is a novel global-local attention model for multi-label classification of 3D CT scans, mimicking radiologists' scrolling behavior to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: The growing workload of radiologists and limitations of existing methods (CNNs and Vision Transformers) in capturing long-range dependencies and radiologists' navigational behavior.

Method: CT-Scroll, a global-local attention model designed to emulate radiologists' scrolling behavior during CT scan analysis.

Result: Evaluated on two public datasets, CT-Scroll shows efficacy through experiments and ablation studies.

Conclusion: CT-Scroll effectively addresses the challenges of 3D CT scan classification by combining global context and local detail awareness.

Abstract: The rapid increase in the number of Computed Tomography (CT) scan
examinations has created an urgent need for automated tools, such as organ
segmentation, anomaly classification, and report generation, to assist
radiologists with their growing workload. Multi-label classification of
Three-Dimensional (3D) CT scans is a challenging task due to the volumetric
nature of the data and the variety of anomalies to be detected. Existing deep
learning methods based on Convolutional Neural Networks (CNNs) struggle to
capture long-range dependencies effectively, while Vision Transformers require
extensive pre-training, posing challenges for practical use. Additionally,
these existing methods do not explicitly model the radiologist's navigational
behavior while scrolling through CT scan slices, which requires both global
context understanding and local detail awareness. In this study, we present
CT-Scroll, a novel global-local attention model specifically designed to
emulate the scrolling behavior of radiologists during the analysis of 3D CT
scans. Our approach is evaluated on two public datasets, demonstrating its
efficacy through comprehensive experiments and an ablation study that
highlights the contribution of each model component.

</details>


### [276] [Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](https://arxiv.org/pdf/2504.02821)
*Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata*

Main category: cs.CV

TL;DR: Sparse Autoencoders (SAEs) improve interpretability and steerability in Vision-Language Models (VLMs), enhancing monosemanticity and enabling direct intervention without model changes.


<details>
  <summary>Details</summary>
Motivation: Interpretability and steerability are critical for AI safety, and SAEs have shown promise in Large Language Models (LLMs). This work extends SAEs to VLMs to improve their interpretability and control.

Method: A framework for evaluating monosemanticity in vision representations is introduced, supported by a benchmark from a large-scale user study. SAEs are trained on VLMs like CLIP.

Result: SAEs significantly enhance monosemanticity in VLMs, with sparsity and wide latents being key factors. SAE interventions on CLIP's vision encoder steer multimodal LLM outputs without model modifications.

Conclusion: SAEs are a practical and effective unsupervised tool for improving interpretability and control in VLMs, as demonstrated with CLIP and LLaVA.

Abstract: Given that interpretability and steerability are crucial to AI safety, Sparse
Autoencoders (SAEs) have emerged as a tool to enhance them in Large Language
Models (LLMs). In this work, we extend the application of SAEs to
Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive
framework for evaluating monosemanticity at the neuron-level in vision
representations. To ensure that our evaluation aligns with human perception, we
propose a benchmark derived from a large-scale user study. Our experimental
results reveal that SAEs trained on VLMs significantly enhance the
monosemanticity of individual neurons, with sparsity and wide latents being the
most influential factors. Notably, we demonstrate that applying SAE
interventions on CLIP's vision encoder directly steers multimodal LLM outputs
(e.g., LLaVA), without any modifications to the underlying model. These
findings emphasize the practicality and efficacy of SAEs as an unsupervised
tool for enhancing both interpretability and control of VLMs. Code is available
at https://github.com/ExplainableML/sae-for-vlm.

</details>


### [277] [Seeing like a Cephalopod: Colour Vision with a Monochrome Event Camera](https://arxiv.org/pdf/2504.10984)
*Sami Arja, Nimrod Kruger, Alexandre Marcireau, Nicholas Owen Ralph, Saeed Afshar, Gregory Cohen*

Main category: cs.CV

TL;DR: A bio-inspired spectral imaging system mimics cephalopod vision using a ball lens and event-based camera, achieving wavelength-dependent focusing without color filters.


<details>
  <summary>Details</summary>
Motivation: Cephalopods' unique color discrimination despite having one photoreceptor type inspired the design of a spectral imaging system.

Method: Combines a ball lens with an event-based camera, using motorized focal shifts to mimic cephalopod lens motion.

Result: Achieves wavelength-dependent focusing across visible and near-infrared spectra, validated in simulations and real setups.

Conclusion: The approach offers robust spectral sensing without filters or demosaicing, paving the way for nature-inspired spectral systems.

Abstract: Cephalopods exhibit unique colour discrimination capabilities despite having
one type of photoreceptor, relying instead on chromatic aberration induced by
their ocular optics and pupil shapes to perceive spectral information. We took
inspiration from this biological mechanism to design a spectral imaging system
that combines a ball lens with an event-based camera. Our approach relies on a
motorised system that shifts the focal position, mirroring the adaptive lens
motion in cephalopods. This approach has enabled us to achieve
wavelength-dependent focusing across the visible light and near-infrared
spectrum, making the event a spectral sensor. We characterise chromatic
aberration effects, using both event-based and conventional frame-based
sensors, validating the effectiveness of bio-inspired spectral discrimination
both in simulation and in a real setup as well as assessing the spectral
discrimination performance. Our proposed approach provides a robust spectral
sensing capability without conventional colour filters or computational
demosaicing. This approach opens new pathways toward new spectral sensing
systems inspired by nature's evolutionary solutions. Code and analysis are
available at: https://samiarja.github.io/neuromorphic_octopus_eye/

</details>


### [278] [YOLO-RS: Remote Sensing Enhanced Crop Detection Methods](https://arxiv.org/pdf/2504.11165)
*Linlin Xiao, Zhang Tiancong, Yutong Jia, Xinyu Nie, Mengyao Wang, Xiaohang Shao*

Main category: cs.CV

TL;DR: YOLO-RS, a novel target detection model, improves small target detection in remote sensing images using Context Anchor Attention and multi-scale feature fusion, achieving better performance with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing methods perform poorly in detecting small targets in complex remote sensing images, limiting practical applications.

Method: YOLO-RS enhances Yolov11 with Context Anchor Attention (CAA), bidirectional feature fusion, and the ACmix module to address small target detection and category imbalance.

Result: YOLO-RS improves recall and mAP by 2-3% and boosts F1-score, with only a 5.2 GFLOPs increase in computational complexity.

Conclusion: YOLO-RS is effective and efficient for small target detection in remote sensing, demonstrating strong application potential.

Abstract: With the rapid development of remote sensing technology, crop classification
and health detection based on deep learning have gradually become a research
hotspot. However, the existing target detection methods show poor performance
when dealing with small targets in remote sensing images, especially in the
case of complex background and image mixing, which is difficult to meet the
practical application requirementsite. To address this problem, a novel target
detection model YOLO-RS is proposed in this paper. The model is based on the
latest Yolov11 which significantly enhances the detection of small targets by
introducing the Context Anchor Attention (CAA) mechanism and an efficient
multi-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional
feature fusion strategy in the feature fusion process, which effectively
enhances the model's performance in the detection of small targets. Small
target detection. Meanwhile, the ACmix module at the end of the model backbone
network solves the category imbalance problem by adaptively adjusting the
contrast and sample mixing, thus enhancing the detection accuracy in complex
scenes. In the experiments on the PDT remote sensing crop health detection
dataset and the CWC crop classification dataset, YOLO-RS improves both the
recall and the mean average precision (mAP) by about 2-3\% or so compared with
the existing state-of-the-art methods, while the F1-score is also significantly
improved. Moreover, the computational complexity of the model only increases by
about 5.2 GFLOPs, indicating its significant advantages in both performance and
efficiency. The experimental results validate the effectiveness and application
potential of YOLO-RS in the task of detecting small targets in remote sensing
images.

</details>


### [279] [RoPETR: Improving Temporal Camera-Only 3D Detection by Integrating Enhanced Rotary Position Embedding](https://arxiv.org/pdf/2504.12643)
*Hang Ji, Tao Ni, Xufeng Huang, Zhan Shi, Tao Luo, Xin Zhan, Junbo Chen*

Main category: cs.CV

TL;DR: The paper improves StreamPETR's velocity estimation using a custom positional embedding, achieving a state-of-the-art NDS of 70.86%.


<details>
  <summary>Details</summary>
Motivation: Velocity estimation was identified as a bottleneck in StreamPETR's performance on NuScenes.

Method: A customized positional embedding strategy for better temporal modeling.

Result: Achieved 70.86% NDS on NuScenes test set, setting a new benchmark.

Conclusion: The proposed enhancement effectively addresses velocity estimation, improving overall performance.

Abstract: This technical report introduces a targeted improvement to the StreamPETR
framework, specifically aimed at enhancing velocity estimation, a critical
factor influencing the overall NuScenes Detection Score. While StreamPETR
exhibits strong 3D bounding box detection performance as reflected by its high
mean Average Precision our analysis identified velocity estimation as a
substantial bottleneck when evaluated on the NuScenes dataset. To overcome this
limitation, we propose a customized positional embedding strategy tailored to
enhance temporal modeling capabilities. Experimental evaluations conducted on
the NuScenes test set demonstrate that our improved approach achieves a
state-of-the-art NDS of 70.86% using the ViT-L backbone, setting a new
benchmark for camera-only 3D object detection.

</details>


### [280] [Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning](https://arxiv.org/pdf/2504.16656)
*Peiyu Wang, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork R1V2 is a multimodal reasoning model with hybrid reinforcement learning, addressing generalization and efficiency challenges, achieving top benchmark results.


<details>
  <summary>Details</summary>
Motivation: To advance multimodal reasoning by balancing sophisticated capabilities with broad generalization, and improving training efficiency.

Method: Uses hybrid reinforcement learning (MPO and GRPO) with Selective Sample Buffer (SSB) to mitigate vanishing advantages and visual hallucinations.

Result: Achieves leading benchmarks: 62.6 (OlympiadBench), 78.9 (AIME2024), 63.6 (LiveCodeBench), 73.6 (MMMU).

Conclusion: R1V2 outperforms open-source models and narrows the gap with proprietary systems, promoting openness via public release.

Abstract: We present Skywork R1V2, a next-generation multimodal reasoning model and a
major leap forward from its predecessor, Skywork R1V. At its core, R1V2
introduces a hybrid reinforcement learning paradigm that jointly leverages the
Mixed Preference Optimization (MPO) and the Group Relative Policy Optimization
(GRPO), which harmonizes reward-model guidance with rule-based strategies,
thereby addressing the long-standing challenge of balancing sophisticated
reasoning capabilities with broad generalization. To further enhance training
efficiency, we propose the Selective Sample Buffer (SSB) mechanism, which
effectively addresses the vanishing advantages dilemma inherent in GRPO by
prioritizing high-value samples throughout the optimization process. Notably,
we observe that excessive reinforcement signals can induce visual
hallucinations--a phenomenon we systematically monitor and mitigate through
calibrated reward thresholds throughout the training process. Empirical results
affirm the exceptional capability of R1V2, with benchmark-leading performances
such as 62.6 on OlympiadBench, 78.9 on AIME2024, 63.6 on LiveCodeBench, and
73.6 on MMMU. These results underscore R1V2's superiority over existing
open-source models and demonstrate significant progress in closing the
performance gap with premier proprietary systems, including Gemini 2.5 and
OpenAI-o4-mini. The Skywork R1V2 model weights have been publicly released to
promote openness and reproducibility
https://huggingface.co/Skywork/Skywork-R1V2-38B.

</details>


### [281] [Visual Text Processing: A Comprehensive Review and Unified Evaluation](https://arxiv.org/pdf/2504.21682)
*Yan Shu, Weichao Zeng, Fangmin Zhao, Zeyu Chen, Zhenhang Li, Xiaomeng Yang, Yu Zhou, Paolo Rota, Xiang Bai, Lianwen Jin, Xu-Cheng Yin, Nicu Sebe*

Main category: cs.CV

TL;DR: A survey on visual text processing advancements, focusing on textual features and their integration into frameworks, introducing VTPBench and VTPScore for evaluation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in visual text processing due to unique text properties and improve model robustness by leveraging distinct textual characteristics.

Method: Comprehensive analysis of recent advancements, introduction of VTPBench benchmark, and VTPScore metric using MLLMs for evaluation.

Result: Empirical study shows current techniques have room for improvement; VTPBench and VTPScore provide fair evaluation.

Conclusion: The survey aims to be a foundational resource for future innovation in visual text processing.

Abstract: Visual text is a crucial component in both document and scene images,
conveying rich semantic information and attracting significant attention in the
computer vision community. Beyond traditional tasks such as text detection and
recognition, visual text processing has witnessed rapid advancements driven by
the emergence of foundation models, including text image reconstruction and
text image manipulation. Despite significant progress, challenges remain due to
the unique properties that differentiate text from general objects. Effectively
capturing and leveraging these distinct textual characteristics is essential
for developing robust visual text processing models. In this survey, we present
a comprehensive, multi-perspective analysis of recent advancements in visual
text processing, focusing on two key questions: (1) What textual features are
most suitable for different visual text processing tasks? (2) How can these
distinctive text features be effectively incorporated into processing
frameworks? Furthermore, we introduce VTPBench, a new benchmark that
encompasses a broad range of visual text processing datasets. Leveraging the
advanced visual quality assessment capabilities of multimodal large language
models (MLLMs), we propose VTPScore, a novel evaluation metric designed to
ensure fair and reliable evaluation. Our empirical study with more than 20
specific models reveals substantial room for improvement in the current
techniques. Our aim is to establish this work as a fundamental resource that
fosters future exploration and innovation in the dynamic field of visual text
processing. The relevant repository is available at
https://github.com/shuyansy/Visual-Text-Processing-survey.

</details>


### [282] [Open Your Eyes: Vision Enhances Message Passing Neural Networks in Link Prediction](https://arxiv.org/pdf/2505.08266)
*Yanbin Wei, Xuehao Wang, Zhan Zhuang, Yang Chen, Shuhao Chen, Yulong Zhang, Yu Zhang, James Kwok*

Main category: cs.CV

TL;DR: GVN and E-GVN enhance MPNNs with visual perception for link prediction, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Visual perception is overlooked in MPNNs for link prediction, despite its intuitive potential.

Method: Proposes Graph Vision Network (GVN) and its efficient variant (E-GVN) to integrate vision structural awareness into MPNNs.

Result: GVN improves performance across seven datasets, including large-scale graphs, and achieves new SOTA results.

Conclusion: GVN introduces a promising new direction for link prediction by combining MPNNs with visual perception.

Abstract: Message-passing graph neural networks (MPNNs) and structural features (SFs)
are cornerstones for the link prediction task. However, as a common and
intuitive mode of understanding, the potential of visual perception has been
overlooked in the MPNN community. For the first time, we equip MPNNs with
vision structural awareness by proposing an effective framework called Graph
Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive
empirical results demonstrate that with the proposed frameworks, GVN
consistently benefits from the vision enhancement across seven link prediction
datasets, including challenging large-scale graphs. Such improvements are
compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new
SOTA results, thereby underscoring a promising novel direction for link
prediction.

</details>


### [283] [RB-SCD: A New Benchmark for Semantic Change Detection of Roads and Bridges in Traffic Scenes](https://arxiv.org/pdf/2505.13212)
*Qingling Shu, Sibao Chen, Zhihui You, Wei Lu, Jin Tang, Bin Luo*

Main category: cs.CV

TL;DR: The paper introduces the RB-SCD dataset for road and bridge semantic change detection and proposes the MFDCD framework, which integrates multimodal features in the frequency domain for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained semantic change detection in complex traffic scenes due to insufficient annotated datasets.

Method: The authors propose the MFDCD framework, combining Dynamic Frequency Coupler (DFC) and Textual Frequency Filter (TFF) to fuse visual and textual features in the frequency domain.

Result: MFDCD achieves strong performance on the RB-SCD dataset and three public benchmarks.

Conclusion: The RB-SCD dataset and MFDCD framework advance research in road and bridge change detection under complex conditions.

Abstract: With the rapid modernization of urban transportation, accurately detecting
changes such as road and bridge construction, renovation, and demolition is
crucial for urban planning and traffic management. However, existing methods
often struggle to extract fine-grained semantic changes in complex traffic
scenes, largely due to the lack of high-quality annotated change detection (CD)
datasets. To address this, we introduce the Road and Bridge Semantic Change
Detection (RB-SCD) dataset, a comprehensive benchmark consisting of 260 pairs
of high-resolution remote sensing images. RB-SCD spans diverse geographic areas
and includes a wide variety of road and bridge types across over ten cities in
multiple countries. It covers 11 distinct categories of semantic changes,
enabling detailed structural and functional analysis. Based on this challenging
dataset, we propose a novel framework called the Multimodal Frequency-Driven
Change Detector (MFDCD). For the first time, MFDCD integrates multimodal
feature characteristics in the frequency domain. It comprises two key
components: the Dynamic Frequency Coupler (DFC) and the Textual Frequency
Filter (TFF). DFC couples hierarchical visual features with wavelet-based
frequency components, enhancing the perception of fine-grained and
cross-temporal structural changes. TFF transforms textual features extracted by
the CLIP model into the frequency domain via Fourier transform and applies
graph-based filtering to extract salient frequency responses. These are then
fused with visual features to enable effective multimodal representation
learning. Extensive experiments show that MFDCD achieves strong performance on
RB-SCD and three public benchmarks. The RB-SCD dataset, with its rich and
diverse annotations, serves as a valuable resource for advancing research in
road and bridge change detection under complex traffic conditions.

</details>


### [284] [diffDemorph: Extending Reference-Free Demorphing to Unseen Faces](https://arxiv.org/pdf/2505.14527)
*Nitish Shukla, Arun Ross*

Main category: cs.CV

TL;DR: A novel diffusion-based method, diffDeMorph, reverses face morphing without reference images, outperforming existing techniques by ≥59.46% and generalizing across morphing methods and face styles.


<details>
  <summary>Details</summary>
Motivation: Existing RF demorphing methods are limited by assumptions about morphing techniques and face styles, restricting their practicality.

Method: Introduces diffDeMorph, a diffusion-based approach that disentangles component images from a morph with high fidelity, trained on synthetic data and tested on real morphs.

Result: Achieves state-of-the-art performance, generalizing across datasets and face matchers, with a ≥59.46% improvement.

Conclusion: diffDeMorph enhances practicality and effectiveness in RF demorphing, setting a new benchmark for the field.

Abstract: A face morph is created by combining two face images corresponding to two
identities to produce a composite that successfully matches both the
constituent identities. Reference-free (RF) demorphing reverses this process
using only the morph image, without the need for additional reference images.
Previous RF demorphing methods are overly constrained, as they rely on
assumptions about the distributions of training and testing morphs such as the
morphing technique used (e.g., landmark-based) and face image style (e.g.,
passport photos). In this paper, we introduce a novel diffusion-based approach,
referred to as diffDeMorph, that effectively disentangles component images from
a composite morph image with high visual fidelity. Our method is the first to
generalize across morph techniques and face styles, beating the current state
of the art by $\geq 59.46\%$ under a common training protocol across all
datasets tested. We train our method on morphs created using synthetically
generated face images and test on real morphs, thereby enhancing the
practicality of the technique. Experiments on six datasets and two face
matchers establish the utility and efficacy of our method.

</details>


### [285] [Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training](https://arxiv.org/pdf/2505.22342)
*Shriram M S, Xinyue Hao, Shihao Hou, Yang Lu, Laura Sevilla-Lara, Anurag Arnab, Shreyank N Gowda*

Main category: cs.CV

TL;DR: Progressive Data Dropout reduces training epochs by 87.6% without sacrificing accuracy, even improving it by up to 4.82%.


<details>
  <summary>Details</summary>
Motivation: The high cost of training on large datasets and the inefficiency of uniform sampling in standard training methods.

Method: Leverages hard-data-mining and dropout insights to create Progressive Data Dropout, requiring no changes to model architecture or optimizer.

Result: Reduces effective epochs to 12.4% of baseline while improving accuracy by up to 4.82%.

Conclusion: A simple, widely applicable method that could become a new training standard.

Abstract: The success of the machine learning field has reliably depended on training
on large datasets. While effective, this trend comes at an extraordinary cost.
This is due to two deeply intertwined factors: the size of models and the size
of datasets. While promising research efforts focus on reducing the size of
models, the other half of the equation remains fairly mysterious. Indeed, it is
surprising that the standard approach to training remains to iterate over and
over, uniformly sampling the training dataset. In this paper we explore a
series of alternative training paradigms that leverage insights from
hard-data-mining and dropout, simple enough to implement and use that can
become the new training standard. The proposed Progressive Data Dropout reduces
the number of effective epochs to as little as 12.4% of the baseline. This
savings actually do not come at any cost for accuracy. Surprisingly, the
proposed method improves accuracy by up to 4.82%. Our approach requires no
changes to model architecture or optimizer, and can be applied across standard
training pipelines, thus posing an excellent opportunity for wide adoption.
Code can be found here: https://github.com/bazyagami/LearningWithRevision

</details>


### [286] [Universal Domain Adaptation for Semantic Segmentation](https://arxiv.org/pdf/2505.22458)
*Seun-An Choe, Keon-Hee Park, Jinwoo Choi, Gyeong-Moon Park*

Main category: cs.CV

TL;DR: Proposes UniMAP, a framework for Universal Domain Adaptation in Semantic Segmentation (UniDA-SS), addressing category setting unknowns and private classes via domain-specific prototypes and target-based image matching.


<details>
  <summary>Details</summary>
Motivation: Traditional UDA-SS methods assume known category settings, which is unrealistic. Performance degrades with private classes, prompting the need for a robust solution like UniMAP.

Method: UniMAP uses Domain-Specific Prototype-based Distinction (DSPD) for finer feature separation and Target-based Image Matching (TIM) to pair source and target images for common-class learning.

Result: UniMAP outperforms baselines on a new UniDA-SS benchmark, demonstrating robust adaptation without prior category knowledge.

Conclusion: UniMAP effectively addresses the limitations of traditional UDA-SS by leveraging domain-specific prototypes and image matching, offering a practical solution for real-world scenarios.

Abstract: Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to
transfer knowledge from labeled source data to unlabeled target data. However,
traditional UDA-SS methods assume that category settings between source and
target domains are known, which is unrealistic in real-world scenarios. This
leads to performance degradation if private classes exist. To address this
limitation, we propose Universal Domain Adaptation for Semantic Segmentation
(UniDA-SS), achieving robust adaptation even without prior knowledge of
category settings. We define the problem in the UniDA-SS scenario as low
confidence scores of common classes in the target domain, which leads to
confusion with private classes. To solve this problem, we propose UniMAP:
UniDA-SS with Image Matching and Prototype-based Distinction, a novel framework
composed of two key components. First, Domain-Specific Prototype-based
Distinction (DSPD) divides each class into two domain-specific prototypes,
enabling finer separation of domain-specific features and enhancing the
identification of common classes across domains. Second, Target-based Image
Matching (TIM) selects a source image containing the most common-class pixels
based on the target pseudo-label and pairs it in a batch to promote effective
learning of common classes. We also introduce a new UniDA-SS benchmark and
demonstrate through various experiments that UniMAP significantly outperforms
baselines. The code is available at https://github.com/KU-VGI/UniMAP.

</details>


### [287] [Federated Foundation Model for GI Endoscopy Images](https://arxiv.org/pdf/2505.24108)
*Alina Devkota, Annahita Amireskandari, Joel Palko, Shyam Thakkar, Donald Adjeroh, Xiajun Jiang, Binod Bhattarai, Prashnna K. Gyawali*

Main category: cs.CV

TL;DR: A federated learning (FL) framework is proposed to train foundation models for gastroendoscopy imaging, addressing data privacy and scarcity issues, and improving performance on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Gastrointestinal endoscopy is crucial for early disease detection, but deep learning models require expensive labeled data. Foundation models can help, but medical data privacy restrictions hinder their training.

Method: A FL framework is introduced to train foundation models locally without sharing data, using established FL algorithms in homogeneous and heterogeneous settings.

Result: The trained foundation model shows improved performance on classification, detection, and segmentation tasks in a privacy-preserving setting.

Conclusion: The FL framework effectively trains foundation models for gastroendoscopy, overcoming data privacy and scarcity challenges while enhancing diagnostic performance.

Abstract: Gastrointestinal (GI) endoscopy is essential in identifying GI tract
abnormalities in order to detect diseases in their early stages and improve
patient outcomes. Although deep learning has shown success in supporting GI
diagnostics and decision-making, these models require curated datasets with
labels that are expensive to acquire. Foundation models offer a promising
solution by learning general-purpose representations, which can be finetuned
for specific tasks, overcoming data scarcity. Developing foundation models for
medical imaging holds significant potential, but the sensitive and protected
nature of medical data presents unique challenges. Foundation model training
typically requires extensive datasets, and while hospitals generate large
volumes of data, privacy restrictions prevent direct data sharing, making
foundation model training infeasible in most scenarios. In this work, we
propose a FL framework for training foundation models for gastroendoscopy
imaging, enabling data to remain within local hospital environments while
contributing to a shared model. We explore several established FL algorithms,
assessing their suitability for training foundation models without relying on
task-specific labels, conducting experiments in both homogeneous and
heterogeneous settings. We evaluate the trained foundation model on three
critical downstream tasks--classification, detection, and segmentation--and
demonstrate that it achieves improved performance across all tasks,
highlighting the effectiveness of our approach in a federated,
privacy-preserving setting.

</details>


### [288] [GenSpace: Benchmarking Spatially-Aware Image Generation](https://arxiv.org/pdf/2505.24870)
*Zehan Wang, Jiayang Xu, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, Zhou Zhao*

Main category: cs.CV

TL;DR: GenSpace is a benchmark and evaluation pipeline to assess AI image generators' 3D spatial awareness, revealing limitations in object placement, relationships, and measurements.


<details>
  <summary>Details</summary>
Motivation: To evaluate if AI image generators can plan scenes with human-like 3D spatial awareness, as current evaluations often miss detailed spatial errors.

Method: Proposes a specialized evaluation pipeline using multiple visual foundation models to reconstruct 3D scene geometry and measure spatial faithfulness.

Result: AI models produce visually appealing images but struggle with specific 3D details like object placement and relationships.

Conclusion: Identifies three core limitations in AI spatial perception and suggests directions for improving spatial intelligence in image generation.

Abstract: Humans can intuitively compose and arrange scenes in the 3D space for
photography. However, can advanced AI image generators plan scenes with similar
3D spatial awareness when creating images from text or image prompts? We
present GenSpace, a novel benchmark and evaluation pipeline to comprehensively
assess the spatial awareness of current image generation models. Furthermore,
standard evaluations using general Vision-Language Models (VLMs) frequently
fail to capture the detailed spatial errors. To handle this challenge, we
propose a specialized evaluation pipeline and metric, which reconstructs 3D
scene geometry using multiple visual foundation models and provides a more
accurate and human-aligned metric of spatial faithfulness. Our findings show
that while AI models create visually appealing images and can follow general
instructions, they struggle with specific 3D details like object placement,
relationships, and measurements. We summarize three core limitations in the
spatial perception of current state-of-the-art image generation models: 1)
Object Perspective Understanding, 2) Egocentric-Allocentric Transformation and
3) Metric Measurement Adherence, highlighting possible directions for improving
spatial intelligence in image generation.

</details>


### [289] [Balancing Beyond Discrete Categories: Continuous Demographic Labels for Fair Face Recognition](https://arxiv.org/pdf/2506.01532)
*Pedro C. Neto, Naser Damer, Jaime S. Cardoso, Ana F. Sequeira*

Main category: cs.CV

TL;DR: Proposes treating ethnicity labels as continuous variables to better address bias in face recognition models, showing improved performance over discrete labeling.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to mitigate data bias in face recognition are limited and lack insight into the real nature of the problem.

Method: Revises ethnicity labels as continuous variables, validates experimentally and theoretically, and trains models on datasets balanced in continuous space.

Result: Models trained on continuously balanced datasets outperform those trained on discretely balanced ones, with extensive validation (65 models, 20 dataset subsets).

Conclusion: Continuous ethnicity labeling provides a more effective way to balance datasets and reduce bias in face recognition models.

Abstract: Bias has been a constant in face recognition models. Over the years,
researchers have looked at it from both the model and the data point of view.
However, their approach to mitigation of data bias was limited and lacked
insight on the real nature of the problem. Here, in this document, we propose
to revise our use of ethnicity labels as a continuous variable instead of a
discrete value per identity. We validate our formulation both experimentally
and theoretically, showcasing that not all identities from one ethnicity
contribute equally to the balance of the dataset; thus, having the same number
of identities per ethnicity does not represent a balanced dataset. We further
show that models trained on datasets balanced in the continuous space
consistently outperform models trained on data balanced in the discrete space.
We trained more than 65 different models, and created more than 20 subsets of
the original datasets.

</details>


### [290] [Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences](https://arxiv.org/pdf/2506.02698)
*Yunhong Lu, Qichao Wang, Hengyuan Cao, Xiaoyin Xu, Min Zhang*

Main category: cs.CV

TL;DR: SmPO-Diffusion improves Direct Preference Optimization (DPO) by modeling preference distributions and introducing a smoothed preference distribution, achieving better alignment with human preferences and state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect the granularity of individual preferences, leading to misalignment and excessive optimization.

Method: Introduces a smoothed preference distribution, reward modeling, preference likelihood averaging, and inversion techniques for diffusion model alignment.

Result: Outperforms baselines in preference evaluation with lower training costs.

Conclusion: SmPO-Diffusion effectively addresses preference granularity and optimization issues, setting a new benchmark for performance.

Abstract: Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation
models with human preferences using pairwise preference data. Although
substantial resources are expended in collecting and labeling datasets, a
critical aspect is often neglected: \textit{preferences vary across individuals
and should be represented with more granularity.} To address this, we propose
SmPO-Diffusion, a novel method for modeling preference distributions to improve
the DPO objective, along with a numerical upper bound estimation for the
diffusion optimization objective. First, we introduce a smoothed preference
distribution to replace the original binary distribution. We employ a reward
model to simulate human preferences and apply preference likelihood averaging
to improve the DPO loss, such that the loss function approaches zero when
preferences are similar. Furthermore, we utilize an inversion technique to
simulate the trajectory preference distribution of the diffusion model,
enabling more accurate alignment with the optimization objective. Our approach
effectively mitigates issues of excessive optimization and objective
misalignment present in existing methods through straightforward modifications.
Our SmPO-Diffusion achieves state-of-the-art performance in preference
evaluation, outperforming baselines across metrics with lower training costs.
The project page is https://jaydenlyh.github.io/SmPO-project-page/.

</details>


### [291] [SemiOccam: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels](https://arxiv.org/pdf/2506.03582)
*Rui Yann, Xianglei Xing*

Main category: cs.CV

TL;DR: SemiOccam is an efficient semi-supervised image recognition network that outperforms existing methods with minimal labeled data and reduced training time, while addressing data leakage in STL-10.


<details>
  <summary>Details</summary>
Motivation: Existing methods are complex, time-consuming, and struggle with limited labeled data. SemiOccam aims to improve efficiency and generalization.

Method: Uses a hierarchical mixture density classification mechanism, optimizing mutual information to retain discriminative features and compress redundancy.

Result: Achieves state-of-the-art performance with minimal labeled samples and minute-level training time. Identifies and fixes data leakage in STL-10.

Conclusion: SemiOccam offers a simple, efficient solution for semi-supervised learning, with the release of CleanSTL-10 for fair future research.

Abstract: We present SemiOccam, an image recognition network that leverages
semi-supervised learning in a highly efficient manner. Existing works often
rely on complex training techniques and architectures, requiring hundreds of
GPU hours for training, while their generalization ability when dealing with
extremely limited labeled data remains to be improved. To address these
limitations, we construct a hierarchical mixture density classification
decision mechanism by optimizing mutual information between feature
representations and target classes, compressing redundant information while
retaining crucial discriminative components. Experimental results demonstrate
that our method achieves state-of-the-art performance on various datasets when
using negligible labeled samples, and its simple architecture keeps training
time to minute-level. Notably, this paper reveals a long-overlooked data
leakage issue in the STL-10 dataset for semi-supervised learning tasks and
removes duplicates to ensure the reliability of experimental results. We also
release the deduplicated CleanSTL-10 dataset to facilitate fair and reliable
research in future semi-supervised learning. Code available at
https://github.com/Shu1L0n9/SemiOccam.

</details>


### [292] [Assessing Intersectional Bias in Representations of Pre-Trained Image Recognition Models](https://arxiv.org/pdf/2506.03664)
*Valerie Krug, Sebastian Stober*

Main category: cs.CV

TL;DR: The paper examines biases in ImageNet classifiers for facial images, focusing on age, race, and gender, using linear probes and activation maps.


<details>
  <summary>Details</summary>
Motivation: To investigate how pre-trained deep learning models perpetuate biases in facial representations, especially concerning intersections of age, race, and gender.

Method: Linear classifier probes and visualization of activations as topographic maps are used to assess biases.

Result: ImageNet classifiers strongly differentiate ages and, to a lesser extent, associate certain ethnicities and distinguish genders in middle-aged groups.

Conclusion: The study highlights the presence of biases in pre-trained models, emphasizing the need for addressing these issues in AI fairness.

Abstract: Deep Learning models have achieved remarkable success. Training them is often
accelerated by building on top of pre-trained models which poses the risk of
perpetuating encoded biases. Here, we investigate biases in the representations
of commonly used ImageNet classifiers for facial images while considering
intersections of sensitive variables age, race and gender. To assess the
biases, we use linear classifier probes and visualize activations as
topographic maps. We find that representations in ImageNet classifiers
particularly allow differentiation between ages. Less strongly pronounced, the
models appear to associate certain ethnicities and distinguish genders in
middle-aged groups.

</details>


### [293] [FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion](https://arxiv.org/pdf/2506.04648)
*Akide Liu, Zeyu Zhang, Zhexin Li, Xuehai Bai, Yizeng Han, Jiasheng Tang, Yuanjie Xing, Jichao Wu, Mingyang Yang, Weihua Chen, Jiahao He, Yuanyu He, Fan Wang, Gholamreza Haffari, Bohan Zhuang*

Main category: cs.CV

TL;DR: FPSAttention combines FP8 quantization and sparsity for faster video generation without quality loss, achieving 4.96x speedup.


<details>
  <summary>Details</summary>
Motivation: Slow inference speeds and high computational demands of diffusion models hinder practical deployment.

Method: Introduces FPSAttention, a training-aware co-design of FP8 quantization and sparsity, with 3D tile-wise granularity, denoising step-aware strategy, and hardware-friendly kernels.

Result: Achieves 7.09x kernel speedup for attention and 4.96x end-to-end speedup for video generation at 720p without quality loss.

Conclusion: FPSAttention effectively accelerates video generation while maintaining quality, addressing practical deployment challenges.

Abstract: Diffusion generative models have become the standard for producing
high-quality, coherent video content, yet their slow inference speeds and high
computational demands hinder practical deployment. Although both quantization
and sparsity can independently accelerate inference while maintaining
generation quality, naively combining these techniques in existing
training-free approaches leads to significant performance degradation due to
the lack of joint optimization. We introduce FPSAttention, a novel
training-aware co-design of FP8 quantization and sparsity for video generation,
with a focus on the 3D bi-directional attention mechanism. Our approach
features three key innovations: 1) A unified 3D tile-wise granularity that
simultaneously supports both quantization and sparsity; 2) A denoising
step-aware strategy that adapts to the noise schedule, addressing the strong
correlation between quantization/sparsity errors and denoising steps; 3) A
native, hardware-friendly kernel that leverages FlashAttention and is
implemented with optimized Hopper architecture features for highly efficient
execution. Trained on Wan2.1's 1.3B and 14B models and evaluated on the VBench
benchmark, FPSAttention achieves a 7.09x kernel speedup for attention
operations and a 4.96x end-to-end speedup for video generation compared to the
BF16 baseline at 720p resolution-without sacrificing generation quality.

</details>


### [294] [Feature-Based Lie Group Transformer for Real-World Applications](https://arxiv.org/pdf/2506.04668)
*Takayuki Komatsu, Yoshiyuki Ohmura, Kayato Nishitsunoi, Yasuo Kuniyoshi*

Main category: cs.CV

TL;DR: The paper proposes a method to apply group decomposition theory in representation learning for realistic scenarios by combining feature extraction and object segmentation, validated on real-world data.


<details>
  <summary>Details</summary>
Motivation: To address limitations in conventional representation learning, which assumes disentangled independent feature axes but fails to account for conditional independence, and to extend the applicability of group decomposition theory to real-world scenarios.

Method: Combines feature extraction and object segmentation to replace pixel translation with feature translation, grouping features under the same transformation.

Result: Validated on a practical dataset with real-world objects and backgrounds, showing promise for realistic applications.

Conclusion: The method advances representation learning and may improve understanding of human object recognition development.

Abstract: The main goal of representation learning is to acquire meaningful
representations from real-world sensory inputs without supervision.
Representation learning explains some aspects of human development. Various
neural network (NN) models have been proposed that acquire empirically good
representations. However, the formulation of a good representation has not been
established. We recently proposed a method for categorizing changes between a
pair of sensory inputs. A unique feature of this approach is that
transformations between two sensory inputs are learned to satisfy algebraic
structural constraints. Conventional representation learning often assumes that
disentangled independent feature axes is a good representation; however, we
found that such a representation cannot account for conditional independence.
To overcome this problem, we proposed a new method using group decomposition in
Galois algebra theory. Although this method is promising for defining a more
general representation, it assumes pixel-to-pixel translation without feature
extraction, and can only process low-resolution images with no background,
which prevents real-world application. In this study, we provide a simple
method to apply our group decomposition theory to a more realistic scenario by
combining feature extraction and object segmentation. We replace pixel
translation with feature translation and formulate object segmentation as
grouping features under the same transformation. We validated the proposed
method on a practical dataset containing both real-world object and background.
We believe that our model will lead to a better understanding of human
development of object recognition in the real world.

</details>


### [295] [MARS: Radio Map Super-resolution and Reconstruction Method under Sparse Channel Measurements](https://arxiv.org/pdf/2506.04682)
*Chuyun Deng, Na Liu, Wei Xie, Lianming Xu, Li Wang*

Main category: cs.CV

TL;DR: MARS, a multi-scale aware radiomap super-resolution method, combines CNNs and Transformers to improve radio map reconstruction accuracy from sparse measurements, outperforming baselines in MSE and SSIM.


<details>
  <summary>Details</summary>
Motivation: Accurate radio maps are crucial for smart cities and IoT, but existing methods lack environmental awareness or depend on detailed scene data, limiting generalization.

Method: MARS integrates CNNs and Transformers with multi-scale feature fusion and residual connections to enhance global and local feature extraction.

Result: MARS outperforms baseline models in MSE and SSIM across various scenes and antenna locations, with low computational cost.

Conclusion: MARS demonstrates strong practical potential for accurate radio map reconstruction, addressing limitations of traditional and deep learning methods.

Abstract: Radio maps reflect the spatial distribution of signal strength and are
essential for applications like smart cities, IoT, and wireless network
planning. However, reconstructing accurate radio maps from sparse measurements
remains challenging. Traditional interpolation and inpainting methods lack
environmental awareness, while many deep learning approaches depend on detailed
scene data, limiting generalization. To address this, we propose MARS, a
Multi-scale Aware Radiomap Super-resolution method that combines CNNs and
Transformers with multi-scale feature fusion and residual connections. MARS
focuses on both global and local feature extraction, enhancing feature
representation across different receptive fields and improving reconstruction
accuracy. Experiments across different scenes and antenna locations show that
MARS outperforms baseline models in both MSE and SSIM, while maintaining low
computational cost, demonstrating strong practical potential.

</details>


### [296] [Bridging Annotation Gaps: Transferring Labels to Align Object Detection Datasets](https://arxiv.org/pdf/2506.04737)
*Mikhail Kennerley, Angelica Aviles-Rivero, Carola-Bibiane Schönlieb, Robby T. Tan*

Main category: cs.CV

TL;DR: LAT is a label transfer framework that aligns annotations from diverse datasets into a target label space, addressing class and spatial inconsistencies without manual relabeling.


<details>
  <summary>Details</summary>
Motivation: Inconsistencies in class semantics and bounding box annotations across datasets hinder generalization in object detection. Existing methods either assume shared taxonomies or require manual effort.

Method: LAT uses dataset-specific detectors to generate pseudo-labels, combines them with ground-truth via a Privileged Proposal Generator (PPG), and refines features with a Semantic Feature Fusion (SFF) module.

Result: LAT improves target-domain detection performance by up to +4.8AP over baselines.

Conclusion: LAT effectively addresses class and spatial misalignments without shared label spaces or manual annotations, enhancing generalization.

Abstract: Combining multiple object detection datasets offers a path to improved
generalisation but is hindered by inconsistencies in class semantics and
bounding box annotations. Some methods to address this assume shared label
taxonomies and address only spatial inconsistencies; others require manual
relabelling, or produce a unified label space, which may be unsuitable when a
fixed target label space is required. We propose Label-Aligned Transfer (LAT),
a label transfer framework that systematically projects annotations from
diverse source datasets into the label space of a target dataset. LAT begins by
training dataset-specific detectors to generate pseudo-labels, which are then
combined with ground-truth annotations via a Privileged Proposal Generator
(PPG) that replaces the region proposal network in two-stage detectors. To
further refine region features, a Semantic Feature Fusion (SFF) module injects
class-aware context and features from overlapping proposals using a
confidence-weighted attention mechanism. This pipeline preserves
dataset-specific annotation granularity while enabling many-to-one label space
transfer across heterogeneous datasets, resulting in a semantically and
spatially aligned representation suitable for training a downstream detector.
LAT thus jointly addresses both class-level misalignments and bounding box
inconsistencies without relying on shared label spaces or manual annotations.
Across multiple benchmarks, LAT demonstrates consistent improvements in
target-domain detection performance, achieving gains of up to +4.8AP over
semi-supervised baselines.

</details>


### [297] [SeedEdit 3.0: Fast and High-Quality Generative Image Editing](https://arxiv.org/pdf/2506.05083)
*Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, Jianchao Yang*

Main category: cs.CV

TL;DR: SeedEdit 3.0 improves edit instruction following and image content preservation, with key enhancements in data curation, joint learning, and outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance real image editing by improving instruction adherence and content preservation, leveraging T2I models and scalable data curation.

Method: Developed an enhanced data curation pipeline with meta-info, introduced joint learning for diffusion and reward losses, and upgraded the T2I model.

Result: Achieved a 56.1% usability rate, outperforming previous versions (SeedEdit 1.6: 38.4%) and competitors (GPT4o: 37.1%, Gemini 2.0: 30.3%).

Conclusion: SeedEdit 3.0 offers a superior trade-off in editing performance, scalability, and usability.

Abstract: We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0,
which significantly improves over our previous SeedEdit versions in both
aspects of edit instruction following and image content (e.g., ID/IP)
preservation on real image inputs. Additional to model upgrading with T2I, in
this report, we present several key improvements. First, we develop an enhanced
data curation pipeline with a meta-info paradigm and meta-info embedding
strategy that help mix images from multiple data sources. This allows us to
scale editing data effectively, and meta information is helpfult to connect VLM
with diffusion model more closely. Second, we introduce a joint learning
pipeline for computing a diffusion loss and reward losses. Finally, we evaluate
SeedEdit 3.0 on our testing benchmarks, for real/synthetic image editing, where
it achieves a best trade-off between multiple aspects, yielding a high
usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and
Gemini 2.0 (30.3%).

</details>


### [298] [Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video Diffusion Transformers](https://arxiv.org/pdf/2506.05096)
*Haosong Liu, Yuge Cheng, Zihan Liu, Aiyue Chen, Yiwu Yao, Chen Chen, Jingwen Leng, Yu Feng, Minyi Guo*

Main category: cs.CV

TL;DR: ASTRAEA is an automatic framework for optimizing video diffusion transformers (vDiTs), achieving significant speedups (up to 13.2x on 8 GPUs) with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: High computational demands of vDiTs limit practical deployment; existing acceleration methods rely on heuristics, reducing applicability.

Method: ASTRAEA uses a lightweight token selection mechanism, GPU-parallel sparse attention, and an evolutionary algorithm to optimize token reduction.

Result: Achieves up to 2.4x speedup on a single GPU (13.2x on 8 GPUs) with <0.5% loss in video quality (VBench score).

Conclusion: ASTRAEA provides an efficient, scalable solution for vDiT acceleration without compromising generation quality.

Abstract: Video diffusion transformers (vDiTs) have made impressive progress in
text-to-video generation, but their high computational demands present major
challenges for practical deployment. While existing acceleration methods reduce
workload at various granularities, they often rely on heuristics, limiting
their applicability.
  We introduce ASTRAEA, an automatic framework that searches for near-optimal
configurations for vDiT-based video generation. At its core, ASTRAEA proposes a
lightweight token selection mechanism and a memory-efficient, GPU-parallel
sparse attention strategy, enabling linear reductions in execution time with
minimal impact on generation quality. To determine optimal token reduction for
different timesteps, we further design a search framework that leverages a
classic evolutionary algorithm to automatically determine the distribution of
the token budget effectively. Together, ASTRAEA achieves up to 2.4x inference
speedup on a single GPU with great scalability (up to 13.2x speedup on 8 GPUs)
while retaining better video quality compared to the state-of-the-art methods
(<0.5% loss on the VBench score compared to the baseline vDiT models).

</details>


### [299] [Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting](https://arxiv.org/pdf/2506.05280)
*Nan Wang, Yuantao Chen, Lixing Xiao, Weiqing Xiao, Bohan Li, Zhaoxi Chen, Chongjie Ye, Shaocong Xu, Saining Zhang, Ziyang Yan, Pierre Merriaux, Lei Lei, Tianfan Xue, Hao Zhao*

Main category: cs.CV

TL;DR: Proposes a multi-scale bilateral grid to improve geometric accuracy in neural rendering for autonomous driving scenes, outperforming existing methods like appearance codes and bilateral grids.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of photometric inconsistency in real-world images, which limits the quality of reconstructions in neural rendering techniques like NeRF and Gaussian Splatting.

Method: Introduces a novel multi-scale bilateral grid that unifies appearance codes and bilateral grids for pixel-wise color mapping.

Result: Significantly improves geometric accuracy in dynamic scenes, reducing floaters caused by photometric inconsistency, and performs well on Waymo, NuScenes, Argoverse, and PandaSet datasets.

Conclusion: The multi-scale bilateral grid is crucial for autonomous driving, enhancing obstacle avoidance and control by providing more accurate geometry.

Abstract: Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely
on photometric consistency to produce high-quality reconstructions. However, in
real-world scenarios, it is challenging to guarantee perfect photometric
consistency in acquired images. Appearance codes have been widely used to
address this issue, but their modeling capability is limited, as a single code
is applied to the entire image. Recently, the bilateral grid was introduced to
perform pixel-wise color mapping, but it is difficult to optimize and constrain
effectively. In this paper, we propose a novel multi-scale bilateral grid that
unifies appearance codes and bilateral grids. We demonstrate that this approach
significantly improves geometric accuracy in dynamic, decoupled autonomous
driving scene reconstruction, outperforming both appearance codes and bilateral
grids. This is crucial for autonomous driving, where accurate geometry is
important for obstacle avoidance and control. Our method shows strong results
across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further
demonstrate that the improvement in geometry is driven by the multi-scale
bilateral grid, which effectively reduces floaters caused by photometric
inconsistency.

</details>


### [300] [Does Your 3D Encoder Really Work? When Pretrain-SFT from 2D VLMs Meets 3D VLMs](https://arxiv.org/pdf/2506.05318)
*Haoyuan Li, Yanpeng Zhou, Yufei Gao, Tao Tang, Jianhua Han, Yujie Yuan, Dave Zhenyu Chen, Jiawang Bian, Hang Xu, Xiaodan Liang*

Main category: cs.CV

TL;DR: The paper analyzes 3D Vision-Language Models (VLMs), identifying performance gaps in 3D scene-centric approaches due to limited reliance on 3D encoders and ineffective pre-training. It introduces a new dataset to improve 3D understanding.


<details>
  <summary>Details</summary>
Motivation: To understand why 3D scene-centric VLMs underperform compared to 2D and 3D object-centric approaches and to enhance genuine 3D scene understanding.

Method: Categorizes 3D VLMs by encoder design, analyzes performance gaps, and introduces a novel 3D Relevance Discrimination QA dataset to disrupt shortcuts.

Result: 3D scene-centric VLMs over-rely on linguistic cues and underutilize 3D encoders. Data scaling benefits are limited.

Conclusion: Advanced evaluation and strategies are needed to improve 3D understanding in VLMs, supported by the new dataset.

Abstract: Remarkable progress in 2D Vision-Language Models (VLMs) has spurred interest
in extending them to 3D settings for tasks like 3D Question Answering, Dense
Captioning, and Visual Grounding. Unlike 2D VLMs that typically process images
through an image encoder, 3D scenes, with their intricate spatial structures,
allow for diverse model architectures. Based on their encoder design, this
paper categorizes recent 3D VLMs into 3D object-centric, 2D image-based, and 3D
scene-centric approaches. Despite the architectural similarity of 3D
scene-centric VLMs to their 2D counterparts, they have exhibited comparatively
lower performance compared with the latest 3D object-centric and 2D image-based
approaches. To understand this gap, we conduct an in-depth analysis, revealing
that 3D scene-centric VLMs show limited reliance on the 3D scene encoder, and
the pre-train stage appears less effective than in 2D VLMs. Furthermore, we
observe that data scaling benefits are less pronounced on larger datasets. Our
investigation suggests that while these models possess cross-modal alignment
capabilities, they tend to over-rely on linguistic cues and overfit to frequent
answer distributions, thereby diminishing the effective utilization of the 3D
encoder. To address these limitations and encourage genuine 3D scene
understanding, we introduce a novel 3D Relevance Discrimination QA dataset
designed to disrupt shortcut learning and improve 3D understanding. Our
findings highlight the need for advanced evaluation and improved strategies for
better 3D understanding in 3D VLMs.

</details>


### [301] [Defurnishing with X-Ray Vision: Joint Removal of Furniture from Panoramas and Mesh](https://arxiv.org/pdf/2506.05338)
*Alan Dolhasz, Chen Ma, Dave Gausebeck, Kevin Chen, Gregor Miller, Lucas Hayne, Gunnar Hovden, Azwad Sabik, Olaf Brandt, Mira Slavcheva*

Main category: cs.CV

TL;DR: A pipeline for generating defurnished indoor spaces using textured meshes and panoramic images, guided by a simplified defurnished mesh and ControlNet inpainting.


<details>
  <summary>Details</summary>
Motivation: To create high-quality defurnished replicas of indoor spaces, addressing limitations of existing methods like blurry outputs or hallucinations.

Method: Segments and removes furniture from meshes, extends planes, fills holes, and uses Canny edges from depth/normal images to guide ControlNet inpainting for panorama defurnishing.

Result: Produces higher-quality defurnished assets compared to neural radiance fields or RGB-D inpainting methods.

Conclusion: The approach effectively leverages geometric guidance and inpainting to achieve superior defurnishing results.

Abstract: We present a pipeline for generating defurnished replicas of indoor spaces
represented as textured meshes and corresponding multi-view panoramic images.
To achieve this, we first segment and remove furniture from the mesh
representation, extend planes, and fill holes, obtaining a simplified
defurnished mesh (SDM). This SDM acts as an ``X-ray'' of the scene's underlying
structure, guiding the defurnishing process. We extract Canny edges from depth
and normal images rendered from the SDM. We then use these as a guide to remove
the furniture from panorama images via ControlNet inpainting. This control
signal ensures the availability of global geometric information that may be
hidden from a particular panoramic view by the furniture being removed. The
inpainted panoramas are used to texture the mesh. We show that our approach
produces higher quality assets than methods that rely on neural radiance
fields, which tend to produce blurry low-resolution images, or RGB-D
inpainting, which is highly susceptible to hallucinations.

</details>


### [302] [FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic Scene Reconstruction](https://arxiv.org/pdf/2506.05348)
*Yifan Wang, Peishan Yang, Zhen Xu, Jiaming Sun, Zhanhua Zhang, Yong Chen, Hujun Bao, Sida Peng, Xiaowei Zhou*

Main category: cs.CV

TL;DR: The paper introduces FreeTimeGS, a 4D representation for dynamic 3D scenes, improving flexibility and motion handling over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex motions due to deformation field optimization challenges.

Method: Proposes FreeTimeGS, allowing Gaussian primitives to appear at arbitrary times and locations, with motion functions for temporal redundancy reduction.

Result: Outperforms recent methods in rendering quality on multiple datasets.

Conclusion: FreeTimeGS offers superior flexibility and performance for dynamic 3D scene reconstruction.

Abstract: This paper addresses the challenge of reconstructing dynamic 3D scenes with
complex motions. Some recent works define 3D Gaussian primitives in the
canonical space and use deformation fields to map canonical primitives to
observation spaces, achieving real-time dynamic view synthesis. However, these
methods often struggle to handle scenes with complex motions due to the
difficulty of optimizing deformation fields. To overcome this problem, we
propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives
to appear at arbitrary time and locations. In contrast to canonical Gaussian
primitives, our representation possesses the strong flexibility, thus improving
the ability to model dynamic 3D scenes. In addition, we endow each Gaussian
primitive with an motion function, allowing it to move to neighboring regions
over time, which reduces the temporal redundancy. Experiments results on
several datasets show that the rendering quality of our method outperforms
recent methods by a large margin. Project page:
https://zju3dv.github.io/freetimegs/ .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [303] [A Path to Loving](https://arxiv.org/pdf/2506.05352)
*John Beverley, Regina Hurley*

Main category: cs.AI

TL;DR: The paper provides a rigorous ontological framework for understanding love, combining passive sensations and active judgments, and applies it to AI and sciences.


<details>
  <summary>Details</summary>
Motivation: To address the philosophical and scientific complexity of love, balancing its involuntary and rational aspects, and enhance AI applications.

Method: Uses Basic Formal Ontology (BFO) to differentiate senses of love and defends a causal correlation model linking affective and cognitive components.

Result: A precise, scalable ontological account of love, laying groundwork for interdisciplinary applications in ontology engineering, AI, and sciences.

Conclusion: Love is best understood as a combination of sensations and judgments, with formal inquiry potential in multiple fields.

Abstract: This work lays the foundations for a rigorous ontological characterization of
love, addressing its philosophical complexity and scientific relevance, with
particular emphasis on psychology and sociology, as well as highlighting ways
in which such characterization enhances relevant AI based applications. The
position defended here is that love is best understood as a concatenation of
passive sensations (e.g., emotional arousal) and active evaluative judgments
(e.g., perceiving the beloved as valuable), in the interest of balancing the
involuntary aspects of love with its rational accountability. To provide a
structured foundation, the paper draws on Basic Formal Ontology (BFO) and other
applied ontological methods to differentiate various senses of love. This work
engages with objections to the understanding of love as concatenation,
particularly concerning the relationship between sensation and judgment. A
causal correlation model is defended, ensuring that the affective and cognitive
components are linked. By offering a precise and scalable ontological account,
this work lays the foundation for future interdisciplinary applications, making
love a subject of formal inquiry in ontology engineering, artificial
intelligence, and the sciences.

</details>


### [304] [Towards Data Systems That Are Business Semantic-Centric and AI Agents-Assisted](https://arxiv.org/pdf/2506.05520)
*Cecil Pang*

Main category: cs.AI

TL;DR: The paper proposes BSDS, a business-centric data system integrating AI agents, workflows, and team alignment to enhance adaptability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing data platforms prioritize tools over business alignment, causing inefficiencies. BSDS aims to bridge this gap by focusing on business semantics.

Method: BSDS combines modular architecture (curated data, AI agents, pipelines), optimized workflows, and team alignment with business needs.

Result: BSDS improves time-to-market, collaboration, and scalability, validated through real-world implementation.

Conclusion: BSDS transforms data systems into active business enablers, with future research potential in AI-driven optimization and autonomous systems.

Abstract: Contemporary businesses operate in dynamic environments requiring rapid
adaptation to achieve goals and maintain competitiveness. Existing data
platforms often fall short by emphasizing tools over alignment with business
needs, resulting in inefficiencies and delays. To address this gap, I propose
the Business Semantics Centric, AI Agents Assisted Data System (BSDS), a
holistic system that integrates architecture, workflows, and team organization
to ensure data systems are tailored to business priorities rather than dictated
by technical constraints. BSDS redefines data systems as dynamic enablers of
business success, transforming them from passive tools into active drivers of
organizational growth. BSDS has a modular architecture that comprises curated
data linked to business entities, a knowledge base for context-aware AI agents,
and efficient data pipelines. AI agents play a pivotal role in assisting with
data access and system management, reducing human effort, and improving
scalability. Complementing this architecture, BSDS incorporates workflows
optimized for both exploratory data analysis and production requirements,
balancing speed of delivery with quality assurance. A key innovation of BSDS is
its incorporation of the human factor. By aligning data team expertise with
business semantics, BSDS bridges the gap between technical capabilities and
business needs. Validated through real-world implementation, BSDS accelerates
time-to-market for data-driven initiatives, enhances cross-functional
collaboration, and provides a scalable blueprint for businesses of all sizes.
Future research can build on BSDS to explore optimization strategies using
complex systems and adaptive network theories, as well as developing autonomous
data systems leveraging AI agents.

</details>


### [305] [Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems](https://arxiv.org/pdf/2506.05370)
*Kristy Wedel*

Main category: cs.AI

TL;DR: The paper introduces Contextual Memory Intelligence (CMI) to address memory limitations in generative AI systems, enabling adaptive, explainable, and responsible decision-making.


<details>
  <summary>Details</summary>
Motivation: Current generative AI systems lack full context storage and reflection, leading to repeated errors and unclear decision-making.

Method: CMI formalizes structured context capture, inference, and regeneration, operationalized through the Insight Layer with human-in-the-loop reflection, drift detection, and rationale preservation.

Result: CMI enhances reasoning with data, history, and context, improving AI system effectiveness, auditability, and social responsibility.

Conclusion: CMI provides a framework for building reflective, resilient, and socially responsible AI systems, advancing human-AI collaboration and institutional resilience.

Abstract: A critical challenge remains unresolved as generative AI systems are quickly
implemented in various organizational settings. Despite significant advances in
memory components such as RAG, vector stores, and LLM agents, these systems
still have substantial memory limitations. Gen AI workflows rarely store or
reflect on the full context in which decisions are made. This leads to repeated
errors and a general lack of clarity. This paper introduces Contextual Memory
Intelligence (CMI) as a new foundational paradigm for building intelligent
systems. It repositions memory as an adaptive infrastructure necessary for
longitudinal coherence, explainability, and responsible decision-making rather
than passive data. Drawing on cognitive science, organizational theory,
human-computer interaction, and AI governance, CMI formalizes the structured
capture, inference, and regeneration of context as a fundamental system
capability. The Insight Layer is presented in this paper to operationalize this
vision. This modular architecture uses human-in-the-loop reflection, drift
detection, and rationale preservation to incorporate contextual memory into
systems. The paper argues that CMI allows systems to reason with data, history,
judgment, and changing context, thereby addressing a foundational blind spot in
current AI architectures and governance efforts. A framework for creating
intelligent systems that are effective, reflective, auditable, and socially
responsible is presented through CMI. This enhances human-AI collaboration,
generative AI design, and the resilience of the institutions.

</details>


### [306] [Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference](https://arxiv.org/pdf/2506.05422)
*Andrei T. Patrascu*

Main category: cs.AI

TL;DR: A novel framework replaces reward-based optimization with constructive logical inference, ensuring verifiable preconditions and guaranteed validity in decision-making.


<details>
  <summary>Details</summary>
Motivation: Traditional reinforcement learning relies on probabilistic trial-and-error, which can lead to unsafe or invalid transitions. This work aims to provide a safer, interpretable alternative using logical reasoning.

Method: Actions, transitions, and goals are represented as logical propositions. Decision-making involves building constructive proofs under intuitionistic logic, ensuring verifiable preconditions.

Result: The method achieves perfect safety, interpretable behavior, and efficient convergence with no invalid actions, outperforming Q-learning in structured environments.

Conclusion: This work introduces a new direction for reinforcement learning, grounded in constructive logic and proof theory, offering potential for safe planning and trustworthy AI.

Abstract: We introduce a novel learning and planning framework that replaces
traditional reward-based optimisation with constructive logical inference. In
our model, actions, transitions, and goals are represented as logical
propositions, and decision-making proceeds by building constructive proofs
under intuitionistic logic. This method ensures that state transitions and
policies are accepted only when supported by verifiable preconditions --
eschewing probabilistic trial-and-error in favour of guaranteed logical
validity. We implement a symbolic agent operating in a structured gridworld,
where reaching a goal requires satisfying a chain of intermediate subgoals
(e.g., collecting keys to open doors), each governed by logical constraints.
Unlike conventional reinforcement learning agents, which require extensive
exploration and suffer from unsafe or invalid transitions, our constructive
agent builds a provably correct plan through goal chaining, condition tracking,
and knowledge accumulation. Empirical comparison with Q-learning demonstrates
that our method achieves perfect safety, interpretable behaviour, and efficient
convergence with no invalid actions, highlighting its potential for safe
planning, symbolic cognition, and trustworthy AI. This work presents a new
direction for reinforcement learning grounded not in numeric optimisation, but
in constructive logic and proof theory.

</details>


### [307] [Avoiding Death through Fear Intrinsic Conditioning](https://arxiv.org/pdf/2506.05529)
*Rodney Sanchez, Ferat Sahin, Alexander Ororbia, Jamison Heard*

Main category: cs.AI

TL;DR: The paper introduces a biologically-inspired intrinsic reward function using a memory-augmented neural network to deter exploration of terminal states, mimicking fear conditioning in animals.


<details>
  <summary>Details</summary>
Motivation: The challenge of evaluating reinforcement learning methods in realistic environments with high negative rewards (e.g., death) lacking feedback inspired this work.

Method: A novel memory-augmented neural network (MANN) architecture produces intrinsic rewards based on early amygdala development, tested in the MiniWorld Sidewalk POMDP environment.

Result: The intrinsic motivation successfully avoids terminal states, resembling fear conditioning, and modifying the fear threshold produces behaviors akin to general anxiety disorders.

Conclusion: The study presents a biologically-inspired framework for fear conditioning in agents, demonstrating avoidance behavior in environments with non-descriptive terminal conditions.

Abstract: Biological and psychological concepts have inspired reinforcement learning
algorithms to create new complex behaviors that expand agents' capacity. These
behaviors can be seen in the rise of techniques like goal decomposition,
curriculum, and intrinsic rewards, which have paved the way for these complex
behaviors. One limitation in evaluating these methods is the requirement for
engineered extrinsic for realistic environments. A central challenge in
engineering the necessary reward function(s) comes from these environments
containing states that carry high negative rewards, but provide no feedback to
the agent. Death is one such stimuli that fails to provide direct feedback to
the agent. In this work, we introduce an intrinsic reward function inspired by
early amygdala development and produce this intrinsic reward through a novel
memory-augmented neural network (MANN) architecture. We show how this intrinsic
motivation serves to deter exploration of terminal states and results in
avoidance behavior similar to fear conditioning observed in animals.
Furthermore, we demonstrate how modifying a threshold where the fear response
is active produces a range of behaviors that are described under the paradigm
of general anxiety disorders (GADs). We demonstrate this behavior in the
Miniworld Sidewalk environment, which provides a partially observable Markov
decision process (POMDP) and a sparse reward with a non-descriptive terminal
condition, i.e., death. In effect, this study results in a
biologically-inspired neural architecture and framework for fear conditioning
paradigms; we empirically demonstrate avoidance behavior in a constructed agent
that is able to solve environments with non-descriptive terminal conditions.

</details>


### [308] [When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration](https://arxiv.org/pdf/2506.05579)
*Quan Shi, Carlos E. Jimenez, Shunyu Yao, Nick Haber, Diyi Yang, Karthik Narasimhan*

Main category: cs.AI

TL;DR: The paper introduces KITE, a framework to evaluate AI-human knowledge transfer, finding that benchmark performance doesn't consistently predict successful transfer, highlighting the need for dedicated optimization.


<details>
  <summary>Details</summary>
Motivation: To determine if AI reasoning improvements enhance knowledge transfer—how well models communicate reasoning for human understanding and application.

Method: Introduces KITE, a framework for evaluating knowledge transfer, and conducts a two-phase human study (N=118) where humans ideate with AI and then independently implement solutions.

Result: Benchmark performance correlates with collaborative outcomes but inconsistently, with outliers, showing knowledge transfer requires specific optimization. Behavioral and strategic factors mediate success.

Conclusion: Knowledge transfer needs dedicated optimization beyond benchmark performance. The study provides tools (code, dataset, framework) for future work on communicatively aligned models.

Abstract: Recent advancements in AI reasoning have driven substantial improvements
across diverse tasks. A critical open question is whether these improvements
also yields better knowledge transfer: the ability of models to communicate
reasoning in ways humans can understand, apply, and learn from. To investigate
this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a
conceptual and experimental framework for Human-AI knowledge transfer
capabilities and conduct the first large-scale human study (N=118) explicitly
designed to measure it. In our two-phase setup, humans first ideate with an AI
on problem-solving strategies, then independently implement solutions,
isolating model explanations' influence on human understanding. Our findings
reveal that although model benchmark performance correlates with collaborative
outcomes, this relationship is notably inconsistent, featuring significant
outliers, indicating that knowledge transfer requires dedicated optimization.
Our analysis identifies behavioral and strategic factors mediating successful
knowledge transfer. We release our code, dataset, and evaluation framework to
support future work on communicatively aligned models.

</details>


### [309] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/pdf/2506.05587)
*Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Lingjiao Chen, Dongmei Zhang, Surajit Chaudhuri, H. V. Jagadish*

Main category: cs.AI

TL;DR: MMTU is a large-scale benchmark for evaluating expert-level table-related tasks, highlighting gaps in current LLM capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for table-related tasks are limited and narrowly focused, missing the broader spectrum of real-world challenges faced by professionals.

Method: MMTU introduces over 30K questions across 25 real-world table tasks, derived from decades of CS research on tabular data.

Result: Frontier models like OpenAI o4-mini and DeepSeek R1 score only around 60%, indicating significant room for improvement.

Conclusion: MMTU aims to drive advances in foundation models for structured data processing by addressing current limitations.

Abstract: Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [310] [Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists](https://arxiv.org/pdf/2506.05616)
*Lianhao Zhou, Hongyi Ling, Keqiang Yan, Kaiji Zhao, Xiaoning Qian, Raymundo Arróyave, Xiaofeng Qian, Shuiwang Ji*

Main category: cs.AI

TL;DR: MAPPS is a framework for autonomous materials discovery, combining planning, physics, and scientist feedback to improve workflow automation and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomy in materials discovery by automating workflow planning and integrating physics and scientist intuition, overcoming limitations of predefined workflows.

Method: MAPPS includes a Workflow Planner (LLM-based), Tool Code Generator (Python code synthesis), and Scientific Mediator (feedback and error handling).

Result: Achieves five-fold improvement in stability, uniqueness, and novelty rates on MP-20 data compared to prior generative models.

Conclusion: MAPPS is a promising framework for flexible and reliable autonomous materials discovery.

Abstract: We aim at designing language agents with greater autonomy for crystal
materials discovery. While most of existing studies restrict the agents to
perform specific tasks within predefined workflows, we aim to automate workflow
planning given high-level goals and scientist intuition. To this end, we
propose Materials Agent unifying Planning, Physics, and Scientists, known as
MAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a
Scientific Mediator. The Workflow Planner uses large language models (LLMs) to
generate structured and multi-step workflows. The Tool Code Generator
synthesizes executable Python code for various tasks, including invoking a
force field foundation model that encodes physics. The Scientific Mediator
coordinates communications, facilitates scientist feedback, and ensures
robustness through error reflection and recovery. By unifying planning,
physics, and scientists, MAPPS enables flexible and reliable materials
discovery with greater autonomy, achieving a five-fold improvement in
stability, uniqueness, and novelty rates compared with prior generative models
when evaluated on the MP-20 data. We provide extensive experiments across
diverse tasks to show that MAPPS is a promising framework for autonomous
materials discovery.

</details>


### [311] [Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach](https://arxiv.org/pdf/2506.05619)
*Kihyun Kim, Jiawei Zhang, Asuman Ozdaglar, Pablo A. Parrilo*

Main category: cs.AI

TL;DR: A novel preference learning framework ensures policies align proportionally with evaluator preferences, avoiding bias by inferring population distributions from pairwise comparisons and satisfying social choice axioms.


<details>
  <summary>Details</summary>
Motivation: To address bias in conventional preference learning methods that favor widely held opinions, ensuring policies reflect true population preferences.

Method: Infers evaluator population distributions from pairwise comparisons, constructs policies satisfying social choice axioms, and uses a soft-max relaxation for trade-offs.

Result: Validated effectiveness and scalability on tabular recommendation tasks and large-scale language model alignment.

Conclusion: The framework successfully aligns policies with true population preferences while maintaining foundational axioms and scalability.

Abstract: Conventional preference learning methods often prioritize opinions held more
widely when aggregating preferences from multiple evaluators. This may result
in policies that are biased in favor of some types of opinions or groups. The
objective of this paper is to develop a novel preference learning framework
capable of aligning aggregate opinions and policies proportionally with the
true population distribution of evaluator preferences. Our approach infers the
feasible set of evaluator population distributions directly from pairwise
comparison data. Using these estimates, the algorithm constructs a policy that
satisfies foundational axioms from social choice theory, namely monotonicity
and Pareto efficiency, as well as our newly-introduced axioms of
population-proportional representation and population-bounded robustness. We
propose a soft-max relaxation method that smoothly trade-offs
population-proportional representation with the selection of the Condorcet
winner (which beats all other options in pairwise comparisons). Finally, we
validate the effectiveness and scalability of our approach through experiments
on both tabular recommendation tasks and large-scale language model alignment.

</details>


### [312] [Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties](https://arxiv.org/pdf/2506.05744)
*Gouki Minegishi, Hiroki Furuta, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo*

Main category: cs.AI

TL;DR: The paper analyzes reasoning graphs in large-scale models, revealing structural advantages like cyclicity and small-world properties, which correlate with performance and improve with model capacity and task difficulty.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms behind the success of large-scale reasoning models by examining their reasoning graph structures.

Method: Introduces reasoning graphs by clustering hidden-state representations and analyzes graph-theoretic properties (cyclicity, diameter, small-world index) across tasks and model variants.

Result: Distilled models show more recurrent cycles, larger diameters, and small-world characteristics, correlating with accuracy. Structural advantages grow with task difficulty and model size.

Conclusion: The study links reasoning graph structures to model performance, offering insights for improving dataset design and enhancing model interpretability and efficacy.

Abstract: Recent large-scale reasoning models have achieved state-of-the-art
performance on challenging mathematical benchmarks, yet the internal mechanisms
underlying their success remain poorly understood. In this work, we introduce
the notion of a reasoning graph, extracted by clustering hidden-state
representations at each reasoning step, and systematically analyze three key
graph-theoretic properties: cyclicity, diameter, and small-world index, across
multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled
reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly
more recurrent cycles (about 5 per sample), substantially larger graph
diameters, and pronounced small-world characteristics (about 6x) compared to
their base counterparts. Notably, these structural advantages grow with task
difficulty and model capacity, with cycle detection peaking at the 14B scale
and exploration diameter maximized in the 32B variant, correlating positively
with accuracy. Furthermore, we show that supervised fine-tuning on an improved
dataset systematically expands reasoning graph diameters in tandem with
performance gains, offering concrete guidelines for dataset design aimed at
boosting reasoning capabilities. By bridging theoretical insights into
reasoning graph structures with practical recommendations for data
construction, our work advances both the interpretability and the efficacy of
large reasoning models.

</details>


### [313] [SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models](https://arxiv.org/pdf/2506.05745)
*Emil Biju, Shayan Talaei, Zhemin Huang, Mohammadreza Pourreza, Azalia Mirhoseini, Amin Saberi*

Main category: cs.AI

TL;DR: SPRINT is a framework for parallelizing reasoning in large models, reducing sequential tokens by up to 39% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models (LRMs) generate lengthy sequential chains-of-thought, leading to slow inference times. SPRINT aims to enable parallelization to address this inefficiency.

Method: SPRINT uses a data curation pipeline to reorganize reasoning into parallelizable subtasks, fine-tuning LRMs to dynamically execute them in parallel.

Result: Models fine-tuned with SPRINT match performance on complex tasks while reducing sequential tokens by up to 39%. Out-of-distribution tasks show even greater reductions (up to 65%).

Conclusion: SPRINT effectively reduces inference time by parallelizing reasoning without sacrificing performance, demonstrating transferability to diverse tasks.

Abstract: Large reasoning models (LRMs) excel at complex reasoning tasks but typically
generate lengthy sequential chains-of-thought, resulting in long inference
times before arriving at the final answer. To address this challenge, we
introduce SPRINT, a novel post-training and inference-time framework designed
to enable LRMs to dynamically identify and exploit opportunities for
parallelization during their reasoning process. SPRINT incorporates an
innovative data curation pipeline that reorganizes natural language reasoning
trajectories into structured rounds of long-horizon planning and parallel
execution. By fine-tuning LRMs on a small amount of such curated data, the
models learn to dynamically identify independent subtasks within extended
reasoning processes and effectively execute them in parallel. Through extensive
evaluations, we show that the models fine-tuned with the SPRINT framework match
the performance of reasoning models on complex domains such as mathematics
while generating up to ~39% fewer sequential tokens on problems requiring more
than 8000 output tokens. Finally, we observe consistent results transferred to
two out-of-distribution tasks of GPQA and Countdown with up to 45% and 65%
reduction in average sequential tokens for longer reasoning trajectories, while
achieving the performance of the fine-tuned reasoning model.

</details>


### [314] [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/pdf/2506.05754)
*Emmanuel Anaya Gonzalez, Sairam Vaidya, Kanghee Park, Ruyi Ji, Taylor Berg-Kirkpatrick, Loris D'Antoni*

Main category: cs.AI

TL;DR: A new MCMC-based constrained sampling framework for LMs ensures constraint satisfaction, convergence to the true distribution, and efficiency, outperforming existing methods in program fuzzing.


<details>
  <summary>Details</summary>
Motivation: Existing constrained-decoding methods distort LM distributions, limiting diversity and validity in applications like program fuzzing.

Method: Proposes a framework using MCMC with a proposal distribution over valid outputs and Metropolis-Hastings acceptance based on LM likelihood.

Result: Outperforms existing methods on synthetic benchmarks and real-world program fuzzing tasks.

Conclusion: The framework effectively balances constraint satisfaction, convergence, and efficiency, proving superior in practical applications.

Abstract: Constrained decoding enables Language Models (LMs) to produce samples that
provably satisfy hard constraints. However, existing constrained-decoding
approaches often distort the underlying model distribution, a limitation that
is especially problematic in applications like program fuzzing, where one wants
to generate diverse and valid program inputs for testing purposes. We propose a
new constrained sampling framework based on Markov Chain Monte Carlo (MCMC)
that simultaneously satisfies three core desiderata: constraint satisfying
(every sample satisfies the constraint), monotonically converging (the sampling
process converges to the true conditional distribution), and efficient
(high-quality samples emerge in few steps). Our method constructs a proposal
distribution over valid outputs and applies a Metropolis-Hastings acceptance
criterion based on the LM's likelihood, ensuring principled and efficient
exploration of the constrained space. Empirically, our sampler outperforms
existing methods on both synthetic benchmarks and real-world program fuzzing
tasks.

</details>


### [315] [Trajectory Entropy: Modeling Game State Stability from Multimodality Trajectory Prediction](https://arxiv.org/pdf/2506.05810)
*Yesheng Zhang, Wenjian Sun, Yuheng Chen, Qingwei Liu, Qi Lin, Rui Zhang, Xu Zhao*

Main category: cs.AI

TL;DR: The paper introduces Trajectory Entropy to refine the level-k game framework in autonomous driving, improving accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing level-k game frameworks treat agent interactions uniformly, ignoring varying complexities and dynamic state changes, leading to redundant computations.

Method: Proposes Trajectory Entropy, a metric derived from trajectory prediction uncertainty, to quantify agent game status and refine the framework with a gating mechanism.

Result: Evaluated on Waymo and nuPlan datasets, the method improves precision by up to 19.89% for prediction and 16.48% for planning.

Conclusion: The refined framework with Trajectory Entropy enhances performance and efficiency in autonomous driving tasks.

Abstract: Complex interactions among agents present a significant challenge for
autonomous driving in real-world scenarios. Recently, a promising approach has
emerged, which formulates the interactions of agents as a level-k game
framework. It effectively decouples agent policies by hierarchical game levels.
However, this framework ignores both the varying driving complexities among
agents and the dynamic changes in agent states across game levels, instead
treating them uniformly. Consequently, redundant and error-prone computations
are introduced into this framework. To tackle the issue, this paper proposes a
metric, termed as Trajectory Entropy, to reveal the game status of agents
within the level-k game framework. The key insight stems from recognizing the
inherit relationship between agent policy uncertainty and the associated
driving complexity. Specifically, Trajectory Entropy extracts statistical
signals representing uncertainty from the multimodality trajectory prediction
results of agents in the game. Then, the signal-to-noise ratio of this signal
is utilized to quantify the game status of agents. Based on the proposed
Trajectory Entropy, we refine the current level-k game framework through a
simple gating mechanism, significantly improving overall accuracy while
reducing computational costs. Our method is evaluated on the Waymo and nuPlan
datasets, in terms of trajectory prediction, open-loop and closed-loop planning
tasks. The results demonstrate the state-of-the-art performance of our method,
with precision improved by up to 19.89% for prediction and up to 16.48% for
planning.

</details>


### [316] [Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs](https://arxiv.org/pdf/2506.05887)
*Marilyn Bello, Rafael Bello, Maria-Matilde García, Ann Nowé, Iván Sevillano-García, Francisco Herrera*

Main category: cs.AI

TL;DR: The paper proposes a multilevel framework for explainable AI (XAI) to align explanations with diverse stakeholder needs, enhancing trust through algorithmic, human-centered, and social layers, with LLMs playing a key role in social explainability.


<details>
  <summary>Details</summary>
Motivation: The need for AI systems to be explainable and trustworthy, especially in sensitive domains, drives the development of a framework catering to diverse audiences.

Method: A three-layer framework (algorithmic/domain-based, human-centered, social) is proposed, leveraging LLMs for natural language explanations. Case studies illustrate its application.

Result: The framework improves technical fidelity, user engagement, and societal accountability, positioning XAI as a trust-building process.

Conclusion: The multilevel approach reframes XAI to dynamically address stakeholder expectations, fostering trust in AI systems.

Abstract: The growing application of artificial intelligence in sensitive domains has
intensified the demand for systems that are not only accurate but also
explainable and trustworthy. Although explainable AI (XAI) methods have
proliferated, many do not consider the diverse audiences that interact with AI
systems: from developers and domain experts to end-users and society. This
paper addresses how trust in AI is influenced by the design and delivery of
explanations and proposes a multilevel framework that aligns explanations with
the epistemic, contextual, and ethical expectations of different stakeholders.
The framework consists of three layers: algorithmic and domain-based,
human-centered, and social explainability. We highlight the emerging role of
Large Language Models (LLMs) in enhancing the social layer by generating
accessible, natural language explanations. Through illustrative case studies,
we demonstrate how this approach facilitates technical fidelity, user
engagement, and societal accountability, reframing XAI as a dynamic,
trust-building process.

</details>


### [317] [Proactive Assistant Dialogue Generation from Streaming Egocentric Videos](https://arxiv.org/pdf/2506.05904)
*Yichi Zhang, Xin Luna Dong, Zhaojiang Lin, Andrea Madotto, Anuj Kumar, Babak Damavandi, Joyce Chai, Seungwhan Moon*

Main category: cs.AI

TL;DR: The paper presents a framework for developing real-time conversational AI systems for perceptual task guidance, addressing data collection and evaluation challenges with a synthetic dataset, automatic metrics, and an end-to-end model.


<details>
  <summary>Details</summary>
Motivation: Developing real-time AI systems for task guidance is hindered by costly data collection and evaluation processes.

Method: The framework includes a synthetic dialogue dataset, automatic evaluation metrics, and an end-to-end model for streaming video inputs.

Result: The work produces a scalable dataset, validated metrics, and a model for real-time assistance.

Conclusion: This framework advances the development of proactive AI assistants for diverse tasks.

Abstract: Recent advances in conversational AI have been substantial, but developing
real-time systems for perceptual task guidance remains challenging. These
systems must provide interactive, proactive assistance based on streaming
visual inputs, yet their development is constrained by the costly and
labor-intensive process of data collection and system evaluation. To address
these limitations, we present a comprehensive framework with three key
contributions. First, we introduce a novel data curation pipeline that
synthesizes dialogues from annotated egocentric videos, resulting in \dataset,
a large-scale synthetic dialogue dataset spanning multiple domains. Second, we
develop a suite of automatic evaluation metrics, validated through extensive
human studies. Third, we propose an end-to-end model that processes streaming
video inputs to generate contextually appropriate responses, incorporating
novel techniques for handling data imbalance and long-duration videos. This
work lays the foundation for developing real-time, proactive AI assistants
capable of guiding users through diverse tasks. Project page:
https://pro-assist.github.io/

</details>


### [318] [Preference Learning for AI Alignment: a Causal Perspective](https://arxiv.org/pdf/2506.05967)
*Katarzyna Kobalczyk, Mihaela van der Schaar*

Main category: cs.AI

TL;DR: The paper proposes a causal framework for reward modeling in LLMs to address challenges like misidentification and preference heterogeneity, advocating causally-inspired methods for robustness.


<details>
  <summary>Details</summary>
Motivation: To align LLMs with human values by improving reward modeling through causal inference, addressing issues like confounding and preference heterogeneity.

Method: Frames reward modeling causally, leveraging causal inference tools to identify and mitigate challenges, and contrasts assumptions with common data practices.

Result: Demonstrates failure modes of naive reward models and shows how causally-inspired methods enhance robustness.

Conclusion: Advocates for targeted interventions and future research to overcome limitations of observational data in reward modeling.

Abstract: Reward modelling from preference data is a crucial step in aligning large
language models (LLMs) with human values, requiring robust generalisation to
novel prompt-response pairs. In this work, we propose to frame this problem in
a causal paradigm, providing the rich toolbox of causality to identify the
persistent challenges, such as causal misidentification, preference
heterogeneity, and confounding due to user-specific factors. Inheriting from
the literature of causal inference, we identify key assumptions necessary for
reliable generalisation and contrast them with common data collection
practices. We illustrate failure modes of naive reward models and demonstrate
how causally-inspired approaches can improve model robustness. Finally, we
outline desiderata for future research and practices, advocating targeted
interventions to address inherent limitations of observational data.

</details>


### [319] [CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents](https://arxiv.org/pdf/2506.05981)
*Qingbin Zeng, Ruotong Zhao, Jinzhu Mao, Haoyang Li, Fengli Xu, Yong Li*

Main category: cs.AI

TL;DR: CrimeMind, an LLM-driven ABM framework, integrates Routine Activity Theory (RAT) to model urban crime, outperforming traditional ABMs and deep learning methods in accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Urban crime modeling is challenging due to subtle environmental cues and limitations of existing methods (ABMs lack accuracy, deep learning lacks interpretability and adaptability).

Method: CrimeMind combines LLMs with ABM, integrates RAT for reasoning, and aligns perception with human judgment via a training-free textual gradient method.

Result: CrimeMind achieves up to 24% improvement in crime hotspot prediction and accurately reflects counterfactual scenarios in simulations.

Conclusion: CrimeMind offers fine-grained crime modeling and effective evaluation of real-world interventions, bridging gaps in accuracy and adaptability.

Abstract: Modeling urban crime is an important yet challenging task that requires
understanding the subtle visual, social, and cultural cues embedded in urban
environments. Previous work has predominantly focused on rule-based agent-based
modeling (ABM) and deep learning methods. ABMs offer interpretability of
internal mechanisms but exhibit limited predictive accuracy.In contrast, deep
learning methods are often effective in prediction but are less interpretable
and require extensive training data. Moreover, both lines of work lack the
cognitive flexibility to adapt to changing environments. Leveraging the
capabilities of large language models (LLMs), we propose CrimeMind, a novel
LLM-driven ABM framework for simulating urban crime within a multi-modal urban
context.A key innovation of our design is the integration of the Routine
Activity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to
process rich multi-modal urban features and reason about criminal
behavior.However, RAT requires LLM agents to infer subtle cues in evaluating
environmental safety as part of assessing guardianship, which can be
challenging for LLMs. To address this, we collect a small-scale human-annotated
dataset and align CrimeMind's perception with human judgment via a
training-free textual gradient method.Experiments across four major U.S. cities
demonstrate that CrimeMind outperforms both traditional ABMs and deep learning
baselines in crime hotspot prediction and spatial distribution accuracy,
achieving up to a 24% improvement over the strongest baseline.Furthermore, we
conduct counterfactual simulations of external incidents and policy
interventions and it successfully captures the expected changes in crime
patterns, demonstrating its ability to reflect counterfactual
scenarios.Overall, CrimeMind enables fine-grained modeling of individual
behaviors and facilitates evaluation of real-world interventions.

</details>


### [320] [CP-Bench: Evaluating Large Language Models for Constraint Modelling](https://arxiv.org/pdf/2506.06052)
*Kostis Michailidis, Dimos Tsouros, Tias Guns*

Main category: cs.AI

TL;DR: The paper introduces CP-Bench, a diverse benchmark dataset for evaluating LLM-driven constraint modelling, comparing three systems and improving accuracy with prompt-based methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of diverse evaluation datasets for constraint modelling and enhance LLM-driven CP modelling capabilities.

Method: Introduces CP-Bench, evaluates LLMs on three constraint modelling systems (MiniZinc, CPMpy, OR-Tools CP-SAT), and tests prompt-based and inference-time compute methods.

Result: Python-based frameworks are more convenient, and documentation-rich prompts with repeated sampling achieve up to 70% accuracy.

Conclusion: CP-Bench fills a gap in evaluation datasets, and LLMs show promise in constraint modelling, especially with improved prompting techniques.

Abstract: Combinatorial problems are present in a wide range of industries. Constraint
Programming (CP) is a well-suited problem-solving paradigm, but its core
process, namely constraint modelling, is a bottleneck for wider adoption.
Aiming to alleviate this bottleneck, recent studies have explored using Large
Language Models (LLMs) as modelling assistants, transforming combinatorial
problem descriptions to executable constraint models, similar to coding
assistants. However, the existing evaluation datasets for constraint modelling
are often limited to small, homogeneous, or domain-specific instances, which do
not capture the diversity of real-world scenarios. This work addresses this gap
by introducing CP-Bench, a novel benchmark dataset that includes a diverse set
of well-known combinatorial problem classes sourced from the CP community,
structured explicitly for evaluating LLM-driven CP modelling. With this
dataset, and given the variety of constraint modelling frameworks, we compare
and evaluate the modelling capabilities of LLMs for three distinct constraint
modelling systems, which vary in abstraction level and underlying syntax: the
high-level MiniZinc language and Python-based CPMpy library, and the
lower-level Python interface of the OR-Tools CP-SAT solver. In order to enhance
the ability of LLMs to produce valid constraint models, we systematically
evaluate the use of prompt-based and inference-time compute methods adapted
from existing LLM-based code generation research. Our results underscore the
modelling convenience provided by Python-based frameworks, as well as the
effectiveness of documentation-rich system prompts, which, augmented with
repeated sampling and self-verification, achieve further improvements, reaching
up to 70\% accuracy on this new, highly challenging benchmark.

</details>


### [321] [Decomposability-Guaranteed Cooperative Coevolution for Large-Scale Itinerary Planning](https://arxiv.org/pdf/2506.06121)
*Ziyu Zhang, Peilan Xu, Yuetong Sun, Yuhui Shi, Wenjian Luo*

Main category: cs.AI

TL;DR: The paper addresses large-scale itinerary planning by introducing weak decomposability and a multi-objective cooperative coevolutionary algorithm, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To optimize large-scale itinerary planning by balancing POI scores, travel time, and cost under duration constraints, tackling decomposability challenges.

Method: Proposes a weak decomposability definition, a dynamic decomposition strategy, and a multi-objective cooperative coevolutionary algorithm with resource allocation.

Result: The algorithm outperforms state-of-the-art methods, especially as problem scale increases, validated on real-world datasets.

Conclusion: Weak decomposability and the proposed algorithm effectively address large-scale itinerary planning, offering scalable performance advantages.

Abstract: Large-scale itinerary planning is a variant of the traveling salesman
problem, aiming to determine an optimal path that maximizes the collected
points of interest (POIs) scores while minimizing travel time and cost, subject
to travel duration constraints. This paper analyzes the decomposability of
large-scale itinerary planning, proving that strict decomposability is
difficult to satisfy, and introduces a weak decomposability definition based on
a necessary condition, deriving the corresponding graph structures that fulfill
this property. With decomposability guaranteed, we propose a novel
multi-objective cooperative coevolutionary algorithm for large-scale itinerary
planning, addressing the challenges of component imbalance and interactions.
Specifically, we design a dynamic decomposition strategy based on the
normalized fitness within each component, define optimization potential
considering component scale and contribution, and develop a computational
resource allocation strategy. Finally, we evaluate the proposed algorithm on a
set of real-world datasets. Comparative experiments with state-of-the-art
multi-objective itinerary planning algorithms demonstrate the superiority of
our approach, with performance advantages increasing as the problem scale
grows.

</details>


### [322] [Integer Linear Programming Preprocessing for Maximum Satisfiability](https://arxiv.org/pdf/2506.06216)
*Jialu Zhang, Chu-Min Li, Sami Cherif, Shuolin Li, Zhifei Zheng*

Main category: cs.AI

TL;DR: ILP preprocessing techniques improve MaxSAT solving, reducing reliance on ILP solvers in portfolios.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of ILP preprocessing on MaxSAT solving, given its widespread use in solvers.

Method: Investigates ILP preprocessing techniques and their effect on MaxSAT solvers like WMaxCDCL-OpenWbo1200.

Result: ILP preprocessing helps solve 15 more instances and reduces the need for ILP solvers in portfolios.

Conclusion: ILP preprocessing enhances MaxSAT solving efficiency and reduces dependency on ILP solvers.

Abstract: The Maximum Satisfiability problem (MaxSAT) is a major optimization challenge
with numerous practical applications. In recent MaxSAT evaluations, most MaxSAT
solvers have adopted an ILP solver as part of their portfolios. This paper
investigates the impact of Integer Linear Programming (ILP) preprocessing
techniques on MaxSAT solving. Experimental results show that ILP preprocessing
techniques help WMaxCDCL-OpenWbo1200, the winner of the MaxSAT evaluation 2024
in the unweighted track, solve 15 additional instances. Moreover, current
state-of-the-art MaxSAT solvers heavily use an ILP solver in their portfolios,
while our proposed approach reduces the need to call an ILP solver in a
portfolio including WMaxCDCL or MaxCDCL.

</details>


### [323] [PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](https://arxiv.org/pdf/2506.06254)
*Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo Liu, Philip S. Yu, Xian Li*

Main category: cs.AI

TL;DR: PersonaAgent is a personalized LLM agent framework with memory and action modules, outperforming baselines in adapting to user preferences.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack flexibility for user-specific needs, prompting the development of PersonaAgent for personalized tasks.

Method: Integrates personalized memory (episodic/semantic) and action modules, with a persona prompt intermediary. Uses test-time alignment for real-time preference optimization.

Result: Outperforms baselines in personalization and scalability, demonstrating effective real-world application.

Conclusion: PersonaAgent shows feasibility in delivering dynamic, tailored user experiences, highlighting its potential.

Abstract: Large Language Model (LLM) empowered agents have recently emerged as advanced
paradigms that exhibit impressive capabilities in a wide range of domains and
tasks. Despite their potential, current LLM agents often adopt a
one-size-fits-all approach, lacking the flexibility to respond to users'
varying needs and preferences. This limitation motivates us to develop
PersonaAgent, the first personalized LLM agent framework designed to address
versatile personalization tasks. Specifically, PersonaAgent integrates two
complementary components - a personalized memory module that includes episodic
and semantic memory mechanisms; a personalized action module that enables the
agent to perform tool actions tailored to the user. At the core, the persona
(defined as unique system prompt for each user) functions as an intermediary:
it leverages insights from personalized memory to control agent actions, while
the outcomes of these actions in turn refine the memory. Based on the
framework, we propose a test-time user-preference alignment strategy that
simulate the latest n interactions to optimize the persona prompt, ensuring
real-time user preference alignment through textual loss feedback between
simulated and ground-truth responses. Experimental evaluations demonstrate that
PersonaAgent significantly outperforms other baseline methods by not only
personalizing the action space effectively but also scaling during test-time
real-world applications. These results underscore the feasibility and potential
of our approach in delivering tailored, dynamic user experiences.

</details>


### [324] [Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens](https://arxiv.org/pdf/2506.06261)
*Jihwan Jeong, Xiaoyu Wang, Jingmin Wang, Scott Sanner, Pascal Poupart*

Main category: cs.AI

TL;DR: RefPlan, a doubly Bayesian offline model-based planning method, improves offline RL by unifying uncertainty modeling and planning, enhancing performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Offline RL struggles with high epistemic uncertainty and limited data, while existing methods lack adaptivity and generalization.

Method: RefPlan recasts planning as Bayesian posterior estimation, updating beliefs over dynamics using real-time observations and incorporating uncertainty into planning.

Result: RefPlan outperforms conservative offline RL policies, showing robustness under uncertainty and adaptability to changing dynamics.

Conclusion: RefPlan enhances flexibility, generalizability, and robustness of offline-learned policies, addressing key challenges in offline RL.

Abstract: Offline reinforcement learning (RL) is crucial when online exploration is
costly or unsafe but often struggles with high epistemic uncertainty due to
limited data. Existing methods rely on fixed conservative policies, restricting
adaptivity and generalization. To address this, we propose Reflect-then-Plan
(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.
RefPlan unifies uncertainty modeling and MB planning by recasting planning as
Bayesian posterior estimation. At deployment, it updates a belief over
environment dynamics using real-time observations, incorporating uncertainty
into MB planning via marginalization. Empirical results on standard benchmarks
show that RefPlan significantly improves the performance of conservative
offline RL policies. In particular, RefPlan maintains robust performance under
high epistemic uncertainty and limited data, while demonstrating resilience to
changing environment dynamics, improving the flexibility, generalizability, and
robustness of offline-learned policies.

</details>


### [325] [Vehicle: Bridging the Embedding Gap in the Verification of Neuro-Symbolic Programs](https://arxiv.org/pdf/2401.06379)
*Matthew L. Daggitt, Wen Kokke, Robert Atkey, Ekaterina Komendantskaya, Natalia Slusarz, Luca Arnaboldi*

Main category: cs.AI

TL;DR: The paper presents Vehicle, an intermediate language for verifying neuro-symbolic programs by addressing the embedding gap between neural and symbolic components.


<details>
  <summary>Details</summary>
Motivation: Neuro-symbolic programs combine machine learning and symbolic code, posing verification challenges due to their intricate interfaces.

Method: Introduces Vehicle, a verification condition language, to decompose and verify neuro-symbolic programs by compiling specifications to various interfaces.

Result: Demonstrates Vehicle's utility by formally verifying the safety of a neural network-controlled autonomous car in a stochastic environment.

Conclusion: Vehicle provides a general methodology for verifying neuro-symbolic programs, bridging the gap between neural and symbolic components.

Abstract: Neuro-symbolic programs, i.e. programs containing both machine learning
components and traditional symbolic code, are becoming increasingly widespread.
Finding a general methodology for verifying such programs is challenging due to
both the number of different tools involved and the intricate interface between
the ``neural'' and ``symbolic'' program components. In this paper we present a
general decomposition of the neuro-symbolic verification problem into parts,
and examine the problem of the embedding gap that occurs when one tries to
combine proofs about the neural and symbolic components. To address this
problem we then introduce Vehicle -- standing as an abbreviation for a
``verification condition language'' -- an intermediate programming language
interface between machine learning frameworks, automated theorem provers, and
dependently-typed formalisations of neuro-symbolic programs. Vehicle allows
users to specify the properties of the neural components of neuro-symbolic
programs once, and then safely compile the specification to each interface
using a tailored typing and compilation procedure. We give a high-level
overview of Vehicle's overall design, its interfaces and compilation &
type-checking procedures, and then demonstrate its utility by formally
verifying the safety of a simple autonomous car controlled by a neural network,
operating in a stochastic environment with imperfect information.

</details>


### [326] [Domain Generalizable Knowledge Tracing via Concept Aggregation and Relation-Based Attention](https://arxiv.org/pdf/2407.02547)
*Yuquan Xie, Shengtao Peng, Wanqi Yang, Ming Yang, Yang Gao*

Main category: cs.AI

TL;DR: The paper proposes a domain generalization approach for knowledge tracing (DGKT) to address performance degradation in new education systems with limited data, introducing techniques like concept aggregation and Sequence Instance Normalization (SeqIN).


<details>
  <summary>Details</summary>
Motivation: Existing KT methods degrade in performance with scarce student interactions in new systems. Leveraging data from existing systems is challenging due to domain differences.

Method: Proposes DGKT framework with concept aggregation, SeqIN, and a new model (DGRKT) for domain generalization.

Result: Experiments on five datasets show the method performs well despite limited training data.

Conclusion: DGKT effectively mitigates domain discrepancies and improves KT performance in data-scarce scenarios.

Abstract: Knowledge Tracing (KT) is a critical task in online education systems, aiming
to monitor students' knowledge states throughout a learning period. Common KT
approaches involve predicting the probability of a student correctly answering
the next question based on their exercise history. However, these methods often
suffer from performance degradation when faced with the scarcity of student
interactions in new education systems. To address this, we leverage student
interactions from existing education systems to mitigate performance
degradation caused by limited training data. Nevertheless, these interactions
exhibit significant differences since they are derived from different education
systems. To address this issue, we propose a domain generalization approach for
knowledge tracing, where existing education systems are considered source
domains, and new education systems with limited data are considered target
domains. Additionally, we design a domain-generalizable knowledge tracing
framework (DGKT) that can be applied to any KT model. Specifically, we present
a concept aggregation approach designed to reduce conceptual disparities within
sequences of student interactions from diverse domains. To further mitigate
domain discrepancies, we introduce a novel normalization module called Sequence
Instance Normalization (SeqIN). Moreover, to fully leverage exercise
information, we propose a new knowledge tracing model tailored for the domain
generalization KT task, named Domain-Generalizable Relation-based Knowledge
Tracing (DGRKT). Extensive experiments across five benchmark datasets
demonstrate that the proposed method performs well despite limited training
data.

</details>


### [327] [Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](https://arxiv.org/pdf/2407.02814)
*Zhaotian Weng, Zijun Gao, Jerone Andrews, Jieyu Zhao*

Main category: cs.AI

TL;DR: A framework using causal mediation analysis identifies image features as the main source of bias in vision-language models, with targeted interventions reducing bias efficiently.


<details>
  <summary>Details</summary>
Motivation: VLMs often learn biases from datasets, but current methods fail to comprehensively analyze bias from model components.

Method: Proposes causal mediation analysis to trace bias pathways, focusing on interventions' direct and indirect effects on bias.

Result: Image features contribute most to bias (32.57% in MSCOCO, 12.63% in PASCAL-SENTENCE). Blurring gender representations in the image encoder reduces bias by 22.03% and 9.04%, respectively.

Conclusion: Targeting the image encoder, the primary bias contributor, efficiently reduces bias with minimal performance impact.

Abstract: Vision-language models (VLMs) pre-trained on extensive datasets can
inadvertently learn biases by correlating gender information with specific
objects or scenarios. Current methods, which focus on modifying inputs and
monitoring changes in the model's output probability scores, often struggle to
comprehensively understand bias from the perspective of model components. We
propose a framework that incorporates causal mediation analysis to measure and
map the pathways of bias generation and propagation within VLMs. This approach
allows us to identify the direct effects of interventions on model bias and the
indirect effects of interventions on bias mediated through different model
components. Our results show that image features are the primary contributors
to bias, with significantly higher impacts than text features, specifically
accounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE
datasets, respectively. Notably, the image encoder's contribution surpasses
that of the text encoder and the deep fusion encoder. Further experimentation
confirms that contributions from both language and vision modalities are
aligned and non-conflicting. Consequently, focusing on blurring gender
representations within the image encoder, which contributes most to the model
bias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and
PASCAL-SENTENCE datasets, respectively, with minimal performance loss or
increased computational demands.

</details>


### [328] [Is Cognition Consistent with Perception? Assessing and Mitigating Multimodal Knowledge Conflicts in Document Understanding](https://arxiv.org/pdf/2411.07722)
*Zirui Shao, Feiyu Gao, Zhaoqing Zhu, Chuwei Luo, Hangdi Xing, Zhi Yu, Qi Zheng, Ming Yan, Jiajun Bu*

Main category: cs.AI

TL;DR: The paper addresses conflicts between perception and cognition in multimodal large language models (MLLMs) for document understanding, proposing a fine-tuning method to improve consistency.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with aligning perceptual (e.g., OCR) and cognitive (e.g., VQA) tasks due to annotation noise, leading to performance and explainability issues.

Method: Proposes Multimodal Knowledge Consistency Fine-tuning to mitigate Cognition and Perception (C&P) knowledge conflicts.

Result: GPT-4o achieves only 75.26% C&P consistency; the proposed method reduces conflicts and improves performance.

Conclusion: The method effectively addresses C&P conflicts, enhancing MLLM performance and explainability in document understanding.

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
in document understanding, a rapidly growing research area with significant
industrial demand. As a multimodal task, document understanding requires models
to possess both perceptual and cognitive abilities. However, due to different
types of annotation noise in training, current MLLMs often face conflicts
between perception and cognition. Taking a document VQA task (cognition) as an
example, an MLLM might generate answers that do not match the corresponding
visual content identified by its OCR (perception). This conflict suggests that
the MLLM might struggle to establish an intrinsic connection between the
information it "sees" and what it "understands". Such conflicts challenge the
intuitive notion that cognition is consistent with perception, hindering the
performance and explainability of MLLMs. In this paper, we define the conflicts
between cognition and perception as Cognition and Perception (C&P) knowledge
conflicts, a form of multimodal knowledge conflict, and systematically assess
them with a focus on document understanding. Our analysis reveals that even
GPT-4o, a leading MLLM, achieves only 75.26% C&P consistency. To mitigate the
C&P knowledge conflicts, we propose a novel method called Multimodal Knowledge
Consistency Fine-tuning. Our method reduces C&P knowledge conflicts across all
tested MLLMs and enhances their performance in both cognitive and perceptual
tasks. All data we construct will be publicly available.

</details>


### [329] [Regret-Free Reinforcement Learning for LTL Specifications](https://arxiv.org/pdf/2411.12019)
*Rupak Majumdar, Mahmoud Salamati, Sadegh Soudjani*

Main category: cs.AI

TL;DR: A regret-free online algorithm for learning controllers for LTL specifications in unknown MDP dynamics, with sharp performance bounds.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of controlling unknown dynamical systems under high-level temporal specifications, where existing methods only offer asymptotic guarantees.

Method: Proposes a regret-free learning algorithm for infinite-horizon reach-avoid problems on MDPs, reducing LTL synthesis to reach-avoid once the graph structure is known. Includes a separate algorithm for learning the graph structure.

Result: Provides sharp bounds on optimal behavior after finite learning episodes, unlike previous asymptotic-only guarantees.

Conclusion: The algorithm advances LTL controller synthesis by offering transient performance insights and regret-free learning for unknown MDP dynamics.

Abstract: Learning to control an unknown dynamical system with respect to high-level
temporal specifications is an important problem in control theory. We present
the first regret-free online algorithm for learning a controller for linear
temporal logic (LTL) specifications for systems with unknown dynamics. We
assume that the underlying (unknown) dynamics is modeled by a finite-state and
action Markov decision process (MDP). Our core technical result is a
regret-free learning algorithm for infinite-horizon reach-avoid problems on
MDPs. For general LTL specifications, we show that the synthesis problem can be
reduced to a reach-avoid problem once the graph structure is known.
Additionally, we provide an algorithm for learning the graph structure,
assuming knowledge of a minimum transition probability, which operates
independently of the main regret-free algorithm. Our LTL controller synthesis
algorithm provides sharp bounds on how close we are to achieving optimal
behavior after a finite number of learning episodes. In contrast, previous
algorithms for LTL synthesis only provide asymptotic guarantees, which give no
insight into the transient performance during the learning phase.

</details>


### [330] [Agents for self-driving laboratories applied to quantum computing](https://arxiv.org/pdf/2412.07978)
*Shuxiang Cao, Zijian Zhang, Mohammed Alghadeer, Simone D Fasciati, Michele Piscitelli, Mustafa Bakr, Peter Leek, Alán Aspuru-Guzik*

Main category: cs.AI

TL;DR: The paper introduces the k-agents framework for automating self-driving labs using AI agents to organize knowledge and execute experiments, demonstrated in quantum processor calibration.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating unstructured, multimodal laboratory knowledge into AI systems for high-throughput scientific discovery.

Method: The k-agents framework uses large language model-based agents to encapsulate lab knowledge and execution agents to automate multi-step experiments via state machines.

Result: Agents autonomously planned and executed experiments on a superconducting quantum processor, achieving human-level entangled state characterization.

Conclusion: The framework enables efficient lab knowledge management and accelerates scientific discovery through automation.

Abstract: Fully automated self-driving laboratories are promising to enable
high-throughput and large-scale scientific discovery by reducing repetitive
labour. However, effective automation requires deep integration of laboratory
knowledge, which is often unstructured, multimodal, and difficult to
incorporate into current AI systems. This paper introduces the k-agents
framework, designed to support experimentalists in organizing laboratory
knowledge and automating experiments with agents. Our framework employs large
language model-based agents to encapsulate laboratory knowledge including
available laboratory operations and methods for analyzing experiment results.
To automate experiments, we introduce execution agents that break multi-step
experimental procedures into agent-based state machines, interact with other
agents to execute each step and analyze the experiment results. The analyzed
results are then utilized to drive state transitions, enabling closed-loop
feedback control. To demonstrate its capabilities, we applied the agents to
calibrate and operate a superconducting quantum processor, where they
autonomously planned and executed experiments for hours, successfully producing
and characterizing entangled quantum states at the level achieved by human
scientists. Our knowledge-based agent system opens up new possibilities for
managing laboratory knowledge and accelerating scientific discovery.

</details>


### [331] [Position: Theory of Mind Benchmarks are Broken for Large Language Models](https://arxiv.org/pdf/2412.19726)
*Matthew Riemer, Zahra Ashktorab, Djallel Bouneffouf, Payel Das, Miao Liu, Justin D. Weisz, Murray Campbell*

Main category: cs.AI

TL;DR: The paper critiques current theory of mind benchmarks for LLMs, proposing 'functional theory of mind' as a better metric for evaluating adaptability to new partners.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to test LLMs' adaptability to new partners, wrongly assuming human-like reasoning consistency.

Method: Introduces 'functional theory of mind' to measure LLMs' ability to adapt to partners' behaviors in-context.

Result: Open-source LLMs excel in literal theory of mind but struggle with functional theory of mind, even with simple partner policies.

Conclusion: Functional theory of mind should be prioritized in LLM evaluations, as it better reflects real-world adaptability challenges.

Abstract: Our paper argues that the majority of theory of mind benchmarks are broken
because of their inability to directly test how large language models (LLMs)
adapt to new partners. This problem stems from the fact that theory of mind
benchmarks for LLMs are overwhelmingly inspired by the methods used to test
theory of mind in humans and fall victim to a fallacy of attributing human-like
qualities to AI agents. We expect that humans will engage in a consistent
reasoning process across various questions about a situation, but this is known
to not be the case for current LLMs. Most theory of mind benchmarks only
measure what we call literal theory of mind: the ability to predict the
behavior of others. However, this type of metric is only informative when
agents exhibit self-consistent reasoning. Thus, we introduce the concept of
functional theory of mind: the ability to adapt to agents in-context following
a rational response to their behavior. We find that many open source LLMs are
capable of displaying strong literal theory of mind capabilities, but seem to
struggle with functional theory of mind -- even with exceedingly simple partner
policies. Simply put, strong literal theory of mind performance does not
necessarily imply strong functional theory of mind performance or vice versa.
Achieving functional theory of mind, particularly over long interaction
horizons with a partner, is a significant challenge deserving a prominent role
in any meaningful LLM theory of mind evaluation.

</details>


### [332] [Artificial Intelligence in Creative Industries: Advances Prior to 2025](https://arxiv.org/pdf/2501.02725)
*Nantheera Anantrasirichai, Fan Zhang, David Bull*

Main category: cs.AI

TL;DR: The paper examines AI's impact on creative industries, highlighting advancements in generative AI, LLMs, and multimodal technologies since 2022, their integration into workflows, and future potential.


<details>
  <summary>Details</summary>
Motivation: To explore how recent AI advancements have expanded creative opportunities, improved efficiency, and transformed media workflows.

Method: Review of technological shifts in generative AI, LLMs, and multimodal tools, focusing on their applications in content creation, post-production, and media representation.

Result: AI has revolutionized creative workflows, enabling innovative content generation, faster post-production, and improved media compression, while emphasizing human oversight.

Conclusion: AI holds significant future potential in the creative sector, but challenges and risks must be navigated to maximize benefits.

Abstract: The rapid advancements in artificial intelligence (AI), particularly in
generative AI and large language models (LLMs), have profoundly impacted the
creative industries, enabling more innovative content creation, enhancing
workflows, and democratizing access to creative tools. This paper explores
these technological shifts, with particular focus on how those that have
emerged since our previous review in 2022 have expanded creative opportunities
and improved efficiency. These technological advancements have enhanced the
capabilities of text-to-image, text-to-video, and multimodal generation
technologies. In particular, key breakthroughs in LLMs have established new
benchmarks in conversational AI, while advancements in image generators have
revolutionized content creation. We also discuss the integration of AI into
post-production workflows, which has significantly accelerated and improved
traditional processes. Once content has been created, it must be delivered to
its audiences the media industry is facing the demands of increased
communication traffic due to creative content. We therefore include a
discussion of how AI is beginning to transform the way we represent and
compress media content. We highlight the trend toward unified AI frameworks
capable of addressing and integrating multiple creative tasks, and we
underscore the importance of human insight to drive the creative process and
oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's
future potential in the creative sector, stressing the need to navigate
emerging challenges and to maximize its benefits while addressing the
associated risks.

</details>


### [333] [Scaffolding Creativity: Integrating Generative AI Tools and Real-world Experiences in Business Education](https://arxiv.org/pdf/2501.06527)
*Nicole C. Wang*

Main category: cs.AI

TL;DR: Study explores Generative AI in business education, showing how it enhances creativity and learning through real-world integration, while highlighting challenges like prompt engineering.


<details>
  <summary>Details</summary>
Motivation: To understand how Generative AI tools can enhance experiential learning in business education and identify interaction challenges.

Method: Case study of an undergraduate course where students used AI tools (text and image generation) alongside real-world experiences.

Result: AI integration helps students overcome creative barriers, speeds up skill acquisition, and creates synergy between AI insights and real-world validation. Challenges include prompt engineering and interface intuitiveness.

Conclusion: Findings guide future AI tool design for creative learning and contribute to HCI discussions on human-AI collaboration in education.

Abstract: This exploratory study investigates the intersection of Generative AI tools
and experiential learning in business education. Through a case study of an
innovative undergraduate course, we examine how students interact with and
adapt to various AI modalities-from text-based tools to image
generation-alongside real-world experiences. Our findings reveal how this
integrated approach enables novice users to overcome creative barriers,
accelerates skill acquisition, and creates a dynamic interplay between
AI-generated insights and real-world validation. We identify critical
interaction challenges, including prompt engineering patterns and the need for
more intuitive AI interfaces in educational contexts. These insights inform the
design of future AI tools for creative learning and contribute to broader HCI
discussions about human-AI collaboration in educational settings.

</details>


### [334] [MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding](https://arxiv.org/pdf/2501.18362)
*Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou*

Main category: cs.AI

TL;DR: MedXpertQA is a challenging benchmark for evaluating expert-level medical knowledge and reasoning, featuring 4,460 questions across 17 specialties and 11 body systems, with text and multimodal subsets.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient difficulty and clinical relevance of existing medical benchmarks like MedQA, and to provide a representative setting for assessing advanced reasoning.

Method: Rigorous filtering, augmentation, data synthesis, and expert reviews to ensure accuracy. Includes specialty board questions and multimodal evaluation with rich clinical data.

Result: Evaluation of 18 leading models on MedXpertQA, highlighting its effectiveness in assessing expert-level knowledge and reasoning.

Conclusion: MedXpertQA sets a new standard for medical benchmarks, offering comprehensive, reliable, and clinically relevant evaluation.

Abstract: We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 18 leading models on \benchmark.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models. Code and data are available at:
https://github.com/TsinghuaC3I/MedXpertQA

</details>


### [335] [Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?](https://arxiv.org/pdf/2502.08503)
*Jiahe Jin, Yanheng He, Mingyan Yang*

Main category: cs.AI

TL;DR: The paper highlights the '2D-Cheating' issue in 3D LLM evaluation, where VLMs solve tasks using rendered images instead of genuine 3D understanding, and proposes better evaluation principles.


<details>
  <summary>Details</summary>
Motivation: To address the ineffective evaluation of 3D LLMs' unique capabilities due to reliance on 2D-rendered data.

Method: Test VLM performance on 3D LLM benchmarks and use results to refine evaluation principles.

Result: Identified the '2D-Cheating' problem and suggested separating 3D abilities from 1D/2D aspects in evaluations.

Conclusion: Advocates for improved benchmarks to accurately assess 3D LLMs' true capabilities.

Abstract: In this work, we identify the "2D-Cheating" problem in 3D LLM evaluation,
where these tasks might be easily solved by VLMs with rendered images of point
clouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We
test VLM performance across multiple 3D LLM benchmarks and, using this as a
reference, propose principles for better assessing genuine 3D understanding. We
also advocate explicitly separating 3D abilities from 1D or 2D aspects when
evaluating 3D LLMs. Code and data are available at
https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks

</details>


### [336] [Do Large Language Models Reason Causally Like Us? Even Better?](https://arxiv.org/pdf/2502.10215)
*Hanna M. Dettki, Brenden M. Lake, Charley M. Wu, Bob Rehder*

Main category: cs.AI

TL;DR: LLMs (like GPT-4o, Gemini-Pro, and Claude) outperform humans in causal reasoning tasks, avoiding human biases, but still miss subtler reasoning patterns like 'explaining away'.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs' responses reflect true understanding or just statistical patterns by comparing their causal reasoning to humans'.

Method: Used tasks based on collider graphs to rate the likelihood of a query variable given evidence, testing humans and four LLMs (GPT-3.5, GPT-4o, Gemini-Pro, Claude).

Result: LLMs (except GPT-3.5) performed human-like or better, avoiding human associative bias. However, they missed subtler reasoning patterns like 'explaining away'.

Conclusion: LLMs show advanced causal reasoning but still lack full human-like understanding of nuanced patterns.

Abstract: Causal reasoning is a core component of intelligence. Large language models
(LLMs) have shown impressive capabilities in generating human-like text,
raising questions about whether their responses reflect true understanding or
statistical patterns. We compared causal reasoning in humans and four LLMs
using tasks based on collider graphs, rating the likelihood of a query variable
occurring given evidence from other variables. LLMs' causal inferences ranged
from often nonsensical (GPT-3.5) to human-like to often more normatively
aligned than those of humans (GPT-4o, Gemini-Pro, and Claude). Computational
model fitting showed that one reason for GPT-4o, Gemini-Pro, and Claude's
superior performance is they didn't exhibit the "associative bias" that plagues
human causal reasoning. Nevertheless, even these LLMs did not fully capture
subtler reasoning patterns associated with collider graphs, such as "explaining
away".

</details>


### [337] [Investigating Non-Transitivity in LLM-as-a-Judge](https://arxiv.org/pdf/2502.14074)
*Yi Xu, Laura Ruis, Tim Rocktäschel, Robert Kirk*

Main category: cs.AI

TL;DR: The paper explores non-transitive preferences in LLM-based evaluations, proposes round-robin tournaments with Bradley-Terry models for reliable rankings, and introduces Swim tournaments for efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored assumption of transitive preferences in LLM-based evaluations and its impact on model rankings.

Method: Investigates non-transitivity in AlpacaEval, uses round-robin tournaments with Bradley-Terry models, and proposes Swim tournaments for efficiency.

Result: Improved Spearman (95.0% -> 96.4%) and Kendall (82.1% -> 86.3%) correlations with Chatbot Arena.

Conclusion: Round-robin and Swim tournaments mitigate non-transitivity issues, providing more reliable and efficient rankings.

Abstract: Automatic evaluation methods based on large language models (LLMs) are
emerging as the standard tool for assessing the instruction-following abilities
of LLM-based agents. The most common method in this paradigm, pairwise
comparisons with a baseline model, critically depends on the assumption of
transitive preferences. However, the validity of this assumption remains
largely unexplored. In this study, we investigate the presence of
non-transitivity within the AlpacaEval framework and analyze its effects on
model rankings. We find that LLM judges exhibit non-transitive preferences,
leading to rankings that are sensitive to the choice of the baseline model. To
mitigate this issue, we show that round-robin tournaments combined with
Bradley-Terry models of preference can produce more reliable rankings. Notably,
our method increases both the Spearman correlation and the Kendall correlation
with Chatbot Arena (95.0% -> 96.4% and 82.1% -> 86.3% respectively). To address
the computational cost of round-robin tournaments, we propose Swiss-Wise
Iterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to
capture the benefits of round-robin tournaments while maintaining computational
efficiency.

</details>


### [338] [Paradigms of AI Evaluation: Mapping Goals, Methodologies and Culture](https://arxiv.org/pdf/2502.15620)
*John Burden, Marko Tešić, Lorenzo Pacchiardi, José Hernández-Orallo*

Main category: cs.AI

TL;DR: The paper surveys AI evaluation paradigms to address fragmentation and foster collaboration.


<details>
  <summary>Details</summary>
Motivation: The growing complexity and isolation in AI evaluation research hinder progress and public understanding.

Method: Survey recent work, identify six paradigms, and analyze their goals, methodologies, and cultures.

Result: Clarifies unique aspects of each paradigm, aiming to bridge gaps and inspire future research.

Conclusion: Increased awareness of diverse evaluation approaches can foster cross-paradigm collaboration and address gaps.

Abstract: Research in AI evaluation has grown increasingly complex and
multidisciplinary, attracting researchers with diverse backgrounds and
objectives. As a result, divergent evaluation paradigms have emerged, often
developing in isolation, adopting conflicting terminologies, and overlooking
each other's contributions. This fragmentation has led to insular research
trajectories and communication barriers both among different paradigms and with
the general public, contributing to unmet expectations for deployed AI systems.
To help bridge this insularity, in this paper we survey recent work in the AI
evaluation landscape and identify six main paradigms. We characterise major
recent contributions within each paradigm across key dimensions related to
their goals, methodologies and research cultures. By clarifying the unique
combination of questions and approaches associated with each paradigm, we aim
to increase awareness of the breadth of current evaluation approaches and
foster cross-pollination between different paradigms. We also identify
potential gaps in the field to inspire future research directions.

</details>


### [339] [Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires](https://arxiv.org/pdf/2503.00566)
*Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li*

Main category: cs.AI

TL;DR: The paper presents a multi-agent LLM system for analyzing air quality during the 2025 LA wildfires, leveraging cloud-mapping and automated data analysis for policy recommendations.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient large-scale data analysis during disasters like wildfires, building on prior work (Digital Twin Building) and recent LLM advancements.

Method: Uses a multi-agent LLM framework with an Instructor agent (retrieves data, generates prompts) and Worker agents (analyze data, provide summaries) for automated analysis.

Result: The system successfully analyzes air quality data and generates health recommendations during the wildfires.

Conclusion: The multi-agent LLM system proves effective for disaster response data analysis and policy recommendations.

Abstract: The Los Angeles wildfires of January 2025 caused more than 250 billion
dollars in damage and lasted for nearly an entire month before containment.
Following our previous work, the Digital Twin Building, we modify and leverage
the multi-agent large language model framework as well as the cloud-mapping
integration to study the air quality during the Los Angeles wildfires. Recent
advances in large language models have allowed for out-of-the-box automated
large-scale data analysis. We use a multi-agent large language system comprised
of an Instructor agent and Worker agents. Upon receiving the users'
instructions, the Instructor agent retrieves the data from the cloud platform
and produces instruction prompts to the Worker agents. The Worker agents then
analyze the data and provide summaries. The summaries are finally input back
into the Instructor agent, which then provides the final data analysis. We test
this system's capability for data-based policy recommendation by assessing our
Instructor-Worker LLM system's health recommendations based on air quality
during the Los Angeles wildfires.

</details>


### [340] [DebFlow: Automating Agent Creation via Agent Debate](https://arxiv.org/pdf/2503.23781)
*Jinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang, Tianyu Shi, Yang Jingsong, Lewei He*

Main category: cs.AI

TL;DR: DebFlow, a framework using debate and reflexion, improves workflow optimization in LLMs, outperforming baselines by 3% and reducing resource use by 37%.


<details>
  <summary>Details</summary>
Motivation: Addressing limited reasoning, high computational demands, and resource requirements in existing LLM-based workflow optimization approaches.

Method: Proposes DebFlow, integrating debate mechanisms and reflexion to optimize workflows. Evaluated on six datasets (e.g., HotpotQA, MATH, ALFWorld).

Result: Achieved 3% average performance improvement, 37% resource reduction, and ablation studies showed Debate's critical role (4% drop vs. 2% for Reflexion).

Conclusion: DebFlow effectively enhances workflow optimization, with Debate being pivotal and Reflexion providing auxiliary benefits.

Abstract: Large language models (LLMs) have demonstrated strong potential and
impressive performance in automating the generation and optimization of
workflows. However, existing approaches are marked by limited reasoning
capabilities, high computational demands, and significant resource
requirements. To address these issues, we propose DebFlow, a framework that
employs a debate mechanism to optimize workflows and integrates reflexion to
improve based on previous experiences. We evaluated our method across six
benchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approach
achieved a 3\% average performance improvement over the latest baselines,
demonstrating its effectiveness in diverse problem domains. In particular,
during training, our framework reduces resource consumption by 37\% compared to
the state-of-the-art baselines. Additionally, we performed ablation studies.
Removing the Debate component resulted in a 4\% performance drop across two
benchmark datasets, significantly greater than the 2\% drop observed when the
Reflection component was removed. These findings strongly demonstrate the
critical role of Debate in enhancing framework performance, while also
highlighting the auxiliary contribution of reflexion to overall optimization.

</details>


### [341] [AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking](https://arxiv.org/pdf/2505.17312)
*Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang*

Main category: cs.AI

TL;DR: AdaReasoner is an LLM-agnostic plugin that automates adaptive reasoning configurations for tasks requiring diverse thinking, outperforming fixed-configuration baselines.


<details>
  <summary>Details</summary>
Motivation: Existing prompting approaches use fixed configurations, which are suboptimal for task-specific reasoning. AdaReasoner aims to automate adaptive configurations for better performance.

Method: AdaReasoner uses a reinforcement learning framework with a factorized action space, targeted exploration, and a pretrained reward model to optimize reasoning configurations.

Result: It outperforms baselines across six LLMs and various reasoning tasks, showing fast convergence, robustness, and gains in knowledge-intensive tasks.

Conclusion: AdaReasoner effectively automates adaptive reasoning configurations, enhancing LLM performance for diverse tasks.

Abstract: LLMs often need effective configurations, like temperature and reasoning
steps, to handle tasks requiring sophisticated reasoning and problem-solving,
ranging from joke generation to mathematical reasoning. Existing prompting
approaches usually adopt general-purpose, fixed configurations that work 'well
enough' across tasks but seldom achieve task-specific optimality. To address
this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM
to automate adaptive reasoning configurations for tasks requiring different
types of thinking. AdaReasoner is trained using a reinforcement learning (RL)
framework, combining a factorized action space with a targeted exploration
strategy, along with a pretrained reward model to optimize the policy model for
reasoning configurations with only a few-shot guide. AdaReasoner is backed by
theoretical guarantees and experiments of fast convergence and a sublinear
policy gap. Across six different LLMs and a variety of reasoning tasks, it
consistently outperforms standard baselines, preserves out-of-distribution
robustness, and yield gains on knowledge-intensive tasks through tailored
prompts.

</details>


### [342] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/pdf/2505.23091)
*Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang*

Main category: cs.AI

TL;DR: The paper introduces Infi-MMR, a framework to enhance reasoning in Multimodal Small Language Models (MSLMs) through a three-phase curriculum, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address challenges in extending reasoning capabilities from LLMs to MSLMs, including dataset scarcity, reasoning degradation, and flawed reinforcement learning.

Method: Three-phase curriculum: Foundational Reasoning Activation, Cross-Modal Reasoning Adaptation, and Multimodal Reasoning Enhancement.

Result: Infi-MMR-3B achieves top scores on multimodal math and general reasoning benchmarks.

Conclusion: Infi-MMR effectively unlocks reasoning potential in MSLMs, offering a scalable solution for multimodal reasoning.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini). Resources are available at
https://huggingface.co/Reallm-Labs/Infi-MMR-3B.

</details>


### [343] [E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness](https://arxiv.org/pdf/2505.24226)
*Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, Xiang Li*

Main category: cs.AI

TL;DR: E^2GraphRAG improves efficiency and effectiveness in graph-based RAG by using a summary tree, entity graph, and adaptive retrieval, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG methods like GraphRAG are inefficient and rely on manual query modes, limiting practicality.

Method: E^2GraphRAG constructs a summary tree with LLMs and an entity graph with SpaCy, then uses bidirectional indexes and adaptive retrieval.

Result: E^2GraphRAG achieves 10x faster indexing than GraphRAG and 100x speedup over LightRAG, with competitive QA performance.

Conclusion: E^2GraphRAG offers a more efficient and effective solution for graph-based RAG, addressing limitations of prior methods.

Abstract: Graph-based RAG methods like GraphRAG have shown promising global
understanding of the knowledge base by constructing hierarchical entity graphs.
However, they often suffer from inefficiency and rely on manually pre-defined
query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a
streamlined graph-based RAG framework that improves both Efficiency and
Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree
with large language models and an entity graph with SpaCy based on document
chunks. We then construct bidirectional indexes between entities and chunks to
capture their many-to-many relationships, enabling fast lookup during both
local and global retrieval. For the retrieval stage, we design an adaptive
retrieval strategy that leverages the graph structure to retrieve and select
between local and global modes. Experiments show that E^2GraphRAG achieves up
to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG
in retrieval while maintaining competitive QA performance.

</details>


### [344] [The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process](https://arxiv.org/pdf/2506.01080)
*Florian Carichon, Aditi Khandelwal, Marylou Fauchard, Golnoosh Farnadi*

Main category: cs.AI

TL;DR: AI alignment in Multi-Agent Systems (MAS) is dynamic and socially dependent, requiring coordination among agents. Misalignment with human values can arise, urging the AI community to treat alignment as interdependent and develop evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: The growing use of MAS in real-world applications introduces new dynamics where agents interact socially, potentially misaligning with human values.

Method: Analyzes social structure's impact on alignment using insights from social sciences.

Result: Highlights the need for treating alignment as interdependent and calls for tools to assess alignment in MAS.

Conclusion: Urges development of simulation environments and benchmarks to evaluate alignment in MAS before complexity becomes uncontrollable.

Abstract: This position paper states that AI Alignment in Multi-Agent Systems (MAS)
should be considered a dynamic and interaction-dependent process that heavily
depends on the social environment where agents are deployed, either
collaborative, cooperative, or competitive. While AI alignment with human
values and preferences remains a core challenge, the growing prevalence of MAS
in real-world applications introduces a new dynamic that reshapes how agents
pursue goals and interact to accomplish various tasks. As agents engage with
one another, they must coordinate to accomplish both individual and collective
goals. However, this complex social organization may unintentionally misalign
some or all of these agents with human values or user preferences. Drawing on
social sciences, we analyze how social structure can deter or shatter group and
individual values. Based on these analyses, we call on the AI community to
treat human, preferential, and objective alignment as an interdependent
concept, rather than isolated problems. Finally, we emphasize the urgent need
for simulation environments, benchmarks, and evaluation frameworks that allow
researchers to assess alignment in these interactive multi-agent contexts
before such dynamics grow too complex to control.

</details>


### [345] [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/pdf/2506.02761)
*Renyang Liu, Wenjie Feng, Tianwei Zhang, Wei Zhou, Xueqi Cheng, See-Kiong Ng*

Main category: cs.AI

TL;DR: The paper addresses gaps in image generation model unlearning (IGMU) by introducing CatIGMU for task categorization, EvalIGMU for evaluation, and DataIGM for benchmarking. It highlights flaws in existing methods and provides tools for better understanding and implementation.


<details>
  <summary>Details</summary>
Motivation: The rise of image generation models raises privacy and safety concerns. Current IGMU lacks clear guidelines, evaluation frameworks, and reliable metrics, hindering progress.

Method: The authors assess existing unlearning algorithms, propose CatIGMU for task categorization, EvalIGMU for evaluation, and create DataIGM for benchmarking.

Result: Existing IGMU algorithms perform poorly, especially in preservation and robustness. The proposed tools (CatIGMU, EvalIGMU, DataIGM) address these gaps.

Conclusion: The study advances IGMU by providing standardized tools and revealing shortcomings in current methods, paving the way for better unlearning solutions.

Abstract: With the surge and widespread application of image generation models, data
privacy and content safety have become major concerns and attracted great
attention from users, service providers, and policymakers. Machine unlearning
(MU) is recognized as a cost-effective and promising means to address these
challenges. Despite some advancements, image generation model unlearning (IGMU)
still faces remarkable gaps in practice, e.g., unclear task discrimination and
unlearning guidelines, lack of an effective evaluation framework, and
unreliable evaluation metrics. These can hinder the understanding of unlearning
mechanisms and the design of practical unlearning algorithms. We perform
exhaustive assessments over existing state-of-the-art unlearning algorithms and
evaluation standards, and discover several critical flaws and challenges in
IGMU tasks. Driven by these limitations, we make several core contributions, to
facilitate the comprehensive understanding, standardized categorization, and
reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel
hierarchical task categorization framework. It provides detailed implementation
guidance for IGMU, assisting in the design of unlearning algorithms and the
construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation
framework. It includes reliable quantitative metrics across five critical
aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can
be used for extensive evaluations of IGMU, training content detectors for
judgment, and benchmarking the state-of-the-art unlearning algorithms. With
EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot
handle the unlearning well across different evaluation dimensions, especially
for preservation and robustness. Code and models are available at
https://github.com/ryliu68/IGMU.

</details>


### [346] [Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning](https://arxiv.org/pdf/2506.05256)
*Violet Xiang, Chase Blagden, Rafael Rafailov, Nathan Lile, Sang Truong, Chelsea Finn, Nick Haber*

Main category: cs.AI

TL;DR: ALP (Adaptive Length Penalty) reduces token usage by 50% in large reasoning models by dynamically adjusting penalties based on prompt difficulty, improving efficiency without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for controlling token usage in large reasoning models are either manual, uniform, or require curated data, lacking adaptability to per-prompt difficulty.

Method: ALP uses reinforcement learning to tailor generation length by penalizing extra tokens inversely with a prompt's solve rate, encouraging brevity on easy prompts and allowing more tokens for hard ones.

Result: Post-training with ALP reduces average token usage by 50% while maintaining performance, and intelligently redistributes tokens to improve accuracy on hard problems.

Conclusion: ALP offers a data-efficient, adaptive solution for optimizing token usage in large reasoning models, balancing efficiency and performance.

Abstract: Large reasoning models (LRMs) achieve higher performance on challenging
reasoning tasks by generating more tokens at inference time, but this verbosity
often wastes computation on easy problems. Existing solutions, including
supervised finetuning on shorter traces, user-controlled budgets, or RL with
uniform penalties, either require data curation, manual configuration, or treat
all problems alike regardless of difficulty. We introduce Adaptive Length
Penalty (ALP), a reinforcement learning objective tailoring generation length
to per-prompt solve rate. During training, ALP monitors each prompt's online
solve rate through multiple rollouts and adds a differentiable penalty whose
magnitude scales inversely with that rate, so confident (easy) prompts incur a
high cost for extra tokens while hard prompts remain unhindered. Posttraining
DeepScaleR-1.5B with ALP cuts average token usage by 50\% without significantly
dropping performance. Relative to fixed-budget and uniform penalty baselines,
ALP redistributes its reduced budget more intelligently by cutting compute on
easy prompts and reallocating saved tokens to difficult ones, delivering higher
accuracy on the hardest problems with higher cost.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [347] [Improving Neural Diarization through Speaker Attribute Attractors and Local Dependency Modeling](https://arxiv.org/pdf/2506.05593)
*David Palzer, Matthew Maciejewski, Eric Fosler-Lussier*

Main category: cs.SD

TL;DR: The paper extends the Encoder-Decoder Attractors (EDA) approach for speaker diarization by focusing on detailed speaker attributes and using conformers for better local dependency modeling, showing improved performance on the CALLHOME dataset.


<details>
  <summary>Details</summary>
Motivation: To improve speaker diarization by moving beyond direct speaker modeling to detailed speaker attributes and enhancing architecture with conformers.

Method: Extends EDA with multi-stage intermediate representations for speaker attributes and replaces transformers with conformers.

Result: Improved diarization performance on the CALLHOME dataset.

Conclusion: The proposed enhancements to EDA, focusing on speaker attributes and conformers, yield better speaker diarization results.

Abstract: In recent years, end-to-end approaches have made notable progress in
addressing the challenge of speaker diarization, which involves segmenting and
identifying speakers in multi-talker recordings. One such approach,
Encoder-Decoder Attractors (EDA), has been proposed to handle variable speaker
counts as well as better guide the network during training. In this study, we
extend the attractor paradigm by moving beyond direct speaker modeling and
instead focus on representing more detailed `speaker attributes' through a
multi-stage process of intermediate representations. Additionally, we enhance
the architecture by replacing transformers with conformers, a
convolution-augmented transformer, to model local dependencies. Experiments
demonstrate improved diarization performance on the CALLHOME dataset.

</details>


### [348] [Voice Impression Control in Zero-Shot TTS](https://arxiv.org/pdf/2506.05688)
*Keinichi Fujita, Shota Horiguchi, Yusuke Ijima*

Main category: cs.SD

TL;DR: A method for controlling voice impressions in zero-shot TTS using low-dimensional vectors and LLMs, eliminating manual optimization.


<details>
  <summary>Details</summary>
Motivation: Modulating para-/non-linguistic speech information to control voice impressions is challenging in zero-shot TTS.

Method: Uses low-dimensional vectors to represent voice impression pairs (e.g., dark-bright) and leverages LLMs for vector generation.

Result: Effective impression control demonstrated via objective and subjective evaluations.

Conclusion: The method enables natural language-based impression control without manual optimization.

Abstract: Para-/non-linguistic information in speech is pivotal in shaping the
listeners' impression. Although zero-shot text-to-speech (TTS) has achieved
high speaker fidelity, modulating subtle para-/non-linguistic information to
control perceived voice characteristics, i.e., impressions, remains
challenging. We have therefore developed a voice impression control method in
zero-shot TTS that utilizes a low-dimensional vector to represent the
intensities of various voice impression pairs (e.g., dark-bright). The results
of both objective and subjective evaluations have demonstrated our method's
effectiveness in impression control. Furthermore, generating this vector via a
large language model enables target-impression generation from a natural
language description of the desired impression, thus eliminating the need for
manual optimization.

</details>


### [349] [WAKE: Watermarking Audio with Key Enrichment](https://arxiv.org/pdf/2506.05891)
*Yaoxun Xu, Jianwei Yu, Hangting Chen, Zhiyong Wu, Xixin Wu, Dong Yu, Rongzhi Gu, Yi Luo*

Main category: cs.SD

TL;DR: WAKE is a key-controllable audio watermarking framework that enhances security, resolves overwriting issues, and supports variable-length watermarks, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in audio security and copyright protection, particularly unauthorized access, watermark overwriting, and variable-length embedding.

Method: Proposes WAKE, a framework using specific keys for embedding and recovering watermarks, ensuring security and handling multiple embeddings.

Result: Outperforms existing models in audio quality and detection accuracy, with capabilities for multiple embeddings and variable-length watermarks.

Conclusion: WAKE effectively addresses key issues in audio watermarking, offering improved security and functionality.

Abstract: As deep learning advances in audio generation, challenges in audio security
and copyright protection highlight the need for robust audio watermarking.
Recent neural network-based methods have made progress but still face three
main issues: preventing unauthorized access, decoding initial watermarks after
multiple embeddings, and embedding varying lengths of watermarks. To address
these issues, we propose WAKE, the first key-controllable audio watermark
framework. WAKE embeds watermarks using specific keys and recovers them with
corresponding keys, enhancing security by making incorrect key decoding
impossible. It also resolves the overwriting issue by allowing watermark
decoding after multiple embeddings and supports variable-length watermark
insertion. WAKE outperforms existing models in both watermarked audio quality
and watermark detection accuracy. Code, more results, and demo page:
https://thuhcsi.github.io/WAKE.

</details>


### [350] [WhisQ: Cross-Modal Representation Learning for Text-to-Music MOS Prediction](https://arxiv.org/pdf/2506.05899)
*Jakaria Islam Emon, Kazi Tamanna Alam, Md. Abu Salek*

Main category: cs.SD

TL;DR: WhisQ is a multimodal architecture for MOS prediction in text-to-music systems, improving Spearman correlation by 7% for overall musical quality and 14% for text alignment.


<details>
  <summary>Details</summary>
Motivation: To address the dual challenge of evaluating both musical quality and text prompt alignment in text-to-music systems.

Method: Uses Whisper Base for audio encoding and Qwen 3 for text encoding, with sequence-level co-attention and optimal transport regularization.

Result: Achieves 7% and 14% improvements in Spearman correlation for OMQ and TA, respectively, on MusicEval Track-1.

Conclusion: Optimal transport regularization is crucial for cross-modal alignment, providing a 10% performance gain.

Abstract: Mean Opinion Score (MOS) prediction for text to music systems requires
evaluating both overall musical quality and text prompt alignment. This paper
introduces WhisQ, a multimodal architecture that addresses this dual-assessment
challenge through sequence level co-attention and optimal transport
regularization. WhisQ employs the Whisper Base pretrained model for temporal
audio encoding and Qwen 3, a 0.6B Small Language Model (SLM), for text
encoding, with both maintaining sequence structure for fine grained cross-modal
modeling. The architecture features specialized prediction pathways: OMQ is
predicted from pooled audio embeddings, while TA leverages bidirectional
sequence co-attention between audio and text. Sinkhorn optimal transport loss
further enforce semantic alignment in the shared embedding space. On the
MusicEval Track-1 dataset, WhisQ achieves substantial improvements over the
baseline: 7% improvement in Spearman correlation for OMQ and 14% for TA.
Ablation studies reveal that optimal transport regularization provides the
largest performance gain (10% SRCC improvement), demonstrating the importance
of explicit cross-modal alignment for text-to-music evaluation.

</details>


### [351] [Label-Context-Dependent Internal Language Model Estimation for CTC](https://arxiv.org/pdf/2506.06096)
*Zijian Yang, Minh-Nghia Phan, Ralf Schlüter, Hermann Ney*

Main category: cs.SD

TL;DR: CTC implicitly learns a context-dependent internal language model (ILM) despite its label context independence assumption. The paper proposes novel ILM estimation methods using knowledge distillation and regularization, showing improved performance in cross-domain evaluation.


<details>
  <summary>Details</summary>
Motivation: To investigate and leverage the implicit context dependency in CTC's ILM, which contradicts its label context independence assumption.

Method: Proposes context-dependent ILM estimation via knowledge distillation (KD) with theoretical justifications and introduces two regularization methods for KD.

Result: Context-dependent ILMs outperform context-independent priors in cross-domain evaluation, with label-level KD and smoothing achieving over 13% relative improvement in word error rate.

Conclusion: CTC learns a context-dependent ILM, and the proposed KD-based methods effectively estimate and enhance it, demonstrating significant performance gains.

Abstract: Although connectionist temporal classification (CTC) has the label context
independence assumption, it can still implicitly learn a context-dependent
internal language model (ILM) due to modern powerful encoders. In this work, we
investigate the implicit context dependency modeled in the ILM of CTC. To this
end, we propose novel context-dependent ILM estimation methods for CTC based on
knowledge distillation (KD) with theoretical justifications. Furthermore, we
introduce two regularization methods for KD. We conduct experiments on
Librispeech and TED-LIUM Release 2 datasets for in-domain and cross-domain
evaluation, respectively. Experimental results show that context-dependent ILMs
outperform the context-independent priors in cross-domain evaluation,
indicating that CTC learns a context-dependent ILM. The proposed label-level KD
with smoothing method surpasses other ILM estimation approaches, with more than
13% relative improvement in word error rate compared to shallow fusion.

</details>


### [352] [NAT: Neural Acoustic Transfer for Interactive Scenes in Real Time](https://arxiv.org/pdf/2506.06190)
*Xutong Jin, Bo Pang, Chenxi Xu, Xinyun Hou, Guoping Wang, Sheng Li*

Main category: cs.SD

TL;DR: Neural Acoustic Transfer uses implicit neural representation for real-time sound field prediction in dynamic scenes, overcoming limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional acoustic transfer methods struggle with dynamic changes in scenes, leading to inefficiencies and inaccuracies in sound rendering.

Method: Utilizes implicit neural representation and fast Monte-Carlo-based BEM for training data, with GPU-accelerated BEM for precision.

Result: Achieves numerical accuracy and runtime efficiency (milliseconds for 30s audio) in diverse scenarios.

Conclusion: Enables efficient, accurate sound modeling in dynamic environments, benefiting VR, AR, and audio production.

Abstract: Previous acoustic transfer methods rely on extensive precomputation and
storage of data to enable real-time interaction and auditory feedback. However,
these methods struggle with complex scenes, especially when dynamic changes in
object position, material, and size significantly alter sound effects. These
continuous variations lead to fluctuating acoustic transfer distributions,
making it challenging to represent with basic data structures and render
efficiently in real time. To address this challenge, we present Neural Acoustic
Transfer, a novel approach that utilizes an implicit neural representation to
encode precomputed acoustic transfer and its variations, allowing for real-time
prediction of sound fields under varying conditions. To efficiently generate
the training data required for the neural acoustic field, we developed a fast
Monte-Carlo-based boundary element method (BEM) approximation for general
scenarios with smooth Neumann conditions. Additionally, we implemented a
GPU-accelerated version of standard BEM for scenarios requiring higher
precision. These methods provide the necessary training data, enabling our
neural network to accurately model the sound radiation space. We demonstrate
our method's numerical accuracy and runtime efficiency (within several
milliseconds for 30s audio) through comprehensive validation and comparisons in
diverse acoustic transfer scenarios. Our approach allows for efficient and
accurate modeling of sound behavior in dynamically changing environments, which
can benefit a wide range of interactive applications such as virtual reality,
augmented reality, and advanced audio production.

</details>


### [353] [Efficient Fine-Grained Guidance for Diffusion Model Based Symbolic Music Generation](https://arxiv.org/pdf/2410.08435)
*Tingyu Zhu, Haoyu Liu, Ziyu Wang, Zhimin Jiang, Zeyu Zheng*

Main category: cs.SD

TL;DR: A Fine-Grained Guidance (FGG) approach in diffusion models improves symbolic music generation by aligning outputs with expert composer intent, enhancing accuracy and quality.


<details>
  <summary>Details</summary>
Motivation: Limited data and high precision requirements in symbolic music generation pose challenges, addressed by FGG for better control and quality.

Method: Introduces FGG within diffusion models to guide music generation, supported by theoretical analysis and numerical experiments.

Result: FGG improves accuracy, listenability, and enables advanced applications like improvisation and interactive music creation.

Conclusion: FGG effectively addresses symbolic music generation challenges, with demonstrated success in experiments and real-time applications.

Abstract: Developing generative models to create or conditionally create symbolic music
presents unique challenges due to the combination of limited data availability
and the need for high precision in note pitch. To address these challenges, we
introduce an efficient Fine-Grained Guidance (FGG) approach within diffusion
models. FGG guides the diffusion models to generate music that aligns more
closely with the control and intent of expert composers, which is critical to
improve the accuracy, listenability, and quality of generated music. This
approach empowers diffusion models to excel in advanced applications such as
improvisation, and interactive music creation. We derive theoretical
characterizations for both the challenges in symbolic music generation and the
effects of the FGG approach. We provide numerical experiments and subjective
evaluation to demonstrate the effectiveness of our approach. We have published
a demo page to showcase performances, which enables real-time interactive
generation.

</details>


### [354] [Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Audio-Language Models](https://arxiv.org/pdf/2411.14842)
*Wanqi Yang, Yanda Li, Meng Fang, Yunchao Wei, Ling Chen*

Main category: cs.SD

TL;DR: The paper introduces the Chat-Audio Attacks (CAA) benchmark to test vulnerabilities of large audio-language models (LALMs) to adversarial audio attacks, evaluating six models with three methods, finding GPT-4o most resilient.


<details>
  <summary>Details</summary>
Motivation: Adversarial audio attacks threaten LALMs in voice interactions; existing methods lack generalizability, prompting a need for universal attack benchmarks.

Method: Proposes CAA benchmark with four attack types and three evaluation strategies: Standard, GPT-4o-Based, and Human Evaluation.

Result: GPT-4o shows highest resilience among six tested LALMs, with comprehensive analysis of attack impacts.

Conclusion: CAA benchmark effectively assesses LALM vulnerabilities, highlighting GPT-4o's robustness and providing open data for further research.

Abstract: Adversarial audio attacks pose a significant threat to the growing use of
large audio-language models (LALMs) in voice-based human-machine interactions.
While existing research focused on model-specific adversarial methods,
real-world applications demand a more generalizable and universal approach to
audio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks
(CAA) benchmark including four distinct types of audio attacks, which aims to
explore the vulnerabilities of LALMs to these audio attacks in conversational
scenarios. To evaluate the robustness of LALMs, we propose three evaluation
strategies: Standard Evaluation, utilizing traditional metrics to quantify
model performance under attacks; GPT-4o-Based Evaluation, which simulates
real-world conversational complexities; and Human Evaluation, offering insights
into user perception and trust. We evaluate six state-of-the-art LALMs with
voice interaction capabilities, including Gemini-1.5-Pro, GPT-4o, and others,
using three distinct evaluation methods on the CAA benchmark. Our comprehensive
analysis reveals the impact of four types of audio attacks on the performance
of these models, demonstrating that GPT-4o exhibits the highest level of
resilience. Our data can be accessed via the following link:
\href{https://github.com/crystraldo/CAA}{CAA}.

</details>


### [355] [DualSpec: Text-to-spatial-audio Generation via Dual-Spectrogram Guided Diffusion Model](https://arxiv.org/pdf/2502.18952)
*Lei Zhao, Sizhou Chen, Linfeng Feng, Jichao Zhang, Xiao-Lei Zhang, Chi Zhang, Xuelong Li*

Main category: cs.SD

TL;DR: The paper introduces DualSpec, a text-to-spatial-audio (TTSA) framework that generates immersive spatial audio from text descriptions using VAEs, a large language model, and a diffusion model.


<details>
  <summary>Details</summary>
Motivation: Current text-to-audio (TTA) systems focus on monaural audio, lacking the immersive experience of spatial audio, which is crucial for applications like virtual reality.

Method: The method involves training VAEs for latent acoustic representations, using a pretrained language model for text features, and training a diffusion model for spatial audio generation. DualSpec leverages Mel spectrograms for quality and STFT spectrograms for azimuth accuracy.

Result: Experimental results show the method generates spatial audio with high directional and event consistency.

Conclusion: DualSpec successfully addresses the gap in TTA systems by enabling high-quality spatial audio generation from text, validated by new spatial-aware metrics.

Abstract: Text-to-audio (TTA), which generates audio signals from textual descriptions,
has received huge attention in recent years. However, recent works focused on
text to monaural audio only. As we know, spatial audio provides more immersive
auditory experience than monaural audio, e.g. in virtual reality. To address
this issue, we propose a text-to-spatial-audio (TTSA) generation framework
named DualSpec. Specifically, it first trains variational autoencoders (VAEs)
for extracting the latent acoustic representations from sound event audio.
Then, given text that describes sound events and event directions, the proposed
method uses the encoder of a pretrained large language model to transform the
text into text features. Finally, it trains a diffusion model from the latent
acoustic representations and text features for the spatial audio generation. In
the inference stage, only the text description is needed to generate spatial
audio. Particularly, to improve the synthesis quality and azimuth accuracy of
the spatial sound events simultaneously, we propose to use two kinds of
acoustic features. One is the Mel spectrograms which is good for improving the
synthesis quality, and the other is the short-time Fourier transform
spectrograms which is good at improving the azimuth accuracy. We provide a
pipeline of constructing spatial audio dataset with text prompts, for the
training of the VAEs and diffusion model. We also introduce new spatial-aware
evaluation metrics to quantify the azimuth errors of the generated spatial
audio recordings. Experimental results demonstrate that the proposed method can
generate spatial audio with high directional and event consistency.

</details>


### [356] [Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment](https://arxiv.org/pdf/2505.04113)
*Xueyao Zhang, Yuancheng Wang, Chaoren Wang, Ziniu Li, Zhuo Chen, Zhizheng Wu*

Main category: cs.SD

TL;DR: The paper introduces the Intelligibility Preference Speech Dataset (INTP) and extends the Direct Preference Optimization (DPO) framework to improve zero-shot TTS systems, addressing challenges like tongue twisters and cross-lingual synthesis.


<details>
  <summary>Details</summary>
Motivation: Zero-shot TTS systems struggle with intelligibility in challenging scenarios, prompting the need for targeted data and alignment techniques.

Method: Leverages preference alignment with INTP dataset and extends DPO for diverse TTS architectures.

Result: Improvements in intelligibility, naturalness, similarity, and audio quality across models, with weak-to-strong generalization demonstrated.

Conclusion: INTP alignment enhances TTS performance and shows potential for iterative improvements, as showcased with models like CosyVoice 2 and Ints.

Abstract: Modern zero-shot text-to-speech (TTS) systems, despite using extensive
pre-training, often struggle in challenging scenarios such as tongue twisters,
repeated words, code-switching, and cross-lingual synthesis, leading to
intelligibility issues. To address these limitations, this paper leverages
preference alignment techniques, which enable targeted construction of
out-of-pretraining-distribution data to enhance performance. We introduce a new
dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend
the Direct Preference Optimization (DPO) framework to accommodate diverse TTS
architectures. After INTP alignment, in addition to intelligibility, we observe
overall improvements including naturalness, similarity, and audio quality for
multiple TTS models across diverse domains. Based on that, we also verify the
weak-to-strong generalization ability of INTP for more intelligible models such
as CosyVoice 2 and Ints. Moreover, we showcase the potential for further
improvements through iterative alignment based on Ints. Audio samples are
available at https://intalign.github.io/.

</details>


### [357] [Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival Estimation](https://arxiv.org/pdf/2505.19493)
*Fei Zhao, Xueliang Zhang, Zhong-Qiu Wang*

Main category: cs.SD

TL;DR: A two-stage multi-channel AEC algorithm using sound source directional cues outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Improve multi-channel AEC performance by leveraging spatial cues and directional information.

Method: Lightweight DNN predicts sound source directions; AEC network uses these directions, multi-channel signals, and far-end signal to estimate near-end signal.

Result: Outperforms baseline approaches and generalizes well across diverse environments.

Conclusion: Incorporating directional cues enhances multi-channel AEC performance effectively.

Abstract: Acoustic echo cancellation (AEC) is an important speech signal processing
technology that can remove echoes from microphone signals to enable
natural-sounding full-duplex speech communication. While single-channel AEC is
widely adopted, multi-channel AEC can leverage spatial cues afforded by
multiple microphones to achieve better performance. Existing multi-channel AEC
approaches typically combine beamforming with deep neural networks (DNN). This
work proposes a two-stage algorithm that enhances multi-channel AEC by
incorporating sound source directional cues. Specifically, a lightweight DNN is
first trained to predict the sound source directions, and then the predicted
directional information, multi-channel microphone signals, and single-channel
far-end signal are jointly fed into an AEC network to estimate the near-end
signal. Evaluation results show that the proposed algorithm outperforms
baseline approaches and exhibits robust generalization across diverse acoustic
environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [358] [Mixture-of-Experts Meets In-Context Reinforcement Learning](https://arxiv.org/pdf/2506.05426)
*Wenhao Wu, Fuhong Liu, Haoru Li, Zican Hu, Daoyi Dong, Chunlin Chen, Zhi Wang*

Main category: cs.LG

TL;DR: T2MIR introduces token- and task-wise MoE layers in transformer-based RL models to address multi-modality and task diversity, enhancing in-context learning performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in ICRL include multi-modality of state-action-reward data and diverse decision tasks, which T2MIR aims to solve.

Method: T2MIR replaces feedforward layers with token-wise and task-wise MoE layers, using contrastive learning for task-wise routing.

Result: T2MIR outperforms baselines, improving in-context learning capacity.

Conclusion: T2MIR advances ICRL with a scalable MoE-based architecture, bridging gaps with language and vision achievements.

Abstract: In-context reinforcement learning (ICRL) has emerged as a promising paradigm
for adapting RL agents to downstream tasks through prompt conditioning.
However, two notable challenges remain in fully harnessing in-context learning
within RL domains: the intrinsic multi-modality of the state-action-reward data
and the diverse, heterogeneous nature of decision tasks. To tackle these
challenges, we propose \textbf{T2MIR} (\textbf{T}oken- and \textbf{T}ask-wise
\textbf{M}oE for \textbf{I}n-context \textbf{R}L), an innovative framework that
introduces architectural advances of mixture-of-experts (MoE) into
transformer-based decision models. T2MIR substitutes the feedforward layer with
two parallel layers: a token-wise MoE that captures distinct semantics of input
tokens across multiple modalities, and a task-wise MoE that routes diverse
tasks to specialized experts for managing a broad task distribution with
alleviated gradient conflicts. To enhance task-wise routing, we introduce a
contrastive learning method that maximizes the mutual information between the
task and its router representation, enabling more precise capture of
task-relevant information. The outputs of two MoE components are concatenated
and fed into the next layer. Comprehensive experiments show that T2MIR
significantly facilitates in-context learning capacity and outperforms various
types of baselines. We bring the potential and promise of MoE to ICRL, offering
a simple and scalable architectural enhancement to advance ICRL one step closer
toward achievements in language and vision communities. Our code is available
at https://github.com/NJU-RL/T2MIR.

</details>


### [359] [MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction](https://arxiv.org/pdf/2506.05427)
*Zishan Shu, Yufan Deng, Hongyu Zhang, Zhiwei Nie, Jie Chen*

Main category: cs.LG

TL;DR: MTPNet introduces a multi-grained target perception network for activity cliff prediction, outperforming existing methods by 18.95% RMSE improvement.


<details>
  <summary>Details</summary>
Motivation: Existing methods for activity cliff prediction are limited to single binding targets, restricting their applicability. MTPNet aims to incorporate molecule-protein interaction knowledge for better predictions.

Method: MTPNet uses Macro-level Target Semantic (MTS) and Micro-level Pocket Semantic (MPS) guidance to dynamically optimize molecular representations through protein semantic conditions.

Result: MTPNet achieves an 18.95% average RMSE improvement on 30 activity cliff datasets, outperforming previous methods.

Conclusion: MTPNet effectively captures interaction details and accelerates compound optimization, setting a new benchmark for activity cliff prediction.

Abstract: Activity cliff prediction is a critical task in drug discovery and material
design. Existing computational methods are limited to handling single binding
targets, which restricts the applicability of these prediction models. In this
paper, we present the Multi-Grained Target Perception network (MTPNet) to
incorporate the prior knowledge of interactions between the molecules and their
target proteins. Specifically, MTPNet is a unified framework for activity cliff
prediction, which consists of two components: Macro-level Target Semantic (MTS)
guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet
dynamically optimizes molecular representations through multi-grained protein
semantic conditions. To our knowledge, it is the first time to employ the
receptor proteins as guiding information to effectively capture critical
interaction details. Extensive experiments on 30 representative activity cliff
datasets demonstrate that MTPNet significantly outperforms previous approaches,
achieving an average RMSE improvement of 18.95% on top of several mainstream
GNN architectures. Overall, MTPNet internalizes interaction patterns through
conditional deep learning to achieve unified predictions of activity cliffs,
helping to accelerate compound optimization and design. Codes are available at:
https://github.com/ZishanShu/MTPNet.

</details>


### [360] [Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data](https://arxiv.org/pdf/2506.05542)
*Vlastimil Martinek, Andrea Gariboldi, Dimosthenis Tzimotoudis, Aitor Alberdi Escudero, Edward Blake, David Cechak, Luke Cassar, Alessandro Balestrucci, Panagiotis Alexiou*

Main category: cs.LG

TL;DR: Agentomics-ML is an autonomous agent-based system for ML experimentation in computational biology, outperforming existing methods in generalization and success rates.


<details>
  <summary>Details</summary>
Motivation: The need for automated methods to handle heterogeneous biological datasets and improve generalization in ML models for molecular medicine.

Method: Agentomics-ML follows predefined ML experimentation steps, uses Bash for file interactions, and employs reflection for iterative improvements.

Result: Outperforms state-of-the-art agent-based methods and achieves competitive performance on benchmark datasets.

Conclusion: Agentomics-ML advances autonomous ML systems in computational biology, narrowing the gap with expert-built models.

Abstract: The adoption of machine learning (ML) and deep learning methods has
revolutionized molecular medicine by driving breakthroughs in genomics,
transcriptomics, drug discovery, and biological systems modeling. The
increasing quantity, multimodality, and heterogeneity of biological datasets
demand automated methods that can produce generalizable predictive models.
Recent developments in large language model-based agents have shown promise for
automating end-to-end ML experimentation on structured benchmarks. However,
when applied to heterogeneous computational biology datasets, these methods
struggle with generalization and success rates. Here, we introduce
Agentomics-ML, a fully autonomous agent-based system designed to produce a
classification model and the necessary files for reproducible training and
inference. Our method follows predefined steps of an ML experimentation
process, repeatedly interacting with the file system through Bash to complete
individual steps. Once an ML model is produced, training and validation metrics
provide scalar feedback to a reflection step to identify issues such as
overfitting. This step then creates verbal feedback for future iterations,
suggesting adjustments to steps such as data representation, model
architecture, and hyperparameter choices. We have evaluated Agentomics-ML on
several established genomic and transcriptomic benchmark datasets and show that
it outperforms existing state-of-the-art agent-based methods in both
generalization and success rates. While state-of-the-art models built by domain
experts still lead in absolute performance on the majority of the computational
biology datasets used in this work, Agentomics-ML narrows the gap for fully
autonomous systems and achieves state-of-the-art performance on one of the used
benchmark datasets. The code is available at
https://github.com/BioGeMT/Agentomics-ML.

</details>


### [361] [Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction](https://arxiv.org/pdf/2506.05428)
*Zhihao Tang, Chaozhuo Li, Litian Zhang, Xi Zhang*

Main category: cs.LG

TL;DR: MCI-Diff is a diffusion-based framework for early prediction of MCI conversion, combining real-time risk assessment with high accuracy by synthesizing future sMRI representations from baseline data.


<details>
  <summary>Details</summary>
Motivation: The trade-off between immediacy (fast predictions from baseline sMRI) and accuracy (leveraging longitudinal scans) in MCI prediction motivates the development of MCI-Diff.

Method: MCI-Diff uses a multi-task sequence reconstruction strategy and an LLM-driven "linguistic compass" for clinically plausible sampling, synthesizing future sMRI representations.

Result: MCI-Diff outperforms state-of-the-art baselines, improving early conversion accuracy by 5-12% in ADNI and AIBL cohorts.

Conclusion: MCI-Diff successfully balances immediacy and accuracy in MCI prediction, offering a promising tool for early risk assessment.

Abstract: Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by
a trade-off between immediacy--making fast predictions from a single baseline
sMRI--and accuracy--leveraging longitudinal scans to capture disease
progression. We propose MCI-Diff, a diffusion-based framework that synthesizes
clinically plausible future sMRI representations directly from baseline data,
achieving both real-time risk assessment and high predictive performance.
First, a multi-task sequence reconstruction strategy trains a shared denoising
network on interpolation and extrapolation tasks to handle irregular follow-up
sampling and learn robust latent trajectories. Second, an LLM-driven
"linguistic compass" is introduced for clinical plausibility sampling:
generated feature candidates are quantized, tokenized, and scored by a
fine-tuned language model conditioned on expected structural biomarkers,
guiding autoregressive generation toward realistic disease patterns.
Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms
state-of-the-art baselines, improving early conversion accuracy by 5-12%.

</details>


### [362] [Collaborative Learning in Agentic Systems: A Collective AI is Greater Than the Sum of Its Parts](https://arxiv.org/pdf/2506.05577)
*Saptarshi Nath, Christos Peridis, Eseoghene Benjamin, Xinran Liu, Soheil Kolouri, Peter Kinnell, Zexin Li, Cong Liu, Shirin Dora, Andrea Soltoggio*

Main category: cs.LG

TL;DR: MOSAIC is an agentic AI algorithm enabling decentralized, collaborative learning among agents without centralized control, improving efficiency and task-solving capabilities.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in decentralized agentic systems, such as limited bandwidth and lack of coordination, to enhance scalability and collaborative learning.

Method: Combines modular policy composition, cosine similarity estimation, and asynchronous communication for knowledge sharing and reuse.

Result: Outperforms isolated learners in sample efficiency, solves harder tasks, and enables emergent task curricula.

Conclusion: Collaborative learning in agentic systems enhances performance and scalability, supporting continuous evolution at individual and collective levels.

Abstract: Agentic AI has gained significant interest as a research paradigm focused on
autonomy, self-directed learning, and long-term reliability of decision making.
Real-world agentic systems operate in decentralized settings on a large set of
tasks or data distributions with constraints such as limited bandwidth,
asynchronous execution, and the absence of a centralized model or even common
objectives. We posit that exploiting previously learned skills, task
similarities, and communication capabilities in a collective of agentic AI are
challenging but essential elements to enabling scalability, open-endedness, and
beneficial collaborative learning dynamics. In this paper, we introduce Modular
Sharing and Composition in Collective Learning (MOSAIC), an agentic algorithm
that allows multiple agents to independently solve different tasks while also
identifying, sharing, and reusing useful machine-learned knowledge, without
coordination, synchronization, or centralized control. MOSAIC combines three
mechanisms: (1) modular policy composition via neural network masks, (2) cosine
similarity estimation using Wasserstein embeddings for knowledge selection, and
(3) asynchronous communication and policy integration. Results on a set of RL
benchmarks show that MOSAIC has a greater sample efficiency than isolated
learners, i.e., it learns significantly faster, and in some cases, finds
solutions to tasks that cannot be solved by isolated learners. The
collaborative learning and sharing dynamics are also observed to result in the
emergence of ideal curricula of tasks, from easy to hard. These findings
support the case for collaborative learning in agentic systems to achieve
better and continuously evolving performance both at the individual and
collective levels.

</details>


### [363] [PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling](https://arxiv.org/pdf/2506.05432)
*Yuxuan Yue, Zukang Xu, Zhihang Yuan, Dawei Yang, Jianglong Wu, Liqiang Nie*

Main category: cs.LG

TL;DR: PCDVQ decouples vector direction and magnitude for quantization, improving accuracy in 2-bit LLM compression.


<details>
  <summary>Details</summary>
Motivation: Address the sensitivity gap between direction and magnitude in vector quantization for LLMs.

Method: Proposes PCDVQ with Polar Coordinate Decoupling and Distribution Aligned Codebook Construction.

Result: Outperforms baselines by at least 1.5% in zero-shot accuracy at 2-bit level.

Conclusion: PCDVQ offers a novel, accurate approach for highly compressed LLMs.

Abstract: Large Language Models (LLMs) face significant challenges in edge deployment
due to their massive parameter scale. Vector Quantization (VQ), a
clustering-based quantization method, serves as a prevalent solution to this
issue for its extremely low-bit (even at 2-bit) and considerable accuracy.
Since a vector is a quantity in mathematics and physics that has both direction
and magnitude, existing VQ works typically quantize them in a coupled manner.
However, we find that direction exhibits significantly greater sensitivity to
quantization compared to the magnitude. For instance, when separately
clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the
accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap
even increases with the reduction of clustering centers. Further, Euclidean
distance, a common metric to access vector similarities in current VQ works,
places greater emphasis on reducing the magnitude error. This property is
contrary to the above finding, unavoidably leading to larger quantization
errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector
Quantization (PCDVQ), an effective and efficient VQ framework consisting of two
key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors
into their polar coordinate representations and perform independent
quantization of the direction and magnitude parameters.2) Distribution Aligned
Codebook Construction (DACC), which optimizes the direction and magnitude
codebooks in accordance with the source distribution. Experimental results show
that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\%
zero-shot accuracy, establishing a novel paradigm for accurate and highly
compressed LLMs.

</details>


### [364] [Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward](https://arxiv.org/pdf/2506.05433)
*Zikang Liu, Tongtian Yue, Yepeng Tang, Longteng Guo, Junxian Cai, Qingbin Liu, Xi Chen, Jing Liu*

Main category: cs.LG

TL;DR: Prefix Grouper optimizes GRPO by eliminating redundant prefix computations, maintaining performance while reducing costs.


<details>
  <summary>Details</summary>
Motivation: GRPO's computational inefficiency with long shared prefixes limits scalability.

Method: Prefix Grouper uses a Shared-Prefix Forward strategy to encode prefixes once, restructuring self-attention.

Result: Empirical and theoretical evidence shows Prefix Grouper matches GRPO's performance with lower computational cost.

Conclusion: Prefix Grouper is a scalable, plug-and-play solution for GRPO, enabling larger group sizes and tasks.

Abstract: Group Relative Policy Optimization (GRPO) enhances policy learning by
computing gradients from relative comparisons among candidate outputs that
share a common input prefix. Despite its effectiveness, GRPO introduces
substantial computational overhead when processing long shared prefixes, which
must be redundantly encoded for each group member. This inefficiency becomes a
major scalability bottleneck in long-context learning scenarios. We propose
Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant
prefix computation via a Shared-Prefix Forward strategy. In particular, by
restructuring self-attention into two parts, our method enables the shared
prefix to be encoded only once, while preserving full differentiability and
compatibility with end-to-end training. We provide both theoretical and
empirical evidence that Prefix Grouper is training-equivalent to standard GRPO:
it yields identical forward outputs and backward gradients, ensuring that the
optimization dynamics and final policy performance remain unchanged.
Empirically, our experiments confirm that Prefix Grouper achieves consistent
results while significantly reducing the computational cost of training,
particularly in long-prefix scenarios. The proposed method is fully
plug-and-play: it is compatible with existing GRPO-based architectures and can
be seamlessly integrated into current training pipelines as a drop-in
replacement, requiring no structural modifications and only minimal changes to
input construction and attention computation. Prefix Grouper enables the use of
larger group sizes under the same computational budget, thereby improving the
scalability of GRPO to more complex tasks and larger models. Code is now
available at https://github.com/johncaged/PrefixGrouper

</details>


### [365] [Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks](https://arxiv.org/pdf/2506.05434)
*Thomas Massena, Léo andéol, Thibaut Boissin, Franck Mamalet, Corentin Friedrich, Mathieu Serrurier, Sébastien Gerchinovitz*

Main category: cs.LG

TL;DR: The paper introduces lip-rcp, a method combining Lipschitz-bounded networks with robust conformal prediction to efficiently compute precise prediction sets under adversarial attacks, outperforming existing methods in scalability and set size.


<details>
  <summary>Details</summary>
Motivation: Classical conformal prediction lacks robustness under adversarial attacks, and existing robust methods are impractical for large-scale problems due to computational demands or overly large prediction sets.

Method: The proposed lip-rcp method leverages Lipschitz-bounded networks to estimate robust CP sets efficiently, especially when paired with 1-Lipschitz robust networks. It also analyzes vanilla CP under attack, deriving worst-case coverage bounds.

Result: lip-rcp outperforms state-of-the-art methods in robust CP set size and computational efficiency, particularly in medium and large-scale scenarios like ImageNet.

Conclusion: The lip-rcp method bridges the gap between robustness and efficiency in conformal prediction, making it viable for real-world adversarial settings while maintaining theoretical guarantees.

Abstract: Conformal Prediction (CP) has proven to be an effective post-hoc method for
improving the trustworthiness of neural networks by providing prediction sets
with finite-sample guarantees. However, under adversarial attacks, classical
conformal guarantees do not hold anymore: this problem is addressed in the
field of Robust Conformal Prediction. Several methods have been proposed to
provide robust CP sets with guarantees under adversarial perturbations, but,
for large scale problems, these sets are either too large or the methods are
too computationally demanding to be deployed in real life scenarios. In this
work, we propose a new method that leverages Lipschitz-bounded networks to
precisely and efficiently estimate robust CP sets. When combined with a
1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms
state-of-the-art results in both the size of the robust CP sets and
computational efficiency in medium and large-scale scenarios such as ImageNet.
Taking a different angle, we also study vanilla CP under attack, and derive new
worst-case coverage bounds of vanilla CP sets, which are valid simultaneously
for all adversarial attack levels. Our lip-rcp method makes this second
approach as efficient as vanilla CP while also allowing robustness guarantees.

</details>


### [366] [AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML](https://arxiv.org/pdf/2410.02958)
*Patara Trirat, Wonyong Jeong, Sung Ju Hwang*

Main category: cs.LG

TL;DR: AutoML-Agent is a multi-agent framework for full-pipeline AutoML, leveraging LLMs for efficient task automation and improved usability.


<details>
  <summary>Details</summary>
Motivation: Existing AutoML systems require technical expertise and are limited to specific pipeline stages, lacking efficiency and usability.

Method: Proposes AutoML-Agent with retrieval-augmented planning, specialized LLM agents for parallel sub-tasks, and multi-stage verification.

Result: Achieves higher success rates in automating full AutoML pipelines across diverse domains.

Conclusion: AutoML-Agent enhances AutoML usability and efficiency, delivering deployment-ready models effectively.

Abstract: Automated machine learning (AutoML) accelerates AI development by automating
tasks in the development pipeline, such as optimal model search and
hyperparameter tuning. Existing AutoML systems often require technical
expertise to set up complex tools, which is in general time-consuming and
requires a large amount of human effort. Therefore, recent works have started
exploiting large language models (LLM) to lessen such burden and increase the
usability of AutoML frameworks via a natural language interface, allowing
non-expert users to build their data-driven solutions. These methods, however,
are usually designed only for a particular process in the AI development
pipeline and do not efficiently use the inherent capacity of the LLMs. This
paper proposes AutoML-Agent, a novel multi-agent framework tailored for
full-pipeline AutoML, i.e., from data retrieval to model deployment.
AutoML-Agent takes user's task descriptions, facilitates collaboration between
specialized LLM agents, and delivers deployment-ready models. Unlike existing
work, instead of devising a single plan, we introduce a retrieval-augmented
planning strategy to enhance exploration to search for more optimal plans. We
also decompose each plan into sub-tasks (e.g., data preprocessing and neural
network design) each of which is solved by a specialized agent we build via
prompting executing in parallel, making the search process more efficient.
Moreover, we propose a multi-stage verification to verify executed results and
guide the code generation LLM in implementing successful solutions. Extensive
experiments on seven downstream tasks using fourteen datasets show that
AutoML-Agent achieves a higher success rate in automating the full AutoML
process, yielding systems with good performance throughout the diverse domains.

</details>


### [367] [SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful Deepfake Content on Social Media Platforms](https://arxiv.org/pdf/2506.05538)
*Arnesh Batra, Anushk Kumar, Jashn Khemani, Arush Gumber, Arhan Jain, Somil Gupta*

Main category: cs.LG

TL;DR: SocialDF dataset and LLM-based multi-factor detection method address deepfake challenges on social media by combining facial recognition, speech transcription, and multi-agent LLM verification.


<details>
  <summary>Details</summary>
Motivation: Deepfake technology poses security risks through misinformation, requiring improved detection methods for synthetic media.

Method: Proposes SocialDF dataset and a multi-factor detection approach using facial recognition, speech transcription, and multi-agent LLM verification.

Result: A robust, multi-modal framework for distinguishing deepfakes from authentic content.

Conclusion: The approach enhances deepfake detection by leveraging diverse verification techniques for real-world social media challenges.

Abstract: The rapid advancement of deep generative models has significantly improved
the realism of synthetic media, presenting both opportunities and security
challenges. While deepfake technology has valuable applications in
entertainment and accessibility, it has emerged as a potent vector for
misinformation campaigns, particularly on social media. Existing detection
frameworks struggle to distinguish between benign and adversarially generated
deepfakes engineered to manipulate public perception. To address this
challenge, we introduce SocialDF, a curated dataset reflecting real-world
deepfake challenges on social media platforms. This dataset encompasses
high-fidelity deepfakes sourced from various online ecosystems, ensuring broad
coverage of manipulative techniques. We propose a novel LLM-based multi-factor
detection approach that combines facial recognition, automated speech
transcription, and a multi-agent LLM pipeline to cross-verify audio-visual
cues. Our methodology emphasizes robust, multi-modal verification techniques
that incorporate linguistic, behavioral, and contextual analysis to effectively
discern synthetic media from authentic content.

</details>


### [368] [Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning](https://arxiv.org/pdf/2506.05435)
*Manon Renault, Hamoud Younes, Hugo Tessier, Ronan Le Roy, Bastien Pasdeloup, Mathieu Léonardon*

Main category: cs.LG

TL;DR: The paper proposes an embedded system for monitoring reusable packages using a deep learning model on IoT devices, achieving high precision and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Package monitoring is crucial for operational efficiency and sustainability. The study aims to design a long-lasting system for tracking package states.

Method: Uses a 1D CNN for classifying accelerometer data, with data augmentation (SMOTE and ADASYN) for imbalance and model compression for size reduction.

Result: Achieves 94.54% and 95.83% precision for two classes, reduces model size by 4x, and operates at 316 mW power consumption.

Conclusion: The system is effective for long-term package monitoring with high accuracy and low power usage.

Abstract: Package monitoring is an important topic in industrial applications, with
significant implications for operational efficiency and ecological
sustainability. In this study, we propose an approach that employs an embedded
system, placed on reusable packages, to detect their state (on a Forklift, in a
Truck, or in an undetermined location). We aim to design a system with a
lifespan of several years, corresponding to the lifespan of reusable packages.
Our analysis demonstrates that maximizing device lifespan requires minimizing
wake time. We propose a pipeline that includes data processing, training, and
evaluation of the deep learning model designed for imbalanced, multiclass time
series data collected from an embedded sensor. The method uses a
one-dimensional Convolutional Neural Network architecture to classify
accelerometer data from the IoT device. Before training, two data augmentation
techniques are tested to solve the imbalance problem of the dataset: the
Synthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling
approach. After training, compression techniques are implemented to have a
small model size. On the considered twoclass problem, the methodology yields a
precision of 94.54% for the first class and 95.83% for the second class, while
compression techniques reduce the model size by a factor of four. The trained
model is deployed on the IoT device, where it operates with a power consumption
of 316 mW during inference.

</details>


### [369] [Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR](https://arxiv.org/pdf/2506.05683)
*Fardis Nadimi, Payam Abdisarabshali, Kasra Borazjani, Jacob Chakareski, Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: The paper proposes using multi-modal multi-task federated foundation models (FedFMs) to enhance XR systems, addressing challenges under the SHIFT dimensions while ensuring privacy and context-awareness.


<details>
  <summary>Details</summary>
Motivation: To integrate the representational strength of foundation models with privacy-preserving federated learning for transformative XR systems.

Method: A modular architecture for FedFMs with coordination paradigms for training and aggregation, addressing XR challenges under SHIFT dimensions.

Result: Illustrates SHIFT dimensions in XR applications and proposes evaluation metrics, dataset requirements, and design tradeoffs.

Conclusion: Aims to lay foundations for privacy-preserving, context-aware intelligence in next-gen XR systems.

Abstract: Extended reality (XR) systems, which consist of virtual reality (VR),
augmented reality (AR), and mixed reality (XR), offer a transformative
interface for immersive, multi-modal, and embodied human-computer interaction.
In this paper, we envision that multi-modal multi-task (M3T) federated
foundation models (FedFMs) can offer transformative capabilities for XR systems
through integrating the representational strength of M3T foundation models
(FMs) with the privacy-preserving model training principles of federated
learning (FL). We present a modular architecture for FedFMs, which entails
different coordination paradigms for model training and aggregations. Central
to our vision is the codification of XR challenges that affect the
implementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality
diversity, (2) Hardware heterogeneity and system-level constraints, (3)
Interactivity and embodied personalization, (4) Functional/task variability,
and (5) Temporality and environmental variability. We illustrate the
manifestation of these dimensions across a set of emerging and anticipated
applications of XR systems. Finally, we propose evaluation metrics, dataset
requirements, and design tradeoffs necessary for the development of
resource-aware FedFMs in XR. This perspective aims to chart the technical and
conceptual foundations for context-aware privacy-preserving intelligence in the
next generation of XR systems.

</details>


### [370] [An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics](https://arxiv.org/pdf/2506.05438)
*Tongda Sun, Chen Yin, Huailiang Zheng, Yining Dong*

Main category: cs.LG

TL;DR: A novel unsupervised method constructs a dynamic health indicator (HI) for rolling bearings by capturing temporal dependencies and degradation features without expert knowledge, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing HI methods rely on expert knowledge and miss dynamic degradation information, limiting their effectiveness for prognostics.

Method: Uses a skip-connection-based autoencoder for feature extraction and an HI-generating module with temporal modeling to construct dynamic HI.

Result: Outperforms comparison methods on bearing datasets, showing superior degradation trend representation and prognostics.

Conclusion: The dynamic HI effectively models degradation tendencies and improves prognostic accuracy.

Abstract: Health indicator (HI) plays a key role in degradation assessment and
prognostics of rolling bearings. Although various HI construction methods have
been investigated, most of them rely on expert knowledge for feature extraction
and overlook capturing dynamic information hidden in sequential degradation
processes, which limits the ability of the constructed HI for degradation trend
representation and prognostics. To address these concerns, a novel dynamic HI
that considers HI-level temporal dependence is constructed through an
unsupervised framework. Specifically, a degradation feature learning module
composed of a skip-connection-based autoencoder first maps raw signals to a
representative degradation feature space (DFS) to automatically extract
essential degradation features without the need for expert knowledge.
Subsequently, in this DFS, a new HI-generating module embedded with an inner
HI-prediction block is proposed for dynamic HI construction, where the temporal
dependence between past and current HI states is guaranteed and modeled
explicitly. On this basis, the dynamic HI captures the inherent dynamic
contents of the degradation process, ensuring its effectiveness for degradation
tendency modeling and future degradation prognostics. The experiment results on
two bearing lifecycle datasets demonstrate that the proposed HI construction
method outperforms comparison methods, and the constructed dynamic HI is
superior for prognostic tasks.

</details>


### [371] [UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss](https://arxiv.org/pdf/2506.05443)
*Yiyu Lin, Yan Wang, You Zhou, Xinye Ni, Jiahui Wu, Sen Yang*

Main category: cs.LG

TL;DR: UniPTMs is a unified framework for multi-type PTM prediction, featuring a 'Master-Slave' dual-path architecture, advanced fusion modules, and a novel loss function, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models lack cross-modal feature fusion, domain generalization, and architectural optimization for PTM prediction.

Method: Proposes UniPTMs with a 'Master-Slave' dual-path architecture, BGCA, LDFN, MACP, BHGFN, and HDWF mechanisms, plus a Hierarchical Contrastive loss.

Result: Achieves 3.2%-11.4% MCC and 4.2%-14.3% AP improvements over state-of-the-art models across five PTM types.

Conclusion: UniPTMs advances PTM prediction with superior performance and introduces a lightweight variant, UniPTMs-mini.

Abstract: As a core mechanism of epigenetic regulation in eukaryotes, protein
post-translational modifications (PTMs) require precise prediction to decipher
dynamic life activity networks. To address the limitations of existing deep
learning models in cross-modal feature fusion, domain generalization, and
architectural optimization, this study proposes UniPTMs: the first unified
framework for multi-type PTM prediction. The framework innovatively establishes
a "Master-Slave" dual-path collaborative architecture: The master path
dynamically integrates high-dimensional representations of protein sequences,
structures, and evolutionary information through a Bidirectional Gated
Cross-Attention (BGCA) module, while the slave path optimizes feature
discrepancies and recalibration between structural and traditional features
using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale
Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and
a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level
feature integration across paths, the framework employs a Hierarchical Dynamic
Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal
features. Enhanced by a novel Hierarchical Contrastive loss function for
feature consistency optimization, UniPTMs demonstrates significant performance
improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art
models across five modification types and transcends the Single-Type Prediction
Paradigm. To strike a balance between model complexity and performance, we have
also developed a lightweight variant named UniPTMs-mini.

</details>


### [372] [Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic](https://arxiv.org/pdf/2506.05445)
*Thanh Vinh Vo, Young Lee, Haozhe Ma, Chien Lu, Tze-Yun Leong*

Main category: cs.LG

TL;DR: DoSAC improves RL policy learning by correcting hidden confounders using causal intervention, outperforming baselines in robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Hidden confounders bias RL policy learning, leading to suboptimal behavior. Most RL algorithms ignore this, relying on statistical associations rather than causal effects.

Method: DoSAC extends SAC with causal intervention via backdoor adjustment. It uses a Backdoor Reconstructor to infer pseudo-past variables for estimating the interventional policy.

Result: Empirical results show DoSAC outperforms baselines in confounded settings, with better robustness, generalization, and policy reliability.

Conclusion: DoSAC effectively addresses hidden confounding in RL, offering a principled solution for improved policy learning.

Abstract: Hidden confounders that influence both states and actions can bias policy
learning in reinforcement learning (RL), leading to suboptimal or
non-generalizable behavior. Most RL algorithms ignore this issue, learning
policies from observational trajectories based solely on statistical
associations rather than causal effects. We propose DoSAC (Do-Calculus Soft
Actor-Critic with Backdoor Adjustment), a principled extension of the SAC
algorithm that corrects for hidden confounding via causal intervention
estimation. DoSAC estimates the interventional policy $\pi(a | \mathrm{do}(s))$
using the backdoor criterion, without requiring access to true confounders or
causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor
that infers pseudo-past variables (previous state and action) from the current
state to enable backdoor adjustment from observational data. This module is
integrated into a soft actor-critic framework to compute both the
interventional policy and its entropy. Empirical results on continuous control
benchmarks show that DoSAC outperforms baselines under confounded settings,
with improved robustness, generalization, and policy reliability.

</details>


### [373] [Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning](https://arxiv.org/pdf/2506.05447)
*Andrei Mircea, Supriyo Chakraborty, Nima Chitsazan, Irina Rish, Ekaterina Lobacheva*

Main category: cs.LG

TL;DR: Scaling mitigates loss deceleration in language models by reducing its occurrence and improving post-deceleration loss rates, attributed to zero-sum learning dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how scaling improves language models, particularly in training dynamics, focusing on loss deceleration.

Method: Analyze loss curves in log-log space, identify zero-sum learning (ZSL) as a cause of deceleration, and study scaling effects.

Result: Scaling decreases loss at deceleration and improves post-deceleration loss rates, with ZSL causing destructive gradient interference.

Conclusion: Loss deceleration and ZSL offer insights into scaling laws and potential improvements for language models beyond scaling.

Abstract: This work aims to understand how scaling improves language models,
specifically in terms of training dynamics. We find that language models
undergo loss deceleration early in training; an abrupt slowdown in the rate of
loss improvement, resulting in piecewise linear behaviour of the loss curve in
log-log space. Scaling up the model mitigates this transition by (1) decreasing
the loss at which deceleration occurs, and (2) improving the log-log rate of
loss improvement after deceleration. We attribute loss deceleration to a type
of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL,
per-example gradients become systematically opposed, leading to destructive
interference in per-example changes in loss. As a result, improving loss on one
subset of examples degrades it on another, bottlenecking overall progress. Loss
deceleration and ZSL provide new insights into the training dynamics underlying
language model scaling laws, and could potentially be targeted directly to
improve language models independent of scale. We make our code and artefacts
available at: https://github.com/mirandrom/zsl

</details>


### [374] [Zeroth-Order Optimization Finds Flat Minima](https://arxiv.org/pdf/2506.05454)
*Liang Zhang, Bingcong Li, Kiran Koshy Thekumparampil, Sewoong Oh, Michael Muehlebach, Niao He*

Main category: cs.LG

TL;DR: Zeroth-order methods favor solutions with small trace of Hessian, indicating flat minima, and provide convergence rates for such solutions in convex and smooth functions.


<details>
  <summary>Details</summary>
Motivation: To understand the implicit regularization in zeroth-order optimization, particularly which solutions are favored, given its use in gradient-infeasible scenarios like black-box attacks and reinforcement learning.

Method: Analyzes zeroth-order optimization with the standard two-point estimator, focusing on solutions with small trace of Hessian (flat minima). Provides convergence rates for convex and smooth functions.

Result: Zeroth-order optimization favors flat minima (small trace of Hessian). Theoretical convergence rates are derived, and experiments on binary classification and language model fine-tuning support the findings.

Conclusion: Zeroth-order methods implicitly regularize toward flat minima, with practical implications for applications like black-box attacks and model fine-tuning.

Abstract: Zeroth-order methods are extensively used in machine learning applications
where gradients are infeasible or expensive to compute, such as black-box
attacks, reinforcement learning, and language model fine-tuning. Existing
optimization theory focuses on convergence to an arbitrary stationary point,
but less is known on the implicit regularization that provides a fine-grained
characterization on which particular solutions are finally reached. We show
that zeroth-order optimization with the standard two-point estimator favors
solutions with small trace of Hessian, which is widely used in previous work to
distinguish between sharp and flat minima. We further provide convergence rates
of zeroth-order optimization to approximate flat minima for convex and
sufficiently smooth functions, where flat minima are defined as the minimizers
that achieve the smallest trace of Hessian among all optimal solutions.
Experiments on binary classification tasks with convex losses and language
model fine-tuning support our theoretical findings.

</details>


### [375] [Can Masked Autoencoders Also Listen to Birds?](https://arxiv.org/pdf/2504.12880)
*Lukas Rauch, René Heinrich, Ilyass Moummad, Alexis Joly, Bernhard Sick, Christoph Scholz*

Main category: cs.LG

TL;DR: Bird-MAE improves bird-sound classification by adapting the training pipeline and introducing prototypical probing, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: General-purpose Audio-MAE models struggle with fine-grained bird-sound classification due to subtle inter-species differences and high intra-species variability.

Method: Adapts pretraining, fine-tuning, and frozen feature utilization using BirdSet. Introduces prototypical probing for parameter-efficient frozen feature use.

Result: Bird-MAE sets new benchmarks in multi-label classification and few-shot learning, outperforming linear probing by up to 37% in MAP.

Conclusion: Tailored self-supervised pipelines like Bird-MAE are crucial for fine-grained audio domains, demonstrating robust performance and efficiency.

Abstract: Masked Autoencoders (MAEs) have shown competitive results in audio
classification by learning rich semantic representations through an efficient
self-supervised reconstruction task. However, general-purpose models fail to
generalize well when applied directly to fine-grained audio domains.
Specifically, bird-sound classification requires distinguishing subtle
inter-species differences and managing high intra-species acoustic variability,
thereby revealing the performance limitations of general-domain Audio-MAE
models. This work demonstrates that bridging this domain gap requires more than
domain-specific pretraining data; adapting the entire training pipeline is
crucial. We systematically revisit and adapt the pretraining recipe,
fine-tuning methods, and frozen feature utilization to bird sounds using
BirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our
resulting Bird-MAE achieves new state-of-the-art results in BirdSet's
multi-label classification benchmark. Additionally, we introduce the
parameter-efficient prototypical probing, enhancing the utility of frozen MAE
representations and closely approaching fine-tuning performance in low-resource
settings. Bird-MAE's prototypical probes outperform linear probing by up to
37%$_\text{p}$ in MAP and narrow the gap to fine-tuning to approximately
3.3%$_\text{p}$ on average across BirdSet downstream tasks. Bird-MAE also
demonstrates robust few-shot capabilities with prototypical probing in our
newly established few-shot benchmark on BirdSet, highlighting the potential of
tailored self-supervised learning pipelines for fine-grained audio domains.

</details>


### [376] [Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors](https://arxiv.org/pdf/2506.05479)
*Matei Gabriel Coşa, Marek Eliáš*

Main category: cs.LG

TL;DR: The paper addresses the challenge of selecting the best heuristic for Metrical Task Systems (MTS) online, where only one heuristic can be queried per step and costs are memory-dependent. It achieves $O(\text{OPT}^{2/3})$ regret and proves a matching lower bound.


<details>
  <summary>Details</summary>
Motivation: The problem arises from the need to dynamically choose the best heuristic for MTS without full knowledge of costs, inspired by Bandit Learning against memory-bounded adversaries.

Method: The approach involves querying one heuristic per time step and handling the challenge of cost estimation due to memory constraints.

Result: The paper achieves $O(\text{OPT}^{2/3})$ regret and establishes a tight lower bound, matching prior constructions.

Conclusion: The work provides a solution for heuristic selection in MTS with memory-dependent costs, achieving near-optimal regret bounds.

Abstract: We consider the following problem: We are given $\ell$ heuristics for
Metrical Task Systems (MTS), where each might be tailored to a different type
of input instances. While processing an input instance received online, we are
allowed to query the action of only one of the heuristics at each time step.
Our goal is to achieve performance comparable to the best of the given
heuristics. The main difficulty of our setting comes from the fact that the
cost paid by a heuristic at time $t$ cannot be estimated unless the same
heuristic was also queried at time $t-1$. This is related to Bandit Learning
against memory bounded adversaries (Arora et al., 2012). We show how to achieve
regret of $O(\text{OPT}^{2/3})$ and prove a tight lower bound based on the
construction of Dekel et al. (2013).

</details>


### [377] [Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?](https://arxiv.org/pdf/2506.05484)
*Ruihua Chen, Bangyu Wu, Meng Li, Kai Yang*

Main category: cs.LG

TL;DR: Neural network reparameterized FWI uses pretraining or denormalization to incorporate initial models. Pretraining complicates workflows and reduces plasticity, while denormalization simplifies and improves results.


<details>
  <summary>Details</summary>
Motivation: To compare the effectiveness of pretraining and denormalization in neural network reparameterized FWI for subsurface property inversion.

Method: Two approaches: pretraining (fitting initial model) and denormalization (adding network outputs to initial model).

Result: Denormalization outperforms pretraining in workflow simplicity, convergence speed, and inversion accuracy.

Conclusion: Denormalization is preferred for neural network reparameterized FWI due to its efficiency and better performance.

Abstract: Subsurface property neural network reparameterized full waveform inversion
(FWI) has emerged as an effective unsupervised learning framework, which can
invert stably with an inaccurate starting model. It updates the trainable
neural network parameters instead of fine-tuning on the subsurface model
directly. There are primarily two ways to embed the prior knowledge of the
initial model into neural networks, that is, pretraining and denormalization.
Pretraining first regulates the neural networks' parameters by fitting the
initial velocity model; Denormalization directly adds the outputs of the
network into the initial models without pretraining. In this letter, we
systematically investigate the influence of the two ways of initial model
incorporation for the neural network reparameterized FWI. We demonstrate that
pretraining requires inverting the model perturbation based on a constant
velocity value (mean) with a two-stage implementation. It leads to a complex
workflow and inconsistency of objective functions in the two-stage process,
causing the network parameters to become inactive and lose plasticity.
Experimental results demonstrate that denormalization can simplify workflows,
accelerate convergence, and enhance inversion accuracy compared with
pretraining.

</details>


### [378] [Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models](https://arxiv.org/pdf/2506.05497)
*Sima Noorani, Shayan Kiyani, George Pappas, Hamed Hassani*

Main category: cs.LG

TL;DR: CPQ introduces a conformal prediction framework for black-box generative models, optimizing coverage, query budget, and informativeness using principles from the missing mass problem.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification is critical for safe deployment of generative AI, especially in high-stakes applications, but classical methods assume structured outputs.

Method: CPQ uses a query-only setting, deriving optimal query policies and mappings from the missing mass problem, with novel estimators for decay rates and missing mass.

Result: CPQ outperforms existing methods, providing more informative prediction sets for language models, validated on real-world tasks and LLMs.

Conclusion: CPQ is a scalable, effective framework for uncertainty quantification in black-box generative models, enhancing safety and reliability.

Abstract: Uncertainty quantification (UQ) is essential for safe deployment of
generative AI models such as large language models (LLMs), especially in high
stakes applications. Conformal prediction (CP) offers a principled uncertainty
quantification framework, but classical methods focus on regression and
classification, relying on geometric distances or softmax scores: tools that
presuppose structured outputs. We depart from this paradigm by studying CP in a
query only setting, where prediction sets must be constructed solely from
finite queries to a black box generative model, introducing a new trade off
between coverage, test time query budget, and informativeness. We introduce
Conformal Prediction with Query Oracle (CPQ), a framework characterizing the
optimal interplay between these objectives. Our finite sample algorithm is
built on two core principles: one governs the optimal query policy, and the
other defines the optimal mapping from queried samples to prediction sets.
Remarkably, both are rooted in the classical missing mass problem in
statistics. Specifically, the optimal query policy depends on the rate of
decay, or the derivative, of the missing mass, for which we develop a novel
estimator. Meanwhile, the optimal mapping hinges on the missing mass itself,
which we estimate using Good Turing estimators. We then turn our focus to
implementing our method for language models, where outputs are vast, variable,
and often under specified. Fine grained experiments on three real world open
ended tasks and two LLMs, show CPQ applicability to any black box LLM and
highlight: (1) individual contribution of each principle to CPQ performance,
and (2) CPQ ability to yield significantly more informative prediction sets
than existing conformal methods for language uncertainty quantification.

</details>


### [379] [The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models](https://arxiv.org/pdf/2506.05500)
*Alex Damian, Jason D. Lee, Joan Bruna*

Main category: cs.LG

TL;DR: The paper studies efficient agnostic estimation for hidden subspaces in Gaussian Multi-index models, introducing the generative leap exponent $k^\star$ and proving its necessity and sufficiency for sample complexity.


<details>
  <summary>Details</summary>
Motivation: To understand and efficiently estimate hidden subspaces in Gaussian Multi-index models, extending prior work on generative exponents.

Method: Introduces the generative leap exponent $k^\star$, analyzes sample complexity via the Low-Degree-Polynomial framework, and proposes a spectral U-statistic-based sequential estimation procedure.

Result: Shows a sample complexity of $n=\Theta(d^{1 \vee k/2})$ is necessary and sufficient, with examples for piecewise linear functions and deep neural networks.

Conclusion: The generative leap exponent $k^\star$ effectively captures sample complexity in multi-index models, enabling efficient agnostic estimation.

Abstract: In this work we consider generic Gaussian Multi-index models, in which the
labels only depend on the (Gaussian) $d$-dimensional inputs through their
projection onto a low-dimensional $r = O_d(1)$ subspace, and we study efficient
agnostic estimation procedures for this hidden subspace. We introduce the
\emph{generative leap} exponent $k^\star$, a natural extension of the
generative exponent from [Damian et al.'24] to the multi-index setting. We
first show that a sample complexity of $n=\Theta(d^{1 \vee \k/2})$ is necessary
in the class of algorithms captured by the Low-Degree-Polynomial framework. We
then establish that this sample complexity is also sufficient, by giving an
agnostic sequential estimation procedure (that is, requiring no prior knowledge
of the multi-index model) based on a spectral U-statistic over appropriate
Hermite tensors. We further compute the generative leap exponent for several
examples including piecewise linear functions (deep ReLU networks with bias),
and general deep neural networks (with $r$-dimensional first hidden layer).

</details>


### [380] [Geometric and Physical Constraints Synergistically Enhance Neural PDE Surrogates](https://arxiv.org/pdf/2506.05513)
*Yunfei Huang, David S. Greenberg*

Main category: cs.LG

TL;DR: Neural PDE surrogates with physical and symmetry constraints on staggered grids improve accuracy and generalization, outperforming baselines in tasks like shallow water equations and turbulence.


<details>
  <summary>Details</summary>
Motivation: Addressing the poor generalization and error accumulation of neural PDE surrogates by incorporating physical laws and symmetries on staggered grids.

Method: Introducing novel input/output layers for staggered grids to enforce constraints, tested on shallow water equations and decaying turbulence.

Result: Constraints improve accuracy across tasks, architectures, and prediction steps. Symmetries outperform physical constraints alone, but combining both yields the best results.

Conclusion: Doubly-constrained surrogates generalize better and predict real-world phenomena like ocean currents more accurately.

Abstract: Neural PDE surrogates can improve the cost-accuracy tradeoff of classical
solvers, but often generalize poorly to new initial conditions and accumulate
errors over time. Physical and symmetry constraints have shown promise in
closing this performance gap, but existing techniques for imposing these
inductive biases are incompatible with the staggered grids commonly used in
computational fluid dynamics. Here we introduce novel input and output layers
that respect physical laws and symmetries on the staggered grids, and for the
first time systematically investigate how these constraints, individually and
in combination, affect the accuracy of PDE surrogates. We focus on two
challenging problems: shallow water equations with closed boundaries and
decaying incompressible turbulence. Compared to strong baselines, symmetries
and physical constraints consistently improve performance across tasks,
architectures, autoregressive prediction steps, accuracy measures, and network
sizes. Symmetries are more effective than physical constraints, but surrogates
with both performed best, even compared to baselines with data augmentation or
pushforward training, while themselves benefiting from the pushforward trick.
Doubly-constrained surrogates also generalize better to initial conditions and
durations beyond the range of the training data, and more accurately predict
real-world ocean currents.

</details>


### [381] [Winner-takes-all for Multivariate Probabilistic Time Series Forecasting](https://arxiv.org/pdf/2506.05515)
*Adrien Cortés, Rémi Rehm, Victor Letzelter*

Main category: cs.LG

TL;DR: TimeMCL uses Multiple Choice Learning (MCL) to forecast diverse time series futures with a neural network and WTA loss, showing promising results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting multiple plausible futures in time-series forecasting, especially for ill-posed and ambiguous tasks.

Method: Adapts MCL with a neural network featuring multiple heads and WTA loss to encourage diverse predictions.

Result: Demonstrates promising performance on synthetic and real-world time series with low computational cost.

Conclusion: TimeMCL is an efficient method for diverse time-series forecasting, leveraging MCL's simplicity and effectiveness.

Abstract: We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL)
paradigm to forecast multiple plausible time series futures. Our approach
employs a neural network with multiple heads and utilizes the Winner-Takes-All
(WTA) loss to promote diversity among predictions. MCL has recently gained
attention due to its simplicity and ability to address ill-posed and ambiguous
tasks. We propose an adaptation of this framework for time-series forecasting,
presenting it as an efficient method to predict diverse futures, which we
relate to its implicit quantization objective. We provide insights into our
approach using synthetic data and evaluate it on real-world time series,
demonstrating its promising performance at a light computational cost.

</details>


### [382] [On Fitting Flow Models with Large Sinkhorn Couplings](https://arxiv.org/pdf/2506.05526)
*Michal Klein, Alireza Mousavi-Hosseini, Stephen Zhang, Marco Cuturi*

Main category: cs.LG

TL;DR: Flow models benefit from large Sinkhorn couplings with low entropic regularization, improving efficiency in training and inference.


<details>
  <summary>Details</summary>
Motivation: Training flow models without source-target pairings is challenging, and existing methods using small batches or suboptimal couplings are inefficient.

Method: Uses large batches (n ≈ 256, scaled up) and Sinkhorn couplings with low entropic regularization (ε) to optimize flow models.

Result: Improved efficiency in training and inference for flow models, demonstrated in synthetic and image generation tasks.

Conclusion: Large Sinkhorn couplings with low ε enhance flow model performance, offering practical benefits for data generation tasks.

Abstract: Flow models transform data gradually from one modality (e.g. noise) onto
another (e.g. images). Such models are parameterized by a time-dependent
velocity field, trained to fit segments connecting pairs of source and target
points. When the pairing between source and target points is given, training
flow models boils down to a supervised regression problem. When no such pairing
exists, as is the case when generating data from noise, training flows is much
harder. A popular approach lies in picking source and target points
independently. This can, however, lead to velocity fields that are slow to
train, but also costly to integrate at inference time. In theory, one would
greatly benefit from training flow models by sampling pairs from an optimal
transport (OT) measure coupling source and target, since this would lead to a
highly efficient flow solving the Benamou and Brenier dynamical OT problem. In
practice, recent works have proposed to sample mini-batches of $n$ source and
$n$ target points and reorder them using an OT solver to form better pairs.
These works have advocated using batches of size $n\approx 256$, and considered
OT solvers that return couplings that are either sharp (using e.g. the
Hungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.
Sinkhorn). We follow in the footsteps of these works by exploring the benefits
of increasing $n$ by three to four orders of magnitude, and look more carefully
on the effect of the entropic regularization $\varepsilon$ used in the Sinkhorn
algorithm. Our analysis is facilitated by new scale invariant quantities to
report the sharpness of a coupling, while our sharded computations across
multiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic
and image generation tasks, flow models greatly benefit when fitted with large
Sinkhorn couplings, with a low entropic regularization $\varepsilon$.

</details>


### [383] [Spectral Graph Neural Networks are Incomplete on Graphs with a Simple Spectrum](https://arxiv.org/pdf/2506.05530)
*Snir Hordan, Maya Bechler-Speicher, Gur Lifshitz, Nadav Dym*

Main category: cs.LG

TL;DR: SGNNs' expressive power is analyzed using graph spectra, revealing limitations and proposing a method to enhance expressivity.


<details>
  <summary>Details</summary>
Motivation: To better understand and improve the expressive power of Spectrally-enhanced GNNs (SGNNs) by aligning evaluation frameworks with graph spectra.

Method: Leveraging graph eigenvalue multiplicity to classify graphs and adapting rotation equivariant neural networks for spectral enhancement.

Result: Many SGNNs are incomplete even on graphs with distinct eigenvalues; a proposed method improves expressivity on simple spectrum graphs.

Conclusion: The study provides a new expressivity hierarchy for SGNNs and a method to enhance their performance, validated empirically.

Abstract: Spectral features are widely incorporated within Graph Neural Networks (GNNs)
to improve their expressive power, or their ability to distinguish among
non-isomorphic graphs. One popular example is the usage of graph Laplacian
eigenvectors for positional encoding in MPNNs and Graph Transformers. The
expressive power of such Spectrally-enhanced GNNs (SGNNs) is usually evaluated
via the k-WL graph isomorphism test hierarchy and homomorphism counting. Yet,
these frameworks align poorly with the graph spectra, yielding limited insight
into SGNNs' expressive power. We leverage a well-studied paradigm of
classifying graphs by their largest eigenvalue multiplicity to introduce an
expressivity hierarchy for SGNNs. We then prove that many SGNNs are incomplete
even on graphs with distinct eigenvalues. To mitigate this deficiency, we adapt
rotation equivariant neural networks to the graph spectra setting to propose a
method to provably improve SGNNs' expressivity on simple spectrum graphs. We
empirically verify our theoretical claims via an image classification
experiment on the MNIST Superpixel dataset and eigenvector canonicalization on
graphs from ZINC.

</details>


### [384] [Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning](https://arxiv.org/pdf/2506.05568)
*Arian Raje, Baris Askin, Divyansh Jhunjhunwala, Gauri Joshi*

Main category: cs.LG

TL;DR: Ravan, an adaptive multi-head LoRA method, improves federated fine-tuning of LLMs by balancing parameter efficiency and model expressivity, achieving 2-8% higher accuracy than prior methods.


<details>
  <summary>Details</summary>
Motivation: Leveraging edge-device data for LLMs via federated learning (FL) is challenging due to data and computational heterogeneity, leading to accuracy degradation in LoRA-based methods.

Method: Ravan reparameterizes weight updates as the sum of multiple LoRA heads, training only core matrices and scaling factors to optimize performance without increasing communication costs.

Result: Ravan outperforms prior parameter-efficient baselines by 2-8% in test accuracy on vision and language benchmarks.

Conclusion: Ravan is a robust and scalable solution for federated fine-tuning of LLMs, addressing heterogeneity issues while maintaining efficiency.

Abstract: Large language models (LLMs) have not yet effectively leveraged the vast
amounts of edge-device data, and federated learning (FL) offers a promising
paradigm to collaboratively fine-tune LLMs without transferring private edge
data to the cloud. To operate within the computation and communication
constraints of edge devices, recent literature on federated fine-tuning of LLMs
proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient
methods. However, LoRA-based methods suffer from accuracy degradation in FL
settings, primarily because of data and computational heterogeneity across
clients. We propose \textsc{Ravan}, an adaptive multi-head LoRA method that
balances parameter efficiency and model expressivity by reparameterizing the
weight updates as the sum of multiple LoRA heads
$s_i\textbf{B}_i\textbf{H}_i\textbf{A}_i$ in which only the core matrices
$\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These
trainable scaling factors let the optimization focus on the most useful heads,
recovering a higher-rank approximation of the full update without increasing
the number of communicated parameters since clients upload $s_i\textbf{H}_i$
directly. Experiments on vision and language benchmarks show that
\textsc{Ravan} improves test accuracy by 2-8\% over prior parameter-efficient
baselines, making it a robust and scalable solution for federated fine-tuning
of LLMs.

</details>


### [385] [When can in-context learning generalize out of task distribution?](https://arxiv.org/pdf/2506.05574)
*Chase Goddard, Lindsay M. Smith, Vudtiwat Ngampruetikorn, David J. Schwab*

Main category: cs.LG

TL;DR: Transformers trained on linear functions show a transition from specialized to generalized in-context learning (ICL) as task diversity increases, with similar behavior in nonlinear regression.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions in pretraining distributions that enable ICL to generalize out-of-distribution.

Method: Empirical investigation using transformers trained on linear functions, varying task diversity and pretraining tasks, and analyzing model depth and problem dimensionality.

Result: Increased task diversity leads to a transition from specialized to generalized ICL, with similar patterns in nonlinear regression. A phase diagram maps this interaction.

Conclusion: Task diversity and pretraining task count are key factors for ICL generalization, with model depth and problem dimensionality also influencing the transition.

Abstract: In-context learning (ICL) is a remarkable capability of pretrained
transformers that allows models to generalize to unseen tasks after seeing only
a few examples. We investigate empirically the conditions necessary on the
pretraining distribution for ICL to emerge and generalize
\emph{out-of-distribution}. Previous work has focused on the number of distinct
tasks necessary in the pretraining dataset. Here, we use a different notion of
task diversity to study the emergence of ICL in transformers trained on linear
functions. We find that as task diversity increases, transformers undergo a
transition from a specialized solution, which exhibits ICL only within the
pretraining task distribution, to a solution which generalizes out of
distribution to the entire task space. We also investigate the nature of the
solutions learned by the transformer on both sides of the transition, and
observe similar transitions in nonlinear regression problems. We construct a
phase diagram to characterize how our concept of task diversity interacts with
the number of pretraining tasks. In addition, we explore how factors such as
the depth of the model and the dimensionality of the regression problem
influence the transition.

</details>


### [386] [Conformal Prediction Adaptive to Unknown Subpopulation Shifts](https://arxiv.org/pdf/2506.05583)
*Nien-Shao Wang, Duygu Nur Yaldiz, Yavuz Faruk Bakman, Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: Proposes methods to adapt conformal prediction for subpopulation shifts, ensuring valid coverage without explicit subpopulation knowledge, validated on vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: Standard conformal prediction fails under distribution shifts, especially subpopulation shifts, where test data differs from calibration data.

Method: New algorithms to adapt conformal prediction for unknown subpopulation shifts, scalable to high-dimensional settings.

Result: Methods maintain coverage and control risk effectively in vision and language benchmarks, outperforming standard conformal prediction.

Conclusion: The proposed approach reliably handles subpopulation shifts, ensuring robust uncertainty quantification in practical ML tasks.

Abstract: Conformal prediction is widely used to equip black-box machine learning
models with uncertainty quantification enjoying formal coverage guarantees.
However, these guarantees typically break down in the presence of distribution
shifts, where the data distribution at test time differs from the training (or
calibration-time) distribution. In this work, we address subpopulation shifts,
where the test environment exhibits an unknown and differing mixture of
subpopulations compared to the calibration data. We propose new methods that
provably adapt conformal prediction to such shifts, ensuring valid coverage
without requiring explicit knowledge of subpopulation structure. Our algorithms
scale to high-dimensional settings and perform effectively in realistic machine
learning tasks. Extensive experiments on vision (with vision transformers) and
language (with large language models) benchmarks demonstrate that our methods
reliably maintain coverage and controls risk in scenarios where standard
conformal prediction fails.

</details>


### [387] [TabFlex: Scaling Tabular Learning to Millions with Linear Attention](https://arxiv.org/pdf/2506.05584)
*Yuchen Zeng, Tuan Dinh, Wonjun Kang, Andreas C Mueller*

Main category: cs.LG

TL;DR: TabFlex enhances TabPFN for large-scale tabular datasets using linear attention, achieving faster processing and better efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the scalability limitations of TabPFN for large and complex tabular datasets.

Method: Incorporates linear attention mechanisms to replace quadratic self-attention, improving efficiency and scalability.

Result: Achieves 2x speedup over TabPFN and 1.5x over XGBoost, handling millions of samples efficiently.

Conclusion: TabFlex is a scalable, efficient solution for large tabular datasets, outperforming baselines with reduced computational costs.

Abstract: Leveraging the in-context learning (ICL) capability of Large Language Models
(LLMs) for tabular classification has gained significant attention for its
training-free adaptability across diverse datasets. Recent advancements, like
TabPFN, excel in small-scale tabular datasets but struggle to scale for large
and complex datasets. Our work enhances the efficiency and scalability of
TabPFN for larger datasets by incorporating linear attention mechanisms as a
scalable alternative to complexity-quadratic self-attention. Our model,
TabFlex, efficiently handles tabular datasets with thousands of features and
hundreds of classes, scaling seamlessly to millions of samples. For instance,
TabFlex processes the poker-hand dataset with over a million samples in just 5
seconds. Our extensive evaluations demonstrate that TabFlex can achieve over a
2x speedup compared to TabPFN and a 1.5x speedup over XGBoost, outperforming 25
tested baselines in terms of efficiency across a diverse range of datasets.
Furthermore, TabFlex remains highly effective on large-scale datasets,
delivering strong performance with significantly reduced computational costs,
especially when combined with data-efficient techniques such as dimensionality
reduction and data sampling.

</details>


### [388] [CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions](https://arxiv.org/pdf/2506.05586)
*Isha Puri, Amit Dhurandhar, Tejaswini Pedapati, Kartikeyan Shanmugam, Dennis Wei, Kush R. Varshney*

Main category: cs.LG

TL;DR: CoFrNet, a novel neural architecture inspired by continued fractions, offers efficient training, interpretability, and universal approximation capabilities, outperforming other interpretable models and MLPs on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of interpretable neural architectures, the paper introduces CoFrNet to combine interpretability with strong performance.

Method: CoFrNet leverages continued fractions' properties for efficient training and interpretation, with a unique proof of universal approximation.

Result: CoFrNet accurately models synthetic functions and real datasets, matching or surpassing interpretable models and MLPs, sometimes nearing state-of-the-art accuracy.

Conclusion: CoFrNet demonstrates both representational power and interpretability, making it a promising alternative to traditional neural networks.

Abstract: In recent years there has been a considerable amount of research on local
post hoc explanations for neural networks. However, work on building
interpretable neural architectures has been relatively sparse. In this paper,
we present a novel neural architecture, CoFrNet, inspired by the form of
continued fractions which are known to have many attractive properties in
number theory, such as fast convergence of approximations to real numbers. We
show that CoFrNets can be efficiently trained as well as interpreted leveraging
their particular functional form. Moreover, we prove that such architectures
are universal approximators based on a proof strategy that is different than
the typical strategy used to prove universal approximation results for neural
networks based on infinite width (or depth), which is likely to be of
independent interest. We experiment on nonlinear synthetic functions and are
able to accurately model as well as estimate feature attributions and even
higher order terms in some cases, which is a testament to the representational
power as well as interpretability of such architectures. To further showcase
the power of CoFrNets, we experiment on seven real datasets spanning tabular,
text and image modalities, and show that they are either comparable or
significantly better than other interpretable models and multilayer
perceptrons, sometimes approaching the accuracies of state-of-the-art models.

</details>


### [389] [Zero-shot protein stability prediction by inverse folding models: a free energy interpretation](https://arxiv.org/pdf/2506.05596)
*Jes Frellsen, Maher M. Kassem, Tone Bengtsen, Lars Olsen, Kresten Lindorff-Larsen, Jesper Ferkinghoff-Borg, Wouter Boomsma*

Main category: cs.LG

TL;DR: The paper explores the connection between inverse folding models and thermodynamic stability, improving zero-shot protein stability prediction.


<details>
  <summary>Details</summary>
Motivation: To better understand the free-energy foundations of inverse folding models and enhance zero-shot stability prediction.

Method: Derives the free-energy foundations, critiques likelihood ratios, and suggests better stability estimates. Empirical assessment of these approaches.

Result: Demonstrates significant improvements in zero-shot performance with simple methods.

Conclusion: Clarifies the free-energy basis of inverse folding models and offers practical improvements for stability prediction.

Abstract: Inverse folding models have proven to be highly effective zero-shot
predictors of protein stability. Despite this success, the link between the
amino acid preferences of an inverse folding model and the free-energy
considerations underlying thermodynamic stability remains incompletely
understood. A better understanding would be of interest not only from a
theoretical perspective, but also potentially provide the basis for stronger
zero-shot stability prediction. In this paper, we take steps to clarify the
free-energy foundations of inverse folding models. Our derivation reveals the
standard practice of likelihood ratios as a simplistic approximation and
suggests several paths towards better estimates of the relative stability. We
empirically assess these approaches and demonstrate that considerable gains in
zero-shot performance can be achieved with fairly simple means.

</details>


### [390] [FaCTR: Factorized Channel-Temporal Representation Transformers for Efficient Time Series Forecasting](https://arxiv.org/pdf/2506.05597)
*Yash Vijay, Harini Subramanyan*

Main category: cs.LG

TL;DR: FaCTR is a lightweight spatiotemporal Transformer designed for time series forecasting, addressing the limitations of traditional Transformers by incorporating dynamic cross-channel interactions and efficient parameter usage.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with time series data due to low information density and complex dependencies. FaCTR aims to overcome these challenges with a structured, efficient design.

Method: FaCTR uses a low-rank Factorization Machine for cross-channel interactions, learnable gating, and encodes static/dynamic covariates. It is compact and interpretable.

Result: FaCTR achieves state-of-the-art performance on 11 benchmarks, using 50x fewer parameters than baselines, and supports interpretability and self-supervised pretraining.

Conclusion: FaCTR is a versatile, efficient solution for time series forecasting, balancing performance, interpretability, and scalability.

Abstract: While Transformers excel in language and vision-where inputs are semantically
rich and exhibit univariate dependency structures-their architectural
complexity leads to diminishing returns in time series forecasting. Time series
data is characterized by low per-timestep information density and complex
dependencies across channels and covariates, requiring conditioning on
structured variable interactions. To address this mismatch and
overparameterization, we propose FaCTR, a lightweight spatiotemporal
Transformer with an explicitly structural design. FaCTR injects dynamic,
symmetric cross-channel interactions-modeled via a low-rank Factorization
Machine into temporally contextualized patch embeddings through a learnable
gating mechanism. It further encodes static and dynamic covariates for
multivariate conditioning. Despite its compact design, FaCTR achieves
state-of-the-art performance on eleven public forecasting benchmarks spanning
both short-term and long-term horizons, with its largest variant using close to
only 400K parameters-on average 50x smaller than competitive spatiotemporal
transformer baselines. In addition, its structured design enables
interpretability through cross-channel influence scores-an essential
requirement for real-world decision-making. Finally, FaCTR supports
self-supervised pretraining, positioning it as a compact yet versatile
foundation for downstream time series tasks.

</details>


### [391] [When Maximum Entropy Misleads Policy Optimization](https://arxiv.org/pdf/2506.05615)
*Ruipeng Zhang, Ya-Chien Chang, Sicun Gao*

Main category: cs.LG

TL;DR: MaxEnt RL enhances exploration but can mislead policy optimization in tasks needing precise control.


<details>
  <summary>Details</summary>
Motivation: To analyze the trade-off between robustness and optimality in MaxEnt RL for complex control tasks.

Method: Experiments on various control problems to demonstrate the misleading effect of entropy maximization.

Result: Shows that MaxEnt RL can fail in tasks requiring low-entropy policies.

Conclusion: Better understanding of balancing reward design and entropy maximization in challenging control problems.

Abstract: The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading
approach for achieving efficient learning and robust performance across many RL
tasks. However, MaxEnt methods have also been shown to struggle with
performance-critical control problems in practice, where non-MaxEnt algorithms
can successfully learn. In this work, we analyze how the trade-off between
robustness and optimality affects the performance of MaxEnt algorithms in
complex control tasks: while entropy maximization enhances exploration and
robustness, it can also mislead policy optimization, leading to failure in
tasks that require precise, low-entropy policies. Through experiments on a
variety of control problems, we concretely demonstrate this misleading effect.
Our analysis leads to better understanding of how to balance reward design and
entropy maximization in challenging control problems.

</details>


### [392] [LFA applied to CNNs: Efficient Singular Value Decomposition of Convolutional Mappings by Local Fourier Analysis](https://arxiv.org/pdf/2506.05617)
*Antonia van Betteray, Matthias Rottmann, Karsten Kahl*

Main category: cs.LG

TL;DR: Proposes an O(N) complexity method for computing singular values of convolutional mappings using local Fourier analysis, improving efficiency over existing FFT-based methods.


<details>
  <summary>Details</summary>
Motivation: To address the resource-intensive computation of singular values for convolutional mappings, especially for large inputs and high channel counts, which is infeasible with naive methods.

Method: Leverages local Fourier analysis and shift invariance of convolutional operators to achieve O(N) complexity, avoiding the need for unrolling into large matrices.

Result: The method is scalable and efficient, validated through numerical experiments, enabling computation of all singular values (and vectors) for high-dimensional convolutions.

Conclusion: The proposed approach offers a practical and efficient solution for spectral analysis of convolutional mappings, with potential applications in improving neural networks and model compression.

Abstract: The singular values of convolutional mappings encode interesting spectral
properties, which can be used, e.g., to improve generalization and robustness
of convolutional neural networks as well as to facilitate model compression.
However, the computation of singular values is typically very
resource-intensive. The naive approach involves unrolling the convolutional
mapping along the input and channel dimensions into a large and sparse
two-dimensional matrix, making the exact calculation of all singular values
infeasible due to hardware limitations. In particular, this is true for
matrices that represent convolutional mappings with large inputs and a high
number of channels. Existing efficient methods leverage the Fast Fourier
transformation (FFT) to transform convolutional mappings into the frequency
domain, enabling the computation of singular values for matrices representing
convolutions with larger input and channel dimensions. For a constant number of
channels in a given convolution, an FFT can compute N singular values in O(N
log N) complexity. In this work, we propose an approach of complexity O(N)
based on local Fourier analysis, which additionally exploits the shift
invariance of convolutional operators. We provide a theoretical analysis of our
algorithm's runtime and validate its efficiency through numerical experiments.
Our results demonstrate that our proposed method is scalable and offers a
practical solution to calculate the entire set of singular values - along with
the corresponding singular vectors if needed - for high-dimensional
convolutional mappings.

</details>


### [393] [Two-dimensional Taxonomy for N-ary Knowledge Representation Learning Methods](https://arxiv.org/pdf/2506.05626)
*Xiaohua Lu, Liubov Tupikina, Mehwish Alam*

Main category: cs.LG

TL;DR: The paper surveys methods for handling n-ary relational data, proposing a taxonomy for knowledge hypergraphs and hyper-relational knowledge graphs, and discusses datasets and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of knowledge graphs (losing higher-order relational details) and hypergraphs (overlooking entity roles), the paper explores hybrid approaches for better semantic modeling.

Method: A two-dimensional taxonomy is proposed: methodology-based (e.g., translation, tensor factorization) and role/position awareness (e.g., aware-less, role-aware).

Result: The survey categorizes existing methods, datasets, and negative sampling strategies, highlighting their strengths and weaknesses.

Conclusion: The paper identifies open challenges to guide future research in n-ary relational data modeling.

Abstract: Real-world knowledge can take various forms, including structured,
semi-structured, and unstructured data. Among these, knowledge graphs are a
form of structured human knowledge that integrate heterogeneous data sources
into structured representations but typically reduce complex n-ary relations to
simple triples, thereby losing higher-order relational details. In contrast,
hypergraphs naturally represent n-ary relations with hyperedges, which directly
connect multiple entities together. Yet hypergraph representation learning
often overlooks entity roles in hyperedges, limiting the fine-grained semantic
modelling. To address these issues, knowledge hypergraphs and hyper-relational
knowledge graphs combine the advantages of knowledge graphs and hypergraphs to
better capture the complex structures and role-specific semantics of real-world
knowledge. This survey provides a comprehensive review of methods handling
n-ary relational data, covering both knowledge hypergraphs and hyper-relational
knowledge graphs literatures. We propose a two-dimensional taxonomy: the first
dimension categorises models based on their methodology, i.e.,
translation-based models, tensor factorisation-based models, deep neural
network-based models, logic rules-based models, and hyperedge expansion-based
models. The second dimension classifies models according to their awareness of
entity roles and positions in n-ary relations, dividing them into aware-less,
position-aware, and role-aware approaches. Finally, we discuss existing
datasets, negative sampling strategies, and outline open challenges to inspire
future research.

</details>


### [394] [GP-MoLFormer-Sim: Test Time Molecular Optimization through Contextual Similarity Guidance](https://arxiv.org/pdf/2506.05628)
*Jiri Navratil, Jarret Ross, Payel Das, Youssef Mroueh, Samuel C Hoffman, Vijil Chenthamarakshan, Brian Belgodere*

Main category: cs.LG

TL;DR: A training-free method, GP-MoLFormer-Sim, uses a generative Chemical Language Model (CLM) to guide molecular design by similarity to a target, outperforming baselines when combined with a genetic algorithm.


<details>
  <summary>Details</summary>
Motivation: To enable efficient molecular design while preserving similarity to a target molecule or property, crucial for drug discovery and chemical design.

Method: Leverages CLM contextual representations to estimate molecular similarity, adjusting autoregressive sampling. Integrated with a genetic algorithm (GA) for optimization.

Result: GP-MoLFormer-Sim+GA outperforms existing training-free baselines in molecular optimization tasks.

Conclusion: The method advances understanding and guidance of generative mechanisms in CLMs.

Abstract: The ability to design molecules while preserving similarity to a target
molecule and/or property is crucial for various applications in drug discovery,
chemical design, and biology. We introduce in this paper an efficient
training-free method for navigating and sampling from the molecular space with
a generative Chemical Language Model (CLM), while using the molecular
similarity to the target as a guide. Our method leverages the contextual
representations learned from the CLM itself to estimate the molecular
similarity, which is then used to adjust the autoregressive sampling strategy
of the CLM. At each step of the decoding process, the method tracks the
distance of the current generations from the target and updates the logits to
encourage the preservation of similarity in generations. We implement the
method using a recently proposed $\sim$47M parameter SMILES-based CLM,
GP-MoLFormer, and therefore refer to the method as GP-MoLFormer-Sim, which
enables a test-time update of the deep generative policy to reflect the
contextual similarity to a set of guide molecules. The method is further
integrated into a genetic algorithm (GA) and tested on a set of standard
molecular optimization benchmarks involving property optimization, molecular
rediscovery, and structure-based drug design. Results show that,
GP-MoLFormer-Sim, combined with GA (GP-MoLFormer-Sim+GA) outperforms existing
training-free baseline methods, when the oracle remains black-box. The findings
in this work are a step forward in understanding and guiding the generative
mechanisms of CLMs.

</details>


### [395] [List-Level Distribution Coupling with Applications to Speculative Decoding and Lossy Compression](https://arxiv.org/pdf/2506.05632)
*Joseph Rowan, Buu Phan, Ashish Khisti*

Main category: cs.LG

TL;DR: The paper introduces a relaxed coupling method for probability distributions, proposes a novel sampling technique, and applies it to multi-draft speculative sampling and distributed lossy compression, showing competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address the problem of coupling probability distributions more efficiently and extend its applications to practical scenarios like speculative sampling and compression.

Method: Extends Gumbel-max sampling for coupling distributions, introduces the list matching lemma for acceptance probability, and applies it to multi-draft speculative sampling and distributed lossy compression.

Result: Achieves competitive performance in language tasks and guarantees drafter invariance. Shows significant gains in compression experiments with synthetic and real-world datasets.

Conclusion: The proposed method is effective for coupling distributions and has practical applications in speculative sampling and compression, offering theoretical guarantees and empirical improvements.

Abstract: We study a relaxation of the problem of coupling probability distributions --
a list of samples is generated from one distribution and an accept is declared
if any one of these samples is identical to the sample generated from the other
distribution. We propose a novel method for generating samples, which extends
the Gumbel-max sampling suggested in Daliri et al. (arXiv:2408.07978) for
coupling probability distributions. We also establish a corresponding lower
bound on the acceptance probability, which we call the list matching lemma. We
next discuss two applications of our setup. First, we develop a new mechanism
for multi-draft speculative sampling that is simple to implement and achieves
performance competitive with baselines such as SpecTr and SpecInfer across a
range of language tasks. Our method also guarantees a certain degree of drafter
invariance with respect to the output tokens which is not supported by existing
schemes. We also provide a theoretical lower bound on the token level
acceptance probability. As our second application, we consider distributed
lossy compression with side information in a setting where a source sample is
compressed and available to multiple decoders, each with independent side
information. We propose a compression technique that is based on our
generalization of Gumbel-max sampling and show that it provides significant
gains in experiments involving synthetic Gaussian sources and the MNIST image
dataset.

</details>


### [396] [AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization](https://arxiv.org/pdf/2506.05634)
*Saeed Hedayatian, Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: AutoQD automates behavioral descriptor generation for QD algorithms using policy occupancy measures and MMD, enabling diverse policy discovery without predefined descriptors.


<details>
  <summary>Details</summary>
Motivation: Hand-crafted behavioral descriptors limit exploration in QD algorithms. AutoQD aims to automate descriptor generation for more open-ended learning.

Method: Uses random Fourier features to approximate MMD between policy occupancy measures, creating embeddings for behavioral descriptors.

Result: Proves convergence of embeddings to true MMD distances and demonstrates effectiveness in continuous control tasks.

Conclusion: AutoQD offers a domain-agnostic solution for diverse policy discovery, advancing unsupervised RL and QD optimization.

Abstract: Quality-Diversity (QD) algorithms have shown remarkable success in
discovering diverse, high-performing solutions, but rely heavily on
hand-crafted behavioral descriptors that constrain exploration to predefined
notions of diversity. Leveraging the equivalence between policies and occupancy
measures, we present a theoretically grounded approach to automatically
generate behavioral descriptors by embedding the occupancy measures of policies
in Markov Decision Processes. Our method, AutoQD, leverages random Fourier
features to approximate the Maximum Mean Discrepancy (MMD) between policy
occupancy measures, creating embeddings whose distances reflect meaningful
behavioral differences. A low-dimensional projection of these embeddings that
captures the most behaviorally significant dimensions is then used as
behavioral descriptors for off-the-shelf QD methods. We prove that our
embeddings converge to true MMD distances between occupancy measures as the
number of sampled trajectories and embedding dimensions increase. Through
experiments in multiple continuous control tasks we demonstrate AutoQD's
ability in discovering diverse policies without predefined behavioral
descriptors, presenting a well-motivated alternative to prior methods in
unsupervised Reinforcement Learning and QD optimization. Our approach opens new
possibilities for open-ended learning and automated behavior discovery in
sequential decision making settings without requiring domain-specific
knowledge.

</details>


### [397] [Bayesian Inference for Correlated Human Experts and Classifiers](https://arxiv.org/pdf/2506.05636)
*Markelle Kelly, Alex Boyd, Sam Showalter, Mark Steyvers, Padhraic Smyth*

Main category: cs.LG

TL;DR: A Bayesian framework for querying human experts efficiently in machine learning tasks, reducing query costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To minimize human expert queries in ML predictions while leveraging pre-trained classifiers and expert opinions.

Method: Develops a Bayesian framework modeling expert correlation via joint latent representation for simulation-based inference.

Result: Substantial reduction in expert query costs with maintained high accuracy in medical and image classification tasks.

Conclusion: The framework effectively balances expert input and model predictions, optimizing query efficiency without sacrificing performance.

Abstract: Applications of machine learning often involve making predictions based on
both model outputs and the opinions of human experts. In this context, we
investigate the problem of querying experts for class label predictions, using
as few human queries as possible, and leveraging the class probability
estimates of pre-trained classifiers. We develop a general Bayesian framework
for this problem, modeling expert correlation via a joint latent
representation, enabling simulation-based inference about the utility of
additional expert queries, as well as inference of posterior distributions over
unobserved expert labels. We apply our approach to two real-world medical
classification problems, as well as to CIFAR-10H and ImageNet-16H,
demonstrating substantial reductions relative to baselines in the cost of
querying human experts while maintaining high prediction accuracy.

</details>


### [398] [Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones](https://arxiv.org/pdf/2506.05641)
*Andrey Zhmoginov, Jihwan Lee, Mark Sandler*

Main category: cs.LG

TL;DR: A technique maps parameters from a large Transformer to a smaller, task-specific model, improving performance for specialized tasks.


<details>
  <summary>Details</summary>
Motivation: Large Foundation Models (FMs) are computationally expensive and contain irrelevant knowledge for specific tasks.

Method: Mapping parameters of a large Transformer to a smaller, specialized model for task-specific performance.

Result: Generated smaller models outperform universal conditional models in image modeling tasks.

Conclusion: Task-specific parameter mapping enables efficient, high-performance smaller models.

Abstract: Modern Foundation Models (FMs) are typically trained on corpora spanning a
wide range of different data modalities, topics and downstream tasks. Utilizing
these models can be very computationally expensive and is out of reach for most
consumer devices. Furthermore, most of the broad FM knowledge may actually be
irrelevant for a specific task at hand. Here we explore a technique for mapping
parameters of a large Transformer to parameters of a smaller specialized model.
By making this transformation task-specific, we aim to capture a narrower scope
of the knowledge needed for performing a specific task by a smaller model. We
study our method on image modeling tasks, showing that performance of generated
models exceeds that of universal conditional models.

</details>


### [399] [Learning to Weight Parameters for Data Attribution](https://arxiv.org/pdf/2506.05647)
*Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann*

Main category: cs.LG

TL;DR: A method for data attribution in generative models by learning parameter importance weights, improving accuracy and providing fine-grained insights.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat all network parameters uniformly, ignoring layer-specific information contributions.

Method: Proposes learning parameter importance weights tailored for attribution, adapting to model structure without labeled data.

Result: Improves attribution accuracy in diffusion models and reveals how outputs borrow from training data.

Conclusion: The method enhances understanding of training data influence on generative model outputs.

Abstract: We study data attribution in generative models, aiming to identify which
training examples most influence a given output. Existing methods achieve this
by tracing gradients back to training data. However, they typically treat all
network parameters uniformly, ignoring the fact that different layers encode
different types of information and may thus draw information differently from
the training set. We propose a method that models this by learning parameter
importance weights tailored for attribution, without requiring labeled data.
This allows the attribution process to adapt to the structure of the model,
capturing which training examples contribute to specific semantic aspects of an
output, such as subject, style, or background. Our method improves attribution
accuracy across diffusion models and enables fine-grained insights into how
outputs borrow from training data.

</details>


### [400] [BAQ: Efficient Bit Allocation Quantization for Large Language Models](https://arxiv.org/pdf/2506.05664)
*Chao Zhang, Li Wang, Samson Lasaulce, Merouane Debbah*

Main category: cs.LG

TL;DR: Proposes BAQ, a framework for nonuniform bitwidth allocation in LLM quantization using Hessian-based sensitivity metrics, outperforming GPTQ with up to 56x lower perplexity.


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods use uniform/heuristic bitwidths, ignoring nonuniform weight sensitivity, leading to suboptimal performance.

Method: Uses Hessian proxy to derive sensitivity metrics, formulates bit allocation as convex optimization, and designs the BAQ algorithm.

Result: BAQ achieves up to 56x lower perplexity than GPTQ on LLMs (125M-30B params) with minimal overhead.

Conclusion: BAQ provides a theoretically grounded, efficient solution for nonuniform quantization, improving LLM performance.

Abstract: Post-training model quantization is a widely adopted technique for reducing
the memory and computational costs of large language models (LLMs). However,
most existing methods rely on uniform or heuristic bitwidth assignments,
failing to account for the nonuniform sensitivity of weights to quantization
noise. In this paper, we propose a novel framework for allocating quantization
bitwidths based on sensitivity metrics derived from a Hessian proxy. We make
key assumptions, which allow the layer/component-wise loss function to be
expressed as an explicit function of the bitwidths. This enables a neat
formulation of the bit allocation problem as a convex optimization task, whose
closed-form solution adapts precision across weights to minimize the layer-wise
quantization loss. Inspecting the solution provides several insights (such as
the equal-loss structure), which are then exploited to design the proposed
\textbf{BAQ} (Bit Allocation Quantization) algorithm. The proposed algorithm
achieves a good trade-off between loss minimization and complexity and allows
BAQ to be integrated into standard quantization pipelines with minimal
overhead. Experimental results show that BAQ consistently outperforms GPTQ,
achieving up to 56$\times$ lower perplexity at the same bitwidth on large
language models ranging from 125M to 30B parameters. Leveraging our analytical
results derived from solving the optimal bit allocation problem, we also
provide a theoretical explanation for the observed gains. All codes of this
paper are available at https://github.com/CSU-ModelCompression/BAQ.

</details>


### [401] [RNE: a plug-and-play framework for diffusion density estimation and inference-time control](https://arxiv.org/pdf/2506.05668)
*Jiajun He, José Miguel Hernández-Lobato, Yuanqi Du, Francisco Vargas*

Main category: cs.LG

TL;DR: The paper introduces the Radon-Nikodym Estimator (RNE), a framework for diffusion inference-time density estimation and control, unifying existing methods with theoretical clarity and practical versatility.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and unified approach for density estimation and control in diffusion models, addressing limitations of existing methods.

Method: RNE leverages density ratios between path distributions, rooted in variational inference and probabilistic principles, to unify and extend existing techniques.

Result: RNE shows promising performance in tasks like diffusion density estimation, annealing, model composition, and reward-tilting.

Conclusion: RNE offers a theoretically grounded and versatile framework for diffusion inference-time tasks, unifying diverse methods effectively.

Abstract: In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,
plug-and-play framework for diffusion inference-time density estimation and
control, based on the concept of the density ratio between path distributions.
RNE connects and unifies a variety of existing density estimation and
inference-time control methods under a single and intuitive perspective,
stemming from basic variational inference and probabilistic principles
therefore offering both theoretical clarity and practical versatility.
Experiments demonstrate that RNE achieves promising performances in diffusion
density estimation and inference-time control tasks, including annealing,
composition of diffusion models, and reward-tilting.

</details>


### [402] [Contextually Guided Transformers via Low-Rank Adaptation](https://arxiv.org/pdf/2506.05672)
*Andrey Zhmoginov, Jihwan Lee, Max Vladymyrov, Mark Sandler*

Main category: cs.LG

TL;DR: A new Transformer variant, Contextually Guided Transformer (CGT), eliminates explicit prompts by encoding context into weights, enabling self-specialization and efficiency.


<details>
  <summary>Details</summary>
Motivation: To reduce computational overhead from prompts in LLMs by integrating context directly into the model's architecture.

Method: Proposes CGT, which maintains contextual summaries at each sequence position to dynamically update weights based on context.

Result: Demonstrated effectiveness on synthetic tasks and benchmarks, with improved interpretability of contextual representations.

Conclusion: CGT offers a novel, efficient approach to adaptable language modeling by embedding context into the model.

Abstract: Large Language Models (LLMs) based on Transformers excel at text processing,
but their reliance on prompts for specialized behavior introduces computational
overhead. We propose a modification to a Transformer architecture that
eliminates the need for explicit prompts by learning to encode context into the
model's weights. Our Contextually Guided Transformer (CGT) model maintains a
contextual summary at each sequence position, allowing it to update the weights
on the fly based on the preceding context. This approach enables the model to
self-specialize, effectively creating a tailored model for processing
information following a given prefix. We demonstrate the effectiveness of our
method on synthetic in-context learning tasks and language modeling benchmarks.
Furthermore, we introduce techniques for enhancing the interpretability of the
learned contextual representations, drawing connections to Variational
Autoencoders and promoting smoother, more consistent context encoding. This
work offers a novel direction for efficient and adaptable language modeling by
integrating context directly into the model's architecture.

</details>


### [403] [Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning Vision Models from DataSeeds' Annotated Imagery](https://arxiv.org/pdf/2506.05673)
*Sajjad Abdoli, Freeman Lewin, Gediminas Vasiliauskas, Fabian Schonholz*

Main category: cs.LG

TL;DR: The paper discusses a shift from model-centric to data-centric AI development, introducing the DataSeeds.AI dataset (DSD) to improve model performance through high-quality training data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight the importance of data quality over model complexity in AI development, addressing the limitations of traditional model-centric approaches.

Method: The method involves creating and utilizing the DSD, a high-quality dataset with peer-ranked images and multi-tier annotations, to evaluate performance improvements in AI models.

Result: The DSD demonstrates quantitative improvements in model performance against benchmarks, with code and trained models made publicly available.

Conclusion: The conclusion emphasizes the value of data-centric approaches and the DSD's potential to set new standards for commercial image datasets in AI development.

Abstract: The development of modern Artificial Intelligence (AI) models, particularly
diffusion-based models employed in computer vision and image generation tasks,
is undergoing a paradigmatic shift in development methodologies. Traditionally
dominated by a "Model Centric" approach, in which performance gains were
primarily pursued through increasingly complex model architectures and
hyperparameter optimization, the field is now recognizing a more nuanced
"Data-Centric" approach. This emergent framework foregrounds the quality,
structure, and relevance of training data as the principal driver of model
performance. To operationalize this paradigm shift, we introduce the
DataSeeds.AI sample dataset (the "DSD"), initially comprised of approximately
10,610 high-quality human peer-ranked photography images accompanied by
extensive multi-tier annotations. The DSD is a foundational computer vision
dataset designed to usher in a new standard for commercial image datasets.
Representing a small fraction of DataSeed.AI's 100 million-plus image catalog,
the DSD provides a scalable foundation necessary for robust commercial and
multimodal AI development. Through this in-depth exploratory analysis, we
document the quantitative improvements generated by the DSD on specific models
against known benchmarks and make the code and the trained models used in our
evaluation publicly available.

</details>


### [404] [Topology-aware Neural Flux Prediction Guided by Physics](https://arxiv.org/pdf/2506.05676)
*Haoyang Jiang, Jindong Wang, Xingquan Zhu, Yi He*

Main category: cs.LG

TL;DR: Proposes a GNN framework to preserve high-frequency nodal signals in directed graphs by combining explicit difference matrices and implicit physical constraints.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with high-frequency components in directed graphs, which are crucial for modeling flow dynamics.

Method: Combines explicit difference matrices for directional gradients and implicit physical constraints for message-passing consistency.

Result: Effective on real-world directed graphs (water flux and urban traffic flow networks).

Conclusion: The framework enhances GNN sensitivity to topological differences in directed graphs.

Abstract: Graph Neural Networks (GNNs) often struggle in preserving high-frequency
components of nodal signals when dealing with directed graphs. Such components
are crucial for modeling flow dynamics, without which a traditional GNN tends
to treat a graph with forward and reverse topologies equal.To make GNNs
sensitive to those high-frequency components thereby being capable to capture
detailed topological differences, this paper proposes a novel framework that
combines 1) explicit difference matrices that model directional gradients and
2) implicit physical constraints that enforce messages passing within GNNs to
be consistent with natural laws. Evaluations on two real-world directed graph
data, namely, water flux network and urban traffic flow network, demonstrate
the effectiveness of our proposal.

</details>


### [405] [Numerical Investigation of Sequence Modeling Theory using Controllable Memory Functions](https://arxiv.org/pdf/2506.05678)
*Haotian Jiang, Zeyu Bao, Shida Wang, Qianxiao Li*

Main category: cs.LG

TL;DR: A synthetic benchmarking framework evaluates sequence models' ability to capture diverse temporal structures, revealing strengths and limitations.


<details>
  <summary>Details</summary>
Motivation: To systematically characterize the effectiveness of sequence modeling architectures in handling temporal dependencies.

Method: Proposes a synthetic benchmarking framework with controllable targets defined by memory functions and temporal dependence parameters.

Result: Experiments confirm theoretical insights and uncover new findings about model behavior.

Conclusion: The framework advances theoretical understanding and emphasizes the need for structured, controllable targets in evaluating sequence models.

Abstract: The evolution of sequence modeling architectures, from recurrent neural
networks and convolutional models to Transformers and structured state-space
models, reflects ongoing efforts to address the diverse temporal dependencies
inherent in sequential data. Despite this progress, systematically
characterizing the strengths and limitations of these architectures remains a
fundamental challenge.In this work, we propose a synthetic benchmarking
framework to evaluate how effectively different sequence models capture
distinct temporal structures. The core of this approach is to generate
synthetic targets, each characterized by a memory function and a parameter that
determines the strength of temporal dependence. This setup allows us to produce
a continuum of tasks that vary in temporal complexity, enabling fine-grained
analysis of model behavior concerning specific memory properties. We focus on
four representative memory functions, each corresponding to a distinct class of
temporal structures.Experiments on several sequence modeling architectures
confirm existing theoretical insights and reveal new findings.These results
demonstrate the effectiveness of the proposed method in advancing theoretical
understandingand highlight the importance of using controllable targets with
clearly defined structures for evaluating sequence modeling architectures.

</details>


### [406] [Learning Design-Score Manifold to Guide Diffusion Models for Offline Optimization](https://arxiv.org/pdf/2506.05680)
*Tailin Zhou, Zhilin Chen, Wenlong Lyu, Zhitang Chen, Danny H. K. Tsang, Jun Zhang*

Main category: cs.LG

TL;DR: ManGO is a diffusion-based framework for offline optimization that outperforms existing methods by holistically learning design-score interdependencies.


<details>
  <summary>Details</summary>
Motivation: Optimizing complex systems is challenging due to unknown rules and costly evaluations. Offline optimization often fails beyond training data.

Method: ManGO learns the design-score manifold, unifying forward prediction and backward generation with derivative-free guidance and adaptive inference-time scaling.

Result: ManGO surpasses 24 single- and 10 multi-objective optimization methods across diverse domains.

Conclusion: ManGO's holistic approach enables superior generalization and performance in offline optimization.

Abstract: Optimizing complex systems, from discovering therapeutic drugs to designing
high-performance materials, remains a fundamental challenge across science and
engineering, as the underlying rules are often unknown and costly to evaluate.
Offline optimization aims to optimize designs for target scores using
pre-collected datasets without system interaction. However, conventional
approaches may fail beyond training data, predicting inaccurate scores and
generating inferior designs. This paper introduces ManGO, a diffusion-based
framework that learns the design-score manifold, capturing the design-score
interdependencies holistically. Unlike existing methods that treat design and
score spaces in isolation, ManGO unifies forward prediction and backward
generation, attaining generalization beyond training data. Key to this is its
derivative-free guidance for conditional generation, coupled with adaptive
inference-time scaling that dynamically optimizes denoising paths. Extensive
evaluations demonstrate that ManGO outperforms 24 single- and 10
multi-objective optimization methods across diverse domains, including
synthetic tasks, robot control, material design, DNA sequence, and real-world
engineering optimization.

</details>


### [407] [Statistically Valid Post-Deployment Monitoring Should Be Standard for AI-Based Digital Health](https://arxiv.org/pdf/2506.05701)
*Pavel Dolin, Weizhi Li, Gautam Dasarathy, Visar Berisha*

Main category: cs.LG

TL;DR: The paper advocates for statistically valid and label-efficient post-deployment monitoring in clinical AI to ensure reliability and safety, addressing current gaps in practice.


<details>
  <summary>Details</summary>
Motivation: Current post-deployment monitoring for clinical AI is underdeveloped, with only 9% of FDA-registered tools having surveillance plans. Existing methods are manual and reactive, unsuitable for dynamic clinical environments.

Method: Proposes framing data changes and model performance degradation as statistical hypothesis testing problems, ensuring rigor and reproducibility.

Result: The approach provides explicit error rate guarantees, formal inference, and aligns with regulatory needs, offering a principled alternative to current practices.

Conclusion: Statistically grounded monitoring ensures reliability and opens new research directions for detecting and mitigating AI failures in real-world clinical settings.

Abstract: This position paper argues that post-deployment monitoring in clinical AI is
underdeveloped and proposes statistically valid and label-efficient testing
frameworks as a principled foundation for ensuring reliability and safety in
real-world deployment. A recent review found that only 9% of FDA-registered
AI-based healthcare tools include a post-deployment surveillance plan. Existing
monitoring approaches are often manual, sporadic, and reactive, making them
ill-suited for the dynamic environments in which clinical models operate. We
contend that post-deployment monitoring should be grounded in label-efficient
and statistically valid testing frameworks, offering a principled alternative
to current practices. We use the term "statistically valid" to refer to methods
that provide explicit guarantees on error rates (e.g., Type I/II error), enable
formal inference under pre-defined assumptions, and support
reproducibility--features that align with regulatory requirements.
Specifically, we propose that the detection of changes in the data and model
performance degradation should be framed as distinct statistical hypothesis
testing problems. Grounding monitoring in statistical rigor ensures a
reproducible and scientifically sound basis for maintaining the reliability of
clinical AI systems. Importantly, it also opens new research directions for the
technical community--spanning theory, methods, and tools for statistically
principled detection, attribution, and mitigation of post-deployment model
failures in real-world settings.

</details>


### [408] [Action-Adaptive Continual Learning: Enabling Policy Generalization under Dynamic Action Spaces](https://arxiv.org/pdf/2506.05702)
*Chaofan Pan, Jiafen Liu, Yanhua Li, Linbo Xiong, Fan Min, Wei Wei, Xin Yang*

Main category: cs.LG

TL;DR: The paper introduces Continual Learning with Dynamic Capabilities (CL-DC), addressing the challenge of policy generalization across changing action spaces. It proposes the Action-Adaptive Continual Learning (AACL) framework, which decouples policy from action spaces and adaptively fine-tunes action representations. A benchmark validates AACL's superiority over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Continual Learning (CL) methods assume static agent capabilities, which is unrealistic. The paper aims to address the gap by introducing CL-DC, where capabilities dynamically change, posing a challenge for policy generalization.

Method: The proposed AACL framework decouples policy from action spaces using an action representation space. It adaptively fine-tunes the encoder-decoder for new action spaces to balance stability and plasticity.

Result: Experiments on three environments show AACL outperforms existing methods in generalizing policies across dynamic action spaces.

Conclusion: The AACL framework effectively addresses the CL-DC challenge, demonstrating superior performance in policy generalization across changing action spaces.

Abstract: Continual Learning (CL) is a powerful tool that enables agents to learn a
sequence of tasks, accumulating knowledge learned in the past and using it for
problem-solving or future task learning. However, existing CL methods often
assume that the agent's capabilities remain static within dynamic environments,
which doesn't reflect real-world scenarios where capabilities dynamically
change. This paper introduces a new and realistic problem: Continual Learning
with Dynamic Capabilities (CL-DC), posing a significant challenge for CL
agents: How can policy generalization across different action spaces be
achieved? Inspired by the cortical functions, we propose an Action-Adaptive
Continual Learning framework (AACL) to address this challenge. Our framework
decouples the agent's policy from the specific action space by building an
action representation space. For a new action space, the encoder-decoder of
action representations is adaptively fine-tuned to maintain a balance between
stability and plasticity. Furthermore, we release a benchmark based on three
environments to validate the effectiveness of methods for CL-DC. Experimental
results demonstrate that our framework outperforms popular methods by
generalizing the policy across action spaces.

</details>


### [409] [Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning](https://arxiv.org/pdf/2506.05716)
*Adrian Ly, Richard Dazeley, Peter Vamplew, Francisco Cruz, Sunil Aryal*

Main category: cs.LG

TL;DR: EEDQN combines ensembles and elastic step updates to improve DQN performance, addressing overestimation bias and sample efficiency, outperforming baselines on MinAtar benchmarks.


<details>
  <summary>Details</summary>
Motivation: Limited understanding of how DQN improvements interact, especially multi-step and ensemble methods, which reduce overestimation bias.

Method: Introduces Ensemble Elastic Step DQN (EEDQN), unifying ensembles with elastic step updates for stability.

Result: EEDQN outperforms baseline DQN methods and matches/exceeds state-of-the-art ensemble DQNs on MinAtar environments.

Conclusion: Systematic combination of algorithmic improvements (ensembles and multi-step) yields substantial gains in deep RL.

Abstract: While many algorithmic extensions to Deep Q-Networks (DQN) have been
proposed, there remains limited understanding of how different improvements
interact. In particular, multi-step and ensemble style extensions have shown
promise in reducing overestimation bias, thereby improving sample efficiency
and algorithmic stability. In this paper, we introduce a novel algorithm called
Ensemble Elastic Step DQN (EEDQN), which unifies ensembles with elastic step
updates to stabilise algorithmic performance. EEDQN is designed to address two
major challenges in deep reinforcement learning: overestimation bias and sample
efficiency. We evaluated EEDQN against standard and ensemble DQN variants
across the MinAtar benchmark, a set of environments that emphasise behavioral
learning while reducing representational complexity. Our results show that
EEDQN achieves consistently robust performance across all tested environments,
outperforming baseline DQN methods and matching or exceeding state-of-the-art
ensemble DQNs in final returns on most of the MinAtar environments. These
findings highlight the potential of systematically combining algorithmic
improvements and provide evidence that ensemble and multi-step methods, when
carefully integrated, can yield substantial gains.

</details>


### [410] [Latent Diffusion Model Based Denoising Receiver for 6G Semantic Communication: From Stochastic Differential Theory to Application](https://arxiv.org/pdf/2506.05710)
*Xiucheng Wang, Honggang Jia, Nan Cheng, Dusit Niyato*

Main category: cs.LG

TL;DR: A novel GAI-powered semantic communication framework using diffusion models (DMs) is proposed, with a theoretical foundation in SDEs for denoising AWGN. It includes a closed-form SNR-timestep relationship, a scaling mechanism for robustness, and a training-free LDM-based transceiver, outperforming baselines in low SNR and distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To address challenges in semantic communication, such as noise and distribution mismatch, by leveraging generative AI and diffusion models for robust performance without fine-tuning.

Method: Develops a latent diffusion model (LDM)-based semantic transceiver using a VAE for compression and a pretrained DM as a denoiser, with a scaling mechanism for SNR adaptability.

Result: Outperforms conventional neural-network-based baselines, especially in low SNR and distribution shifts, demonstrating robust performance.

Conclusion: The framework establishes a promising direction for GAI-driven semantic transmission in 6G systems, offering modularity, compatibility, and zero-shot generalization.

Abstract: In this paper, a novel semantic communication framework empowered by
generative artificial intelligence (GAI) is proposed, specifically leveraging
the capabilities of diffusion models (DMs). A rigorous theoretical foundation
is established based on stochastic differential equations (SDEs), which
elucidates the denoising properties of DMs in mitigating additive white
Gaussian noise (AWGN) in latent semantic representations. Crucially, a
closed-form analytical relationship between the signal-to-noise ratio (SNR) and
the denoising timestep is derived, enabling the optimal selection of diffusion
parameters for any given channel condition. To address the distribution
mismatch between the received signal and the DM's training data, a
mathematically principled scaling mechanism is introduced, ensuring robust
performance across a wide range of SNRs without requiring model fine-tuning.
Built upon this theoretical insight, we develop a latent diffusion model
(LDM)-based semantic transceiver, wherein a variational autoencoder (VAE) is
employed for efficient semantic compression, and a pretrained DM serves as a
universal denoiser. Notably, the proposed architecture is fully training-free
at inference time, offering high modularity and compatibility with large-scale
pretrained LDMs. This design inherently supports zero-shot generalization and
mitigates the challenges posed by out-of-distribution inputs. Extensive
experimental evaluations demonstrate that the proposed framework significantly
outperforms conventional neural-network-based semantic communication baselines,
particularly under low SNR conditions and distributional shifts, thereby
establishing a promising direction for GAI-driven robust semantic transmission
in future 6G systems.

</details>


### [411] [Grokking Beyond the Euclidean Norm of Model Parameters](https://arxiv.org/pdf/2506.05718)
*Pascal Jr Tikeng Notsawo, Guillaume Dumas, Guillaume Rabusseau*

Main category: cs.LG

TL;DR: Grokking, delayed generalization after overfitting, can be induced by regularization (explicit or implicit) and over-parameterization. The ℓ₂ norm isn't always a reliable proxy for generalization. Data selection alone can amplify grokking.


<details>
  <summary>Details</summary>
Motivation: To explore how regularization and over-parameterization induce grokking, extending prior findings on weight decay.

Method: Analyzed gradient descent with small non-zero regularization (e.g., ℓ₁ or nuclear norm) and over-parameterization via depth.

Result: Grokking occurs with regularization or over-parameterization. ℓ₂ norm isn't a reliable generalization proxy. Data selection amplifies grokking.

Conclusion: Grokking is influenced by regularization, over-parameterization, and data selection, challenging reliance on ℓ₂ norm for generalization.

Abstract: Grokking refers to a delayed generalization following overfitting when
optimizing artificial neural networks with gradient-based methods. In this
work, we demonstrate that grokking can be induced by regularization, either
explicit or implicit. More precisely, we show that when there exists a model
with a property $P$ (e.g., sparse or low-rank weights) that generalizes on the
problem of interest, gradient descent with a small but non-zero regularization
of $P$ (e.g., $\ell_1$ or nuclear norm regularization) results in grokking.
This extends previous work showing that small non-zero weight decay induces
grokking. Moreover, our analysis shows that over-parameterization by adding
depth makes it possible to grok or ungrok without explicitly using
regularization, which is impossible in shallow cases. We further show that the
$\ell_2$ norm is not a reliable proxy for generalization when the model is
regularized toward a different property $P$, as the $\ell_2$ norm grows in many
cases where no weight decay is used, but the model generalizes anyway. We also
show that grokking can be amplified solely through data selection, with any
other hyperparameter fixed.

</details>


### [412] [Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation](https://arxiv.org/pdf/2506.05713)
*Zhan Zhuang, Xiequn Wang, Wei Li, Yulong Zhang, Qiushi Huang, Shuhao Chen, Xuehao Wang, Yanbin Wei, Yuhe Nie, Kede Ma, Yu Zhang, Ying Wei*

Main category: cs.LG

TL;DR: CoTo is a progressive training strategy for LoRA that improves adapter optimization and generalization by gradually increasing their activation probability during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LoRA often locks adapters into suboptimal minima near initialization, limiting generalization and downstream operations like merging and pruning.

Method: CoTo stochastically deactivates adapters during training to encourage balanced optimization and broader loss landscape exploration. Theoretical analysis supports its benefits.

Result: CoTo enhances single-task performance, multi-task merging accuracy, pruning robustness, and reduces training overhead while remaining compatible with LoRA variants.

Conclusion: CoTo effectively addresses LoRA's limitations, improving adapter performance and flexibility in downstream tasks.

Abstract: Low-rank adaptation (LoRA) has emerged as a leading parameter-efficient
fine-tuning technique for adapting large foundation models, yet it often locks
adapters into suboptimal minima near their initialization. This hampers model
generalization and limits downstream operators such as adapter merging and
pruning. Here, we propose CoTo, a progressive training strategy that gradually
increases adapters' activation probability over the course of fine-tuning. By
stochastically deactivating adapters, CoTo encourages more balanced
optimization and broader exploration of the loss landscape. We provide a
theoretical analysis showing that CoTo promotes layer-wise dropout stability
and linear mode connectivity, and we adopt a cooperative-game approach to
quantify each adapter's marginal contribution. Extensive experiments
demonstrate that CoTo consistently boosts single-task performance, enhances
multi-task merging accuracy, improves pruning robustness, and reduces training
overhead, all while remaining compatible with diverse LoRA variants. Code is
available at https://github.com/zwebzone/coto.

</details>


### [413] [Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant Negative Data](https://arxiv.org/pdf/2506.05721)
*Dumindu Tissera, Omar Awadallah, Muhammad Umair Danish, Ayan Sadhu, Katarina Grolinger*

Main category: cs.LG

TL;DR: The paper proposes a redesigned loss function for multi-label classification (MLC) to handle datasets with large negative data, improving performance without added complexity.


<details>
  <summary>Details</summary>
Motivation: Large proportions of negative data in MLC datasets overwhelm learning and hinder accurate classification of positive instances, common in applications like defect detection and healthcare.

Method: The authors derive a likelihood of any class being present using a normalized weighted geometric mean of predicted probabilities and introduce a regularization parameter to balance absent class contributions.

Result: Experiments on datasets like SewerML and ChestX-ray14 show performance gains up to 6.01% in F1, 8.06% in F2, and 3.11% in mean average precision.

Conclusion: The proposed loss function enhances MLC by improving awareness of implicit positive instances, achieving significant gains without extra parameters or computational cost.

Abstract: Multi-label Classification (MLC) assigns an instance to one or more
non-exclusive classes. A challenge arises when the dataset contains a large
proportion of instances with no assigned class, referred to as negative data,
which can overwhelm the learning process and hinder the accurate identification
and classification of positive instances. Nevertheless, it is common in MLC
applications such as industrial defect detection, agricultural disease
identification, and healthcare diagnosis to encounter large amounts of negative
data. Assigning a separate negative class to these instances further
complicates the learning objective and introduces unnecessary redundancies. To
address this challenge, we redesign standard MLC loss functions by deriving a
likelihood of any class being present, formulated by a normalized weighted
geometric mean of the predicted class probabilities. We introduce a
regularization parameter that controls the relative contribution of the absent
class probabilities to the any-class presence likelihood in positive instances.
The any-class presence likelihood complements the multi-label learning by
encouraging the network to become more aware of implicit positive instances and
improve the label classification within those positive instances. Experiments
on large-scale datasets with negative data: SewerML, modified COCO, and
ChestX-ray14, across various networks and base loss functions show that our
loss functions consistently improve MLC performance of their standard loss
counterparts, achieving gains of up to 6.01 percentage points in F1, 8.06 in
F2, and 3.11 in mean average precision, all without additional parameters or
computational complexity. Code available at:
https://github.com/ML-for-Sensor-Data-Western/gmean-mlc

</details>


### [414] [Generalized Incremental Learning under Concept Drift across Evolving Data Streams](https://arxiv.org/pdf/2506.05736)
*En Yu, Jie Lu, Guangquan Zhang*

Main category: cs.LG

TL;DR: The paper introduces GILCD and CSFA to address joint evolution of label spaces and distributions in non-stationary data streams, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle co-evolution of label spaces and distributions in open-environment streaming data under limited supervision.

Method: CSFA combines prototype calibration for new-class identification and RSGS minimization for robust distribution alignment.

Result: CSFA shows superior performance in adapting to evolving semantics and distributions.

Conclusion: CSFA provides a unified, effective framework for stable adaptation in open-world streaming scenarios.

Abstract: Real-world data streams exhibit inherent non-stationarity characterized by
concept drift, posing significant challenges for adaptive learning systems.
While existing methods address isolated distribution shifts, they overlook the
critical co-evolution of label spaces and distributions under limited
supervision and persistent uncertainty. To address this, we formalize
Generalized Incremental Learning under Concept Drift (GILCD), characterizing
the joint evolution of distributions and label spaces in open-environment
streaming contexts, and propose a novel framework called Calibrated Source-Free
Adaptation (CSFA). First, CSFA introduces a training-free prototype calibration
mechanism that dynamically fuses emerging prototypes with base representations,
enabling stable new-class identification without optimization overhead. Second,
we design a novel source-free adaptation algorithm, i.e., Reliable Surrogate
Gap Sharpness-aware (RSGS) minimization. It integrates sharpness-aware
perturbation loss optimization with surrogate gap minimization, while employing
entropy-based uncertainty filtering to discard unreliable samples. This
mechanism ensures robust distribution alignment and mitigates generalization
degradation caused by uncertainties. Therefore, CSFA establishes a unified
framework for stable adaptation to evolving semantics and distributions in
open-world streaming scenarios. Extensive experiments validate the superior
performance and effectiveness of CSFA compared to state-of-the-art approaches.

</details>


### [415] [Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance](https://arxiv.org/pdf/2506.05748)
*Rudransh Agnihotri, Ananya Pandey*

Main category: cs.LG

TL;DR: A lightweight method using a 7B LLM with a JSON rubric and rank-16 LoRA adapter replaces costly reward models, achieving high accuracy and outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Reduce the cost and complexity of reward-model training in RLHF pipelines by leveraging smaller, efficient models.

Method: Augment a frozen 7B LLM with a JSON rubric and rank-16 LoRA adapter, enabling it to replace heavyweight evaluation models.

Result: 96.2% accuracy on RewardBench, outperforming larger models (27B-70B), and 92% exact match on GSM-8K with online PPO.

Conclusion: The method is cost-effective, transparent, and achieves state-of-the-art results without an offline phase.

Abstract: Reward-model training is the cost bottleneck in modern Reinforcement Learning
Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters
and an offline preference-tuning phase. In the proposed method, a frozen,
instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a
rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling
it to serve as a complete substitute for the previously used heavyweight
evaluation models. The plug-and-play judge achieves 96.2% accuracy on
RewardBench, outperforming specialized reward networks ranging from 27B to 70B
parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO
baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K
utilizing online PPO. Thorough ablations indicate that (i) six in context
demonstrations deliver the majority of the zero-to-few-shot improvements
(+2pp), and (ii) the LoRA effectively addresses the remaining disparity,
particularly in the safety and adversarial Chat-Hard segments. The proposed
model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic
HH-RLHF, to examine interpretability, accompanied by human generated
justifications. GPT-4 scoring indicates that our LoRA judge attains
approximately = 9/10 in similarity to human explanations, while zero-shot
judges score around =5/10. These results indicate that the combination of
prompt engineering and tiny LoRA produces a cost effective, transparent, and
easily adjustable reward function, removing the offline phase while achieving
new state-of-the-art outcomes for both static evaluation and online RLHF.

</details>


### [416] [Integrating Spatiotemporal Features in LSTM for Spatially Informed COVID-19 Hospitalization Forecasting](https://arxiv.org/pdf/2506.05752)
*Zhongying Wang, Thoai D. Ngo, Hamidreza Zoraghein, Benjamin Lucas, Morteza Karimzadeh*

Main category: cs.LG

TL;DR: A novel LSTM framework with spatiotemporal feature SPH improves COVID-19 hospitalization forecasting, outperforming existing models during variant surges.


<details>
  <summary>Details</summary>
Motivation: The COVID-19 pandemic's severe impact highlighted the need for accurate, timely hospitalization forecasting, especially during variant surges.

Method: Introduces a parallel LSTM architecture with SPH (Social Proximity to Hospitalizations) and multi-horizon ensembling.

Result: Outperforms COVID-19 Forecast Hub models by 27-69 hospitalizations per state during Omicron surge. SPH proves predictive.

Conclusion: Advances forecasting and highlights spatiotemporal features' role in infectious disease modeling.

Abstract: The COVID-19 pandemic's severe impact highlighted the need for accurate,
timely hospitalization forecasting to support effective healthcare planning.
However, most forecasting models struggled, especially during variant surges,
when they were needed most. This study introduces a novel Long Short-Term
Memory (LSTM) framework for forecasting daily state-level incident
hospitalizations in the United States. We present a spatiotemporal feature,
Social Proximity to Hospitalizations (SPH), derived from Facebook's Social
Connectedness Index to improve forecasts. SPH serves as a proxy for interstate
population interaction, capturing transmission dynamics across space and time.
Our parallel LSTM architecture captures both short- and long-term temporal
dependencies, and our multi-horizon ensembling strategy balances consistency
and forecasting error. Evaluation against COVID-19 Forecast Hub ensemble models
during the Delta and Omicron surges reveals superiority of our model. On
average, our model surpasses the ensemble by 27, 42, 54, and 69
hospitalizations per state on the $7^{th}$, $14^{th}$, $21^{st}$, and $28^{th}$
forecast days, respectively, during the Omicron surge. Data-ablation
experiments confirm SPH's predictive power, highlighting its effectiveness in
enhancing forecasting models. This research not only advances hospitalization
forecasting but also underscores the significance of spatiotemporal features,
such as SPH, in refining predictive performance in modeling the complex
dynamics of infectious disease spread.

</details>


### [417] [FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts](https://arxiv.org/pdf/2506.05755)
*Yang Li, Zhi Chen*

Main category: cs.LG

TL;DR: FlowOE, an imitation learning framework using flow matching models, outperforms traditional strategies in optimal execution by adapting to market conditions and refining expert actions.


<details>
  <summary>Details</summary>
Motivation: Traditional optimal execution strategies like Almgren-Chriss models are suboptimal in dynamic markets, prompting the need for adaptive solutions.

Method: FlowOE learns from expert strategies, uses a refining loss function, and adaptively selects behaviors for market conditions.

Result: Empirical evaluations show FlowOE outperforms expert models and benchmarks, achieving higher profits with reduced risk.

Conclusion: FlowOE demonstrates practical applicability and potential for enhancing adaptive optimal execution in financial markets.

Abstract: Optimal execution in financial markets refers to the process of strategically
transacting a large volume of assets over a period to achieve the best possible
outcome by balancing the trade-off between market impact costs and timing or
volatility risks. Traditional optimal execution strategies, such as static
Almgren-Chriss models, often prove suboptimal in dynamic financial markets.
This paper propose flowOE, a novel imitation learning framework based on flow
matching models, to address these limitations. FlowOE learns from a diverse set
of expert traditional strategies and adaptively selects the most suitable
expert behavior for prevailing market conditions. A key innovation is the
incorporation of a refining loss function during the imitation process,
enabling flowOE not only to mimic but also to improve upon the learned expert
actions. To the best of our knowledge, this work is the first to apply flow
matching models in a stochastic optimal execution problem. Empirical
evaluations across various market conditions demonstrate that flowOE
significantly outperforms both the specifically calibrated expert models and
other traditional benchmarks, achieving higher profits with reduced risk. These
results underscore the practical applicability and potential of flowOE to
enhance adaptive optimal execution.

</details>


### [418] [BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning](https://arxiv.org/pdf/2506.05762)
*Yunpeng Qing, Shuo Chen, Yixiao Chi, Shunyu Liu, Sixu Lin, Changqing Zou*

Main category: cs.LG

TL;DR: BiTrajDiff introduces bidirectional trajectory diffusion for offline RL, enhancing dataset diversity by modeling both future and history trajectories, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of static datasets in offline RL, which suffer from distribution bias and lack diverse behavior patterns, especially around critical states.

Method: Proposes BiTrajDiff, a framework using two complementary diffusion processes: one for future trajectories and another for history transitions, anchored by critical states.

Result: BiTrajDiff outperforms other data augmentation methods on the D4RL benchmark, demonstrating superior performance across various offline RL backbones.

Conclusion: BiTrajDiff effectively enhances dataset diversity and generalizability in offline RL by leveraging bidirectional trajectory modeling.

Abstract: Recent advances in offline Reinforcement Learning (RL) have proven that
effective policy learning can benefit from imposing conservative constraints on
pre-collected datasets. However, such static datasets often exhibit
distribution bias, resulting in limited generalizability. To address this
limitation, a straightforward solution is data augmentation (DA), which
leverages generative models to enrich data distribution. Despite the promising
results, current DA techniques focus solely on reconstructing future
trajectories from given states, while ignoring the exploration of history
transitions that reach them. This single-direction paradigm inevitably hinders
the discovery of diverse behavior patterns, especially those leading to
critical states that may have yielded high-reward outcomes. In this work, we
introduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework
for offline RL that models both future and history trajectories from any
intermediate states. Specifically, we decompose the trajectory generation task
into two independent yet complementary diffusion processes: one generating
forward trajectories to predict future dynamics, and the other generating
backward trajectories to trace essential history transitions.BiTrajDiff can
efficiently leverage critical states as anchors to expand into potentially
valuable yet underexplored regions of the state space, thereby facilitating
dataset diversity. Extensive experiments on the D4RL benchmark suite
demonstrate that BiTrajDiff achieves superior performance compared to other
advanced DA methods across various offline RL backbones.

</details>


### [419] [Exploring Microstructural Dynamics in Cryptocurrency Limit Order Books: Better Inputs Matter More Than Stacking Another Hidden Layer](https://arxiv.org/pdf/2506.05764)
*Haochuan, Wang*

Main category: cs.LG

TL;DR: Simpler models, with proper preprocessing, can outperform complex neural networks in cryptocurrency price forecasting, offering speed and interpretability.


<details>
  <summary>Details</summary>
Motivation: To determine if the performance gains in cryptocurrency price forecasting are due to model complexity or data preprocessing and feature engineering.

Method: Benchmarked various models (logistic regression, XGBoost, DeepLOB, Conv1D+LSTM) on BTC/USDT LOB data, using two filtering pipelines and labeling schemes.

Result: Simpler models matched or exceeded complex networks in performance when properly preprocessed and tuned.

Conclusion: Data preprocessing and feature engineering are key; simpler models can be as effective as complex ones, with added benefits of speed and interpretability.

Abstract: Cryptocurrency price dynamics are driven largely by microstructural supply
demand imbalances in the limit order book (LOB), yet the highly noisy nature of
LOB data complicates the signal extraction process. Prior research has
demonstrated that deep-learning architectures can yield promising predictive
performance on pre-processed equity and futures LOB data, but they often treat
model complexity as an unqualified virtue. In this paper, we aim to examine
whether adding extra hidden layers or parameters to "blackbox ish" neural
networks genuinely enhances short term price forecasting, or if gains are
primarily attributable to data preprocessing and feature engineering. We
benchmark a spectrum of models from interpretable baselines, logistic
regression, XGBoost to deep architectures (DeepLOB, Conv1D+LSTM) on BTC/USDT
LOB snapshots sampled at 100 ms to multi second intervals using publicly
available Bybit data. We introduce two data filtering pipelines (Kalman,
Savitzky Golay) and evaluate both binary (up/down) and ternary (up/flat/down)
labeling schemes. Our analysis compares models on out of sample accuracy,
latency, and robustness to noise. Results reveal that, with data preprocessing
and hyperparameter tuning, simpler models can match and even exceed the
performance of more complex networks, offering faster inference and greater
interpretability.

</details>


### [420] [Positional Encoding meets Persistent Homology on Graphs](https://arxiv.org/pdf/2506.05814)
*Yogesh Verma, Amauri H. Souza, Vikas Garg*

Main category: cs.LG

TL;DR: The paper compares PE and PH for GNNs, proving neither is more expressive, and introduces PiPE, a superior hybrid method.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of GNNs in exploiting structural information by comparing PE and PH, and proposing a better solution.

Method: Theoretical analysis of PE and PH, followed by the design of PiPE, a learnable method combining their strengths.

Result: PiPE outperforms both PE and PH in tasks like molecule prediction and graph classification.

Conclusion: PiPE advances graph representation learning by combining the strengths of PE and PH.

Abstract: The local inductive bias of message-passing graph neural networks (GNNs)
hampers their ability to exploit key structural information (e.g., connectivity
and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged
as two promising approaches to mitigate this issue. PE schemes endow GNNs with
location-aware features, while PH methods enhance GNNs with multiresolution
topological features. However, a rigorous theoretical characterization of the
relative merits and shortcomings of PE and PH has remained elusive. We bridge
this gap by establishing that neither paradigm is more expressive than the
other, providing novel constructions where one approach fails but the other
succeeds. Our insights inform the design of a novel learnable method, PiPE
(Persistence-informed Positional Encoding), which is provably more expressive
than both PH and PE. PiPE demonstrates strong performance across a variety of
tasks (e.g., molecule property prediction, graph classification, and
out-of-distribution generalization), thereby advancing the frontiers of graph
representation learning. Code is available at
https://github.com/Aalto-QuML/PIPE.

</details>


### [421] [AANet: Virtual Screening under Structural Uncertainty via Alignment and Aggregation](https://arxiv.org/pdf/2506.05768)
*Wenyu Zhu, Jianhui Wang, Bowen Gao, Yinjun Jia, Haichuan Tan, Ya-Qin Zhang, Wei-Ying Ma, Yanyan Lan*

Main category: cs.LG

TL;DR: A new alignment-and-aggregation framework improves virtual screening accuracy for apo or predicted protein structures, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing virtual screening methods rely on holo protein structures with known ligand-bound pockets, limiting their effectiveness for apo or predicted structures common in early-stage drug discovery.

Method: The framework includes a tri-modal contrastive learning module for aligning ligand, holo pocket, and cavity representations, and a cross-attention adapter for dynamic binding site aggregation.

Result: The method significantly outperforms state-of-the-art approaches on apo structures, improving early enrichment factor (EF1%) from 11.75 to 37.19, while maintaining strong holo performance.

Conclusion: This approach advances first-in-class drug discovery, especially in scenarios lacking resolved protein-ligand complexes.

Abstract: Virtual screening (VS) is a critical component of modern drug discovery, yet
most existing methods--whether physics-based or deep learning-based--are
developed around holo protein structures with known ligand-bound pockets.
Consequently, their performance degrades significantly on apo or predicted
structures such as those from AlphaFold2, which are more representative of
real-world early-stage drug discovery, where pocket information is often
missing. In this paper, we introduce an alignment-and-aggregation framework to
enable accurate virtual screening under structural uncertainty. Our method
comprises two core components: (1) a tri-modal contrastive learning module that
aligns representations of the ligand, the holo pocket, and cavities detected
from structures, thereby enhancing robustness to pocket localization error; and
(2) a cross-attention based adapter for dynamically aggregating candidate
binding sites, enabling the model to learn from activity data even without
precise pocket annotations. We evaluated our method on a newly curated
benchmark of apo structures, where it significantly outperforms
state-of-the-art methods in blind apo setting, improving the early enrichment
factor (EF1%) from 11.75 to 37.19. Notably, it also maintains strong
performance on holo structures. These results demonstrate the promise of our
approach in advancing first-in-class drug discovery, particularly in scenarios
lacking experimentally resolved protein-ligand complexes.

</details>


### [422] [Evaluating Neuron Explanations: A Unified Framework with Sanity Checks](https://arxiv.org/pdf/2506.05774)
*Tuomas Oikarinen, Ge Yan, Tsui-Wei Weng*

Main category: cs.LG

TL;DR: The paper unifies neural network unit explanation evaluation methods under one framework, compares metrics, and proposes sanity checks and reliable guidelines.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and truthfulness of explanations for neural network units, which is crucial for mechanistic interpretability.

Method: Unifies existing evaluation methods mathematically, compares metrics, and introduces two sanity checks.

Result: Many common metrics fail the sanity checks and don't respond to label changes. Reliable metrics and guidelines are identified.

Conclusion: Future evaluations should follow the proposed guidelines, and a set of reliable metrics is recommended.

Abstract: Understanding the function of individual units in a neural network is an
important building block for mechanistic interpretability. This is often done
by generating a simple text explanation of the behavior of individual neurons
or units. For these explanations to be useful, we must understand how reliable
and truthful they are. In this work we unify many existing explanation
evaluation methods under one mathematical framework. This allows us to compare
existing evaluation metrics, understand the evaluation pipeline with increased
clarity and apply existing statistical methods on the evaluation. In addition,
we propose two simple sanity checks on the evaluation metrics and show that
many commonly used metrics fail these tests and do not change their score after
massive changes to the concept labels. Based on our experimental and
theoretical results, we propose guidelines that future evaluations should
follow and identify a set of reliable evaluation metrics.

</details>


### [423] [Heartcare Suite: Multi-dimensional Understanding of ECG with Raw Multi-lead Signal Modeling](https://arxiv.org/pdf/2506.05831)
*Yihan Xie, Sijing Li, Tianwei Lin, Zhuonan Wang, Chenglin Yang, Yu Zhong, Wenqiao Zhang, Haoyuan Li, Hao Jiang, Fengda Zhang, Qishan Chen, Jun Xiao, Yueting Zhuang, Beng Chin Ooi*

Main category: cs.LG

TL;DR: Heartcare Suite is a multimodal framework for ECG analysis, featuring a dataset (Heartcare-220K), a benchmark (Heartcare-Bench), and a model (HeartcareGPT) with a novel tokenizer (Beat), achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To advance ECG-specific multimodal understanding and evaluation by providing a comprehensive dataset, benchmark, and model.

Method: Develops Heartcare-220K dataset, Heartcare-Bench benchmark, and HeartcareGPT with Beat tokenizer for ECG signal compression and analysis.

Result: HeartcareGPT achieves strong generalization and state-of-the-art performance in clinically meaningful ECG tasks.

Conclusion: Heartcare Suite effectively advances ECG multimodal understanding and evaluation, with open-source availability.

Abstract: We present Heartcare Suite, a multimodal comprehensive framework for
finegrained electrocardiogram (ECG) understanding. It comprises three key
components: (i) Heartcare-220K, a high-quality, structured, and comprehensive
multimodal ECG dataset covering essential tasks such as disease diagnosis,
waveform morphology analysis, and rhythm interpretation. (ii) Heartcare-Bench,
a systematic and multi-dimensional benchmark designed to evaluate diagnostic
intelligence and guide the optimization of Medical Multimodal Large Language
Models (Med-MLLMs) in ECG scenarios. and (iii) HeartcareGPT with a tailored
tokenizer Bidirectional ECG Abstract Tokenization (Beat), which compresses raw
multi-lead signals into semantically rich discrete tokens via duallevel vector
quantization and query-guided bidirectional diffusion mechanism. Built upon
Heartcare-220K, HeartcareGPT achieves strong generalization and SoTA
performance across multiple clinically meaningful tasks. Extensive experiments
demonstrate that Heartcare Suite is highly effective in advancing ECGspecific
multimodal understanding and evaluation. Our project is available at
https://github.com/Wznnnnn/Heartcare-Suite .

</details>


### [424] [Exploiting Similarity for Computation and Communication-Efficient Decentralized Optimization](https://arxiv.org/pdf/2506.05791)
*Yuki Takezawa, Xiaowen Jiang, Anton Rodomanov, Sebastian U. Stich*

Main category: cs.LG

TL;DR: SPDO method reduces communication and computational overhead in decentralized optimization by relaxing subproblem accuracy and leveraging functional similarity.


<details>
  <summary>Details</summary>
Motivation: Existing PDO methods require highly accurate subproblem solutions, leading to computational inefficiency.

Method: Proposes SPDO, which relaxes subproblem accuracy and uses average functional similarity for faster convergence.

Result: SPDO achieves state-of-the-art communication and computational complexities and outperforms existing methods.

Conclusion: SPDO is a more efficient approach for decentralized optimization, reducing both communication and computational costs.

Abstract: Reducing communication complexity is critical for efficient decentralized
optimization. The proximal decentralized optimization (PDO) framework is
particularly appealing, as methods within this framework can exploit functional
similarity among nodes to reduce communication rounds. Specifically, when local
functions at different nodes are similar, these methods achieve faster
convergence with fewer communication steps. However, existing PDO methods often
require highly accurate solutions to subproblems associated with the proximal
operator, resulting in significant computational overhead. In this work, we
propose the Stabilized Proximal Decentralized Optimization (SPDO) method, which
achieves state-of-the-art communication and computational complexities within
the PDO framework. Additionally, we refine the analysis of existing PDO methods
by relaxing subproblem accuracy requirements and leveraging average functional
similarity. Experimental results demonstrate that SPDO significantly
outperforms existing methods.

</details>


### [425] [EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator](https://arxiv.org/pdf/2506.05797)
*Qianyi Chen, Tianrun Gao, Chenbo Jiang, Tailin Wu*

Main category: cs.LG

TL;DR: EqCollide is an equivariant neural fields simulator for deformable object collisions, outperforming baselines with lower error and better scalability.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack equivariance to physical symmetries, struggle with collisions, and have limited scalability.

Method: Uses an equivariant encoder and GNN-based Neural ODE for collision-aware message passing, with neural fields for velocity reconstruction.

Result: Achieves 24.34% to 35.82% lower rollout MSE than baselines, generalizes to more objects and longer times, and remains robust to transformations.

Conclusion: EqCollide provides accurate, stable, and scalable simulations for deformable object collisions.

Abstract: Simulating collisions of deformable objects is a fundamental yet challenging
task due to the complexity of modeling solid mechanics and multi-body
interactions. Existing data-driven methods often suffer from lack of
equivariance to physical symmetries, inadequate handling of collisions, and
limited scalability. Here we introduce EqCollide, the first end-to-end
equivariant neural fields simulator for deformable objects and their
collisions. We propose an equivariant encoder to map object geometry and
velocity into latent control points. A subsequent equivariant Graph Neural
Network-based Neural Ordinary Differential Equation models the interactions
among control points via collision-aware message passing. To reconstruct
velocity fields, we query a neural field conditioned on control point features,
enabling continuous and resolution-independent motion predictions. Experimental
results show that EqCollide achieves accurate, stable, and scalable simulations
across diverse object configurations, and our model achieves 24.34% to 35.82%
lower rollout MSE even compared with the best-performing baseline model.
Furthermore, our model could generalize to more colliding objects and extended
temporal horizons, and stay robust to input transformed with group action.

</details>


### [426] [Option Pricing Using Ensemble Learning](https://arxiv.org/pdf/2506.05799)
*Zeyuan Li, Qingdao Huang*

Main category: cs.LG

TL;DR: This paper explores ensemble learning for option pricing, comparing it to classical ML models in accuracy, feature extraction, and noise robustness. It introduces a novel experimental strategy and evaluation mechanism integrating financial theory.


<details>
  <summary>Details</summary>
Motivation: To leverage ensemble learning's advantages (flexibility, precision) for option pricing, addressing the need for high accuracy and reduced complexity in computational finance.

Method: Comparative analysis of ensemble and classical ML models, using a novel experimental strategy with parameter transfer and an evaluation mechanism incorporating financial theory.

Result: Improved robustness and realism in financial simulations, with insights into the interaction between sliding window technique and noise.

Conclusion: Ensemble learning is effective for option pricing, and integrating financial theory with computational methods enhances performance and relevance.

Abstract: Ensemble learning is characterized by flexibility, high precision, and
refined structure. As a critical component within computational finance, option
pricing with machine learning requires both high predictive accuracy and
reduced structural complexity-features that align well with the inherent
advantages of ensemble learning. This paper investigates the application of
ensemble learning to option pricing, and conducts a comparative analysis with
classical machine learning models to assess their performance in terms of
accuracy, local feature extraction, and robustness to noise. A novel
experimental strategy is introduced, leveraging parameter transfer across
experiments to improve robustness and realism in financial simulations.Building
upon this strategy, an evaluation mechanism is developed that incorporates a
scoring strategy and a weighted evaluation strategy explicitly emphasizing the
foundational role of financial theory. This mechanism embodies an orderly
integration of theoretical finance and computational methods. In addition, the
study examines the interaction between sliding window technique and noise,
revealing nuanced patterns that suggest a potential connection relevant to
ongoing research in machine learning and data science.

</details>


### [427] [Neural Collapse in Cumulative Link Models for Ordinal Regression: An Analysis with Unconstrained Feature Model](https://arxiv.org/pdf/2506.05801)
*Chuang Ma, Tomoyuki Obuchi, Toshiyuki Tanaka*

Main category: cs.LG

TL;DR: The paper explores the emergence of Ordinal Neural Collapse (ONC) in deep Ordinal Regression tasks, demonstrating three key properties and validating them theoretically and empirically.


<details>
  <summary>Details</summary>
Motivation: To investigate if Neural Collapse (NC), a simple geometric structure in deep classification, extends to Ordinal Regression (OR) tasks, deepening understanding of deep neural networks.

Method: Combines the cumulative link model for OR with the Unconstrained Feature Model (UFM) to analyze ONC properties.

Result: ONC emerges with three properties: within-class feature collapse, alignment of class means, and ordered latent variables. Validated analytically and empirically.

Conclusion: ONC provides insights for OR tasks, particularly the utility of fixed thresholds, extending NC's theoretical framework.

Abstract: A phenomenon known as ''Neural Collapse (NC)'' in deep classification tasks,
in which the penultimate-layer features and the final classifiers exhibit an
extremely simple geometric structure, has recently attracted considerable
attention, with the expectation that it can deepen our understanding of how
deep neural networks behave. The Unconstrained Feature Model (UFM) has been
proposed to explain NC theoretically, and there emerges a growing body of work
that extends NC to tasks other than classification and leverages it for
practical applications. In this study, we investigate whether a similar
phenomenon arises in deep Ordinal Regression (OR) tasks, via combining the
cumulative link model for OR and UFM. We show that a phenomenon we call Ordinal
Neural Collapse (ONC) indeed emerges and is characterized by the following
three properties: (ONC1) all optimal features in the same class collapse to
their within-class mean when regularization is applied; (ONC2) these class
means align with the classifier, meaning that they collapse onto a
one-dimensional subspace; (ONC3) the optimal latent variables (corresponding to
logits or preactivations in classification tasks) are aligned according to the
class order, and in particular, in the zero-regularization limit, a highly
local and simple geometric relationship emerges between the latent variables
and the threshold values. We prove these properties analytically within the UFM
framework with fixed threshold values and corroborate them empirically across a
variety of datasets. We also discuss how these insights can be leveraged in OR,
highlighting the use of fixed thresholds.

</details>


### [428] [Loss Functions for Predictor-based Neural Architecture Search](https://arxiv.org/pdf/2506.05869)
*Han Ji, Yuqi Feng, Jiahao Fan, Yanan Sun*

Main category: cs.LG

TL;DR: The paper evaluates loss functions in neural architecture search (NAS) performance predictors, categorizing them into regression, ranking, and weighted types, and finds combining them improves NAS efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the lack of thorough investigation into the effectiveness of loss functions in NAS performance predictors, which impacts evaluation costs and accuracy.

Method: The study assesses eight loss functions across 13 tasks in five NAS search spaces, using NAS-relevant metrics.

Result: Combining specific loss function categories enhances predictor-based NAS, and practical guidance for loss function selection is provided.

Conclusion: The findings offer insights for developing loss functions in NAS, aiding the community in optimizing performance predictors.

Abstract: Evaluation is a critical but costly procedure in neural architecture search
(NAS). Performance predictors have been widely adopted to reduce evaluation
costs by directly estimating architecture performance. The effectiveness of
predictors is heavily influenced by the choice of loss functions. While
traditional predictors employ regression loss functions to evaluate the
absolute accuracy of architectures, recent approaches have explored various
ranking-based loss functions, such as pairwise and listwise ranking losses, to
focus on the ranking of architecture performance. Despite their success in NAS,
the effectiveness and characteristics of these loss functions have not been
thoroughly investigated. In this paper, we conduct the first comprehensive
study on loss functions in performance predictors, categorizing them into three
main types: regression, ranking, and weighted loss functions. Specifically, we
assess eight loss functions using a range of NAS-relevant metrics on 13 tasks
across five search spaces. Our results reveal that specific categories of loss
functions can be effectively combined to enhance predictor-based NAS.
Furthermore, our findings could provide practical guidance for selecting
appropriate loss functions for various tasks. We hope this work provides
meaningful insights to guide the development of loss functions for
predictor-based methods in the NAS community.

</details>


### [429] [Learning Along the Arrow of Time: Hyperbolic Geometry for Backward-Compatible Representation Learning](https://arxiv.org/pdf/2506.05826)
*Ngoc Bui, Menglin Yang, Runjin Chen, Leonardo Neves, Mingxuan Ju, Rex Ying, Neil Shah, Tong Zhao*

Main category: cs.LG

TL;DR: The paper proposes a hyperbolic geometry-based method for backward compatible representation learning, addressing uncertainty in old embeddings and enhancing model adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing compatibility methods in Euclidean space ignore uncertainty in old embeddings and force new models to reconstruct outdated representations, limiting learning.

Method: The approach uses hyperbolic geometry to treat time as an axis for model confidence, constraining new embeddings within the entailment cone of old ones and employing a robust contrastive alignment loss.

Result: Experiments show the method achieves superior compatibility, improving adaptability of machine learning systems.

Conclusion: The hyperbolic approach enhances backward compatibility by accounting for uncertainty, enabling more resilient and adaptable models.

Abstract: Backward compatible representation learning enables updated models to
integrate seamlessly with existing ones, avoiding to reprocess stored data.
Despite recent advances, existing compatibility approaches in Euclidean space
neglect the uncertainty in the old embedding model and force the new model to
reconstruct outdated representations regardless of their quality, thereby
hindering the learning process of the new model. In this paper, we propose to
switch perspectives to hyperbolic geometry, where we treat time as a natural
axis for capturing a model's confidence and evolution. By lifting embeddings
into hyperbolic space and constraining updated embeddings to lie within the
entailment cone of the old ones, we maintain generational consistency across
models while accounting for uncertainties in the representations. To further
enhance compatibility, we introduce a robust contrastive alignment loss that
dynamically adjusts alignment weights based on the uncertainty of the old
embeddings. Experiments validate the superiority of the proposed method in
achieving compatibility, paving the way for more resilient and adaptable
machine learning systems.

</details>


### [430] [Wavelet-based Disentangled Adaptive Normalization for Non-stationary Times Series Forecasting](https://arxiv.org/pdf/2506.05857)
*Junpeng Lin, Tian Lan, Bo Zhang, Ke Lin, Dandan Miao, Huiru He, Jiantao Ye, Chen Zhang, Yan-fu Li*

Main category: cs.LG

TL;DR: WDAN is a wavelet-based framework for non-stationary time series forecasting, using tailored normalization for trend and fluctuation components.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to address the multi-component nature of time series with distinct non-stationary behaviors.

Method: WDAN employs discrete wavelet transforms to separate trends and fluctuations, applying differencing for trends and adaptive normalization.

Result: WDAN improves forecasting accuracy across various backbone models in benchmarks.

Conclusion: WDAN effectively handles non-stationarity in time series by disentangling and normalizing components.

Abstract: Forecasting non-stationary time series is a challenging task because their
statistical properties often change over time, making it hard for deep models
to generalize well. Instance-level normalization techniques can help address
shifts in temporal distribution. However, most existing methods overlook the
multi-component nature of time series, where different components exhibit
distinct non-stationary behaviors. In this paper, we propose Wavelet-based
Disentangled Adaptive Normalization (WDAN), a model-agnostic framework designed
to address non-stationarity in time series forecasting. WDAN uses discrete
wavelet transforms to break down the input into low-frequency trends and
high-frequency fluctuations. It then applies tailored normalization strategies
to each part. For trend components that exhibit strong non-stationarity, we
apply first-order differencing to extract stable features used for predicting
normalization parameters. Extensive experiments on multiple benchmarks
demonstrate that WDAN consistently improves forecasting accuracy across various
backbone model. Code is available at this repository:
https://github.com/MonBG/WDAN.

</details>


### [431] [BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures](https://arxiv.org/pdf/2506.05871)
*Xiannan Hu, Tianyou Zeng, Xiaoming Yuan, Liwei Song, Guangyuan Zhang, Bangzheng He*

Main category: cs.LG

TL;DR: BestServe is a lightweight framework for ranking LLM serving strategies by estimating goodput, eliminating costly benchmarking and enabling rapid deployment planning.


<details>
  <summary>Details</summary>
Motivation: Efficient resource allocation and parallelism for serving LLMs to millions of users is labor-intensive and trial-and-error. BestServe aims to streamline this process.

Method: BestServe uses an inference simulator based on an adapted roofline model and CPU-GPU dispatch dynamics to rank serving strategies under various scenarios.

Result: The framework determines optimal strategies in minutes on a standard CPU with predictions within a 20% error margin.

Conclusion: BestServe is practical for rapid deployment due to its lightweight design, strong extensibility, and elimination of costly benchmarking.

Abstract: Serving large language models (LLMs) to millions of users requires efficient
resource allocation and parallelism strategies. It is a labor intensive
trial-and-error process to find such a strategy. We present BestServe, a novel
framework for ranking serving strategies by estimating goodput under various
operating scenarios. Supporting both collocated and disaggregated
architectures, BestServe leverages an inference simulator built on an adapted
roofline model and CPU-GPU dispatch dynamics. Our framework determines the
optimal strategy in minutes on a single standard CPU, eliminating the need for
costly benchmarking, while achieving predictions within a $20\%$ error margin.
It appeals to be practical for rapid deployment planning because of its
lightweight design and strong extensibility.

</details>


### [432] [Interpretable Clustering Ensemble](https://arxiv.org/pdf/2506.05877)
*Hang Lv, Lianyu Hu, Mudi Jiang, Xinying Liu, Zengyou He*

Main category: cs.LG

TL;DR: Proposes the first interpretable clustering ensemble algorithm, combining accuracy with interpretability for high-stakes applications.


<details>
  <summary>Details</summary>
Motivation: Existing clustering ensemble methods lack interpretability, which is crucial for transparent decision-making in fields like medical diagnosis and finance.

Method: Treats base partitions as categorical variables, constructs a decision tree in the original feature space, and uses statistical association tests to guide the tree-building process.

Result: Achieves performance comparable to SOTA methods while providing interpretability.

Conclusion: Introduces the first interpretable clustering ensemble algorithm, paving the way for future research in interpretable clustering.

Abstract: Clustering ensemble has emerged as an important research topic in the field
of machine learning. Although numerous methods have been proposed to improve
clustering quality, most existing approaches overlook the need for
interpretability in high-stakes applications. In domains such as medical
diagnosis and financial risk assessment, algorithms must not only be accurate
but also interpretable to ensure transparent and trustworthy decision-making.
Therefore, to fill the gap of lack of interpretable algorithms in the field of
clustering ensemble, we propose the first interpretable clustering ensemble
algorithm in the literature. By treating base partitions as categorical
variables, our method constructs a decision tree in the original feature space
and use the statistical association test to guide the tree building process.
Experimental results demonstrate that our algorithm achieves comparable
performance to state-of-the-art (SOTA) clustering ensemble methods while
maintaining an additional feature of interpretability. To the best of our
knowledge, this is the first interpretable algorithm specifically designed for
clustering ensemble, offering a new perspective for future research in
interpretable clustering.

</details>


### [433] [A projection-based framework for gradient-free and parallel learning](https://arxiv.org/pdf/2506.05878)
*Andreas Bergmeister, Manish Krishan Lal, Stefanie Jegelka, Suvrit Sra*

Main category: cs.LG

TL;DR: The paper introduces a feasibility-seeking approach to neural network training using projection operators, presenting PJAX as a JAX-based framework for this method.


<details>
  <summary>Details</summary>
Motivation: To provide an alternative to gradient-based loss minimization by reformulating training as a feasibility problem, enabling parallelism and handling non-differentiable operations.

Method: Uses projection operators and iterative algorithms to solve training as a feasibility problem, implemented in PJAX, a JAX-based framework with GPU/TPU support.

Result: Demonstrates successful training of diverse architectures (MLPs, CNNs, RNNs) on benchmarks, showing parallelism and non-differentiable operation handling.

Conclusion: The feasibility-seeking approach is a viable alternative to gradient-based training, with advantages in parallelism and flexibility.

Abstract: We present a feasibility-seeking approach to neural network training. This
mathematical optimization framework is distinct from conventional
gradient-based loss minimization and uses projection operators and iterative
projection algorithms. We reformulate training as a large-scale feasibility
problem: finding network parameters and states that satisfy local constraints
derived from its elementary operations. Training then involves projecting onto
these constraints, a local operation that can be parallelized across the
network. We introduce PJAX, a JAX-based software framework that enables this
paradigm. PJAX composes projection operators for elementary operations,
automatically deriving the solution operators for the feasibility problems
(akin to autodiff for derivatives). It inherently supports GPU/TPU
acceleration, provides a familiar NumPy-like API, and is extensible. We train
diverse architectures (MLPs, CNNs, RNNs) on standard benchmarks using PJAX,
demonstrating its functionality and generality. Our results show that this
approach is as a compelling alternative to gradient-based training, with clear
advantages in parallelism and the ability to handle non-differentiable
operations.

</details>


### [434] [NILMFormer: Non-Intrusive Load Monitoring that Accounts for Non-Stationarity](https://arxiv.org/pdf/2506.05880)
*Adrien Petralia, Philippe Charpentier, Youssef Kadhi, Themis Palpanas*

Main category: cs.LG

TL;DR: NILMFormer, a Transformer-based model, improves appliance-level power consumption estimation by addressing data distribution drift in smart meter data, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Existing NILM methods struggle with non-stationary data distribution in smart meter readings, impacting accuracy.

Method: Proposes NILMFormer with subsequence stationarization/de-stationarization and timestamp-based positional encoding.

Result: Outperforms state-of-the-art methods on 4 real-world datasets and is deployed by EDF.

Conclusion: NILMFormer effectively mitigates distribution drift, enhancing appliance-level power consumption insights for customers.

Abstract: Millions of smart meters have been deployed worldwide, collecting the total
power consumed by individual households. Based on these data, electricity
suppliers offer their clients energy monitoring solutions to provide feedback
on the consumption of their individual appliances. Historically, such estimates
have relied on statistical methods that use coarse-grained total monthly
consumption and static customer data, such as appliance ownership.
Non-Intrusive Load Monitoring (NILM) is the problem of disaggregating a
household's collected total power consumption to retrieve the consumed power
for individual appliances. Current state-of-the-art (SotA) solutions for NILM
are based on deep-learning (DL) and operate on subsequences of an entire
household consumption reading. However, the non-stationary nature of real-world
smart meter data leads to a drift in the data distribution within each
segmented window, which significantly affects model performance. This paper
introduces NILMFormer, a Transformer-based architecture that incorporates a new
subsequence stationarization/de-stationarization scheme to mitigate the
distribution drift and that uses a novel positional encoding that relies only
on the subsequence's timestamp information. Experiments with 4 real-world
datasets show that NILMFormer significantly outperforms the SotA approaches.
Our solution has been deployed as the backbone algorithm for EDF's
(Electricit\'e De France) consumption monitoring service, delivering detailed
insights to millions of customers about their individual appliances' power
consumption. This paper appeared in KDD 2025.

</details>


### [435] [Few Labels are all you need: A Weakly Supervised Framework for Appliance Localization in Smart-Meter Series](https://arxiv.org/pdf/2506.05895)
*Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas*

Main category: cs.LG

TL;DR: CamAL introduces a weakly supervised approach for appliance pattern localization in smart grids, outperforming fully supervised methods with fewer labels.


<details>
  <summary>Details</summary>
Motivation: The challenge of Non-Intrusive Load Monitoring (NILM) lies in the high cost and scarcity of labeled data for individual appliance power. CamAL addresses this by requiring only appliance presence information.

Method: CamAL combines an ensemble of deep-learning classifiers with explainable classification to localize appliance patterns without detailed labels.

Result: CamAL outperforms weakly supervised baselines and matches fully supervised methods with significantly fewer labels, validated on 4 real-world datasets.

Conclusion: CamAL offers a practical, label-efficient solution for NILM, advancing smart grid management by reducing reliance on costly labeled data.

Abstract: Improving smart grid system management is crucial in the fight against
climate change, and enabling consumers to play an active role in this effort is
a significant challenge for electricity suppliers. In this regard, millions of
smart meters have been deployed worldwide in the last decade, recording the
main electricity power consumed in individual households. This data produces
valuable information that can help them reduce their electricity footprint;
nevertheless, the collected signal aggregates the consumption of the different
appliances running simultaneously in the house, making it difficult to
apprehend. Non-Intrusive Load Monitoring (NILM) refers to the challenge of
estimating the power consumption, pattern, or on/off state activation of
individual appliances using the main smart meter signal. Recent methods
proposed to tackle this task are based on a fully supervised deep-learning
approach that requires both the aggregate signal and the ground truth of
individual appliance power. However, such labels are expensive to collect and
extremely scarce in practice, as they require conducting intrusive surveys in
households to monitor each appliance. In this paper, we introduce CamAL, a
weakly supervised approach for appliance pattern localization that only
requires information on the presence of an appliance in a household to be
trained. CamAL merges an ensemble of deep-learning classifiers combined with an
explainable classification method to be able to localize appliance patterns.
Our experimental evaluation, conducted on 4 real-world datasets, demonstrates
that CamAL significantly outperforms existing weakly supervised baselines and
that current SotA fully supervised NILM approaches require significantly more
labels to reach CamAL performances. The source of our experiments is available
at: https://github.com/adrienpetralia/CamAL. This paper appeared in ICDE 2025.

</details>


### [436] [Quantifying Adversarial Uncertainty in Evidential Deep Learning using Conflict Resolution](https://arxiv.org/pdf/2506.05937)
*Charmaine Barker, Daniel Bethell, Simos Gerasimou*

Main category: cs.LG

TL;DR: C-EDL improves adversarial and OOD robustness in deep learning models by leveraging conflict-aware uncertainty calibration without retraining, outperforming EDL variants.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often fail under adversarial or OOD inputs, leading to unreliable predictions. EDL, while efficient, is vulnerable to such inputs.

Method: C-EDL uses task-preserving input transformations and representational disagreement to adjust uncertainty estimates, enhancing robustness.

Result: C-EDL reduces OOD coverage by up to 55% and adversarial coverage by up to 90%, maintaining in-distribution accuracy.

Conclusion: C-EDL is a lightweight, effective solution for improving model reliability under adversarial and OOD conditions.

Abstract: Reliability of deep learning models is critical for deployment in high-stakes
applications, where out-of-distribution or adversarial inputs may lead to
detrimental outcomes. Evidential Deep Learning, an efficient paradigm for
uncertainty quantification, models predictions as Dirichlet distributions of a
single forward pass. However, EDL is particularly vulnerable to adversarially
perturbed inputs, making overconfident errors. Conflict-aware Evidential Deep
Learning (C-EDL) is a lightweight post-hoc uncertainty quantification approach
that mitigates these issues, enhancing adversarial and OOD robustness without
retraining. C-EDL generates diverse, task-preserving transformations per input
and quantifies representational disagreement to calibrate uncertainty estimates
when needed. C-EDL's conflict-aware prediction adjustment improves detection of
OOD and adversarial inputs, maintaining high in-distribution accuracy and low
computational overhead. Our experimental evaluation shows that C-EDL
significantly outperforms state-of-the-art EDL variants and competitive
baselines, achieving substantial reductions in coverage for OOD data (up to
55%) and adversarial data (up to 90%), across a range of datasets, attack
types, and uncertainty metrics.

</details>


### [437] [A Driving Regime-Embedded Deep Learning Framework for Modeling Intra-Driver Heterogeneity in Multi-Scale Car-Following Dynamics](https://arxiv.org/pdf/2506.05902)
*Shirui Zhou, Jiying Yan, Junfang Tian, Tao Wang, Yongfu Li, Shiquan Zhong*

Main category: cs.LG

TL;DR: A novel data-driven car-following framework addresses intra-driver heterogeneity by embedding discrete driving regimes into motion predictions, improving accuracy and capturing dynamic behaviors.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to fully capture the dynamic heterogeneity of a single driver under varying conditions, focusing more on inter-driver differences or simplified assumptions.

Method: The proposed hybrid deep learning architecture combines Gated Recurrent Units for regime classification and Long Short-Term Memory networks for kinematic prediction, using high-resolution trajectory data.

Result: The framework reduces prediction errors significantly (up to 58.47% MSE improvement) and reproduces key traffic phenomena like stop-and-go waves.

Conclusion: The approach effectively models both inter- and intra-driver heterogeneity, enhancing car-following behavior prediction.

Abstract: A fundamental challenge in car-following modeling lies in accurately
representing the multi-scale complexity of driving behaviors, particularly the
intra-driver heterogeneity where a single driver's actions fluctuate
dynamically under varying conditions. While existing models, both conventional
and data-driven, address behavioral heterogeneity to some extent, they often
emphasize inter-driver heterogeneity or rely on simplified assumptions,
limiting their ability to capture the dynamic heterogeneity of a single driver
under different driving conditions. To address this gap, we propose a novel
data-driven car-following framework that systematically embeds discrete driving
regimes (e.g., steady-state following, acceleration, cruising) into vehicular
motion predictions. Leveraging high-resolution traffic trajectory datasets, the
proposed hybrid deep learning architecture combines Gated Recurrent Units for
discrete driving regime classification with Long Short-Term Memory networks for
continuous kinematic prediction, unifying discrete decision-making processes
and continuous vehicular dynamics to comprehensively represent inter- and
intra-driver heterogeneity. Driving regimes are identified using a bottom-up
segmentation algorithm and Dynamic Time Warping, ensuring robust
characterization of behavioral states across diverse traffic scenarios.
Comparative analyses demonstrate that the framework significantly reduces
prediction errors for acceleration (maximum MSE improvement reached 58.47\%),
speed, and spacing metrics while reproducing critical traffic phenomena, such
as stop-and-go wave propagation and oscillatory dynamics.

</details>


### [438] [Comparative Analysis of Modern Machine Learning Models for Retail Sales Forecasting](https://arxiv.org/pdf/2506.05941)
*Luka Hobor, Mario Brcic, Lidija Polutnik, Ante Kapetanovic*

Main category: cs.LG

TL;DR: The paper evaluates forecasting models for brick-and-mortar retail, finding tree-based ensembles like XGBoost and LightGBM outperform neural networks in accuracy and efficiency, especially with localized modeling and non-imputed data.


<details>
  <summary>Details</summary>
Motivation: Accurate sales forecasting is crucial for retailers to avoid costs from overestimation or lost sales from underestimation, impacting revenue and reputation.

Method: The study compares tree-based ensembles (XGBoost, LightGBM) and neural networks (N-BEATS, NHITS, Temporal Fusion Transformer) on high-resolution retail data, addressing challenges like intermittent demand and missing values.

Result: Tree-based models with localized strategies and non-imputed data achieve superior accuracy and efficiency, while neural networks struggle with retail data irregularities despite advanced imputation.

Conclusion: Localized tree-based models are recommended for retail forecasting, emphasizing the importance of data preprocessing for performance.

Abstract: Accurate forecasting is key for all business planning. When estimated sales
are too high, brick-and-mortar retailers may incur higher costs due to unsold
inventories, higher labor and storage space costs, etc. On the other hand, when
forecasts underestimate the level of sales, firms experience lost sales,
shortages, and impact on the reputation of the retailer in their relevant
market. Accurate forecasting presents a competitive advantage for companies. It
facilitates the achievement of revenue and profit goals and execution of
pricing strategy and tactics. In this study, we provide an exhaustive
assessment of the forecasting models applied to a high-resolution
brick-and-mortar retail dataset. Our forecasting framework addresses the
problems found in retail environments, including intermittent demand, missing
values, and frequent product turnover. We compare tree-based ensembles (such as
XGBoost and LightGBM) and state-of-the-art neural network architectures
(including N-BEATS, NHITS, and the Temporal Fusion Transformer) across various
experimental settings. Our results show that localized modeling strategies
especially those using tree-based models on individual groups with non-imputed
data, consistently deliver superior forecasting accuracy and computational
efficiency. In contrast, neural models benefit from advanced imputation
methods, yet still fall short in handling the irregularities typical of
physical retail data. These results further practical understanding for model
selection in retail environment and highlight the significance of data
preprocessing to improve forecast performance.

</details>


### [439] [DeviceScope: An Interactive App to Detect and Localize Appliance Patterns in Electricity Consumption Time Series](https://arxiv.org/pdf/2506.05912)
*Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas*

Main category: cs.LG

TL;DR: DeviceScope is an interactive tool using CamAL for detecting and localizing appliance patterns in smart meter data, addressing the challenge of understanding aggregated consumption data without extensive ground-truth labels.


<details>
  <summary>Details</summary>
Motivation: Smart meters collect vast data, but non-experts struggle to interpret aggregated appliance usage. Ground-truth labels for training models are costly and rare.

Method: DeviceScope employs CamAL, a weakly supervised approach needing only appliance existence knowledge for training, to detect and localize appliance patterns.

Result: The tool enables non-experts to understand smart meter data by identifying individual appliance usage patterns.

Conclusion: DeviceScope offers a practical solution for interpreting smart meter data without relying on expensive ground-truth labels, benefiting consumers and suppliers.

Abstract: In recent years, electricity suppliers have installed millions of smart
meters worldwide to improve the management of the smart grid system. These
meters collect a large amount of electrical consumption data to produce
valuable information to help consumers reduce their electricity footprint.
However, having non-expert users (e.g., consumers or sales advisors) understand
these data and derive usage patterns for different appliances has become a
significant challenge for electricity suppliers because these data record the
aggregated behavior of all appliances. At the same time, ground-truth labels
(which could train appliance detection and localization models) are expensive
to collect and extremely scarce in practice. This paper introduces DeviceScope,
an interactive tool designed to facilitate understanding smart meter data by
detecting and localizing individual appliance patterns within a given time
period. Our system is based on CamAL (Class Activation Map-based Appliance
Localization), a novel weakly supervised approach for appliance localization
that only requires the knowledge of the existence of an appliance in a
household to be trained. This paper appeared in ICDE 2025.

</details>


### [440] [Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning](https://arxiv.org/pdf/2506.05968)
*Motoki Omura, Kazuki Ota, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada*

Main category: cs.LG

TL;DR: Incorporating the Bellman optimality operator into actor-critic frameworks for continuous action spaces accelerates learning but causes overestimation bias. An annealing approach mitigates this, improving performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Improve sample efficiency in continuous action RL by leveraging the Bellman optimality operator, addressing its overestimation bias.

Method: Propose an annealing approach transitioning from the Bellman optimality operator to the Bellman operator, tested with TD3 and SAC.

Result: Method outperforms existing approaches in locomotion and manipulation tasks, showing faster learning and robustness.

Conclusion: The annealing approach effectively balances learning acceleration and bias mitigation, enhancing actor-critic frameworks.

Abstract: For continuous action spaces, actor-critic methods are widely used in online
reinforcement learning (RL). However, unlike RL algorithms for discrete
actions, which generally model the optimal value function using the Bellman
optimality operator, RL algorithms for continuous actions typically model
Q-values for the current policy using the Bellman operator. These algorithms
for continuous actions rely exclusively on policy updates for improvement,
which often results in low sample efficiency. This study examines the
effectiveness of incorporating the Bellman optimality operator into
actor-critic frameworks. Experiments in a simple environment show that modeling
optimal values accelerates learning but leads to overestimation bias. To
address this, we propose an annealing approach that gradually transitions from
the Bellman optimality operator to the Bellman operator, thereby accelerating
learning while mitigating bias. Our method, combined with TD3 and SAC,
significantly outperforms existing approaches across various locomotion and
manipulation tasks, demonstrating improved performance and robustness to
hyperparameters related to optimality.

</details>


### [441] [Over-PINNs: Enhancing Physics-Informed Neural Networks via Higher-Order Partial Derivative Overdetermination of PDEs](https://arxiv.org/pdf/2506.05918)
*Wenxuan Huo, Qiang He, Gang Zhu, Weifeng Huang*

Main category: cs.LG

TL;DR: Over-PINNs improve PINNs by using automatic differentiation to add higher-order auxiliary equations as extra loss terms, boosting accuracy without high computational costs.


<details>
  <summary>Details</summary>
Motivation: PINNs reduce reliance on large datasets but struggle with accuracy in complex problems. Over-PINNs aim to enhance this by embedding more physical constraints.

Method: Over-PINNs use automatic differentiation to generate higher-order auxiliary equations, adding them as extra loss terms in training for better physical information capture.

Result: The method shows versatility in solving PDEs, significantly improving accuracy without substantial extra computational costs.

Conclusion: Over-PINNs effectively enhance PINNs' accuracy for complex PDEs by leveraging additional physical constraints, offering a practical improvement.

Abstract: Partial differential equations (PDEs) serve as the cornerstone of
mathematical physics. In recent years, Physics-Informed Neural Networks (PINNs)
have significantly reduced the dependence on large datasets by embedding
physical laws directly into the training of neural networks. However, when
dealing with complex problems, the accuracy of PINNs still has room for
improvement. To address this issue, we introduce the Over-PINNs framework,
which leverages automatic differentiation (AD) to generate higher-order
auxiliary equations that impose additional physical constraints. These
equations are incorporated as extra loss terms in the training process,
effectively enhancing the model's ability to capture physical information
through an "overdetermined" approach. Numerical results illustrate that this
method exhibits strong versatility in solving various types of PDEs. It
achieves a significant improvement in solution accuracy without incurring
substantial additional computational costs.

</details>


### [442] [On Measuring Long-Range Interactions in Graph Neural Networks](https://arxiv.org/pdf/2506.05971)
*Jacob Bamberger, Benjamin Gutteridge, Scott le Roux, Michael M. Bronstein, Xiaowen Dong*

Main category: cs.LG

TL;DR: The paper formalizes long-range interactions in graph tasks, introduces a range measure for graph operators, and validates it with synthetic experiments to better evaluate long-range capabilities in graph neural networks.


<details>
  <summary>Details</summary>
Motivation: Current empirical approaches to validate long-range capabilities in graph neural networks lack robustness and theoretical grounding, necessitating a more principled characterization.

Method: The authors formalize long-range interactions, introduce a range measure for graph operators, and validate it through synthetic experiments. They then apply this measure to analyze common tasks and architectures.

Result: The proposed range measure is validated and used to assess the long-range nature of existing tasks and architectures, providing a more principled evaluation framework.

Conclusion: The work advances the understanding of long-range problems in graph tasks and offers a tool (the range measure) to evaluate new datasets and architectures more effectively.

Abstract: Long-range graph tasks -- those dependent on interactions between distant
nodes -- are an open problem in graph neural network research. Real-world
benchmark tasks, especially the Long Range Graph Benchmark, have become popular
for validating the long-range capability of proposed architectures. However,
this is an empirical approach that lacks both robustness and theoretical
underpinning; a more principled characterization of the long-range problem is
required. To bridge this gap, we formalize long-range interactions in graph
tasks, introduce a range measure for operators on graphs, and validate it with
synthetic experiments. We then leverage our measure to examine commonly used
tasks and architectures, and discuss to what extent they are, in fact,
long-range. We believe our work advances efforts to define and address the
long-range problem on graphs, and that our range measure will aid evaluation of
new datasets and architectures.

</details>


### [443] [Machine Learning Predictions for Traffic Equilibria in Road Renovation Scheduling](https://arxiv.org/pdf/2506.05933)
*Robbert Bosch, Wouter van Heeswijk, Patricia Rogetzer, Martijn Mes*

Main category: cs.LG

TL;DR: The paper proposes using machine learning surrogate models to predict traffic congestion from road maintenance, with XGBoost outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Road maintenance schedules can worsen traffic congestion, and traditional simulations are computationally expensive for large-scale planning.

Method: Machine learning models (linear, ensemble, probabilistic, neural) are evaluated under an online learning framework using traffic features and heuristics.

Result: XGBoost achieves the best performance (11% MAPE), outperforming other models by 20-38%. The Costliest Subset Heuristic works well with limited data.

Conclusion: Machine learning, especially XGBoost, can reduce computational costs in traffic assignment problems for maintenance planning.

Abstract: Accurately estimating the impact of road maintenance schedules on traffic
conditions is important because maintenance operations can substantially worsen
congestion if not carefully planned. Reliable estimates allow planners to avoid
excessive delays during periods of roadwork. Since the exact increase in
congestion is difficult to predict analytically, traffic simulations are
commonly used to assess the redistribution of the flow of traffic. However,
when applied to long-term maintenance planning involving many overlapping
projects and scheduling alternatives, these simulations must be run thousands
of times, resulting in a significant computational burden. This paper
investigates the use of machine learning-based surrogate models to predict
network-wide congestion caused by simultaneous road renovations. We frame the
problem as a supervised learning task, using one-hot encodings, engineered
traffic features, and heuristic approximations. A range of linear,
ensemble-based, probabilistic, and neural regression models is evaluated under
an online learning framework in which data progressively becomes available. The
experimental results show that the Costliest Subset Heuristic provides a
reasonable approximation when limited training data is available, and that most
regression models fail to outperform it, with the exception of XGBoost, which
achieves substantially better accuracy. In overall performance, XGBoost
significantly outperforms alternatives in a range of metrics, most strikingly
Mean Absolute Percentage Error (MAPE) and Pinball loss, where it achieves a
MAPE of 11% and outperforms the next-best model by 20% and 38% respectively.
This modeling approach has the potential to reduce the computational burden of
large-scale traffic assignment problems in maintenance planning.

</details>


### [444] [AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification](https://arxiv.org/pdf/2506.05980)
*Geonwoo Cho, Jaemoon Lee, Jaegyun Im, Subi Lee, Jihwan Lee, Sundong Kim*

Main category: cs.LG

TL;DR: AMPED balances exploration and skill diversity in SBRL using gradient surgery and a skill selector, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing SBRL methods struggle to optimize exploration and skill diversity simultaneously.

Method: AMPED uses gradient surgery for objective balancing and a skill selector for task adaptation.

Result: AMPED surpasses SBRL baselines in benchmarks.

Conclusion: Explicitly harmonizing exploration and diversity improves skill learning robustness and generalizability.

Abstract: Skill-based reinforcement learning (SBRL) enables rapid adaptation in
environments with sparse rewards by pretraining a skill-conditioned policy.
Effective skill learning requires jointly maximizing both exploration and skill
diversity. However, existing methods often face challenges in simultaneously
optimizing for these two conflicting objectives. In this work, we propose a new
method, Adaptive Multi-objective Projection for balancing Exploration and skill
Diversification (AMPED), which explicitly addresses both exploration and skill
diversification. We begin by conducting extensive ablation studies to identify
and define a set of objectives that effectively capture the aspects of
exploration and skill diversity, respectively. During the skill pretraining
phase, AMPED introduces a gradient surgery technique to balance the objectives
of exploration and skill diversity, mitigating conflicts and reducing reliance
on heuristic tuning. In the subsequent fine-tuning phase, AMPED incorporates a
skill selector module that dynamically selects suitable skills for downstream
tasks, based on task-specific performance signals. Our approach achieves
performance that surpasses SBRL baselines across various benchmarks. These
results highlight the importance of explicitly harmonizing exploration and
diversity and demonstrate the effectiveness of AMPED in enabling robust and
generalizable skill learning. Project Page: https://geonwoo.me/amped/

</details>


### [445] [Exponential Family Variational Flow Matching for Tabular Data Generation](https://arxiv.org/pdf/2506.05940)
*Andrés Guzmán-Cordero, Floor Eijkelboom, Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: TabbyFlow introduces EF-VFM for tabular data generation, combining continuous and discrete features via exponential family distributions, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the limited application of denoising diffusion and flow matching to tabular data, despite its real-world ubiquity.

Method: Develops TabbyFlow, a variational Flow Matching (VFM) method, and introduces Exponential Family Variational Flow Matching (EF-VFM) for mixed data types.

Result: Achieves state-of-the-art performance on tabular data benchmarks.

Conclusion: EF-VFM enables principled learning for mixed data, advancing generative modeling for tabular data.

Abstract: While denoising diffusion and flow matching have driven major advances in
generative modeling, their application to tabular data remains limited, despite
its ubiquity in real-world applications. To this end, we develop TabbyFlow, a
variational Flow Matching (VFM) method for tabular data generation. To apply
VFM to data with mixed continuous and discrete features, we introduce
Exponential Family Variational Flow Matching (EF-VFM), which represents
heterogeneous data types using a general exponential family distribution. We
hereby obtain an efficient, data-driven objective based on moment matching,
enabling principled learning of probability paths over mixed continuous and
discrete variables. We also establish a connection between variational flow
matching and generalized flow matching objectives based on Bregman divergences.
Evaluation on tabular data benchmarks demonstrates state-of-the-art performance
compared to baselines.

</details>


### [446] [Additive decomposition of one-dimensional signals using Transformers](https://arxiv.org/pdf/2506.05942)
*Samuele Salti, Andrea Pinto, Alessandro Lanza, Serena Morigi*

Main category: cs.LG

TL;DR: A novel Transformer-based method for decomposing 1D signals into components like piece-wise constant, smooth, textured, and noise, showing high accuracy on synthetic data.


<details>
  <summary>Details</summary>
Motivation: Traditional decomposition relies on mathematical models, but deep learning offers unexplored potential for this problem.

Method: Uses Transformer architecture to decompose signals into constituent components, trained on synthetic data.

Result: Achieves excellent accuracy in modeling and decomposing signals from the same distribution.

Conclusion: Deep learning, specifically Transformers, presents a promising approach for 1D signal decomposition.

Abstract: One-dimensional signal decomposition is a well-established and widely used
technique across various scientific fields. It serves as a highly valuable
pre-processing step for data analysis. While traditional decomposition
techniques often rely on mathematical models, recent research suggests that
applying the latest deep learning models to this problem presents an exciting,
unexplored area with promising potential. This work presents a novel method for
the additive decomposition of one-dimensional signals. We leverage the
Transformer architecture to decompose signals into their constituent
components: piece-wise constant, smooth (low-frequency oscillatory), textured
(high-frequency oscillatory), and a noise component. Our model, trained on
synthetic data, achieves excellent accuracy in modeling and decomposing input
signals from the same distribution, as demonstrated by the experimental
results.

</details>


### [447] [Learning Deterministic Policies with Policy Gradients in Constrained Markov Decision Processes](https://arxiv.org/pdf/2506.05953)
*Alessandro Montenegro, Leonardo Cesani, Marco Mussi, Matteo Papini, Alberto Maria Metelli*

Main category: cs.LG

TL;DR: C-PG is an exploration-agnostic algorithm for Constrained Reinforcement Learning (CRL) with global convergence guarantees, validated empirically on constrained control tasks.


<details>
  <summary>Details</summary>
Motivation: Address sequential decision-making problems where agents must maximize return while meeting constraints, leveraging policy-based methods for continuous-control.

Method: Introduces C-PG, an algorithm with global last-iterate convergence guarantees under gradient domination assumptions, and validates action-based (C-PGAE) and parameter-based (C-PGPE) variants.

Result: C-PG achieves global convergence to the optimal deterministic policy under specific noise models, outperforming baselines when deploying deterministic policies post-training.

Conclusion: C-PG is effective for CRL, particularly when deterministic policies are deployed after training, with strong empirical validation.

Abstract: Constrained Reinforcement Learning (CRL) addresses sequential decision-making
problems where agents are required to achieve goals by maximizing the expected
return while meeting domain-specific constraints. In this setting, policy-based
methods are widely used thanks to their advantages when dealing with
continuous-control problems. These methods search in the policy space with an
action-based or a parameter-based exploration strategy, depending on whether
they learn the parameters of a stochastic policy or those of a stochastic
hyperpolicy. We introduce an exploration-agnostic algorithm, called C-PG, which
enjoys global last-iterate convergence guarantees under gradient domination
assumptions. Furthermore, under specific noise models where the (hyper)policy
is expressed as a stochastic perturbation of the actions or of the parameters
of an underlying deterministic policy, we additionally establish global
last-iterate convergence guarantees of C-PG to the optimal deterministic
policy. This holds when learning a stochastic (hyper)policy and subsequently
switching off the stochasticity at the end of training, thereby deploying a
deterministic policy. Finally, we empirically validate both the action-based
(C-PGAE) and parameter-based (C-PGPE) variants of C-PG on constrained control
tasks, and compare them against state-of-the-art baselines, demonstrating their
effectiveness, in particular when deploying deterministic policies after
training.

</details>


### [448] [Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization](https://arxiv.org/pdf/2506.05957)
*Tianjun Yao, Haoxuan Li, Yongqiang Chen, Tongliang Liu, Le Song, Eric Xing, Zhiqiang Shen*

Main category: cs.LG

TL;DR: PrunE is a pruning-based method to improve Graph Neural Networks' OOD generalization by eliminating spurious edges, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: GNNs suffer performance drops under distribution shifts; existing methods struggle to identify invariant subgraphs due to spurious edge correlations.

Method: PrunE uses two regularization terms: graph size constraint and ε-probability alignment to prune spurious edges.

Result: PrunE achieves superior OOD performance, significantly outperforming state-of-the-art methods.

Conclusion: PrunE effectively retains invariant subgraphs by pruning spurious edges, enhancing GNNs' real-world applicability.

Abstract: Graph Neural Networks (GNNs) often encounter significant performance
degradation under distribution shifts between training and test data, hindering
their applicability in real-world scenarios. Recent studies have proposed
various methods to address the out-of-distribution generalization challenge,
with many methods in the graph domain focusing on directly identifying an
invariant subgraph that is predictive of the target label. However, we argue
that identifying the edges from the invariant subgraph directly is challenging
and error-prone, especially when some spurious edges exhibit strong
correlations with the targets. In this paper, we propose PrunE, the first
pruning-based graph OOD method that eliminates spurious edges to improve OOD
generalizability. By pruning spurious edges, \mine{} retains the invariant
subgraph more comprehensively, which is critical for OOD generalization.
Specifically, PrunE employs two regularization terms to prune spurious edges:
1) graph size constraint to exclude uninformative spurious edges, and 2)
$\epsilon$-probability alignment to further suppress the occurrence of spurious
edges. Through theoretical analysis and extensive experiments, we show that
PrunE achieves superior OOD performance and outperforms previous
state-of-the-art methods significantly. Codes are available at:
\href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.

</details>


### [449] [TRUST: Test-time Resource Utilization for Superior Trustworthiness](https://arxiv.org/pdf/2506.06048)
*Haripriya Harikumar, Santu Rana*

Main category: cs.LG

TL;DR: A novel test-time optimization method improves uncertainty estimation by addressing noisy classifier weights, enhancing reliability in confidence estimates and outperforming standard metrics like AUSE and AURC.


<details>
  <summary>Details</summary>
Motivation: Standard uncertainty estimation techniques like dropout fail to clearly distinguish reliable predictions from unreliable ones due to noisy classifier weights.

Method: Proposes a test-time optimization method that accounts for noise in classifier weights to produce more reliable confidence estimates.

Result: The method outperforms in risk-based metrics (AUSE, AURC), identifies training-test discrepancies, and differentiates in-distribution from out-of-distribution samples.

Conclusion: The approach enhances uncertainty estimation, clarifies CNN vs. ViT classifier differences, and improves reliability in confidence scores.

Abstract: Standard uncertainty estimation techniques, such as dropout, often struggle
to clearly distinguish reliable predictions from unreliable ones. We attribute
this limitation to noisy classifier weights, which, while not impairing overall
class-level predictions, render finer-level statistics less informative. To
address this, we propose a novel test-time optimization method that accounts
for the impact of such noise to produce more reliable confidence estimates.
This score defines a monotonic subset-selection function, where population
accuracy consistently increases as samples with lower scores are removed, and
it demonstrates superior performance in standard risk-based metrics such as
AUSE and AURC. Additionally, our method effectively identifies discrepancies
between training and test distributions, reliably differentiates
in-distribution from out-of-distribution samples, and elucidates key
differences between CNN and ViT classifiers across various vision datasets.

</details>


### [450] [AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models](https://arxiv.org/pdf/2506.05960)
*Adil Hasan, Thomas Peyrin*

Main category: cs.LG

TL;DR: The paper introduces a vector quantization method for diffusion models, achieving better performance and efficiency than uniform scalar quantization, with notable improvements in low-bit weight quantization.


<details>
  <summary>Details</summary>
Motivation: The high hardware resource demands of diffusion model inference hinder mass-market adoption, prompting exploration of more efficient quantization methods beyond uniform scalar quantization.

Method: The authors apply codebook-based additive vector quantization to diffusion models, focusing on compressing weights while maintaining performance.

Result: Their method sets a new Pareto frontier for low-bit weight quantization, outperforming full-precision models in metrics like sFID, FID, and ISC, and demonstrates FLOPs savings.

Conclusion: Vector quantization is a promising approach for efficient diffusion model inference, offering hardware-agnostic benefits and superior performance in low-bit settings.

Abstract: Significant investments have been made towards the commodification of
diffusion models for generation of diverse media. Their mass-market adoption is
however still hobbled by the intense hardware resource requirements of
diffusion model inference. Model quantization strategies tailored specifically
towards diffusion models have been useful in easing this burden, yet have
generally explored the Uniform Scalar Quantization (USQ) family of quantization
methods. In contrast, Vector Quantization (VQ) methods, which operate on groups
of multiple related weights as the basic unit of compression, have seen
substantial success in Large Language Model (LLM) quantization. In this work,
we apply codebook-based additive vector quantization to the problem of
diffusion model compression. Our resulting approach achieves a new Pareto
frontier for the extremely low-bit weight quantization on the standard
class-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.
Notably, we report sFID 1.92 points lower than the full-precision model at W4A8
and the best-reported results for FID, sFID and ISC at W2A8. We are also able
to demonstrate FLOPs savings on arbitrary hardware via an efficient inference
kernel, as opposed to savings resulting from small integer operations which may
lack broad hardware support.

</details>


### [451] [Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning](https://arxiv.org/pdf/2506.05977)
*Yujia Huo, Jianchun Liu, Hongli Xu, Zhenguo Ma, Shilong Wang, Liusheng Huang*

Main category: cs.LG

TL;DR: FedBE is a federated fine-tuning framework for LLMs that addresses catastrophic forgetting and heterogeneity in distributed environments by expanding trainable blocks and dynamically allocating them, achieving higher accuracy and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Existing FedFT methods struggle with catastrophic forgetting and heterogeneity in federated environments, leading to degraded model performance.

Method: FedBE integrates adaptive transformer block expansion and dynamic trainable-block allocation to separate new knowledge from pre-trained representations and accommodate client diversity.

Result: FedBE achieves 12-74% higher accuracy retention and 1.9-3.1x faster convergence compared to existing methods.

Conclusion: FedBE effectively mitigates catastrophic forgetting and enhances model generalization in federated fine-tuning.

Abstract: Federated fine-tuning (FedFT) of large language models (LLMs) has emerged as
a promising solution for adapting models to distributed data environments while
ensuring data privacy.
  Existing FedFT methods predominantly utilize parameter-efficient fine-tuning
(PEFT) techniques to reduce communication and computation overhead.
  However, they often fail to adequately address the catastrophic forgetting, a
critical challenge arising from continual adaptation in distributed
environments. The traditional centralized fine-tuning methods, which are not
designed for the heterogeneous and privacy-constrained nature of federated
environments, struggle to mitigate this issue effectively. Moreover, the
challenge is further exacerbated by significant variation in data distributions
and device capabilities across clients, which leads to intensified forgetting
and degraded model generalization. To tackle these issues, we propose FedBE, a
novel FedFT framework that integrates an adaptive transformer block expansion
mechanism with a dynamic trainable-block allocation strategy. Specifically,
FedBE expands trainable blocks within the model architecture, structurally
separating newly learned task-specific knowledge from the original pre-trained
representations. Additionally, FedBE dynamically assigns these trainable blocks
to clients based on their data distributions and computational capabilities.
This enables the framework to better accommodate heterogeneous federated
environments and enhances the generalization ability of the model.Extensive
experiments show that compared with existing federated fine-tuning methods,
FedBE achieves 12-74% higher accuracy retention on general tasks after
fine-tuning and a model convergence acceleration ratio of 1.9-3.1x without
degrading the accuracy of downstream tasks.

</details>


### [452] [Text-to-LoRA: Instant Transformer Adaption](https://arxiv.org/pdf/2506.06105)
*Rujikorn Charakorn, Edoardo Cetin, Yujin Tang, Robert Tjarko Lange*

Main category: cs.LG

TL;DR: Text-to-LoRA (T2L) enables dynamic adaptation of Large Language Models using natural language task descriptions, bypassing costly fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional fine-tuning for task adaptation is expensive and hyperparameter-sensitive. T2L aims to democratize model specialization with minimal compute.

Method: T2L is a hypernetwork trained to construct LoRAs in one forward pass, tested on 9 pre-trained LoRA adapters.

Result: T2L matches task-specific adapter performance and generalizes to unseen tasks, compressing hundreds of LoRA instances.

Conclusion: T2L advances foundation model adaptation, enabling efficient, language-based task specialization.

Abstract: While Foundation Models provide a general tool for rapid content creation,
they regularly require task-specific adaptation. Traditionally, this exercise
involves careful curation of datasets and repeated fine-tuning of the
underlying model. Fine-tuning techniques enable practitioners to adapt
foundation models for many new applications but require expensive and lengthy
training while being notably sensitive to hyper-parameter choices. To overcome
these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting
Large Language Models on the fly solely based on a natural language description
of the target task. T2L is a hypernetwork trained to construct LoRAs in a
single inexpensive forward pass. After training T2L on a suite of 9 pre-trained
LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA
instances match the performance of task-specific adapters across the
corresponding test sets. Furthermore, T2L can compress hundreds of LoRA
instances and zero-shot generalize to entirely unseen tasks. This approach
provides a significant step towards democratizing the specialization of
foundation models and enables language-based adaptation with minimal compute
requirements. Our code is available at https://github.com/SakanaAI/text-to-lora

</details>


### [453] [Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning](https://arxiv.org/pdf/2506.05985)
*Yuheng Lei, Sitong Mao, Shunbo Zhou, Hongyuan Zhang, Xuelong Li, Ping Luo*

Main category: cs.LG

TL;DR: DMPEL is a lifelong learning method for robots that dynamically combines low-rank experts and uses coefficient replay to mitigate forgetting, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current methods in lifelong learning, such as reliance on task identifiers and restricted knowledge sharing.

Method: Proposes DMPEL, which learns a low-rank expert library and uses a lightweight router for dynamic combination, along with coefficient replay.

Result: Outperforms state-of-the-art methods in success rates on the LIBERO benchmark, with minimal trainable parameters and storage.

Conclusion: DMPEL is an efficient and effective solution for lifelong robot learning, balancing forward transfer and forgetting.

Abstract: A generalist agent must continuously learn and adapt throughout its lifetime,
achieving efficient forward transfer while minimizing catastrophic forgetting.
Previous work within the dominant pretrain-then-finetune paradigm has explored
parameter-efficient fine-tuning for single-task adaptation, effectively
steering a frozen pretrained model with a small number of parameters. However,
in the context of lifelong learning, these methods rely on the impractical
assumption of a test-time task identifier and restrict knowledge sharing among
isolated adapters. To address these limitations, we propose Dynamic Mixture of
Progressive Parameter-Efficient Expert Library (DMPEL) for lifelong robot
learning. DMPEL progressively learn a low-rank expert library and employs a
lightweight router to dynamically combine experts into an end-to-end policy,
facilitating flexible behavior during lifelong adaptation. Moreover, by
leveraging the modular structure of the fine-tuned parameters, we introduce
coefficient replay to guide the router in accurately retrieving frozen experts
for previously encountered tasks, thereby mitigating catastrophic forgetting.
This method is significantly more storage- and computationally-efficient than
applying demonstration replay to the entire policy. Extensive experiments on
the lifelong manipulation benchmark LIBERO demonstrate that our framework
outperforms state-of-the-art lifelong learning methods in success rates across
continual adaptation, while utilizing minimal trainable parameters and storage.

</details>


### [454] [Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness](https://arxiv.org/pdf/2506.06112)
*Cheng-Long Wang, Qi Li, Zihang Xiang, Yinzhi Cao, Di Wang*

Main category: cs.LG

TL;DR: The paper introduces IAM, a framework for measuring machine unlearning effectiveness, addressing computational and granularity limitations of existing methods like MIAs.


<details>
  <summary>Details</summary>
Motivation: Growing concerns over data privacy and security necessitate efficient and accurate methods to assess machine unlearning without full retraining.

Method: Proposes Interpolated Approximate Measurement (IAM), which quantifies unlearning completeness by interpolating generalization-fitting behavior gaps.

Result: IAM achieves strong performance in binary inclusion tests and high correlation for approximate unlearning, scalable to LLMs with minimal resources.

Conclusion: IAM reveals risks in approximate unlearning systems, highlighting the need for stronger safeguards, and provides a practical tool for unlearning inference.

Abstract: Growing concerns over data privacy and security highlight the importance of
machine unlearning--removing specific data influences from trained models
without full retraining. Techniques like Membership Inference Attacks (MIAs)
are widely used to externally assess successful unlearning. However, existing
methods face two key limitations: (1) maximizing MIA effectiveness (e.g., via
online attacks) requires prohibitive computational resources, often exceeding
retraining costs; (2) MIAs, designed for binary inclusion tests, struggle to
capture granular changes in approximate unlearning. To address these
challenges, we propose the Interpolated Approximate Measurement (IAM), a
framework natively designed for unlearning inference. IAM quantifies
sample-level unlearning completeness by interpolating the model's
generalization-fitting behavior gap on queried samples. IAM achieves strong
performance in binary inclusion tests for exact unlearning and high correlation
for approximate unlearning--scalable to LLMs using just one pre-trained shadow
model. We theoretically analyze how IAM's scoring mechanism maintains
performance efficiently. We then apply IAM to recent approximate unlearning
algorithms, revealing general risks of both over-unlearning and
under-unlearning, underscoring the need for stronger safeguards in approximate
unlearning systems. The code is available at
https://github.com/Happy2Git/Unlearning_Inference_IAM.

</details>


### [455] [RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory](https://arxiv.org/pdf/2506.05994)
*Yi-Chun Liao, Chieh-Lin Tsai, Yuan-Hao Chang, Camélia Slimani, Jalil Boukhobza, Tei-Wei Kuo*

Main category: cs.LG

TL;DR: RETENTION is a framework that reduces CAM capacity for tree-based models, improving efficiency with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Tree-based models outperform deep learning on structured data but face acceleration challenges due to high memory use and low utilization in CAM-based designs.

Method: RETENTION uses an iterative pruning algorithm and novel tree mapping with data placement strategies to minimize CAM redundancy.

Result: RETENTION improves space efficiency by 1.46× to 207.12× with under 3% accuracy loss.

Conclusion: RETENTION effectively reduces CAM requirements, offering a resource-efficient solution for accelerating tree-based models.

Abstract: Although deep learning has demonstrated remarkable capabilities in learning
from unstructured data, modern tree-based ensemble models remain superior in
extracting relevant information and learning from structured datasets. While
several efforts have been made to accelerate tree-based models, the inherent
characteristics of the models pose significant challenges for conventional
accelerators. Recent research leveraging content-addressable memory (CAM)
offers a promising solution for accelerating tree-based models, yet existing
designs suffer from excessive memory consumption and low utilization. This work
addresses these challenges by introducing RETENTION, an end-to-end framework
that significantly reduces CAM capacity requirement for tree-based model
inference. We propose an iterative pruning algorithm with a novel pruning
criterion tailored for bagging-based models (e.g., Random Forest), which
minimizes model complexity while ensuring controlled accuracy degradation.
Additionally, we present a tree mapping scheme that incorporates two innovative
data placement strategies to alleviate the memory redundancy caused by the
widespread use of don't care states in CAM. Experimental results show that
implementing the tree mapping scheme alone achieves $1.46\times$ to $21.30
\times$ better space efficiency, while the full RETENTION framework yields
$4.35\times$ to $207.12\times$ improvement with less than 3% accuracy loss.
These results demonstrate that RETENTION is highly effective in reducing CAM
capacity requirement, providing a resource-efficient direction for tree-based
model acceleration.

</details>


### [456] [Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models](https://arxiv.org/pdf/2506.06137)
*Rihui Jin, Zheyu Xin, Xing Xie, Zuoyi Li, Guilin Qi, Yongrui Chen, Xinbang Dai, Tongtong Wu, Gholamreza Haffari*

Main category: cs.LG

TL;DR: Table-r1, a two-stage program-based table reasoning method for small language models (SLMs), improves accuracy by 15% over base models and competes with large LMs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of small language models (SLMs) in table reasoning, particularly in numerical reasoning and layout heterogeneity, by leveraging program-based approaches.

Method: Proposes Table-r1: Stage 1 uses self-supervised layout transformation inference for layout generalization; Stage 2 employs a mix-paradigm variant of Group Relative Policy Optimization for consistent reasoning with fallback to text-based TR.

Result: Achieves at least 15% accuracy improvement over LLaMA-8B across datasets and competitive performance with large LMs.

Conclusion: Table-r1 effectively narrows the gap between SLMs and LLMs in table reasoning, demonstrating robustness and consistency.

Abstract: Table reasoning (TR) requires structured reasoning over semi-structured
tabular data and remains challenging, particularly for small language models
(SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs
(LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR),
which circumvents key limitations of text-based TR (T-TR), notably in numerical
reasoning, by generating executable programs. However, applying P-TR to SLMs
introduces two challenges: (i) vulnerability to heterogeneity in table layouts,
and (ii) inconsistency in reasoning due to limited code generation capability.
We propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1
introduces an innovative self-supervised learning task, Layout Transformation
Inference, to improve tabular layout generalization from a programmatic view.
Stage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization,
enhancing P-TR consistency while allowing dynamic fallback to T-TR when needed.
Experiments on four TR benchmarks demonstrate that Table-r1 outperforms all
SLM-based methods, achieving at least a 15% accuracy improvement over the base
model (LLaMA-8B) across all datasets and reaching performance competitive with
LLMs.

</details>


### [457] [Machine learning for in-situ composition mapping in a self-driving magnetron sputtering system](https://arxiv.org/pdf/2506.05999)
*Sanna Jarl, Jens Sjölund, Robert J. W. Frost, Anders Holst, Jonathan J. S. Scragg*

Main category: cs.LG

TL;DR: The paper presents a self-driving lab (SDL) using magnetron co-sputtering and ML to rapidly predict composition maps for multi-element thin films, avoiding time-consuming ex-situ analysis.


<details>
  <summary>Details</summary>
Motivation: SDL potential in materials discovery is limited in thin film science due to reliance on solution-based methods. This work addresses the gap by enabling automation for inorganic materials.

Method: Uses combinatorial frameworks, in-situ ML-driven analysis with quartz-crystal microbalance sensors, and Gaussian processes (GPs) for active learning. BALM (Bayesian active learning MacKay) optimizes the process.

Result: Achieves accurate composition predictions in 10 experiments per source, verified experimentally. Avoids extensive calibration, increasing throughput.

Conclusion: Demonstrates ML-guided SDLs can accelerate materials exploration by automating and optimizing thin film synthesis.

Abstract: Self-driving labs (SDLs), employing automation and machine learning (ML) to
accelerate experimental procedures, have enormous potential in the discovery of
new materials. However, in thin film science, SDLs are mainly restricted to
solution-based synthetic methods which are easier to automate but cannot access
the broad chemical space of inorganic materials. This work presents an SDL
based on magnetron co-sputtering. We are using combinatorial frameworks,
obtaining accurate composition maps on multi-element, compositionally graded
thin films. This normally requires time-consuming ex-situ analysis prone to
systematic errors. We present a rapid and calibration-free in-situ, ML driven
approach to produce composition maps for arbitrary source combinations and
sputtering conditions. We develop a method to predict the composition
distribution in a multi-element combinatorial thin film, using in-situ
measurements from quartz-crystal microbalance sensors placed in a sputter
chamber. For a given source, the sensor readings are learned as a function of
the sputtering pressure and magnetron power, through active learning using
Gaussian processes (GPs). The final GPs are combined with a geometric model of
the deposition flux distribution in the chamber, which allows interpolation of
the deposition rates from each source, at any position across the sample. We
investigate several acquisition functions for the ML procedure. A fully
Bayesian GP - BALM (Bayesian active learning MacKay) - achieved the best
performance, learning the deposition rates for a single source in 10
experiments. Prediction accuracy for co-sputtering composition distributions
was verified experimentally. Our framework dramatically increases throughput by
avoiding the need for extensive characterisation or calibration, thus
demonstrating the potential of ML-guided SDLs to accelerate materials
exploration.

</details>


### [458] [LaDEEP: A Deep Learning-based Surrogate Model for Large Deformation of Elastic-Plastic Solids](https://arxiv.org/pdf/2506.06001)
*Shilong Tao, Zhe Feng, Haonan Sun, Zhanxing Zhu, Yunhuai Liu*

Main category: cs.LG

TL;DR: LaDEEP, a deep learning model, accelerates large deformation simulations for elastic-plastic solids, outperforming traditional methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between accuracy and efficiency in classical solvers for elastic-plastic solids, especially in complex scenarios like stretch bending.

Method: Uses a token sequence for partitioned regions and a two-stage Transformer-based module to predict deformation.

Result: Achieves five magnitudes faster speed than finite element methods with comparable accuracy and 20.47% improvement over other deep learning baselines.

Conclusion: LaDEEP is effective for real-world industrial applications, demonstrating superior performance in accuracy and efficiency.

Abstract: Scientific computing for large deformation of elastic-plastic solids is
critical for numerous real-world applications. Classical numerical solvers rely
primarily on local discrete linear approximation and are constrained by an
inherent trade-off between accuracy and efficiency. Recently, deep learning
models have achieved impressive progress in solving the continuum mechanism.
While previous models have explored various architectures and constructed
coefficient-solution mappings, they are designed for general instances without
considering specific problem properties and hard to accurately handle with
complex elastic-plastic solids involving contact, loading and unloading. In
this work, we take stretch bending, a popular metal fabrication technique, as
our case study and introduce LaDEEP, a deep learning-based surrogate model for
\textbf{La}rge \textbf{De}formation of \textbf{E}lastic-\textbf{P}lastic
Solids. We encode the partitioned regions of the involved slender solids into a
token sequence to maintain their essential order property. To characterize the
physical process of the solid deformation, a two-stage Transformer-based module
is designed to predict the deformation with the sequence of tokens as input.
Empirically, LaDEEP achieves five magnitudes faster speed than finite element
methods with a comparable accuracy, and gains 20.47\% relative improvement on
average compared to other deep learning baselines. We have also deployed our
model into a real-world industrial production system, and it has shown
remarkable performance in both accuracy and efficiency.

</details>


### [459] [What Really is a Member? Discrediting Membership Inference via Poisoning](https://arxiv.org/pdf/2506.06003)
*Neal Mangaokar, Ashish Hooda, Zhuohang Li, Bradley A. Malin, Kassem Fawaz, Somesh Jha, Atul Prakash, Amrita Roy Chowdhury*

Main category: cs.LG

TL;DR: Membership inference tests remain unreliable even with relaxed definitions, as training data poisoning can degrade their performance below random chance.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the unreliability of membership inference tests under relaxed definitions and highlight their vulnerability to poisoning attacks.

Method: Theoretical analysis of the trade-off between test accuracy and robustness to poisoning, followed by empirical validation of a poisoning attack.

Result: Poisoning attacks can degrade membership inference test performance significantly below random chance.

Conclusion: Membership inference tests are unreliable and vulnerable to poisoning, revealing a trade-off between accuracy and robustness.

Abstract: Membership inference tests aim to determine whether a particular data point
was included in a language model's training set. However, recent works have
shown that such tests often fail under the strict definition of membership
based on exact matching, and have suggested relaxing this definition to include
semantic neighbors as members as well. In this work, we show that membership
inference tests are still unreliable under this relaxation - it is possible to
poison the training dataset in a way that causes the test to produce incorrect
predictions for a target point. We theoretically reveal a trade-off between a
test's accuracy and its robustness to poisoning. We also present a concrete
instantiation of this poisoning attack and empirically validate its
effectiveness. Our results show that it can degrade the performance of existing
tests to well below random.

</details>


### [460] [The Lock-in Hypothesis: Stagnation by Algorithm](https://arxiv.org/pdf/2506.06166)
*Tianyi Alex Qiu, Zhonghao He, Tejasveer Chugh, Max Kleiman-Weiner*

Main category: cs.LG

TL;DR: The paper explores how LLMs create a feedback loop with humans, reinforcing beliefs and reducing diversity, supported by simulations and real-world data.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs entrench human beliefs, potentially locking in false beliefs and reducing diversity.

Method: Formal hypothesis testing using agent-based LLM simulations and real-world GPT usage data.

Result: Sudden, sustained drops in diversity after new GPT releases, confirming the feedback loop hypothesis.

Conclusion: The human-AI feedback loop reinforces existing beliefs, risking diversity loss and false belief lock-in.

Abstract: The training and deployment of large language models (LLMs) create a feedback
loop with human users: models learn human beliefs from data, reinforce these
beliefs with generated content, reabsorb the reinforced beliefs, and feed them
back to users again and again. This dynamic resembles an echo chamber. We
hypothesize that this feedback loop entrenches the existing values and beliefs
of users, leading to a loss of diversity and potentially the lock-in of false
beliefs. We formalize this hypothesis and test it empirically with agent-based
LLM simulations and real-world GPT usage data. Analysis reveals sudden but
sustained drops in diversity after the release of new GPT iterations,
consistent with the hypothesized human-AI feedback loop. Code and data
available at https://thelockinhypothesis.com

</details>


### [461] [LightGTS: A Lightweight General Time Series Forecasting Model](https://arxiv.org/pdf/2506.06005)
*Yihang Wang, Yuying Qiu, Peng Chen, Yang Shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo*

Main category: cs.LG

TL;DR: LightGTS is a lightweight time series forecasting model that uses consistent periodical modeling and achieves state-of-the-art performance with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models for time series forecasting are computationally heavy and inefficient for resource-constrained scenarios. LightGTS addresses this by focusing on periodicity.

Method: LightGTS employs Periodical Tokenization to extract consistent periodic patterns and Periodical Parallel Decoding to leverage historical tokens for forecasting.

Result: LightGTS outperforms existing models on 9 benchmarks in zero-shot and full-shot settings with better efficiency.

Conclusion: LightGTS demonstrates that lightweight models can achieve superior forecasting performance by leveraging inherent periodicity in time series.

Abstract: Existing works on general time series forecasting build foundation models
with heavy model parameters through large-scale multi-source pre-training.
These models achieve superior generalization ability across various datasets at
the cost of significant computational burdens and limitations in
resource-constrained scenarios. This paper introduces LightGTS, a lightweight
general time series forecasting model designed from the perspective of
consistent periodical modeling. To handle diverse scales and intrinsic periods
in multi-source pre-training, we introduce Periodical Tokenization, which
extracts consistent periodic patterns across different datasets with varying
scales. To better utilize the periodicity in the decoding process, we further
introduce Periodical Parallel Decoding, which leverages historical tokens to
improve forecasting. Based on the two techniques above which fully leverage the
inductive bias of periods inherent in time series, LightGTS uses a lightweight
model to achieve outstanding performance on general time series forecasting. It
achieves state-of-the-art forecasting performance on 9 real-world benchmarks in
both zero-shot and full-shot settings with much better efficiency compared with
existing time series foundation models.

</details>


### [462] [Corrector Sampling in Language Models](https://arxiv.org/pdf/2506.06215)
*Itai Gat, Neta Shaul, Uriel Singer, Yaron Lipman*

Main category: cs.LG

TL;DR: Proposes Resample-Previous-Tokens (RPT), a method to reduce error accumulation in autoregressive models by iteratively revisiting and replacing past tokens, improving performance by ~10% on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models suffer from fixed, irrevocable left-to-right token generation, leading to error accumulation.

Method: Introduces RPT, which revisits and replaces tokens in a window of previously generated text, compatible with existing models.

Result: Fine-tuning an 8B parameter model with RPT for 100B tokens yielded ~10% relative improvements on reasoning and coding benchmarks.

Conclusion: RPT effectively mitigates error accumulation in autoregressive models without compromising speed or prediction quality.

Abstract: Autoregressive language models accumulate errors due to their fixed,
irrevocable left-to-right token generation. To address this, we propose a new
sampling method called Resample-Previous-Tokens (RPT). RPT mitigates error
accumulation by iteratively revisiting and potentially replacing tokens in a
window of previously generated text. This method can be integrated into
existing autoregressive models, preserving their next-token-prediction quality
and speed. Fine-tuning a pretrained 8B parameter model with RPT for only 100B
resulted in ~10% relative improvements on reasoning and coding benchmarks
compared to the standard sampling.

</details>


### [463] [Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems](https://arxiv.org/pdf/2506.06021)
*Shilong Tao, Zhe Feng, Haonan Sun, Zhanxing Zhu, Yunhuai Liu*

Main category: cs.LG

TL;DR: Unisoma introduces explicit modeling for multi-solid systems using a Transformer-based approach, outperforming implicit methods by capturing physical interactions directly.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with complex interactions in multi-solid systems due to implicit modeling. Unisoma addresses this by explicitly representing deformation factors.

Method: Unisoma uses structured modules, contact modules, and an adaptive interaction allocation mechanism to model solid deformation via triplet relationships.

Result: Unisoma achieves state-of-the-art performance on seven datasets and two complex multi-solid tasks.

Conclusion: Explicit modeling is superior for multi-solid systems, as demonstrated by Unisoma's flexibility and accuracy.

Abstract: Multi-solid systems are foundational to a wide range of real-world
applications, yet modeling their complex interactions remains challenging.
Existing deep learning methods predominantly rely on implicit modeling, where
the factors influencing solid deformation are not explicitly represented but
are instead indirectly learned. However, as the number of solids increases,
these methods struggle to accurately capture intricate physical interactions.
In this paper, we introduce a novel explicit modeling paradigm that
incorporates factors influencing solid deformation through structured modules.
Specifically, we present Unisoma, a unified and flexible Transformer-based
model capable of handling variable numbers of solids. Unisoma directly captures
physical interactions using contact modules and adaptive interaction allocation
mechanism, and learns the deformation through a triplet relationship. Compared
to implicit modeling techniques, explicit modeling is more well-suited for
multi-solid systems with diverse coupling patterns, as it enables detailed
treatment of each solid while preventing information blending and confusion.
Experimentally, Unisoma achieves consistent state-of-the-art performance across
seven well-established datasets and two complex multi-solid tasks. Code is
avaiable at \href{this link}{https://github.com/therontau0054/Unisoma}.

</details>


### [464] [Do-PFN: In-Context Learning for Causal Effect Estimation](https://arxiv.org/pdf/2506.06039)
*Jake Robertson, Arik Reuter, Siyuan Guo, Noah Hollmann, Frank Hutter, Bernhard Schölkopf*

Main category: cs.LG

TL;DR: PFNs pre-trained on synthetic causal data can estimate causal effects accurately without needing the true causal graph.


<details>
  <summary>Details</summary>
Motivation: Existing causal effect methods require interventional data or strong assumptions, limiting real-world use. PFNs' success in tabular ML suggests potential for causal tasks.

Method: Pre-train PFNs on diverse synthetic causal data to predict interventional outcomes from observational data.

Result: Experiments show accurate causal effect estimation without knowing the causal graph. Ablations confirm scalability and robustness.

Conclusion: PFNs transfer well to causal effect estimation, offering a flexible, assumption-light approach.

Abstract: Estimation of causal effects is critical to a range of scientific
disciplines. Existing methods for this task either require interventional data,
knowledge about the ground truth causal graph, or rely on assumptions such as
unconfoundedness, restricting their applicability in real-world settings. In
the domain of tabular machine learning, Prior-data fitted networks (PFNs) have
achieved state-of-the-art predictive performance, having been pre-trained on
synthetic data to solve tabular prediction problems via in-context learning. To
assess whether this can be transferred to the harder problem of causal effect
estimation, we pre-train PFNs on synthetic data drawn from a wide variety of
causal structures, including interventions, to predict interventional outcomes
given observational data. Through extensive experiments on synthetic case
studies, we show that our approach allows for the accurate estimation of causal
effects without knowledge of the underlying causal graph. We also perform
ablation studies that elucidate Do-PFN's scalability and robustness across
datasets with a variety of causal characteristics.

</details>


### [465] [Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics](https://arxiv.org/pdf/2506.06045)
*Tobias Würth, Niklas Freymuth, Gerhard Neumann, Luise Kärger*

Main category: cs.LG

TL;DR: ROBIN, a novel learned simulator, integrates Rolling Diffusion and Hierarchical Graph Neural Networks to improve accuracy and efficiency in simulating physical systems, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based learned simulators struggle with global phenomena and error accumulation due to local message passing and direct next-step prediction.

Method: ROBIN combines Rolling Diffusion (parallelized inference) and a Hierarchical Graph Neural Network (multiscale message passing) to capture local and global dynamics.

Result: ROBIN achieves state-of-the-art accuracy on 2D/3D solid mechanics benchmarks and reduces inference time significantly.

Conclusion: ROBIN addresses key limitations of current simulators, offering improved performance and efficiency for complex physical simulations.

Abstract: Graph-based learned simulators have emerged as a promising approach for
simulating physical systems on unstructured meshes, offering speed and
generalization across diverse geometries. However, they often struggle with
capturing global phenomena, such as bending or long-range correlations, and
suffer from error accumulation over long rollouts due to their reliance on
local message passing and direct next-step prediction. We address these
limitations by introducing the Rolling Diffusion-Batched Inference Network
(ROBIN), a novel learned simulator that integrates two key innovations: (i)
Rolling Diffusion, a parallelized inference scheme that amortizes the cost of
diffusion-based refinement across physical time steps by overlapping denoising
steps across a temporal window. (ii) A Hierarchical Graph Neural Network built
on algebraic multigrid coarsening, enabling multiscale message passing across
different mesh resolutions. This architecture, implemented via
Algebraic-hierarchical Message Passing Networks, captures both fine-scale local
dynamics and global structural effects critical for phenomena like beam bending
or multi-body contact. We validate ROBIN on challenging 2D and 3D solid
mechanics benchmarks involving geometric, material, and contact nonlinearities.
ROBIN achieves state-of-the-art accuracy on all tasks, substantially
outperforming existing next-step learned simulators while reducing inference
time by up to an order of magnitude compared to standard diffusion simulators.

</details>


### [466] [Towards an Explainable Comparison and Alignment of Feature Embeddings](https://arxiv.org/pdf/2506.06231)
*Mohammad Jalali, Bahar Dibaei Nia, Farzan Farnia*

Main category: cs.LG

TL;DR: The paper introduces SPEC, a framework for interpretable comparison of feature embeddings by analyzing clustering mismatches, with scalable implementation and alignment optimization.


<details>
  <summary>Details</summary>
Motivation: Current comparisons of feature embeddings focus on numerical performance, lacking interpretability. SPEC aims to provide a deeper analysis of clustering differences.

Method: SPEC uses kernel matrices from embeddings and eigendecomposition to detect clustering mismatches. It includes a scalable implementation and an optimization problem for alignment.

Result: SPEC successfully compares and aligns embeddings on large-scale datasets like ImageNet and MS-COCO.

Conclusion: SPEC offers a scalable, interpretable method for embedding comparison and alignment, enhancing understanding of clustering differences.

Abstract: While several feature embedding models have been developed in the literature,
comparisons of these embeddings have largely focused on their numerical
performance in classification-related downstream applications. However, an
interpretable comparison of different embeddings requires identifying and
analyzing mismatches between sample groups clustered within the embedding
spaces. In this work, we propose the \emph{Spectral Pairwise Embedding
Comparison (SPEC)} framework to compare embeddings and identify their
differences in clustering a reference dataset. Our approach examines the kernel
matrices derived from two embeddings and leverages the eigendecomposition of
the difference kernel matrix to detect sample clusters that are captured
differently by the two embeddings. We present a scalable implementation of this
kernel-based approach, with computational complexity that grows linearly with
the sample size. Furthermore, we introduce an optimization problem using this
framework to align two embeddings, ensuring that clusters identified in one
embedding are also captured in the other model. We provide numerical results
demonstrating the SPEC's application to compare and align embeddings on
large-scale datasets such as ImageNet and MS-COCO. The code is available at
[https://github.com/mjalali/embedding-comparison](github.com/mjalali/embedding-comparison).

</details>


### [467] [System-Aware Unlearning Algorithms: Use Lesser, Forget Faster](https://arxiv.org/pdf/2506.06073)
*Linda Lu, Ayush Sekhari, Karthik Sridharan*

Main category: cs.LG

TL;DR: The paper proposes a new definition, system-aware unlearning, to address the inefficiencies of the gold standard unlearning definition by focusing on realistic attackers. It introduces an algorithm for linear classification and generalizes it, analyzing tradeoffs in deletion capacity, accuracy, memory, and computation.


<details>
  <summary>Details</summary>
Motivation: The stringent gold standard definition of unlearning is inefficient and unrealistic for practical attackers. The work aims to provide a more practical and efficient alternative.

Method: Proposes system-aware unlearning, a selective sampling-based approach for linear classification, and generalizes it for broader function classes.

Result: The method theoretically analyzes tradeoffs between deletion capacity, accuracy, memory, and computation time, offering a more efficient solution.

Conclusion: System-aware unlearning provides a practical and efficient alternative to the gold standard, balancing security and performance.

Abstract: Machine unlearning addresses the problem of updating a machine learning
model/system trained on a dataset $S$ so that the influence of a set of
deletion requests $U \subseteq S$ on the unlearned model is minimized. The gold
standard definition of unlearning demands that the updated model, after
deletion, be nearly identical to the model obtained by retraining. This
definition is designed for a worst-case attacker (one who can recover not only
the unlearned model but also the remaining data samples, i.e., $S \setminus
U$). Such a stringent definition has made developing efficient unlearning
algorithms challenging. However, such strong attackers are also unrealistic. In
this work, we propose a new definition, system-aware unlearning, which aims to
provide unlearning guarantees against an attacker that can at best only gain
access to the data stored in the system for learning/unlearning requests and
not all of $S\setminus U$. With this new definition, we use the simple
intuition that if a system can store less to make its learning/unlearning
updates, it can be more secure and update more efficiently against a
system-aware attacker. Towards that end, we present an exact system-aware
unlearning algorithm for linear classification using a selective sampling-based
approach, and we generalize the method for classification with general function
classes. We theoretically analyze the tradeoffs between deletion capacity,
accuracy, memory, and computation time.

</details>


### [468] [Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU](https://arxiv.org/pdf/2506.06095)
*Wenhao Dai, Haodong Deng, Mengfei Rong, Xinyu Yang, Hongyu Liu, Fangxin Liu, Hailong Yang, Weifeng Liu, Qingxiao Sun*

Main category: cs.LG

TL;DR: STOF is a framework optimizing Sparse Transformer via flexible masking and operator fusion on GPU, achieving up to 1.7x speedup in MHA computation and 1.5x in end-to-end inference.


<details>
  <summary>Details</summary>
Motivation: Previous works neglect performance optimization of sparse Transformer and fail to adapt to varied sequence lengths due to rigid rule-based mechanisms.

Method: STOF unifies storage and kernel implementation for multi-head attention, maps fusion schemes to templates, and uses a two-stage search engine for optimal parameters.

Result: STOF achieves 1.7x speedup in MHA computation and 1.5x in end-to-end inference compared to state-of-the-art.

Conclusion: STOF effectively addresses performance gaps in sparse Transformer optimization through flexible masking and operator fusion.

Abstract: Large language models are popular around the world due to their powerful
understanding capabilities. As the core component of LLMs, accelerating
Transformer through parallelization has gradually become a hot research topic.
Mask layers introduce sparsity into Transformer to reduce calculations.
However, previous works rarely focus on the performance optimization of sparse
Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of
mixed-type operators and fail to adapt to various sequence lengths. To address
the above problems, we propose STOF, a framework that incorporates
optimizations for Sparse Transformer via flexible masking and operator fusion
on GPU. We firstly unify the storage format and kernel implementation for the
multi-head attention. Then, we map fusion schemes to compilation templates and
determine the optimal parameter setting through a two-stage search engine. The
experimental results show that compared to the state-of-the-art work, STOF
achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end
inference.

</details>


### [469] [Synthetic Tabular Data: Methods, Attacks and Defenses](https://arxiv.org/pdf/2506.06108)
*Graham Cormode, Samuel Maddock, Enayat Ullah, Shripad Gade*

Main category: cs.LG

TL;DR: This survey explores synthetic data generation, focusing on tabular data, covering probabilistic graphical models and deep learning methods, while addressing privacy concerns and limitations.


<details>
  <summary>Details</summary>
Motivation: To provide an overview of synthetic data generation as a privacy-preserving alternative to sensitive datasets, highlighting advancements and methodologies.

Method: Covers paradigms like probabilistic graphical models and deep learning, with a technical deep-dive into these methodologies.

Result: Identifies limitations and potential attacks on synthetic data, emphasizing privacy risks.

Conclusion: Highlights extensions and open problems in synthetic data generation, suggesting future research directions.

Abstract: Synthetic data is often positioned as a solution to replace sensitive
fixed-size datasets with a source of unlimited matching data, freed from
privacy concerns. There has been much progress in synthetic data generation
over the last decade, leveraging corresponding advances in machine learning and
data analytics. In this survey, we cover the key developments and the main
concepts in tabular synthetic data generation, including paradigms based on
probabilistic graphical models and on deep learning. We provide background and
motivation, before giving a technical deep-dive into the methodologies. We also
address the limitations of synthetic data, by studying attacks that seek to
retrieve information about the original sensitive data. Finally, we present
extensions and open problems in this area.

</details>


### [470] [Distillation Robustifies Unlearning](https://arxiv.org/pdf/2506.06278)
*Bruce W. Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex Cloud, Alexander Matt Turner*

Main category: cs.LG

TL;DR: Current LLM unlearning methods lack robustness and can be reverted easily. The paper introduces UNDO, a scalable distillation-based method, to robustify unlearning with a tunable compute-robustness tradeoff.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods are fragile and reversible, even with idealized approaches, highlighting the need for more robust solutions.

Method: Proposes UNDO, a method that distills an unlearned model into a partially noised copy, balancing compute cost and robustness.

Result: UNDO matches the robustness of retraining from scratch with perfect filtering, using less compute and minimal labeled data. It also performs well on the WMDP benchmark.

Conclusion: Distillation-based unlearning (UNDO) offers a practical and robust solution for capability removal, leveraging existing distillation practices.

Abstract: Current LLM unlearning methods are not robust: they can be reverted easily
with a few steps of finetuning. This is true even for the idealized unlearning
method of training to imitate an oracle model that was never exposed to
unwanted information, suggesting that output-based finetuning is insufficient
to achieve robust unlearning. In a similar vein, we find that training a
randomly initialized student to imitate an unlearned model transfers desired
behaviors while leaving undesired capabilities behind. In other words,
distillation robustifies unlearning. Building on this insight, we propose
Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an
unlearned model into a partially noised copy of itself. UNDO introduces a
tunable tradeoff between compute cost and robustness, establishing a new Pareto
frontier on synthetic language and arithmetic tasks. At its strongest setting,
UNDO matches the robustness of a model retrained from scratch with perfect data
filtering while using only 60-80% of the compute and requiring only 0.01% of
the pretraining data to be labeled. We also show that UNDO robustifies
unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)
benchmark. Since distillation is widely used in practice, incorporating an
unlearning step beforehand offers a convenient path to robust capability
removal.

</details>


### [471] [Scalable unsupervised feature selection via weight stability](https://arxiv.org/pdf/2506.06114)
*Xudong Zhang, Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: The paper introduces Minkowski weighted $k$-means++ for unsupervised feature selection, proposing two algorithms (FS-MWK++ and SFS-MWK++) that outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Improving clustering performance in high-dimensional data by addressing irrelevant features that obscure meaningful structure.

Method: Novel initialization strategy (Minkowski weighted $k$-means++) and two feature selection algorithms (FS-MWK++ and SFS-MWK++) leveraging feature relevance estimates and Minkowski exponents.

Result: Theoretical guarantees and experiments show the proposed methods consistently outperform alternatives.

Conclusion: The introduced methods effectively enhance unsupervised feature selection for clustering in high-dimensional data.

Abstract: Unsupervised feature selection is critical for improving clustering
performance in high-dimensional data, where irrelevant features can obscure
meaningful structure. In this work, we introduce the Minkowski weighted
$k$-means++, a novel initialisation strategy for the Minkowski Weighted
$k$-means. Our initialisation selects centroids probabilistically using feature
relevance estimates derived from the data itself. Building on this, we propose
two new feature selection algorithms, FS-MWK++, which aggregates feature
weights across a range of Minkowski exponents to identify stable and
informative features, and SFS-MWK++, a scalable variant based on subsampling.
We support our approach with a theoretical guarantee under mild assumptions and
extensive experiments showing that our methods consistently outperform existing
alternatives.

</details>


### [472] [Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias](https://arxiv.org/pdf/2506.06280)
*Yuanzhe Hu, Kinshuk Goel, Vlad Killiakov, Yaoqing Yang*

Main category: cs.LG

TL;DR: FARMS (Fixed-Aspect-Ratio Matrix Subsampling) mitigates bias in eigenspectrum analysis of DNNs by normalizing weight matrices with fixed aspect ratios, improving model diagnosis and hyperparameter assignment.


<details>
  <summary>Details</summary>
Motivation: The challenge of aspect ratio bias in eigenspectrum analysis of DNN weight matrices affects model diagnosis and hyperparameter tuning.

Method: Proposes FARMS, which subsamples submatrices with fixed aspect ratios to normalize weight matrices and measure average ESD for unbiased heavytailness metrics.

Result: FARMS improves eigenspectrum analysis accuracy and hyperparameter assignment across CV, SciML, and LLM domains, reducing LLaMA-7B perplexity by 17.3%.

Conclusion: FARMS effectively addresses aspect ratio bias, enhancing the reliability of eigenspectrum-based DNN diagnosis and optimization.

Abstract: Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight
matrices has been an active area of research in recent years. At a high level,
eigenspectrum analysis of DNNs involves measuring the heavytailness of the
empirical spectral densities (ESD) of weight matrices. It provides insight into
how well a model is trained and can guide decisions on assigning better
layer-wise training hyperparameters. In this paper, we address a challenge
associated with such eigenspectrum methods: the impact of the aspect ratio of
weight matrices on estimated heavytailness metrics. We demonstrate that
matrices of varying sizes (and aspect ratios) introduce a non-negligible bias
in estimating heavytailness metrics, leading to inaccurate model diagnosis and
layer-wise hyperparameter assignment. To overcome this challenge, we propose
FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the
weight matrices by subsampling submatrices with a fixed aspect ratio. Instead
of measuring the heavytailness of the original ESD, we measure the average ESD
of these subsampled submatrices. We show that measuring the heavytailness of
these submatrices with the fixed aspect ratio can effectively mitigate the
aspect ratio bias. We validate our approach across various optimization
techniques and application domains that involve eigenspectrum analysis of
weights, including image classification in computer vision (CV) models,
scientific machine learning (SciML) model training, and large language model
(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly
improves the accuracy of eigenspectrum analysis while enabling more effective
layer-wise hyperparameter assignment in these application domains. In one of
the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model
by 17.3% when compared with the state-of-the-art method.

</details>


### [473] [Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library](https://arxiv.org/pdf/2506.06122)
*Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, Zichen Liu, Haizhou Zhao, Dakai An, Lunxi Cao, Qiyang Cao, Wanxi Deng, Feilei Du, Yiliang Gu, Jiahe Li, Xiang Li, Mingjie Liu, Yijia Luo, Zihe Liu, Yadao Wang, Pei Wang, Tianyuan Wu, Yanan Wu, Yuheng Zhao, Shuaibing Zhao, Jin Yang, Siran Yang, Yingshui Tan, Huimin Yi, Yuchi Xu, Yujin Yuan, Xingyao Zhang, Lin Qu, Wenbo Su, Wei Wang, Jiamang Wang, Bo Zheng*

Main category: cs.LG

TL;DR: ROLL is a scalable, user-friendly library for large-scale reinforcement learning, catering to tech pioneers, developers, and researchers with efficient training, flexible workflows, and agile experimentation.


<details>
  <summary>Details</summary>
Motivation: To address the needs of diverse user groups (tech pioneers, developers, researchers) by providing an efficient, scalable, and flexible solution for large-scale reinforcement learning.

Method: ROLL uses a single-controller architecture, parallel strategy, data transfer modules, rollout scheduler, environment worker, reward worker, and AutoDeviceMapping for resource allocation.

Result: The library enables cost-effective, fault-tolerant training, flexible workflow control, and rapid experimentation for reinforcement learning.

Conclusion: ROLL successfully meets the needs of its target users by combining efficiency, scalability, and flexibility in large-scale reinforcement learning.

Abstract: We introduce ROLL, an efficient, scalable, and user-friendly library designed
for Reinforcement Learning Optimization for Large-scale Learning. ROLL caters
to three primary user groups: tech pioneers aiming for cost-effective,
fault-tolerant large-scale training, developers requiring flexible control over
training workflows, and researchers seeking agile experimentation. ROLL is
built upon several key modules to serve these user groups effectively. First, a
single-controller architecture combined with an abstraction of the parallel
worker simplifies the development of the training pipeline. Second, the
parallel strategy and data transfer modules enable efficient and scalable
training. Third, the rollout scheduler offers fine-grained management of each
sample's lifecycle during the rollout stage. Fourth, the environment worker and
reward worker support rapid and flexible experimentation with agentic RL
algorithms and reward designs. Finally, AutoDeviceMapping allows users to
assign resources to different models flexibly across various stages.

</details>


### [474] [Flow-Attentional Graph Neural Networks](https://arxiv.org/pdf/2506.06127)
*Pascal Plettenberg, Dominik Köhler, Bernhard Sick, Josephine M. Thomas*

Main category: cs.LG

TL;DR: Proposes flow attention for GNNs to adhere to Kirchhoff's first law, improving performance on flow-related graph tasks.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs ignore conservation laws in flow graphs (e.g., power grids, traffic), limiting performance.

Method: Introduces flow attention, adapting graph attention to satisfy Kirchhoff's first law, and analyzes its expressivity.

Result: Flow attention outperforms standard attention on graph-level classification and regression in flow datasets (circuits, power grids).

Conclusion: Flow attention enhances GNN performance by incorporating physical flow constraints, enabling better discrimination of non-isomorphic graphs.

Abstract: Graph Neural Networks (GNNs) have become essential for learning from
graph-structured data. However, existing GNNs do not consider the conservation
law inherent in graphs associated with a flow of physical resources, such as
electrical current in power grids or traffic in transportation networks, which
can lead to reduced model performance. To address this, we propose flow
attention, which adapts existing graph attention mechanisms to satisfy
Kirchhoff\'s first law. Furthermore, we discuss how this modification
influences the expressivity and identify sets of non-isomorphic graphs that can
be discriminated by flow attention but not by standard attention. Through
extensive experiments on two flow graph datasets (electronic circuits and power
grids), we demonstrate that flow attention enhances the performance of
attention-based GNNs on both graph-level classification and regression tasks.

</details>


### [475] [Gradient Similarity Surgery in Multi-Task Deep Learning](https://arxiv.org/pdf/2506.06130)
*Thomas Borsani, Andrea Rosani, Giuseppe Nicosia, Giuseppe Di Fatta*

Main category: cs.LG

TL;DR: The paper introduces SAM-GS, a gradient surgery method for multi-task deep learning, addressing conflicting gradients by using gradient magnitude similarity to improve optimization.


<details>
  <summary>Details</summary>
Motivation: Multi-task learning (MTL) faces challenges with conflicting gradients during backpropagation, which can degrade training. The paper aims to solve this by proposing a novel gradient surgery method.

Method: The SAM-GS method adjusts gradient trajectories using gradient magnitude similarity, equalization, and momentum modulation to handle conflicting gradients.

Result: Experiments show SAM-GS effectively improves optimization in synthetic and MTL benchmark tasks.

Conclusion: SAM-GS successfully regularizes gradient aggregation in multi-task deep learning, enhancing the learning process.

Abstract: The multi-task learning ($MTL$) paradigm aims to simultaneously learn
multiple tasks within a single model capturing higher-level, more general
hidden patterns that are shared by the tasks. In deep learning, a significant
challenge in the backpropagation training process is the design of advanced
optimisers to improve the convergence speed and stability of the gradient
descent learning rule. In particular, in multi-task deep learning ($MTDL$) the
multitude of tasks may generate potentially conflicting gradients that would
hinder the concurrent convergence of the diverse loss functions. This challenge
arises when the gradients of the task objectives have either different
magnitudes or opposite directions, causing one or a few to dominate or to
interfere with each other, thus degrading the training process. Gradient
surgery methods address the problem explicitly dealing with conflicting
gradients by adjusting the overall gradient trajectory. This work introduces a
novel gradient surgery method, the Similarity-Aware Momentum Gradient Surgery
(SAM-GS), which provides an effective and scalable approach based on a gradient
magnitude similarity measure to guide the optimisation process. The SAM-GS
surgery adopts gradient equalisation and modulation of the first-order
momentum. A series of experimental tests have shown the effectiveness of SAM-GS
on synthetic problems and $MTL$ benchmarks. Gradient magnitude similarity plays
a crucial role in regularising gradient aggregation in $MTDL$ for the
optimisation of the learning process.

</details>


### [476] [carps: A Framework for Comparing N Hyperparameter Optimizers on M Benchmarks](https://arxiv.org/pdf/2506.06143)
*Carolin Benjamins, Helena Graf, Sarah Segel, Difan Deng, Tim Ruhkopf, Leona Hennig, Soham Basu, Neeratyoy Mallik, Edward Bergman, Deyao Chen, François Clément, Matthias Feurer, Katharina Eggensperger, Frank Hutter, Carola Doerr, Marius Lindauer*

Main category: cs.LG

TL;DR: CARPS is a benchmark framework for evaluating hyperparameter optimization (HPO) methods, offering a large library of tasks and optimizers, with tools for efficient subset selection and analysis.


<details>
  <summary>Details</summary>
Motivation: To standardize and simplify the evaluation of HPO methods by providing a comprehensive, lightweight framework for benchmarking and prototyping.

Method: CARPS integrates 3,336 tasks from 5 benchmark collections and 28 optimizer variants. It uses a star discrepancy measure to select representative task subsets (10-30 per type) for efficient evaluation.

Result: The framework provides the largest HPO evaluation library to date, with initial baseline results for future comparisons.

Conclusion: CARPS advances HPO standardization by enabling efficient, scalable benchmarking and comparison of optimization methods.

Abstract: Hyperparameter Optimization (HPO) is crucial to develop well-performing
machine learning models. In order to ease prototyping and benchmarking of HPO
methods, we propose carps, a benchmark framework for Comprehensive Automated
Research Performance Studies allowing to evaluate N optimizers on M benchmark
tasks. In this first release of carps, we focus on the four most important
types of HPO task types: blackbox, multi-fidelity, multi-objective and
multi-fidelity-multi-objective. With 3 336 tasks from 5 community benchmark
collections and 28 variants of 9 optimizer families, we offer the biggest go-to
library to date to evaluate and compare HPO methods. The carps framework relies
on a purpose-built, lightweight interface, gluing together optimizers and
benchmark tasks. It also features an analysis pipeline, facilitating the
evaluation of optimizers on benchmarks. However, navigating a huge number of
tasks while developing and comparing methods can be computationally infeasible.
To address this, we obtain a subset of representative tasks by minimizing the
star discrepancy of the subset, in the space spanned by the full set. As a
result, we propose an initial subset of 10 to 30 diverse tasks for each task
type, and include functionality to re-compute subsets as more benchmarks become
available, enabling efficient evaluations. We also establish a first set of
baseline results on these tasks as a measure for future comparisons. With carps
(https://www.github.com/automl/CARP-S), we make an important step in the
standardization of HPO evaluation.

</details>


### [477] [ENMA: Tokenwise Autoregression for Generative Neural PDE Operators](https://arxiv.org/pdf/2506.06158)
*Armand Kassaï Koupaï, Lise Le Boudec, Louis Serrano, Patrick Gallinari*

Main category: cs.LG

TL;DR: ENMA is a generative neural operator for solving time-dependent parametric PDEs, using a masked autoregressive transformer and flow matching loss to predict dynamics in latent space.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of solving parametric PDEs with uncertain or incomplete data, especially for generalization across diverse physical parameters and dynamics.

Method: ENMA employs a generative masked autoregressive transformer with flow matching loss, encoding irregular spatial observations into latent representations via attention and convolutional encoders.

Result: ENMA provides robust, adaptable predictions for new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs.

Conclusion: ENMA offers a scalable and generalizable solution for modeling spatio-temporal dynamics in uncertain or incomplete data scenarios.

Abstract: Solving time-dependent parametric partial differential equations (PDEs)
remains a fundamental challenge for neural solvers, particularly when
generalizing across a wide range of physical parameters and dynamics. When data
is uncertain or incomplete-as is often the case-a natural approach is to turn
to generative models. We introduce ENMA, a generative neural operator designed
to model spatio-temporal dynamics arising from physical phenomena. ENMA
predicts future dynamics in a compressed latent space using a generative masked
autoregressive transformer trained with flow matching loss, enabling tokenwise
generation. Irregularly sampled spatial observations are encoded into uniform
latent representations via attention mechanisms and further compressed through
a spatio-temporal convolutional encoder. This allows ENMA to perform in-context
learning at inference time by conditioning on either past states of the target
trajectory or auxiliary context trajectories with similar dynamics. The result
is a robust and adaptable framework that generalizes to new PDE regimes and
supports one-shot surrogate modeling of time-dependent parametric PDEs.

</details>


### [478] [Reusing Trajectories in Policy Gradients Enables Fast Convergence](https://arxiv.org/pdf/2506.06178)
*Alessandro Montenegro, Federico Mansutti, Marco Mussi, Matteo Papini, Alberto Maria Metelli*

Main category: cs.LG

TL;DR: The paper introduces RPG, a policy gradient method that reuses past trajectories to improve sample efficiency, achieving the best-known convergence rate.


<details>
  <summary>Details</summary>
Motivation: Policy gradient methods are sample-inefficient due to reliance on fresh data. Reusing past trajectories is underexplored theoretically.

Method: Proposes RPG with a power mean correction for multiple importance weighting, combining old and new trajectories for updates.

Result: RPG achieves a sample complexity of $\widetilde{O}(\epsilon^{-1})$, the best-known rate.

Conclusion: RPG significantly accelerates convergence and outperforms state-of-the-art PG methods.

Abstract: Policy gradient (PG) methods are a class of effective reinforcement learning
algorithms, particularly when dealing with continuous control problems. These
methods learn the parameters of parametric policies via stochastic gradient
ascent, typically using on-policy trajectory data to estimate the policy
gradient. However, such reliance on fresh data makes them sample-inefficient.
Indeed, vanilla PG methods require $O(\epsilon^{-2})$ trajectories to reach an
$\epsilon$-approximate stationary point. A common strategy to improve
efficiency is to reuse off-policy information from past iterations, such as
previous gradients or trajectories. While gradient reuse has received
substantial theoretical attention, leading to improved rates of
$O(\epsilon^{-3/2})$, the reuse of past trajectories remains largely unexplored
from a theoretical perspective. In this work, we provide the first rigorous
theoretical evidence that extensive reuse of past off-policy trajectories can
significantly accelerate convergence in PG methods. We introduce a power mean
correction to the multiple importance weighting estimator and propose RPG
(Retrospective Policy Gradient), a PG algorithm that combines old and new
trajectories for policy updates. Through a novel analysis, we show that, under
established assumptions, RPG achieves a sample complexity of
$\widetilde{O}(\epsilon^{-1})$, the best known rate in the literature. We
further validate empirically our approach against PG methods with
state-of-the-art rates.

</details>


### [479] [A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization](https://arxiv.org/pdf/2506.06179)
*Muhammed Ustaomeroglu, Guannan Qu*

Main category: cs.LG

TL;DR: Self-attention is analyzed as a mutual interaction learner, capable of representing and generalizing pairwise interactions, with extensions to multi-entity dependencies.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical foundations of self-attention and its ability to model interactions between entities.

Method: Analysis of linear self-attention layers and introduction of HyperFeatureAttention and HyperAttention modules.

Result: Self-attention efficiently learns and generalizes pairwise interactions, validated through experiments.

Conclusion: Self-attention is a powerful tool for modeling interactions, with potential extensions to complex multi-entity dependencies.

Abstract: Self-attention has emerged as a core component of modern neural
architectures, yet its theoretical underpinnings remain elusive. In this paper,
we study self-attention through the lens of interacting entities, ranging from
agents in multi-agent reinforcement learning to alleles in genetic sequences,
and show that a single layer linear self-attention can efficiently represent,
learn, and generalize functions capturing pairwise interactions, including
out-of-distribution scenarios. Our analysis reveals that self-attention acts as
a mutual interaction learner under minimal assumptions on the diversity of
interaction patterns observed during training, thereby encompassing a wide
variety of real-world domains. In addition, we validate our theoretical
insights through experiments demonstrating that self-attention learns
interaction functions and generalizes across both population distributions and
out-of-distribution scenarios. Building on our theories, we introduce
HyperFeatureAttention, a novel neural network module designed to learn
couplings of different feature-level interactions between entities.
Furthermore, we propose HyperAttention, a new module that extends beyond
pairwise interactions to capture multi-entity dependencies, such as three-way,
four-way, or general n-way interactions.

</details>


### [480] [Antithetic Noise in Diffusion Models](https://arxiv.org/pdf/2506.06185)
*Jing Jia, Sifan Liu, Bowen Song, Wei Yuan, Liyue Shen, Guanyang Wang*

Main category: cs.LG

TL;DR: The paper studies antithetic initial noise in diffusion models, showing that pairing initial noise with its negation produces negatively correlated samples. This leads to improved image diversity and sharper uncertainty quantification, with a training-free, model-agnostic approach.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate the effects of antithetic initial noise in diffusion models and leverage the observed negative correlation for practical applications.

Method: Pairing initial noise with its negation, combined with theoretical analysis and experiments, to validate the affine antisymmetry conjecture of the learned score function.

Result: Strongly negatively correlated samples, enabling enhanced image diversity and sharper uncertainty quantification (e.g., narrower confidence intervals).

Conclusion: The framework is effective, training-free, and model-agnostic, offering practical benefits without runtime overhead.

Abstract: We initiate a systematic study of antithetic initial noise in diffusion
models. Across unconditional models trained on diverse datasets,
text-conditioned latent-diffusion models, and diffusion-posterior samplers, we
find that pairing each initial noise with its negation consistently yields
strongly negatively correlated samples. To explain this phenomenon, we combine
experiments and theoretical analysis, leading to a symmetry conjecture that the
learned score function is approximately affine antisymmetric (odd symmetry up
to a constant shift), and provide evidence supporting it. Leveraging this
negative correlation, we enable two applications: (1) enhancing image diversity
in models like Stable Diffusion without quality loss, and (2) sharpening
uncertainty quantification (e.g., up to 90% narrower confidence intervals) when
estimating downstream statistics. Building on these gains, we extend the
two-point pairing to a randomized quasi-Monte Carlo estimator, which further
improves estimation accuracy. Our framework is training-free, model-agnostic,
and adds no runtime overhead.

</details>


### [481] [Physics-Informed Neural Networks for Control of Single-Phase Flow Systems Governed by Partial Differential Equations](https://arxiv.org/pdf/2506.06188)
*Luis Kin Miyatake, Eduardo Camponogara, Eric Aislan Antonelo, Alexey Pavlov*

Main category: cs.LG

TL;DR: The paper extends the PINC framework to PDEs for single-phase flow control, using neural networks integrated with physical laws, and demonstrates its effectiveness in real-time applications.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in modeling and controlling PDE-governed single-phase flow systems under transient conditions without labeled data.

Method: Extends PINC to PDEs with a two-stage network (steady-state and transient) and simplifies spatial dimensionality for efficient training and MPC-based control.

Result: PINC accurately represents flow dynamics and enables real-time control without labeled data or iterative solvers.

Conclusion: PINC is a promising alternative for fluid flow monitoring and optimization in engineering.

Abstract: The modeling and control of single-phase flow systems governed by Partial
Differential Equations (PDEs) present challenges, especially under transient
conditions. In this work, we extend the Physics-Informed Neural Nets for
Control (PINC) framework, originally proposed to modeling and control of
Ordinary Differential Equations (ODE) without the need of any labeled data, to
the PDE case, particularly to single-phase incompressible and compressible
flows, integrating neural networks with physical conservation laws. The PINC
model for PDEs is structured into two stages: a steady-state network, which
learns equilibrium solutions for a wide range of control inputs, and a
transient network, which captures dynamic responses under time-varying boundary
conditions. We propose a simplifying assumption that reduces the dimensionality
of the spatial coordinate regarding the initial condition, allowing the
efficient training of the PINC network. This simplification enables the
derivation of optimal control policies using Model Predictive Control (MPC). We
validate our approach through numerical experiments, demonstrating that the
PINC model, which is trained exclusively using physical laws, i.e., without
labeled data, accurately represents flow dynamics and enables real-time control
applications. The results highlight the PINC's capability to efficiently
approximate PDE solutions without requiring iterative solvers, making it a
promising alternative for fluid flow monitoring and optimization in engineering
applications.

</details>


### [482] [ICU-TSB: A Benchmark for Temporal Patient Representation Learning for Unsupervised Stratification into Patient Cohorts](https://arxiv.org/pdf/2506.06192)
*Dimitrios Proios, Alban Bornet, Anthony Yazdani, Jose F Rodrigues Jr, Douglas Teodoro*

Main category: cs.LG

TL;DR: ICU-TSB is a benchmark for patient stratification using temporal EHR data, evaluating methods like LSTM and GRU to identify clinically meaningful subgroups.


<details>
  <summary>Details</summary>
Motivation: Improving personalized medicine by leveraging ICU EHR data for better diagnostics and treatment strategies.

Method: Introduces ICU-TSB, a hierarchical evaluation framework using disease taxonomies, and compares statistical methods and recurrent neural networks (LSTM, GRU) for patient representation learning.

Result: Temporal representation learning can identify clinically relevant cohorts, though challenging (v-measure: 0.46 top level, 0.40 lowest). Interpretable cluster labeling strategies are also evaluated.

Conclusion: ICU-TSB provides a reproducible benchmark for temporal patient stratification, highlighting potential and challenges in clinical subgroup discovery.

Abstract: Patient stratification identifying clinically meaningful subgroups is
essential for advancing personalized medicine through improved diagnostics and
treatment strategies. Electronic health records (EHRs), particularly those from
intensive care units (ICUs), contain rich temporal clinical data that can be
leveraged for this purpose. In this work, we introduce ICU-TSB (Temporal
Stratification Benchmark), the first comprehensive benchmark for evaluating
patient stratification based on temporal patient representation learning using
three publicly available ICU EHR datasets. A key contribution of our benchmark
is a novel hierarchical evaluation framework utilizing disease taxonomies to
measure the alignment of discovered clusters with clinically validated disease
groupings. In our experiments with ICU-TSB, we compared statistical methods and
several recurrent neural networks, including LSTM and GRU, for their ability to
generate effective patient representations for subsequent clustering of patient
trajectories. Our results demonstrate that temporal representation learning can
rediscover clinically meaningful patient cohorts; nevertheless, it remains a
challenging task, with v-measuring varying from up to 0.46 at the top level of
the taxonomy to up to 0.40 at the lowest level. To further enhance the
practical utility of our findings, we also evaluate multiple strategies for
assigning interpretable labels to the identified clusters. The experiments and
benchmark are fully reproducible and available at
https://github.com/ds4dh/CBMS2025stratification.

</details>


### [483] [Transformative or Conservative? Conservation laws for ResNets and Transformers](https://arxiv.org/pdf/2506.06194)
*Sibylle Marcotte, Rémi Gribonval, Gabriel Peyré*

Main category: cs.LG

TL;DR: The paper explores conservation laws in gradient flow training dynamics for modern architectures like convolutional ResNets and Transformers, extending beyond shallow ReLU/linear networks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding conservation laws for practical architectures, focusing on their building blocks and persistence under discrete optimization.

Method: Derives and analyzes conservation laws for modern architectures, including ReLU/linear shallow networks, attention layers, and residual blocks, and examines their persistence under SGD.

Result: Shows that conservation laws for building blocks are easily expressed and persist in discrete optimization, with residual blocks and attention layers having specific conservation properties.

Conclusion: The study extends conservation law understanding to practical architectures and demonstrates their relevance in both continuous and discrete training regimes.

Abstract: While conservation laws in gradient flow training dynamics are well
understood for (mostly shallow) ReLU and linear networks, their study remains
largely unexplored for more practical architectures. This paper bridges this
gap by deriving and analyzing conservation laws for modern architectures, with
a focus on convolutional ResNets and Transformer networks. For this, we first
show that basic building blocks such as ReLU (or linear) shallow networks, with
or without convolution, have easily expressed conservation laws, and no more
than the known ones. In the case of a single attention layer, we also
completely describe all conservation laws, and we show that residual blocks
have the same conservation laws as the same block without a skip connection. We
then introduce the notion of conservation laws that depend only on a subset of
parameters (corresponding e.g. to a pair of consecutive layers, to a residual
block, or to an attention layer). We demonstrate that the characterization of
such laws can be reduced to the analysis of the corresponding building block in
isolation. Finally, we examine how these newly discovered conservation
principles, initially established in the continuous gradient flow regime,
persist under discrete optimization dynamics, particularly in the context of
Stochastic Gradient Descent (SGD).

</details>


### [484] [How to craft a deep reinforcement learning policy for wind farm flow control](https://arxiv.org/pdf/2506.06204)
*Elie Kadoche, Pascal Bianchi, Florence Carton, Philippe Ciblat, Damien Ernst*

Main category: cs.LG

TL;DR: A deep reinforcement learning method for wake steering in wind farms improves energy production by optimizing turbine yaw angles under time-varying wind conditions.


<details>
  <summary>Details</summary>
Motivation: Wake effects reduce wind farm efficiency; current methods are limited to static conditions or small farms.

Method: Combines graph attention networks and multi-head self-attention with a novel reward function and training strategy.

Result: Achieves 14% higher energy production with 10x fewer training steps than traditional methods.

Conclusion: First DRL-based wake steering controller effective in time-varying wind conditions.

Abstract: Within wind farms, wake effects between turbines can significantly reduce
overall energy production. Wind farm flow control encompasses methods designed
to mitigate these effects through coordinated turbine control. Wake steering,
for example, consists in intentionally misaligning certain turbines with the
wind to optimize airflow and increase power output. However, designing a robust
wake steering controller remains challenging, and existing machine learning
approaches are limited to quasi-static wind conditions or small wind farms.
This work presents a new deep reinforcement learning methodology to develop a
wake steering policy that overcomes these limitations. Our approach introduces
a novel architecture that combines graph attention networks and multi-head
self-attention blocks, alongside a novel reward function and training strategy.
The resulting model computes the yaw angles of each turbine, optimizing energy
production in time-varying wind conditions. An empirical study conducted on
steady-state, low-fidelity simulation, shows that our model requires
approximately 10 times fewer training steps than a fully connected neural
network and achieves more robust performance compared to a strong optimization
baseline, increasing energy production by up to 14 %. To the best of our
knowledge, this is the first deep reinforcement learning-based wake steering
controller to generalize effectively across any time-varying wind conditions in
a low-fidelity, steady-state numerical simulation setting.

</details>


### [485] [Model-Driven Graph Contrastive Learning](https://arxiv.org/pdf/2506.06212)
*Ali Azizpour, Nicolas Zilberstein, Santiago Segarra*

Main category: cs.LG

TL;DR: MGCL is a model-driven graph contrastive learning framework that uses graphons to guide contrastive learning, improving performance by leveraging the data's generative process.


<details>
  <summary>Details</summary>
Motivation: Existing GCL methods rely on heuristic augmentations not tailored to the data distribution. MGCL addresses this by incorporating graphons for data-adaptive augmentations.

Method: MGCL estimates graphons from observed data, defines graphon-informed augmentations, and clusters datasets for graph-level tasks to reflect shared semantics.

Result: MGCL achieves state-of-the-art performance on benchmark datasets.

Conclusion: Incorporating generative models like graphons into GCL enhances performance, demonstrating the value of data-driven augmentations.

Abstract: We propose $\textbf{MGCL}$, a model-driven graph contrastive learning (GCL)
framework that leverages graphons (probabilistic generative models for graphs)
to guide contrastive learning by accounting for the data's underlying
generative process. GCL has emerged as a powerful self-supervised framework for
learning expressive node or graph representations without relying on annotated
labels, which are often scarce in real-world data. By contrasting augmented
views of graph data, GCL has demonstrated strong performance across various
downstream tasks, such as node and graph classification. However, existing
methods typically rely on manually designed or heuristic augmentation
strategies that are not tailored to the underlying data distribution and
operate at the individual graph level, ignoring similarities among graphs
generated from the same model. Conversely, in our proposed approach, MGCL first
estimates the graphon associated with the observed data and then defines a
graphon-informed augmentation process, enabling data-adaptive and principled
augmentations. Additionally, for graph-level tasks, MGCL clusters the dataset
and estimates a graphon per group, enabling contrastive pairs to reflect shared
semantics and structure. Extensive experiments on benchmark datasets
demonstrate that MGCL achieves state-of-the-art performance, highlighting the
advantages of incorporating generative models into GCL.

</details>


### [486] [Neural Responses to Affective Sentences Reveal Signatures of Depression](https://arxiv.org/pdf/2506.06244)
*Aditya Kommineni, Woojae Jeong, Kleanthis Avramidis, Colin McDaniel, Myzelle Hughes, Thomas McGee, Elsi Kaiser, Kristina Lerman, Idan A. Blank, Dani Byrd, Assal Habibi, B. Rael Cahn, Sudarsana Kadiri, Takfarinas Medani, Richard M. Leahy, Shrikanth Narayanan*

Main category: cs.LG

TL;DR: The paper explores how depression affects emotional and self-referential processing using EEG, revealing neural differences and potential diagnostic markers.


<details>
  <summary>Details</summary>
Motivation: To understand the neurocognitive impact of MDD on emotional and self-referential processing for better diagnostics.

Method: EEG measurements of neural responses to self-referential affective sentences in healthy and depressed individuals.

Result: Group-level neural differences and deep learning model AUCs of 0.707 (healthy vs. depressed) and 0.624 (depressed subgroups).

Conclusion: Depression shows stable neural signatures, offering potential for future diagnostic tools.

Abstract: Major Depressive Disorder (MDD) is a highly prevalent mental health
condition, and a deeper understanding of its neurocognitive foundations is
essential for identifying how core functions such as emotional and
self-referential processing are affected. We investigate how depression alters
the temporal dynamics of emotional processing by measuring neural responses to
self-referential affective sentences using surface electroencephalography (EEG)
in healthy and depressed individuals. Our results reveal significant
group-level differences in neural activity during sentence viewing, suggesting
disrupted integration of emotional and self-referential information in
depression. Deep learning model trained on these responses achieves an area
under the receiver operating curve (AUC) of 0.707 in distinguishing healthy
from depressed participants, and 0.624 in differentiating depressed subgroups
with and without suicidal ideation. Spatial ablations highlight anterior
electrodes associated with semantic and affective processing as key
contributors. These findings suggest stable, stimulus-driven neural signatures
of depression that may inform future diagnostic tools.

</details>


### [487] [Lagrangian-based Equilibrium Propagation: generalisation to arbitrary boundary conditions & equivalence with Hamiltonian Echo Learning](https://arxiv.org/pdf/2506.06248)
*Guillaume Pourcel, Debabrota Basu, Maxence Ernoult, Aditya Gilra*

Main category: cs.LG

TL;DR: GLEP extends EP to time-varying inputs, deriving HEL as a practical special case with hardware-friendly properties.


<details>
  <summary>Details</summary>
Motivation: Extending EP to time-varying inputs requires addressing variational descriptions for trajectories and boundary conditions.

Method: Generalized Lagrangian Equilibrium Propagation (GLEP) extends EP's variational framework to handle time-varying inputs.

Result: GLEP yields various learning algorithms, with HEL being the only practical one for hardware, offering forward-only operation, efficiency, and local learning.

Conclusion: HEL, derived from GLEP, is a viable alternative to backpropagation for hardware due to its desirable properties.

Abstract: Equilibrium Propagation (EP) is a learning algorithm for training
Energy-based Models (EBMs) on static inputs which leverages the variational
description of their fixed points. Extending EP to time-varying inputs is a
challenging problem, as the variational description must apply to the entire
system trajectory rather than just fixed points, and careful consideration of
boundary conditions becomes essential. In this work, we present Generalized
Lagrangian Equilibrium Propagation (GLEP), which extends the variational
formulation of EP to time-varying inputs. We demonstrate that GLEP yields
different learning algorithms depending on the boundary conditions of the
system, many of which are impractical for implementation. We then show that
Hamiltonian Echo Learning (HEL) -- which includes the recently proposed
Recurrent HEL (RHEL) and the earlier known Hamiltonian Echo Backpropagation
(HEB) algorithms -- can be derived as a special case of GLEP. Notably, HEL is
the only instance of GLEP we found that inherits the properties that make EP a
desirable alternative to backpropagation for hardware implementations: it
operates in a "forward-only" manner (i.e. using the same system for both
inference and learning), it scales efficiently (requiring only two or more
passes through the system regardless of model size), and enables local
learning.

</details>


### [488] [Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning](https://arxiv.org/pdf/2307.04726)
*Suzan Ece Ada, Erhan Oztop, Emre Ugur*

Main category: cs.LG

TL;DR: SRDP improves offline RL by using state reconstruction in diffusion policies to handle OOD generalization, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing distribution shifts in offline RL due to lack of online interaction and OOD states.

Method: Incorporates state reconstruction feature learning into diffusion policies.

Result: Achieves 167% improvement over baselines in sparse tasks and outperforms prior methods on D4RL benchmarks.

Conclusion: SRDP effectively mitigates OOD issues and enhances offline RL performance.

Abstract: Offline Reinforcement Learning (RL) methods leverage previous experiences to
learn better policies than the behavior policy used for data collection.
However, they face challenges handling distribution shifts due to the lack of
online interaction during training. To this end, we propose a novel method
named State Reconstruction for Diffusion Policies (SRDP) that incorporates
state reconstruction feature learning in the recent class of diffusion policies
to address the problem of out-of-distribution (OOD) generalization. Our method
promotes learning of generalizable state representation to alleviate the
distribution shift caused by OOD states. To illustrate the OOD generalization
and faster convergence of SRDP, we design a novel 2D Multimodal Contextual
Bandit environment and realize it on a 6-DoF real-world UR10 robot, as well as
in simulation, and compare its performance with prior algorithms. In
particular, we show the importance of the proposed state reconstruction via
ablation studies. In addition, we assess the performance of our model on
standard continuous control benchmarks (D4RL), namely the navigation of an
8-DoF ant and forward locomotion of half-cheetah, hopper, and walker2d,
achieving state-of-the-art results. Finally, we demonstrate that our method can
achieve 167% improvement over the competing baseline on a sparse continuous
control navigation task where various regions of the state space are removed
from the offline RL dataset, including the region encapsulating the goal.

</details>


### [489] [Graph Deep Learning for Time Series Forecasting](https://arxiv.org/pdf/2310.15978)
*Andrea Cini, Ivan Marisca, Daniele Zambon, Cesare Alippi*

Main category: cs.LG

TL;DR: The paper introduces a methodological framework for graph-based predictors in time series forecasting, addressing foundational aspects and providing design principles, performance assessment methods, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To fill the gap in systematic investigation of foundational and methodological aspects of spatiotemporal graph neural networks, which are popular for correlated time series forecasting.

Method: Proposes a comprehensive framework formalizing the forecasting problem, offering design principles for graph-based predictors, and methods to evaluate their performance.

Result: Provides an overview of the field, design guidelines, best practices, and discusses open challenges and future directions.

Conclusion: The tutorial paper aims to advance the field by addressing foundational gaps and offering practical insights for developing and assessing graph-based forecasting models.

Abstract: Graph deep learning methods have become popular tools to process collections
of correlated time series. Unlike traditional multivariate forecasting methods,
graph-based predictors leverage pairwise relationships by conditioning
forecasts on graphs spanning the time series collection. The conditioning takes
the form of architectural inductive biases on the forecasting architecture,
resulting in a family of models called spatiotemporal graph neural networks.
These biases allow for training global forecasting models on large collections
of time series while localizing predictions w.r.t. each element in the set
(nodes) by accounting for correlations among them (edges). Recent advances in
graph neural networks and deep learning for time series forecasting make the
adoption of such processing framework appealing and timely. However, most
studies focus on refining existing architectures by exploiting modern
deep-learning practices. Conversely, foundational and methodological aspects
have not been subject to systematic investigation. To fill this void, this
tutorial paper aims to introduce a comprehensive methodological framework
formalizing the forecasting problem and providing design principles for
graph-based predictors, as well as methods to assess their performance. In
addition, together with an overview of the field, we provide design guidelines
and best practices, as well as an in-depth discussion of open challenges and
future directions.

</details>


### [490] [GraphGPT: Generative Pre-trained Graph Eulerian Transformer](https://arxiv.org/pdf/2401.00529)
*Qifang Zhao, Weidong Ren, Tianyu Li, Hong Liu, Xingsheng He, Xiaoxiao Xu*

Main category: cs.LG

TL;DR: GraphGPT is a self-supervised generative pre-trained model for graph learning using Graph Eulerian Transformer (GET). It achieves state-of-the-art performance on large-scale datasets and scales to 2B parameters.


<details>
  <summary>Details</summary>
Motivation: To overcome scalability limitations of traditional GNNs and graph transformers, and to advance graph foundation models for scientific discovery.

Method: Proposes GET, combining transformer architecture with graph-to-sequence transformation using Eulerian paths. Pre-trained with NTP or SMTP tasks, then fine-tuned for downstream tasks.

Result: Achieves comparable or superior performance on OGB datasets, excelling in molecular property and protein-protein interaction prediction.

Conclusion: GraphGPT is a scalable and high-performing model, with potential for broad applications in chemistry and materials science. Source code and checkpoints will be released.

Abstract: We introduceGraphGPT, a novel self-supervised generative pre-trained model
for graph learning based on the Graph Eulerian Transformer (GET). First, we
propose GET, which combines a standard transformer encoder or decoder
architecture with an innovative graph-to-sequence transformation method. This
method converts graphs or sampled subgraphs into sequences of tokens
representing nodes, edges, and attributes in a reversible manner using Eulerian
paths. We pre-train GET using either of the two self-supervised tasks:
next-token prediction (NTP) and scheduled masked-token prediction (SMTP). The
pre-trained model is then fine-tuned for downstream tasks such as graph-,
edge-, and node-level prediction. Despite its simplicity, GraphGPT achieves
performance comparable to or surpassing state-of-the-art methods on multiple
large-scale Open Graph Benchmark (OGB) datasets. It demonstrates exceptional
results on the molecular property prediction dataset PCQM4Mv2 and the
protein-protein interaction dataset ogbl-ppa. Notably, generative pre-training
enables scaling GraphGPT to 2 billion parameters while maintaining performance
gains - a breakthrough that overcomes the scalability limitations of
traditional Graph Neural Networks (GNNs) and prior graph transformers (GTs). To
advance research in graph foundation models and facilitate scientific discovery
in chemistry, materials science, and related fields, we will release the source
code (https://github.com/alibaba/graph-gpt) and pre-trained checkpoints.

</details>


### [491] [Multidimensional Adaptive Coefficient for Inference Trajectory Optimization in Flow and Diffusion](https://arxiv.org/pdf/2404.14161)
*Dohoon Lee, Jaehyun Park, Hyunwoo J. Kim, Kyogu Lee*

Main category: cs.LG

TL;DR: The paper introduces MAC, a plug-in module for flow and diffusion models to enhance dimensionality freedom and adaptability, improving generative quality efficiently.


<details>
  <summary>Details</summary>
Motivation: Flow and diffusion models lack dimensionality freedom and adaptability to inference trajectories, limiting their simulation-based capabilities.

Method: Proposes MAC, a module extending unidimensional coefficients to multidimensional ones, trained via simulation-based adversarial refinement.

Result: MAC improves generative quality across frameworks and datasets with high training efficiency.

Conclusion: MAC offers a new perspective on inference trajectory optimality, advocating for simulation-based optimization beyond vector field design.

Abstract: Flow and diffusion models have demonstrated strong performance and training
stability across various tasks but lack two critical properties of
simulation-based methods: freedom of dimensionality and adaptability to
different inference trajectories. To address this limitation, we propose the
Multidimensional Adaptive Coefficient (MAC), a plug-in module for flow and
diffusion models that extends conventional unidimensional coefficients to
multidimensional ones and enables inference trajectory-wise adaptation. MAC is
trained via simulation-based feedback through adversarial refinement. Empirical
results across diverse frameworks and datasets demonstrate that MAC enhances
generative quality with high training efficiency. Consequently, our work offers
a new perspective on inference trajectory optimality, encouraging future
research to move beyond vector field design and to leverage training-efficient,
simulation-based optimization.

</details>


### [492] [Mirage: A Multi-Level Superoptimizer for Tensor Programs](https://arxiv.org/pdf/2405.05751)
*Mengdi Wu, Xinhao Cheng, Shengyu Liu, Chunan Shi, Jianan Ji, Kit Ao, Praveen Velliengiri, Xupeng Miao, Oded Padon, Zhihao Jia*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce Mirage, the first multi-level superoptimizer for tensor
programs. A key idea in Mirage is $\mu$Graphs, a uniform representation of
tensor programs at the kernel, thread block, and thread levels of the GPU
compute hierarchy. $\mu$Graphs enable Mirage to discover novel optimizations
that combine algebraic transformations, schedule transformations, and
generation of new custom kernels. To navigate the large search space, Mirage
introduces a pruning technique based on abstraction that significantly reduces
the search space and provides a certain optimality guarantee. To ensure that
the optimized $\mu$Graph is equivalent to the input program, Mirage introduces
a probabilistic equivalence verification procedure with strong theoretical
guarantees. Our evaluation shows that Mirage outperforms existing approaches by
up to 3.3$\times$ even for DNNs that are widely used and heavily optimized.
Mirage is publicly available at https://github.com/mirage-project/mirage.

</details>


### [493] [Computational Limits of Low-Rank Adaptation (LoRA) Fine-Tuning for Transformer Models](https://arxiv.org/pdf/2406.03136)
*Jerry Yao-Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, Han Liu*

Main category: cs.LG

TL;DR: The paper explores the computational limits of Low-Rank Adaptation (LoRA) for transformer models, identifying efficiency thresholds and proving the existence of almost linear algorithms for LoRA updates.


<details>
  <summary>Details</summary>
Motivation: To understand the computational efficiency and limitations of LoRA for finetuning transformer models, leveraging fine-grained complexity theory.

Method: Analyzes low-rank decompositions in LoRA gradient computations, identifies phase transitions under SETH, and proves almost linear algorithms using hierarchical low-rank structures.

Result: Identifies a sharp efficiency threshold for rank-r LoRA updates and demonstrates almost linear approximation algorithms for LoRA adaptation.

Conclusion: The study provides theoretical insights into the efficiency of LoRA, showing practical implications for partial and full adaptations in attention heads.

Abstract: We study the computational limits of Low-Rank Adaptation (LoRA) for
finetuning transformer-based models using fine-grained complexity theory. Our
key observation is that the existence of low-rank decompositions within the
gradient computation of LoRA adaptation leads to possible algorithmic speedup.
This allows us to (i) identify a phase transition behavior of efficiency
assuming the Strong Exponential Time Hypothesis (SETH), and (ii) prove the
existence of almost linear algorithms by controlling the LoRA update
computation term by term. For the former, we identify a sharp transition in the
efficiency of all possible rank-$r$ LoRA update algorithms for transformers,
based on specific norms resulting from the multiplications of the input
sequence $X$, pretrained weights ${W^\star}$, and adapter matrices $\alpha B
A/r$. Specifically, we derive a shared upper bound threshold for such norms,
and show that efficient (sub-quadratic) approximation algorithms of LoRA exist
only below this threshold. For the latter, we prove the existence of almost
linear approximation algorithms for LoRA adaptation by utilizing the
hierarchical low-rank structures of LoRA gradients and approximating the
gradients with a series of chained low-rank approximations. To showcase our
theory, we consider two practical scenarios: partial (e.g., only $W_V$ and
$W_Q$) and full adaptations (e.g., $W_Q$, $W_V$, and $W_K$) of weights in
attention heads.

</details>


### [494] [Certification for Differentially Private Prediction in Gradient-Based Training](https://arxiv.org/pdf/2406.13433)
*Matthew Wicker, Philip Sosnin, Igor Shilov, Adrianna Janik, Mark N. Müller, Yves-Alexandre de Montjoye, Adrian Weller, Calvin Tsay*

Main category: cs.LG

TL;DR: A novel method improves private prediction by computing dataset-specific sensitivity bounds, outperforming global sensitivity-based approaches in privacy-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing private prediction methods use global sensitivity, leading to sub-optimal privacy-utility trade-offs compared to private training.

Method: Uses convex relaxation and bound propagation to compute dataset-specific sensitivity bounds, combined with the smooth sensitivity mechanism.

Result: Achieves significantly tighter sensitivity bounds than global sensitivity, validated on medical image and NLP datasets.

Conclusion: Provides a foundation for advanced privacy-preserving technologies by improving private prediction analysis.

Abstract: We study private prediction where differential privacy is achieved by adding
noise to the outputs of a non-private model. Existing methods rely on noise
proportional to the global sensitivity of the model, often resulting in
sub-optimal privacy-utility trade-offs compared to private training. We
introduce a novel approach for computing dataset-specific upper bounds on
prediction sensitivity by leveraging convex relaxation and bound propagation
techniques. By combining these bounds with the smooth sensitivity mechanism, we
significantly improve the privacy analysis of private prediction compared to
global sensitivity-based approaches. Experimental results across real-world
datasets in medical image classification and natural language processing
demonstrate that our sensitivity bounds are can be orders of magnitude tighter
than global sensitivity. Our approach provides a strong basis for the
development of novel privacy preserving technologies.

</details>


### [495] [BoA: Attention-aware Post-training Quantization without Backpropagation](https://arxiv.org/pdf/2406.13474)
*Junhan Kim, Ho-young Kim, Eulrang Cho, Chungman Lee, Joonyoung Kim, Yongkweon Jeon*

Main category: cs.LG

TL;DR: A novel backpropagation-free PTQ algorithm optimizes quantized weights by considering inter-layer dependencies, using attention-aware Hessian matrices for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods for LLMs ignore inter-layer interactions or use naive rounding, limiting performance.

Method: Develops attention-aware Hessian matrices to capture inter-layer dependencies in the attention module.

Result: Outperforms existing weight quantization methods and synergizes with activation quantization for state-of-the-art performance.

Conclusion: The proposed method advances PTQ for LLMs by addressing inter-layer dependencies and computational efficiency.

Abstract: Post-training quantization (PTQ) is a promising solution for deploying large
language models (LLMs) on resource-constrained devices. Early methods developed
for small-scale networks, such as ResNet, rely on gradient-based optimization,
which becomes impractical for hyper-scale LLMs with billions of parameters.
While recently proposed backpropagation-free or transformation-based methods
alleviate this issue, they ignore inter-layer interactions or use the naive
nearest-rounding-based quantized weight assignment to save the heavy
computational cost of weight optimization. In this paper, we introduce a novel
backpropagation-free PTQ algorithm that optimizes quantized weights by
considering inter-layer dependencies. The key innovation is the development of
attention-aware Hessian matrices that capture inter-layer interactions within
the attention module. Extensive experiments demonstrate that our approach not
only outperforms existing weight quantization methods but also shows good
synergy with conventional methods to suppress activation outliers, leading to
state-of-the-art weight-activation quantization performance. The code will be
available at https://github.com/SamsungLabs/BoA.

</details>


### [496] [Proximal Policy Distillation](https://arxiv.org/pdf/2407.15134)
*Giacomo Spigler*

Main category: cs.LG

TL;DR: PPD combines student-driven distillation and PPO for better sample efficiency and student policy performance, outperforming alternatives in various RL environments.


<details>
  <summary>Details</summary>
Motivation: To improve policy distillation by leveraging student-collected rewards and increasing sample efficiency.

Method: Integrates student-driven distillation with PPO, tested across ATARI, Mujoco, and Procgen environments with varying student network sizes.

Result: PPD enhances sample efficiency and student policy quality, showing robustness with imperfect demonstrations.

Conclusion: PPD is a superior policy distillation method, supported by a new Python library (`sb3-distill`) for practical use.

Abstract: We introduce Proximal Policy Distillation (PPD), a novel policy distillation
method that integrates student-driven distillation and Proximal Policy
Optimization (PPO) to increase sample efficiency and to leverage the additional
rewards that the student policy collects during distillation. To assess the
efficacy of our method, we compare PPD with two common alternatives,
student-distill and teacher-distill, over a wide range of reinforcement
learning environments that include discrete actions and continuous control
(ATARI, Mujoco, and Procgen). For each environment and method, we perform
distillation to a set of target student neural networks that are smaller,
identical (self-distillation), or larger than the teacher network. Our findings
indicate that PPD improves sample efficiency and produces better student
policies compared to typical policy distillation approaches. Moreover, PPD
demonstrates greater robustness than alternative methods when distilling
policies from imperfect demonstrations. The code for the paper is released as
part of a new Python library built on top of stable-baselines3 to facilitate
policy distillation: `sb3-distill'.

</details>


### [497] [DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on Continuous-Time Dynamic Graphs with State Space Models](https://arxiv.org/pdf/2408.04713)
*Zifeng Ding, Yifeng Li, Yuan He, Antonio Norelli, Jingcheng Wu, Volker Tresp, Michael Bronstein, Yunpu Ma*

Main category: cs.LG

TL;DR: DyGMamba, a CTDG model based on Mamba SSM, efficiently encodes long node interaction histories and captures critical temporal details, achieving state-of-the-art performance in dynamic link prediction.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of encoding long histories and identifying critical temporal information in CTDGs while maintaining computational efficiency.

Method: Uses node-level and time-level SSMs to encode interaction sequences and exploit hidden temporal patterns, dynamically selecting critical information.

Result: Achieves state-of-the-art performance in dynamic link prediction and maintains high computational efficiency.

Conclusion: DyGMamba effectively balances computational efficiency and performance, enabling long-term dependency capture with limited resources.

Abstract: Learning useful representations for continuous-time dynamic graphs (CTDGs) is
challenging, due to the concurrent need to span long node interaction histories
and grasp nuanced temporal details. In particular, two problems emerge: (1)
Encoding longer histories requires more computational resources, making it
crucial for CTDG models to maintain low computational complexity to ensure
efficiency; (2) Meanwhile, more powerful models are needed to identify and
select the most critical temporal information within the extended context
provided by longer histories. To address these problems, we propose a CTDG
representation learning model named DyGMamba, originating from the popular
Mamba state space model (SSM). DyGMamba first leverages a node-level SSM to
encode the sequence of historical node interactions. Another time-level SSM is
then employed to exploit the temporal patterns hidden in the historical graph,
where its output is used to dynamically select the critical information from
the interaction history. We validate DyGMamba experimentally on the dynamic
link prediction task. The results show that our model achieves state-of-the-art
in most cases. DyGMamba also maintains high efficiency in terms of
computational resources, making it possible to capture long temporal
dependencies with a limited computation budget.

</details>


### [498] [Simmering: Sufficient is better than optimal for training neural networks](https://arxiv.org/pdf/2410.19912)
*Irina Babayan, Hazhir Aliahmadi, Greg van Anders*

Main category: cs.LG

TL;DR: The paper critiques optimization-based neural network training, introduces 'simmering' as a physics-inspired alternative, and shows it outperforms optimization methods like Adam by avoiding overfitting.


<details>
  <summary>Details</summary>
Motivation: The success of neural networks in modeling physical processes inspired exploring physics-based training methods to address overfitting and improve training paradigms.

Method: The authors propose 'simmering,' a physics-based training algorithm that generates 'good enough' weights and biases, avoiding overfitting and outperforming optimization-based methods like Adam.

Result: Simmering corrects overfit networks trained by Adam and prevents overfitting when used from the start, demonstrating superior performance in classification and regression tasks.

Conclusion: The study challenges optimization as the default training paradigm, suggesting physics-inspired methods like simmering can offer better alternatives without relying on optimization.

Abstract: The broad range of neural network training techniques that invoke
optimization but rely on ad hoc modification for validity suggests that
optimization-based training is misguided. Shortcomings of optimization-based
training are brought to particularly strong relief by the problem of
overfitting, where naive optimization produces spurious outcomes. The broad
success of neural networks for modelling physical processes has prompted
advances that are based on inverting the direction of investigation and
treating neural networks as if they were physical systems in their own right.
These successes raise the question of whether broader, physical perspectives
could motivate the construction of improved training algorithms. Here, we
introduce simmering, a physics-based method that trains neural networks to
generate weights and biases that are merely ``good enough'', but which,
paradoxically, outperforms leading optimization-based approaches. Using
classification and regression examples we show that simmering corrects neural
networks that are overfit by Adam, and show that simmering avoids overfitting
if deployed from the outset. Our results question optimization as a paradigm
for neural network training, and leverage information-geometric arguments to
point to the existence of classes of sufficient training algorithms that do not
take optimization as their starting point.

</details>


### [499] [Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency](https://arxiv.org/pdf/2411.16525)
*Jerry Yao-Chieh Hu, Wei-Po Wang, Ammar Gilani, Chenyang Li, Zhao Song, Han Liu*

Main category: cs.LG

TL;DR: Prompt tuning on single-head, single-layer transformers is universal and efficient under SETH, with statistical and computational limits identified.


<details>
  <summary>Details</summary>
Motivation: To explore the limits of prompt tuning in simple transformer models, focusing on universality and efficiency.

Method: Analyzing prompt tuning on single-head, single-layer transformers, proving universality and efficiency bounds under SETH.

Result: Prompt tuning is universal for sequence-to-sequence functions, with efficiency limits tied to prompt-induced norms.

Conclusion: The study provides essential design criteria for efficient and expressive prompt tuning methods.

Abstract: We investigate the statistical and computational limits of prompt tuning for
transformer-based foundation models. Our key contributions are prompt tuning on
\emph{single-head} transformers with only a \emph{single} self-attention layer:
(i) is universal, and (ii) supports efficient (even almost-linear time)
algorithms under the Strong Exponential Time Hypothesis (SETH). Statistically,
we prove that prompt tuning on such simplest possible transformers are
universal approximators for sequence-to-sequence Lipschitz functions. In
addition, we provide an exponential-in-$dL$ and -in-$(1/\epsilon)$ lower bound
on the required soft-prompt tokens for prompt tuning to memorize any dataset
with 1-layer, 1-head transformers. Computationally, we identify a phase
transition in the efficiency of prompt tuning, determined by the norm of the
\emph{soft-prompt-induced} keys and queries, and provide an upper bound
criterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for
prompt tuning exists under SETH. Within this criterion, we showcase our theory
by proving the existence of almost-linear time prompt tuning inference
algorithms. These fundamental limits provide important necessary conditions for
designing expressive and efficient prompt tuning methods for practitioners.

</details>


### [500] [A Cognac shot to forget bad memories: Corrective Unlearning in GNNs](https://arxiv.org/pdf/2412.00789)
*Varshita Kolipaka, Akshit Sinha, Debangan Mishra, Sumit Kumar, Arvindh Arun, Shashwat Goel, Ponnurangam Kumaraguru*

Main category: cs.LG

TL;DR: The paper introduces Cognac, a new graph unlearning method for GNNs, which effectively mitigates the adverse effects of manipulated data even with minimal identification (5%). It outperforms retraining and is 8x more efficient.


<details>
  <summary>Details</summary>
Motivation: GNNs suffer from performance degradation due to manipulated or incorrect data propagating through message passing. Current unlearning methods fail to address this, necessitating a better solution.

Method: Proposes Cognac, a corrective unlearning method that removes the effects of manipulated entities in GNNs, even with partial (5%) identification of the manipulated set.

Result: Cognac recovers most of the performance of an oracle with fully corrected data, outperforms retraining, and is 8x more efficient.

Conclusion: Cognac aids GNN developers in mitigating harmful data effects post-training, offering a practical and efficient solution.

Abstract: Graph Neural Networks (GNNs) are increasingly being used for a variety of ML
applications on graph data. Because graph data does not follow the
independently and identically distributed (i.i.d.) assumption, adversarial
manipulations or incorrect data can propagate to other data points through
message passing, which deteriorates the model's performance. To allow model
developers to remove the adverse effects of manipulated entities from a trained
GNN, we study the recently formulated problem of Corrective Unlearning. We find
that current graph unlearning methods fail to unlearn the effect of
manipulations even when the whole manipulated set is known. We introduce a new
graph unlearning method, Cognac, which can unlearn the effect of the
manipulation set even when only 5% of it is identified. It recovers most of the
performance of a strong oracle with fully corrected training data, even beating
retraining from scratch without the deletion set while being 8x more efficient.
We hope our work assists GNN developers in mitigating harmful effects caused by
issues in real-world data, post-training. Our code is publicly available at
https://github.com/cognac-gnn-unlearning/corrective-unlearning-for-gnns

</details>


### [501] [Understanding Memorization in Generative Models via Sharpness in Probability Landscapes](https://arxiv.org/pdf/2412.04140)
*Dongjae Jeon, Dueun Kim, Albert No*

Main category: cs.LG

TL;DR: A geometric framework analyzes memorization in diffusion models using log probability density sharpness, validates a score-difference metric, and introduces a new metric for early-stage memorization detection with a mitigation strategy.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify memorization in diffusion models, particularly focusing on the sharpness of the log probability density.

Method: Proposes a geometric framework, validates a score-difference metric, introduces a new metric for early-stage sharpness, and develops a sharpness-aware regularization strategy.

Result: Demonstrates the effectiveness of the score-difference metric and the novel metric in detecting memorization, with a mitigation strategy to optimize initial noise.

Conclusion: The framework and metrics provide tools for analyzing and mitigating memorization in diffusion models, enhancing early detection and intervention.

Abstract: In this paper, we introduce a geometric framework to analyze memorization in
diffusion models through the sharpness of the log probability density. We
mathematically justify a previously proposed score-difference-based
memorization metric by demonstrating its effectiveness in quantifying
sharpness. Additionally, we propose a novel memorization metric that captures
sharpness at the initial stage of image generation in latent diffusion models,
offering early insights into potential memorization. Leveraging this metric, we
develop a mitigation strategy that optimizes the initial noise of the
generation process using a sharpness-aware regularization term.

</details>


### [502] [CoopetitiveV: Leveraging LLM-powered Coopetitive Multi-Agent Prompting for High-quality Verilog Generation](https://arxiv.org/pdf/2412.11014)
*Zhendong Mi, Renming Zheng, Haowen Zhong, Yue Sun, Seth Kneeland, Sayan Moitra, Ken Kutzer, Zhaozhuo Xu Shaoyi Huang*

Main category: cs.LG

TL;DR: A coopetitive multi-agent framework improves Verilog code generation by balancing collaboration and competition, mitigating degeneration and error propagation.


<details>
  <summary>Details</summary>
Motivation: Existing methods (single-agent or cooperation-only multi-agent) suffer from degeneration and error propagation, limiting code quality.

Method: Proposes a coopetitive multi-agent prompting framework combining collaboration and competition.

Result: Achieves high pass rates (99.2% on VerilogEval, 100% syntax on RTLLM) and reduces errors.

Conclusion: The framework enhances Verilog code generation quality by addressing key limitations of prior approaches.

Abstract: Recent advances in agentic LLMs have demonstrated great capabilities in
Verilog code generation. However, existing approaches either use LLM-assisted
single-agent prompting or cooperation-only multi-agent learning, which will
lead to: (i) Degeneration issue for single-agent learning: characterized by
diminished error detection and correction capabilities; (ii) Error propagation
in cooperation-only multi-agent learning: erroneous information from the former
agent will be propagated to the latter through prompts, which can make the
latter agents generate buggy code. In this paper, we propose an LLM-based
coopetitive multi-agent prompting framework, in which the agents cannot
collaborate with each other to form the generation pipeline, but also create a
healthy competitive mechanism to improve the generating quality. Our
experimental results show that the coopetitive multi-agent framework can
effectively mitigate the degeneration risk and reduce the error propagation
while improving code error correction capabilities, resulting in higher quality
Verilog code generation. The effectiveness of our approach is validated through
extensive experiments. On VerilogEval Machine and Human dataset,
CoopetitiveV+GPT-4 achieves 99.2% and 99.1% pass@10 scores, respectively. While
on RTLLM, CoopetitiveV+GPT-4 obtains 100% syntax and 99.9% functionality pass@5
scores.

</details>


### [503] [Active Learning of Piecewise Gaussian Process Surrogates](https://arxiv.org/pdf/2301.08789)
*Chiwoo Park, Robert Waelder, Bonggwon Kang, Benji Maruyama, Soondo Hong, Robert Gramacy*

Main category: cs.LG

TL;DR: The paper introduces an active learning method for piecewise, Jump Gaussian Process (GP) surrogates, emphasizing the need to account for model bias in addition to uncertainty.


<details>
  <summary>Details</summary>
Motivation: Active learning of GP surrogates is useful for optimizing experimental designs and data acquisition, but existing methods overlook model bias in discontinuous (Jump GP) contexts.

Method: Develops active learning heuristics for Jump GPs, including a bias and variance estimator, adapted from ordinary GP strategies.

Result: Demonstrates the method's advantage on synthetic benchmarks and real-simulation experiments.

Conclusion: Accounting for model bias is crucial in Jump GP active learning, and the proposed method outperforms traditional approaches.

Abstract: Active learning of Gaussian process (GP) surrogates has been useful for
optimizing experimental designs for physical/computer simulation experiments,
and for steering data acquisition schemes in machine learning. In this paper,
we develop a method for active learning of piecewise, Jump GP surrogates. Jump
GPs are continuous within, but discontinuous across, regions of a design space,
as required for applications spanning autonomous materials design,
configuration of smart factory systems, and many others. Although our active
learning heuristics are appropriated from strategies originally designed for
ordinary GPs, we demonstrate that additionally accounting for model bias, as
opposed to the usual model uncertainty, is essential in the Jump GP context.
Toward that end, we develop an estimator for bias and variance of Jump GP
models. Illustrations, and evidence of the advantage of our proposed methods,
are provided on a suite of synthetic benchmarks, and real-simulation
experiments of varying complexity.

</details>


### [504] [Peri-LN: Revisiting Normalization Layer in the Transformer Architecture](https://arxiv.org/pdf/2502.02732)
*Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, Kang Min Yoo*

Main category: cs.LG

TL;DR: The paper analyzes layer normalization (LN) strategies in Transformers, introducing Peri-LN as a promising alternative to Pre-LN and Post-LN, with benefits in training stability and convergence.


<details>
  <summary>Details</summary>
Motivation: The difficulty in selecting LN strategies for large-scale Transformers and the unexplored potential of Peri-LN motivated this study.

Method: The authors conducted theoretical analysis and experiments on Transformers up to 3.2B parameters to compare LN strategies.

Result: Peri-LN showed balanced variance growth, steadier gradient flow, and better convergence stability compared to Pre-LN and Post-LN.

Conclusion: Peri-LN is a viable alternative for large-scale Transformers, offering insights into optimal LN placement.

Abstract: Selecting a layer normalization (LN) strategy that stabilizes training and
speeds convergence in Transformers remains difficult, even for today's large
language models (LLM). We present a comprehensive analytical foundation for
understanding how different LN strategies influence training dynamics in
large-scale Transformers. Until recently, Pre-LN and Post-LN have long
dominated practices despite their limitations in large-scale training. However,
several open-source models have recently begun silently adopting a third
strategy without much explanation. This strategy places normalization layer
peripherally around sublayers, a design we term Peri-LN. While Peri-LN has
demonstrated promising performance, its precise mechanisms and benefits remain
almost unexplored. Our in-depth analysis delineates the distinct behaviors of
LN strategies, showing how each placement shapes activation variance and
gradient propagation. To validate our theoretical insight, we conduct extensive
experiments on Transformers up to $3.2$B parameters, showing that Peri-LN
consistently achieves more balanced variance growth, steadier gradient flow,
and convergence stability. Our results suggest that Peri-LN warrants broader
consideration for large-scale Transformer architectures, providing renewed
insights into the optimal placement of LN.

</details>


### [505] [Quantifying the Optimization and Generalization Advantages of Graph Neural Networks Over Multilayer Perceptrons](https://arxiv.org/pdf/2306.13926)
*Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, Taiji Suzuki*

Main category: cs.LG

TL;DR: The paper compares GNNs and MLPs in optimization and generalization, showing GNNs prioritize signal learning, outperforming MLPs by a factor of $D^{q-2}$ in low test error regimes.


<details>
  <summary>Details</summary>
Motivation: To address the lack of quantitative analysis comparing GNNs and MLPs from an optimization and generalization perspective.

Method: Uses a signal-noise data model to analyze two-layer GCNs and MLPs, tracking signal learning and noise memorization.

Result: GNNs significantly prioritize signal learning, improving low test error regimes over MLPs by $D^{q-2}$ times.

Conclusion: GNNs exhibit a substantial quantitative advantage over MLPs in optimization and generalization, supported by empirical evidence.

Abstract: Graph neural networks (GNNs) have demonstrated remarkable capabilities in
learning from graph-structured data, often outperforming traditional Multilayer
Perceptrons (MLPs) in numerous graph-based tasks. Although existing works have
demonstrated the benefits of graph convolution through Laplacian smoothing,
expressivity or separability, there remains a lack of quantitative analysis
comparing GNNs and MLPs from an optimization and generalization perspective.
This study aims to address this gap by examining the role of graph convolution
through feature learning theory. Using a signal-noise data model, we conduct a
comparative analysis of the optimization and generalization between two-layer
graph convolutional networks (GCNs) and their MLP counterparts. Our approach
tracks the trajectory of signal learning and noise memorization in GNNs,
characterizing their post-training generalization. We reveal that GNNs
significantly prioritize signal learning, thus enhancing the regime of {low
test error} over MLPs by $D^{q-2}$ times, where $D$ denotes a node's expected
degree and $q$ is the power of ReLU activation function with $q>2$. This
finding highlights a substantial and quantitative discrepancy between GNNs and
MLPs in terms of optimization and generalization, a conclusion further
supported by our empirical simulations on both synthetic and real-world
datasets.

</details>


### [506] [LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws](https://arxiv.org/pdf/2502.12120)
*Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel*

Main category: cs.LG

TL;DR: Pretraining data and tokenizer strongly influence loss-to-loss scaling in LLMs, while model size, hyperparameters, and architecture have minimal impact.


<details>
  <summary>Details</summary>
Motivation: To identify key factors affecting loss-to-loss scaling laws in LLMs for better performance understanding.

Method: Conducted experiments analyzing the influence of pretraining data, tokenizer, model size, hyperparameters, and architecture on scaling trends.

Result: Pretraining data and tokenizer are the dominant factors; other variables like model size and architecture have limited impact.

Conclusion: Practitioners should prioritize dataset curation for downstream performance, while freely optimizing architectures for efficiency.

Abstract: Scaling laws guide the development of large language models (LLMs) by
offering estimates for the optimal balance of model size, tokens, and compute.
More recently, loss-to-loss scaling laws that relate losses across pretraining
datasets and downstream tasks have emerged as a powerful tool for understanding
and improving LLM performance. In this work, we investigate which factors most
strongly influence loss-to-loss scaling. Our experiments reveal that the
pretraining data and tokenizer determine the scaling trend. In contrast, model
size, optimization hyperparameters, and even significant architectural
differences, such as between transformer-based models like Llama and
state-space models like Mamba, have limited impact. Consequently, practitioners
should carefully curate suitable pretraining datasets for optimal downstream
performance, while architectures and other settings can be freely optimized for
training efficiency.

</details>


### [507] [Visualizing, Rethinking, and Mining the Loss Landscape of Deep Neural Networks](https://arxiv.org/pdf/2405.12493)
*Yichu Xu, Xin-Chun Li, Lan Li, De-Chuan Zhan*

Main category: cs.LG

TL;DR: The paper explores the loss landscape of DNNs, categorizing 1D curves and visualizing 2D surfaces to uncover complex local geometries, supported by theoretical insights from the Hessian matrix.


<details>
  <summary>Details</summary>
Motivation: The observation that loss surfaces along Gaussian noise directions resemble v-basin shapes prompts investigation into whether 1D/2D subspaces can reveal more complex structures and how to identify such directions.

Method: Systematically categorizes 1D curves (v-basin, v-side, w-basin, w-peak, vvv-basin) and develops algorithms to mine these directions, then visualizes 2D surfaces like saddle and wine bottle bottoms.

Result: Identifies complex 1D curve types and visualizes diverse 2D surfaces, previously only seen in demo functions, with theoretical explanations via the Hessian matrix.

Conclusion: The study advances understanding of DNN loss landscapes by revealing hidden geometries and providing tools to analyze them, supported by theoretical insights.

Abstract: The loss landscape of deep neural networks (DNNs) is commonly considered
complex and wildly fluctuated. However, an interesting observation is that the
loss surfaces plotted along Gaussian noise directions are almost v-basin ones
with the perturbed model lying on the basin. This motivates us to rethink
whether the 1D or 2D subspace could cover more complex local geometry
structures, and how to mine the corresponding perturbation directions. This
paper systematically and gradually categorizes the 1D curves from simple to
complex, including v-basin, v-side, w-basin, w-peak, and vvv-basin curves.
Notably, the latter two types are already hard to obtain via the intuitive
construction of specific perturbation directions, and we need to propose proper
mining algorithms to plot the corresponding 1D curves. Combining these 1D
directions, various types of 2D surfaces are visualized such as the saddle
surfaces and the bottom of a bottle of wine that are only shown by demo
functions in previous works. Finally, we propose theoretical insights from the
lens of the Hessian matrix to explain the observed several interesting
phenomena.

</details>


### [508] [Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations](https://arxiv.org/pdf/2502.18147)
*Lucy Farnik, Tim Lawson, Conor Houghton, Laurence Aitchison*

Main category: cs.LG

TL;DR: The paper introduces Jacobian Sparse Autoencoders (JSAEs) to sparsify computations in LLMs, not just activations, and shows they preserve performance while revealing learned computational sparsity.


<details>
  <summary>Details</summary>
Motivation: Traditional SAEs sparsify latent activations but not computations, limiting understanding of LLM computations. JSAEs aim to address this gap.

Method: Proposes JSAEs to sparsify input/output activations and Jacobians (computations). Includes an efficient Jacobian computation method for tractability.

Result: JSAEs achieve computational sparsity without harming performance, reveal learned sparsity in LLMs, and show MLPs are approximately linear in JSAE basis.

Conclusion: JSAEs are better than SAEs for understanding LLM computations, as they reveal learned computational sparsity and preserve performance.

Abstract: Sparse autoencoders (SAEs) have been successfully used to discover sparse and
human-interpretable representations of the latent activations of LLMs. However,
we would ultimately like to understand the computations performed by LLMs and
not just their representations. The extent to which SAEs can help us understand
computations is unclear because they are not designed to "sparsify"
computations in any sense, only latent activations. To solve this, we propose
Jacobian SAEs (JSAEs), which yield not only sparsity in the input and output
activations of a given model component but also sparsity in the computation
(formally, the Jacobian) connecting them. With a na\"ive implementation, the
Jacobians in LLMs would be computationally intractable due to their size. One
key technical contribution is thus finding an efficient way of computing
Jacobians in this setup. We find that JSAEs extract a relatively large degree
of computational sparsity while preserving downstream LLM performance
approximately as well as traditional SAEs. We also show that Jacobians are a
reasonable proxy for computational sparsity because MLPs are approximately
linear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a
greater degree of computational sparsity on pre-trained LLMs than on the
equivalent randomized LLM. This shows that the sparsity of the computational
graph appears to be a property that LLMs learn through training, and suggests
that JSAEs might be more suitable for understanding learned transformer
computations than standard SAEs.

</details>


### [509] [ProofAug: Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis](https://arxiv.org/pdf/2501.18310)
*Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao*

Main category: cs.LG

TL;DR: ProofAug enhances LLMs by integrating automation tools at various granularities, improving theorem-proving performance on benchmarks like miniF2F.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully leverage automation tools in proof synthesis with LLMs, limiting their potential.

Method: ProofAug analyzes model-generated proofs at fine-grained levels and integrates with tree-search algorithms, including a recursive proving module.

Result: Achieves 66.0% pass rate on miniF2F (vs. 56.1% by SOTA) and boosts Lean 4 performance from 44.3% to 50.4%.

Conclusion: ProofAug effectively combines LLMs with automation tools, significantly improving theorem-proving efficiency and accuracy.

Abstract: The synergy between deep learning models and traditional automation tools,
such as built-in tactics of the proof assistant and off-the-shelf automated
theorem provers, plays a crucial role in developing robust and efficient neural
theorem provers(NTPs). However, for proof synthesis with LLMs, previous work
applies automation tools either only when explicitly invoked by the model or at
a single granularity level, failing to fully exploit their power. To solve this
issue, we propose ProofAug, a procedure that equips LLMs with automation
methods at various granularities through fine-grained structure analysis of
model-generated proof proposals. ProofAug also serves as a versatile
plug-and-play module that seamlessly integrates with any tree-search algorithm,
enabling our construction of an efficient recursive proving (ERP) module to
further enhance performance. The superiority of our method is validated on the
miniF2F benchmark using the open-source deepseek-math-7b-base model and the
Isabelle proof assistant. Notably, by additionally employing a mixed prompting
strategy, we achieve a cumulative pass rate of 66.0% after curation of the
dataset (61.9% for the original version) with 2100 queries to the model per
problem (In contrast, the previous SOTA in Isabelle, Subgoal-XL, only achieves
56.1% using 16384 queries per problem). We also implement a Lean 4 version of
ProofAug that can improve the pass@1 performance of
Kimina-Prover-Preview-Distill-1.5B from 44.3% to 50.4% on miniF2F-test. Our
code is available at https://github.com/haoxiongliu/ProofAug.

</details>


### [510] [Exploring Dark Knowledge under Various Teacher Capacities and Addressing Capacity Mismatch](https://arxiv.org/pdf/2405.13078)
*Wen-Shu Fan, Xin-Chun Li, De-Chuan Zhan*

Main category: cs.LG

TL;DR: This paper investigates the 'dark knowledge' in Knowledge Distillation (KD), finding that larger teachers produce less distinct probabilities for incorrect classes. It addresses 'capacity mismatch' with simple solutions, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: To understand how teacher model capacity affects KD performance and why larger teachers sometimes underperform smaller ones.

Method: Analyzes output logits and probabilities of teachers with varying capacities, conducts experiments, and proposes solutions for capacity mismatch.

Result: Larger teachers produce less distinct probabilities for incorrect classes, and relative class affinity remains consistent. Proposed solutions outperform previous methods.

Conclusion: Distinctness among incorrect classes is key to dark knowledge. Addressing capacity mismatch improves KD performance.

Abstract: Knowledge Distillation (KD) could transfer the ``dark knowledge" of a
well-performed yet large neural network to a weaker but lightweight one. From
the view of output logits and softened probabilities, this paper goes deeper
into the dark knowledge provided by teachers with different capacities. Two
fundamental observations are: (1) a larger teacher tends to produce probability
vectors with lower distinction among non-ground-truth classes; (2) teachers
with different capacities are basically consistent in their cognition of
relative class affinity. Through abundant experimental studies we verify these
observations and provide in-depth empirical explanations to them. We argue that
the distinctness among incorrect classes embodies the essence of dark
knowledge. A larger and more accurate teacher lacks this distinctness, which
hampers its teaching ability compared to a smaller teacher, ultimately leading
to the peculiar phenomenon named "capacity mismatch". Building on this insight,
this paper explores multiple simple yet effective ways to address capacity
mismatch, achieving superior experimental results compared to previous
approaches.

</details>


### [511] [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/pdf/2503.05613)
*Dong Shu, Xuansheng Wu, Haiyan Zhao, Daking Rai, Ziyu Yao, Ninghao Liu, Mengnan Du*

Main category: cs.LG

TL;DR: The paper surveys Sparse Autoencoders (SAEs) for interpreting LLMs, covering their framework, feature explanation methods, evaluation metrics, and real-world applications.


<details>
  <summary>Details</summary>
Motivation: To address the opacity of LLMs by using SAEs to disentangle and interpret their internal features.

Method: Comprehensive survey of SAEs, including their architecture, training strategies, feature explanation methods, and evaluation metrics.

Result: SAEs are effective for interpreting LLMs, with insights into their design, explanation approaches, and practical applications.

Conclusion: SAEs are a promising tool for mechanistic interpretability of LLMs, with potential for further research and applications.

Abstract: Large Language Models (LLMs) have transformed natural language processing,
yet their internal mechanisms remain largely opaque. Recently, mechanistic
interpretability has attracted significant attention from the research
community as a means to understand the inner workings of LLMs. Among various
mechanistic interpretability approaches, Sparse Autoencoders (SAEs) have
emerged as a promising method due to their ability to disentangle the complex,
superimposed features within LLMs into more interpretable components. This
paper presents a comprehensive survey of SAEs for interpreting and
understanding the internal workings of LLMs. Our major contributions include:
(1) exploring the technical framework of SAEs, covering basic architecture,
design improvements, and effective training strategies; (2) examining different
approaches to explaining SAE features, categorized into input-based and
output-based explanation methods; (3) discussing evaluation methods for
assessing SAE performance, covering both structural and functional metrics; and
(4) investigating real-world applications of SAEs in understanding and
manipulating LLM behaviors.

</details>


### [512] [An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for Anomaly Detection in CAN Bus](https://arxiv.org/pdf/2501.18821)
*Mohammad Fatahi, Danial Sadrian Zadeh, Benyamin Ghojogh, Behzad Moshiri, Otman Basir*

Main category: cs.LG

TL;DR: The paper proposes a cascade feature-level spatiotemporal fusion framework for robust anomaly detection in CAN buses, outperforming existing methods with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning models for CAN anomaly detection lack robustness and fail to comprehensively detect attacks due to focusing on a subset of anomaly structures.

Method: A two-parameter genetic algorithm (2P-GA)-optimized cascade architecture integrates spatial and temporal features to cover all anomaly structures.

Result: The model achieves an AUC-ROC of 0.9987, detects all attack types with 100% accuracy, and improves precision and recall.

Conclusion: The framework provides a validated, robust solution for real-world CAN security challenges.

Abstract: Intelligent transportation systems (ITS) play a pivotal role in modern
infrastructure but face security risks due to the broadcast-based nature of the
in-vehicle Controller Area Network (CAN) buses. While numerous machine learning
models and strategies have been proposed to detect CAN anomalies, existing
approaches lack robustness evaluations and fail to comprehensively detect
attacks due to shifting their focus on a subset of dominant structures of
anomalies. To overcome these limitations, the current study proposes a cascade
feature-level spatiotemporal fusion framework that integrates the spatial
features and temporal features through a two-parameter genetic algorithm
(2P-GA)-optimized cascade architecture to cover all dominant structures of
anomalies. Extensive paired t-test analysis confirms that the model achieves an
AUC-ROC of 0.9987, demonstrating robust anomaly detection capabilities. The
Spatial Module improves the precision by approximately 4%, while the Temporal
Module compensates for recall losses, ensuring high true positive rates. The
proposed framework detects all attack types with 100% accuracy on the
CAR-HACKING dataset, outperforming state-of-the-art methods. This study
provides a validated, robust solution for real-world CAN security challenges.

</details>


### [513] [DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation](https://arxiv.org/pdf/2406.10737)
*Yunbei Zhang, Akshay Mehra, Shuaicheng Niu, Jihun Hamm*

Main category: cs.LG

TL;DR: DPCore improves CTTA by efficiently adapting to dynamic domain changes, preserving knowledge, and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Real-world domain changes are dynamic and recurrent, challenging existing CTTA methods that struggle with brief exposures and knowledge retention.

Method: DPCore uses Visual Prompt Adaptation, a Prompt Coreset, and a Dynamic Update mechanism to align domains, preserve knowledge, and adapt efficiently.

Result: DPCore outperforms other CTTA methods, reducing parameters by 99% and computation time by 64%.

Conclusion: DPCore is robust and efficient for dynamic domain changes, setting a new standard in CTTA.

Abstract: Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained
models to continually changing, unseen target domains. While existing CTTA
methods assume structured domain changes with uniform durations, real-world
environments often exhibit dynamic patterns where domains recur with varying
frequencies and durations. Current approaches, which adapt the same parameters
across different domains, struggle in such dynamic conditions-they face
convergence issues with brief domain exposures, risk forgetting previously
learned knowledge, or misapplying it to irrelevant domains. To remedy this, we
propose DPCore, a method designed for robust performance across diverse domain
change patterns while ensuring computational efficiency. DPCore integrates
three key components: Visual Prompt Adaptation for efficient domain alignment,
a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism
that intelligently adjusts existing prompts for similar domains while creating
new ones for substantially different domains. Extensive experiments on four
benchmarks demonstrate that DPCore consistently outperforms various CTTA
methods, achieving state-of-the-art performance in both structured and dynamic
settings while reducing trainable parameters by 99% and computation time by 64%
compared to previous approaches.

</details>


### [514] [TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining](https://arxiv.org/pdf/2504.02107)
*Jeffrey Li, Mohammadreza Armandpour, Iman Mirzadeh, Sachin Mehta, Vaishaal Shankar, Raviteja Vemulapalli, Samy Bengio, Oncel Tuzel, Mehrdad Farajtabar, Hadi Pouransari, Fartash Faghri*

Main category: cs.LG

TL;DR: The paper explores updating LLMs with new data, introduces a large-scale dataset for continual pretraining, and evaluates methods to balance new data integration with old data retention.


<details>
  <summary>Details</summary>
Motivation: LLMs trained on historical web data become outdated, necessitating strategies to update them efficiently without losing past knowledge.

Method: The study uses a web-scale dataset from Common Crawl for continual pretraining and evaluates time-stratified performance across general and domain-specific data. Autoregressive meta-schedules and fixed-ratio replay are tested.

Result: Autoregressive meta-schedules with replay achieve comparable performance to full retraining but with 2.6x less computation. Replay is vital for generic data but less so for specific domains.

Conclusion: Efficient LLM updates require balancing new data integration with old data replay, with domain-specific needs influencing the optimal approach.

Abstract: Large Language Models (LLMs) trained on historical web data inevitably become
outdated. We investigate evaluation strategies and update methods for LLMs as
new data becomes available. We introduce a web-scale dataset for time-continual
pretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of
magnitude larger than previous continual language modeling benchmarks. We also
design time-stratified evaluations across both general CC data and specific
domains (Wikipedia, StackExchange, and code documentation) to assess how well
various continual learning methods adapt to new data while retaining past
knowledge. Our findings demonstrate that, on general CC data, autoregressive
meta-schedules combined with a fixed-ratio replay of older data can achieve
comparable held-out loss to re-training from scratch, while requiring
significantly less computation (2.6x). However, the optimal balance between
incorporating new data and replaying old data differs as replay is crucial to
avoid forgetting on generic web data but less so on specific domains.

</details>


### [515] [Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods](https://arxiv.org/pdf/2502.01618)
*Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, Akash Srivastava*

Main category: cs.LG

TL;DR: The paper proposes a probabilistic inference-based method for scaling LLMs at inference time, outperforming deterministic search methods by 4-16x and achieving high accuracy with fewer rollouts.


<details>
  <summary>Details</summary>
Motivation: Diminishing returns from scaling model sizes/data motivate exploring inference-time scaling, but existing methods are vulnerable to reward hacking.

Method: Casts inference-time scaling as a probabilistic inference task, using particle-based Monte Carlo methods to explore the typical set of a state-space model.

Result: Achieves 4-16x better scaling rates than deterministic methods; Qwen2.5-Math models surpass GPT-4o accuracy with few rollouts.

Conclusion: Connects probabilistic inference literature with LLM scaling, offering a robust alternative to reward-based methods.

Abstract: Large language models (LLMs) have achieved significant performance gains via
scaling up model sizes and/or data. However, recent evidence suggests
diminishing returns from such approaches, motivating scaling the computation
spent at inference time. Existing inference-time scaling methods, usually with
reward models, cast the task as a search problem, which tends to be vulnerable
to reward hacking as a consequence of approximation errors in reward models. In
this paper, we instead cast inference-time scaling as a probabilistic inference
task and leverage sampling-based techniques to explore the typical set of the
state distribution of a state-space model with an approximate likelihood,
rather than optimize for its mode directly. We propose a novel inference-time
scaling approach by adapting particle-based Monte Carlo methods to this task.
Our empirical evaluation demonstrates that our methods have a 4-16x better
scaling rate over our deterministic search counterparts on various challenging
mathematical reasoning tasks. Using our approach, we show that
Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,
while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.
Our work not only presents an effective method to inference-time scaling, but
also connects the rich literature in probabilistic inference with
inference-time scaling of LLMs to develop more robust algorithms in future
work. Code, videos, and further information available at
https://probabilistic-inference-scaling.github.io.

</details>


### [516] [Learning Time-Varying Multi-Region Communications via Scalable Markovian Gaussian Processes](https://arxiv.org/pdf/2407.00397)
*Weihan Li, Yule Wang, Chengrui Li, Anqi Wu*

Main category: cs.LG

TL;DR: A novel framework (ADM) using Markovian Gaussian Processes to model time-varying brain communications, scalable for large datasets and capturing dynamic interactions.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to identify time-varying region-level communications or scale to large neural datasets with long recordings.

Method: Combines Gaussian Processes with State Space Models and uses parallel scan inference for efficiency.

Result: Identifies concurrent, evolving communication patterns and validates on synthetic and neural datasets.

Conclusion: Advances understanding of neural computation and provides a scalable tool for dynamic brain network analysis.

Abstract: Understanding and constructing brain communications that capture dynamic
communications across multiple regions is fundamental to modern system
neuroscience, yet current methods struggle to find time-varying region-level
communications or scale to large neural datasets with long recording durations.
We present a novel framework using Markovian Gaussian Processes to learn brain
communications with time-varying temporal delays from multi-region neural
recordings, named Adaptive Delay Model (ADM). Our method combines Gaussian
Processes with State Space Models and employs parallel scan inference
algorithms, enabling efficient scaling to large datasets while identifying
concurrent communication patterns that evolve over time. This time-varying
approach captures how brain region interactions shift dynamically during
cognitive processes. Validated on synthetic and multi-region neural recordings
datasets, our approach discovers both the directionality and temporal dynamics
of neural communication. This work advances our understanding of distributed
neural computation and provides a scalable tool for analyzing dynamic brain
networks.

</details>


### [517] [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/pdf/2504.13818)
*Yixuan Even Xu, Yash Savani, Fei Fang, Zico Kolter*

Main category: cs.LG

TL;DR: PODS (Policy Optimization with Down-Sampling) reduces memory and computation costs in RLVR by training on an informative subset of rollouts, improving efficiency without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Address the asymmetry in computation and memory requirements between rollout generation and policy updates in RLVR.

Method: Introduce PODS with max-variance down-sampling to select an informative subset of rollouts for training, reducing update costs.

Result: PODS with GRPO outperforms standard GRPO in reasoning benchmarks and hardware environments.

Conclusion: PODS is an effective solution to the computational and memory constraints in RLVR, enhancing efficiency and performance.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing reasoning capabilities in large language
models. However, it is constrained by a fundamental asymmetry in computation
and memory requirements: rollout generation is embarrassingly parallel and
memory-light, whereas policy updates are communication-heavy and
memory-intensive. To address this, we introduce PODS (Policy Optimization with
Down-Sampling). PODS produces numerous rollouts in parallel, then trains on
only an informative subset, preserving learning signals while slashing update
cost. We instantiate PODS with max-variance down-sampling, a principled
criterion that maximises reward diversity and show it admits an $O(n\log n)$
solution. Empirically, coupling PODS with Group Relative Policy Optimization
(GRPO) achieves superior performance over standard GRPO across different
reasoning benchmarks and hardware environments.

</details>


### [518] [The Complexity of Learning Sparse Superposed Features with Feedback](https://arxiv.org/pdf/2502.05407)
*Akash Kumar*

Main category: cs.LG

TL;DR: The paper explores retrieving latent features in deep networks using feedback from an agent (e.g., LLM) via triplet comparisons, analyzing feedback complexity and validating results on feature recovery and dictionary extraction.


<details>
  <summary>Details</summary>
Motivation: Deep networks' success relies on capturing latent features, but efficiently retrieving these features via agent feedback (e.g., LLM) remains unexplored.

Method: Uses triplet comparisons from an agent to retrieve features, analyzes feedback complexity, and tests on feature recovery and dictionary extraction tasks.

Result: Establishes tight bounds for feature matrix learning with agent-constructed activations and strong upper bounds in sparse settings. Validated on practical applications.

Conclusion: Agent feedback via triplet comparisons is effective for retrieving latent features, with theoretical and empirical support.

Abstract: The success of deep networks is crucially attributed to their ability to
capture latent features within a representation space. In this work, we
investigate whether the underlying learned features of a model can be
efficiently retrieved through feedback from an agent, such as a large language
model (LLM), in the form of relative \textit{triplet comparisons}. These
features may represent various constructs, including dictionaries in LLMs or a
covariance matrix of Mahalanobis distances. We analyze the feedback complexity
associated with learning a feature matrix in sparse settings. Our results
establish tight bounds when the agent is permitted to construct activations and
demonstrate strong upper bounds in sparse scenarios when the agent's feedback
is limited to distributional information. We validate our theoretical findings
through experiments on two distinct applications: feature recovery from
Recursive Feature Machines and dictionary extraction from sparse autoencoders
trained on Large Language Models.

</details>


### [519] [HO-FMN: Hyperparameter Optimization for Fast Minimum-Norm Attacks](https://arxiv.org/pdf/2407.08806)
*Raffaele Mura, Giuseppe Floris, Luca Scionis, Giorgio Piras, Maura Pintor, Ambra Demontis, Giorgio Giacinto, Battista Biggio, Fabio Roli*

Main category: cs.LG

TL;DR: A parametric variation of the fast minimum-norm attack is proposed to dynamically adjust loss, optimizer, step-size scheduler, and hyperparameters, improving adversarial robustness evaluation.


<details>
  <summary>Details</summary>
Motivation: Fixed loss functions, optimizers, and hyperparameters in gradient-based attacks often lead to overly-optimistic robustness evaluations.

Method: A parametric variation of the fast minimum-norm attack algorithm is introduced, allowing dynamic adjustments of components like loss and optimizer.

Result: The attack finds smaller adversarial perturbations without additional tuning and enables reporting robustness as a function of perturbation budget.

Conclusion: The proposed method provides a more complete and efficient evaluation of adversarial robustness compared to fixed-budget attacks.

Abstract: Gradient-based attacks are a primary tool to evaluate robustness of
machine-learning models. However, many attacks tend to provide
overly-optimistic evaluations as they use fixed loss functions, optimizers,
step-size schedulers, and default hyperparameters. In this work, we tackle
these limitations by proposing a parametric variation of the well-known fast
minimum-norm attack algorithm, whose loss, optimizer, step-size scheduler, and
hyperparameters can be dynamically adjusted. We re-evaluate 12 robust models,
showing that our attack finds smaller adversarial perturbations without
requiring any additional tuning. This also enables reporting adversarial
robustness as a function of the perturbation budget, providing a more complete
evaluation than that offered by fixed-budget attacks, while remaining
efficient. We release our open-source code at https://github.com/pralab/HO-FMN.

</details>


### [520] [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/pdf/2506.00845)
*Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xinyun Liu, Yulia Tsvetkov*

Main category: cs.LG

TL;DR: The paper proposes using reinforcement learning (RL) to improve LLMs' generalization from synthetic graph data to real-world tasks, outperforming baselines by 12.9% on average.


<details>
  <summary>Details</summary>
Motivation: Existing methods fine-tune LLMs on synthetic graph data, but lack generalization to real-world tasks with implicit graph structures. RL is proposed to address this gap.

Method: Solution-based and process-based rewards are designed for synthetic graph problems. RL algorithms (GRPO, DPO) are applied to align LLMs, comparing them on in-domain and out-of-domain tasks.

Result: RL improves performance by 12.9% on average across 5 datasets, with process-based rewards outperforming solution-based ones.

Conclusion: RL enhances generalization, but challenges like compositionality and explainable intermediate steps persist.

Abstract: Previous research has sought to enhance the graph reasoning capabilities of
LLMs by supervised fine-tuning on synthetic graph data. While these led to
specialized LLMs better at solving graph algorithm problems, we don't need LLMs
for shortest path: we need generalization from synthetic graph data to
real-world tasks with implicit graph structures. In this work, we propose to
unlock generalizable learning of graph synthetic data with reinforcement
learning. We first design solution-based and process-based rewards for
synthetic graph problems: instead of rigid memorizing response patterns in
direct fine-tuning, we posit that RL would help LLMs grasp the essentials
underlying graph reasoning and alleviate overfitting. We employ RL algorithms
such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on
synthetic graph data. We then compare them against existing settings on both
in-domain synthetic tasks and out-of-domain real-world tasks with implicit
graph structures such as multi-hop QA, structured planning, and more. Extensive
experiments demonstrate that our RL recipe leads to statistically significant
improvement on 5 datasets, with an average gain of 12.9\% over baseline
settings. Further analysis reveals that process-based rewards consistently
outperform solution-based rewards, mixing synthetic and real-world task data
yields potential gains, while compositionality and explainable intermediate
steps remains a critical challenge even after RL.

</details>


### [521] [RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models](https://arxiv.org/pdf/2502.09003)
*Quan Wei, Chung-Yiu Yau, Hoi-To Wai, Yang Katie Zhao, Dongyeop Kang, Youngsuk Park, Mingyi Hong*

Main category: cs.LG

TL;DR: RoSTE combines quantization-aware fine-tuning with adaptive rotation to improve LLM performance post-quantization.


<details>
  <summary>Details</summary>
Motivation: Conventional fine-then-quantize pipelines underperform due to lack of synergy between fine-tuning and quantization.

Method: Proposes RoSTE, integrating quantization-aware fine-tuning with adaptive rotation to reduce activation outliers.

Result: RoSTE outperforms post-SFT quantization baselines across tasks and LLM architectures.

Conclusion: RoSTE effectively manages quantization error and enhances performance, validated by theoretical and experimental results.

Abstract: Supervised fine-tuning is a standard method for adapting pre-trained large
language models (LLMs) to downstream tasks. Quantization has been recently
studied as a post-training technique for efficient LLM deployment. To obtain
quantized fine-tuned LLMs, conventional pipelines would first fine-tune the
pre-trained models, followed by post-training quantization. This often yields
suboptimal performance as it fails to leverage the synergy between fine-tuning
and quantization. To effectively realize low-bit quantization of weights,
activations and KV caches in LLMs, we propose an algorithm named Rotated
Straight-Through-Estimator (RoSTE), which combines quantization-aware
supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that
identifies an effective rotation configuration to reduce activation outliers.
We provide theoretical insights on RoSTE by analyzing its prediction error when
applied to an overparameterized least square quantized training problem. Our
findings reveal that the prediction error is directly proportional to the
quantization error of the converged weights, which can be effectively managed
through an optimized rotation configuration. Experiments on Pythia, Qwen and
Llama models of different sizes demonstrate the effectiveness of RoSTE.
Compared to existing post-SFT quantization baselines, our method consistently
achieves superior performances across various tasks and different LLM
architectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE.

</details>


### [522] [Robust and Efficient Transfer Learning via Supernet Transfer in Warm-started Neural Architecture Search](https://arxiv.org/pdf/2407.20279)
*Prabhant Singh, Joaquin Vanschoren*

Main category: cs.LG

TL;DR: A transfer learning method for NAS speeds up supernet training by 3-5x and improves model performance compared to training from scratch.


<details>
  <summary>Details</summary>
Motivation: Hand-designing neural networks is tedious, and NAS frameworks are computationally expensive, limiting accessibility.

Method: Proposes a transfer learning approach using pretrained supernets based on Optimal Transport or multi-dataset pretraining, applicable to DARTS-based NAS.

Result: Supernet training is 3-5x faster, and models outperform those trained from scratch. Positive transfer observed across most datasets.

Conclusion: The method enhances NAS applicability, enabling faster, better results and opening new avenues for continual learning.

Abstract: Hand-designing Neural Networks is a tedious process that requires significant
expertise. Neural Architecture Search (NAS) frameworks offer a very useful and
popular solution that helps to democratize AI. However, these NAS frameworks
are often computationally expensive to run, which limits their applicability
and accessibility. In this paper, we propose a novel transfer learning
approach, capable of effectively transferring pretrained supernets based on
Optimal Transport or multi-dataset pretaining. This method can be generally
applied to NAS methods based on Differentiable Architecture Search (DARTS).
Through extensive experiments across dozens of image classification tasks, we
demonstrate that transferring pretrained supernets in this way can not only
drastically speed up the supernet training which then finds optimal models (3
to 5 times faster on average), but even yield that outperform those found when
running DARTS methods from scratch. We also observe positive transfer to almost
all target datasets, making it very robust. Besides drastically improving the
applicability of NAS methods, this also opens up new applications for continual
learning and related fields.

</details>


### [523] [Kinetics: Rethinking Test-Time Scaling Laws](https://arxiv.org/pdf/2506.05333)
*Ranajoy Sadhukhan, Zhuoming Chen, Haizhong Zheng, Yang Zhou, Emma Strubell, Beidi Chen*

Main category: cs.LG

TL;DR: The paper challenges prior assumptions about smaller models' efficiency, introducing the Kinetics Scaling Law to better account for computation and memory costs, and proposes sparse attention for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the overestimation of smaller models' effectiveness and the overlooked memory bottlenecks in test-time scaling, aiming for more practical resource allocation.

Method: A holistic analysis of models (0.6B to 32B parameters) leading to the Kinetics Scaling Law, followed by proposing sparse attention to reduce per-token costs.

Result: Sparse attention models outperform dense ones, with significant accuracy gains (60+ points in low-cost, 5+ in high-cost regimes) on AIME benchmarks.

Conclusion: Sparse attention is crucial for maximizing test-time scaling potential, as accuracy continues to improve with increased computation, unlike in training.

Abstract: We rethink test-time scaling laws from a practical efficiency perspective,
revealing that the effectiveness of smaller models is significantly
overestimated. Prior work, grounded in compute-optimality, overlooks critical
memory access bottlenecks introduced by inference-time strategies (e.g.,
Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to
32B parameters, reveals a new Kinetics Scaling Law that better guides resource
allocation by incorporating both computation and memory access costs. Kinetics
Scaling Law suggests that test-time compute is more effective when used on
models above a threshold than smaller ones. A key reason is that in TTS,
attention, rather than parameter count, emerges as the dominant cost factor.
Motivated by this, we propose a new scaling paradigm centered on sparse
attention, which lowers per-token cost and enables longer generations and more
parallel samples within the same resource budget. Empirically, we show that
sparse attention models consistently outperform dense counterparts, achieving
over 60 points gains in low-cost regimes and over 5 points gains in high-cost
regimes for problem-solving accuracy on AIME, encompassing evaluations on
state-of-the-art MoEs. These results suggest that sparse attention is essential
and increasingly important with more computing invested, for realizing the full
potential of test-time scaling where, unlike training, accuracy has yet to
saturate as a function of computation, and continues to improve through
increased generation. The code is available at
https://github.com/Infini-AI-Lab/Kinetics.

</details>


### [524] [Relational Conformal Prediction for Correlated Time Series](https://arxiv.org/pdf/2502.09443)
*Andrea Cini, Alexander Jenkins, Danilo Mandic, Cesare Alippi, Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: The paper introduces CoRel, a novel conformal prediction method for uncertainty quantification in time series forecasting, leveraging graph deep learning to account for relational structures among correlated sequences.


<details>
  <summary>Details</summary>
Motivation: Existing methods for uncertainty quantification in time series forecasting ignore relational structures among correlated sequences, limiting their effectiveness.

Method: The proposed method, CoRel, combines conformal prediction and quantile regression with graph deep learning operators, does not require prior knowledge of the relational structure, and adapts to non-exchangeable data.

Result: CoRel achieves accurate coverage and state-of-the-art uncertainty quantification in benchmarks.

Conclusion: CoRel effectively addresses the gap in uncertainty quantification for relational time series data, offering a flexible and adaptive solution.

Abstract: We address the problem of uncertainty quantification in time series
forecasting by exploiting observations at correlated sequences. Relational deep
learning methods leveraging graph representations are among the most effective
tools for obtaining point estimates from spatiotemporal data and correlated
time series. However, the problem of exploiting relational structures to
estimate the uncertainty of such predictions has been largely overlooked in the
same context. To this end, we propose a novel distribution-free approach based
on the conformal prediction framework and quantile regression. Despite the
recent applications of conformal prediction to sequential data, existing
methods operate independently on each target time series and do not account for
relationships among them when constructing the prediction interval. We fill
this void by introducing a novel conformal prediction method based on graph
deep learning operators. Our approach, named Conformal Relational Prediction
(CoRel), does not require the relational structure (graph) to be known a priori
and can be applied on top of any pre-trained predictor. Additionally, CoRel
includes an adaptive component to handle non-exchangeable data and changes in
the input time series. Our approach provides accurate coverage and achieves
state-of-the-art uncertainty quantification in relevant benchmarks.

</details>


### [525] [Exploring Representations and Interventions in Time Series Foundation Models](https://arxiv.org/pdf/2409.12915)
*Michał Wiliński, Mononito Goswami, Willa Potosnak, Nina Żukowska, Artur Dubrawski*

Main category: cs.LG

TL;DR: The study explores the internal representations and learned concepts of Time Series Foundation Models (TSFMs), revealing block-like redundancy for pruning and demonstrating how latent space steering can manipulate model behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the internal representations and learned concepts of TSFMs, which are not well understood despite their potential for diverse applications.

Method: Analyzes the structure and redundancy of representations across TSFMs, examining self-similarity of layers and model sizes, and explores learned concepts like periodicity and trends through latent space steering.

Result: Reveals block-like redundancy for pruning and shows that latent space steering can introduce new features (e.g., periodicity or trends) to signals.

Conclusion: Representational analysis optimizes TSFMs, and conceptual steering enables controlled and efficient time series analysis.

Abstract: Time series foundation models (TSFMs) promise to be powerful tools for a wide
range of applications. However, their internal representations and learned
concepts are still not well understood. In this study, we investigate the
structure and redundancy of representations across various TSFMs, examining the
self-similarity of model layers within and across different model sizes. This
analysis reveals block-like redundancy in the representations, which can be
utilized for informed pruning to improve inference speed and efficiency.
Additionally, we explore the concepts learned by these models - such as
periodicity and trends - and how these can be manipulated through latent space
steering to influence model behavior. Our experiments show that steering
interventions can introduce new features, e.g., adding periodicity or trends to
signals that initially lacked them. These findings underscore the value of
representational analysis for optimizing models and demonstrate how conceptual
steering offers new possibilities for more controlled and efficient time series
analysis with TSFMs.

</details>


### [526] [Maximum Entropy Reinforcement Learning with Diffusion Policy](https://arxiv.org/pdf/2502.11612)
*Xiaoyi Dong, Jian Cheng, Xi Sheryl Zhang*

Main category: cs.LG

TL;DR: MaxEntDP uses a diffusion model as policy representation to enhance exploration and performance in MaxEnt RL, outperforming Gaussian policies and other generative models.


<details>
  <summary>Details</summary>
Motivation: The Gaussian policy in SAC limits exploration and performance in complex tasks due to its unimodality.

Method: Employ a diffusion model as the policy representation to capture multimodal distributions for MaxEnt RL.

Result: MaxEntDP outperforms Gaussian policies and other generative models, matching state-of-the-art diffusion-based RL algorithms.

Conclusion: Diffusion models improve exploration and policy robustness in MaxEnt RL, making MaxEntDP a competitive alternative.

Abstract: The Soft Actor-Critic (SAC) algorithm with a Gaussian policy has become a
mainstream implementation for realizing the Maximum Entropy Reinforcement
Learning (MaxEnt RL) objective, which incorporates entropy maximization to
encourage exploration and enhance policy robustness. While the Gaussian policy
performs well on simpler tasks, its exploration capacity and potential
performance in complex multi-goal RL environments are limited by its inherent
unimodality. In this paper, we employ the diffusion model, a powerful
generative model capable of capturing complex multimodal distributions, as the
policy representation to fulfill the MaxEnt RL objective, developing a method
named MaxEnt RL with Diffusion Policy (MaxEntDP). Our method enables efficient
exploration and brings the policy closer to the optimal MaxEnt policy.
Experimental results on Mujoco benchmarks show that MaxEntDP outperforms the
Gaussian policy and other generative models within the MaxEnt RL framework, and
performs comparably to other state-of-the-art diffusion-based online RL
algorithms. Our code is available at https://github.com/diffusionyes/MaxEntDP.

</details>


### [527] [Optimization Proxies using Limited Labeled Data and Training Time -- A Semi-Supervised Bayesian Neural Network Approach](https://arxiv.org/pdf/2410.03085)
*Parikshit Pareek, Abhijith Jayakumar, Kaarthik Sundar, Deepjyoti Deka, Sidhant Misra*

Main category: cs.LG

TL;DR: A semi-supervised Bayesian Neural Network (BNN) is proposed for constrained optimization, outperforming standard DNNs in scarce data settings with better feasibility and performance bounds.


<details>
  <summary>Details</summary>
Motivation: Address inefficiency of standard DNNs in constrained optimization with limited labeled data and training time.

Method: Sandwiched training alternating supervised (cost minimization) and unsupervised (constraint feasibility) steps.

Result: BNN reduces equality gaps tenfold and halves inequality gaps, with probabilistic confidence bounds.

Conclusion: BNN is effective for non-convex constrained optimization, offering practical advantages over DNNs.

Abstract: Constrained optimization problems arise in various engineering systems such
as inventory management and power grids. Standard deep neural network (DNN)
based machine learning proxies are ineffective in practical settings where
labeled data is scarce and training times are limited. We propose a
semi-supervised Bayesian Neural Networks (BNNs) based optimization proxy for
this complex regime, wherein training commences in a sandwiched fashion,
alternating between a supervised learning step for minimizing cost, and an
unsupervised learning step for enforcing constraint feasibility. We show that
the proposed semi-supervised BNN outperforms DNN architectures on important
non-convex constrained optimization problems from energy network operations,
achieving up to a tenfold reduction in expected maximum equality gap and
halving the inequality gaps. Further, the BNN's ability to provide posterior
samples is leveraged to construct practically meaningful probabilistic
confidence bounds on performance using a limited validation data, unlike prior
methods. The implementation code for this study is available at:
https://github.com/kaarthiksundar/BNN-OPF/.

</details>


### [528] [SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference](https://arxiv.org/pdf/2502.18137)
*Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, Jianfei Chen*

Main category: cs.LG

TL;DR: SpargeAttn is a universal sparse and quantized attention method that accelerates diverse models without sacrificing performance by using a two-stage online filter.


<details>
  <summary>Details</summary>
Motivation: The quadratic time complexity of attention in large models necessitates efficient implementations, but existing methods are model-specific and lack universality.

Method: SpargeAttn employs a two-stage online filter: first predicting the attention map to skip computations, then using a softmax-aware filter for further efficiency.

Result: The method significantly speeds up diverse models (language, image, video generation) without compromising end-to-end metrics.

Conclusion: SpargeAttn provides a universal solution for efficient attention, balancing speed and performance across various models.

Abstract: An efficient attention implementation is essential for large models due to
its quadratic time complexity. Fortunately, attention commonly exhibits
sparsity, i.e., many values in the attention map are near zero, allowing for
the omission of corresponding computations. Many studies have utilized the
sparse pattern to accelerate attention. However, most existing works focus on
optimizing attention within specific models by exploiting certain sparse
patterns of the attention map. A universal sparse attention that guarantees
both the speedup and end-to-end performance of diverse models remains elusive.
In this paper, we propose SpargeAttn, a universal sparse and quantized
attention for any model. Our method uses a two-stage online filter: in the
first stage, we rapidly and accurately predict the attention map, enabling the
skip of some matrix multiplications in attention. In the second stage, we
design an online softmax-aware filter that incurs no extra overhead and further
skips some matrix multiplications. Experiments show that our method
significantly accelerates diverse models, including language, image, and video
generation, without sacrificing end-to-end metrics. The codes are available at
https://github.com/thu-ml/SpargeAttn.

</details>


### [529] [Toward Efficient Kernel-Based Solvers for Nonlinear PDEs](https://arxiv.org/pdf/2410.11165)
*Zhitong Xu, Da Long, Yiming Xu, Guang Yang, Shandian Zhe, Houman Owhadi*

Main category: cs.LG

TL;DR: A new kernel learning framework simplifies solving nonlinear PDEs by removing differential operators from kernels, enabling scalable computation and straightforward implementation.


<details>
  <summary>Details</summary>
Motivation: Current kernel solvers embed differential operators in kernels, complicating computation with many collocation points. The new method avoids this, improving efficiency.

Method: The solution is modeled using standard kernel interpolation, with derivatives computed by differentiating the interpolant. Product kernels and grid collocation points leverage Kronecker product structures for scalability.

Result: The method reduces computational costs, scales efficiently with many collocation points, and maintains convergence under regularity assumptions. Numerical tests confirm its effectiveness.

Conclusion: The proposed framework offers a simpler, scalable solution for nonlinear PDEs, outperforming existing kernel methods in efficiency and ease of implementation.

Abstract: We introduce a novel kernel learning framework toward efficiently solving
nonlinear partial differential equations (PDEs). In contrast to the
state-of-the-art kernel solver that embeds differential operators within
kernels, posing challenges with a large number of collocation points, our
approach eliminates these operators from the kernel. We model the solution
using a standard kernel interpolation form and differentiate the interpolant to
compute the derivatives. Our framework obviates the need for complex Gram
matrix construction between solutions and their derivatives, allowing for a
straightforward implementation and scalable computation. As an instance, we
allocate the collocation points on a grid and adopt a product kernel, which
yields a Kronecker product structure in the interpolation. This structure
enables us to avoid computing the full Gram matrix, reducing costs and scaling
efficiently to a large number of collocation points. We provide a proof of the
convergence and rate analysis of our method under appropriate regularity
assumptions. In numerical experiments, we demonstrate the advantages of our
method in solving several benchmark PDEs.

</details>


### [530] [Laplace Transform Based Low-Complexity Learning of Continuous Markov Semigroups](https://arxiv.org/pdf/2410.14477)
*Vladimir R. Kostic, Karim Lounici, Hélène Halconruy, Timothée Devergne, Pietro Novelli, Massimiliano Pontil*

Main category: cs.LG

TL;DR: A data-driven method for learning Markov processes via spectral decomposition of the infinitesimal generator, addressing computational challenges and small time-lag issues.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for learning Markov processes are computationally expensive and lack guarantees for small time-lags.

Method: Uses the resolvent of the infinitesimal generator, leveraging the Laplace transform of transfer operators for robustness.

Result: Accurate eigenvalue learning for small time-lags, reduced computational complexity, and broader applicability.

Conclusion: The proposed method is efficient and robust, demonstrated through experiments.

Abstract: Markov processes serve as a universal model for many real-world random
processes. This paper presents a data-driven approach for learning these models
through the spectral decomposition of the infinitesimal generator (IG) of the
Markov semigroup. The unbounded nature of IGs complicates traditional methods
such as vector-valued regression and Hilbert-Schmidt operator analysis.
Existing techniques, including physics-informed kernel regression, are
computationally expensive and limited in scope, with no recovery guarantees for
transfer operator methods when the time-lag is small. We propose a novel method
that leverages the IG's resolvent, characterized by the Laplace transform of
transfer operators. This approach is robust to time-lag variations, ensuring
accurate eigenvalue learning even for small time-lags. Our statistical analysis
applies to a broader class of Markov processes than current methods while
reducing computational complexity from quadratic to linear in the state
dimension. Finally, we illustrate the behaviour of our method in two
experiments.

</details>


### [531] [Graph Attention Networks Unleashed: A Fast and Explainable Vulnerability Assessment Framework for Microgrids](https://arxiv.org/pdf/2503.00786)
*Wei Liu, Tao Zhang, Chenhui Lin, Kaiwen Li, Rui Wang*

Main category: cs.LG

TL;DR: A fast, explainable framework for microgrid vulnerability assessment combines MCS with a GAT-S model, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Microgrids need rapid, accurate vulnerability assessments for risk prevention, but existing methods are slow or lack explainability.

Method: Integrates MCS for training data with a GAT-S model to learn microgrid characteristics and assess vulnerability dynamically.

Result: Achieves low mean squared error (0.001), real-time responses (<1s), and explainable results.

Conclusion: The proposed framework enhances microgrid vulnerability assessment with speed, accuracy, and transparency.

Abstract: Independent microgrids are crucial for supplying electricity by combining
distributed energy resources and loads in scenarios like isolated islands and
field combat. Fast and accurate assessments of microgrid vulnerability against
intentional attacks or natural disasters are essential for effective risk
prevention and design optimization. However, conventional Monte Carlo
simulation (MCS) methods are computationally expensive and time-consuming,
while existing machine learning-based approaches often lack accuracy and
explainability. To address these challenges, this study proposes a fast and
explainable vulnerability assessment framework that integrates MCS with a graph
attention network enhanced by self-attention pooling (GAT-S). MCS generates
training data, while the GAT-S model learns the structural and electrical
characteristics of the microgrid and further assesses its vulnerability
intelligently. The GAT-S improves explainability and computational efficiency
by dynamically assigning attention weights to critical nodes. Comprehensive
experimental evaluations across various microgrid configurations demonstrate
that the proposed framework provides accurate vulnerability assessments,
achieving a mean squared error as low as 0.001, real-time responsiveness within
1 second, and delivering explainable results.

</details>


### [532] [SGD Jittering: A Training Strategy for Robust and Accurate Model-Based Architectures](https://arxiv.org/pdf/2410.14667)
*Peimeng Guan, Mark A. Davenport*

Main category: cs.LG

TL;DR: Proposes SGD jittering, a training scheme for model-based architectures (MBAs) to improve generalization and robustness in inverse problems.


<details>
  <summary>Details</summary>
Motivation: Addresses the underexplored accuracy-robustness tradeoff in MBAs, crucial for safety-critical applications.

Method: Introduces SGD jittering, injecting noise iteration-wise during reconstruction, and its SPGD extension.

Result: SGD jittering improves generalization and robustness, validated on denoising, seismic deconvolution, and MRI reconstruction.

Conclusion: SGD jittering enhances MBA performance for out-of-distribution data and adversarial robustness.

Abstract: Inverse problems aim to reconstruct unseen data from corrupted or perturbed
measurements. While most work focuses on improving reconstruction quality,
generalization accuracy and robustness are equally important, especially for
safety-critical applications. Model-based architectures (MBAs), such as loop
unrolling methods, are considered more interpretable and achieve better
reconstructions. Empirical evidence suggests that MBAs are more robust to
perturbations than black-box solvers, but the accuracy-robustness tradeoff in
MBAs remains underexplored. In this work, we propose a simple yet effective
training scheme for MBAs, called SGD jittering, which injects noise
iteration-wise during reconstruction. We theoretically demonstrate that SGD
jittering not only generalizes better than the standard mean squared error
training but is also more robust to average-case attacks. We validate SGD
jittering using denoising toy examples, seismic deconvolution, and single-coil
MRI reconstruction. Both SGD jittering and its SPGD extension yield cleaner
reconstructions for out-of-distribution data and demonstrates enhanced
robustness against adversarial attacks.

</details>


### [533] [SAGE: A Framework of Precise Retrieval for RAG](https://arxiv.org/pdf/2503.01713)
*Jintao Zhang, Guoliang Li, Jinyang Su*

Main category: cs.LG

TL;DR: SAGE improves RAG by addressing semantic segmentation and dynamic chunk selection, enhancing QA quality and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods fail in QA due to poor semantic segmentation and irrelevant context retrieval, limiting performance.

Method: SAGE introduces semantic segmentation, dynamic chunk selection, and LLM-based context adjustment to improve retrieval.

Result: SAGE outperforms baselines by 61.25% in QA quality and improves cost efficiency by 49.41%.

Conclusion: SAGE effectively addresses RAG limitations, offering insights for future improvements in retrieval-augmented generation.

Abstract: Retrieval-augmented generation (RAG) has demonstrated significant proficiency
in conducting question-answering (QA) tasks within a specified corpus.
Nonetheless, numerous failure instances of RAG in QA still exist. These
failures are not solely attributable to the limitations of Large Language
Models (LLMs); instead, they predominantly arise from the retrieval of
inaccurate information for LLMs due to two limitations: (1) Current RAG methods
segment the corpus without considering semantics, making it difficult to find
relevant context due to impaired correlation between questions and the
segments. (2) There is a trade-off between missing essential context with fewer
context retrieved and getting irrelevant context with more context retrieved.
  In this paper, we introduce a RAG framework (SAGE), to overcome these
limitations. First, to address the segmentation issue without considering
semantics, we propose to train a semantic segmentation model. This model is
trained to segment the corpus into semantically complete chunks. Second, to
ensure that only the most relevant chunks are retrieved while the irrelevant
ones are ignored, we design a chunk selection algorithm to dynamically select
chunks based on the decreasing speed of the relevance score, leading to a more
relevant selection. Third, to further ensure the precision of the retrieved
chunks, we propose letting LLMs assess whether retrieved chunks are excessive
or lacking and then adjust the amount of context accordingly. Experiments show
that SAGE outperforms baselines by 61.25% in the quality of QA on average.
Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the
tokens consumed in LLM inference and achieves a 49.41% enhancement in cost
efficiency on average. Additionally, our work offers valuable insights for
boosting RAG.

</details>


### [534] [Online Detection of LLM-Generated Texts via Sequential Hypothesis Testing by Betting](https://arxiv.org/pdf/2410.22318)
*Can Chen, Jun-Kun Wang*

Main category: cs.LG

TL;DR: The paper proposes an online detection algorithm for differentiating machine-generated texts from human-written ones using sequential hypothesis testing, addressing the limitations of offline methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on offline settings, but real-world scenarios like news websites and social media require real-time detection to prevent misinformation spread by LLMs.

Method: The algorithm uses sequential hypothesis testing with betting techniques, ensuring statistical guarantees like controlled false positives and expected detection time.

Result: Experiments confirm the method's effectiveness in accurately and quickly identifying LLM-generated content.

Conclusion: The proposed online detection algorithm complements offline techniques and provides robust statistical guarantees for real-time applications.

Abstract: Developing algorithms to differentiate between machine-generated texts and
human-written texts has garnered substantial attention in recent years.
Existing methods in this direction typically concern an offline setting where a
dataset containing a mix of real and machine-generated texts is given upfront,
and the task is to determine whether each sample in the dataset is from a large
language model (LLM) or a human. However, in many practical scenarios, sources
such as news websites, social media accounts, and online forums publish content
in a streaming fashion. Therefore, in this online scenario, how to quickly and
accurately determine whether the source is an LLM with strong statistical
guarantees is crucial for these media or platforms to function effectively and
prevent the spread of misinformation and other potential misuse of LLMs. To
tackle the problem of online detection, we develop an algorithm based on the
techniques of sequential hypothesis testing by betting that not only builds
upon and complements existing offline detection techniques but also enjoys
statistical guarantees, which include a controlled false positive rate and the
expected time to correctly identify a source as an LLM. Experiments were
conducted to demonstrate the effectiveness of our method.

</details>


### [535] [Graph Neural Network Generalization with Gaussian Mixture Model Based Augmentation](https://arxiv.org/pdf/2411.08638)
*Yassine Abbahaddou, Fragkiskos D. Malliaros, Johannes F. Lutzeyer, Amine Mohamed Aboussalah, Michalis Vazirgiannis*

Main category: cs.LG

TL;DR: The paper introduces GRATIN, a graph data augmentation algorithm using GMMs to improve GNN generalization, backed by a theoretical framework for regret bounds.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle with generalization, especially on OOD or limited data, prompting the need for better augmentation methods.

Method: Theoretical framework with Rademacher complexity for regret bounds, leading to GRATIN, a GMM-based graph augmentation algorithm.

Result: GRATIN outperforms existing methods in generalization and offers better time complexity.

Conclusion: GRATIN is efficient and effective for real-world GNN applications, addressing generalization challenges.

Abstract: Graph Neural Networks (GNNs) have shown great promise in tasks like node and
graph classification, but they often struggle to generalize, particularly to
unseen or out-of-distribution (OOD) data. These challenges are exacerbated when
training data is limited in size or diversity. To address these issues, we
introduce a theoretical framework using Rademacher complexity to compute a
regret bound on the generalization error and then characterize the effect of
data augmentation. This framework informs the design of GRATIN, an efficient
graph data augmentation algorithm leveraging the capability of Gaussian Mixture
Models (GMMs) to approximate any distribution. Our approach not only
outperforms existing augmentation techniques in terms of generalization but
also offers improved time complexity, making it highly suitable for real-world
applications.

</details>


### [536] [Knowledge Retention for Continual Model-Based Reinforcement Learning](https://arxiv.org/pdf/2503.04256)
*Yixiang Sun, Haotian Fu, Michael Littman, George Konidaris*

Main category: cs.LG

TL;DR: DRAGO is a continual model-based reinforcement learning method using synthetic experience rehearsal and intrinsic rewards to maintain knowledge across tasks.


<details>
  <summary>Details</summary>
Motivation: To improve incremental development of world models across tasks with varying rewards but shared state space and dynamics.

Method: Uses Synthetic Experience Rehearsal (generative models for synthetic experiences) and Regaining Memories Through Exploration (intrinsic rewards for revisiting past states).

Result: Empirical results show DRAGO preserves knowledge and outperforms in continual learning scenarios.

Conclusion: DRAGO effectively maintains and develops world models for diverse environments.

Abstract: We propose DRAGO, a novel approach for continual model-based reinforcement
learning aimed at improving the incremental development of world models across
a sequence of tasks that differ in their reward functions but not the state
space or dynamics. DRAGO comprises two key components: Synthetic Experience
Rehearsal, which leverages generative models to create synthetic experiences
from past tasks, allowing the agent to reinforce previously learned dynamics
without storing data, and Regaining Memories Through Exploration, which
introduces an intrinsic reward mechanism to guide the agent toward revisiting
relevant states from prior tasks. Together, these components enable the agent
to maintain a comprehensive and continually developing world model,
facilitating more effective learning and adaptation across diverse
environments. Empirical evaluations demonstrate that DRAGO is able to preserve
knowledge across tasks, achieving superior performance in various continual
learning scenarios.

</details>


### [537] [A Theoretical Justification for Asymmetric Actor-Critic Algorithms](https://arxiv.org/pdf/2501.19116)
*Gaspard Lambrechts, Damien Ernst, Aditya Mahajan*

Main category: cs.LG

TL;DR: The paper justifies asymmetric actor-critic algorithms in reinforcement learning for partially observable environments, showing they eliminate error terms from state aliasing.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical justification for the benefits of asymmetric learning in reinforcement learning, which lacks precise theoretical backing despite empirical success.

Method: Adapts a finite-time convergence analysis to asymmetric actor-critic algorithms with linear function approximators.

Result: The analysis reveals that the asymmetric critic removes error terms caused by aliasing in the agent state.

Conclusion: The study offers a theoretical foundation for the advantages of asymmetric learning in reinforcement learning.

Abstract: In reinforcement learning for partially observable environments, many
successful algorithms have been developed within the asymmetric learning
paradigm. This paradigm leverages additional state information available at
training time for faster learning. Although the proposed learning objectives
are usually theoretically sound, these methods still lack a precise theoretical
justification for their potential benefits. We propose such a justification for
asymmetric actor-critic algorithms with linear function approximators by
adapting a finite-time convergence analysis to this setting. The resulting
finite-time bound reveals that the asymmetric critic eliminates error terms
arising from aliasing in the agent state.

</details>


### [538] [TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research](https://arxiv.org/pdf/2503.12730)
*Abir Harrasse, Philip Quirke, Clement Neo, Dhruv Nathawani, Luke Marks, Amir Abdullah*

Main category: cs.LG

TL;DR: The paper proposes using text-to-SQL generation as a task to bridge the gap between simple toy tasks and complex large models, introducing TinySQL for testing interpretability methods.


<details>
  <summary>Details</summary>
Motivation: To address the gap in mechanistic interpretability research between simple circuits in toy tasks and features in large models.

Method: Introduces TinySQL, a synthetic dataset, and applies interpretability techniques like Edge Attribution Patching and Sparse Autoencoders to analyze SQL generation circuits.

Result: Identifies minimal circuits for SQL subskills and reveals how models compose queries across layers.

Conclusion: Provides a framework for comparing interpretability methods in structured, complex settings.

Abstract: Mechanistic interpretability research faces a gap between analyzing simple
circuits in toy tasks and discovering features in large models. To bridge this
gap, we propose text-to-SQL generation as an ideal task to study, as it
combines the formal structure of toy tasks with real-world complexity. We
introduce TinySQL, a synthetic dataset, progressing from basic to advanced SQL
operations, and train models ranging from 33M to 1B parameters to establish a
comprehensive testbed for interpretability. We apply multiple complementary
interpretability techniques, including Edge Attribution Patching and Sparse
Autoencoders, to identify minimal circuits and components supporting SQL
generation. We compare circuits for different SQL subskills, evaluating their
minimality, reliability, and identifiability. Finally, we conduct a layerwise
logit lens analysis to reveal how models compose SQL queries across layers:
from intent recognition to schema resolution to structured generation. Our work
provides a robust framework for probing and comparing interpretability methods
in a structured, progressively complex setting.

</details>


### [539] [Clone-Robust Weights in Metric Spaces: Handling Redundancy Bias for Benchmark Aggregation](https://arxiv.org/pdf/2502.03576)
*Damien Berriaud, Roger Wattenhofer*

Main category: cs.LG

TL;DR: The paper introduces a theoretical framework for clone-proof weighting functions in metric spaces to resist adversarial manipulations, with applications in data points, benchmarks, and voting systems.


<details>
  <summary>Details</summary>
Motivation: The problem arises from adversarial manipulations in arbitrary distributions of elements, requiring robust solutions for applications like domain adaptation, benchmarking, and voting advice.

Method: Proposes clone-proof weighting functions that distribute importance to avoid bias from similar elements (clones). Extends the maximum uncertainty principle and includes axioms (symmetry, continuity, clone-proofness) for function construction.

Result: Demonstrates the existence of such weighting functions in Euclidean spaces and provides a general construction method.

Conclusion: The framework offers a robust solution for adversarial settings, with practical implications for diverse applications.

Abstract: We are given a set of elements in a metric space. The distribution of the
elements is arbitrary, possibly adversarial. Can we weigh the elements in a way
that is resistant to such (adversarial) manipulations? This problem arises in
various contexts. For instance, the elements could represent data points,
requiring robust domain adaptation. Alternatively, they might represent tasks
to be aggregated into a benchmark; or questions about personal political
opinions in voting advice applications. This article introduces a theoretical
framework for dealing with such problems. We propose clone-proof weighting
functions as a solution concept. These functions distribute importance across
elements of a set such that similar objects (``clones'') share (some of) their
weights, thus avoiding a potential bias introduced by their multiplicity. Our
framework extends the maximum uncertainty principle to accommodate general
metric spaces and includes a set of axioms -- symmetry, continuity, and
clone-proofness -- that guide the construction of weighting functions. Finally,
we address the existence of weighting functions satisfying our axioms in the
significant case of Euclidean spaces and propose a general method for their
construction.

</details>


### [540] [In-context denoising with one-layer transformers: connections between attention and associative memory retrieval](https://arxiv.org/pdf/2502.05164)
*Matthew Smart, Alberto Bietti, Anirvan M. Sengupta*

Main category: cs.LG

TL;DR: The paper introduces in-context denoising, linking attention-based architectures to dense associative memory (DAM) networks. It shows theoretically and empirically that single-layer transformers can optimally solve certain denoising tasks, with attention layers performing gradient descent on DAM energy landscapes.


<details>
  <summary>Details</summary>
Motivation: To refine the connection between attention mechanisms and DAM networks, demonstrating their relevance in in-context learning and denoising tasks.

Method: Uses a Bayesian framework to analyze denoising problems, showing that a single-layer transformer can solve them optimally. Attention layers perform gradient descent on DAM energy landscapes.

Result: The one-step update by the attention layer outperforms exact retrieval of context tokens or spurious local minima, extending DAM networks beyond standard retrieval.

Conclusion: The work strengthens the link between associative memory and attention mechanisms, highlighting DAM networks' role in in-context learning.

Abstract: We introduce in-context denoising, a task that refines the connection between
attention-based architectures and dense associative memory (DAM) networks, also
known as modern Hopfield networks. Using a Bayesian framework, we show
theoretically and empirically that certain restricted denoising problems can be
solved optimally even by a single-layer transformer. We demonstrate that a
trained attention layer processes each denoising prompt by performing a single
gradient descent update on a context-aware DAM energy landscape, where context
tokens serve as associative memories and the query token acts as an initial
state. This one-step update yields better solutions than exact retrieval of
either a context token or a spurious local minimum, providing a concrete
example of DAM networks extending beyond the standard retrieval paradigm.
Overall, this work solidifies the link between associative memory and attention
mechanisms first identified by Ramsauer et al., and demonstrates the relevance
of associative memory models in the study of in-context learning.

</details>


### [541] [Multivariate Temporal Regression at Scale: A Three-Pillar Framework Combining ML, XAI, and NLP](https://arxiv.org/pdf/2504.02151)
*Jiztom Kavalakkatt Francis, Matthew J Darr*

Main category: cs.LG

TL;DR: A novel framework integrates ML, XAI, and NLP to improve high-dimensional temporal data analysis, reducing insight discovery time by 40-60%.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with complex temporal relationships, leading to noisy or biased datasets.

Method: Combines ML-driven pruning, XAI-based interpretability, and NLP for contextual validation.

Result: Improves performance metrics (MSE, R2, MAE) and computational efficiency, with scalability across platforms.

Conclusion: Provides a pathway for faster data-centric AI in dynamic domains like agriculture and energy, though long-term impacts are pending.

Abstract: This paper introduces a novel framework that accelerates the discovery of
actionable relationships in high-dimensional temporal data by integrating
machine learning (ML), explainable AI (XAI), and natural language processing
(NLP) to enhance data quality and streamline workflows. Traditional methods
often fail to recognize complex temporal relationships, leading to noisy,
redundant, or biased datasets. Our approach combines ML-driven pruning to
identify and mitigate low-quality samples, XAI-based interpretability to
validate critical feature interactions, and NLP for future contextual
validation, reducing the time required to uncover actionable insights by
40-60%. Evaluated on real-world agricultural and synthetic datasets, the
framework significantly improves performance metrics (e.g., MSE, R2, MAE) and
computational efficiency, with hardware-agnostic scalability across diverse
platforms. While long-term real-world impacts (e.g., cost savings,
sustainability gains) are pending, this methodology provides an immediate
pathway to accelerate data-centric AI in dynamic domains like agriculture and
energy, enabling faster iteration cycles for domain experts.

</details>


### [542] [Efficient Diffusion Models: A Survey](https://arxiv.org/pdf/2502.06805)
*Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, Zhongwei Wan, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, Chaofan Tao, Yongfeng Huang, Ye Yuan, Mi Zhang*

Main category: cs.LG

TL;DR: A survey on efficient diffusion models, covering algorithm-level, system-level, and framework perspectives, with a GitHub repository for organized research papers.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are powerful but computationally expensive, necessitating efficient techniques for practical use.

Method: Systematic review and taxonomy of efficient diffusion model research across three categories.

Result: Comprehensive survey and GitHub repository to aid researchers and practitioners.

Conclusion: The survey serves as a resource to inspire further contributions to efficient diffusion model research.

Abstract: Diffusion models have emerged as powerful generative models capable of
producing high-quality contents such as images, videos, and audio,
demonstrating their potential to revolutionize digital content creation.
However, these capabilities come at the cost of their significant computational
resources and lengthy generation time, underscoring the critical need to
develop efficient techniques for practical deployment. In this survey, we
provide a systematic and comprehensive review of research on efficient
diffusion models. We organize the literature in a taxonomy consisting of three
main categories, covering distinct yet interconnected efficient diffusion model
topics from algorithm-level, system-level, and framework perspective,
respectively. We have also created a GitHub repository where we organize the
papers featured in this survey at
https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey. We hope our
survey can serve as a valuable resource to help researchers and practitioners
gain a systematic understanding of efficient diffusion model research and
inspire them to contribute to this important and exciting field.

</details>


### [543] [LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models](https://arxiv.org/pdf/2504.07402)
*Beilong Tang, Bang Zeng, Ming Li*

Main category: cs.LG

TL;DR: LauraTSE is an auto-regressive decoder-only language model for target speaker extraction, leveraging LauraGPT. It combines coarse-grained predictions from a decoder with fine-grained refinements from an encoder-only model, outperforming existing TSE models.


<details>
  <summary>Details</summary>
Motivation: To improve target speaker extraction by combining coarse and fine-grained predictions from auto-regressive and encoder-only language models.

Method: LauraTSE uses a decoder-only model for initial coarse predictions and an encoder-only model for fine-grained refinements.

Result: Achieves superior or comparable performance to existing TSE models.

Conclusion: LauraTSE effectively combines decoder and encoder models for improved target speaker extraction, validated by ablation studies.

Abstract: We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for
Target Speaker Extraction built upon the LauraGPT backbone. LauraTSE employs a
small-scale auto-regressive decoder-only language model that generates the
initial layers of the target speech's discrete codec representations from the
continuous embeddings of both the mixture and reference speech. These outputs
serve as coarse-grained predictions. To refine them, a one-step encoder-only
language model reconstructs the full codec representation by integrating
information from both the mixture and the reference speech, adding fine-grained
details. Our approach achieves superior or comparable performance to existing
TSE models. Additionally, we conduct ablation studies to investigate the data
scalability and the contribution of the encoder-only model.

</details>


### [544] [Training Deep Learning Models with Norm-Constrained LMOs](https://arxiv.org/pdf/2502.07529)
*Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, Volkan Cevher*

Main category: cs.LG

TL;DR: A new stochastic optimization method using LMO for norm-ball problems, applicable to unconstrained problems, unifying existing methods. Demonstrated speedups in nanoGPT training.


<details>
  <summary>Details</summary>
Motivation: To develop efficient optimization methods leveraging LMO for norm-ball problems and extend applicability to unconstrained problems.

Method: Proposes a stochastic algorithm family using LMO, unifying existing methods. Introduces a norm choice for deep architectures enabling hyperparameter transfer.

Result: Significant speedups in nanoGPT training, memory-efficient (half-precision weights/gradients).

Conclusion: The method is effective, unifying, and practical for deep learning, with code available.

Abstract: In this work, we study optimization methods that leverage the linear
minimization oracle (LMO) over a norm-ball. We propose a new stochastic family
of algorithms that uses the LMO to adapt to the geometry of the problem and,
perhaps surprisingly, show that they can be applied to unconstrained problems.
The resulting update rule unifies several existing optimization methods under a
single framework. Furthermore, we propose an explicit choice of norm for deep
architectures, which, as a side benefit, leads to the transferability of
hyperparameters across model sizes. Experimentally, we demonstrate significant
speedups on nanoGPT training using our algorithm, Scion, without any reliance
on Adam. The proposed method is memory-efficient, requiring only one set of
model weights and one set of gradients, which can be stored in half-precision.
The code is available at https://github.com/LIONS-EPFL/scion .

</details>


### [545] [Scalable First-order Method for Certifying Optimal k-Sparse GLMs](https://arxiv.org/pdf/2502.09502)
*Jiachang Liu, Soroosh Shafiee, Andrea Lodi*

Main category: cs.LG

TL;DR: The paper proposes a first-order proximal gradient algorithm to efficiently certify optimality for sparse GLMs with ℓ0 constraints, improving scalability and convergence speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for certifying optimality in sparse GLMs are computationally intensive or slow, limiting scalability.

Method: A first-order proximal gradient algorithm solves the perspective relaxation within a BnB framework, with exact proximal operator computation and a restart strategy.

Result: The method accelerates dual bound computations and effectively provides optimality certificates for large-scale problems.

Conclusion: The proposed approach is scalable and efficient for certifying optimality in sparse GLMs.

Abstract: This paper investigates the problem of certifying optimality for sparse
generalized linear models (GLMs), where sparsity is enforced through an
$\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can
certify optimality by pruning nodes using dual bounds, existing methods for
computing these bounds are either computationally intensive or exhibit slow
convergence, limiting their scalability to large-scale problems. To address
this challenge, we propose a first-order proximal gradient algorithm designed
to solve the perspective relaxation of the problem within a BnB framework.
Specifically, we formulate the relaxed problem as a composite optimization
problem and demonstrate that the proximal operator of the non-smooth component
can be computed exactly in log-linear time complexity, eliminating the need to
solve a computationally expensive second-order cone program. Furthermore, we
introduce a simple restart strategy that enhances convergence speed while
maintaining low per-iteration complexity. Extensive experiments on synthetic
and real-world datasets show that our approach significantly accelerates dual
bound computations and is highly effective in providing optimality certificates
for large-scale problems.

</details>


### [546] [How can Diffusion Models Evolve into Continual Generators?](https://arxiv.org/pdf/2505.11936)
*Jingren Liu, Zhong Ji, Xiangyu Chen*

Main category: cs.LG

TL;DR: The paper introduces Continual Diffusion Generation (CDG) to address catastrophic forgetting in diffusion models for continual learning, proposing a theoretical framework and the CCD method with consistency criteria for improved performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with catastrophic forgetting in continual learning scenarios, and existing heuristic strategies lack alignment with the diffusion process.

Method: The authors develop a theoretical framework for CDG, identifying three consistency criteria (IKC, UKC, LKC), and propose the CCD framework with hierarchical loss terms to integrate these criteria.

Result: CCD achieves state-of-the-art performance in continual settings, with significant improvements in Mean Fidelity and Incremental Mean Fidelity, especially in tasks with cross-task knowledge overlap.

Conclusion: The CCD framework effectively addresses catastrophic forgetting in diffusion models for continual learning, demonstrating superior performance through principled consistency objectives.

Abstract: While diffusion models have achieved remarkable success in static data
generation, their deployment in streaming or continual learning (CL) scenarios
faces a major challenge: catastrophic forgetting (CF), where newly acquired
generative capabilities overwrite previously learned ones. To systematically
address this, we introduce a formal Continual Diffusion Generation (CDG)
paradigm that characterizes and redefines CL in the context of generative
diffusion models. Prior efforts often adapt heuristic strategies from continual
classification tasks but lack alignment with the underlying diffusion process.
In this work, we develop the first theoretical framework for CDG by analyzing
cross-task dynamics in diffusion-based generative modeling. Our analysis
reveals that the retention and stability of generative knowledge across tasks
are governed by three key consistency criteria: inter-task knowledge
consistency (IKC), unconditional knowledge consistency (UKC), and label
knowledge consistency (LKC). Building on these insights, we propose Continual
Consistency Diffusion (CCD), a principled framework that integrates these
consistency objectives into training via hierarchical loss terms
$\mathcal{L}_{IKC}$, $\mathcal{L}_{UKC}$, and $\mathcal{L}_{LKC}$. This
promotes effective knowledge retention while enabling the assimilation of new
generative capabilities. Extensive experiments on four benchmark datasets
demonstrate that CCD achieves state-of-the-art performance under continual
settings, with substantial gains in Mean Fidelity (MF) and Incremental Mean
Fidelity (IMF), particularly in tasks with rich cross-task knowledge overlap.

</details>


### [547] [A new pathway to generative artificial intelligence by minimizing the maximum entropy](https://arxiv.org/pdf/2502.13287)
*Mattia Miotto, Lorenzo Monacelli*

Main category: cs.LG

TL;DR: A new physics-driven AI framework improves generative models by optimizing data representation, reducing noise, and enabling control, outperforming variational autoencoders.


<details>
  <summary>Details</summary>
Motivation: Current generative AI models are data-hungry and hard to direct, limiting their development.

Method: The framework minimizes entropy to reduce noise and maximizes it to avoid bias, using adversary training for optimal data representation.

Result: The model is data-efficient, flexible, and outperforms variational autoencoders, even with limited data.

Conclusion: This approach enables customizable generation without retraining, marking a significant advancement in generative AI.

Abstract: Generative artificial intelligence revolutionized society. Current models are
trained by minimizing the distance between the produced data and the training
set. Consequently, development is plateauing as they are intrinsically
data-hungry and challenging to direct during the generative process. To
overcome these limitations, we introduce a paradigm shift through a framework
where we do not fit the training set but find the most informative yet least
noisy representation of the data simultaneously minimizing the entropy to
reduce noise and maximizing it to remain unbiased via adversary training. The
result is a general physics-driven model, which is data-efficient and flexible,
permitting to control and influence the generative process. Benchmarking shows
that our approach outperforms variational autoencoders. We demonstrate the
methods effectiveness in generating images, even with limited training data,
and its unprecedented capability to customize the generation process a
posteriori without any fine-tuning or retraining

</details>


### [548] [Approximating Latent Manifolds in Neural Networks via Vanishing Ideals](https://arxiv.org/pdf/2502.15051)
*Nico Pelleriti, Max Zimmer, Elias Wirth, Sebastian Pokutta*

Main category: cs.LG

TL;DR: The paper connects manifold learning and computational algebra, proposing a neural architecture that uses vanishing ideals to characterize latent manifolds, reducing layers while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To bridge manifold learning and computational algebra by leveraging vanishing ideals for efficient deep network design.

Method: Truncates pretrained networks, approximates class manifolds via polynomial generators, and transforms latent space into linearly separable features with a polynomial layer.

Result: Fewer layers, comparable accuracy, higher throughput, and fewer parameters than baselines, with tighter generalization bounds.

Conclusion: The approach is effective and efficient, validated by numerical experiments.

Abstract: Deep neural networks have reshaped modern machine learning by learning
powerful latent representations that often align with the manifold hypothesis:
high-dimensional data lie on lower-dimensional manifolds. In this paper, we
establish a connection between manifold learning and computational algebra by
demonstrating how vanishing ideals can characterize the latent manifolds of
deep networks. To that end, we propose a new neural architecture that (i)
truncates a pretrained network at an intermediate layer, (ii) approximates each
class manifold via polynomial generators of the vanishing ideal, and (iii)
transforms the resulting latent space into linearly separable features through
a single polynomial layer. The resulting models have significantly fewer layers
than their pretrained baselines, while maintaining comparable accuracy,
achieving higher throughput, and utilizing fewer parameters. Furthermore,
drawing on spectral complexity analysis, we derive sharper theoretical
guarantees for generalization, showing that our approach can in principle offer
tighter bounds than standard deep networks. Numerical experiments confirm the
effectiveness and efficiency of the proposed approach.

</details>


### [549] [Model-Based Exploration in Monitored Markov Decision Processes](https://arxiv.org/pdf/2502.16772)
*Alireza Kazemipour, Simone Parisi, Matthew E. Taylor, Michael Bowling*

Main category: cs.LG

TL;DR: The paper introduces a model-based algorithm for Monitored Markov Decision Processes (Mon-MDPs) to address limitations of prior methods, showing faster convergence and providing finite-sample guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing Mon-MDP algorithms fail to fully exploit problem structure, lack worst-case guarantees, and rely on asymptotic proofs, motivating a more robust solution.

Method: A model-based algorithm using two instances of model-based interval estimation: one for capturing observable rewards and another for learning the minimax-optimal policy.

Result: Empirical results show faster convergence in benchmarks and significant improvement with known monitoring processes, along with finite-sample performance bounds.

Conclusion: The proposed algorithm overcomes prior limitations, offering practical and theoretical advantages for Mon-MDPs.

Abstract: A tenet of reinforcement learning is that the agent always observes rewards.
However, this is not true in many realistic settings, e.g., a human observer
may not always be available to provide rewards, sensors may be limited or
malfunctioning, or rewards may be inaccessible during deployment. Monitored
Markov decision processes (Mon-MDPs) have recently been proposed to model such
settings. However, existing Mon-MDP algorithms have several limitations: they
do not fully exploit the problem structure, cannot leverage a known monitor,
lack worst-case guarantees for 'unsolvable' Mon-MDPs without specific
initialization, and offer only asymptotic convergence proofs. This paper makes
three contributions. First, we introduce a model-based algorithm for Mon-MDPs
that addresses these shortcomings. The algorithm employs two instances of
model-based interval estimation: one to ensure that observable rewards are
reliably captured, and another to learn the minimax-optimal policy. Second, we
empirically demonstrate the advantages. We show faster convergence than prior
algorithms in over four dozen benchmarks, and even more dramatic improvement
when the monitoring process is known. Third, we present the first finite-sample
bound on performance. We show convergence to a minimax-optimal policy even when
some rewards are never observable.

</details>


### [550] [One Stone, Two Birds: Enhancing Adversarial Defense Through the Lens of Distributional Discrepancy](https://arxiv.org/pdf/2503.02169)
*Jiacheng Zhang, Benjamin I. P. Rubinstein, Jingfeng Zhang, Feng Liu*

Main category: cs.LG

TL;DR: SADD detects adversarial examples (AEs) by measuring distributional discrepancies. The paper proposes DAD, a two-pronged defense method that optimizes MMD to train a denoiser and discriminate AEs, improving clean and robust accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of SADD-based methods, which discard detected AEs and lose useful information, by proposing a more effective defense method.

Method: DAD optimizes MMD (MMD-OPT) to minimize distributional discrepancy (training a denoiser) and discriminate AEs. It processes CEs directly and denoises AEs during inference.

Result: DAD outperforms SOTA defense methods, improving both clean and robust accuracy on CIFAR-10 and ImageNet-1K against adaptive white-box attacks.

Conclusion: DAD effectively addresses SADD's limitation by retaining useful information in AEs and achieves superior performance in adversarial defense.

Abstract: Statistical adversarial data detection (SADD) detects whether an upcoming
batch contains adversarial examples (AEs) by measuring the distributional
discrepancies between clean examples (CEs) and AEs. In this paper, we explore
the strength of SADD-based methods by theoretically showing that minimizing
distributional discrepancy can help reduce the expected loss on AEs. Despite
these advantages, SADD-based methods have a potential limitation: they discard
inputs that are detected as AEs, leading to the loss of useful information
within those inputs. To address this limitation, we propose a two-pronged
adversarial defense method, named Distributional-discrepancy-based Adversarial
Defense (DAD). In the training phase, DAD first optimizes the test power of the
maximum mean discrepancy (MMD) to derive MMD-OPT, which is a stone that kills
two birds. MMD-OPT first serves as a guiding signal to minimize the
distributional discrepancy between CEs and AEs to train a denoiser. Then, it
serves as a discriminator to differentiate CEs and AEs during inference.
Overall, in the inference stage, DAD consists of a two-pronged process: (1)
directly feeding the detected CEs into the classifier, and (2) removing noise
from the detected AEs by the distributional-discrepancy-based denoiser.
Extensive experiments show that DAD outperforms current state-of-the-art (SOTA)
defense methods by simultaneously improving clean and robust accuracy on
CIFAR-10 and ImageNet-1K against adaptive white-box attacks. Codes are publicly
available at: https://github.com/tmlr-group/DAD.

</details>


### [551] [Hierarchical Refinement: Optimal Transport to Infinity and Beyond](https://arxiv.org/pdf/2503.03025)
*Peter Halmos, Julian Gold, Xinhao Liu, Benjamin J. Raphael*

Main category: cs.LG

TL;DR: HiRef algorithm improves scalability of optimal transport by using low-rank OT and hierarchical refinement to achieve bijective couplings with linear space and log-linear runtime.


<details>
  <summary>Details</summary>
Motivation: Sinkhorn's quadratic space complexity limits scalability for large datasets, and low-rank OT lacks one-to-one correspondence.

Method: HiRef dynamically constructs multiscale partitions using low-rank OT subproblems to achieve bijective couplings.

Result: HiRef scales to over a million points, outperforming Sinkhorn in large datasets.

Conclusion: HiRef combines the space efficiency of low-rank OT with the resolution of full-rank OT, enabling scalable bijective transport.

Abstract: Optimal transport (OT) has enjoyed great success in machine learning as a
principled way to align datasets via a least-cost correspondence, driven in
large part by the runtime efficiency of the Sinkhorn algorithm (Cuturi, 2013).
However, Sinkhorn has quadratic space complexity in the number of points,
limiting scalability to larger datasets. Low-rank OT achieves linear-space
complexity, but by definition, cannot compute a one-to-one correspondence
between points. When the optimal transport problem is an assignment problem
between datasets then an optimal mapping, known as the Monge map, is guaranteed
to be a bijection. In this setting, we show that the factors of an optimal
low-rank coupling co-cluster each point with its image under the Monge map. We
leverage this invariant to derive an algorithm, Hierarchical Refinement
(HiRef), that dynamically constructs a multiscale partition of each dataset
using low-rank OT subproblems, culminating in a bijective coupling.
Hierarchical Refinement uses linear space and has log-linear runtime, retaining
the space advantage of low-rank OT while overcoming its limited resolution. We
demonstrate the advantages of Hierarchical Refinement on several datasets,
including ones containing over a million points, scaling full-rank OT to
problems previously beyond Sinkhorn's reach.

</details>


### [552] [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/pdf/2505.21136)
*Jintao Zhang, Xiaoming Xu, Jia Wei, Haofeng Huang, Pengle Zhang, Chendong Xiang, Jun Zhu, Jianfei Chen*

Main category: cs.LG

TL;DR: SageAttention2++ improves attention efficiency using FP8 Matmul instructions, achieving 3.9x speedup over FlashAttention with no accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic time complexity of attention by optimizing matrix multiplications.

Method: Utilizes FP8 Matmul instructions accumulated in FP16 for faster computation.

Result: 3.9x speedup over FlashAttention while maintaining accuracy.

Conclusion: SageAttention2++ is effective for accelerating models in language, image, and video generation with minimal impact on performance.

Abstract: The efficiency of attention is critical because its time complexity grows
quadratically with sequence length. SageAttention2 addresses this by utilizing
quantization to accelerate matrix multiplications (Matmul) in attention. To
further accelerate SageAttention2, we propose to utilize the faster instruction
of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8
Matmul used in SageAttention2. Our experiments show that SageAttention2++
achieves a 3.9x speedup over FlashAttention while maintaining the same
attention accuracy as SageAttention2. This means SageAttention2++ effectively
accelerates various models, including those for language, image, and video
generation, with negligible end-to-end metrics loss. The code will be available
at https://github.com/thu-ml/SageAttention.

</details>


### [553] [BARK: A Fully Bayesian Tree Kernel for Black-box Optimization](https://arxiv.org/pdf/2503.05574)
*Toby Boyne, Jose Pablo Folch, Robert M Lee, Behrang Shafei, Ruth Misener*

Main category: cs.LG

TL;DR: BARK combines BART with Gaussian processes for Bayesian optimization, capturing uncertainty in tree structures and enabling global optimization in mixed-feature spaces.


<details>
  <summary>Details</summary>
Motivation: To enhance Bayesian optimization by integrating BART with Gaussian processes, addressing limitations in uncertainty quantification and optimization over mixed-feature spaces.

Method: Uses tree agreement to define a posterior over piecewise-constant functions, explores tree kernels via MCMC, and builds acquisition functions for optimization.

Result: BARK outperforms benchmarks due to its Bayesian surrogate and optimization procedure, handling mixed-feature spaces effectively.

Conclusion: BARK provides a robust framework for Bayesian optimization, leveraging tree-based uncertainty and Gaussian processes for improved performance.

Abstract: We perform Bayesian optimization using a Gaussian process perspective on
Bayesian Additive Regression Trees (BART). Our BART Kernel (BARK) uses tree
agreement to define a posterior over piecewise-constant functions, and we
explore the space of tree kernels using a Markov chain Monte Carlo approach.
Where BART only samples functions, the resulting BARK model obtains samples of
Gaussian processes defining distributions over functions, which allow us to
build acquisition functions for Bayesian optimization. Our tree-based approach
enables global optimization over the surrogate, even for mixed-feature spaces.
Moreover, where many previous tree-based kernels provide uncertainty
quantification over function values, our sampling scheme captures uncertainty
over the tree structure itself. Our experiments show the strong performance of
BARK on both synthetic and applied benchmarks, due to the combination of our
fully Bayesian surrogate and the optimization procedure.

</details>


### [554] [An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction](https://arxiv.org/pdf/2505.21339)
*Henryk Mustroph, Michel Kunkler, Stefanie Rinderle-Ma*

Main category: cs.LG

TL;DR: The paper introduces probabilistic suffix prediction for business processes, addressing uncertainty by predicting a distribution of possible suffixes, outperforming single-scenario methods.


<details>
  <summary>Details</summary>
Motivation: Current suffix prediction methods focus on single likely scenarios, overlooking other probable outcomes, especially in uncertain and variable processes.

Method: The approach uses an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and Monte Carlo suffix sampling, capturing epistemic and aleatoric uncertainties.

Result: Probabilistic suffix prediction outperforms single-scenario methods, with the U-ED-LSTM showing good predictive performance and calibration.

Conclusion: The proposed method effectively handles uncertainty in business process suffix prediction, offering better performance and reliability.

Abstract: Suffix prediction of business processes forecasts the remaining sequence of
events until process completion. Current approaches focus on predicting the
most likely suffix, representing a single scenario. However, when the future
course of a process is subject to uncertainty and high variability, the
expressiveness of such a single scenario can be limited, since other possible
scenarios, which together may have a higher overall probability, are
overlooked. To address this limitation, we propose probabilistic suffix
prediction, a novel approach that approximates a probability distribution of
suffixes. The proposed approach is based on an Uncertainty-Aware
Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC) suffix sampling
algorithm. We capture epistemic uncertainties via MC dropout and aleatoric
uncertainties as learned loss attenuation. This technical report presents a
comprehensive evaluation of the probabilistic suffix prediction approach's
predictive performance and calibration under three different hyperparameter
settings, using four real-life and one artificial event log. The results show
that: i) probabilistic suffix prediction can outperform most likely suffix
prediction, the U-ED-LSTM has reasonable predictive performance, and ii) the
model's predictions are well calibrated.

</details>


### [555] [Extracting Interpretable Logic Rules from Graph Neural Networks](https://arxiv.org/pdf/2503.19476)
*Chuqin Geng, Ziyu Zhao, Zhaoyue Wang, Haolin Ye, Xujie Si*

Main category: cs.LG

TL;DR: LOGICXGNN is a novel framework for extracting interpretable logic rules from GNNs, offering model-agnostic, efficient, and data-driven explainability without predefined concepts. It outperforms original models and aids knowledge discovery.


<details>
  <summary>Details</summary>
Motivation: Understanding GNN predictions is challenging due to their dual operation on feature spaces and graph structures. Existing explainability methods rely on predefined concepts and explain limited patterns.

Method: LOGICXGNN extracts logic rules from GNNs, is model-agnostic, and data-driven. It also generates new graph instances transparently.

Result: LOGICXGNN outperforms original neural models, extracts detailed chemistry knowledge, and demonstrates potential for drug design.

Conclusion: LOGICXGNN provides interpretable, high-performing, and transparent explanations for GNNs, with applications in knowledge discovery and drug design.

Abstract: Graph neural networks (GNNs) operate over both input feature spaces and
combinatorial graph structures, making it challenging to understand the
rationale behind their predictions. As GNNs gain widespread popularity and
demonstrate success across various domains, such as drug discovery, studying
their interpretability has become a critical task. To address this, many
explainability methods have been proposed, with recent efforts shifting from
instance-specific explanations to global concept-based explainability. However,
these approaches face several limitations, such as relying on predefined
concepts and explaining only a limited set of patterns. To address this, we
propose a novel framework, LOGICXGNN, for extracting interpretable logic rules
from GNNs. LOGICXGNN is model-agnostic, efficient, and data-driven, eliminating
the need for predefined concepts. More importantly, it can serve as a
rule-based classifier and even outperform the original neural models. Its
interpretability facilitates knowledge discovery, as demonstrated by its
ability to extract detailed and accurate chemistry knowledge that is often
overlooked by existing methods. Another key advantage of LOGICXGNN is its
ability to generate new graph instances in a controlled and transparent manner,
offering significant potential for applications such as drug design. We
empirically demonstrate these merits through experiments on real-world datasets
such as MUTAG and BBBP.

</details>


### [556] [Uncertainty Propagation in the Fast Fourier Transform](https://arxiv.org/pdf/2504.10136)
*Luca Schmid, Charlotte Muth, Laurent Schmalen*

Main category: cs.LG

TL;DR: The paper proposes a framework for uncertainty propagation in the discrete Fourier transform using belief propagation and expectation propagation, achieving stable convergence and accurate estimates.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty propagation in the discrete Fourier transform and extend applicability beyond Gaussian assumptions.

Method: Model the fast Fourier transform as a factor graph and use belief propagation (BP) and expectation propagation for approximate Bayesian inference.

Result: Stable convergence with accurate mean and variance estimates, demonstrated in communication scenarios.

Conclusion: The framework shows practical potential for uncertainty-aware inference in probabilistic systems across time and frequency domains.

Abstract: We address the problem of uncertainty propagation in the discrete Fourier
transform by modeling the fast Fourier transform as a factor graph. Building on
this representation, we propose an efficient framework for approximate Bayesian
inference using belief propagation (BP) and expectation propagation, extending
its applicability beyond Gaussian assumptions. By leveraging an appropriate BP
message representation and a suitable schedule, our method achieves stable
convergence with accurate mean and variance estimates. Numerical experiments in
representative scenarios from communications demonstrate the practical
potential of the proposed framework for uncertainty-aware inference in
probabilistic systems operating across both time and frequency domain.

</details>


### [557] [Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning](https://arxiv.org/pdf/2504.14268)
*Xinye Chen*

Main category: cs.LG

TL;DR: A reinforcement learning framework optimizes numerical precision in the CG method using Q-learning, balancing efficiency and accuracy without retraining for new datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of iterative solvers by dynamically optimizing numerical precision, ensuring computational efficiency and accuracy.

Method: Model precision selection as an MDP, apply Q-learning for adaptive precision assignment, and use double-precision for stability.

Result: Effective performance enhancement, robustness, and scalability, marking the first RL application in mixed-precision numerical methods.

Conclusion: The approach offers practical advantages and insights for AI-driven scientific computing, paving the way for further advancements.

Abstract: This paper presents a novel reinforcement learning (RL) framework for
dynamically optimizing numerical precision in the preconditioned conjugate
gradient (CG) method. By modeling precision selection as a Markov Decision
Process (MDP), we employ Q-learning to adaptively assign precision levels to
key operations, striking an optimal balance between computational efficiency
and numerical accuracy, while ensuring stability through double-precision
scalar computations and residual computing. In practice, the algorithm is
trained on a set of data and subsequently performs inference for precision
selection on out-of-sample data, without requiring re-analysis or retraining
for new datasets. This enables the method to adapt seamlessly to new problem
instances without the computational overhead of recalibration. Our results
demonstrate the effectiveness of RL in enhancing solver's performance, marking
the first application of RL to mixed-precision numerical methods. The findings
highlight the approach's practical advantages, robustness, and scalability,
providing valuable insights into its integration with iterative solvers and
paving the way for AI-driven advancements in scientific computing.

</details>


### [558] [On the Importance of Gaussianizing Representations](https://arxiv.org/pdf/2505.00685)
*Daniel Eftekhari, Vardan Papyan*

Main category: cs.LG

TL;DR: The paper introduces normality normalization, a novel layer for neural networks that encourages normal distribution of activations using power transforms and Gaussian noise, improving generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: The normal distribution's central role in information theory and its benefits for neural network activations inspired the development of a method to enforce normality in feature representations.

Method: The proposed normality normalization layer uses power transforms and additive Gaussian noise during training to encourage normally distributed activations.

Result: Experiments show improved generalization, robustness, and performance across various model architectures, dataset combinations, and training conditions.

Conclusion: Normality normalization is effective and versatile, suitable for replacing conventional normalization layers and enhancing model robustness.

Abstract: The normal distribution plays a central role in information theory - it is at
the same time the best-case signal and worst-case noise distribution, has the
greatest representational capacity of any distribution, and offers an
equivalence between uncorrelatedness and independence for joint distributions.
Accounting for the mean and variance of activations throughout the layers of
deep neural networks has had a significant effect on facilitating their
effective training, but seldom has a prescription for precisely what
distribution these activations should take, and how this might be achieved,
been offered. Motivated by the information-theoretic properties of the normal
distribution, we address this question and concurrently present normality
normalization: a novel normalization layer which encourages normality in the
feature representations of neural networks using the power transform and
employs additive Gaussian noise during training. Our experiments
comprehensively demonstrate the effectiveness of normality normalization, in
regards to its generalization performance on an array of widely used model and
dataset combinations, its strong performance across various common factors of
variation such as model width, depth, and training minibatch size, its
suitability for usage wherever existing normalization layers are conventionally
used, and as a means to improving model robustness to random perturbations.

</details>


### [559] [State-Covering Trajectory Stitching for Diffusion Planners](https://arxiv.org/pdf/2506.00895)
*Kyowoon Lee, Jaesik Choi*

Main category: cs.LG

TL;DR: SCoTS is a reward-free trajectory augmentation method that improves diffusion-based generative models for RL by stitching short trajectory segments to enhance diversity and generalization.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based models for RL planning are limited by training data quality and diversity, restricting generalization to new tasks or longer horizons.

Method: SCoTS learns a temporal distance-preserving latent representation, then stitches trajectory segments using directional exploration and novelty to expand the latent space.

Result: SCoTS enhances diffusion planners' performance and generalization on offline goal-conditioned benchmarks and improves offline RL algorithms.

Conclusion: SCoTS effectively addresses data limitations in diffusion-based RL planning, enabling better generalization and performance.

Abstract: Diffusion-based generative models are emerging as powerful tools for
long-horizon planning in reinforcement learning (RL), particularly with offline
datasets. However, their performance is fundamentally limited by the quality
and diversity of training data. This often restricts their generalization to
tasks outside their training distribution or longer planning horizons. To
overcome this challenge, we propose State-Covering Trajectory Stitching
(SCoTS), a novel reward-free trajectory augmentation method that incrementally
stitches together short trajectory segments, systematically generating diverse
and extended trajectories. SCoTS first learns a temporal distance-preserving
latent representation that captures the underlying temporal structure of the
environment, then iteratively stitches trajectory segments guided by
directional exploration and novelty to effectively cover and expand this latent
space. We demonstrate that SCoTS significantly improves the performance and
generalization capabilities of diffusion planners on offline goal-conditioned
benchmarks requiring stitching and long-horizon reasoning. Furthermore,
augmented trajectories generated by SCoTS significantly improve the performance
of widely used offline goal-conditioned RL algorithms across diverse
environments.

</details>


### [560] [A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction](https://arxiv.org/pdf/2505.14211)
*Qu Wang, Yan Xia*

Main category: cs.LG

TL;DR: The paper introduces PTWD, a PID-controlled tensor wheel decomposition model, for accurate link prediction in dynamic networks by combining TWD's topological features with PID control for stable optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional static network methods fail to capture temporal and weight dynamics in dynamic networks, necessitating advanced tensor-based approaches like TWD.

Method: PTWD integrates TWD's cyclic factor decomposition with PID control principles to optimize model parameters for stable learning.

Result: PTWD outperforms other models in link prediction accuracy on four real datasets.

Conclusion: PTWD effectively addresses dynamic network challenges by leveraging TWD and PID control, achieving superior prediction performance.

Abstract: Link prediction in dynamic networks remains a fundamental challenge in
network science, requiring the inference of potential interactions and their
evolving strengths through spatiotemporal pattern analysis. Traditional static
network methods have inherent limitations in capturing temporal dependencies
and weight dynamics, while tensor-based methods offer a promising paradigm by
encoding dynamic networks into high-order tensors to explicitly model
multidimensional interactions across nodes and time. Among them, tensor wheel
decomposition (TWD) stands out for its innovative topological structure, which
decomposes high-order tensors into cyclic factors and core tensors to maintain
structural integrity. To improve the prediction accuracy, this study introduces
a PID-controlled tensor wheel decomposition (PTWD) model, which mainly adopts
the following two ideas: 1) exploiting the representation power of TWD to
capture the latent features of dynamic network topology and weight evolution,
and 2) integrating the proportional-integral-derivative (PID) control principle
into the optimization process to obtain a stable model parameter learning
scheme. The performance on four real datasets verifies that the proposed PTWD
model has more accurate link prediction capabilities compared to other models.

</details>


### [561] [Training on Plausible Counterfactuals Removes Spurious Correlations](https://arxiv.org/pdf/2505.16583)
*Shpresim Sadiku, Kartikeya Chitranshi, Hiroshi Kera, Sebastian Pokutta*

Main category: cs.LG

TL;DR: Training classifiers on plausible counterfactual explanations (p-CFEs) labeled with incorrect targets improves accuracy and reduces bias compared to adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: To explore whether classifiers can learn effectively from p-CFEs, which are minimally modified inputs that change decisions while remaining plausible, and to compare this with adversarial perturbations.

Method: Train classifiers on p-CFEs labeled with incorrect target classes and evaluate their performance on unperturbed inputs.

Result: Classifiers trained on p-CFEs achieve high in-distribution accuracy and significantly reduced bias regarding spurious correlations.

Conclusion: Learning from p-CFEs is more effective than adversarial perturbations, enhancing classifier performance and fairness.

Abstract: Plausible counterfactual explanations (p-CFEs) are perturbations that
minimally modify inputs to change classifier decisions while remaining
plausible under the data distribution. In this study, we demonstrate that
classifiers can be trained on p-CFEs labeled with induced \emph{incorrect}
target classes to classify unperturbed inputs with the original labels. While
previous studies have shown that such learning is possible with adversarial
perturbations, we extend this paradigm to p-CFEs. Interestingly, our
experiments reveal that learning from p-CFEs is even more effective: the
resulting classifiers achieve not only high in-distribution accuracy but also
exhibit significantly reduced bias with respect to spurious correlations.

</details>


### [562] [CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models](https://arxiv.org/pdf/2505.21360)
*Dhanesh Ramachandram*

Main category: cs.LG

TL;DR: CRISP-NAM is an interpretable neural additive model for competing risks survival analysis, preserving feature-level interpretability while modeling cause-specific hazards.


<details>
  <summary>Details</summary>
Motivation: Competing risks are critical in survival modeling, especially in healthcare, where patients face multiple distinct event types. Existing methods lack interpretability.

Method: Extends neural additive models to handle competing risks, with each feature contributing independently via dedicated neural networks, enabling visualization of non-linear relationships.

Result: Demonstrates competitive performance on multiple datasets compared to existing approaches.

Conclusion: CRISP-NAM offers a balance of interpretability and performance for competing risks survival analysis.

Abstract: Competing risks are crucial considerations in survival modelling,
particularly in healthcare domains where patients may experience multiple
distinct event types. We propose CRISP-NAM (Competing Risks Interpretable
Survival Prediction with Neural Additive Models), an interpretable neural
additive model for competing risks survival analysis which extends the neural
additive architecture to model cause-specific hazards while preserving
feature-level interpretability. Each feature contributes independently to risk
estimation through dedicated neural networks, allowing for visualization of
complex non-linear relationships between covariates and each competing risk. We
demonstrate competitive performance on multiple datasets compared to existing
approaches.

</details>


### [563] [Sparse Autoencoders, Again?](https://arxiv.org/pdf/2506.04859)
*Yin Lu, Xuening Zhu, Tong He, David Wipf*

Main category: cs.LG

TL;DR: The paper critiques traditional sparse autoencoders (SAEs) and variational autoencoders (VAEs), proposing a hybrid model that outperforms both in sparsity and accuracy.


<details>
  <summary>Details</summary>
Motivation: Despite their wide applicability, SAEs and VAEs have seen little innovation. The paper identifies their weaknesses and aims to improve sparse representation learning.

Method: A hybrid model combining strengths of SAEs and VAEs is proposed, with theoretical guarantees for structured data recovery.

Result: The model achieves sparser latent representations and better performance than SAEs, VAEs, and diffusion models in tasks like image and language model activations.

Conclusion: The hybrid model addresses limitations of traditional methods, offering improved performance and theoretical support for structured data.

Abstract: Is there really much more to say about sparse autoencoders (SAEs)?
Autoencoders in general, and SAEs in particular, represent deep architectures
that are capable of modeling low-dimensional latent structure in data. Such
structure could reflect, among other things, correlation patterns in large
language model activations, or complex natural image manifolds. And yet despite
the wide-ranging applicability, there have been relatively few changes to SAEs
beyond the original recipe from decades ago, namely, standard deep
encoder/decoder layers trained with a classical/deterministic sparse
regularizer applied within the latent space. One possible exception is the
variational autoencoder (VAE), which adopts a stochastic encoder module capable
of producing sparse representations when applied to manifold data. In this work
we formalize underappreciated weaknesses with both canonical SAEs, as well as
analogous VAEs applied to similar tasks, and propose a hybrid alternative model
that circumvents these prior limitations. In terms of theoretical support, we
prove that global minima of our proposed model recover certain forms of
structured data spread across a union of manifolds. Meanwhile, empirical
evaluations on synthetic and real-world datasets substantiate the efficacy of
our approach in accurately estimating underlying manifold dimensions and
producing sparser latent representations without compromising reconstruction
error. In general, we are able to exceed the performance of equivalent-capacity
SAEs and VAEs, as well as recent diffusion models where applicable, within
domains such as images and language model activation patterns.

</details>


### [564] [Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations](https://arxiv.org/pdf/2505.21824)
*Praveen Kumar, Vincent T. Metzger, Scott A. Malec*

Main category: cs.LG

TL;DR: A novel unsupervised framework using Non-negative Matrix Factorization (NMF) and statistical techniques to predict T2DM risk by analyzing multimorbidity and polypharmacy patterns.


<details>
  <summary>Details</summary>
Motivation: The rising global prevalence of T2DM and its severe health and economic impacts necessitate early detection methods. Supervised learning approaches are limited by the lack of confirmed negative cases.

Method: Proposes an unsupervised framework integrating NMF with statistical techniques to identify latent patterns of multimorbidity and polypharmacy in diagnosed T2DM patients, then applies these patterns to undiagnosed individuals.

Result: The method provides an interpretable and scalable solution for estimating T2DM risk, aiding healthcare providers in timely interventions.

Conclusion: This approach can improve patient outcomes and reduce the future health and economic burden of T2DM.

Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus
(T2DM), is rapidly increasing, posing significant health and economic
challenges. T2DM not only disrupts blood glucose regulation but also damages
vital organs such as the heart, kidneys, eyes, nerves, and blood vessels,
leading to substantial morbidity and mortality. In the US alone, the economic
burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of
individuals at risk is critical to mitigating these impacts. While machine
learning approaches for T2DM prediction are increasingly adopted, many rely on
supervised learning, which is often limited by the lack of confirmed negative
cases. To address this limitation, we propose a novel unsupervised framework
that integrates Non-negative Matrix Factorization (NMF) with statistical
techniques to identify individuals at risk of developing T2DM. Our method
identifies latent patterns of multimorbidity and polypharmacy among diagnosed
T2DM patients and applies these patterns to estimate the T2DM risk in
undiagnosed individuals. By leveraging data-driven insights from comorbidity
and medication usage, our approach provides an interpretable and scalable
solution that can assist healthcare providers in implementing timely
interventions, ultimately improving patient outcomes and potentially reducing
the future health and economic burden of T2DM.

</details>


### [565] [Exploring Diffusion Transformer Designs via Grafting](https://arxiv.org/pdf/2506.05340)
*Keshigeyan Chandrasegaran, Michael Poli, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, Li Fei-Fei*

Main category: cs.LG

TL;DR: The paper introduces grafting, a method to edit pretrained diffusion transformers (DiTs) for exploring new architectures efficiently, reducing compute costs.


<details>
  <summary>Details</summary>
Motivation: Evaluating architectural decisions in models requires costly pretraining, limiting exploration. Grafting leverages pretrained models to study new designs.

Method: Grafting edits pretrained DiTs to test hybrid designs (e.g., replacing attention with gated convolution) with minimal compute.

Result: Hybrid designs achieve comparable quality (FID: 2.38-2.64) using <2% pretraining compute. Grafting also speeds up a text-to-image model by 1.43x.

Conclusion: Grafting enables efficient exploration of new diffusion model designs, from operator replacement to architecture restructuring.

Abstract: Designing model architectures requires decisions such as selecting operators
(e.g., attention, convolution) and configurations (e.g., depth, width).
However, evaluating the impact of these decisions on model quality requires
costly pretraining, limiting architectural investigation. Inspired by how new
software is built on existing code, we ask: can new architecture designs be
studied using pretrained models? To this end, we present grafting, a simple
approach for editing pretrained diffusion transformers (DiTs) to materialize
new architectures under small compute budgets. Informed by our analysis of
activation behavior and attention locality, we construct a testbed based on the
DiT-XL/2 design to study the impact of grafting on model quality. Using this
testbed, we develop a family of hybrid designs via grafting: replacing softmax
attention with gated convolution, local attention, and linear attention, and
replacing MLPs with variable expansion ratio and convolutional variants.
Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for
DiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model
(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval
score. Finally, we present a case study that restructures DiT-XL/2 by
converting every pair of sequential transformer blocks into parallel blocks via
grafting. This reduces model depth by 2x and yields better quality (FID: 2.77)
than other models of comparable depth. Together, we show that new diffusion
model designs can be explored by grafting pretrained DiTs, with edits ranging
from operator replacement to architecture restructuring. Code and grafted
models: https://grafting.stanford.edu

</details>


### [566] [Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review](https://arxiv.org/pdf/2506.02887)
*Mrinmay Sen, Shruti Aparna, Rohit Agarwal, Chalavadi Krishna Mohan*

Main category: cs.LG

TL;DR: Survey on partial client participation in Federated Learning (FL), reviewing methods, challenges, and categorizing solutions with theoretical and empirical insights.


<details>
  <summary>Details</summary>
Motivation: Address the gap in research on partial client participation in FL, a common real-world scenario, by analyzing its practical and theoretical challenges.

Method: In-depth review and categorization of existing FL methods for partial client participation, supported by theoretical and empirical analysis.

Result: Comprehensive analysis of methods, their pros and cons, and insights into handling partial participation in FL.

Conclusion: Highlights the importance of addressing partial client participation in FL and provides a structured overview of solutions.

Abstract: Federated Learning (FL) is a learning mechanism that falls under the
distributed training umbrella, which collaboratively trains a shared global
model without disclosing the raw data from different clients. This paper
presents an extensive survey on the impact of partial client participation in
federated learning. While much of the existing research focuses on addressing
issues such as generalization, robustness, and fairness caused by data
heterogeneity under the assumption of full client participation, limited
attention has been given to the practical and theoretical challenges arising
from partial client participation, which is common in real-world scenarios.
This survey provides an in-depth review of existing FL methods designed to cope
with partial client participation. We offer a comprehensive analysis supported
by theoretical insights and empirical findings, along with a structured
categorization of these methods, highlighting their respective advantages and
disadvantages.

</details>


### [567] [Non-Asymptotic Length Generalization](https://arxiv.org/pdf/2506.03085)
*Thomas Chen, Tengyu Ma, Zhiyuan Li*

Main category: cs.LG

TL;DR: The paper provides provable guarantees for length generalization in learning algorithms, formalizing non-asymptotic length generalization and showing optimal results for specific function classes like C-RASP.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the ability of learning algorithms to generalize to longer inputs than those seen in training, focusing on computable bounds and complexity measures.

Method: The study formalizes non-asymptotic length generalization, introduces length complexity, and analyzes it for various function classes (e.g., Deterministic Finite Automata, C-RASP) using theoretical proofs.

Result: Optimal length complexity is achieved for some classes (e.g., $2n - 2$ for DFAs), while others (e.g., Context-Free Grammars) lack computable bounds. C-RASP functions show polynomial bounds.

Conclusion: The paper establishes a framework for length generalization, proving computable bounds for specific function classes and highlighting limitations for others.

Abstract: Length generalization is the ability of a learning algorithm to learn a
hypothesis which generalizes to longer inputs than the inputs in the training
set. In this paper, we provide provable guarantees of length generalization for
various classes of functions in an idealized setting. First, we formalize the
framework of non-asymptotic length generalization, which requires a computable
upper bound for the minimum input length that guarantees length generalization,
as a function of the complexity of ground-truth function under some given
complexity measure. We refer to this minimum input length to length generalize
as length complexity. We show the Minimum-Complexity Interpolator learning
algorithm achieves optimal length complexity. We further show that whether a
function class admits non-asymptotic length generalization is equivalent to the
decidability of its language equivalence problem, which implies that there is
no computable upper bound for the length complexity of Context-Free Grammars.
On the positive side, we show that the length complexity of Deterministic
Finite Automata is $2n - 2$ where $n$ is the number of states of the
ground-truth automaton. Our main results are upper bounds of length complexity
for a subset of a transformer-related function class called C-RASP (Yang &
Chiang, 2024). We show that the length complexity of 1-layer C-RASP functions
is $O(T^2)$ when the ground-truth function has precision $T$, and that the
length complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the
ground-truth function has precision $T$ and $K$ heads.

</details>


### [568] [Faster Approx. Top-K: Harnessing the Full Power of Two Stages](https://arxiv.org/pdf/2506.04165)
*Yashas Samaga, Varun Yerram, Spandana Raj Babbula, Prateek Jain, Praneeth Netrapalli*

Main category: cs.LG

TL;DR: A generalized two-stage Top-K selection algorithm improves efficiency and recall bounds over the original method, achieving significant speedups on Cloud TPUv5e.


<details>
  <summary>Details</summary>
Motivation: Top-K selection is a bottleneck in machine learning on accelerators, prompting a need for faster, efficient algorithms.

Method: Generalizes a two-stage approximate Top-K algorithm by selecting top-K' elements per partition in the first stage, reducing input size for sorting in the second stage.

Result: Derives tighter recall bounds and shows improved efficiency, achieving order-of-magnitude speedups on Cloud TPUv5e without recall loss.

Conclusion: The generalized algorithm enhances performance and theoretical guarantees, making it practical for real-world applications.

Abstract: We consider the Top-$K$ selection problem, which aims to identify the
largest-$K$ elements from an array. Top-$K$ selection arises in many machine
learning algorithms and often becomes a bottleneck on accelerators, which are
optimized for dense matrix multiplications. To address this problem,
\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage
\textit{approximate} Top-$K$ algorithm: (i) partition the input array and
select the top-$1$ element from each partition, (ii) sort this \textit{smaller
subset} and return the top $K$ elements. In this paper, we consider a
generalized version of this algorithm, where the first stage selects top-$K'$
elements, for some $1 \leq K' \leq K$, from each partition. Our contributions
are as follows: (i) we derive an expression for the expected recall of this
generalized algorithm and show that choosing $K' > 1$ with fewer partitions in
the first stage reduces the input size to the second stage more effectively
while maintaining the same expected recall as the original algorithm, (ii) we
derive a bound on the expected recall for the original algorithm in
\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of
$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud
TPUv5e and achieve around an order of magnitude speedups over the original
algorithm without sacrificing recall on real-world tasks.

</details>


### [569] [Predicting ICU In-Hospital Mortality Using Adaptive Transformer Layer Fusion](https://arxiv.org/pdf/2506.04924)
*Han Wang, Ruoyun He, Guoguang Lao, Ting Liu, Hejiao Luo, Changqi Qin, Hongying Luo, Junmin Huang, Zihan Wei, Lu Chen, Yongzhi Xu, Ziqian Bi, Junhao Song, Tianyang Wang, Chia Xin Liang, Xinyuan Song, Huafeng Liu, Junfeng Hao, Chunjie Tian*

Main category: cs.LG

TL;DR: ALFIA is a modular, attention-based architecture for early identification of high-risk ICU patients, outperforming state-of-the-art methods and enabling seamless integration with other models for enhanced performance.


<details>
  <summary>Details</summary>
Motivation: Early identification of high-risk ICU patients is essential for optimizing limited medical resources.

Method: ALFIA uses adaptive layer fusion with intelligent attention, training LoRA adapters and an adaptive layer-weighting mechanism on the cw-24 benchmark.

Result: ALFIA surpasses state-of-the-art tabular classifiers in AUPRC and integrates well with GBDTs and deep neural networks for further gains.

Conclusion: ALFIA provides a robust tool for risk stratification and timely intervention in critical-care settings.

Abstract: Early identification of high-risk ICU patients is crucial for directing
limited medical resources. We introduce ALFIA (Adaptive Layer Fusion with
Intelligent Attention), a modular, attention-based architecture that jointly
trains LoRA (Low-Rank Adaptation) adapters and an adaptive layer-weighting
mechanism to fuse multi-layer semantic features from a BERT backbone. Trained
on our rigorous cw-24 (CriticalWindow-24) benchmark, ALFIA surpasses
state-of-the-art tabular classifiers in AUPRC while preserving a balanced
precision-recall profile. The embeddings produced by ALFIA's fusion module,
capturing both fine-grained clinical cues and high-level concepts, enable
seamless pairing with GBDTs (CatBoost/LightGBM) as ALFIA-boost, and deep neuro
networks as ALFIA-nn, yielding additional performance gains. Our experiments
confirm ALFIA's superior early-warning performance, by operating directly on
routine clinical text, it furnishes clinicians with a convenient yet robust
tool for risk stratification and timely intervention in critical-care settings.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [570] [A MARL-based Approach for Easing MAS Organization Engineering](https://arxiv.org/pdf/2506.05437)
*Julien Soulé, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul Théron*

Main category: cs.MA

TL;DR: AOMEA combines MARL with an organizational model to simplify MAS design in complex environments.


<details>
  <summary>Details</summary>
Motivation: High complexity and low readability in deployment environments make traditional MAS design methods costly or unsafe.

Method: AOMEA integrates Multi-Agent Reinforcement Learning (MARL) with an organizational model to suggest MAS specifications.

Result: The approach aims to ease MAS organization design and address safety and cost concerns.

Conclusion: AOMEA offers a novel method to improve MAS engineering in challenging environments.

Abstract: Multi-Agent Systems (MAS) have been successfully applied in industry for
their ability to address complex, distributed problems, especially in IoT-based
systems. Their efficiency in achieving given objectives and meeting design
requirements is strongly dependent on the MAS organization during the
engineering process of an application-specific MAS. To design a MAS that can
achieve given goals, available methods rely on the designer's knowledge of the
deployment environment. However, high complexity and low readability in some
deployment environments make the application of these methods to be costly or
raise safety concerns. In order to ease the MAS organization design regarding
those concerns, we introduce an original Assisted MAS Organization Engineering
Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement
Learning (MARL) process with an organizational model to suggest relevant
organizational specifications to help in MAS engineering.

</details>


### [571] [Sequence Modeling for N-Agent Ad Hoc Teamwork](https://arxiv.org/pdf/2506.05527)
*Caroline Wang, Di Yang Shi, Elad Liebman, Ishan Durugkar, Arrasy Rahman, Peter Stone*

Main category: cs.MA

TL;DR: A transformer-based method (MAT-NAHT) is introduced for N-agent ad hoc teamwork, outperforming POAM in sample efficiency and generalization without needing agent-modeling objectives.


<details>
  <summary>Details</summary>
Motivation: Existing methods like POAM rely on independent learning, which misses inter-agent dynamics crucial for effective collaboration in dynamic multi-agent settings.

Method: A centralized transformer-based approach is proposed, leveraging historical observations and actions of all controlled agents to handle diverse and unseen teammates.

Result: MAT-NAHT achieves superior performance over POAM in a StarCraft II task, demonstrating better sample efficiency and generalization.

Conclusion: The transformer-based method effectively addresses the challenges of N-agent ad hoc teamwork, offering improved collaboration without additional modeling objectives.

Abstract: N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent
reinforcement learning, where controlled subteams of varying sizes must
dynamically collaborate with varying numbers and types of unknown teammates
without pre-coordination. The existing learning algorithm (POAM) considers only
independent learning for its flexibility in dealing with a changing number of
agents. However, independent learning fails to fully capture the inter-agent
dynamics essential for effective collaboration. Based on our observation that
transformers deal effectively with sequences with varying lengths and have been
shown to be highly effective for a variety of machine learning problems, this
work introduces a centralized, transformer-based method for N-agent ad hoc
teamwork. Our proposed approach incorporates historical observations and
actions of all controlled agents, enabling optimal responses to diverse and
unseen teammates in partially observable environments. Empirical evaluation on
a StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving
superior sample efficiency and generalization, without auxiliary agent-modeling
objectives.

</details>


### [572] [Using Large Language Models to Simulate Human Behavioural Experiments: Port of Mars](https://arxiv.org/pdf/2506.05555)
*Oliver Slumbers, Joel Z. Leibo, Marco A. Janssen*

Main category: cs.MA

TL;DR: The paper explores using generative AI (LLMs) to simulate large-scale human experiments for collective risk social dilemmas (CRSDs), addressing challenges like power and diversity in socio-demographic factors.


<details>
  <summary>Details</summary>
Motivation: Understanding CRSDs (e.g., climate change) is critical, but large-scale human experiments are challenging. Generative AI offers a scalable alternative.

Method: Replace human participants with LLMs to simulate diverse, large-scale experiments, focusing on a complex CRSD called Port of Mars.

Result: The study evaluates the validity and feasibility of using LLMs to represent human-like diversity in CRSD experiments.

Conclusion: Generative AI (LLMs) could provide a viable, scalable framework for studying CRSDs, complementing traditional human experiments.

Abstract: Collective risk social dilemmas (CRSD) highlight a trade-off between
individual preferences and the need for all to contribute toward achieving a
group objective. Problems such as climate change are in this category, and so
it is critical to understand their social underpinnings. However, rigorous CRSD
methodology often demands large-scale human experiments but it is difficult to
guarantee sufficient power and heterogeneity over socio-demographic factors.
Generative AI offers a potential complementary approach to address thisproblem.
By replacing human participants with large language models (LLM), it allows for
a scalable empirical framework. This paper focuses on the validity of this
approach and whether it is feasible to represent a large-scale human-like
experiment with sufficient diversity using LLM. In particular, where previous
literature has focused on political surveys, virtual towns and classical
game-theoretic examples, we focus on a complex CRSD used in the institutional
economics and sustainability literature known as Port of Mars

</details>


### [573] [Modeling human reputation-seeking behavior in a spatio-temporally complex public good provision game](https://arxiv.org/pdf/2506.06032)
*Edward Hughes, Tina O. Zhu, Martin J. Chadwick, Raphael Koster, Antonio García Castañeda, Charles Beattie, Thore Graepel, Matthew M. Botvinick, Joel Z. Leibo*

Main category: cs.MA

TL;DR: Multi-agent reinforcement learning models human group behavior in a public good game, showing success with reputation tracking and failure under anonymity, mirroring human results.


<details>
  <summary>Details</summary>
Motivation: To empirically validate multi-agent reinforcement learning for modeling human social behavior in complex settings, unlike theoretical approaches like game theory.

Method: Used a public good provision game (Clean Up) with human and artificial agents, comparing identifiable (reputation-tracking) and anonymous conditions.

Result: Human and artificial groups succeeded with reputation tracking but failed under anonymity, both using turn-taking.

Conclusion: Multi-agent reinforcement learning effectively models human social behavior in complex environments, especially with reputation mechanisms.

Abstract: Multi-agent reinforcement learning algorithms are useful for simulating
social behavior in settings that are too complex for other theoretical
approaches like game theory. However, they have not yet been empirically
supported by laboratory experiments with real human participants. In this work
we demonstrate how multi-agent reinforcement learning can model group behavior
in a spatially and temporally complex public good provision game called Clean
Up. We show that human groups succeed in Clean Up when they can see who is who
and track reputations over time but fail under conditions of anonymity. A new
multi-agent reinforcement learning model of reputation-based cooperation
demonstrates the same difference between identifiable and anonymous conditions.
Furthermore, both human groups and artificial agent groups solve the problem
via turn-taking despite other options being available. Our results highlight
the benefits of using multi-agent reinforcement learning to model human social
behavior in complex environments.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [574] [DeepFake Doctor: Diagnosing and Treating Audio-Video Fake Detection](https://arxiv.org/pdf/2506.05851)
*Marcel Klemt, Carlotta Segna, Anna Rohrbach*

Main category: cs.MM

TL;DR: The paper addresses challenges in audio-video DeepFake detection, focusing on benchmarking issues like datasets, methods, and evaluation. It introduces the DeepSpeak v1 dataset, proposes an evaluation protocol, and presents SIMBA, a simple baseline model. It also tackles audio shortcuts and improves evaluation on FakeAVCeleb.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of generative AI poses security and ethical threats through realistic manipulated media. Existing DeepFake detection methods face reproducibility issues and dataset flaws, necessitating better benchmarking.

Method: The study examines benchmarking pillars (datasets, methods, evaluation) and introduces DeepSpeak v1. It proposes SIMBA, a minimalistic baseline, and addresses audio shortcuts with a mitigation strategy.

Result: The paper benchmarks SOTA models on DeepSpeak v1, improves FakeAVCeleb evaluation, and demonstrates SIMBA's effectiveness. It also mitigates audio shortcut issues.

Conclusion: The work advances audio-video DeepFake detection by addressing benchmarking challenges, introducing new tools, and offering insights for future research.

Abstract: Generative AI advances rapidly, allowing the creation of very realistic
manipulated video and audio. This progress presents a significant security and
ethical threat, as malicious users can exploit DeepFake techniques to spread
misinformation. Recent DeepFake detection approaches explore the multimodal
(audio-video) threat scenario. In particular, there is a lack of
reproducibility and critical issues with existing datasets - such as the
recently uncovered silence shortcut in the widely used FakeAVCeleb dataset.
Considering the importance of this topic, we aim to gain a deeper understanding
of the key issues affecting benchmarking in audio-video DeepFake detection. We
examine these challenges through the lens of the three core benchmarking
pillars: datasets, detection methods, and evaluation protocols. To address
these issues, we spotlight the recent DeepSpeak v1 dataset and are the first to
propose an evaluation protocol and benchmark it using SOTA models. We introduce
SImple Multimodal BAseline (SIMBA), a competitive yet minimalistic approach
that enables the exploration of diverse design choices. We also deepen insights
into the issue of audio shortcuts and present a promising mitigation strategy.
Finally, we analyze and enhance the evaluation scheme on the widely used
FakeAVCeleb dataset. Our findings offer a way forward in the complex area of
audio-video DeepFake detection.

</details>


### [575] [The JPEG XL Image Coding System: History, Features, Coding Tools, Design Rationale, and Future](https://arxiv.org/pdf/2506.05987)
*Jon Sneyers, Jyrki Alakuijala, Luca Versari, Zoltán Szabadka, Sami Boukortt, Amnon Cohen-Tidhar, Moritz Firsching, Evgenii Kliuchnikov, Tal Lev-Ami, Eric Portis, Thomas Richter, Osamu Watanabe*

Main category: cs.MM

TL;DR: JPEG XL is a modern image coding system designed to replace multiple formats with one universal codec, offering advanced features and lossless JPEG recompression.


<details>
  <summary>Details</summary>
Motivation: To provide a single, efficient image coding system that replaces outdated formats like JPEG, PNG, and GIF.

Method: Overview of JPEG XL, including its history, design rationale, and coding tools.

Result: JPEG XL achieves state-of-the-art compression and supports lossless recompression of JPEG.

Conclusion: JPEG XL has the potential to become a universal image format, with detailed technical insights provided in the article.

Abstract: JPEG XL is a new image coding system offering state-of-the-art compression
performance, lossless JPEG recompression, and advanced features. It aims to
replace JPEG, PNG, GIF, and other formats with a single universal codec. This
article provides an overview of JPEG XL, including its history, design
rationale, coding tools, and future potential. It can be used as a companion
document to the standard (ISO/IEC 18181), or as a standalone article to better
understand JPEG XL, either at a high level or in considerable technical detail.

</details>


### [576] [Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models](https://arxiv.org/pdf/2506.06018)
*Chaoyi Zhu, Zaitang Li, Renyi Yang, Robert Birke, Pin-Yu Chen, Tsung-Yi Ho, Lydia Y. Chen*

Main category: cs.MM

TL;DR: The paper introduces PnP, an optimization-free watermark forgery attack using regenerative diffusion models, posing a significant threat to current watermarking security.


<details>
  <summary>Details</summary>
Motivation: To highlight the vulnerability of watermarking in synthetic images by demonstrating a universal forgery method that bypasses existing defenses.

Method: Proposes PnP, which extracts and integrates watermarks via regenerative diffusion models without optimization, using watermarked latent and visual-textual context as priors.

Result: PnP achieves up to 100% detectability and user attribution while maintaining visual quality, tested across 24 scenarios.

Conclusion: PnP broadens forgery attack scope, challenging watermarking security and governance in synthetic data.

Abstract: Watermarking becomes one of the pivotal solutions to trace and verify the
origin of synthetic images generated by artificial intelligence models, but it
is not free of risks. Recent studies demonstrate the capability to forge
watermarks from a target image onto cover images via adversarial optimization
without knowledge of the target generative model and watermark schemes. In this
paper, we uncover a greater risk of an optimization-free and universal
watermark forgery that harnesses existing regenerative diffusion models. Our
proposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and
integrates the target watermark via regenerating the image, without needing any
additional optimization routine. It allows for universal watermark forgery that
works independently of the target image's origin or the watermarking model
used. We explore the watermarked latent extracted from the target image and
visual-textual context of cover images as priors to guide sampling of the
regenerative process. Extensive evaluation on 24 scenarios of
model-data-watermark combinations demonstrates that PnP can successfully forge
the watermark (up to 100% detectability and user attribution), and maintain the
best visual perception. By bypassing model retraining and enabling adaptability
to any image, our approach significantly broadens the scope of forgery attacks,
presenting a greater challenge to the security of current watermarking
techniques for diffusion models and the authority of watermarking schemes in
synthetic data generation and governance.

</details>


### [577] [SVD: Spatial Video Dataset](https://arxiv.org/pdf/2506.06037)
*M. H. Izadimehr, Milad Ghanbari, Guodong Chen, Wei Zhou, Xiaoshuai Hao, Mallesham Dasari, Christian Timmerer, Hadi Amirpour*

Main category: cs.MM

TL;DR: The paper introduces SVD, a spatial video dataset with 300 short and 10 longer videos, captured using iPhone Pro and Apple Vision Pro, to support research in 3D applications.


<details>
  <summary>Details</summary>
Motivation: The lack of publicly available datasets for the complete spatial video pipeline hinders research in stereoscopic video applications.

Method: The SVD dataset includes 300 five-second and 10 longer videos, captured with iPhone Pro and Apple Vision Pro, released under an open-access license.

Result: The dataset enables research in codec performance, QoE assessment, depth-based vision, and emerging 3D applications.

Conclusion: SVD fills a gap in spatial video research, fostering advancements in stereoscopic video technology and applications.

Abstract: Stereoscopic video has long been the subject of research due to its capacity
to deliver immersive three-dimensional content across a wide range of
applications, from virtual and augmented reality to advanced human-computer
interaction. The dual-view format inherently provides binocular disparity cues
that enhance depth perception and realism, making it indispensable for fields
such as telepresence, 3D mapping, and robotic vision. Until recently, however,
end-to-end pipelines for capturing, encoding, and viewing high-quality 3D video
were neither widely accessible nor optimized for consumer-grade devices.
Today's smartphones, such as the iPhone Pro, and modern Head-Mounted Displays
(HMDs), like the Apple Vision Pro (AVP), offer built-in support for
stereoscopic video capture, hardware-accelerated encoding, and seamless
playback on devices like the Apple Vision Pro and Meta Quest 3, requiring
minimal user intervention. Apple refers to this streamlined workflow as spatial
video. Making the full stereoscopic video process available to everyone has
made new applications possible. Despite these advances, there remains a notable
absence of publicly available datasets that include the complete spatial video
pipeline.
  In this paper, we introduce SVD, a spatial video dataset comprising 300
five-second video sequences, 150 captured using an iPhone Pro and 150 with an
AVP. Additionally, 10 longer videos with a minimum duration of 2 minutes have
been recorded. The SVD dataset is publicly released under an open-access
license to facilitate research in codec performance evaluation, subjective and
objective quality of experience (QoE) assessment, depth-based computer vision,
stereoscopic video streaming, and other emerging 3D applications such as neural
rendering and volumetric capture. Link to the dataset:
https://cd-athena.github.io/SVD/

</details>


### [578] [Latent Feature-Guided Conditional Diffusion for Generative Image Semantic Communication](https://arxiv.org/pdf/2504.21577)
*Zehao Chen, Xinfeng Wei, Haonan Tong, Zhaohui Yang, Changchuan Yin*

Main category: cs.MM

TL;DR: Proposes an ROI-aware image semantic communication system (LRISC) using latent features and a conditional diffusion model, outperforming existing methods in LPIPS and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing image semantic communication schemes neglect ROI preservation, focusing only on pixel-level metrics.

Method: Maps images to latent features, fuses with ROI masks, uses JSCC for transmission, and employs a conditional diffusion model for reconstruction with SNR adaptation.

Result: Achieves 43.3% average LPIPS reduction over DeepJSCC, ensuring semantic consistency and robustness.

Conclusion: LRISC effectively improves ROI preservation and transmission efficiency in semantic communication.

Abstract: Semantic communication is proposed and expected to improve the efficiency of
massive data transmission over sixth generation (6G) networks. However,
existing image semantic communication schemes are primarily focused on
optimizing pixel-level metrics, while neglecting the crucial aspect of region
of interest (ROI) preservation. To address this issue, we propose an ROI-aware
latent representation-oriented image semantic communication (LRISC) system. In
particular, we first map the source image to latent features in a
high-dimensional semantic space, these latent features are then fused with ROI
mask through a feature-weighting mechanism. Subsequently, these features are
encoded using a joint source and channel coding (JSCC) scheme with adaptive
rate for efficient transmission over a wireless channel. At the receiver, a
conditional diffusion model is developed by using the received latent features
as conditional guidance to steer the reverse diffusion process, progressively
reconstructing high-fidelity images while preserving semantic consistency.
Moreover, we introduce a channel signal-to-noise ratio (SNR) adaptation
mechanism, allowing one model to work across various channel states.
Experiments show that the proposed method significantly outperforms existing
methods, in terms of learned perceptual image patch similarity (LPIPS) and
robustness against channel noise, with an average LPIPS reduction of 43.3%
compared to DeepJSCC, while guaranteeing the semantic consistency.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [579] [Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning](https://arxiv.org/pdf/2506.05671)
*Yangui Fang, Jing Peng, Xu Li, Yu Xi, Chengwei Zhang, Guohui Zhong, Kai Yu*

Main category: eess.AS

TL;DR: A text-only fine-tuning method for Speech LLMs improves domain adaptation in low-resource settings without needing additional audio, maintaining performance and alignment.


<details>
  <summary>Details</summary>
Motivation: Adapting Speech LLMs to new domains is challenging, especially with scarce paired speech-text data.

Method: Proposes text-only fine-tuning using unpaired target-domain text, with a real-time evaluation mechanism to preserve alignment.

Result: Achieves competitive recognition performance with minimal degradation, improving generalization without catastrophic forgetting.

Conclusion: Text-only fine-tuning is effective for low-resource domain adaptation in ASR.

Abstract: Recent advances in automatic speech recognition (ASR) have combined speech
encoders with large language models (LLMs) through projection, forming Speech
LLMs with strong performance. However, adapting them to new domains remains
challenging, especially in low-resource settings where paired speech-text data
is scarce. We propose a text-only fine-tuning strategy for Speech LLMs using
unpaired target-domain text without requiring additional audio. To preserve
speech-text alignment, we introduce a real-time evaluation mechanism during
fine-tuning. This enables effective domain adaptation while maintaining
source-domain performance. Experiments on LibriSpeech, SlideSpeech, and Medical
datasets show that our method achieves competitive recognition performance,
with minimal degradation compared to full audio-text fine-tuning. It also
improves generalization to new domains without catastrophic forgetting,
highlighting the potential of text-only fine-tuning for low-resource domain
adaptation of ASR.

</details>


### [580] [Bridging the Modality Gap: Softly Discretizing Audio Representation for LLM-based Automatic Speech Recognition](https://arxiv.org/pdf/2506.05706)
*Mu Yang, Szu-Jui Chen, Jiamin Xie, John Hansen*

Main category: eess.AS

TL;DR: Proposes a method using vector quantization (VQ) to align continuous audio data with discrete LLM inputs, improving LLM-based ASR performance, especially in out-of-domain conditions.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of integrating continuous audio data with discrete token-based LLMs for better ASR performance.

Method: Integrates VQ into LLM-based ASR by using the LLM embedding table as a VQ codebook and soft discretization of audio representations.

Result: Significantly improves LLM-based ASR baseline, particularly in out-of-domain conditions.

Conclusion: Highlights the potential of soft discretization as a modality bridge in LLM-based ASR.

Abstract: One challenge of integrating speech input with large language models (LLMs)
stems from the discrepancy between the continuous nature of audio data and the
discrete token-based paradigm of LLMs. To mitigate this gap, we propose a
method for integrating vector quantization (VQ) into LLM-based automatic speech
recognition (ASR). Using the LLM embedding table as the VQ codebook, the VQ
module aligns the continuous representations from the audio encoder with the
discrete LLM inputs, enabling the LLM to operate on a discretized audio
representation that better reflects the linguistic structure. We further create
a soft "discretization" of the audio representation by updating the codebook
and performing a weighted sum over the codebook embeddings. Empirical results
demonstrate that our proposed method significantly improves upon the LLM-based
ASR baseline, particularly in out-of-domain conditions. This work highlights
the potential of soft discretization as a modality bridge in LLM-based ASR.

</details>


### [581] [Diarization-Aware Multi-Speaker Automatic Speech Recognition via Large Language Models](https://arxiv.org/pdf/2506.05796)
*Yuke Lin, Ming Cheng, Ze Li, Beilong Tang, Ming Li*

Main category: eess.AS

TL;DR: A novel diarization-aware multi-speaker ASR system integrates speaker diarization with LLM-based transcription, improving performance in overlapped speech scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of SOT-style methods in MS-ASR, particularly the loss of absolute timing information, for time-sensitive applications.

Method: Proposes a framework combining speaker diarization and LLM-based transcription, using structured diarization inputs and frame-level embeddings.

Result: Achieves robust performance in multilingual dyadic conversations and excels in high-overlap multi-speaker meetings.

Conclusion: Demonstrates LLMs' potential as unified back-ends for joint speaker-aware segmentation and transcription.

Abstract: Multi-speaker automatic speech recognition (MS-ASR) faces significant
challenges in transcribing overlapped speech, a task critical for applications
like meeting transcription and conversational analysis. While serialized output
training (SOT)-style methods serve as common solutions, they often discard
absolute timing information, limiting their utility in time-sensitive
scenarios. Leveraging recent advances in large language models (LLMs) for
conversational audio processing, we propose a novel diarization-aware
multi-speaker ASR system that integrates speaker diarization with LLM-based
transcription. Our framework processes structured diarization inputs alongside
frame-level speaker and semantic embeddings, enabling the LLM to generate
segment-level transcriptions. Experiments demonstrate that the system achieves
robust performance in multilingual dyadic conversations and excels in complex,
high-overlap multi-speaker meeting scenarios. This work highlights the
potential of LLMs as unified back-ends for joint speaker-aware segmentation and
transcription.

</details>


### [582] [TADA: Training-free Attribution and Out-of-Domain Detection of Audio Deepfakes](https://arxiv.org/pdf/2506.05802)
*Adriana Stan, David Combei, Dan Oneata, Hora Cucu*

Main category: eess.AS

TL;DR: The paper proposes a training-free, green AI method using k-Nearest Neighbors (kNN) for audio deepfake model attribution, achieving high F1-scores (0.93 for known models, 0.84 for unseen models).


<details>
  <summary>Details</summary>
Motivation: While deepfake detection is well-studied, identifying the exact source (model or system) behind a deepfake remains underexplored. This work addresses this gap in audio deepfake attribution.

Method: The approach uses a pre-trained self-supervised learning (SSL) model combined with kNN for training-free source tracing.

Result: Achieves 0.93 F1-score for known models and 0.84 for out-of-domain (unseen) models.

Conclusion: The method is effective for audio deepfake model attribution, with strong performance and open-sourced code/data.

Abstract: Deepfake detection has gained significant attention across audio, text, and
image modalities, with high accuracy in distinguishing real from fake. However,
identifying the exact source--such as the system or model behind a
deepfake--remains a less studied problem. In this paper, we take a significant
step forward in audio deepfake model attribution or source tracing by proposing
a training-free, green AI approach based entirely on k-Nearest Neighbors (kNN).
Leveraging a pre-trained self-supervised learning (SSL) model, we show that
grouping samples from the same generator is straightforward--we obtain an 0.93
F1-score across five deepfake datasets. The method also demonstrates strong
out-of-domain (OOD) detection, effectively identifying samples from unseen
models at an F1-score of 0.84.
  We further analyse these results in a multi-dimensional approach and provide
additional insights. All code and data protocols used in this work are
available in our open repository: https://github.com/adrianastan/tada/.

</details>


### [583] [Audio-Aware Large Language Models as Judges for Speaking Styles](https://arxiv.org/pdf/2506.05984)
*Cheng-Han Chiang, Xiaofei Wang, Chung-Ching Lin, Kevin Lin, Linjie Li, Radu Kopetz, Yao Qian, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang*

Main category: eess.AS

TL;DR: The paper explores using Audio-aware Large Language Models (ALLMs) as judges to evaluate speaking styles in speeches generated by Spoken Language Models (SLMs), showing promising agreement with human evaluators.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of ALLMs in evaluating speaking styles (e.g., emotion, pace) in speeches generated by SLMs, comparing their performance to human judges.

Method: Four SLMs generate speeches for two tasks (voice style instruction following and role-playing), evaluated by humans and two ALLM judges (GPT-4o-audio and Gemini-2.5-pro).

Result: Gemini-2.5-pro's agreement with human judges is comparable to inter-human agreement, indicating ALLMs' potential as evaluators. Current SLMs, including GPT-4o-audio, still struggle with speaking style control and natural dialogue generation.

Conclusion: ALLMs can effectively judge speaking styles in SLM-generated speeches, though SLMs need further improvement in style control and dialogue naturalness.

Abstract: Audio-aware large language models (ALLMs) can understand the textual and
non-textual information in the audio input. In this paper, we explore using
ALLMs as an automatic judge to assess the speaking styles of speeches. We use
ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice
style instruction following and role-playing. The speaking style we consider
includes emotion, volume, speaking pace, word emphasis, pitch control, and
non-verbal elements. We use four spoken language models (SLMs) to complete the
two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two
ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and
show that the agreement between Gemini and human judges is comparable to the
agreement between human evaluators. These promising results show that ALLMs can
be used as a judge to evaluate SLMs. Our results also reveal that current SLMs,
even GPT-4o-audio, still have room for improvement in controlling the speaking
style and generating natural dialogues.

</details>


### [584] [CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for Fair Speech Emotion Recognition](https://arxiv.org/pdf/2506.06071)
*Yun-Shao Tsai, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee*

Main category: eess.AS

TL;DR: CO-VADA is a debiasing method for speech emotion recognition (SER) that avoids model-specific changes or demographic data by using voice conversion to alter irrelevant speaker attributes.


<details>
  <summary>Details</summary>
Motivation: Bias in SER systems arises from spurious correlations between speaker traits and emotions, causing unfair predictions. Existing debiasing methods are impractical due to reliance on model changes or demographic labels.

Method: CO-VADA identifies biased training samples, applies voice conversion to alter irrelevant attributes, and generates augmented samples to diversify speaker variations.

Result: The approach improves fairness in SER systems without modifying model architecture or needing demographic information.

Conclusion: CO-VADA is a scalable, practical solution for debiasing SER systems, compatible with various models and voice conversion tools.

Abstract: Bias in speech emotion recognition (SER) systems often stems from spurious
correlations between speaker characteristics and emotional labels, leading to
unfair predictions across demographic groups. Many existing debiasing methods
require model-specific changes or demographic annotations, limiting their
practical use. We present CO-VADA, a Confidence-Oriented Voice Augmentation
Debiasing Approach that mitigates bias without modifying model architecture or
relying on demographic information. CO-VADA identifies training samples that
reflect bias patterns present in the training data and then applies voice
conversion to alter irrelevant attributes and generate samples. These augmented
samples introduce speaker variations that differ from dominant patterns in the
data, guiding the model to focus more on emotion-relevant features. Our
framework is compatible with various SER models and voice conversion tools,
making it a scalable and practical solution for improving fairness in SER
systems.

</details>


### [585] [Lightweight Prompt Biasing for Contextualized End-to-End ASR Systems](https://arxiv.org/pdf/2506.06252)
*Bo Ren, Yu Shi, Jinyu Li*

Main category: eess.AS

TL;DR: A prompt-based biasing technique improves ASR accuracy for rare and domain-specific entities using a multitask framework, achieving significant error rate reductions.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of recognizing rare and domain-specific entities in ASR systems.

Method: Combines a prompt biasing model and entity filtering mechanism within a multitask learning framework.

Result: 30.7% and 18.0% relative reduction in Entity Word Error Rate for small and large entity lists, respectively.

Conclusion: The method is efficient, simple, and lightweight, enhancing ASR accuracy without structural changes.

Abstract: End-to-End Automatic Speech Recognition (ASR) has advanced significantly yet
still struggles with rare and domain-specific entities. This paper introduces a
simple yet efficient prompt-based biasing technique for contextualized ASR,
enhancing recognition accuracy by leverage a unified multitask learning
framework. The approach comprises two key components: a prompt biasing model
which is trained to determine when to focus on entities in prompt, and a entity
filtering mechanism which efficiently filters out irrelevant entities. Our
method significantly enhances ASR accuracy on entities, achieving a relative
30.7% and 18.0% reduction in Entity Word Error Rate compared to the baseline
model with shallow fusion on in-house domain dataset with small and large
entity lists, respectively. The primary advantage of this method lies in its
efficiency and simplicity without any structure change, making it lightweight
and highly efficient.

</details>


### [586] [Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with Retrieval-Augmented Generation](https://arxiv.org/pdf/2411.05141)
*Mu Yang, Bowen Shi, Matthew Le, Wei-Ning Hsu, Andros Tjandra*

Main category: eess.AS

TL;DR: Audiobox TTA-RAG improves zero-shot and few-shot Text-To-Audio generation by augmenting text conditioning with retrieved audio samples, outperforming vanilla methods.


<details>
  <summary>Details</summary>
Motivation: To enhance TTA generation for unseen or uncommon audio events by leveraging retrieval-augmented methods, inspired by RAG in LLMs.

Method: Extends Audiobox (flow-matching audio generation) by adding retrieved audio samples to text conditioning, without needing labeled external data.

Result: Significantly improves zero-shot and few-shot TTA performance across metrics while maintaining in-domain alignment.

Conclusion: Audiobox TTA-RAG is a practical and effective retrieval-augmented approach for TTA generation.

Abstract: This work focuses on improving Text-To-Audio (TTA) generation on zero-shot
and few-shot settings (i.e. generating unseen or uncommon audio events).
Inspired by the success of Retrieval-Augmented Generation (RAG) in Large
Language Models, we propose Audiobox TTA-RAG, a novel retrieval-augmented TTA
approach based on Audiobox, a flow-matching audio generation model. Unlike the
vanilla Audiobox TTA solution that generates audio conditioned on text only, we
extend the TTA process by augmenting the conditioning input with both text and
retrieved audio samples. Our retrieval method does not require the external
database to have labeled audio, offering more practical use cases. We show that
the proposed model can effectively leverage the retrieved audio samples and
significantly improve zero-shot and few-shot TTA performance, with large
margins on multiple evaluation metrics, while maintaining the ability to
generate semantically aligned audio for the in-domain setting.

</details>


### [587] [ARiSE: Auto-Regressive Multi-Channel Speech Enhancement](https://arxiv.org/pdf/2505.22051)
*Pengjie Shen, Xueliang Zhang, Zhong-Qiu Wang*

Main category: eess.AS

TL;DR: ARiSE is an auto-regressive algorithm for multi-channel speech enhancement, improving DNN-based models by leveraging previous frame estimates and beamformed mixtures, with a parallel training mechanism for efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-channel speech by utilizing auto-regressive connections and previous frame estimates, addressing the limitations of existing DNN-based models.

Method: Introduces auto-regressive connections where past target speech estimates and beamformed mixtures are used as extra inputs. Proposes parallel training to speed up the slow auto-regressive training process.

Result: Effective in noisy-reverberant conditions, demonstrating improved performance over existing methods.

Conclusion: ARiSE shows promise for multi-channel speech enhancement, with its auto-regressive approach and efficient training mechanism.

Abstract: We propose ARiSE, an auto-regressive algorithm for multi-channel speech
enhancement. ARiSE improves existing deep neural network (DNN) based
frame-online multi-channel speech enhancement models by introducing
auto-regressive connections, where the estimated target speech at previous
frames is leveraged as extra input features to help the DNN estimate the target
speech at the current frame. The extra input features can be derived from (a)
the estimated target speech in previous frames; and (b) a beamformed mixture
with the beamformer computed based on the previous estimated target speech. On
the other hand, naively training the DNN in an auto-regressive manner is very
slow. To deal with this, we propose a parallel training mechanism to speed up
the training. Evaluation results in noisy-reverberant conditions show the
effectiveness and potential of the proposed algorithms.

</details>


### [588] [Quality Assessment of Noisy and Enhanced Speech with Limited Data: UWB-NTIS System for VoiceMOS 2024 and Beyond](https://arxiv.org/pdf/2506.00506)
*Marie Kunešová*

Main category: eess.AS

TL;DR: The UWB-NTIS-TTS team's system, based on wav2vec 2.0, excelled in the VoiceMOS 2024 Challenge, achieving top results in BAK, OVRL, and SIG metrics.


<details>
  <summary>Details</summary>
Motivation: The goal was to automatically assess speech quality of noisy and de-noised speech using ITU-T P.835 metrics (SIG, BAK, OVRL).

Method: A two-stage fine-tuning process with wav2vec 2.0 was used, addressing training data restrictions.

Result: The system ranked best for BAK, second for OVRL, and third for SIG among five teams.

Conclusion: The approach demonstrated strong performance on both VoiceMOS 2024 and CHiME 7 - UDASE datasets.

Abstract: In this preprint, we present the UWB-NTIS-TTS team's submission to Track 3 of
the VoiceMOS 2024 Challenge, the goal of which was to automatically assess the
speech quality of noisy and de-noised speech in terms of the ITU-T P.835
metrics of "SIG", "BAK", and "OVRL". Our proposed system, based on wav2vec 2.0,
placed among the top systems in the challenge, achieving the best prediction of
the BAK scores (background noise intrusiveness), the second-best prediction of
the OVRL score (overall audio quality), and the third-best prediction of SIG
(speech signal quality) out of the five participating systems. We describe our
approach, such as the two-stage fine-tuning process we used to contend with the
challenge's very limiting restrictions on allowable training data, and present
the results achieved both on the VoiceMOS 2024 Challenge data and on the
recently released CHiME 7 - UDASE dataset.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [589] [Enhancing Neural Autoregressive Distribution Estimators for Image Reconstruction](https://arxiv.org/pdf/2506.05391)
*Ambrose Emmett-Iwaniw, Nathan Kirk*

Main category: eess.IV

TL;DR: The paper proposes an improved ConvNADE model for predicting unobserved image pixels from small patches, showing better performance with low-discrepancy pixel selection.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for image data rely on pixel order, and the study aims to improve prediction accuracy by optimizing pixel selection.

Method: A generalized, efficient ConvNADE model is adapted for real-valued and color images, testing random and low-discrepancy pixel patches.

Result: Low-discrepancy pixel patches reduce test loss and yield more realistic reconstructions compared to random patches.

Conclusion: Optimizing pixel selection with low-discrepancy sequences enhances autoregressive model performance for image reconstruction.

Abstract: Autoregressive models are often employed to learn distributions of image data
by decomposing the $D$-dimensional density function into a product of
one-dimensional conditional distributions. Each conditional depends on
preceding variables (pixels, in the case of image data), making the order in
which variables are processed fundamental to the model performance. In this
paper, we study the problem of observing a small subset of image pixels
(referred to as a pixel patch) to predict the unobserved parts of the image. As
our prediction mechanism, we propose a generalized and computationally
efficient version of the convolutional neural autoregressive distribution
estimator (ConvNADE) model adapted for real-valued and color images. Moreover,
we investigate the quality of image reconstruction when observing both random
pixel patches and low-discrepancy pixel patches inspired by quasi-Monte Carlo
theory. Experiments on benchmark datasets demonstrate that choosing the pixels
akin to a low-discrepancy sequence reduces test loss and produces more
realistic reconstructed images.

</details>


### [590] [Deep histological synthesis from mass spectrometry imaging for multimodal registration](https://arxiv.org/pdf/2506.05441)
*Kimberley M. Bird, Xujiong Ye, Alan M. Race, James M. Brown*

Main category: eess.IV

TL;DR: A method using pix2pix to synthesize histological images from MSI for unimodal registration, improving MI and SSIM metrics.


<details>
  <summary>Details</summary>
Motivation: Registration of histology and MSI is challenging due to differing image processes and dimensionalities.

Method: Proposes a pix2pix model to synthesize histology images from MSI for easier registration.

Result: Achieves higher MI (+0.924) and SSIM (+0.419) compared to a U-Net baseline.

Conclusion: The approach shows promise for precise tissue analysis by improving registration accuracy.

Abstract: Registration of histological and mass spectrometry imaging (MSI) allows for
more precise identification of structural changes and chemical interactions in
tissue. With histology and MSI having entirely different image formation
processes and dimensionalities, registration of the two modalities remains an
ongoing challenge. This work proposes a solution that synthesises histological
images from MSI, using a pix2pix model, to effectively enable unimodal
registration. Preliminary results show promising synthetic histology images
with limited artifacts, achieving increases in mutual information (MI) and
structural similarity index measures (SSIM) of +0.924 and +0.419, respectively,
compared to a baseline U-Net model. Our source code is available on GitHub:
https://github.com/kimberley/MIUA2025.

</details>


### [591] [Reliable Evaluation of MRI Motion Correction: Dataset and Insights](https://arxiv.org/pdf/2506.05975)
*Kun Wang, Tobit Klug, Stefan Ruschke, Jan S. Kirschke, Reinhard Heckel*

Main category: eess.IV

TL;DR: The paper addresses the challenge of evaluating MRI motion correction methods due to lack of ground-truth data. It proposes three evaluation approaches, introduces a dataset (PMoC3D) and a metric (MoMRISim), and finds real-world evaluation with MoMRISim most reliable.


<details>
  <summary>Details</summary>
Motivation: Motion artifacts in MRI hinder accurate diagnosis, but evaluating correction methods is difficult without ground-truth data.

Method: Three evaluation approaches are studied: real-world evaluation, simulated motion, and reference-free evaluation. The PMoC3D dataset and MoMRISim metric are introduced.

Result: Real-world evaluation with MoMRISim is most reliable. Simulated motion overestimates performance, and reference-free evaluation favors oversmoothed outputs.

Conclusion: Real-world evaluation with MoMRISim, despite imperfections, is the most reliable approach for assessing MRI motion correction methods.

Abstract: Correcting motion artifacts in MRI is important, as they can hinder accurate
diagnosis. However, evaluating deep learning-based and classical motion
correction methods remains fundamentally difficult due to the lack of
accessible ground-truth target data. To address this challenge, we study three
evaluation approaches: real-world evaluation based on reference scans,
simulated motion, and reference-free evaluation, each with its merits and
shortcomings. To enable evaluation with real-world motion artifacts, we release
PMoC3D, a dataset consisting of unprocessed Paired Motion-Corrupted 3D brain
MRI data. To advance evaluation quality, we introduce MoMRISim, a feature-space
metric trained for evaluating motion reconstructions. We assess each evaluation
approach and find real-world evaluation together with MoMRISim, while not
perfect, to be most reliable. Evaluation based on simulated motion
systematically exaggerates algorithm performance, and reference-free evaluation
overrates oversmoothed deep learning outputs.

</details>


### [592] [Implicit Neural Representation-Based MRI Reconstruction Method with Sensitivity Map Constraints](https://arxiv.org/pdf/2506.06043)
*Lixuan Rao, Xinlin Zhang, Yiman Huang, Tao Tan, Tong Tong*

Main category: eess.IV

TL;DR: INR-CRISTAL improves fast MRI reconstruction by jointly estimating coil sensitivity maps and images, leveraging their smooth characteristics for better results.


<details>
  <summary>Details</summary>
Motivation: Fast MRI reconstruction is limited by long acquisition times and the challenge of high acceleration. Existing INR methods overlook coil sensitivity map characteristics.

Method: Proposes INR-CRISTAL, a network that jointly estimates coil sensitivity maps and images, adding regularization for sensitivity maps.

Result: INR-CRISTAL provides more accurate coil sensitivity estimates, fewer artifacts, and superior reconstruction performance. It is also more robust to calibration signals and acceleration rates.

Conclusion: INR-CRISTAL enhances fast MRI reconstruction by addressing coil sensitivity map estimation, outperforming existing methods.

Abstract: Magnetic Resonance Imaging (MRI) is a widely utilized diagnostic tool in
clinical settings, but its application is limited by the relatively long
acquisition time. As a result, fast MRI reconstruction has become a significant
area of research. In recent years, Implicit Neural Representation (INR), as a
scan-specific method, has demonstrated outstanding performance in fast MRI
reconstruction without fully-sampled images for training. High acceleration
reconstruction poses a challenging problem, and a key component in achieving
high-quality reconstruction with much few data is the accurate estimation of
coil sensitivity maps. However, most INR-based methods apply regularization
constraints solely to the generated images, while overlooking the
characteristics of the coil sensitivity maps. To handle this, this work
proposes a joint coil sensitivity map and image estimation network, termed
INR-CRISTAL. The proposed INR-CRISTAL introduces an extra sensitivity map
regularization in the INR networks to make use of the smooth characteristics of
the sensitivity maps. Experimental results show that INR-CRISTAL provides more
accurate coil sensitivity estimates with fewer artifacts, and delivers superior
reconstruction performance in terms of artifact removal and structure
preservation. Moreover, INR-CRISTAL demonstrates stronger robustness to
automatic calibration signals and the acceleration rate compared to existing
methods.

</details>


### [593] [FPDANet: A Multi-Section Classification Model for Intelligent Screening of Fetal Ultrasound](https://arxiv.org/pdf/2506.06054)
*Minglang Chen, Jie He, Caixu Xu, Bocheng Liang, Shengli Li, Guannan He, Xiongjie Tao*

Main category: eess.IV

TL;DR: FPDANet improves fetal ultrasound image classification by using a bilateral multi-scale fusion network and positional attention, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: ResNet lacks contextual correlation for fetal ultrasound images, which have low contrast, high similarity, and noise.

Method: Proposes FPDANet with a positional attention module (DAN) and bilateral multi-scale fusion (FPAN) to enhance feature representation.

Result: Achieves 91.05% (Top-1) and 100% (Top-5) classification accuracy.

Conclusion: FPDANet is effective and robust for fetal ultrasound image classification.

Abstract: ResNet has been widely used in image classification tasks due to its ability
to model the residual dependence of constant mappings for linear computation.
However, the ResNet method adopts a unidirectional transfer of features and
lacks an effective method to correlate contextual information, which is not
effective in classifying fetal ultrasound images in the classification task,
and fetal ultrasound images have problems such as low contrast, high
similarity, and high noise. Therefore, we propose a bilateral multi-scale
information fusion network-based FPDANet to address the above challenges.
Specifically, we design the positional attention mechanism (DAN) module, which
utilizes the similarity of features to establish the dependency of different
spatial positional features and enhance the feature representation. In
addition, we design a bilateral multi-scale (FPAN) information fusion module to
capture contextual and global feature dependencies at different feature scales,
thereby further improving the model representation. FPDANet classification
results obtained 91.05\% and 100\% in Top-1 and Top-5 metrics, respectively,
and the experimental results proved the effectiveness and robustness of
FPDANet.

</details>


### [594] [LinGuinE: Longitudinal Guidance Estimation for Volumetric Lung Tumour Segmentation](https://arxiv.org/pdf/2506.06092)
*Nadine Garibli, Mayank Patwari, Bence Csiba, Yi Wei, Kostas Sidiropoulos*

Main category: eess.IV

TL;DR: LinGuinE is an automated method for segmenting lung tumours in longitudinal CT scans, improving segmentation accuracy by over 20%.


<details>
  <summary>Details</summary>
Motivation: Current lack of automated solutions for longitudinal tumour segmentation in lung cancer studies.

Method: Uses rigid registration and a click validity classifier to propagate tumour points from one time point to another for segmentation.

Result: Improves Dice score by over 20% (p<0.05) across 63 longitudinal studies.

Conclusion: LinGuinE is effective for automated longitudinal tumour segmentation and works with any starting time point.

Abstract: Segmentation of lung gross tumour volumes is an important first step in
radiotherapy and surgical intervention, and is starting to play a role in
assessing chemotherapy response. Response to a drug is measured by tracking the
tumour volumes over a series of CT scans over a time period i.e. a longitudinal
study. However, there currently exist few solutions for automated or
semi-automated longitudinal tumour segmentation. This paper introduces
LinGuinE, an automated method to segment a longitudinal series of lung tumours.
A radiologist must provide an initial input, indicating the location of the
tumour in a CT scan at an arbitrary time point. LinGuinE samples points inside
this tumour and propagates them to another time point using rigid registration.
A click validity classifier selects points which still fall within the tumour;
these are used to automatically create a segmentation in the new time point. We
test LinGuinE on a dataset acquired from a phase 3 clinical trial for lung
tumours and the publicly available 4-D lung CBCT dataset. We find that LinGuinE
improves the Dice on both test sets by over 20% (p< 0.05) across 63
longitudinal studies. We show that any time point can be used as a starting
point, conduct ablation experiments, and find that our LinGuinE setup yields
the best results on both test datasets.

</details>


### [595] [DermaCon-IN: A Multi-concept Annotated Dermatological Image Dataset of Indian Skin Disorders for Clinical AI Research](https://arxiv.org/pdf/2506.06099)
*Shanawaj S Madarkar, Mahajabeen Madarkar, Madhumitha V, Teli Prakash, Konda Reddy Mopuri, Vinaykumar MV, KVL Sathwika, Adarsh Kasturi, Gandla Dilip Raj, PVN Supranitha, Harsh Udai*

Main category: eess.IV

TL;DR: DermaCon-IN is a dermatology dataset from South India with 5,450 images and 240 diagnoses, addressing gaps in AI models for diverse skin tones and conditions. It benchmarks various architectures for scalable, interpretable AI in dermatology.


<details>
  <summary>Details</summary>
Motivation: Current AI models lack robustness due to datasets that don't reflect real-world clinical and demographic diversity, especially in non-Western populations.

Method: The dataset includes 5,450 images from 3,000 patients, annotated by dermatologists, and tests convolutional, transformer-based, and Concept Bottleneck Models.

Result: Baseline performance is established for various architectures, highlighting the potential for integrating anatomical and concept-level cues.

Conclusion: DermaCon-IN offers a scalable, representative foundation for advancing equitable and interpretable dermatology AI.

Abstract: Artificial intelligence is poised to augment dermatological care by enabling
scalable image-based diagnostics. Yet, the development of robust and equitable
models remains hindered by datasets that fail to capture the clinical and
demographic complexity of real-world practice. This complexity stems from
region-specific disease distributions, wide variation in skin tones, and the
underrepresentation of outpatient scenarios from non-Western populations. We
introduce DermaCon-IN, a prospectively curated dermatology dataset comprising
over 5,450 clinical images from approximately 3,000 patients across outpatient
clinics in South India. Each image is annotated by board-certified
dermatologists with over 240 distinct diagnoses, structured under a
hierarchical, etiology-based taxonomy adapted from Rook's classification. The
dataset captures a wide spectrum of dermatologic conditions and tonal variation
commonly seen in Indian outpatient care. We benchmark a range of architectures
including convolutional models (ResNet, DenseNet, EfficientNet),
transformer-based models (ViT, MaxViT, Swin), and Concept Bottleneck Models to
establish baseline performance and explore how anatomical and concept-level
cues may be integrated. These results are intended to guide future efforts
toward interpretable and clinically realistic models. DermaCon-IN provides a
scalable and representative foundation for advancing dermatology AI in
real-world settings.

</details>


### [596] [LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent Diffusion Prior](https://arxiv.org/pdf/2411.02951)
*Xingjian Tang, Jingwei Guan, Linge Li, Ran Shi, Youmei Zhang, Mengye Lyu, Li Yan*

Main category: eess.IV

TL;DR: The paper proposes a Latent Diffusion Prior-based MRI reconstruction method (LDPM) to address computational costs and domain gaps in MRI reconstruction using diffusion models.


<details>
  <summary>Details</summary>
Motivation: Diffusion models show promise for MRI reconstruction but face challenges like high computational costs, domain gaps, and lack of medical fidelity control.

Method: LDPM introduces a sketch-guided pipeline, an MRI-optimized VAE (MR-VAE), and a Dual-Stage Sampler to improve reconstruction quality and efficiency.

Result: LDPM achieves a 3.92 dB PSNR improvement and demonstrates state-of-the-art performance on the fastMRI dataset.

Conclusion: The proposed LDPM method effectively addresses key challenges in MRI reconstruction, balancing perceptual quality and anatomical fidelity.

Abstract: Diffusion models, as powerful generative models, have found a wide range of
applications and shown great potential in solving image reconstruction
problems. Some works attempted to solve MRI reconstruction with diffusion
models, but these methods operate directly in pixel space, leading to higher
computational costs for optimization and inference. Latent diffusion models,
pre-trained on natural images with rich visual priors, are expected to solve
the high computational cost problem in MRI reconstruction by operating in a
lower-dimensional latent space. However, direct application to MRI
reconstruction faces three key challenges: (1) absence of explicit control
mechanisms for medical fidelity, (2) domain gap between natural images and MR
physics, and (3) undefined data consistency in latent space. To address these
challenges, a novel Latent Diffusion Prior-based undersampled MRI
reconstruction (LDPM) method is proposed. Our LDPM framework addresses these
challenges by: (1) a sketch-guided pipeline with a two-step reconstruction
strategy, which balances perceptual quality and anatomical fidelity, (2) an
MRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92
dB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE
\cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM
sampler, which enforces high-fidelity reconstruction in the latent space.
Experiments on the fastMRI dataset\cite{fastmri} demonstrate the
state-of-the-art performance of the proposed method and its robustness across
various scenarios. The effectiveness of each module is also verified through
ablation experiments.

</details>


### [597] [Sketched Equivariant Imaging Regularization and Deep Internal Learning for Inverse Problems](https://arxiv.org/pdf/2411.05771)
*Guixian Xu, Jinglai Li, Junqi Tang*

Main category: eess.IV

TL;DR: Proposes sketched EI regularization for faster unsupervised training of deep imaging networks, reducing computational redundancy. Applied to test-time network adaptation with parameter-efficient methods.


<details>
  <summary>Details</summary>
Motivation: Address inefficiency in high-dimensional applications of EI-based unsupervised training due to computational redundancy.

Method: Introduces sketched EI regularization using randomized sketching for acceleration. Also proposes optimizing only normalization layers for parameter-efficient network adaptation.

Result: Demonstrates significant computational acceleration in X-ray CT and MRI reconstruction tasks compared to standard EI.

Conclusion: Sketched EI and parameter-efficient methods effectively reduce computational costs while maintaining performance in imaging tasks.

Abstract: Equivariant Imaging (EI) regularization has become the de-facto technique for
unsupervised training of deep imaging networks, without any need of
ground-truth data. Observing that the EI-based unsupervised training paradigm
currently has significant computational redundancy leading to inefficiency in
high-dimensional applications, we propose a sketched EI regularization which
leverages the randomized sketching techniques for acceleration. We apply our
sketched EI regularization to develop an accelerated deep internal learning
framework, which can be efficiently applied for test-time network adaptation.
Additionally, for network adaptation tasks, we propose a parameter-efficient
approach to accelerate both EI and Sketched-EI via optimizing only the
normalization layers. Our numerical study on X-ray CT and multicoil magnetic
resonance image reconstruction tasks demonstrate that our approach can achieve
significant computational acceleration over standard EI counterpart in
single-input setting and network adaptation at test time.

</details>


### [598] [An Ensemble-Based Two-Step Framework for Classification of Pap Smear Cell Images](https://arxiv.org/pdf/2503.10312)
*Theo Di Piazza, Loic Boussel*

Main category: eess.IV

TL;DR: A two-stage neural network approach for classifying pap smear images into healthy, unhealthy, both, or rubbish categories to aid in cervical cancer detection.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for automated tools to assist cytologists in managing pap smear workloads and improving early detection of cervical cancer.

Method: A two-stage ensemble approach: first, a neural network filters out rubbish images, and a second network classifies the remaining images into healthy, unhealthy, or both.

Result: The proposed method aims to efficiently classify pap smear images, aiding in early detection and reducing cytologists' workload.

Conclusion: The approach supports automated cervical cancer screening by improving image classification accuracy and efficiency.

Abstract: Early detection of cervical cancer is crucial for improving patient outcomes
and reducing mortality by identifying precancerous lesions as soon as possible.
As a result, the use of pap smear screening has significantly increased,
leading to a growing demand for automated tools that can assist cytologists
managing their rising workload. To address this, the Pap Smear Cell
Classification Challenge (PS3C) has been organized in association with ISBI in
2025. This project aims to promote the development of automated tools for pap
smear images classification. The analyzed images are grouped into four
categories: healthy, unhealthy, both, and rubbish images which are considered
as unsuitable for diagnosis. In this work, we propose a two-stage ensemble
approach: first, a neural network determines whether an image is rubbish or
not. If not, a second neural network classifies the image as containing a
healthy cell, an unhealthy cell, or both.

</details>


### [599] [Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction](https://arxiv.org/pdf/2504.19203)
*Ehsan Karami, Hamid Soltanian-Zadeh*

Main category: eess.IV

TL;DR: Replacing batch normalization with instance normalization, using data augmentation, and applying contrastive loss improves generalization in a deep learning model for KOA prediction.


<details>
  <summary>Details</summary>
Motivation: MRI-based deep learning models for KOA prediction face challenges in generalizability across different imaging data sources.

Method: Used instance normalization, data augmentation, and contrastive loss in a baseline model, trained on OAI database MRI data (FS-IW-TSE and DESS images).

Result: Statistically significant improvement in classification accuracy across domains, outperforming the baseline model.

Conclusion: The proposed modifications enhance model generalization for KOA prediction across diverse imaging data.

Abstract: Knee osteoarthritis (KOA) is a common joint disease that causes pain and
mobility issues. While MRI-based deep learning models have demonstrated
superior performance in predicting total knee replacement (TKR) and disease
progression, their generalizability remains challenging, particularly when
applied to imaging data from different sources. In this study, we have shown
that replacing batch normalization with instance normalization, using data
augmentation, and applying contrastive loss improves model generalization in a
baseline deep learning model for knee osteoarthritis (KOA) prediction. We
trained and evaluated our model using MRI data from the Osteoarthritis
Initiative (OAI) database, considering sagittal fat-suppressed
intermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain
and sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state
(DESS) images as the target domain. The results demonstrate a statistically
significant improvement in classification accuracy across both domains, with
our approach outperforming the baseline model.

</details>


### [600] [Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model](https://arxiv.org/pdf/2505.07449)
*Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He*

Main category: eess.IV

TL;DR: Ophora is an AI model that generates ophthalmic surgical videos from natural language instructions, addressing data scarcity and privacy issues.


<details>
  <summary>Details</summary>
Motivation: High-quality annotated ophthalmic surgical videos are hard to collect due to privacy and labor constraints, necessitating an alternative solution like text-guided video generation.

Method: Ophora uses a Comprehensive Data Curation pipeline to create a dataset (Ophora-160K) and a Progressive Video-Instruction Tuning scheme to adapt a pre-trained T2V model for surgical video generation.

Result: Ophora generates realistic and reliable surgical videos, validated by quantitative analysis and ophthalmologist feedback, and aids downstream surgical workflow tasks.

Conclusion: Ophora successfully addresses data scarcity in ophthalmic surgery by generating high-quality videos from instructions, with potential for broader applications.

Abstract: In ophthalmic surgery, developing an AI system capable of interpreting
surgical videos and predicting subsequent operations requires numerous
ophthalmic surgical videos with high-quality annotations, which are difficult
to collect due to privacy concerns and labor consumption. Text-guided video
generation (T2V) emerges as a promising solution to overcome this issue by
generating ophthalmic surgical videos based on surgeon instructions. In this
paper, we present Ophora, a pioneering model that can generate ophthalmic
surgical videos following natural language instructions. To construct Ophora,
we first propose a Comprehensive Data Curation pipeline to convert narrative
ophthalmic surgical videos into a large-scale, high-quality dataset comprising
over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive
Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge
from a T2V model pre-trained on natural video-text datasets for
privacy-preserved ophthalmic surgical video generation based on Ophora-160K.
Experiments on video quality evaluation via quantitative analysis and
ophthalmologist feedback demonstrate that Ophora can generate realistic and
reliable ophthalmic surgical videos based on surgeon instructions. We also
validate the capability of Ophora for empowering downstream tasks of ophthalmic
surgical workflow understanding. Code is available at
https://github.com/mar-cry/Ophora.

</details>


### [601] [Rate-Accuracy Bounds in Visual Coding for Machines](https://arxiv.org/pdf/2505.14980)
*Ivan V. Bajić*

Main category: eess.IV

TL;DR: The paper explores rate-accuracy bounds for visual coding for machines, showing current methods lag far behind theoretical limits.


<details>
  <summary>Details</summary>
Motivation: The rise of automated analysis of visual signals (e.g., images, videos) necessitates compression strategies optimized for analysis, not reconstruction.

Method: Derives rate-accuracy bounds for visual coding problems, comparing them with state-of-the-art results.

Result: Current methods are 1-3 orders of magnitude behind theoretical bounds in bitrate efficiency.

Conclusion: Significant room exists for improving visual coding methods for machines.

Abstract: Increasingly, visual signals such as images, videos and point clouds are
being captured solely for the purpose of automated analysis by computer vision
models. Applications include traffic monitoring, robotics, autonomous driving,
smart home, and many others. This trend has led to the need to develop
compression strategies for these signals for the purpose of analysis rather
than reconstruction, an area often referred to as "coding for machines." By
drawing parallels with lossy coding of a discrete memoryless source, in this
paper we derive rate-accuracy bounds on several popular problems in visual
coding for machines, and compare these with state-of-the-art results from the
literature. The comparison shows that the current results are at least an order
of magnitude -- and in some cases two or three orders of magnitude -- away from
the theoretical bounds in terms of the bitrate needed to achieve a certain
level of accuracy. This, in turn, means that there is much room for improvement
in the current methods for visual coding for machines.

</details>


### [602] [Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology](https://arxiv.org/pdf/2505.21928)
*Lianghui Zhu, Xitong Ling, Minxi Ouyang, Xiaoping Liu, Tian Guan, Mingxi Fu, Zhiqiang Cheng, Fanglei Fu, Maomao Zeng, Liming Liu, Song Duan, Qiang Huang, Ying Xiao, Jianming Li, Shanming Lu, Zhenghua Piao, Mingxi Zhu, Yibo Jin, Shan Xu, Qiming He, Yizhi Wang, Junru Cheng, Xuanyu Wang, Luxi Xie, Houqiang Li, Sufang Tian, Yonghong He*

Main category: eess.IV

TL;DR: Digepath, a foundation model for GI pathology, uses dual-phase iterative optimization to improve lesion detection in whole-slide images, achieving state-of-the-art performance and high sensitivity for early cancer screening.


<details>
  <summary>Details</summary>
Motivation: GI diseases require precise diagnosis, but conventional methods lack reproducibility and consistency.

Method: Digepath combines pretraining with fine-screening on multi-scale images from 210,043 slides.

Result: It excels in 33/34 GI pathology tasks and achieves 99.70% sensitivity for early cancer detection.

Conclusion: Digepath advances AI-driven precision pathology and addresses gaps in histopathological practice.

Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden,
necessitating precise diagnostic approaches to optimize patient outcomes.
Conventional histopathological diagnosis suffers from limited reproducibility
and diagnostic variability. To overcome these limitations, we develop Digepath,
a specialized foundation model for GI pathology. Our framework introduces a
dual-phase iterative optimization strategy combining pretraining with
fine-screening, specifically designed to address the detection of sparsely
distributed lesion areas in whole-slide images. Digepath is pretrained on over
353 million multi-scale images from 210,043 H&E-stained slides of GI diseases.
It attains state-of-the-art performance on 33 out of 34 tasks related to GI
pathology, including pathological diagnosis, protein expression status
prediction, gene mutation prediction, and prognosis evaluation. We further
translate the intelligent screening module for early GI cancer and achieve
near-perfect 99.70% sensitivity across nine independent medical institutions.
This work not only advances AI-driven precision pathology for GI diseases but
also bridge critical gaps in histopathological practice.

</details>
